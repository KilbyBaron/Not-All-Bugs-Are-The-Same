Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Outward issue link (Duplicate),Outward issue link (Problem/Incident),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Supercedes),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reviewer),Custom field (Reviewers),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Since Version),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
MigrationManager attempts to pull schema from different major version nodes,CASSANDRA-14928,13203519,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,aweisberg,aweisberg,aweisberg,10/Dec/18 19:49,12/Mar/19 14:20,13/Mar/19 22:35,13/Dec/18 18:33,2.2.x,3.0.x,3.11.x,4.0,Legacy/Distributed Metadata,,,,0,pull-request-available,,,"MigrationManager will do the version check against nodes it hasn't connected to yet so it doesn't know their messaging service version. We should also check the version in gossip as an additional layer of protection.

This causes many of the upgrade tests to fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-12-11 22:46:55.496,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 13 18:33:55 UTC 2018,,,,,,0|s01d3s:,9223372036854775807,,,,,,djoshi3,,,,,,,,,,,"10/Dec/18 21:57;aweisberg;||Branch|CircleCI|
|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...aweisberg:14928-2.2?expand=1]|[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14928-2%2E2]|
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...aweisberg:14928-3.0?expand=1]|[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14928-3%2E0]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...aweisberg:14928-3.11?expand=1]|[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14928-3%2E11]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...aweisberg:14928-trunk?expand=1]|[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14928-trunk]|

","11/Dec/18 22:46;djoshi3;[~aweisberg] the changes look good. I wonder if it would be better to add a helper {{FBUtilities::getReleaseVersionMajor()}}. Also note that the {{FBUtilities::getReleaseVersionString()}} may return a string ""Unknown"" in which case you'll experience an exception. Ideally, we'd like to assert that the version is not ""unknown"".","11/Dec/18 23:54;githubbot;Github user aweisberg commented on a diff in the pull request:

    https://github.com/apache/cassandra-dtest/pull/41#discussion_r240837053
  
    --- Diff: upgrade_tests/upgrade_manifest.py ---
    @@ -70,22 +89,19 @@ def clone_with_local_env_version(self):
             return self
     
     
    -indev_2_0_x = None  # None if release not likely
    -current_2_0_x = VersionMeta(name='current_2_0_x', family='2.0.x', variant='current', version='2.0.17', min_proto_v=1, max_proto_v=2, java_versions=(7,))
    -
     indev_2_1_x = VersionMeta(name='indev_2_1_x', family='2.1.x', variant='indev', version='github:apache/cassandra-2.1', min_proto_v=1, max_proto_v=3, java_versions=(7, 8))
    -current_2_1_x = VersionMeta(name='current_2_1_x', family='2.1.x', variant='current', version='2.1.17', min_proto_v=1, max_proto_v=3, java_versions=(7, 8))
    +current_2_1_x = VersionMeta(name='current_2_1_x', family='2.1.x', variant='current', version='2.1.20', min_proto_v=1, max_proto_v=3, java_versions=(7, 8))
     
     indev_2_2_x = VersionMeta(name='indev_2_2_x', family='2.2.x', variant='indev', version='github:apache/cassandra-2.2', min_proto_v=1, max_proto_v=4, java_versions=(7, 8))
    -current_2_2_x = VersionMeta(name='current_2_2_x', family='2.2.x', variant='current', version='2.2.9', min_proto_v=1, max_proto_v=4, java_versions=(7, 8))
    +current_2_2_x = VersionMeta(name='current_2_2_x', family='2.2.x', variant='current', version='2.2.13', min_proto_v=1, max_proto_v=4, java_versions=(7, 8))
     
    -indev_3_0_x = VersionMeta(name='indev_3_0_x', family='3.0.x', variant='indev', version='github:apache/cassandra-3.0', min_proto_v=3, max_proto_v=4, java_versions=(8,))
    -current_3_0_x = VersionMeta(name='current_3_0_x', family='3.0.x', variant='current', version='3.0.12', min_proto_v=3, max_proto_v=4, java_versions=(8,))
    +indev_3_0_x = VersionMeta(name='indev_3_0_x', family='3.0.x', variant='indev', version='github:aweisberg/cassandra-3.0', min_proto_v=3, max_proto_v=4, java_versions=(8,))
    --- End diff --
    
    Like I said in the JIRA I will update the branches on commit once https://issues.apache.org/jira/browse/CASSANDRA-14928 is merged.
",12/Dec/18 21:54;djoshi3;LGTM +1,13/Dec/18 18:33;aweisberg;Committed as [505a03c77764351e1b649e8c7d73d0421e7bcc13|https://github.com/apache/cassandra/commit/505a03c77764351e1b649e8c7d73d0421e7bcc13]. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Short read protection in presence of almost-purgeable range tombstones may cause permanent data loss,CASSANDRA-14515,13165618,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,iamaleksey,iamaleksey,iamaleksey,12/Jun/18 16:46,12/Mar/19 14:20,13/Mar/19 22:35,20/Jun/18 15:15,3.0.17,3.11.3,4.0,,,,,,0,,,,"Because read responses don't necessarily close their open RT bounds, it's possible to lose data during short read protection, if a closing bound is compacted away between two adjacent reads from a node.",,,,,,,,,,,,,,CASSANDRA-14672,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-20 00:07:07.939,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 31 12:50:51 UTC 2018,,,,,,0|i3us1z:,9223372036854775807,,,,,bdeggleston,bdeggleston,,,,,,,,,,,"12/Jun/18 16:56;iamaleksey;It came up during CASSANDRA-14330 investigation/resolution that a read response doesn't necessarily close its outstanding RT. This happens because we stop constructing the response as soon as we've counted sufficient rows to satisfy the requested limit from a node. The fix was incomplete, however, and rather than fixing the assertion we should instead fix the underlying issue, and put an artificial lid on any read response. Otherwise the following sequence of events is possible:

1. The coordinator is sending one of requests to node {{A}}, with limit of {{n}}
2. Node {{A}} replies with a sequence: {{rt-[}}, {{row-0}}, {{row-1}}, {{row-2}}, ..., {{row-n}}
3. {{rt}} is past gc grace, and gets compacted away
4. Some of the rows from {{A}} end up shadowed by deletions from other replicas, and SRP triggers a follow-up read request
5. Node {{A}} replies with a sequence that doesn't contain {{rt-]}}, because it's been compacted away

As a result we have an open-ended RT that can propagate over RR and erase rows it was never intended to erase.","19/Jun/18 23:07;iamaleksey;3.0 branch [here|https://github.com/iamaleksey/cassandra/commits/14515-3.0]. Also, in addition to the unit tests in the branch, 14330 dtest is now essentially a dtest for this ticket.","19/Jun/18 23:29;iamaleksey;[3.11 branch|https://github.com/iamaleksey/cassandra/commits/14515-3.11] and [4.0 branch|https://github.com/iamaleksey/cassandra/commits/14515-4.0].

CI [here|https://circleci.com/workflow-run/98c78715-3725-422f-99c3-81fa4e199666], [here|https://circleci.com/workflow-run/ead96ef6-1d10-4d39-af0b-16a656add658], and [here|https://circleci.com/workflow-run/cce784b8-3131-48b5-aa0d-a3be98cc9741].","20/Jun/18 00:07;bdeggleston;+1, assuming the tests look good. Also, you left some circleci stuff in there which should be removed on commit.","20/Jun/18 00:22;cscotta;Thanks for the quick review! [~beobal], can you review as wel?","20/Jun/18 14:46;beobal;+1 LGTM too. Tests do look good - there's 1 failure in a semi-relevant dtest ({{consistency_test.py::TestConsistency::test_short_read}} on the 3.11 branch), but I've verified it's unrelated to this patch.","20/Jun/18 15:15;iamaleksey;Thanks guys. Committed to 3.0 as [4e23c9e4dba6ee772531d82980f73234bd41869a|https://github.com/apache/cassandra/commit/4e23c9e4dba6ee772531d82980f73234bd41869a] and merged upwards.

The nits you both had (offline) were addressed on commit.",29/Aug/18 07:10;sivann;please check CASSANDRA-14672 it appears this commit might have caused a new issue.,"31/Dec/18 12:50;laxmikant99;Hi All,

Just want to understand the impact area of this ticket  for ppl using 3.11.2 (if table has PK as (id, clust1, clust2) )
{code:java}
1. DELETE FROM test.tombstones WHERE id = ? (causes partition tombstone)
2. DELETE FROM test.tombstones WHERE id = ? AND clust1 = ? (causes multi row tombstone) 
3. DELETE FROM test.tombstones WHERE id = ? AND clust1 = ? AND clust2 = ? (causes single row tombstone) 
4. DELETE FROM test.tombstones WHERE id = ? AND clust1 > ? AND clust1 <= ? (causes range tombstone) 
5. INSERT INTO test.tombstones (id, clust1 ,clust2, val1) VALUES ('id1', 'a','b','c' ) USING TTL 1; (causes TTL tombstone) 
6. DELETE val1 from test.tombstones WHERE id = ? AND clust1 = ? AND clust2 = ? (causes cell tombstone){code}
I assume that 6th query (cell tombstone) is out of danger but what about other queries ?
 Is the only 4th query is impacted or all of the above (except 6th one) ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client TOPOLOGY_CHANGE  messages have wrong port.,CASSANDRA-14398,13153507,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,aweisberg,greg.bestland,greg.bestland,18/Apr/18 22:00,12/Mar/19 14:20,13/Mar/19 22:35,19/Apr/18 16:19,4.0,,,,Legacy/Core,,,,0,client-impacting,,,"Summary:

TOPOLOGY_CHANGE events that are recieved by the client(Driver), with C* 4.0 (Trunk). Contain the storage port (7000) rather than the native port (9042). I believe this is due to changes stuck in for CASSANDRA-7544.  

I was able to track it down to this specific call here.
 [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/StorageService.java#L1703]

We need an accurate port number from this Topology change event, otherwise we won't be able to correlate node up event against the system.peers_v2 table accurately.

C* version I'm testing against : 4.0.x (trunk)

Steps to reproduce:

1. create a single node, cluster, in this case I'm using ccm.
{code:java}
ccm create 400-1 --install-dir=/Users/gregbestland/git/cassandra
ccm populate -n 1
ccm start
{code}
2. Connect the java driver, and check metadata.
 see that there is one node with the correct native port (9042).

3. Add a brand new node:
{code:java}
ccm add node2 -i 127.0.0.2
ccm node2 start
{code}
4. Incoming topology change message to client will have the storage port, rather then the native port in the message.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-04-19 01:06:22.771,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 19 16:19:38 UTC 2018,,,,,,0|i3sqev:,9223372036854775807,,,,,greg.bestland,greg.bestland,,,,,,,,,,,"19/Apr/18 01:06;aweisberg;https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14398-trunk
Proposed fix:
https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-14398-trunk?expand=1",19/Apr/18 15:39;greg.bestland;Accidentally marked as ready to commit. Still reviewing and testing will be done shortly.,19/Apr/18 15:43;greg.bestland;+1 works perfectly. Thanks a ton.,19/Apr/18 16:19;aweisberg;Committed as [cb67bfc1639ded1b6937e7347ad42177ea3f24e3|https://github.com/apache/cassandra/commit/cb67bfc1639ded1b6937e7347ad42177ea3f24e3]. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internode messaging handshake sends wrong messaging version number,CASSANDRA-14540,13167817,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jasobrown,jasobrown,jasobrown,23/Jun/18 12:16,12/Mar/19 14:20,13/Mar/19 22:35,25/Jun/18 13:42,4.0,,,,Legacy/Streaming and Messaging,,,,0,,,,"With the refactor of internode messaging to netty in 4.0, we abstracted the protocol handshakes messages into a class and handlers. There is a bug when the initiator of the connection sends, in the third message of the handshake, it's own default protocol version number ({{MessagingService.current_version}}), rather than the negotiated version. This was not causing any obvious problems when CASSANDRA-8457 was initially committed, but the bug is exposed after CASSANDRA-7544. The problem is during rolling upgrades of 3.0/3.X to 4.0, nodes cannot correctly connect. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-24 20:03:32.595,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 25 13:42:58 UTC 2018,,,,,,0|i3v5vz:,9223372036854775807,,,,,djoshi3,djoshi3,,,,,,,,,,,"23/Jun/18 12:18;jasobrown;Here is a one-line patch to send the correctly negotiated version.

||internode-msg-version-fix||
|[branch|https://github.com/jasobrown/cassandra/tree/internode-msg-version-fix]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/internode-msg-version-fix]|
||

Note: the failing {{MessagingServiceTest}} utest is unrelated to this change, and is failing on [vanilla trunk|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/trunk-circle]. I'll resolve that in a separate ticket.

[~aweisberg] / [~djoshi3]: Can one of you review, please?","24/Jun/18 20:03;djoshi3;Hi [~jasobrown], this is a good catch. Just curious if the upgrade tests dtest caught this?

Regarding, the change, would it be possible to add a test in {{OutboundHandshakeHandlerTest}} to check that we use the negotiated version number?","24/Jun/18 22:29;jasobrown;upgrade_tests are currently disabled/non-funcational (https://issues.apache.org/jira/browse/CASSANDRA-14421) (sadpanda). I'm working on that soon-ish. 

I can add a test, and thanks for the push!","24/Jun/18 23:03;jasobrown;Tests added to {{OutboundHandshakeHandlerTest}}, and a new commit is on the same branch.","25/Jun/18 05:16;djoshi3;Thanks, [~jasobrown] LGTM +1",25/Jun/18 13:42;jasobrown;committed as sha {{5db822b71ad7278ca6443455d029dd79e22388d8}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building javadoc with Java11 fails,CASSANDRA-14988,13209854,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,tommy_s,tommy_s,tommy_s,16/Jan/19 10:24,12/Mar/19 14:20,13/Mar/19 22:35,22/Jan/19 19:14,4.0,,,,Documentation/Javadoc,,,,0,Java11,,,"When building trunk with Java11 building javadoc fails with this error:
{noformat}
[javadoc] /repos/tmp/cassandra/src/java/org/apache/cassandra/hints/HintsBufferPool.java:28: error: package sun.nio.ch is not visible
[javadoc] import sun.nio.ch.DirectBuffer;
[javadoc] ^
[javadoc] (package sun.nio.ch is declared in module java.base, which does not export it to the unnamed module)
[javadoc] 1 error{noformat}
This import is unused and was probably added by mistake, removing it fixes the problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-01-22 07:47:08.8,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 22 19:16:24 UTC 2019,,,,,,0|y0028o:,9223372036854775807,,,,,,djoshi3,,,,,,,,,,,16/Jan/19 10:54;tommy_s;I have a branch with a patch to remove the unused import here: [cassandra-14988|https://github.com/tommystendahl/cassandra/tree/cassandra-14988],"22/Jan/19 07:47;djoshi3;Hi [~tommy_s] I can't repro this issue. My build goes through fine with OpenJDK 11.0.2. See below -

{noformat}
     [exec] copying static files... done
     [exec] copying extra files... done
     [exec] dumping search index in English (code: en) ... done
     [exec] dumping object inventory... done
     [exec] build succeeded, 153 warnings.
     [exec]
     [exec] The HTML pages are in build/html.
     [exec]
     [exec] Build finished. The HTML pages are in build/html.

BUILD SUCCESSFUL
Total time: 1 minute 58 seconds
{noformat}

Could you please add more info here? JDK version, are you using OpenJDK or Oracle JDK? How are you invoking the doc generation?  What are your environment variables set to esp. JAVA_HOME and JAVA8_HOME?","22/Jan/19 09:34;tommy_s;Hi [~djoshi3]

Thanks for looking in to this issue.

I use Oracle jdk 11.0.2 and ant 1.10.5.

 
{noformat}
$ echo $JAVA_HOME
/usr/lib/jvm/jdk-11.0.2/

$ echo $JAVA8_HOME
/usr/lib/jvm/jdk1.8.0_191{noformat}
To reproduce the problem I build with {{ant realclean build javadoc}}, if I just do {{ant realclean javadoc}} I don't get this problem.

I tested with OpenJdk 11.0.2 and got the same issue, I have also tested Oracle jdk 11.0 and 11.0.1.

 ",22/Jan/19 17:39;djoshi3;[~tommy_s] I was able to repro it with the steps you provided. Your change fixes it. +1.,"22/Jan/19 19:14;jjirsa;Committed as {{4aa022e215bcffbf84becea9253ef07b2e87a19b}}
","22/Jan/19 19:16;djoshi3;Thanks, [~jjirsa]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After deleting data in 3.11.3, reads fail with ""open marker and close marker have different deletion times""",CASSANDRA-14672,13181807,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,iamaleksey,sivann,sivann,29/Aug/18 07:09,12/Mar/19 14:20,13/Mar/19 22:35,25/Sep/18 16:48,3.0.18,3.11.4,4.0,,Legacy/Local Write-Read Paths,,,,0,,,,"We had 3.11.0, then we upgraded to 3.11.3 last week. We routinely perform deletions as the one described below. After upgrading we run the following deletion query:

 
{code:java}
DELETE FROM measurement_events_dbl WHERE measurement_source_id IN ( 9df798a2-6337-11e8-b52b-42010afa015a,  9df7717e-6337-11e8-b52b-42010afa015a, a08b8042-6337-11e8-b52b-42010afa015a, a08e52cc-6337-11e8-b52b-42010afa015a, a08e6654-6337-11e8-b52b-42010afa015a, a08e6104-6337-11e8-b52b-42010afa015a, a08e6c76-6337-11e8-b52b-42010afa015a, a08e5a9c-6337-11e8-b52b-42010afa015a, a08bcc50-6337-11e8-b52b-42010afa015a) AND year IN (2018) AND measurement_time >= '2018-07-19 04:00:00'{code}
 

Immediately after that, trying to read the last value produces an error:
{code:java}
select * FROM measurement_events_dbl WHERE measurement_source_id = a08b8042-6337-11e8-b52b-42010afa015a AND year IN (2018) order by measurement_time desc limit 1;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 2 failures"" info={'failures': 2, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}{code}
 

And the following exception: 
{noformat}
WARN [ReadStage-4] 2018-08-29 06:59:53,505 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-4,5,main]: {}
java.lang.RuntimeException: java.lang.IllegalStateException: UnfilteredRowIterator for pvpms_mevents.measurement_events_dbl has an illegal RT bounds sequence: open marker and close marker have different deletion times
 at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2601) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_181]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.3.jar:3.11.3]
 at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181]
Caused by: java.lang.IllegalStateException: UnfilteredRowIterator for pvpms_mevents.measurement_events_dbl has an illegal RT bounds sequence: open marker and close marker have different deletion times
 at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.ise(RTBoundValidator.java:103) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.applyToMarker(RTBoundValidator.java:81) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:148) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:136) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:92) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:79) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:308) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:187) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:180) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:176) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:76) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:352) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1889) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2597) ~[apache-cassandra-3.11.3.jar:3.11.3]
 ... 5 common frames omitted
 Suppressed: java.lang.IllegalStateException: UnfilteredRowIterator for pvpms_mevents.measurement_events_dbl has an illegal RT bounds sequence: expected all RTs to be closed, but the last one is open
 at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.ise(RTBoundValidator.java:103) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.onPartitionClose(RTBoundValidator.java:96) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.BaseRows.runOnClose(BaseRows.java:91) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.BaseIterator.close(BaseIterator.java:86) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:309) ~[apache-cassandra-3.11.3.jar:3.11.3]
 ... 12 common frames omitted
 
{noformat}
 

We have 9 nodes ~2TB each, leveled compaction, repairs run daily in sequence.

Table definition is:
{noformat}
CREATE TABLE pvpms_mevents.measurement_events_dbl (
 measurement_source_id uuid,
 year int,
 measurement_time timestamp,
 event_reception_time timestamp,
 quality double,
 value double,
 PRIMARY KEY ((measurement_source_id, year), measurement_time)
) WITH CLUSTERING ORDER BY (measurement_time ASC)
 AND bloom_filter_fp_chance = 0.1
 AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
 AND comment = ''
 AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'}
 AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
 AND crc_check_chance = 1.0
 AND dclocal_read_repair_chance = 0.1
 AND default_time_to_live = 0
 AND gc_grace_seconds = 864000
 AND max_index_interval = 2048
 AND memtable_flush_period_in_ms = 0
 AND min_index_interval = 128
 AND read_repair_chance = 0.0
 AND speculative_retry = '99PERCENTILE';{noformat}
 

We host those on GCE and recreated all the nodes with disk snapshots, and we reproduced the error: after re-running the DELETE with all nodes up and no other queries running, the error was reproduced immediately.

 

We tried so far:

re-running repairs on all nodes and running nodetool garbagecollect with no success.

We downgraded to 3.11.2 for now, no issues so far.

This may be related to CASSANDRA-14515","CentOS 7, GCE, 9 nodes, 4TB disk/~2TB full each, level compaction, timeseries data",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-17 08:29:13.157,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 25 16:48:30 UTC 2018,,,,,,0|i3xjaf:,9223372036854775807,3.0.17,3.11.3,,,bdeggleston,bdeggleston,,,,3.0.0,,,,,,,"17/Sep/18 08:29;mitsis;We did some testing on a replica environment. Specifically added some printf statements to check the timestamps on open and close marker that Cassandra complains.

The replication factor is 3, nodes 6, 8 & 9 contain the data. Node 8 & 9 are throwing the exception, node 6 when queried with consistency one does not fail (returns proper data - see below why).

On node8 & node9 the following timestamps are found:

Printf:

{noformat}
==> openMarkerDeletionTime: null
==> openMarkerDeletionTime: deletedAt=1537103654634113, localDeletion=1537103654
==> deletionTime: deletedAt=1530205388555918, localDeletion=1530205388
{noformat}

Exception:
{noformat}
WARN [ReadStage-1] 2018-09-16 14:40:44,252 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-1,5,main]: {}
java.lang.IllegalStateException: ==> UnfilteredRowIterator for pvpms_mevents.measurement_events_dbl has an illegal RT bounds sequence: open marker and close marker have different deletion times [deletedAt=1537103654634113, localDeletion=1537103654 deletedAt=1530205388555918, localDeletion=1530205388]
{noformat}

Converted timestamps:
{noformat}
openDeletionTime: Sun Sep 16 13:14:14 UTC 2018
closeDeletionTime: Thu Jun 28 17:03:08 UTC 2018
{noformat}

DELETE FROM command above was run on Sep 16.

On node6 :

Printf:
{noformat}
==> openMarkerDeletionTime: null
==> openMarkerDeletionTime: deletedAt=1537103654634113, localDeletion=1537103654
==> deletionTime: deletedAt=1537103654634113, localDeletion=1537103654
{noformat}

Converted timestamps:
{noformat}
Sun Sep 16 13:14:14 UTC 2018
{noformat}

No Exception!

We did a json dump of the specified data from node 8 & 9 and found a range_tombstone_bound with timestamp start/end of ""2018-06-28T17:03:08Z"" that contains data from Jul & Aug (see rows below). On node 6 the same same data are not within a tombstone marker (it’s the same json without the range_tombstone_bound).

{noformat}
   ""partition"" : {
      {
        ""type"" : ""range_tombstone_bound"",
        ""start"" : {
          ""type"" : ""exclusive"",
          ""clustering"" : [ ""2018-06-27 04:55:00.000Z"" ],
          ""deletion_info"" : { ""marked_deleted"" : ""2018-06-28T17:03:08.555918Z"", ""local_delete_time"" : ""2018-06-28T17:03:08Z"" }
        }
      },
      {
        ""type"" : ""row"",
        ""position"" : 83860313,
        ""clustering"" : [ ""2018-06-27 05:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-06-28T19:45:30.803293Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-06-28 19:45:30.784Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 408307.66 }
        ]
      },
…
      {
        ""type"" : ""row"",
        ""position"" : 83953463,
        ""clustering"" : [ ""2018-07-19 03:45:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-07-19T03:46:29.195118Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-07-19 03:46:29.193Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 593846.06 }
        ]
      },
…
      {
        ""type"" : ""row"",
        ""position"" : 84054985,
        ""clustering"" : [ ""2018-08-11 04:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-08-11T04:01:15.708470Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-08-11 04:01:15.703Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 372654.53 }
        ]
      },
      {
        ""type"" : ""range_tombstone_bound"",
        ""end"" : {
          ""type"" : ""inclusive"",
          ""deletion_info"" : { ""marked_deleted"" : ""2018-06-28T17:03:08.555918Z"", ""local_delete_time"" : ""2018-06-28T17:03:08Z"" }
        }
      }
{noformat}

We have downgraded on the mean time to Cassandra 3.11.2. Shouldn't at least these inconsistencies have more graceful assertions?
","17/Sep/18 13:29;jjirsa;In chatting with the folks who wrote CASSANDRA-14515 offline (namely [~iamaleksey] and [~benedict] ), it sounds like what you're seeing is likely corruption that CASSANDRA-14515 was meant to protect. That is: the bug in cassandra 3.11.0 to 3.11.2 that causes data loss (14515) is also leaving your data files in an invalid corrupt state. The exception is letting you know it's broken, and in this case, that you've probably lost data due to that bug.

[~iamaleksey] / [~benedict] - any thoughts on how to prove this is really just 14515 corruption? Any ideas on recovery (will scrub help here)? ","18/Sep/18 09:56;iamaleksey;[~jjirsa] I don't think we can say that this was CASSANDRA-14515 corruption specifically.

[~mitsis] Where is this dump from? I'm interested in seeing both mismatched bounds in a dump from an affected node, to prove conclusively that there is corrupt data on disk - and I'm yet to see the half with deletion time in September.","18/Sep/18 11:55;mitsis;[~iamaleksey] The dump is from the ""bad"" node 9 and truncated heavily. I'm presenting the dumps that contain the specific partition key below. These dumps are after DELETE (see in ticket the full command) was run on Sep 16 on a replica cluster created with snapshots from Aug 29.

From node 9 there are 3 SSTables with that partition key:

* SSTable 1 below (I've truncated a lot of rows):

{noformat}
mc-2228-big-Data.json 
  {
    ""partition"" : {
      ""key"" : [ ""a08b8042-6337-11e8-b52b-42010afa015a"", ""2018"" ],
      ""position"" : 83756793
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 83756879,
        ""clustering"" : [ ""2018-06-03 18:45:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-06-03T19:48:27.570903Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-06-03 19:48:27.570Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 185532.77 }
        ]
      },
*
* many more rows
*
      {
        ""type"" : ""row"",
        ""position"" : 83860241,
        ""clustering"" : [ ""2018-06-27 04:45:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-06-27T04:50:08.969007Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-06-27 04:50:08.968Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 408307.66 }
        ]
      },
      {
        ""type"" : ""range_tombstone_bound"",
        ""start"" : {
          ""type"" : ""exclusive"",
          ""clustering"" : [ ""2018-06-27 04:55:00.000Z"" ],
          ""deletion_info"" : { ""marked_deleted"" : ""2018-06-28T17:03:08.555918Z"", ""local_delete_time"" : ""2018-06-28T17:03:08Z"" }
        }
      },
      {
        ""type"" : ""row"",
        ""position"" : 83860313,
        ""clustering"" : [ ""2018-06-27 05:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-06-28T19:45:30.803293Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-06-28 19:45:30.784Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 408307.66 }
        ]
      },
*
* many more rows
* 
      {
        ""type"" : ""row"",
        ""position"" : 84054985,
        ""clustering"" : [ ""2018-08-11 04:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-08-11T04:01:15.708470Z"" },
        ""cells"" : [
          { ""name"" : ""event_reception_time"", ""value"" : ""2018-08-11 04:01:15.703Z"" },
          { ""name"" : ""quality"", ""value"" : 100.0 },
          { ""name"" : ""value"", ""value"" : 372654.53 }
        ]
      },
      {
        ""type"" : ""range_tombstone_bound"",
        ""end"" : {
          ""type"" : ""inclusive"",
          ""deletion_info"" : { ""marked_deleted"" : ""2018-06-28T17:03:08.555918Z"", ""local_delete_time"" : ""2018-06-28T17:03:08Z"" }
        }
      }
    ]
  }
{noformat}

* SSTable 2 (I've truncated NOTHING)

{noformat}
mc-35045-big-Data.json 
  {
    ""partition"" : {
      ""key"" : [ ""a08b8042-6337-11e8-b52b-42010afa015a"", ""2018"" ],
      ""position"" : 194666614
    },
    ""rows"" : [
      {
        ""type"" : ""range_tombstone_bound"",
        ""start"" : {
          ""type"" : ""inclusive"",
          ""clustering"" : [ ""2018-07-19 04:00:00.000Z"" ],
          ""deletion_info"" : { ""marked_deleted"" : ""2018-09-16T13:14:14.634113Z"", ""local_delete_time"" : ""2018-09-16T13:14:14Z"" }
        }
      },
      {
        ""type"" : ""range_tombstone_bound"",
        ""end"" : {
          ""type"" : ""inclusive"",
          ""deletion_info"" : { ""marked_deleted"" : ""2018-09-16T13:14:14.634113Z"", ""local_delete_time"" : ""2018-09-16T13:14:14Z"" }
        }
      }
    ]
  }
{noformat}

* SSTable 3 contains rows with dates *before ""2018-06-03 18:45:00.000Z""* with no tombstone information so not presented here.

From node 6 (the ""good"" node) there are also 3 SSTables with that partition key:

* SSTable 1 below (I've truncated NOTHING):

{noformat}
mc-2763209-big-Data.json 
  {
    ""partition"" : {
      ""key"" : [ ""a08b8042-6337-11e8-b52b-42010afa015a"", ""2018"" ],
      ""position"" : 101207516
    },
    ""rows"" : [
      {
        ""type"" : ""range_tombstone_bound"",
        ""start"" : {
          ""type"" : ""inclusive"",
          ""clustering"" : [ ""2018-07-19 04:00:00.000Z"" ],
          ""deletion_info"" : { ""marked_deleted"" : ""2018-09-16T08:37:18.429276Z"", ""local_delete_time"" : ""2018-09-16T08:37:18Z"" }
        }
      },
      {
        ""type"" : ""range_tombstone_bound"",
        ""end"" : {
          ""type"" : ""inclusive"",
          ""deletion_info"" : { ""marked_deleted"" : ""2018-09-16T08:37:18.429276Z"", ""local_delete_time"" : ""2018-09-16T08:37:18Z"" }
        }
      }
    ]
  }
{noformat}

* SSTable 2 below (I've truncated NOTHING):

{noformat}
mc-2763214-big-Data.json 
  {
    ""partition"" : {
      ""key"" : [ ""a08b8042-6337-11e8-b52b-42010afa015a"", ""2018"" ],
      ""position"" : 191837790
    },
    ""rows"" : [
      {
        ""type"" : ""range_tombstone_bound"",
        ""start"" : {
          ""type"" : ""inclusive"",
          ""clustering"" : [ ""2018-07-19 04:00:00.000Z"" ],
          ""deletion_info"" : { ""marked_deleted"" : ""2018-09-16T13:14:14.634113Z"", ""local_delete_time"" : ""2018-09-16T13:14:14Z"" }
        }
      },
      {
        ""type"" : ""range_tombstone_bound"",
        ""end"" : {
          ""type"" : ""inclusive"",
          ""deletion_info"" : { ""marked_deleted"" : ""2018-09-16T13:14:14.634113Z"", ""local_delete_time"" : ""2018-09-16T13:14:14Z"" }
        }
      }
    ]
  }
{noformat}

* SSTable 3 contains rows with dates *up to ""2018-08-17 13:30:00.000Z""* with no tombstone information so not presented here.
","18/Sep/18 12:18;iamaleksey;Thanks. At first glance, individually these dumps seem normal. So this could be a bug in reading/merging, or even a bug in RT validation logic.

I'll do my best to investigate further this week.",18/Sep/18 12:30;iamaleksey;[~mitsis] JIRA no longer displays email addresses (presumably due to GDPR). If you could maybe share those sstables alongside schema with me (privately and confidentially) please contact me at aleksey@apache.org. Having the sstables will go a long way with helping to reproduce locally and determine what the problem is.,"21/Sep/18 16:38;iamaleksey;Quick update: the bug is legit, the problem lies in {{PurgeFunction}} applied to the merged iterator, before the filtering step. Its logic to partially purge a range tombstone is broken ({{PurgeFunction.applyToMarker()}}), or, rather, what's broken are {{RangeTombstoneBoundaryMarker.createCorrespondingCloseMarker()}} and {{RangeTombstoneBoundaryMarker.createCorrespondingOpenMarker()}} methods - in case of reverse iteration they don't swap the deletion times, leading to creation of a range tombstone marker with an invalid deletion time when reading in DESC order.

The patch is a trivial two-liner, but I'll need to finish unit tests and complete analysis of worst-case impact of the issue in 3.11.2 and corresponding 3.0 versions before {{RTBoundValidator}} introduction.","24/Sep/18 15:34;iamaleksey;3.0: [code|https://github.com/iamaleksey/cassandra/commits/14672-3.0], [utests|https://circleci.com/gh/iamaleksey/cassandra/771], [dtests|https://circleci.com/gh/iamaleksey/cassandra/789]
3.11: [code|https://github.com/iamaleksey/cassandra/commits/14672-3.11], [utests|https://circleci.com/gh/iamaleksey/cassandra/772], [dtests|https://circleci.com/gh/iamaleksey/cassandra/784]
4.0: [code|https://github.com/iamaleksey/cassandra/commits/14672-4.0], [utests|https://circleci.com/gh/iamaleksey/cassandra/775], [dtests|https://circleci.com/gh/iamaleksey/cassandra/786]","24/Sep/18 15:39;iamaleksey;Will rerun dtests once the underlying issue with the yaml has been taken care of, but it's otherwise ready to review.",24/Sep/18 21:57;iamaleksey;[~bdeggleston] CI's fine now,25/Sep/18 16:02;bdeggleston;+1,"25/Sep/18 16:48;iamaleksey;Thanks, committed to 3.0 as [d496dca6729853ece49d68c4837fed35149c95d0|https://github.com/apache/cassandra/commit/d496dca6729853ece49d68c4837fed35149c95d0] and merged upwards with 3.11 and 4.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable loader exception when loading 3.0/3.11 compact tables into 4.0,CASSANDRA-14895,13198746,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,djoshi3,aweisberg,aweisberg,15/Nov/18 21:39,12/Mar/19 14:20,13/Mar/19 22:35,15/Jan/19 16:59,4.0,,,,Legacy/Tools,,,,0,4.0-pre-rc-bugs,,,"While working on the upgrade tests I added 3.0/3.11 to current tests for loading old version sstables using sstable loader. [The tests for loading compact sstables fail.|addedhttps://github.com/apache/cassandra-dtest/blob/master/upgrade_tests/storage_engine_upgrade_test.py#L466]

It doesn't help to alter the table to drop compact storage and then run rebuild and cleanup before attempting to load into current.

Failed to list files in /var/folders/vx/2fcrbbx12g9bppxk7h41ww700000gn/T/dtest-4_4vb5jj/test/node1/data1_copy/ks/counter1-f4dc7fc0e91011e892e9c3e97b26557e
java.lang.RuntimeException: Unknown column value during deserialization
java.lang.RuntimeException: Failed to list files in /var/folders/vx/2fcrbbx12g9bppxk7h41ww700000gn/T/dtest-4_4vb5jj/test/node1/data1_copy/ks/counter1-f4dc7fc0e91011e892e9c3e97b26557e
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:77)
	at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getFiles(LifecycleTransaction.java:561)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:76)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:166)
	at org.apache.cassandra.tools.BulkLoader.load(BulkLoader.java:83)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:49)
Caused by: java.lang.RuntimeException: Unknown column value during deserialization
	at org.apache.cassandra.db.SerializationHeader$Component.toHeader(SerializationHeader.java:317)
	at org.apache.cassandra.io.sstable.format.SSTableReader.openForBatch(SSTableReader.java:440)
	at org.apache.cassandra.io.sstable.SSTableLoader.lambda$openSSTables$0(SSTableLoader.java:121)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.lambda$innerList$2(LogAwareFileLister.java:99)
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)
	at java.util.TreeMap$EntrySpliterator.forEachRemaining(TreeMap.java:2969)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.innerList(LogAwareFileLister.java:101)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:73)
	... 5 more
Exception in thread ""main"" org.apache.cassandra.tools.BulkLoadException: java.lang.RuntimeException: Failed to list files in /var/folders/vx/2fcrbbx12g9bppxk7h41ww700000gn/T/dtest-4_4vb5jj/test/node1/data1_copy/ks/counter1-f4dc7fc0e91011e892e9c3e97b26557e
	at org.apache.cassandra.tools.BulkLoader.load(BulkLoader.java:96)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:49)
Caused by: java.lang.RuntimeException: Failed to list files in /var/folders/vx/2fcrbbx12g9bppxk7h41ww700000gn/T/dtest-4_4vb5jj/test/node1/data1_copy/ks/counter1-f4dc7fc0e91011e892e9c3e97b26557e
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:77)
	at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getFiles(LifecycleTransaction.java:561)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:76)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:166)
	at org.apache.cassandra.tools.BulkLoader.load(BulkLoader.java:83)
	... 1 more
Caused by: java.lang.RuntimeException: Unknown column value during deserialization
	at org.apache.cassandra.db.SerializationHeader$Component.toHeader(SerializationHeader.java:317)
	at org.apache.cassandra.io.sstable.format.SSTableReader.openForBatch(SSTableReader.java:440)
	at org.apache.cassandra.io.sstable.SSTableLoader.lambda$openSSTables$0(SSTableLoader.java:121)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.lambda$innerList$2(LogAwareFileLister.java:99)
	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)
	at java.util.TreeMap$EntrySpliterator.forEachRemaining(TreeMap.java:2969)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.innerList(LogAwareFileLister.java:101)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:73)
	... 5 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-01-15 06:49:42.472,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 15 18:05:15 UTC 2019,,,,,,0|s00jxk:,9223372036854775807,,,,,aweisberg,aweisberg,,,,,,,,,,,02/Jan/19 18:32;aweisberg;So it doesn't get lost this is a test bug that is going to be fixed in CASSANDRA-14421,"10/Jan/19 22:22;aweisberg;This didn't get merged in with CASSANDRA-14421, I'll merge it now.",10/Jan/19 22:22;aweisberg;https://github.com/aweisberg/cassandra-dtest/commit/c081a8cffd5a8a9e4a48284c2e13c8acd3a043eb,15/Jan/19 02:59;aweisberg;Passes https://circleci.com/gh/aweisberg/cassandra/2494#tests/containers/,15/Jan/19 06:49;djoshi3;Thanks [~aweisberg]. Is there anything you need from me on this ticket?,"15/Jan/19 16:23;aweisberg;All good, going to commit this today. I just lost track of where I had run the tests last time so I am running them again. Took a while because it was both upgrade and dtests.","15/Jan/19 16:59;aweisberg;Committed as [4c1479b5f457c3a8ed0302461ef79331cc13e798|https://github.com/apache/cassandra-dtest/commit/4c1479b5f457c3a8ed0302461ef79331cc13e798] to the dtests. Thanks!

Test results https://circleci.com/gh/aweisberg/cassandra/tree/14895-trunk","15/Jan/19 18:05;djoshi3;Thanks, [~aweisberg]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reverse order queries in presence of range tombstones may cause permanent data loss,CASSANDRA-14513,13165591,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,beobal,beobal,beobal,12/Jun/18 15:01,12/Mar/19 14:20,13/Mar/19 22:35,14/Jun/18 17:23,3.0.17,3.11.3,4.0,,Legacy/Core,Legacy/CQL,Legacy/Local Write-Read Paths,,0,,,,"Slice queries in descending sort order can create oversized artificial range tombstones. At CL > ONE, read repair can propagate these tombstones to all replicas, wiping out vast data ranges that they mistakenly cover.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-12 16:01:47.456,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 14 18:52:21 UTC 2018,,,,,,0|i3urvz:,9223372036854775807,,,,,iamaleksey,iamaleksey,,,,,,,,,,,"12/Jun/18 15:56;beobal;The problem manifests when executing a slice query with reverse ordering against an indexed partition if the upper bound of the query precedes the first clustering in the partition for a given SSTable.

The initial search of the index correctly identifies that the slice bounds are not contained within the partition and {{ReverseIndexedReader::setForSlice}} returns an empty iterator. However, it doesn’t update the pointer to the current index block in {{IndexState}}. The pointer remains set to the size of the column index, so that when the initial empty iterator is exhausted {{ReversedIndexReader::hasNextInternal}} incorrectly assumes that there is more to do, bumps the pointer back one to the last index block and starts reading.

If a range tombstone spans the boundary between the penultimate and final index blocks, the iterator will emit the end marker after first altering the bounds to match those of the query. The assumption made is that only data that falls within the bounds of the query slice will be read from disk and so adjusting the tombstone bounds in this way is simply a narrowing of the range tombstone. The index block pointer bug invalidates this assumption and so a wholly new and invalid marker is generated.

On a single node this new marker alone can shadow live data in other sstables, but the effect is transient. A tombstone never gets written to disk and when the SSTable is compacted, the layout of the partition on disk will _likely_ no longer trigger the bug (though is no guarantee of this).

In a multi-node scenario read repair can cause the erroneous marker to be matched to an (unrelated) marker from another replica, creating a new tombstone, potentially with a very wide range. This is then propagated to all replicas, causing data loss from the partition.","12/Jun/18 16:01;iamaleksey;A dtest representing both scenarios can be found [here|https://github.com/iamaleksey/cassandra-dtest/commits/14513].

{{test_14513_transient}} shows that the issue can be reproduced with just one node - although there is no permanent data loss here, just queries not returning all the results they are supposed to. Which is bad in itself, but not as bad as the other scenario.

{{test_14513_permanent}} illustrates how that oversized tombstone can be propagated by read repair to every replica and wipe out the partition.

Both tests are a bit longer than they need be - minimal reproduction can be achieved in half as much code, but I opted for showing the full impact in an intentionally more verbose manner.","12/Jun/18 16:41;beobal;Trivial fix is to correctly adjust the current index pointer in IndexState when the slice bounds are found to be wholly before the start of the partition. It might make sense to open a follow up JIRA to investigate whether the modification of the tombstone bounds (in {{RangeTombstoneList::reverseIterator}}, and maybe {{forwardIterator}} if necessary) can be tightened up by asserting that any newly generated bounds are not disjoint from the query slice.
||branch||CircleCI||
|[3.0|https://github.com/beobal/cassandra/tree/14513-3.0]|[circle|https://circleci.com/workflow-run/16abca6e-d7e8-4671-aaeb-4f9de32b8190]|
|[3.11|https://github.com/beobal/cassandra/tree/14513-3.11]|[circle|https://circleci.com/workflow-run/7c79b1d7-26db-436f-b156-cdf05284b85a]|
|[trunk|https://github.com/beobal/cassandra/tree/14513-4.0]|[circle|https://circleci.com/workflow-run/076bf598-8a40-4637-9de2-f936c62f8863]|

 CI runs are using [~iamaleksey]'s dtest branch mentioned in the previous comment.",14/Jun/18 11:16;iamaleksey;+1,14/Jun/18 17:23;beobal;Thanks. Committed to 3.0 as {{eb91942f64972bef04c4e965dcdf788ae1f21a60}} and merged to 3.11 & trunk.,14/Jun/18 18:52;iamaleksey;Committed the dtests as [51c8352020b8df3fe04344ae88c29d2a73a228bd|https://github.com/apache/cassandra-dtest/commit/51c8352020b8df3fe04344ae88c29d2a73a228bd].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to start repair process,CASSANDRA-14530,13166955,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,,pheonixs,pheonixs,19/Jun/18 14:24,12/Mar/19 14:20,13/Mar/19 22:35,21/Jun/18 11:55,,,,,Consistency/Repair,Legacy/Core,Local/Compaction,,0,,,," 

Then repair starts, cassandra goes down.","Description: Debian GNU/Linux 8.5 (jessie)
 Codename: jessie

16GB mem. 6 allocated by java only.

Linux XXXX04 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2+deb8u3 (2016-07-02) x86_64 GNU/Linux

cassandra 3.11.2

 

Output of command

XProf file attached below
{code:java}
XXXXXXXXX04:/var/lib/cassandra# nodetool repair archive statuses -full
[2018-06-19 17:17:38,170] Starting repair command #1 (84fd0590-73cb-11e8-9815-6d9cda004dd5), repairing keyspace archive with repair options (parallelism: parallel, primary range: false, incremental: false, job threads: 1, ColumnFamilies: [statuses], dataCenters: [], hosts: [], # of ranges: 967, pull repair: false)
Exception occurred during clean-up. java.lang.reflect.UndeclaredThrowableException
Cassandra has shutdown.
error: [2018-06-19 17:17:42,233] JMX connection closed. You should check server log for repair status of keyspace archive(Subsequent keyspaces are not going to be repaired).
-- StackTrace --
java.io.IOException: [2018-06-19 17:17:42,233] JMX connection closed. You should check server log for repair status of keyspace archive(Subsequent keyspaces are not going to be repaired).
at org.apache.cassandra.tools.RepairRunner.handleConnectionFailed(RepairRunner.java:98)
at org.apache.cassandra.utils.progress.jmx.JMXNotificationProgressListener.handleNotification(JMXNotificationProgressListener.java:86)
at javax.management.NotificationBroadcasterSupport.handleNotification(NotificationBroadcasterSupport.java:275)
at javax.management.NotificationBroadcasterSupport$SendNotifJob.run(NotificationBroadcasterSupport.java:352)
at javax.management.NotificationBroadcasterSupport$1.execute(NotificationBroadcasterSupport.java:337)
at javax.management.NotificationBroadcasterSupport.sendNotification(NotificationBroadcasterSupport.java:248)
at javax.management.remote.rmi.RMIConnector.sendNotification(RMIConnector.java:441)
at javax.management.remote.rmi.RMIConnector.access$1200(RMIConnector.java:121)
at javax.management.remote.rmi.RMIConnector$RMIClientCommunicatorAdmin.gotIOException(RMIConnector.java:1531)
at javax.management.remote.rmi.RMIConnector$RMINotifClient.fetchNotifs(RMIConnector.java:1352)
at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.fetchOneNotif(ClientNotifForwarder.java:655)
at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.fetchNotifs(ClientNotifForwarder.java:607)
at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.doRun(ClientNotifForwarder.java:471)
at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.run(ClientNotifForwarder.java:452)
at com.sun.jmx.remote.internal.ClientNotifForwarder$LinearExecutor$1.run(ClientNotifForwarder.java:108)

{code}",,,,,,,,,,,,,,,,,19/Jun/18 14:24;pheonixs;hs_err_1529417753.log;https://issues.apache.org/jira/secure/attachment/12928363/hs_err_1529417753.log,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 21 11:55:19 UTC 2018,,,,,,0|i3v0kn:,9223372036854775807,3.11.2,,,,,,,,,,,,,,,,21/Jun/18 11:55;pheonixs;Seems to be fixed by setting vm.max_map_count = 1048575,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTables stop being compacted,CASSANDRA-14423,13155605,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,KurtG,KurtG,KurtG,27/Apr/18 04:05,12/Mar/19 14:20,13/Mar/19 22:35,29/Jun/18 08:16,2.2.13,3.0.17,3.11.3,,Local/Compaction,,,,1,,,,"So seeing a problem in 3.11.0 where SSTables are being lost from the view and not being included in compactions/as candidates for compaction. It seems to get progressively worse until there's only 1-2 SSTables in the view which happen to be the most recent SSTables and thus compactions completely stop for that table.

The SSTables seem to still be included in reads, just not compactions.

The issue can be fixed by restarting C*, as it will reload all SSTables into the view, but this is only a temporary fix. User defined/major compactions still work - not clear if they include the result back in the view but is not a good work around.

This also results in a discrepancy between SSTable count and SSTables in levels for any table using LCS.
{code:java}
Keyspace : xxx
Read Count: 57761088
Read Latency: 0.10527088681224288 ms.
Write Count: 2513164
Write Latency: 0.018211106398149903 ms.
Pending Flushes: 0
Table: xxx
SSTable count: 10
SSTables in each level: [2, 0, 0, 0, 0, 0, 0, 0, 0]
Space used (live): 894498746
Space used (total): 894498746
Space used by snapshots (total): 0
Off heap memory used (total): 11576197
SSTable Compression Ratio: 0.6956629530569777
Number of keys (estimate): 3562207
Memtable cell count: 0
Memtable data size: 0
Memtable off heap memory used: 0
Memtable switch count: 87
Local read count: 57761088
Local read latency: 0.108 ms
Local write count: 2513164
Local write latency: NaN ms
Pending flushes: 0
Percent repaired: 86.33
Bloom filter false positives: 43
Bloom filter false ratio: 0.00000
Bloom filter space used: 8046104
Bloom filter off heap memory used: 8046024
Index summary off heap memory used: 3449005
Compression metadata off heap memory used: 81168
Compacted partition minimum bytes: 104
Compacted partition maximum bytes: 5722
Compacted partition mean bytes: 175
Average live cells per slice (last five minutes): 1.0
Maximum live cells per slice (last five minutes): 1
Average tombstones per slice (last five minutes): 1.0
Maximum tombstones per slice (last five minutes): 1
Dropped Mutations: 0
{code}
Also for STCS we've confirmed that SSTable count will be different to the number of SSTables reported in the Compaction Bucket's. In the below example there's only 3 SSTables in a single bucket - no more are listed for this table. Compaction thresholds haven't been modified for this table and it's a very basic KV schema.
{code:java}
Keyspace : yyy
    Read Count: 30485
    Read Latency: 0.06708991307200263 ms.
    Write Count: 57044
    Write Latency: 0.02204061776873992 ms.
    Pending Flushes: 0
        Table: yyy
        SSTable count: 19
        Space used (live): 18195482
        Space used (total): 18195482
        Space used by snapshots (total): 0
        Off heap memory used (total): 747376
        SSTable Compression Ratio: 0.7607394576769735
        Number of keys (estimate): 116074
        Memtable cell count: 0
        Memtable data size: 0
        Memtable off heap memory used: 0
        Memtable switch count: 39
        Local read count: 30485
        Local read latency: NaN ms
        Local write count: 57044
        Local write latency: NaN ms
        Pending flushes: 0
        Percent repaired: 79.76
        Bloom filter false positives: 0
        Bloom filter false ratio: 0.00000
        Bloom filter space used: 690912
        Bloom filter off heap memory used: 690760
        Index summary off heap memory used: 54736
        Compression metadata off heap memory used: 1880
        Compacted partition minimum bytes: 73
        Compacted partition maximum bytes: 124
        Compacted partition mean bytes: 96
        Average live cells per slice (last five minutes): NaN
        Maximum live cells per slice (last five minutes): 0
        Average tombstones per slice (last five minutes): NaN
        Maximum tombstones per slice (last five minutes): 0
        Dropped Mutations: 0 
{code}
{code:java}
Apr 27 03:10:39 cassandra[9263]: TRACE o.a.c.d.c.SizeTieredCompactionStrategy Compaction buckets are [[BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67168-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67167-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67166-big-Data.db')]]
{code}
Also for every LCS table we're seeing the following warning being spammed (seems to be in line with anticompaction spam):
{code:java}
Apr 26 21:30:09 cassandra[9263]: WARN  o.a.c.d.c.LeveledCompactionStrategy Live sstable /var/lib/cassandra/data/xxx/xxx-8c3ef9e0e3fc11e6868a8fd39a64fd59/mc-79024-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.{code}
This is a vnodes cluster with 256 tokens per node, and the only thing that seems like it could be causing issues is anticompactions.

CASSANDRA-14079 might be related but doesn't quite describe the same issue, and in this case we're using only a single disk for data. Have yet to reproduce but figured worth reporting here first.",,,,,,,,,,,,,,,CASSANDRA-14550,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-14 13:59:13.146,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 11 00:26:11 UTC 2018,,,,,,0|i3t3a7:,9223372036854775807,3.11.0,3.11.2,,,spodxx@gmail.com,krummas,,,,,,,,,,,"14/Jun/18 03:03;KurtG;Figured this out. tl;dr is that full repairs are completely broken. We add the repaired sstables to the transaction for anti-compaction but we never do anything with them or re-write them (because they are already repaired), and thus they get ""removed"" from the compaction strategies SSTables along with the unrepaired SSTables that got anti-compacted.

 

This essentially means that full repairs has been terribly broken for a long time, haven't checked how far back yet but it seems reasonable to say 2.1. Going to mark this as a blocker for 3.11.3 only while doing more research.","14/Jun/18 03:12;KurtG;Looks like we only _stopped_ anti-compacting repaired SSTables in CASSANDRA-13153, so this bug only occurs since 2.2.10, 3.0.13, and 3.11.0.",14/Jun/18 11:02;KurtG;got a patch & test for 3.11 [here|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14423-3.11]. Started on the wrong branch so will backport to 2.2 and 3.0 tomorrow...,"14/Jun/18 13:59;spodxx@gmail.com;{quote} and thus they get ""removed"" from the compaction strategies SSTables along with the unrepaired SSTables that got anti-compacted.{quote}

Where exactly does this happen?","15/Jun/18 03:46;KurtG;In {{org.apache.cassandra.db.compaction.CompactionManager#submitAntiCompaction}} we create a transaction over all SSTables included in the repair (including repaired SSTables when doing full repair) and pass that through to {{performAntiCompaction}} in which two things can happen:

1. The SSTable is fully contained within the repairing ranges, and in that case we mutate repairedAt to the current time of repair and add it to {{mutatedRepairStatuses}}
2. The SSTable isn't fully contained within the repairing ranges (highly likely if vnodes or single tokens with >RF nodes). In this case we don't add the _already repaired_ SSTable to {{mutatedRepairStatuses}}.

We then remove all SSTables from the transaction in {{mutatedRepairStatuses}} [here|https://github.com/apache/cassandra/blob/191ad7b87a4ded26be4ab0bd192ef676f059276c/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L704].

If *2* occured, the already repaired SSTables were not in {{mutatedRepairStatuses}} and thus didn't get removed from the transaction and when {{txn.finish()}} is called they get removed from the CompactionStrategy's view of sstables via {{org.apache.cassandra.db.lifecycle.LifecycleTransaction#doCommit}} calling {{Tracker#notifySSTablesChanged}} which will not include the already repaired SSTables.

The reason CASSANDRA-13153 brought this bug to light was because up until that point we _were_ anti-compacting already repaired SSTables, and thus upon anti-compaction (rewrite) they would be added back into the transaction and the old SSTable would be removed as usual and the new SSTable would take its place.

Seeing as the existing consensus seems to be that there's no real value at the moment in mutating repaired times on already repaired SSTables I think the best solution is to not include the repaired SSTables in the transaction in the first place. This corresponds with how trunk currently works and also is a lot cleaner, which is how it works in my patch mentioned above. The alternative would be to remove them from the transaction regardless of if they were mutated, but this seems pointless considering we don't do anything with it. If we ever decide there is value in updating repairedAt on already repaired SSTables, we can add it back and handle it then. ","15/Jun/18 05:05;KurtG;Patches for all branches:

|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...kgreav:14423-2.2]|
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14423-3.0]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14423-3.11]|","15/Jun/18 11:46;spodxx@gmail.com;Thanks for the detailed description. Finishing a LifecycleTransaction does indeed seem to remove the sstables the transactions has been created for from the compaction strategy. Simply ignoring repaired sstables, by filtering them as currently implemented, should be changed in that case. 

But it looks like your patch effectively reverts [67637d1|https://github.com/apache/cassandra/commit/67637d1] (CASSANDRA-11172) which needs some more careful consideration.

/cc [~krummas]

","18/Jun/18 01:59;KurtG;[~spodxx@gmail.com] That change was to ensure that we didn't send repair status change notifications where the SSTable was already marked as repaired. My change doesn't revert that,  as we no longer pass through the repaired SSTables to {{performAntiCompaction}} so there is no need to filter them out. Granted it does change the behaviour of {{performAntiCompaction}} and if someone were to call it and pass in repaired SSTables, it would produce the old behaviour. But arguably you should never be passing already repaired SSTables to {{performAntiCompaction}}. At the moment {{performAntiCompaction}} is only ever used by submitAntiCompaction in the codebase, so it's only a problem if 3rd party tools are using it. If we're worried maybe adding a test to the start of {{performAntiCompaction}} to check the SSTables aren't already repaired would be the way to go?

utests:
[3.11|https://circleci.com/gh/kgreav/cassandra/163]
[3.0|https://circleci.com/gh/kgreav/cassandra/167]
[2.2|https://circleci.com/gh/kgreav/cassandra/165]","18/Jun/18 11:58;spodxx@gmail.com;Can we move the status check into {{performAnticompaction}} by adding already repaired sstables to {{nonAnticompacting}}? I think filtering there would be more coherent, given we also create a corresponding log message and use the same code path for canceling/releasing such sstables. We also keep updating repairedAt this way, in case of fully contained sstables (and the triggered event notification related to that).","18/Jun/18 12:30;spodxx@gmail.com;I'd like to add for interested readers that full repairs on subranges (e.g. using reaper) will not be affected by this issue. In this case, ""Not a global repair, will not do anticompaction"" will occur in your logs.","19/Jun/18 12:27;KurtG;OK. I've updated all 3 branches to use {{nonAnticompacting}} for filtering. Personally, I'm not a huge fan of this but it avoids the behaviour change issues previously mentioned and still fixes the problem. Also as I see it, a cleaner fix would require some refactoring of {{submitAntiCompaction}} and {{performAntiCompaction}} to decouple anti-compaction from repair, which is not possible in current releases anyway (to clarify, I don't think {{performAntiCompaction}} should care about whether SSTables are repaired or not). I can see a future though where it's possible to anti-compact SSTables separate to a repair, but I'll think about that for a separate ticket for trunk.

Side-notes related to patches: I took the liberty of making one log message less spammy ""SSTable x will be anticompacted on range..."" - with many vnodes this becomes ridiculously spammy. It's not a lot better in the patched version, but at least it's a one line log message with no repeated information.

And I took the liberty of fixing the ""anti-compacted from x to y SSTables"" log message in {{doAntiCompaction}}. This was broken as {{repaired.originals.size}} could/would return a wrong size after \{{repaired.split(sstableGroup)}} was called.

Also, the assert at the end of {{performAntiCompaction}} had to be removed as we're removing {{nonAnticompacting}} from the transaction which violates the assert, this is fine however as the assert was effectively incorrect in the first place, and only valid due to the bug.

Finally, I split the tests out into a separate method ({{shouldAntiCompact}}), and added more test cases

|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...kgreav:14423-2.2]|[utests|https://circleci.com/gh/kgreav/cassandra/169]|
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14423-3.0]|[utests|https://circleci.com/gh/kgreav/cassandra/171]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14423-3.11]|[utests|https://circleci.com/gh/kgreav/cassandra/173]|

I'm working on a unit test for trunk, but it's going to have to be a fair bit different, as the repaired SSTables are filtered out prior to creation of the transaction. I don't think it needs to block review of the other branches however.

 

 ","21/Jun/18 07:14;spodxx@gmail.com;I think this is going in the right direction.

As for log messages, the ""does not intersect repaired ranges"" message seems to be now incorrectly shown for already repaired sstables.
{quote}Also, the assert at the end of performAntiCompaction had to be removed as we're removing nonAnticompacting from the transaction which violates the assert, this is fine however as the assert was effectively incorrect in the first place, and only valid due to the bug.
{quote}
Help me out, in which case do we end up removing an sstable in nonAnticompacting from txn.originals that would not also be removed from sstableIterator?","26/Jun/18 12:08;spodxx@gmail.com;Took another closer look at the patch today and I think that this should work. I put the assert back in, as I still can't see why it should not hold anymore. Also did some minor wording changes to a log message, changelog and git log, if that's ok for you, [~KurtG].
 
* [2.2|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-14423-2.2]
 [circleci|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-2.2]
[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/578/]
* [3.0|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-14423-3.0]
[circleci|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-3.0]
[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/579/]
* [3.11|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-14423-3.11]
[circleci|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-3.11]
[dtests|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/580/]

The 2.2 circleci test failures seem to be flaky. Dtests still running.","26/Jun/18 22:43;KurtG;Sorry for the delay, was looking at this yesterday but yeah, it turned out the assert failing was just me imagining things. That all seems good to me [~spodxx@gmail.com].",27/Jun/18 08:18;krummas;Should we stop doing anticompaction at all after full repairs instead? Clearly no one does {{--full}} repairs right now and letting users do non-incremental full repairs might be good until 4.0 (CASSANDRA-9143).,"27/Jun/18 10:34;KurtG;[~krummas] I've suggested that we at the very least provide a flag to skip anti-compaction from full repairs before, but it was all deemed too [complicated|https://issues.apache.org/jira/browse/CASSANDRA-13885?focusedCommentId=16206922&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16206922]. Regardless, I think a flag would be perfectly fine and it's still desirable so at least people can go back to at the very least running full repairs successfully without having to worry about SSTables being marked repaired. However, I don't think we can go and change the default behaviour, purely because people _could_ still be running full repairs on earlier versions of 3.x/3.0 before this bug came along.",27/Jun/18 11:13;krummas;the patches lgtm,"28/Jun/18 08:02;spodxx@gmail.com;[~KurtG], do you mind to go on by committing 2.2/3.0/3.11 patches now and address trunk in a separate ticket?","29/Jun/18 00:45;KurtG;[~spodxx@gmail.com], not at all. Probably won't have time to look at trunk until next week, and I'm fairly sure it'll just be a case of making the test work so I'll create another ticket for trunk.",29/Jun/18 00:51;KurtG;Created CASSANDRA-14550 for the trunk port.,"29/Jun/18 07:20;michaelsembwever;Jumping on this to commit. Two (other) reviewers have +1 this now, and tests have passed.

Repeating for clarity, the patches and their tests were…
||Branch||uTest||dTest||
|[cassandra-2.2_14423|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-14423-2.2]|[!https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-2.2.svg?style=svg!|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-2.2]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/583/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/583]|
|[cassandra-3.0_14423|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-14423-3.0]|[!https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-3.0.svg?style=svg!|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-3.0]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/581/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/581]|
|[cassandra-3.11_14423|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-14423-3.11]|[!https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-3.11.svg?style=svg!|https://circleci.com/gh/spodkowinski/cassandra/tree/CASSANDRA-14423-3.11]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/580/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/580]|",29/Jun/18 08:16;michaelsembwever;committed as  f8912ce9329a8bc360e93cf61e56814135fbab39,"09/Jul/18 13:49;mshuler;Build fails on JDK7.
{noformat}
((2.2.13-tentative))mshuler@hana:~/git/cassandra$ java -version
java version ""1.7.0_80""
Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)
((2.2.13-tentative))mshuler@hana:~/git/cassandra$ ant
Buildfile: /home/mshuler/git/cassandra/build.xml

init:
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/classes/main
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/classes/thrift
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/test/lib
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/test/classes
    [mkdir] Created dir: /home/mshuler/git/cassandra/src/gen-java
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/lib
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/jacoco
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/jacoco/partials

maven-ant-tasks-localrepo:
     [copy] Copying 1 file to /home/mshuler/git/cassandra/build

maven-ant-tasks-download:

maven-ant-tasks-init:

maven-declare-dependencies:

maven-ant-tasks-retrieve-build:
[artifact:dependencies] Building ant file: /home/mshuler/git/cassandra/build/build-dependencies.xml
[artifact:dependencies] Building ant file: /home/mshuler/git/cassandra/build/build-dependencies-sources.xml
     [copy] Copying 65 files to /home/mshuler/git/cassandra/build/lib/jars
     [copy] Copying 41 files to /home/mshuler/git/cassandra/build/lib/sources
     [copy] Copying 25 files to /home/mshuler/git/cassandra/build/lib/jars
    [unzip] Expanding: /home/mshuler/git/cassandra/build/lib/jars/org.jacoco.agent-0.7.5.201505241946.jar into /home/mshuler/git/cassandra/build/lib/jars

check-gen-cql3-grammar:

gen-cql3-grammar:
     [echo] Building Grammar /home/mshuler/git/cassandra/src/java/org/apache/cassandra/cql3/Cql.g  ...

generate-cql-html:

build-project:
     [echo] apache-cassandra: /home/mshuler/git/cassandra/build.xml
    [javac] Compiling 45 source files to /home/mshuler/git/cassandra/build/classes/thrift
    [javac] warning: Supported source version 'RELEASE_6' from annotation processor 'org.openjdk.jmh.generators.BenchmarkProcessor' less than -source '1.7'
    [javac] Note: /home/mshuler/git/cassandra/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java uses or overrides a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 1 warning
    [javac] Compiling 1168 source files to /home/mshuler/git/cassandra/build/classes/main
    [javac] Note: Processing compiler hints annotations
    [javac] warning: Supported source version 'RELEASE_6' from annotation processor 'org.openjdk.jmh.generators.BenchmarkProcessor' less than -source '1.7'
    [javac] Note: Processing compiler hints annotations
    [javac] Note: Writing compiler command file at META-INF/hotspot_compiler
    [javac] Note: Done processing compiler hints annotations
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/db/compaction/CompactionManager.java:584: error: cannot find symbol
    [javac]                     logger.info(""SSTable {} ({}) will be anticompacted on ranges: {}"", sstable, sstableBounds, String.join("", "", anticompactRanges));
    [javac]                                                                                                                      ^
    [javac]   symbol:   method join(String,List<String>)
    [javac]   location: class String
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 1 error
    [javac] 1 warning

BUILD FAILED
/home/mshuler/git/cassandra/build.xml:827: Compile failed; see the compiler error output for details.

Total time: 32 seconds
{noformat}","10/Jul/18 00:13;michaelsembwever;This will break running Cassandra-2.2 on jdk1.7
","10/Jul/18 00:31;michaelsembwever;||Branch||uTest||
|[cassandra-2.2_14423.1|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-2.2_14423.1]|[!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-2.2_14423.1.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-2.2_14423.1]|

[~mshuler], can you quick review this?

(also created CASSANDRA-14563)","10/Jul/18 15:17;mshuler;Thanks! That commit looks good to me.
- {{ant artifacts}} builds again successfully on JDK7
- {{ant test -Dtest.name=AntiCompactionTest}} passes successfully on JDK7",11/Jul/18 00:26;michaelsembwever;Committed as 3482370df5672c9337a16a8a52baba53b70a4fe8,,,,,,,,,,,,,,,,,,,,,
3.0 schema migration pulls from later version incompatible nodes,CASSANDRA-14896,13198747,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jasobrown,aweisberg,aweisberg,15/Nov/18 21:45,12/Mar/19 14:20,13/Mar/19 22:35,30/Nov/18 21:18,4.0,,,,Legacy/Core,Legacy/CQL,,,0,4.0-pre-rc-bugs,,,"I saw this in upgrade tests. The checks in 3.0 and 3.11 are slightly different and 3.0 in some scenarios it is pulling schema from a later version. This causes upgrade tests to have errors in the logs due to additional columns from configurable storage port.

{noformat}
Failed: Error details: 
Errors seen in logs for: node2
node2: ERROR [MessagingService-Incoming-/127.0.0.1] 2018-11-15 21:17:46,739 CassandraDaemon.java:207 - Exception in thread Thread[MessagingService-Incoming-/127.0.0.1,5,main]
java.lang.RuntimeException: Unknown column additional_write_policy during deserialization
	at org.apache.cassandra.db.Columns$Serializer.deserialize(Columns.java:433) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.db.SerializationHeader$Serializer.deserializeForMessaging(SerializationHeader.java:440) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.deserializeHeader(UnfilteredRowIteratorSerializer.java:190) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize30(PartitionUpdate.java:686) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:674) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:337) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:346) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:641) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:624) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.17.jar:3.0.17]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.17.jar:3.0.17]
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-28 22:56:37.254,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 30 21:18:49 UTC 2018,,,,,,0|s00jxs:,9223372036854775807,,,,,aweisberg,aweisberg,,,,,,,,,,,28/Nov/18 19:38;aweisberg;4.0 is providing the wrong max version when connecting to other nodes which causes them to think that 4.0 nodes are older version nodes.,"28/Nov/18 22:56;jasobrown;[~aweisberg] is correct. On the third (and last) message of the internode messaging handshake, the node is [incorrectly sending back|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/OutboundHandshakeHandler.java#L180] the messaging version is received from the peer; it should be sending back it's own {{MessagingService.current_version}}.

Here's a one-line fix for sending the correct messaging version in {{ThirdHandshakeMessage}} as well as fixing the unit test that ensures the version being sent from {{OutboundHandshakeHandler}}
||14896||
|[branch|https://github.com/jasobrown/cassandra/tree/14896]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14896]|",28/Nov/18 23:00;aweisberg;+1 to that fix. I've tested it and it worked for me.,"28/Nov/18 23:15;jasobrown;The only utest that failed was {{DistributedReadWritePathTest.writeWithSchemaDisagreement}}, which failed with ""Forked Java VM exited abnormally"". I ran locally and all was fine, so chalking it up to a testing fluke. Will commit shortly.",28/Nov/18 23:26;jasobrown;Committed as sha \{{c5dee08dfb791ba28fecc8ca8b25a4a4d7e9cb07}},"29/Nov/18 14:53;tommy_s;I'm not sure why but this seams to course some problems when upgrading 3.x->4.0 with server encryption enabled. The 4.0 node can't connect to any 3.x node, if I build 4.0 without this patch I get the issue reported in CASSANDRA-14848 and the 4.0 node connects to one of the 3.x nodes. My patch for CASSANDRA-14848 does not help so I'm not sure what the problem is.","29/Nov/18 16:40;aweisberg;[~tommy_s] unfortunate this didn't help. 

I will probably get to the SSL issue soon if [~jasobrown] doesn't just because the upgrade tests have SSL tests so I will hit it there.","29/Nov/18 19:50;aweisberg;I +1ed the wrong fix :-(

The version in the constructor needs to be the peer's version. It's [this line here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/HandshakeProtocol.java#L248] that needs to specify the current version.","29/Nov/18 22:21;jasobrown;The problem with my first patch is that we need the peer's messaging version in order serialize the {{InetaddressAndPort}} correctly to the peer. We still need to write the local node's messaging version into the message, however.

Patch here:

||v2||
|[branch|https://github.com/jasobrown/cassandra/tree/14896-v2]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14896-v2]|
||","29/Nov/18 22:29;aweisberg;The comment on writing the max version to the message should specify that it's the maximum supported version of this node. As opposed to the supported to the version it's using to connect.

Other then that +1",30/Nov/18 21:18;jasobrown;Committed v2 patch as {{f3609995c09570d523527d9bd0fd69c2bc65d986}} with updated comments per [~aweisberg]'s recommendation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Severe concurrency issues in STCS,DTCS,TWCS,TMD.Topology,TypeParser",CASSANDRA-14871,13196766,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,snazy,snazy,snazy,07/Nov/18 08:37,12/Mar/19 14:07,13/Mar/19 22:35,01/Feb/19 11:35,3.0.18,3.11.4,4.0,,Legacy/Core,,,,0,,,,"   There are a couple of places in the code base that do not respect that j.u.HashMap + related classes are not thread safe and some parts rely on internals of the implementation of HM, which can change.

We have observed failures like {{NullPointerException}} and  {{ConcurrentModificationException}} as well as wrong behavior.

Affected areas in the code base:
 * {{SizeTieredCompactionStrategy}}
 * {{DateTieredCompactionStrategy}}
 * {{TimeWindowCompactionStrategy}}
 * {{TokenMetadata.Topology}}
 * {{TypeParser}}
 * streaming / concurrent access to {{LifecycleTransaction}} (handled in CASSANDRA-14554)

While the patches for the compaction strategies + {{TypeParser}} are pretty straight forward, the patch for {{TokenMetadata.Topology}} requires it to be made immutable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-12-01 00:35:19.039,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 01 11:35:43 UTC 2019,,,,,,0|s007qg:,9223372036854775807,,,,,bdeggleston,bdeggleston,,,,,,,,,,,"07/Nov/18 08:40;snazy;Patches:

|[3.0|https://github.com/snazy/cassandra/tree/14871-fix-concurrency-issues-3.0]|[3.11|https://github.com/snazy/cassandra/tree/14871-fix-concurrency-issues-3.11]|[trunk|https://github.com/snazy/cassandra/tree/14871-fix-concurrency-issues-trunk]","01/Dec/18 00:35;bdeggleston;I was taking a look at the 3.0 changes and had a few comments, I'll take a look at the other versions on Monday. Sorry if I'm stepping on your toes [~jkni], I'm also happy to take over the review if you're busy.

DateTieredCompactionStrategy
 * access to {{sstables}} is unguarded in {{getNextBackgroundSSTables}}

TokenMetadata
 * {{@VisibleForTesting}} annotation on topology field could be removed
 * Could you either use a read lock in {{TokenMetadata#getTopology}}, or rename it getTopologyUnsafe? I realize it’s not used in a way might have visibility issues at the moment, but it is silently deviating from the concurrency strategy used everywhere else for that field. It should either conform to the strategy, or have a name scary enough to make people read the method docs before using it. (I’d prefer just using a read lock for simplicity/consistency)
 * Since we’re now using copy on write for {{TokenMetadata.Topology}} changes and treating {{Topology}} as immutable, it would be clearer if the mutating methods were split out into a builder class and the Topology only exposed getter type methods. This might be overkill for 3.x, but it makes sense for trunk.","03/Dec/18 16:11;jkni;No apology necessary, [~bdeggleston]. Thanks for taking a look! I'm busy enough that this hasn't made its way up my queue yet. If you have time/interest, please feel free to take over the review.","04/Dec/18 23:37;bdeggleston;Thanks Joel, set myself as reviewer.

I spent some more time looking at the compaction strategy and TypeParser parts of the patch, and have some comments there as well.

[DateTiered/SizeTiered/TimeWindow]CompactionStrategy
 * access to sstables should be synchronized with the same granularity as the rest of the strategy state (ie: {{synchronized (this)}}). I don’t see any reason why we’d want to synchronize access to the sstable collection separately.

TypeParser
 * Again, if we’re using copy on write for the cache, we should make the cache immutable. Also we shouldn’t allocate a new map in the synchronized block unless we verify it’s still missing the key we’re interested in.

See previous comment for TokenMetadata","05/Dec/18 21:36;bdeggleston;[~snazy] I've implemented my feedback here, let me know what you think.

|[3.0|https://github.com/bdeggleston/cassandra/tree/14871-3.0]|[3.11|https://github.com/bdeggleston/cassandra/tree/14871-3.11]|[trunk|https://github.com/bdeggleston/cassandra/tree/14871-trunk]|","07/Dec/18 12:40;snazy;[~bdeggleston], thank you! Will take a look.","07/Dec/18 12:56;snazy;Few comments:
 * I think [this lock|https://github.com/bdeggleston/cassandra/commit/7eeec9be03be6d326432fc715e9dce4b173acdf4#diff-b9ead760fa9628889810dd64e6507d9cR1279] has no ""real"" effect - mean, the method just returns the reference to {{topology}}. But we should make {{topology}} {{volatile}}. WDYT?
 * The builder-approach for {{Topology}} is nice!
 * Not sure how  [this|https://github.com/bdeggleston/cassandra/commit/0a8f3909098a233bca42d651b8242b288bb2c557#diff-052bdc412f1a356a3fb5409de51dceb5R108] could actually help. It's definitely fine as a safety net though. WDYT about replacing it with {{assert type != null : ""Parsing '"" + str + ""' yielded null, which is a bug"";}} right before the {{synchronized}}.
 * +1 on the other changes!

 ","07/Dec/18 17:40;bdeggleston;thanks for taking a look Robert.

bq. I think this lock has no ""real"" effect - mean, the method just returns the reference to topology. But we should make topology volatile. WDYT?

Agreed, fixed.

bq. Not sure how  this could actually help. It's definitely fine as a safety net though. WDYT about replacing it with assert type != null : ""Parsing '"" + str + ""' yielded null, which is a bug""; right before the synchronized.

Right, it's just to quickly catch future bugs. I'd prefer to keep it as is if that's ok with you. Right now it's verifying the entire un-cached case up to and including the caching of the type. Regarding using {{assert}}, I avoid it outside of tests because asserts can be disabled, and {{Preconditions}} and {{Verify}} do a better job of communicating what you're checking (imo).","09/Dec/18 10:14;lizg;sorry , wrong click , change status to ""ready to commit"" , i don't know how to rollback",09/Dec/18 16:03;aweisberg;NP. Workflow -> Cancel Commit.,"10/Dec/18 12:55;snazy;I'm fine with using {{Verify}}, but it should include the string that yielded {{null}}. Like {{Verify.verify(type != null, ""Parsing %s yielded null, which is a bug"", str)}} and move that after the assignment of {{type}}. That way we don't need the {{cache.get(str)}} in the synchronized block, because {{type}} is then guaranteed to be non-null.

EDIT: So instead of
{code:java}
       synchronized (TypeParser.class)
       {
            if (!cache.containsKey(str))
            {
                ImmutableMap.Builder<String, AbstractType<?>> builder = ImmutableMap.builder();
                builder.putAll(cache).put(str, type);
                cache = builder.build();
            }
            AbstractType<?> rtype = cache.get(str);
            Verify.verify(rtype != null);
            return rtype;
        }
{code}
like this:
{code:java}
        if (!isEOS(str, i) && str.charAt(i) == '(')
            type = getAbstractType(name, new TypeParser(str, i));
        else
            type = getAbstractType(name);
        Verify.verify(type != null, ""Parsing %s yielded null, which is a bug"", str);

       // <comments>
       synchronized (TypeParser.class)
       {
            if (!cache.containsKey(str))
            {
                ImmutableMap.Builder<String, AbstractType<?>> builder = ImmutableMap.builder();
                builder.putAll(cache).put(str, type);
                cache = builder.build();
            }
            return type;
        }
{code}
",12/Dec/18 23:23;bdeggleston;pushed up Verify fix,"14/Dec/18 12:47;snazy;Why the new {{Verify.verify()}}? Mean, it looks very paranoid like you don't trust the map-builder.","14/Dec/18 16:02;bdeggleston;Mainly a check against future bugs. Like I said earlier, the benefit of putting the verify at the end there in the first place is to serve as a check against the entire method. I guess if it really bothers you, you can remove it. I don't think we should spend much more time discussing the specifics of verify statements though.",07/Jan/19 22:48;bdeggleston;[~snazy] is this ready to commit?,29/Jan/19 00:29;bdeggleston;[~snazy] ping,"29/Jan/19 09:37;snazy;Ah, sorry, missed your ping.

I've incorporated the changes into my branches and going to kick-off CI.

If you're ok, I'd go ahead, squash the changes and commit it.",29/Jan/19 12:07;snazy;CI results [trunk|https://circleci.com/gh/snazy/cassandra/84] [3.11|https://circleci.com/gh/snazy/cassandra/83] [3.0|https://circleci.com/gh/snazy/cassandra/82],31/Jan/19 00:01;bdeggleston;[~snazy] go for it,"01/Feb/19 11:35;snazy;Thanks for the review!

Committed as [bc18b4dd4e33020d0d58c3701077d0af5c39bce6|https://github.com/apache/cassandra/commit/bc18b4dd4e33020d0d58c3701077d0af5c39bce6] to [cassandra-3.0|https://github.com/apache/cassandra/tree/cassandra-3.0], [merged|https://github.com/apache/cassandra/commit/16c96c20dadfbda98d4b5daf7f6c169b691459b9] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11], [merged|https://github.com/apache/cassandra/commit/ef6c5f8ba9dc18e7a5bacfd5d8461ae5d9f12df4] to [trunk|https://github.com/apache/cassandra/tree/trunk].",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counters fail to increment in 2.1/2.2 to 3.X mixed version clusters,CASSANDRA-14958,13208067,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,iamaleksey,aweisberg,aweisberg,07/Jan/19 16:15,12/Mar/19 14:07,13/Mar/19 22:35,14/Jan/19 18:09,3.0.18,3.11.4,,,Feature/Counters,,,,0,,,,"The upgrade test for this is failing
https://circleci.com/gh/aweisberg/cassandra/2362#tests/containers/1

I confirmed that this is occurring manually using cqlsh against the cluster constructed by the dtest.
{noformat}
cqlsh> describe schema;

CREATE KEYSPACE ks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;

CREATE TABLE ks.clicks (
    userid int,
    url text,
    total counter,
    PRIMARY KEY (userid, url)
) WITH COMPACT STORAGE
    AND CLUSTERING ORDER BY (url ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

cqlsh> use ks;
cqlsh:ks> UPDATE clicks SET total = total + 1 WHERE userid = 1 AND url = 'http://foo.com';
cqlsh:ks> SELECT total FROM clicks WHERE userid = 1 AND url = 'http://foo.com'
      ... ;

 total
-------
     0

(1 rows)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-01-11 16:30:29.313,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 15 20:05:07 UTC 2019,,,,,,0|u00l3c:,9223372036854775807,,,,,benedict,benedict,,,,,,,,,,,"11/Jan/19 16:30;iamaleksey;Unfortunately there is indeed an issue.

In a mixed-version 2.1 + 3.0 cluster, when 3.0 coordinates a counter update request, and chooses to forward the counter mutation to a different leader, such an update would be lost.

For that to happen you need to have a mixed mode cluster with 3.0 node both coordinating and opting to *not* be a leader itself, which is actually a rather likely scenario. I'm (very) surprised that this upgrade bug hasn't been noticed before (before CASSANDRA-14881 that is).","11/Jan/19 18:40;iamaleksey;Code: [3.0|https://github.com/iamaleksey/cassandra/commits/14958-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/14958-3.11].
CI: [3.0|https://circleci.com/workflow-run/ad96e947-dff3-4d63-9c34-3e5220e550e4], [3.11|https://circleci.com/workflow-run/8378d5c3-a517-486f-8df0-b8d36c05bd79].

To fix another compatibility issue, CASSANDRA-13691 switched from using local shards to stash counter update values to a special sentinel id. {{LegacyLayout}} code was in one place unfortunately not updated to reflect the change, breaking counter update cell serialisation to legacy nodes, always sending 0-increments.

The linked branches address the issue. There is no new test as this scenario is already covered pretty well by the (now working) upgrade tests.","14/Jan/19 14:04;iamaleksey;Upgrade test results showing the issue's now fixed: [3.0|https://circleci.com/gh/iamaleksey/cassandra/914], [3.11|https://circleci.com/gh/iamaleksey/cassandra/915].",14/Jan/19 16:57;benedict;+1,"14/Jan/19 18:09;iamaleksey;Cheers. Committed to 3.0 as [ebfa280fac2f43fb88e2e87d81f35b8017222a12|https://github.com/apache/cassandra/commit/ebfa280fac2f43fb88e2e87d81f35b8017222a12] and merged into 3.11, -s ours into trunk.",14/Jan/19 19:33;aweisberg;Thanks! Don't forget to remove the skip annotation on upgrade_tests/cql_tests.py test_counters.,"15/Jan/19 20:05;iamaleksey;[~aweisberg] Would've forgotten if not for your comment - thank you. And just to confirm, two proper upgrade test runs with the annotation removed, with counter tests passing: [3.0|https://circleci.com/gh/iamaleksey/cassandra/921], [3.11|https://circleci.com/gh/iamaleksey/cassandra/923].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CASSANDRA-9608 broke running Cassandra and tests in IntelliJ under Java 8,CASSANDRA-14627,13177602,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jrwest,jrwest,jrwest,08/Aug/18 06:28,12/Mar/19 14:07,13/Mar/19 22:35,10/Aug/18 12:42,,,,,Legacy/Testing,,,,1,Java11,,,"CASSANDRA-9608 added a couple hard-coded options to workspace.xml that are not supported in Java 8: https://github.com/apache/cassandra/commit/6ba2fb9395226491872b41312d978a169f36fcdb#diff-59e65c5abf01f83a11989765ada76841. 

{code}
Unrecognized option: --add-exports
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
{code}

To reproduce:
1. Update to the most recent trunk
2. rm -rf .idea && ant generate-idea-files
3. Re-open the project in IntelliJ (using Java 8) and run Cassandra or a test. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-08-08 06:55:02.092,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 10 12:42:32 UTC 2018,,,,,,0|i3wtef:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,08/Aug/18 06:55;djoshi3;I can repro this issue.,08/Aug/18 10:13;iamaleksey;See CASSANDRA-14613 too.,"08/Aug/18 16:44;jrwest;This is slightly different from CASSANDRA-14613 in that {{ide/workspace.xml}} is broken instead of {{ide/idea-iml-file.xml}} but I'm happy to dupe this to it. I do think a short term fix for this is warranted: at a minimum, breaking Java 11 in the IDE instead of Java 8.","08/Aug/18 16:45;jrwest;I should also note, the local workaround, in the meantime, is to manually delete the Java 11 arguments from {{ide/workspace.xml}} or from the specific IntelliJ configurations being used. ","08/Aug/18 17:56;jasobrown;I propose we remove the java11 support from the idea files for now. Should probably look at the eclipse files, as well.",08/Aug/18 17:58;jrwest;I'll post a fix later today or tomorrow that breaks Java 11 support in IDEA and fixes Java 8. We can open a separate ticket to track fixing Java 11 separately. ,09/Aug/18 16:00;jrwest;[branch|https://github.com/jrwest/cassandra/tree/14627-trunk] [tests | https://circleci.com/gh/jrwest/cassandra/tree/14627-trunk],10/Aug/18 12:42;jasobrown;+1. committed as sha {{d78310d53f9d00dcd26feb0ec4802a2a182fdd24}}. Thanks for the patch!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Static collection deletions are corrupted in 3.0 -> 2.{1,2} messages",CASSANDRA-14568,13172138,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,benedict,benedict,benedict,13/Jul/18 22:31,12/Mar/19 14:07,13/Mar/19 22:35,14/Sep/18 10:28,3.0.17,3.11.3,,,,,,,0,,,,"In 2.1 and 2.2, row and complex deletions were represented as range tombstones.  LegacyLayout is our compatibility layer, that translates the relevant RT patterns in 2.1/2.2 to row/complex deletions in 3.0, and vice versa.  Unfortunately, it does not handle the special case of static row deletions, they are treated as regular row deletions. Since static rows are themselves never directly deleted, the only issue is with collection deletions.

Collection deletions in 2.1/2.2 were encoded as a range tombstone, consisting of a sequence of the clustering keys’ data for the affected row, followed by the bytes representing the name of the collection column.  STATIC_CLUSTERING contains zero clusterings, so by treating the deletion as for a regular row, zero clusterings are written to precede the column name of the erased collection, so the column name is written at position zero.

This can exhibit itself in at least two ways:
 # If the type of your first clustering key is a variable width type, new deletes will begin appearing covering the clustering key represented by the column name.
 ** If you have multiple clustering keys, you will receive a RT covering all those rows with a matching first clustering key.
 ** This RT will be valid as far as the system is concerned, and go undetected unless there are outside data quality checks in place.
 # Otherwise, an invalid size of data will be written to the clustering and sent over the network to the 2.1 node.
 ** The 2.1/2.2 node will handle this just fine, even though the record is junk.  Since it is a deletion covering impossible data, there will be no user-API visible effect.  But if received as a write from a 3.0 node, it will dutifully persist the junk record.
 ** The 3.0 node that originally sent this junk, may later coordinate a read of the partition, and will notice a digest mismatch, read-repair and serialize the junk to disk
 ** The sstable containing this record is now corrupt; the deserialization expects fixed-width data, but it encounters too many (or too few) bytes, and is now at an incorrect position to read its structural information
 ** (Alternatively when the 2.1 node is upgraded this will occur on eventual compaction)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-16 14:09:57.904,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 14 10:28:24 UTC 2018,,,,,,0|i3vw0f:,9223372036854775807,,,,,,iamaleksey,slebresne,,,,,,,,,,13/Jul/18 23:35;benedict;Patch available [here|https://github.com/belliottsmith/cassandra/tree/CASSANDRA-14568],"16/Jul/18 14:09;iamaleksey;+1, ship it.","16/Jul/18 17:44;benedict;Committed as d52c7b8c59, 31d5d870f9 and d3a994b105

tests were run against [circleci|https://circleci.com/workflow-run/c818c58a-83e3-4731-89d7-ab0b04d26d62] (unfortunately right now this link is down); this showed some failing auth dtests, but these code paths were unaffected by this patch and I confirmed they pass locally.","15/Aug/18 15:59;slebresne;Sorry I'm a bit slow to check this one, but I don't think the fix here is correct.

That is, while it makes sure complex deletions are handled for static rows, I believe it also doesn't encode the ""composite"" in the same way than 2.x was (the added unit test passes because it only check the encoding round-trips, which it does, but I believe things don't work properly if tested against a true 2.1 node).

Namely, the encoding of the ""clustering prefix"" (what comes before the column name) for a static cell in 2.x uses N empty components (see [here|https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/db/composites/CompoundSparseCellNameType.java#L68-L69]), where N is the table clustering size, while the patch uses an empty clustering (that is, put the column name as first component, no matter how many clustering the table has; see [here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L210-L214] and, in [serializeCompound|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L2394-L2400], there will be no clustering elements since {{STATIC_PREFIX}} has size 0 and {{CompositeType.Builder}} does not compensate for that in any way). ","15/Aug/18 18:07;benedict;Thanks for the input, when fixing this I had indeed assumed the issue was only with deserialization on 3.0, not serialization also.

I can now see there is an existing method for creating a staticBound, although it's not presently clear to me if it was ever plausibly invoked, so it will take a while to corroborate that we don't break any other assumptions by setting the LegacyLayout.bound to something that breaks the 3.0 definition of a static clustering.  AFAICT, we do directly reference it in places where the distinction matters, so we may have to introduce a legacyBound and modernBound for which a distinction is only made in the static case.","16/Aug/18 07:50;slebresne;It's absolutely possible I'm missing some problem, as god knows those backward compatibility issues often comes with nasty surprises, but I don't think the problem is too hard to solve.

That is, {{LegacyBound}} is only here for backward compatibility and never used by 3.0 before being converting first, so I'm sure to understand what you mean by ""(setting it) to something that breaks the 3.0 definition of a static clustering"".

I believe we should simply make sure that when encoding to 2.x a {{STATIC_CLUSTERING}}, we append ""clustering size"" empty values before adding the rest (the column name), and when decoding, we assume those empty values are here (but ignore them). This is what we do for ""cell names"" ([here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L287-L291] and [here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L329-L330]) and to the best of my knowledge is working properly.

In other words, I ""think"" we need to do the 2 following changes:
# revert the change made by this patch in [{{LegacyLayout.decodeBound}}|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L210-L215].
# make sure that in [{{LegacyLayout.serializeCompound}}|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L2394-L2400] (and {{serializeSizeCompound}}), the static case is specialized to automatically add the empty values, which might possibly be better handler directly within {{CompositeType.Builder}}.
","16/Aug/18 08:10;benedict;Wouldn't the ""more correct"" place to change it be [in the bound construction|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L803]?  Presumably the LegacyBound.bound property should be correctly legacy-fied so that any other LegacyLayout code correctly interprets / sorts it?  This is what I was planning on with [my wip branch|https://github.com/belliottsmith/cassandra/commit/5b6742a7a6104bbd88d784db4d3bf7cd990cf057#diff-1a4af2aebd51e301cf0e73126722a8a4R805].

My concern was that LRT.start.bound is [directly referenced in UnfilteredDeserializer when converting to a RTMarker|https://github.com/belliottsmith/cassandra/blob/14568-2/src/java/org/apache/cassandra/db/UnfilteredDeserializer.java#L728].  I realise in the light of morning that this would not be a problem during _serialization_, but maybe this is another bug in deserialization?
","16/Aug/18 09:07;benedict;bq. My concern was that LRT.start.bound is directly referenced in UnfilteredDeserializer when converting to a RTMarker.

Hmm.  Thinking on it some more, I guess this is not a problem due to the fact that we never (in any extant version) actually issue any deletions that (in 3.0) would be represented as RTs spanning static rows, so the problematic cases *should* all be converted to collection tombstones only.  I will add some comments to LegacyLayout elaborating the inconsistencies of modern/legacy static clusterings as part of the patch.

So, I'm now comfortable with fixing either location, I think.  Though I need to code dive a bit more to be absolutely certain.","16/Aug/18 09:47;benedict;OK. So, [here|https://github.com/belliottsmith/cassandra/commits/14568-2] is may second attempt at fixing this.

In the process of adding improved assertion logic, I realised we might have another bug around dropped static collection column, that could have resulted in decoding a static collection deletion as a whole-static-row deletion (with unknown semantics, since I vaguely recall that our correctness in some places depends on there being no such deletions).  

In essence: if on looking up the collectionNameBytes, we found no collectionName (due, for instance, to it having been dropped), we would be left with a only a complete static row bound to construct.  

Perhaps I should split this fix into a separate ticket, for a separate CHANGES.txt mention?

We clearly need to introduce some upgrade dtests to cover these cases as well","24/Aug/18 13:42;slebresne;Sorry for the slow turn over. Last version lgtm, though as mention, upgrade dtests would be great to validate it.

Re: dropped static collection columns, the bug looks legit as well so kudos for noticing that. And I personally don't mind committing it here directly.","24/Aug/18 13:46;benedict;Great, thanks for taking the time to confirm.

bq. upgrade dtests would be great to validate it.

Yep, absolutely.  That will have to wait until things settle down in a couple of weeks,  after the freeze.  I may commit this ahead of then, just so we have the bug (at least much more probably) fixed for anyone who might need it more urgently.","24/Aug/18 15:49;slebresne;bq. I may commit this ahead of then

Wfm. Maybe have [~iamaleksey] check our logic one more time but otherwise +1 from me.",28/Aug/18 17:12;iamaleksey;Looks good to me as well. Sorry for failing to spot this in the first place. Old memories are fading.,"13/Sep/18 16:25;benedict;[3.0|https://github.com/belliottsmith/cassandra/tree/14568] [CI|https://circleci.com/workflow-run/70397395-3585-4b4c-904f-a55c070cf359]

Thanks both for your review.  I have split out CASSANDRA-14749, and pushed an updated patch simply missing this part.  If either of you could give a quick cursory +1, I'll commit them both.",14/Sep/18 08:36;slebresne;+1 on this one.,14/Sep/18 10:28;benedict;Thanks.  Committed as [68dbeb34c9404ee3cd7db00cc112e27c9a4b1f6f|https://github.com/apache/cassandra/commit/68dbeb34c9404ee3cd7db00cc112e27c9a4b1f6f].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regression in paging queries in mixed version clusters ,CASSANDRA-14919,13202226,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,beobal,beobal,beobal,04/Dec/18 10:24,12/Mar/19 14:07,13/Mar/19 22:35,06/Dec/18 16:18,3.0.18,3.11.4,,,CQL/Interpreter,Messaging/Internode,Messaging/Thrift,,0,,,,"The changes to handling legacy bounds in CASSANDRA-14568/CASSANDRA-14749/CASSANDRA-14912 break paging queries where the coordinator is a legacy node and the replica is an upgraded node. 

The long-held assumption made by {{LegacyLayout::decodeBound}} that ""There can be more components than the clustering size only in the case this is the bound of a collection range tombstone."" is not true as serialized paged read commands may also include these type of bounds in their {{SliceQueryFilter}}. The additional checks the more recent tickets add cause such queries to error when processed by a 3.0 replica.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-12-04 15:53:31.157,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 06 16:18:25 UTC 2018,,,,,,0|s0157c:,9223372036854775807,,,,,,benedict,,,,,,,,,,,"04/Dec/18 14:59;beobal;This issue showed up during work on CASSANDRA-14421 to re-enable the upgrade dtests. Using [~aweisberg]'s dtest branch as a starting point, I've verified that the upgrade tests concerned with paging are now passing (after some additional tweaks - see CASSANDRA-14920). I've only run this locally though and haven't updated the CircleCI config to add those tests, leaving that to 14421.

The CircleCI runs below are running with the current dtest master branch:

||branch||CI||
|[14919-3.0|https://github.com/beobal/cassandra/tree/14919-3.0]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14919-3.0]|
|[14919-3.11|https://github.com/beobal/cassandra/tree/14919-3.11]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14919-3.11]|



 

 ",04/Dec/18 15:53;aweisberg;Typo in the links to the code.,"04/Dec/18 16:41;beobal;{quote}Typo in the links to the code{quote}

oops, actually a failure to push those 2 branches. Fixed now","04/Dec/18 21:57;benedict;I'm largely +1 the changes, but I wonder if we should be inserting some assertions around our expectations here.

For instance, [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-f53ee75d79fb30b497f6d1584d51d7d3R2429], [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-861bc950c82973fb8f97f179e35be7f3R1586] and [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-f53ee75d79fb30b497f6d1584d51d7d3R374] at least we pretty much require that there be no column name in the payload?  If this is somehow present, something looks likely to have gone awry, since we are discarding it and have nothing in place to suggest this is OK (unlike [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-861bc950c82973fb8f97f179e35be7f3R1093])?

I'm also honestly not certain what the correct behaviour [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-f53ee75d79fb30b497f6d1584d51d7d3R1245] is.  Presumably for thrift schemas, it is impossible to receive a collection name.  However I'm very out-of-touch with the compatibility mechanisms for accessing CQL via thrift.  Can we legitimately receive a collection RT?  I doubt it, but I cannot be sure.","06/Dec/18 13:32;beobal;{quote}For instance, [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-f53ee75d79fb30b497f6d1584d51d7d3R2429], [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-861bc950c82973fb8f97f179e35be7f3R1586] and [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-f53ee75d79fb30b497f6d1584d51d7d3R374] at least we pretty much require that there be no column name in the payload? If this is somehow present, something looks likely to have gone awry, since we are discarding it and have nothing in place to suggest this is OK (unlike [here|https://github.com/beobal/cassandra/commit/b8bb748dca93deeae7c50319ab0634f599fedea9#diff-861bc950c82973fb8f97f179e35be7f3R1093])?  
{quote}

Actually, in these slice cases (as opposed to RTs), this is safe and equivalent to the pre-14568/14749 behaviour. Previously, the column name element from a query slice would be erroneously identified as a collection name, but this was not problematic as we only ever used the {{LegacyBound.slice}} field, which was correctly constructed as a result of the final element being popped from the list of components in {{decodeBound}}. The same thing holds now, the difference being that we no longer incorrectly assign the {{LegacyBound}} a collection name.

{quote}I'm also honestly not certain what the correct behaviour here is. Presumably for thrift schemas, it is impossible to receive a collection name. However I'm very out-of-touch with the compatibility mechanisms for accessing CQL via thrift. Can we legitimately receive a collection RT? I doubt it, but I cannot be sure.
{quote}
It's technically possible to receive a collection name here as they can always be hand rolled, but that's also fine. If a client were to send a deletion with a range slice that emulates a collection tombstone, we'd process it as normal and delete the collection. It's going down exactly the same path as non-thrift requests, so an invalid slice bound (e.g. like a bound ending with a missing or non-collection column name) would be rejected.  
","06/Dec/18 13:52;benedict;bq. Actually, in these slice cases (as opposed to RTs), this is safe

But it isn't safe for the other end to have actually sent this, I think?  Since we ignore the value, we aren't going to be semantically equivalent to whatever was intended - either the other end has a bug, asking for something nonsensical, or we have a bug in that we treat it erroneously?

With paging we know it is fine because there are comments indicating we expect the column name, and that our treatment of ignoring it is correct, but in these cases I'm not sure it is logically possible to respond correctly to a request with this component?

I agree ignoring it would be equivalent to 3.0.0 behaviour, but it might help our future selves to document and enforce these expectations.

This isn't something I would block commit on, if you disagree though.

bq. It's technically possible to receive a collection name here

I agree it's technically possible, I am just unsure if our API intended to actually support this, and if it is misuse to use it (if we are to introduce assertions).  Looking at 2.2, though, we only refuse super columns because our new schema cannot support them.  So it's probably the case that this remains equivalent in behaviour to 2.2, which is all that really matters.

+1, however you decide to proceed.","06/Dec/18 16:18;beobal;I'm all for helping our future selves and I hope that the comments and link to this discussion will do that. I've stopped short of enforcing the expectations though, as what we have now fixes the breakage introduced by 14568/14749/14912 whilst retaining the fixes from those issues. It also preserves the behaviour of previous 3.0/3.x versions so I think this has the minimal risk of introducing new, unforeseen regressions. If it does transpire that there other cases where this existing behaviour is harmful, or this patch breaks something else we didn't consider, we can revisit then.

Committed to 3.0 in {{11043610e38281f650f289a7f9286d306f1369e3}} and merged to 3.11/trunk, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a new tool to dump audit logs,CASSANDRA-14885,13197551,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vinaykumarcse,vinaykumarcse,vinaykumarcse,10/Nov/18 02:41,12/Mar/19 14:20,13/Mar/19 22:35,23/Jan/19 13:39,4.0,,,,Legacy/Tools,,,,0,,,,"As part of CASSANDRA-12151, AuditLogging feature uses [fqltool|https://github.com/apache/cassandra/blob/trunk/tools/bin/fqltool] to dump audit log file contents in human-readable text format from binary logging format ([BinLog| https://issues.apache.org/jira/browse/CASSANDRA-13983]).

The goal of this ticket is to create a separate tool to dump audit logs instead of relying fqltool and let fqltool be full query log specific.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-01-22 13:31:51.117,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 23 13:39:01 UTC 2019,,,,,,0|s00cko:,9223372036854775807,,,,,krummas,krummas,,,,,,,,,,,"22/Jan/19 13:31;krummas;looks good, just a few minor comments;
* the script should probably move to {{tools/bin/...}}
* in the test you could use {{Files.createTempDirectory(""foo"")}} instead of the {{createTempDir}} method
* also make sure you delete that directory on exit (not just the files in it)","23/Jan/19 09:24;vinaykumarcse;Thanks for the review [~krummas]. I have rebased on latest trunk and fixed your review comments. I have also added a batch file for {{auditlogviewer}} tool.

 
||Branch||utests||dtests||
|[patch|https://github.com/vinaykumarchella/cassandra/commits/trunk_CASSANDRA-14885]|[Circle CI|https://circleci.com/gh/vinaykumarchella/cassandra/340]|[Circle CI|https://circleci.com/gh/vinaykumarchella/cassandra/339]|

 ",23/Jan/19 13:39;krummas;and committed as {{7d138e20ea987d44fffbc47de4674b253b7431ff}} - thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle ant-optional dependency,CASSANDRA-14915,13201256,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,29/Nov/18 06:43,12/Mar/19 14:20,13/Mar/19 22:35,20/Dec/18 12:48,3.0.18,3.11.4,4.0,,Build,,,,0,,,,"CASSANDRA-13117 added a JUnit task which dumps threads on unit test timeout, and it depends on a class in {{org.apache.tools.ant.taskdefs.optional}} which seems to not always be present depending on how {{ant}} was installed. It can cause this error when building;
{code:java}
Throws: cassandra-trunk/build.xml:1134: taskdef A class needed by class org.krummas.junit.JStackJUnitTask cannot be found:
org/apache/tools/ant/taskdefs/optional/junit/JUnitTask  using the classloader
AntClassLoader[/.../cassandra-trunk/lib/jstackjunit-0.0.1.jar]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-30 17:00:42.17,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 20 12:48:32 UTC 2018,,,,,,0|s00za8:,9223372036854775807,,,,,jmeredithco,jmeredithco,,,,,,,,,,,"29/Nov/18 07:46;krummas;tested on Debian, Linux Mint and Fedora and with the ant 1.10 tarball, it fails only on Fedora, workaround for now is to {{yum install ant-optional}}, but working on a patch to not fail the build if it is not available.","30/Nov/18 08:39;krummas;patch: https://github.com/krummas/cassandra/commits/marcuse/junit
tests: https://circleci.com/workflow-run/85f433df-51f5-4b93-9c56-38788064041e

this will allow users to build cassandra without ant-optional

[~jmeredithco] do you have time to review? (you made some changes to build.xml lately so figured you would be a good candidate)","30/Nov/18 17:00;jmeredithco;I've confirmed the patch allows you to get beyond the initial error in the description on Fedora 26 and complete an {{ant build}}. However, it still requires ant-junit to be installed to get any of the test targets to complete (which are needed for {{jar}}/{{package}} targets).

Is this what you were hoping to achieve, or did you also expect the unit tests to be able to run with just {{ant}} and {{java-1.8.0-openjdk-devel}} installed?

An alternative would be to be more explicit about which dependencies are needed to build on Fedora in the docs.

 

 ","30/Nov/18 17:40;krummas;yeah we need to document that tests depend on ant-optional on fedora, I'll update the patch with that next week","30/Nov/18 18:53;jmeredithco;I think that class is always packaged up in the {{ant-junit}} dependency, I grabbed the rpm and expanded it and checked the class is there.  I couldn't find an {{ant-optional}} package that included it under the standard repos.

{code}
[jmeredith@localhost x]$ jar tvf ./usr/share/java/ant/ant-junit.jar | grep JUnitTask
1746 Wed Mar 01 08:56:26 MST 2017 org/apache/tools/ant/taskdefs/optional/junit/JUnitTask$1.class
{code}

If you always need to install {{ant-junit}} on Fedora, does this patch still have value other than permitting the build task to succeed (and none of the test tasks)?","30/Nov/18 20:06;jmeredithco;I worked out you meant ant-optional on the Debian derived ones - agreed we'll just need document requiring ant-junit on Fedora et al, and will test the patch against a distro with ant-optional available too.","30/Nov/18 20:38;krummas;bq. I worked out you meant ant-optional
ah sorry about that","30/Nov/18 20:41;jmeredithco;Gave Ubuntu 16.04 a go - if you don't have {{ant-optionals}} installed the patch allows the {{ant build}} command to succeed, however the tests still won't run without {{ant-optionals}} installed as that contains the junit integration. 

Anyway, patch works as advertised (lowering the test timeout still triggers stack traces) - however I'm not sure why somebody would want to run without ant-junit available.

+1","19/Dec/18 13:52;krummas;3.0 branch: https://github.com/krummas/cassandra/commits/marcuse/14915-3.0
3.0 test run: https://circleci.com/workflow-run/6328c243-e81d-48cb-bb46-75920200d73e
3.11 branch: https://github.com/krummas/cassandra/commits/marcuse/14915-3.11
3.11 test run: https://circleci.com/workflow-run/eeed3f28-4230-48e7-8fc0-41db30df45da
trunk branch: https://github.com/krummas/cassandra/commits/marcuse/14915-trunk
trunk test run: https://circleci.com/workflow-run/6ae5b998-caa5-41ff-b172-f74c9a266950

note the documentation update on the trunk branch:
https://github.com/apache/cassandra/compare/trunk...krummas:marcuse/14915-trunk?expand=1#diff-1a8570f8bfa11221c5ea6eb81bf4f22eR55
I only added it to the trunk branch since we don't have any in-tree docs in the other branches, but users will most likely google the error and find this^",19/Dec/18 15:11;jmeredithco;Patches all look good.  Does org.apache.cassandra.distributed.DistributedReadWritePathTest on trunk experience intermittent failures? I had a quick look but couldn't see much more than abnormal VM exit as the reason.,19/Dec/18 15:25;krummas;[~jmeredithco] yeah I think it might be because I ran on the low resource circle config - there is CASSANDRA-14922 which seems to be addressing those issues,"20/Dec/18 12:48;krummas;and committed as {{23d722ee3a7b91bfe5ee6fe64d090ff247e80438}}, thanks!

of course I messed up the commit message and said ant-junit instead of ant-optional, the doc change should be correct though",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some comparisons used for verifying paging queries in dtests only test the column names and not values,CASSANDRA-14920,13202230,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,aweisberg,beobal,beobal,04/Dec/18 10:31,12/Mar/19 14:19,13/Mar/19 22:35,15/Jan/19 21:35,,,,,Test/dtest,,,,0,,,,"The implementation of {{PageAssertionMixin::assertEqualIgnoreOrder}} introduced in CASSANDRA-14134 can't be used to compare expected and actual results when the row data is represented by a {{dict}}. The underlying {{list_to_hashed_dict}} function expected lists of values and when it encounters a dict, it constructs its normalized list using only the keys. So the actual result values may be completely wrong, but as long as the field names are the same the equality check will pass. This affects only {{paging_tests.py}} and {{upgrade_tests/paging_test.py}}, and looks like it maybe a leftover from an earlier dev iteration, as some tests in the affected fixtures are already using the alternative (and correct) {{assertions.py::assert_lists_equal_ignoring_order}}.
",,,,,,,,,,,,,,,CASSANDRA-14421,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-01-15 21:34:57.42,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 15 21:34:57 UTC 2019,,,,,,0|s01588:,9223372036854775807,,,,,,,,,,,,,,,,,"15/Jan/19 21:34;aweisberg;I resolved this as part of 14421, I modifed https://github.com/apache/cassandra-dtest/commit/84598f11513f4c1dc0be4d7115a47b59940a649e#diff-96abe2232f1118eb0579d88abe504a9eL167 a bit and also pointed some tests at the other equality method that creates an order by sorting on a key.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix query pager DEBUG log leak causing hit in paged reads throughput,CASSANDRA-14318,13145729,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,adejanovski,adejanovski,adejanovski,16/Mar/18 14:25,12/Mar/19 14:19,13/Mar/19 22:35,30/Mar/18 15:18,2.2.13,,,,,,,,2,lhf,performance,,"Debug logging can involve in many cases (especially very low latency ones) a very important overhead on the read path in 2.2 as we've seen when upgrading clusters from 2.0 to 2.2.

The performance impact was especially noticeable on the client side metrics, where p99 could go up to 10 times higher, while ClientRequest metrics recorded by Cassandra didn't show any overhead.

Below shows latencies recorded on the client side with debug logging on first, and then without it :

!debuglogging.png!  

We generated a flame graph before turning off debug logging that shows the read call stack is dominated by debug logging : 

!flame_graph_snapshot.png!

I've attached the original flame graph for exploration.

Once disabled, the new flame graph shows that the read call stack gets extremely thin, which is further confirmed by client recorded metrics : 

!flame22 nodebug sjk svg.png!

The query pager code has been reworked since 3.0 and it looks like log.debug() calls are gone there, but for 2.2 users and to prevent such issues to appear with default settings, I really think debug logging should be disabled by default.",,,,,,,,,,,,CASSANDRA-14326,,,,,,27/Mar/18 16:04;adejanovski;cassandra-2.2-debug.yaml;https://issues.apache.org/jira/secure/attachment/12916415/cassandra-2.2-debug.yaml,16/Mar/18 13:55;adejanovski;debuglogging.png;https://issues.apache.org/jira/secure/attachment/12914871/debuglogging.png,16/Mar/18 14:05;adejanovski;flame22 nodebug sjk svg.png;https://issues.apache.org/jira/secure/attachment/12914868/flame22+nodebug+sjk+svg.png,16/Mar/18 14:05;adejanovski;flame22-nodebug-sjk.svg;https://issues.apache.org/jira/secure/attachment/12914867/flame22-nodebug-sjk.svg,16/Mar/18 13:59;adejanovski;flame22-sjk.svg;https://issues.apache.org/jira/secure/attachment/12914870/flame22-sjk.svg,16/Mar/18 14:00;adejanovski;flame_graph_snapshot.png;https://issues.apache.org/jira/secure/attachment/12914869/flame_graph_snapshot.png,,,,,,6.0,,,,,,,,,,,,,,,,,,,2018-03-16 20:59:10.357,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 03 12:45:59 UTC 2018,,,,,,0|i3rem7:,9223372036854775807,2.2.12,,,,pauloricardomg,pauloricardomg,,,,2.2.1,,,,,,,"16/Mar/18 14:31;adejanovski;Here's the [patch for 2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...thelastpickle:disable-debug-logging-by-default]

It should be mergeable in 3.0/3.11/4.0 without a problem.",16/Mar/18 20:59;vinaykumarcse;+1 on your perf test results. Patch LGTM.,"17/Mar/18 18:21;jjordan;CASSANDRA-10241 created that debug.log to be a “production” debug log. If there are things being logged at DEBUG which cause performance issues we should disable those or move them to TRACE, not turn off the debug.log.","17/Mar/18 19:18;adejanovski;Fair enough, I'll go down the read and write path to see if there's debug logging in v3 and above, and switch debug to trace in the query pager in 2.2.","18/Mar/18 19:25;pauloricardomg;Hey Alexander, would you care to re-run the experiments with the query pager DEBUG logs changed to TRACE level to check if debug.log logging overhead will still persist? Thanks!","19/Mar/18 13:22;adejanovski;[~pauloricardomg], thanks for assigning the ticket to me.

Whatever the implementation, I'll run new performance tests and generate flame graphs to verify the impact of the changes, no worries.

I'll wait a bit for a consensus to come out of the discussion on the dev ML before moving on, as there seem to be some conflicting views on what should be done.",19/Mar/18 13:41;jjordan;No matter what is decided we should move those log messages to TRACE.  So I think we can proceed here no matter what.,19/Mar/18 16:10;adejanovski;Fair point. I'll write the patch and run the benchmarks.,"19/Mar/18 23:21;jjirsa;If you havent yet started the second run of benchmarks, you may also want to move the debug log statements from ReadCallback.java (one deals with digest mismatch exceptions, forgot what the other is) to trace as well. ","27/Mar/18 16:10;adejanovski;[~jjirsa]: apparently the ReadCallback class already logs at TRACE and not DEBUG on the latest 2.2.

I've created the fix that downgrades debug logging to trace logging in the query pager classes, and here are the results : 

debug on - no fix :
{noformat}
Results:
op rate : 6681 [read_event_1:1109, read_event_2:1119, read_event_3:4452]
partition rate : 6681 [read_event_1:1109, read_event_2:1119, read_event_3:4452]
row rate : 6681 [read_event_1:1109, read_event_2:1119, read_event_3:4452]
latency mean : 19,1 [read_event_1:15,4, read_event_2:15,4, read_event_3:21,0]
latency median : 15,6 [read_event_1:14,2, read_event_2:14,0, read_event_3:16,3]
latency 95th percentile : 39,1 [read_event_1:28,4, read_event_2:28,6, read_event_3:44,2]
latency 99th percentile : 75,6 [read_event_1:52,9, read_event_2:53,6, read_event_3:87,7]
latency 99.9th percentile : 315,7 [read_event_1:101,0, read_event_2:110,1, read_event_3:361,1]
latency max : 609,1 [read_event_1:319,6, read_event_2:315,9, read_event_3:609,1]
Total partitions : 993050 [read_event_1:164882, read_event_2:166381, read_event_3:661787]
Total errors : 0 [read_event_1:0, read_event_2:0, read_event_3:0]
total gc count : 189
total gc mb : 56464
total gc time (s) : 7
avg gc time(ms) : 37
stdev gc time(ms) : 8
Total operation time : 00:02:28{noformat}
 

 

debug off - no fix :
{noformat}
Results:
op rate : 12655 [read_event_1:2141, read_event_2:2093, read_event_3:8422]
partition rate : 12655 [read_event_1:2141, read_event_2:2093, read_event_3:8422]
row rate : 12655 [read_event_1:2141, read_event_2:2093, read_event_3:8422]
latency mean : 10,1 [read_event_1:10,1, read_event_2:10,1, read_event_3:10,1]
latency median : 9,2 [read_event_1:9,2, read_event_2:9,2, read_event_3:9,3]
latency 95th percentile : 15,2 [read_event_1:15,8, read_event_2:15,9, read_event_3:15,7]
latency 99th percentile : 29,3 [read_event_1:44,5, read_event_2:45,1, read_event_3:41,3]
latency 99.9th percentile : 52,7 [read_event_1:67,9, read_event_2:66,9, read_event_3:67,1]
latency max : 268,0 [read_event_1:257,1, read_event_2:263,3, read_event_3:268,0]
Total partitions : 983056 [read_event_1:166311, read_event_2:162570, read_event_3:654175]
Total errors : 0 [read_event_1:0, read_event_2:0, read_event_3:0]
total gc count : 100
total gc mb : 31529
total gc time (s) : 4
avg gc time(ms) : 37
stdev gc time(ms) : 5
Total operation time : 00:01:17{noformat}
 

 

debug on - with fix :
{noformat}
Results:
op rate : 12289 [read_event_1:2058, read_event_2:2051, read_event_3:8181]
partition rate : 12289 [read_event_1:2058, read_event_2:2051, read_event_3:8181]
row rate : 12289 [read_event_1:2058, read_event_2:2051, read_event_3:8181]
latency mean : 10,4 [read_event_1:10,4, read_event_2:10,4, read_event_3:10,4]
latency median : 9,4 [read_event_1:9,4, read_event_2:9,4, read_event_3:9,4]
latency 95th percentile : 16,3 [read_event_1:16,8, read_event_2:17,3, read_event_3:16,2]
latency 99th percentile : 36,6 [read_event_1:44,3, read_event_2:46,6, read_event_3:37,2]
latency 99.9th percentile : 62,2 [read_event_1:78,0, read_event_2:77,1, read_event_3:80,8]
latency max : 251,2 [read_event_1:246,9, read_event_2:249,9, read_event_3:251,2]
Total partitions : 1000000 [read_event_1:167422, read_event_2:166861, read_event_3:665717]
Total errors : 0 [read_event_1:0, read_event_2:0, read_event_3:0]
total gc count : 102
total gc mb : 31843
total gc time (s) : 4
avg gc time(ms) : 38
stdev gc time(ms) : 6
Total operation time : 00:01:21{noformat}
 

 

So we have similar performance with debug logging off and with the fix and debug on.
 The difference in throughput is pretty massive as we roughly get *twice the read throughput* with the fix.

Latencies without the fix and with the fix : 

p95 : 35ms -> 16ms
 p99 : 75ms -> 36ms

I've ran all tests several times, alternating with and without the fix to make sure caches were not making a difference, and results were consistent with what's pasted above.
 It's been running on a single node using an i3.xlarge instance for Cassandra and another i3.large for running cassandra-stress.

 

*One pretty interesting thing to note* is that when I tested with the predefined mode of cassandra-stress, no paging occurred and the performance difference was not noticeable. This is due to the fact that the predefined mode generates COMPACT STORAGE tables, which involve a different read path (apparently). I think anyone performing benchmarks for Cassandra changes should be aware that the predefined mode isn't relevant and that a user defined test should be used (maybe we should create one that would be used as standard benchmark). 
 Here's the one I used : [^cassandra-2.2-debug.yaml]

With the following commands for writing : 

 
{noformat}
/usr/bin/cassandra-stress user profile=/home/ec2-user/cassandra-2.2-debug.yaml n=1000000 'ops(insert=1)' cl=LOCAL_ONE no-warmup -node 172.31.31.42 -mode native cql3 compression=lz4 -rate threads=128{noformat}
 

And for reading : 

 
{noformat}
/usr/bin/cassandra-stress user profile=/home/ec2-user/cassandra-2.2-debug.yaml n=1000000 'ops(read_event_1=1,read_event_2=1,read_event_3=4)' cl=LOCAL_ONE no-warmup -node 172.31.31.42 -mode native cql3 compression=lz4 -rate threads=128{noformat}
 

[~pauloricardomg] : here's the [patch|https://github.com/apache/cassandra/compare/cassandra-2.2...thelastpickle:CASSANDRA-14318] if you're willing to review/commit it, and the unit test results in [CircleCI|https://circleci.com/gh/thelastpickle/cassandra/178].","27/Mar/18 16:57;adejanovski;For the record, the same tests on 3.11.2 didn't show any notable performance difference between debug on and off : 

Cassandra 3.11.2 debug on : 
{noformat}
Results:
Op rate : 18 777 op/s [read_event_1: 3 165 op/s, read_event_2: 3 109 op/s, read_event_3: 12 562 op/s]
Partition rate : 6 215 pk/s [read_event_1: 3 165 pk/s, read_event_2: 3 109 pk/s, read_event_3: 0 pk/s]
Row rate : 6 215 row/s [read_event_1: 3 165 row/s, read_event_2: 3 109 row/s, read_event_3: 0 row/s]
Latency mean : 6,7 ms [read_event_1: 6,7 ms, read_event_2: 6,7 ms, read_event_3: 6,6 ms]
Latency median : 5,0 ms [read_event_1: 5,0 ms, read_event_2: 5,0 ms, read_event_3: 4,9 ms]
Latency 95th percentile : 15,6 ms [read_event_1: 15,5 ms, read_event_2: 15,9 ms, read_event_3: 15,5 ms]
Latency 99th percentile : 43,3 ms [read_event_1: 42,7 ms, read_event_2: 44,2 ms, read_event_3: 43,2 ms]
Latency 99.9th percentile : 82,0 ms [read_event_1: 80,3 ms, read_event_2: 82,4 ms, read_event_3: 82,1 ms]
Latency max : 272,4 ms [read_event_1: 272,4 ms, read_event_2: 268,7 ms, read_event_3: 245,1 ms]
Total partitions : 330 970 [read_event_1: 165 386, read_event_2: 165 584, read_event_3: 0]
Total errors : 0 [read_event_1: 0, read_event_2: 0, read_event_3: 0]
Total GC count : 42
Total GC memory : 13,102 GiB
Total GC time : 1,8 seconds
Avg GC time : 42,4 ms
StdDev GC time : 1,3 ms
Total operation time : 00:00:53{noformat}
 


Cassandra 3.11.2 debug off : 
{noformat}
Results:
Op rate : 18 853 op/s [read_event_1: 3 138 op/s, read_event_2: 3 137 op/s, read_event_3: 12 578 op/s]
Partition rate : 6 275 pk/s [read_event_1: 3 138 pk/s, read_event_2: 3 137 pk/s, read_event_3: 0 pk/s]
Row rate : 6 275 row/s [read_event_1: 3 138 row/s, read_event_2: 3 137 row/s, read_event_3: 0 row/s]
Latency mean : 6,7 ms [read_event_1: 6,7 ms, read_event_2: 6,7 ms, read_event_3: 6,7 ms]
Latency median : 5,0 ms [read_event_1: 5,1 ms, read_event_2: 5,1 ms, read_event_3: 5,0 ms]
Latency 95th percentile : 15,5 ms [read_event_1: 15,5 ms, read_event_2: 15,6 ms, read_event_3: 15,4 ms]
Latency 99th percentile : 39,9 ms [read_event_1: 41,0 ms, read_event_2: 39,6 ms, read_event_3: 39,6 ms]
Latency 99.9th percentile : 73,3 ms [read_event_1: 73,4 ms, read_event_2: 71,6 ms, read_event_3: 73,6 ms]
Latency max : 367,0 ms [read_event_1: 240,5 ms, read_event_2: 250,3 ms, read_event_3: 367,0 ms]
Total partitions : 332 852 [read_event_1: 166 447, read_event_2: 166 405, read_event_3: 0]
Total errors : 0 [read_event_1: 0, read_event_2: 0, read_event_3: 0]
Total GC count : 46
Total GC memory : 14,024 GiB
Total GC time : 2,0 seconds
Avg GC time : 42,7 ms
StdDev GC time : 3,9 ms
Total operation time : 00:00:53{noformat}
The improvement over 2.2 is nice though :)

 ","30/Mar/18 15:18;pauloricardomg;{quote}I think anyone performing benchmarks for Cassandra changes should be aware that the predefined mode isn't relevant and that a user defined test should be used (maybe we should create one that would be used as standard benchmark).
{quote}
Good find! Can you check if this is the case in trunk, and if so maybe open a lhf ticket to change that?
{quote}For the record, the same tests on 3.11.2 didn't show any notable performance difference between debug on and off
{quote}
Nice to know we managed to handle all debug/verbose log leaks there. It will be easier to maintain this after CASSANDRA-14326.
{quote}here's the patch if you're willing to review/commit it, and the unit test results in CircleCI.
{quote}
Thanks for the patch, experiments and analysis! Even though 2.2 is on critical fixes only mode, 50% is a significant performance hit on throughput for this workload, and since the patch is pretty simple I don't see a reason not to commit it.

CI looks good. I added a CHANGES.txt not and committed as {{ac77e5e7742548f7c7c25da3923841f59d4b2713}} to cassandra-2.2 branch.","03/Apr/18 12:45;adejanovski;Thanks for reviewing and merging [~pauloricardomg] !

CASSANDRA-10857 removed compact storage options in trunk and the standard1&counter1 tables are no longer using it : [https://github.com/apache/cassandra/commit/07fbd8ee6042797aaade90357d625ba9d79c31e0#diff-e5d5cb263c5c84c322cd09391af46d7dL141] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix setting min/max compaction threshold with LCS,CASSANDRA-14388,13152896,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,17/Apr/18 05:43,12/Mar/19 14:19,13/Mar/19 22:35,19/Jun/18 16:04,4.0,,,,Local/Compaction,,,,0,lcs,,,To be able to actually set max/min_threshold in compaction options we need to remove it from the options map when validating.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-04-17 14:11:15.742,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 19 16:03:56 UTC 2018,,,,,,0|i3smnb:,9223372036854775807,,,,,ifesdjeen,ifesdjeen,,,,,,,,,,,"17/Apr/18 05:55;krummas;https://github.com/krummas/cassandra/commits/marcuse/14388

This patch also replaces the use of {{MAX_COMPACTING_L0}} with {{max_threshold}} in {{LeveledManifest}} - not 100% sure this is the best way since it will increase the number of sstables needed to run STCS in L0, but at the same time it will run actual LCS with more sstables. An alternative would be to check if there is more than 32 ({{MAX_COMPACTING_L0}}) sstables in L0, if so, grab {{max_threshold}} sstables and run STCS on them, wdyt [~cnlwsu]?","17/Apr/18 14:11;cnlwsu;I like the alternative personally
{quote}An alternative would be to check if there is more than 32 ({{MAX_COMPACTING_L0}}) sstables in L0, if so, grab {{max_threshold}} sstables and run STCS on them
{quote}
Thinking mostly for cases when its critically far behind (ie 50k in L0) and a lot are tiny (from bad repairs), just want to quickly reduce the number of them. The {{MAX_COMPACTING_L0}} I think makes sense for normal use, but when theres a huge backlog just want STCS to chew up backlog faster than 32 at a time. But if increase L0 max compacting to 1000 it may not kick off STCS until already negatively impacting things.

Might be nice to have {{MAX_COMPACTING_L0}} as another table option but that could be done another ticket.","18/Apr/18 07:12;krummas;pushed a commit with MAX_COMPACTING_L0 as the trigger for STCS in L0: https://github.com/krummas/cassandra/commits/marcuse/14388
tests: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14388",30/May/18 07:21;ifesdjeen;Should we add a news entry about the fact that {{MAX_COMPACTING_L0}} will now be overridden by the CFS max compaction threshold? And/or a ticket to make {{MAX_COMPACTING_L0}} configurable.,"30/May/18 08:55;krummas;setting [~ifesdjeen] as reviewer

Pushed a commit with an updated NEWS.txt entry - we could make MAX_COMPACTING_L0 configurable later if someone thinks it is necessary but 32 seems to be a good value right now, especially after this since it will only decide when to run a STCS compaction","19/Jun/18 16:03;ifesdjeen;Committed as d52bdaefda366b4485acb4e8852b3c0549b184bd to [trunk|https://github.com/apache/cassandra/commit/d52bdaefda366b4485acb4e8852b3c0549b184bd].
Thank you for the patch!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LWTs keep failing in trunk after immutable refactor,CASSANDRA-14356,13149158,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,burmanm,burmanm,burmanm,30/Mar/18 12:52,12/Mar/19 14:19,13/Mar/19 22:35,18/Jun/18 09:47,4.0,,,,Legacy/Core,,,,0,LWT,,,"In the PaxosState, the original assert check is in the form of:

assert promised.update.metadata() == accepted.update.metadata() && accepted.update.metadata() == mostRecentCommit.update.metadata();

However, after the change to make TableMetadata immutable this no longer works as these instances are not necessarily the same (or never). This causes the LWTs to fail although they're still correctly targetting the same table.

From IRC:

<pcmanus> It's a bug alright. Though really, the assertion should be on the metadata ids, cause TableMetadata#equals does more than what we want.
<pcmanus> That is, replacing by .equals() is not ok. That would reject throw on any change to a table metadata, while the spirit of the assumption was to sanity check both update were on the same table.","OpenJDK Runtime Environment (build 1.8.0_161-b14), Cassandra 4.0 commit c22ee2bd451d030e99cfb65be839bbc735a5352f (29.3.2018 14:01)",,,,,,,,,,,,,,,,,30/Mar/18 16:04;burmanm;CASSANDRA-14356.diff;https://issues.apache.org/jira/secure/attachment/12917039/CASSANDRA-14356.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-06-17 04:26:38.83,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 18 10:05:24 UTC 2018,,,,,,0|i3rzpb:,9223372036854775807,4.0,,,,michaelsembwever,michaelsembwever,,,,,,,,,,,"17/Jun/18 04:26;michaelsembwever;This exception is evident in Reaper's builds against Cassandra trunk, ref [travis|https://travis-ci.org/thelastpickle/cassandra-reaper/branches]","17/Jun/18 04:40;michaelsembwever;|| Branch || uTest || aTest || dTest ||
|[trunk_14356|https://github.com/thelastpickle/cassandra/tree/mck/trunk_14356]|[!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Ftrunk_14356.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Ftrunk_14356]| [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/35/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/35] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/574/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/574] |",17/Jun/18 18:44;jjirsa;cc [~iamaleksey] for visibility.,"17/Jun/18 22:50;michaelsembwever;[~iamaleksey], any objections if I commit this? It makes sense, looks good, to me, and has been tested and verified against Reaper.","18/Jun/18 09:37;iamaleksey;[~michaelsembwever] Change LGTM, ship it (:",18/Jun/18 10:05;michaelsembwever;Committed as 717c108374,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThrottledUnfilteredIterator failed on UnfilteredRowIterator with only partition level info,CASSANDRA-14315,13145298,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,15/Mar/18 07:57,12/Mar/19 14:19,13/Mar/19 22:35,20/Mar/18 02:22,4.0,,,,Feature/Materialized Views,,,,0,,,,"When repairing base table with MV, in order to avoid OOM, Cassandra-13299 added ThrottledUnfilteredIterator to split large partition into small chunks, but it didn't handle partition without unfiltered properly.

{code:title=repro}
// create cell tombstone, range tombstone, partition deletion
createTable(""CREATE TABLE %s (pk int, ck1 int, ck2 int, v1 int, v2 int, PRIMARY KEY (pk, ck1, ck2))"");
// partition deletion
execute(""DELETE FROM %s USING TIMESTAMP 160 WHERE pk=1"");

// flush and generate 1 sstable
ColumnFamilyStore cfs = Keyspace.open(keyspace()).getColumnFamilyStore(currentTable());
cfs.forceBlockingFlush();
cfs.disableAutoCompaction();
cfs.forceMajorCompaction();

assertEquals(1, cfs.getLiveSSTables().size());
SSTableReader reader = cfs.getLiveSSTables().iterator().next();

try (ISSTableScanner scanner = reader.getScanner();
        CloseableIterator<UnfilteredRowIterator> throttled = ThrottledUnfilteredIterator.throttle(scanner, 100))
{
    assertTrue(throttled.hasNext());
    UnfilteredRowIterator iterator = throttled.next();
    assertFalse(throttled.hasNext());
    assertFalse(iterator.hasNext());
    assertEquals(iterator.partitionLevelDeletion().markedForDeleteAt(), 160);
}
{code}",,,,,,,,,,,,,,,,,,19/Mar/18 10:20;jasonstack;dtest.png;https://issues.apache.org/jira/secure/attachment/12915107/dtest.png,19/Mar/18 10:20;jasonstack;unit test.png;https://issues.apache.org/jira/secure/attachment/12915108/unit+test.png,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2018-03-20 02:22:17.547,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 20 03:00:07 UTC 2018,,,,,,0|i3rbyf:,9223372036854775807,,,,,pauloricardomg,pauloricardomg,,,,,,,,,,,"15/Mar/18 07:58;jasonstack;| [trunk|https://github.com/jasonstack/cassandra/commits/throttle-trunk]|
| [dtest|https://github.com/jasonstack/cassandra-dtest/commits/throttle-mv] |

Changes:
1. when batch size is 0, don't throttle and return itself. (opt out)
2. when the original UnfilteredRowIterator has only partition level info, return itself once.
3. removed the assertion ""UnfilteredPartitionIterator should not contain empty partitions""",20/Mar/18 02:22;pauloricardomg;Good catch! Patch and tests LGTM. Committed as {{5b9e985474e696a83d23e7cf4bedaf360cdb1eaf}} to trunk. Thanks!,20/Mar/18 02:43;pauloricardomg;Committed dtest as {{2c1b986bc82ad29a4db06158043aceaaf473e17c}}.,20/Mar/18 03:00;jasonstack;Thanks for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle repeat open bound from SRP in read repair,CASSANDRA-14330,13146590,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,20/Mar/18 16:27,12/Mar/19 14:19,13/Mar/19 22:35,23/Mar/18 14:56,3.0.17,3.11.3,4.0,,,,,,0,,,,"If there is an open range tombstone in an iterator, a short read protection request for it will include a repeat open bound. Currently, {{DataResolver}} doesn't expect this, and will raise an assertion, timing out the request:
{code:java}
java.lang.AssertionError: Error merging RTs on test.test: merged=null, versions=[Marker EXCL_START_BOUND(0)@0, null], sources={[/127.0.0.1, /127.0.0.2]}, responses:
    /127.0.0.1 => [test.test] key=0 partition_deletion=deletedAt=-9223372036854775808, localDeletion=2147483647 columns=[[] | []]
       Row[info=[ts=1] ]: ck=0 | ,
   /127.0.0.2 => [test.test] key=0 partition_deletion=deletedAt=-9223372036854775808, localDeletion=2147483647 columns=[[] | []]
       Row[info=[ts=-9223372036854775808] del=deletedAt=1, localDeletion=1521572669 ]: ck=0 |
       Row[info=[ts=1] ]: ck=1 | 
{code}
As this is a completely normal/common scenario, we should allow for this, and relax the assertion.

Additionally, the linked branch makes the re-throwing {{AssertionError}} more detailed and more correct: the responses are now printed out in the correct order, respecting {{isReversed}}, the command causing the assertion is now logged, as is {{isReversed}} itself, and local deletion times for RTs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-23 14:29:28.965,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 23 14:56:26 UTC 2018,,,,,,0|i3rjxj:,9223372036854775807,,,,,slebresne,slebresne,,,,,,,,,,,"21/Mar/18 15:52;iamaleksey;Branches for [3.0|https://github.com/iamaleksey/cassandra/tree/14330-3.0], [3.11|https://github.com/iamaleksey/cassandra/tree/14330-3.11], and [4.0|https://github.com/iamaleksey/cassandra/tree/14330-4.0]. CI: [3.0|https://circleci.com/gh/iamaleksey/cassandra/70], [3.11|https://circleci.com/gh/iamaleksey/cassandra/73], [4.0|https://circleci.com/gh/iamaleksey/cassandra/74].

Minimal reproduction dtest [here|https://github.com/iamaleksey/cassandra-dtest/tree/14330].","23/Mar/18 14:29;slebresne;+1, fix lgtm. Nice job tracking that down and on the minimal test.","23/Mar/18 14:56;iamaleksey;Thanks for the prompt review, [~slebresne]

Committed as [3153c630c499edf3c523d8e5a3db6f1d6c52ad4c|https://github.com/apache/cassandra/commit/3153c630c499edf3c523d8e5a3db6f1d6c52ad4c] to 3.0 and merged upwards. Committed the dtest as [dac3d7535cc120a6615257fb9dd05988e9901dc4|https://github.com/apache/cassandra-dtest/commit/dac3d7535cc120a6615257fb9dd05988e9901dc4].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra not starting when using enhanced startup scripts in windows,CASSANDRA-14418,13155164,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sphirke,sphirke,sphirke,25/Apr/18 15:45,12/Mar/19 14:19,13/Mar/19 22:35,12/May/18 12:34,3.0.17,3.11.3,4.0,,,,,,0,,,,"I am using Apache Cassandra 3.11.2 with my application. 

My application is getting installed under C:/Program Files/My Application/Some Folder/.

And cassandra C:/Program Files/My Application/Some Folder/cassandra.

So when I am using enhanced startup scripts cassandra not getting up and running and I am getting below error:

""Error: Could not find or load main class Files\My""

One of the solution I got is moving cassandra to another location where location path does not contain spaces. But this is not good way of getting this problem resolved.

After doing detailed analysis of all the scripts I found the solution below:

Inside file cassandra-env.ps1 at line number 380:

Replace line:

$env:JVM_OPTS = ""$env:JVM_OPTS -XX:CompileCommandFile=$env:CASSANDRA_CONF\hotspot_compiler""

with line

$env:JVM_OPTS = ""$env:JVM_OPTS -XX:CompileCommandFile=""""$env:CASSANDRA_CONF\hotspot_compiler""""""

Fix here is the double quotes added before $env:CASSANDRA_CONF and at the end.

At other places this case is well handled. But missed at this place.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-04-25 22:37:01.202,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue May 15 12:52:51 UTC 2018,,,,,,0|i3t0kv:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"25/Apr/18 22:37;jasobrown;the fix 'seems' legit, but as I'm not a Windows user I cannot confirm.

If there's any Windows user who can confirm, I'm happy to commit. /cc [~JoshuaMcKenzie] [~blerer] are either of you two still using Windows, and can check out this fix?",25/Apr/18 23:23;djoshi3;[~jasobrown] i have a windows VM - i can check.,"26/Apr/18 20:09;JoshuaMcKenzie;This fix immediately passes the smell test for me. Those double quotes on env vars were the bane of my existence when I was working on those scripts.

I'll leave you to the testing Dinesh, but I have high hopes.",10/May/18 05:32;sphirke;Any progress further..? Will this be available officially in the upcoming releases?,10/May/18 12:02;jasobrown;[~djoshi3] did you have a chance to give this shot?,12/May/18 08:24;djoshi3;[~sphirke] [~jasobrown] Apologies for the delay. I have been busy. I can confirm that the fix works. It would be useful to double check if there are other places in the script where we may encounter similar issues.,"12/May/18 12:34;jasobrown;Thanks for confirming, [~djoshi3].

Looks like this line (and a similar one with the same quoting problem) was introduced in CASSANDRA-10939 (committed to 3.0+). I've gone ahead and fixed both those lines for 3.0 and up. Committed as sha {{b9b2a4e1a07af518cebd4441469c940d5ac0c2ea}}. Thanks, all!",15/May/18 12:51;sphirke;Any idea about 3.11.3 release date?,15/May/18 12:52;sphirke;And thank you very much guys for help.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Bounds instead of Range to represent sstable first/last token when checking how to anticompact sstables,CASSANDRA-14411,13154380,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,23/Apr/18 06:23,12/Mar/19 14:19,13/Mar/19 22:35,24/Apr/18 07:02,2.2.13,3.0.17,3.11.3,4.0,Consistency/Repair,,,,0,,,,"There is currently a chance of missing marking a token as repaired due to the fact that we use Range which are (a, b] to represent first/last token in sstables instead of Bounds which are [a, b].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-04-23 16:44:31.716,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 24 07:02:18 UTC 2018,,,,,,0|i3svqn:,9223372036854775807,,,,,bdeggleston,bdeggleston,,,,,,,,,,,"23/Apr/18 06:25;krummas;https://github.com/krummas/cassandra/commits/marcuse/14411

tests:
https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14411","23/Apr/18 07:27;krummas;minimal patches for 2.2 -> 3.11: 
https://github.com/krummas/cassandra/commits/marcuse/14411-2.2
https://github.com/krummas/cassandra/commits/marcuse/14411-3.0
https://github.com/krummas/cassandra/commits/marcuse/14411-3.11

tests for 3.11: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14411-3%2E11","23/Apr/18 16:44;bdeggleston;circle seems to be down, but +1 assuming tests look good","24/Apr/18 07:02;krummas;committed as {{334dca9aa825e6d353aa04fd97016ac1077ff132}}, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Unable to parse targets for index"" on upgrade to Cassandra 3.0.10-3.0.16",CASSANDRA-14468,13161735,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jrwest,wadey,wadey,24/May/18 10:39,12/Mar/19 14:19,13/Mar/19 22:35,31/Jul/18 19:35,3.0.18,3.11.4,,,,,,,0,,,,"I am attempting to upgrade from Cassandra 2.2.10 to 3.0.16. I am getting this error:

{code}
org.apache.cassandra.exceptions.ConfigurationException: Unable to parse targets for index idx_foo (""666f6f"")
	at org.apache.cassandra.index.internal.CassandraIndex.parseTarget(CassandraIndex.java:800) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.index.internal.CassandraIndex.indexCfsMetadata(CassandraIndex.java:747) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:645) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:251) [apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569) [apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697) [apache-cassandra-3.0.16.jar:3.0.16]
{code}

It looks like this might be related to CASSANDRA-14104 that was just added to 3.0.16 ",,,,,,,,,,,,,,,CASSANDRA-12516,CASSANDRA-14104,,01/Jun/18 12:29;wadey;data.tar.gz;https://issues.apache.org/jira/secure/attachment/12926084/data.tar.gz,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-05-31 23:29:07.119,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 31 19:34:44 UTC 2018,,,,,,0|i3u453:,9223372036854775807,3.0.10,3.0.12,3.0.15,3.0.16,iamaleksey,iamaleksey,,,,3.0.10,,,,,,,"24/May/18 10:50;wadey;I get the same error trying to upgrade from 2.2.10 -> 3.0.15, so it looks like the error is unrelated to CASSANDRA-14104","24/May/18 10:55;wadey;Upgrade from 2.2.10 -> 3.0.9 does not hit this error, so I will search for the version that introduces the failure.",24/May/18 10:59;wadey;The issue first appears to 3.0.10. Upgrades to 3.0.9 work but upgrades to 3.0.10+ fail with the error.,"24/May/18 11:55;wadey;Simplified a bit, the schema basically looks like this (from cassandra-cli, this is a legacy schema):

{code}
create column family foos
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'UTF8Type'
  and key_validation_class = 'UTF8Type'
  and read_repair_chance = 0.0
  and dclocal_read_repair_chance = 0.1
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and cells_per_row_to_cache = '0'
  and default_time_to_live = 0
  and speculative_retry = 'NONE'
  and bloom_filter_fp_chance = 0.01
  and column_metadata = [
    {column_name : '666f6f',
    validation_class : UTF8Type,
    index_name : 'idx_foo',
    index_type : 0}]
and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.LZ4Compressor'};
{code}

It looks like this in cqlsh (from 2.2):

{code}
CREATE TABLE myks.foos (
    key text PRIMARY KEY,
    ""666f6f"" text
) WITH COMPACT STORAGE
    AND bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = 'NONE';
CREATE INDEX idx_foo ON myks.foos (""666f6f"");
{code}","24/May/18 12:38;wadey;I have a guess that it might be due to CASSANDRA-12516, because my column names are of type BytesType but that change forces column identifiers to load as UTF8Type.",31/May/18 23:29;jasobrown;[~jrwest] interesting in taking a look at this?,01/Jun/18 06:20;jrwest;Can take a look next week,"01/Jun/18 12:05;wadey;Ok I've dug a bit further and found out my simplified schema will not reproduce it. After running a debugger, it appears that the issue somehow related to ColumnIdentifier.internedInstances. For some reason, some entries in this map have their ""text"" set to the actual name of the column (like ""foo"") and some have their text set to the hex bytes of the name (like ""666f6f""). I think the issue is that the InternedKey only matches on the bytes of the column name, so both lookups with ""foo"" and ""666f6f"" will pull the same interned entry from the map. The stacktrace happens when the schema has the hex byte version of the name, but the internedmap has the ascii version.

My current guess is there are similar column names in other tables and they are getting interned incorrectly. I'll see if I can dig into this more.","01/Jun/18 12:10;wadey;Interestingly, it looks like the ticket that caused this issue was trying to solve a very similar issue! CASSANDRA-12516

The problem is that the type is listed as ""UTF8Type"" for both calls to getInterned. I'm guessing when its being called with hex bytes it should not be UTF8Type and this is the bug.","01/Jun/18 12:15;wadey;I think the issue is that {{type}} on this line is the type for the value of the column, and not the type for the name of the column: https://github.com/apache/cassandra/blob/cassandra-3.0.16/src/java/org/apache/cassandra/schema/SchemaKeyspace.java#L1063","01/Jun/18 12:28;wadey;I was able to reproduce by creating two column families. Each with a column named ""foo"", one with comparator type BytesType and one with UTF8Type.

I will attach the Cassandra 2.2.10 data directory from this reproduction. If you start up Cassandra 3.0.10+ with this data it will reproduce the error.","24/Jul/18 01:39;jrwest;[~wadey], sorry for taking so long to get to this but I finally had some time today. I agree with your assessment so far but unfortunately don’t have much to add. It looks like CASSANDRA-12516 fixed what the cache was keyed on but not all the {{getInterned}} call sites. Indeed the {{type}} column is the CQL (value) type. Further, we no longer have the comparator after {{LegacySchemaMigrator}} runs (of note, {{LegacySchemaMigrator}} does use {{getInterned}} as intended but since we lose the comparator that only makes things worse)*.

[~iamaleksey], do you have any thoughts on this since you reported the original issue?

\* I’m actually just getting familiar with this code but I think [~jasobrown] referred this ticket to me because of the initial relation to 2i",24/Jul/18 11:18;iamaleksey;Ugh. This is both unfortunate and tricky. Let me think how we can mitigate this.,"24/Jul/18 15:12;iamaleksey;[~wadey] [~jrwest] You are right with your analysis. I'm sorry I missed this one during CASSANDRA-12516 review. The good news is that it should be trivial to fix (unless I'm missing something).

All we need to do is skip interning in {{SchemaKeyspace.createColumnFromRow()}}.",24/Jul/18 19:06;jrwest;[~iamaleksey] tried that locally. It looks like {{ColumnDefinition}} requires {{ColumnIdentifier}} to be interned: [https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/config/ColumnDefinition.java#L156.] ,"24/Jul/18 19:19;iamaleksey;[~jrwest] Right. I'm trying to find out why it asserts that, but can't. Even equals impl. doesn't have a == shortcut. I *think* it should be safe to drop that assertion altogether (maybe with some more research first).

The alternative is not pretty at all, and won't work in 4.0 - the alternative being replicating {{CFMetaData.getColumnDefinitionNameComparator()}} method but without having all of the data yet. Ugh.",24/Jul/18 19:45;jrwest;[~iamaleksey] ugh indeed. I'll lend an extra pair of eyes to whether or not that assertion is needed. ,"26/Jul/18 18:29;jrwest;[~iamaleksey] reading the code again, I *think* it should be safe to drop as well, for the reasons you list. The {{ColumnIdentifier}} in the {{ColumnDefinition}}/{{ColumnMetadata}} will be different (by reference) than the ones returned by {{Literal#prepare}} but since they are structurally equal that should be ok. Otherwise, its hard to separate out its initial intention since it was committed as part of CASSANDRA-8099. ","26/Jul/18 19:11;iamaleksey;[~jrwest] Agreed. Do it (or [~wadey], if he prefers), and I'll review promptly.",26/Jul/18 20:17;jrwest;Assigned to myself,"31/Jul/18 15:23;jrwest;I would like to add a dtest for this but wanted to push up the patch to get review started.

||trunk||3.0||
|[branch|https://github.com/jrwest/cassandra/tree/14468-trunk]|[branch|https://github.com/jrwest/cassandra/tree/14468-3.0]|
|[tests|https://circleci.com/gh/jrwest/cassandra/tree/14468-trunk]|[tests|https://circleci.com/gh/jrwest/cassandra/tree/14468-3.0]|","31/Jul/18 16:32;iamaleksey;+1, I'll commit shortly.","31/Jul/18 19:34;iamaleksey;Committed as [4b00601e831690e4ccf4ea95f70c09381d0ce49a|https://github.com/apache/cassandra/commit/4b00601e831690e4ccf4ea95f70c09381d0ce49a] to 3.0 and merged up. Thanks. A regression dtest would be nice, though not necessary. But whenever you have one, let me know and I'll commit pronto.",,,,,,,,,,,,,,,,,,,,,,,,,
dtests not determining C* version correctly,CASSANDRA-14420,13155436,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,beobal,beobal,26/Apr/18 15:20,12/Mar/19 14:19,13/Mar/19 22:35,09/May/18 18:37,,,,,,,,,0,dtest,,,"In the course of CASSANDRA-14134, the means of extracting the C* version under test before starting a cluster became broken. This is necessary in cases where we want to gate values in cassandra.yaml based on version, so a couple of tests are affected. The specifics are that the global {{CASSANDRA_VERSION_FROM_BUILD}} was hardcoded to '4.0' and the ways in which the various tests use it have meant that it was undetected until now.

Also, the {{fixture_since}} which we use to implement the {{@since}} annotation is broken when a {{--cassandra-version}} is supplied, rather than {{--cassandra-dir}}, meaning testing against released versions from git isn't working right now.

Tests directly affected:
 * {{auth_test.py}} - CASSANDRA-13985 added some gating of yaml props and additional checks on CQL results based on the build version. These failed on 3.11, which is how this issue was uncovered, but they're also broken on 2.2 on builds.apache.org
 * {{user_functions_test.py}} - gates setting a yaml property when version < 3.0. Failing on 2.2.
 * {{upgrade_tests}} - a number of these use the variable, but I don't think they're actually being run at the moment.
 * {{repair_tests/repair_test.py}}, {{replace_address_test.py}} & {{thrift_test}} all use the global, but only to verify that the version is not 3.9. As we're not running CI for that version, no-one noticed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-05-01 16:58:48.521,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed May 09 18:37:19 UTC 2018,,,,,,0|i3t293:,9223372036854775807,,,,,bdeggleston,bdeggleston,,,,,,,,,,,"27/Apr/18 17:22;beobal;I've pushed a branch [here|https://github.com/beobal/cassandra-dtest/commits/14420] which removes {{dtest.py::CASSANDRA_VERSION_FROM_BUILD}} and refactors {{auth_test::TestAuthRoles}}. dtests look good (i.e. back to how they were) against [2.2|https://circleci.com/workflow-run/ace01378-6551-424d-b1d4-b3358271b599], [3.11|https://circleci.com/workflow-run/894a2090-a78f-4763-aab9-b338273ddf6d] and [trunk|https://circleci.com/workflow-run/51cb13e7-1e38-4796-a950-4a3c95fd561c] (there's one failing dtest on trunk, but it's failed occasionally before so we already have CASSANDRA-14157 for it).

[~bdeggleston], seeing as you touched {{auth_test}} just recently, would you mind taking a look please?
","01/May/18 16:58;bdeggleston;I have a few notes:

* could we rename parse_dtest_config to dtest_config. While I realize the fixture function itself is doing the parsing, it seems a little strange to be passing the dtest config into the misc setup methods with that name
* you could probably set the parse_dtest_config fixture scope to something like module or session, so it's not reinstantiated for every test
* auth_test:TestAuthRoles.role has a mutable default argument which can lead to difficult to diagnose bugs. It's default should be None, then evaluated in the function body as {{options = options or {}}}
* I don't feel too strongly about this, but it looks like the parse_dtest_config argument is only used in a handful of fixture_dtest_setup_override implementations. Maybe it would be better to have a separate fixture setup for places where we need to consult the config? Otoh, passing the config into one of the main setup method seems like a reasonable thing to do. WDYT?
* There's a {{parse_dtest_config}} definition in user_functions_test:TestUserFunctions that's just behaving as a pass through. Is this left over from some debugging something, or is there a reason it's there? If it's doing something, could you add a comment explaining what?","03/May/18 07:34;beobal;Thanks [~bdeggleston], I've pushed an additional commit addressing your comments and re-run CI:
 * [2.2|https://circleci.com/gh/beobal/cassandra/199]
 * [3.0|https://circleci.com/gh/beobal/cassandra/197]
 * [3.11|https://circleci.com/gh/beobal/cassandra/198]
 * [trunk|https://circleci.com/gh/beobal/cassandra/200]","09/May/18 16:14;bdeggleston;+1, thanks","09/May/18 18:37;beobal;Thanks, committed to master in [a06c0e70|https://github.com/apache/cassandra-dtest/commit/a06c0e700f335cbdfcd683cadc358766d959aca0]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DelimiterAnalyzer: IllegalArgumentException: The key argument was zero-length,CASSANDRA-14450,13159991,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,michaelsembwever,michaelsembwever,michaelsembwever,17/May/18 11:03,12/Mar/19 14:19,13/Mar/19 22:35,18/May/18 04:29,3.11.3,4.0,,,Feature/SASI,,,,0,,,,"The [DelimiterAnalyzer|https://issues.apache.org/jira/browse/CASSANDRA-14247] can throw an IllegalArgumentException if there is no text between two delimiters. 

{noformat}
ERROR [MutationStage-1] 2018-05-17 13:55:09,734 StorageProxy.java:1417 - Failed to apply mutation locally : {}
java.lang.RuntimeException: The key argument was zero-length for ks: zipkin2, table: span
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1353) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:626) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:470) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:227) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:232) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:241) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.service.StorageProxy$8.runMayThrow(StorageProxy.java:1411) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2650) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_171]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171]
Caused by: java.lang.IllegalArgumentException: The key argument was zero-length
	at com.googlecode.concurrenttrees.radix.ConcurrentRadixTree.putInternal(ConcurrentRadixTree.java:520) ~[concurrent-trees-2.4.0.jar:na]
	at com.googlecode.concurrenttrees.radix.ConcurrentRadixTree.putIfAbsent(ConcurrentRadixTree.java:123) ~[concurrent-trees-2.4.0.jar:na]
	at org.apache.cassandra.index.sasi.memory.TrieMemIndex$ConcurrentPrefixTrie.putIfAbsent(TrieMemIndex.java:178) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.memory.TrieMemIndex$ConcurrentTrie.add(TrieMemIndex.java:123) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.memory.TrieMemIndex.add(TrieMemIndex.java:94) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.memory.IndexMemtable.index(IndexMemtable.java:65) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.conf.ColumnIndex.index(ColumnIndex.java:104) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.SASIIndex$1.insertRow(SASIIndex.java:258) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.SecondaryIndexManager$WriteTimeTransaction.onInserted(SecondaryIndexManager.java:915) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:333) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:295) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.utils.btree.BTree.buildInternal(BTree.java:139) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.utils.btree.BTree.build(BTree.java:121) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.utils.btree.BTree.update(BTree.java:178) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition.addAllWithSizeDelta(AtomicBTreePartition.java:156) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:282) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1335) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-05-17 23:04:32.88,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri May 18 04:29:23 UTC 2018,,,,,,0|i3ttsf:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"17/May/18 11:44;michaelsembwever;|| Branch || uTest || aTest || dTest ||
|[cassandra-3.11_14450|https://github.com/thelastpickle/cassandra/tree/mck/cassandra-3.11_14450]|[!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.11_14450.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Fcassandra-3.11_14450]| [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/33/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/33] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/565/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/565] |
|[trunk_14450|https://github.com/thelastpickle/cassandra/tree/mck/trunk_14450]|[!https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Ftrunk_14450.svg?style=svg!|https://circleci.com/gh/thelastpickle/cassandra/tree/mck%2Ftrunk_14450]| [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/34/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/34] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/568/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/568] |","17/May/18 23:01;michaelsembwever;[~mkjellman], have you got any time for a review? it's a simple fix and would be good to get corrected before the class comes out in any released version.","17/May/18 23:04;jasobrown;I can take look tomorrow, as I'm not sure about [~mkjellman]'s availability these days.","18/May/18 01:36;jasobrown;change looks fine to me, +1.

 ",18/May/18 02:38;michaelsembwever;Thanks. Committed as 7068ef62548c1ff8d17be7d0a1e71a5f098010e6,18/May/18 04:29;jasobrown;don't forget to update the status and the fix version ;),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"nodetool - Occasional high CPU on large, CPU capable machines",CASSANDRA-14475,13162534,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tsteinmaurer,tsteinmaurer,tsteinmaurer,29/May/18 09:09,12/Mar/19 14:19,13/Mar/19 22:35,29/May/18 12:22,3.11.3,4.0,,,Tool/nodetool,,,,0,,,,"Periodically calling nodetool every 5 min results in increased CPU usage by nodetool only on a machine with 32 physical cores (64 vCPUs) according to our monitoring:
!nodetool_highcpu_gcthreads1_cassandra_JIRA.png|width=600!

Investigation and testing has shown that it is related to running with default number of parallel GC threads which is 43 on this particular machine. We see a System.gc() according to flight recorder but no real evidence from where it comes from. The nodetool call in question is simply gathering e.g. the version with ""nodetool version"".

After explicitly setting the number of parallel GC threads to 1, the high CPU is entirely gone (see chart above), without impacting nodetool being executed successfully. 1 parallel GC thread should be sufficient for nodetool anyway I think.",,,,,,,,,,,,,,,,,,29/May/18 09:26;tsteinmaurer;nodetool_gc_threads.patch;https://issues.apache.org/jira/secure/attachment/12925521/nodetool_gc_threads.patch,29/May/18 09:19;tsteinmaurer;nodetool_highcpu_gcthreads1_cassandra_JIRA.png;https://issues.apache.org/jira/secure/attachment/12925520/nodetool_highcpu_gcthreads1_cassandra_JIRA.png,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2018-05-29 11:09:56.221,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue May 29 13:05:38 UTC 2018,,,,,,0|i3u91r:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,29/May/18 09:27;tsteinmaurer;Attached patch is based on Cassandra 3.11 and for Linux only.,"29/May/18 11:09;KurtG;+1 from me. I can't see any valid reason why you would want to have more than 1 GC thread for nodetool. All the work is done within Cassandra, so there shouldn't be any adverse effects to only having 1 GC thread for the tool (famous last words).","29/May/18 12:18;spodxx@gmail.com;This is probably not really nodetool specific, but an acknowledged JVM issue, fixed in Java 9.

https://bugs.java.com/bugdatabase/view_bug.do?bug_id=6858051","29/May/18 12:22;jasobrown;lgtm. committed as sha {{b8cbdde2b854229d950d7087ac1847f8453d2b1e}}. Thanks, [~tsteinmaurer]!","29/May/18 12:26;tsteinmaurer;[~jasobrown], thanks! Sorry for being a pain, but any chance to get this back-ported for 2.1+?","29/May/18 13:05;jasobrown;2.1 is critical fixes only, and 2.2 is close in that category. I debated if this patch should go into 3.0 or not, but then opted for stability in the 3.0 branch. Also, your patch as for 3.11, so I used that as my base barnch :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fqltool should open chronicle queue read only and a GC bug,CASSANDRA-14504,13164747,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,07/Jun/18 17:42,12/Mar/19 14:19,13/Mar/19 22:35,18/Jun/18 16:53,4.0,,,,Legacy/Tools,,,,0,fqltool,,,"There are two issues with fqltool.

The first is that it doesn't open the chronicle queue read only so it won't work if it doesn't have write permissions and it's not clear if it's safe to open the queue to write if the server is also still appending.

The next issue is that NativeBytesStore.toTemporaryDirectByteBuffer() returns a ByteBuffer that doesn't strongly reference the memory it refers to resulting it in sometimes being reclaimed and containing the wrong data when we go to read from it. At least that is the theory. Simple solution is to use toByteArray() and that seems to make it work consistently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-18 16:10:18.775,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 18 16:53:38 UTC 2018,,,,,,0|i3umon:,9223372036854775807,,,,,beobal,beobal,,,,,,,,,,,"07/Jun/18 18:31;aweisberg;[Fixes|https://github.com/apache/cassandra/compare/trunk...aweisberg:cassandra-14504-trunk?expand=1]
[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14504-trunk]",18/Jun/18 16:10;beobal;LGTM (minus the circle yaml change ofc),18/Jun/18 16:53;aweisberg;Commited as [c6570fac180b6f816efb47cbd9b7fe30c771835d|https://github.com/apache/cassandra/commit/c6570fac180b6f816efb47cbd9b7fe30c771835d]. Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncOneResponse uses the incorrect timeout,CASSANDRA-14509,13165050,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,djoshi3,djoshi3,djoshi3,08/Jun/18 23:37,12/Mar/19 14:19,13/Mar/19 22:35,11/Jun/18 15:57,4.0,,,,Legacy/Core,,,,0,,,,"{{AsyncOneResponse}} has a bug where it uses the initial timeout value instead of the adjustedTimeout. Combined with passing in the wrong {{TimeUnit}}, it leads to a shorter timeout than expected. This can have unintended consequences for example, in {{StorageService::sendReplicationNotification}} instead of waiting 10 seconds ({{request_timeout_in_ms}}), we wait for {{10000}} Nano Seconds.
",,,,,,,,,,,,,,,CASSANDRA-14514,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-11 15:57:55.769,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 11 15:57:55 UTC 2018,,,,,,0|i3uojz:,9223372036854775807,,,,,krummas,krummas,,,,,,,,,,,"08/Jun/18 23:42;djoshi3;||14509||
|[branch|https://github.com/dineshjoshi/cassandra/tree/trunk-14509]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/trunk-14509]|
||",09/Jun/18 00:53;djoshi3;[~krummas] I have updated the branch with a unit test.,"11/Jun/18 15:57;krummas;and committed as {{6da5fb56c8e0777843e88359a45a461a9f9eb639}} without the test.runners change - if that is a problem we should probably fix that in a separate ticket, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool import row cache invalidation races with adding sstables to tracker,CASSANDRA-14529,13166796,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jrwest,jrwest,jrwest,18/Jun/18 20:49,12/Mar/19 14:19,13/Mar/19 22:35,20/Jun/18 13:13,4.0,,,,,,,,0,,,,"CASSANDRA-6719 introduced {{nodetool import}} with row cache invalidation, which [occurs before adding new sstables to the tracker|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/SSTableImporter.java#L137-L178]. Stale reads will result after a read is interleaved with the read row's invalidation and adding the containing file to the tracker.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-20 13:13:21.299,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 20 13:13:21 UTC 2018,,,,,,0|i3uzlb:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"18/Jun/18 21:59;jrwest;Made the cache invalidation run after the files are added to the tracker. This is similar to [streaming|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/streaming/CassandraStreamReceiver.java#L207-L210]. There is still a race condition but the worst case is only invalidation of a cached copy of the newly added data. 

Branch: [https://github.com/jrwest/cassandra/commits/14529-trunk]
 Tests: [https://circleci.com/gh/jrwest/cassandra/tree/14529-trunk]",20/Jun/18 13:13;jasobrown;Committed as sha {{73e70340a173b8ff56665cddb70756e83f7d37b0}}. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Report why native_transport_port fails to bind,CASSANDRA-14544,13168464,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jroper,jroper,jroper,26/Jun/18 20:48,12/Mar/19 14:19,13/Mar/19 22:35,27/Jun/18 22:42,4.0,,,,,,,,0,,,,"On line 164 of {{org/apache/cassandra/transport/Server.java}}, the cause of a failure to bind to the server port is swallowed:

[https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/transport/Server.java#L163-L164]

{code:java}
        if (!bindFuture.awaitUninterruptibly().isSuccess())
            throw new IllegalStateException(String.format(""Failed to bind port %d on %s."", socket.getPort(), socket.getAddress().getHostAddress()));
{code}

So we're told that the bind failed, but we're left guessing as to why. The cause of the bind failure should be passed to the {{IllegalStateException}}, so that we can then proceed with debugging, like so:

{code:java}
        if (!bindFuture.awaitUninterruptibly().isSuccess())
            throw new IllegalStateException(String.format(""Failed to bind port %d on %s."", socket.getPort(), socket.getAddress().getHostAddress()),
                bindFuture.cause());
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-26 21:58:54.887,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 27 22:42:41 UTC 2018,,,,,,0|i3v9mv:,9223372036854775807,,,,,djoshi3,djoshi3,,,,,,,,,,,26/Jun/18 21:58;djoshi3;[~jroper] would you like to submit a patch for this?,"26/Jun/18 22:26;jroper;Patch is here:

https://github.com/apache/cassandra/compare/trunk...jroper:throw-cause

You can pull it by running:

{noformat}
git pull https://github.com/jroper/cassandra throw-cause
{noformat}","27/Jun/18 18:51;djoshi3;Thanks, [~jroper] - the tests are running: [https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/throw-cause]","27/Jun/18 22:00;djoshi3;So... after struggling with {{MessagingServiceTest}} failures on CircleCI, I was able to determine that the failures on CircleCI are unrelated to this patch. It seems CircleCI containers are not isolated and the failure is due to multiple tests attempting to listen on the same IP/Port combination simultaneously.

Anyway, I'm +1 on this patch. [~aweisberg] could you please help commit this patch?",27/Jun/18 22:42;aweisberg;Committed as [85ceec8855683b8bf71e009c8ed102ec91d85a41|https://github.com/apache/cassandra/commit/85ceec8855683b8bf71e009c8ed102ec91d85a41]. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtest: log-watching thread leak and thread starvation,CASSANDRA-14558,13170063,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,spodxx@gmail.com,spodxx@gmail.com,spodxx@gmail.com,04/Jul/18 12:29,12/Mar/19 14:19,13/Mar/19 22:35,09/Jul/18 07:33,,,,,Test/dtest,,,,0,dtest,,,"We get occasional build timeouts on b.a.o after pytest becomes unresponsive for over 20 minutes. This will result in a thread dump like this one:

[https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-2.2-dtest/117/consoleFull]

If you look for ""Log-watching thread starting"" messages and the dump, it becomes quite obvious whats the issue here.

I had a quick look at the python3 / pytest related changes in CASSANDRA-14134 and it seems that some of the handling around dtest_setup's {{log_watch_thread}} var has been changed in a way that would prevent eventually yielding the allocated thread.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-07 00:12:29.508,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 09 07:33:43 UTC 2018,,,,,,0|i3vjhj:,9223372036854775807,,,,,KurtG,KurtG,,,,,,,,,,,"06/Jul/18 10:23;spodxx@gmail.com;[~KurtG], can you elaborate a bit on why you think this isn't an actual issue, as you mentioned in #dev? Shouldn't we have ""Log-watching thread exiting"" messages in the log, if the behaviour isn't broken in a way as described in this ticket?
","07/Jul/18 00:12;KurtG;Turns out I was imagining a return statement when I glanced quickly yesterday. Patch is fine and fixes the described issue, but not sure why I'm still getting the same errors after applying the patch. I'll do a couple more runs and see if it's something environment related.",09/Jul/18 03:19;KurtG;Looks like it was something in my environment. Working perfectly now. +1 on patch.,"09/Jul/18 07:33;spodxx@gmail.com;Committed as c98469d86 , thanks for bringing it up and reviewing my patch, Kurt!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incomplete handling of exceptions when decoding incoming messages,CASSANDRA-14574,13172775,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasobrown,iamaleksey,iamaleksey,17/Jul/18 21:14,12/Mar/19 14:19,13/Mar/19 22:35,17/Aug/18 12:59,4.0,,,,Legacy/Streaming and Messaging,,,,0,,,,"{{MessageInHandler.decode()}} occasionally reads the payload incorrectly, passing the full message to {{MessageIn.read()}} instead of just the payload bytes.

You can see the stack trace in the logs from this [CI run|https://circleci.com/gh/iamaleksey/cassandra/437#tests/containers/38].

{code}
Caused by: java.lang.AssertionError: null
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:351)
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:371)
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:335)
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:158)
	at org.apache.cassandra.net.async.MessageInHandler.decode(MessageInHandler.java:132)
{code}

Reconstructed, truncated stream passed to {{MessageIn.read()}}:
{{0000000b000743414c5f42414301002a01e1a5c9b089fd11e8b517436ee1243007040000005d10fc50ec}}
You can clearly see parameters in there encoded before the payload:
{{[43414c5f424143 - CAL_BAC] [01 - ONE_BYTE] [002a - 42, payload size] 01 e1 a5 c9 b0 89 fd 11 e8 b5 17 43 6e e1 24 30 07 04 00 00 00 1d 10 fc 50 ec}}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-17 22:17:54.134,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 17 12:59:21 UTC 2018,,,,,,0|i3vzxj:,9223372036854775807,,,,,djoshi3,djoshi3,,,,,,,,,,,17/Jul/18 21:16;iamaleksey;[~jasobrown] ^,17/Jul/18 22:17;jasobrown;Probably a regression due to CASSANDRA-14485. Will investigate,"18/Jul/18 06:22;jasobrown;I can repro both on circleci and my laptop, running the dtest that failed for [~iamaleksey]: materialized_views_test.py::TestMaterializedViews::test_populate_mv_after_insert_wide_rows","18/Jul/18 11:53;iamaleksey;bq. Probably a regression due to CASSANDRA-14485. Will investigate

FWIW, I think I saw that same issue in CI before CASSANDRA-14485 got committed. So I think it's likelier that it wasn't caused by it than it was.","19/Jul/18 17:38;jasobrown;In short, I wasn't handling all error cases correctly. I was correctly handling the case where there is a single message contained in the {{ByteBuf}} that is fully deserialized, and then if some exception happens in the pipeline, we close the channel and everything is fine. However, if there are multiple messages in the buffer, or the buffer is not fully consumed when deserializing, this is where the problems are. In the catch block of {{MessageInHandler.decode()}}, I am calling {{exceptionHandled()}}, which closes the channel. However, as we derive from {{ByteToMessageDecoder}}, as it is responding to the channel close event, it will see there are unconsumed bytes in the buffer (called {{cumulator}} in the class), and (re-)invoke {{decode()}}. Unfortunately, if you are in a bad state and partway through the stream, you will fail to correctly deserialize any messages and it's downhill from there (you start looping over the same failure pattern: exception, call close channel, {{ByteToMessageDecoder}} calls {{decode()}}, repeat ...). The most safe thing to do here is pass the caught exception to {{ByteToMessageDecoder}}, and prevent any future processing in the {{decode()}} method.

The patch here resolves the error handling in the inbound pipeline (see below for details on the failing dtest):
||14574||
|[branch|https://github.com/jasobrown/cassandra/tree/14574]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14574]|

This patch does several things in the {{MessageInHandler.decode()}} method's exception block (which is where the problems lie):
 - explicitly throws the exception from the handler to the parent {{ByteToMessageDecoder}}, where it can properly break out of the while loop in {{callDecode()}}, and more properly send the exception to the {{exceptionHandled()}} method (which is overridden in {{BaseMessageInHandler}}) where we close the channel.
 - moves the {{ByteBuf}} 's readIndex to the end of the buffer, to make it appear as though the buffer has been fully 'consumed'. This optimizes (and helps with correctness of) {{ByteToMessageDecoder}}, because when the channel is closed, {{ByteToMessageDecoder.channelInputClosed()}} attempts, several times, to ensure all the bytes from the backing {{ByteBuf}} ({{cumulator}}) are consumed. Even though the state of the implementing handler is borked, the parent {{ByteToMessageDecoder}} will still keep trying to make sure all the bytes in {{cumulator}} are consumed before closing the channel. Thus, forcing the readIndex to the end of the buffer avoids that situation.
 - adds an explicit {{CLOSED}} state to the {{MessageInHandler}}, and the handler's state is set to {{CLOSED}} when a message fails to be deserialized or other error, for example: when the table doesn't exist (see below). While this is probably not completely necessary for correctness due to the other changes (primarily the one about moving the readIndex to the end of the buffer), it makes the state of the handler much more explicit, depends less on knowledge of the internal details of netty, and more resilient to implementation changes in the netty library itself.

 

After this fix, the {{materialized_views_test.py::TestMaterializedViews::test_populate_mv_after_insert_wide_rows}} dtest still fails, however, not with the same exception stack trace as reported. Instead, this one:
{noformat}
io.netty.handler.codec.DecoderException: org.apache.cassandra.exceptions.UnknownTableException: Couldn't find table with id 3694c0c0-8b6b-11e8-841f-cd3e85e9c250. If a table was just created, this is likely due to the schemanot being fully propagated.  Please wait for schema agreement on table creation.
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:459)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1342)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:934)
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:979)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:404)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:307)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.cassandra.exceptions.UnknownTableException: Couldn't find table with id 3694c0c0-8b6b-11e8-841f-cd3e85e9c250. If a table was just created, this is likely due to the schemanot being fully propagated.  Please wait for schema agreement on table creation.
	at org.apache.cassandra.schema.Schema.getExistingTableMetadata(Schema.java:438)
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:612)
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:353)
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:371)
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:335)
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:158)
	at org.apache.cassandra.net.async.MessageInHandler.decode(MessageInHandler.java:130)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428)
{noformat}
Thus the problem here appears to be request not finding the correct table in the schema. In my test local runs with the above patch applied, that table is id correct and eventually exists (for the MView), but not when the message comes in.

The reason why I had not seen this dtest failure in the past (and dtest runs were green), is because it was only exposed by the recent commit for CASSANDRA-13426. I bisected back to a few commits before CASSANDRA-14485 (started on sha {{2bad5d5b6d2134ecd3db63d02aa2274299d1d748}}), and it identified CASSANDRA-13426 as the commit that caused {{materialized_views_test.py::TestMaterializedViews::test_populate_mv_after_insert_wide_rows}} to start failing. My fix corrects the nastier part of that failure, but there's another issue that is outside the scope of the internode messaging.","19/Jul/18 17:51;jasobrown;UPDATE: looks like [~iamaleksey] had already seen the original dtest failure for that dtest, and [commented here|https://issues.apache.org/jira/browse/CASSANDRA-13426?focusedCommentId=16436169&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16436169]. Thus, I'm gonna ignore the dtest fail, as well.","19/Jul/18 19:04;djoshi3;Hi [~jasobrown] and [~iamaleksey], I don't wish to hijack this ticket but I strongly recommend marking this test as an expected failure. See my comment on [CASSANDRA-14571|https://issues.apache.org/jira/browse/CASSANDRA-14571].",19/Jul/18 19:40;iamaleksey;[~djoshi3] Nah. The patch for it is incoming (see [here|https://github.com/iamaleksey/cassandra/commits/14571-4.0]).,"19/Jul/18 20:06;djoshi3;Thanks, [~iamaleksey].","19/Jul/18 22:16;iamaleksey;[~jasobrown] Thanks for looking into it quickly and coming up with a patch. I'm wondering however if maybe this is a good time to improve handling of errors like this by skipping the remaining payload bytes, to leave the stream in a valid state, without closing down connections.",19/Jul/18 22:52;iamaleksey;Changed the title to reflect the actual issue versus my initial incorrect/incomplete diagnosis (nothing racy here).,"20/Jul/18 00:20;iamaleksey;No longer important, since the source is now known, but I got a different instance of what I think is the same bug, triggered [here|https://circleci.com/gh/iamaleksey/cassandra/468#tests/containers/26].","20/Jul/18 04:38;jasobrown;bq.  longer important, since the source is now known, but I got a different instance of what I think is the same bug, triggered here

Yup, looks like the broken behavior of constantly evaluating the buffer even though we're in a bad (incorrect) spot in the stream.

bq. I'm wondering however if maybe this is a good time to improve handling of errors like this by skipping the remaining payload bytes, to leave the stream in a valid state, without closing down connections.

I agree, especially after seeing that we drop the connection when we get a message for a table we don't know about yet. (I'll have to spelunk the git log to uncover the original logic for that one). That's one example, of course. However, I'd like to separate that reevaluation effort from resolving this issue. That way we can unblock this faster.",20/Jul/18 04:42;jasobrown;created CASSANDRA-14575 to explore when we can safely ignore a failed internode message,"20/Jul/18 10:41;iamaleksey;bq. I agree, especially after seeing that we drop the connection when we get a message for a table we don't know about yet. (I'll have to spelunk the git log to uncover the original logic for that one). 

That would be CASSANDRA-14168. As for when it's safe to ignore a failed to deser message - at least in the case of unknown table id it is, and that's a common enough scenario. Think someone creates a table and starts writes before waiting for schema to propagate. Or batchlog replays a mutation to a node on which a table is either not yet known, or has been dropped since. Or, occasionally, when we add new tables and use them during mixed mode/upgrade period. I'm pretty sure there is a JIRA somewhere, by Tyler, to address just this, but we never quite came around to it.","15/Aug/18 09:13;djoshi3;Hi [~jasobrown], overall the changes look good. I have a few changes that would eliminate some code duplication, adds annotation for methods exposed for testing. Other than the refactor, I have moved to using {{ByteBuf::skipBytes(int)}} instead of explicitly setting the readerIndex. Other decoders in the Netty code prefer that as well. Using {{skipBytes}} also goes through Netty's leak detection mechanism while setting the {{readerIndex}} doesn't seem to trigger it.

I have mocked up the changes in this branch - [https://github.com/apache/cassandra/compare/trunk...dineshjoshi:jasobrown-14574-trunk-review?expand=1]

I also think we should add a dtest which simulates a corruption in the byte stream possibly using Byteman.","15/Aug/18 15:21;jasobrown;[~djoshi3] Thanks for reviewing. I agree with the changes you've proposed (nice find on the {{ByteBuf.skipBytes()}}), and have cherry picked the two commits from your branch and rerun the tests:

||14574||
|[branch|https://github.com/jasobrown/cassandra/tree/14574]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14574]|
||

Working on a dtest now; not so straight-forward, but can be done.",16/Aug/18 13:41;jasobrown;dtest branch [added here|https://github.com/jasobrown/cassandra-dtest/tree/14574]. Waiting for tests to run.,16/Aug/18 22:09;djoshi3;The dtest looks good! I'm +1 on the patch.,"17/Aug/18 12:59;jasobrown;Committed to c* as sha {{298416a7445aa50874caebc779ca3094b32f3e31}}, committed to dtest as sha {{6e80b1846c308bb13d0b700263c89f10caa17d28}}. Thanks, all!",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set broadcast address in internode messagaing handshake,CASSANDRA-14579,13173550,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,aweisberg,jasobrown,jasobrown,20/Jul/18 21:25,12/Mar/19 14:19,13/Mar/19 22:35,20/Jul/18 22:21,4.0,,,,,,,,0,,,,"I am incorrectly setting the local address into the internode messaging handshake, rather than the broadcast address. This bug existed since CASSANDRA-8457, but had no effect until CASSANDRA-14485. Originally discovered by [~aweisberg].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-20 21:42:03.265,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 20 22:21:11 UTC 2018,,,,,,0|i3w4p3:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"20/Jul/18 21:28;jasobrown; One-liner fix here:

||14579||
|[branch|https://github.com/jasobrown/cassandra/tree/14579]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14579]|
||

",20/Jul/18 21:42;aweisberg;+1 This fix has been working for me and when I researched things this is what we used in prior versions for this message.,"20/Jul/18 22:21;jasobrown;committed as sha {{c4263d26b43a4a65a31f213a10f6fbd68217825c}}. Thanks, [~aweisberg]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix race condition in MV build/propagate when there is existing data in the base table,CASSANDRA-14571,13172697,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,17/Jul/18 15:44,12/Mar/19 14:19,13/Mar/19 22:35,20/Jul/18 08:55,4.0,,,,Feature/Materialized Views,,,,0,,,,"CASSANDRA-13426 exposed a race in MV initialisation and building, which now breaks, consistently, {{materialized_views_test.py::TestMaterializedViews::test_populate_mv_after_insert_wide_rows}}.

CASSANDRA-14168 is also directly related.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-19 19:04:33.538,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 20 08:55:11 UTC 2018,,,,,,0|i3vzg7:,9223372036854775807,,,,,beobal,beobal,,,,,,,,,,,17/Jul/18 21:15;iamaleksey;Also related - as in found while working on this ticket - CASSANDRA-14574.,"19/Jul/18 19:04;djoshi3;I recommend marking this test as an expected failure until this ticket is fixed. Having tests fail and then ignored causes confusion (See: CASSANDRA-14574, CASSANDRA-13426).
{code}
@xfail(""Should be addressed with CASSANDRA-14571"")
def test_populate_mv_after_insert_wide_rows(self):{code}
That way the test will still be run and still be included in the report but it wont fail the test suite. This is better than just ignoring dtest run failures.","20/Jul/18 00:19;iamaleksey;[~beobal] Pushed the branch [here|https://github.com/iamaleksey/cassandra/commits/14571-4.0].

Had CircleCI run all three jobs multiple times. Examples [here|https://circleci.com/gh/iamaleksey/cassandra/465], [here|https://circleci.com/gh/iamaleksey/cassandra/467], and [here|https://circleci.com/gh/iamaleksey/cassandra/462], and more where it came from.

It's a tiny patch, it makes a best-effort attempt at waiting for schema to converge. If it fails, it just logs. Hard-throwing caused some other MV-related tests to time out occasionally, sadly.

If you can review it (it's really small, I promise) or maybe even commit, with whatever changes you feel necessary, tomorrow - I'll super appreciate it (I'll be away, but trunk dtests never rest and will keep falling until this gets in).

Cheers (:","20/Jul/18 08:55;beobal;LGTM, committed to trunk in {{13150b001a8ddf82a77ac9525c446b7e9e32325c}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
some empty/invalid bounds aren't caught by SelectStatement,CASSANDRA-14849,13194237,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,25/Oct/18 20:10,12/Mar/19 14:19,13/Mar/19 22:35,13/Dec/18 22:23,4.0,,,,Legacy/CQL,Legacy/Local Write-Read Paths,,,0,,,,"Nonsensical clustering bounds like ""c >= 100 AND c < 100"" aren't converted to Slices.NONE like they should be. Although this seems to be completely benign, it is technically incorrect and complicates some testing since it can cause memtables and sstables to return different results for the same data for these bounds in some cases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-10-29 14:02:33.337,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 13 22:23:45 UTC 2018,,,,,,0|i3znf3:,9223372036854775807,,,,,iamaleksey,iamaleksey,,,,,,,,,,,"25/Oct/18 20:13;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/14849-trunk]
[circle|https://circleci.com/workflow-run/8f1492ae-d04e-4880-a93b-b9ff891d855d]","29/Oct/18 14:02;iamaleksey;I think the patch is correct, but I have a few issues with it.

1. We are inlining a copy of {{ClusteringComparator.compare()}}, ish, into {{Slice.isEmpty()}}. If the former changes somehow, there is a risk of forgetting to apply the difference to the inlined version.
 2. There is duplication of work. After making the regular {{compare()}} call, we are going through the motions again in the common case.
 3. There are different returns with some nesting involved that makes it a bit trickier to follow than necessary.

I *think* essentially we are just lacking a {{cmp == 0 && one of the bounds is exclusive}} condition, and the whole method can be simplified quite a bit (relatively). Pushed an illustration/review branch with those issues handled [here|https://github.com/iamaleksey/cassandra/commits/14849-review].","29/Oct/18 21:02;bdeggleston;Nice, that's much more succinct. Pushed up your changes, plus some expansion of the unit test.",30/Oct/18 15:01;iamaleksey;+1,"13/Dec/18 22:23;bdeggleston;Committed to trunk as [a41b861fa4d4acfbcce13dd62b1e8f48be22f8ed|https://github.com/apache/cassandra/commit/a41b861fa4d4acfbcce13dd62b1e8f48be22f8ed], thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rows that cross index block boundaries can cause incomplete reverse reads in some cases.,CASSANDRA-14803,13189264,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,03/Oct/18 18:34,12/Mar/19 14:19,13/Mar/19 22:35,04/Oct/18 20:08,3.0.x,3.11.x,,,Legacy/Local Write-Read Paths,,,,0,,,,"When we're reading 2.1 sstables in reverse, we skip the first row of an index block if it's split across index boundaries. The entire row will be read at the end of the next block. In some cases though, the only thing in this index block is the partial row, so we return an empty iterator. The empty iterator is then interpreted as the end of the row further down the call stack, so we return early without reading the rest of the data. This only affects 3.x during upgrades from 2.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-10-04 16:05:35.561,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 29 23:52:29 UTC 2019,,,,,,0|i3ysx3:,9223372036854775807,,,,,beobal,beobal,,,,,,,,,,,"03/Oct/18 23:01;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/14803-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14803-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/14803-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14803-3.11]|

The sstable used for the test was generated from [here|https://github.com/bdeggleston/cassandra/tree/14803-2.1]

Since this is testing a specific problem upgrading from 2.x-3.x, it didn't seem like LegacySSTableTest was the right place for this","04/Oct/18 16:05;iamaleksey;My apologies, was typing in what I thought was a different window, and JIRA apparently has keyboard shortcuts.","04/Oct/18 16:31;beobal;The fix and test itself LGTM, but I don't quite see what's differentiates this test from the others in {{LegacySSTableTest}}. What am I missing?

Also, it merges to 3.11 cleanly, but the test won't compile because {{DatabaseDescriptor::setDaemonInitialized}} has been replaced there.",04/Oct/18 17:27;bdeggleston;pushed up an update that moves the test into {{LegacySSTableTest}} and fixes compile problem,"04/Oct/18 18:07;beobal;Thanks, LGTM
 +1 assuming the 3.11 tests pass, which I'm sure they will.","04/Oct/18 20:08;bdeggleston;committed as [ab0e30e75904e4d637f07b2ec64334073eb061ec|https://github.com/apache/cassandra/commit/ab0e30e75904e4d637f07b2ec64334073eb061ec], thanks","29/Jan/19 23:52;jjirsa;[~bdeggleston] / [~beobal] - please set the fixver.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] read_repair_test.TestReadRepair,CASSANDRA-14603,13174884,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,jasobrown,jasobrown,26/Jul/18 15:05,12/Mar/19 14:19,13/Mar/19 22:35,30/Jul/18 08:43,2.2.13,3.0.17,,,Test/dtest,,,,0,dtest,,,"tests {{test_alter_rf_and_run_read_repair}} and {{test_read_repair_chance}} consistently fail on 3.0; the latter also fails on 2.2. I suspect it's the same cause, as the output from pytest shows the same error in the same shared function ({{check_data_on_each_replica}}):

{noformat}
            res = rows_to_list(session.execute(stmt))
            logger.debug(""Actual result: "" + str(res))
            expected = [[1, 1, 1]] if expect_fully_repaired or n == initial_replica else [[1, 1, None]]
            if res != expected:
>               raise NotRepairedException()
E               read_repair_test.NotRepairedException

read_repair_test.py:204: NotRepairedException
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-27 14:24:58.976,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 30 08:43:54 UTC 2018,,,,,,0|i3wcxb:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"27/Jul/18 14:24;beobal;The problem is with the tests - when I updated these for CASSANDRA-14134 I forgot that value skipping during reads was disabled until CASSANDRA-10657 re-introduced in 3.4

PR for the dtest fixes: https://github.com/apache/cassandra-dtest/pull/33
CI runs with the fix: [2.2|https://circleci.com/gh/beobal/cassandra/250], [3.0|https://circleci.com/gh/beobal/cassandra/252], [3.11|https://circleci.com/gh/beobal/cassandra/252]",28/Jul/18 16:14;jasobrown;+1,"30/Jul/18 08:43;beobal;Thanks, committed to cassandra-dtest in [daa37bf|https://github.com/apache/cassandra-dtest/commit/daa37bfb56fbcf627abc7179fd5924af08bac429]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DTEST] fix write_failures_test.py::TestWriteFailures::test_thrift,CASSANDRA-14583,13173740,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,23/Jul/18 08:02,12/Mar/19 14:19,13/Mar/19 22:35,25/Jul/18 05:56,,,,,,,,,0,dtest,,,"seems it needs a {{WITH COMPACT STORAGE}} to avoid failing like this:
{code}
write_failures_test.py::TestWriteFailures::test_thrift swapoff: Not superuser.
01:23:57,245 ccm DEBUG Log-watching thread starting.

INTERNALERROR> Traceback (most recent call last):
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/main.py"", line 178, in wrap_session
INTERNALERROR>     session.exitstatus = doit(config, session) or 0
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/main.py"", line 215, in _main
INTERNALERROR>     config.hook.pytest_runtestloop(session=session)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 617, in __call__
INTERNALERROR>     return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 222, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 216, in <lambda>
INTERNALERROR>     firstresult=hook.spec_opts.get('firstresult'),
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 201, in _multicall
INTERNALERROR>     return outcome.get_result()
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 76, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 180, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/main.py"", line 236, in pytest_runtestloop
INTERNALERROR>     item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 617, in __call__
INTERNALERROR>     return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 222, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 216, in <lambda>
INTERNALERROR>     firstresult=hook.spec_opts.get('firstresult'),
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 201, in _multicall
INTERNALERROR>     return outcome.get_result()
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 76, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 180, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/flaky/flaky_pytest_plugin.py"", line 81, in pytest_runtest_protocol
INTERNALERROR>     self.runner.pytest_runtest_protocol(item, nextitem)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/runner.py"", line 64, in pytest_runtest_protocol
INTERNALERROR>     runtestprotocol(item, nextitem=nextitem)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/runner.py"", line 79, in runtestprotocol
INTERNALERROR>     reports.append(call_and_report(item, ""call"", log))
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/flaky/flaky_pytest_plugin.py"", line 120, in call_and_report
INTERNALERROR>     report = hook.pytest_runtest_makereport(item=item, call=call)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 617, in __call__
INTERNALERROR>     return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 222, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/__init__.py"", line 216, in <lambda>
INTERNALERROR>     firstresult=hook.spec_opts.get('firstresult'),
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 196, in _multicall
INTERNALERROR>     gen.send(outcome)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/skipping.py"", line 123, in pytest_runtest_makereport
INTERNALERROR>     rep = outcome.get_result()
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 76, in get_result
INTERNALERROR>     raise ex[1].with_traceback(ex[2])
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/pluggy/callers.py"", line 180, in _multicall
INTERNALERROR>     res = hook_impl.function(*args)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/runner.py"", line 331, in pytest_runtest_makereport
INTERNALERROR>     longrepr = item.repr_failure(excinfo)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/python.py"", line 675, in repr_failure
INTERNALERROR>     return self._repr_failure_py(excinfo, style=style)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/python.py"", line 668, in _repr_failure_py
INTERNALERROR>     return super(FunctionMixin, self)._repr_failure_py(excinfo, style=style)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/nodes.py"", line 295, in _repr_failure_py
INTERNALERROR>     tbfilter=tbfilter,
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 476, in getrepr
INTERNALERROR>     return fmt.repr_excinfo(self)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 717, in repr_excinfo
INTERNALERROR>     reprtraceback = self.repr_traceback(excinfo)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 664, in repr_traceback
INTERNALERROR>     reprentry = self.repr_traceback_entry(entry, einfo)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 624, in repr_traceback_entry
INTERNALERROR>     s = self.get_source(source, line_index, excinfo, short=short)
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 568, in get_source
INTERNALERROR>     lines.extend(self.get_exconly(excinfo, indent=indent, markall=True))
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 575, in get_exconly
INTERNALERROR>     exlines = excinfo.exconly(tryshort=True).split(""\n"")
INTERNALERROR>   File ""/home/cassandra/cassandra/venv/lib/python3.6/site-packages/_pytest/_code/code.py"", line 426, in exconly
INTERNALERROR>     lines = format_exception_only(self.type, self.value)
INTERNALERROR>   File ""/usr/lib/python3.6/traceback.py"", line 136, in format_exception_only
INTERNALERROR>     return list(TracebackException(etype, value, None).format_exception_only())
INTERNALERROR>   File ""/usr/lib/python3.6/traceback.py"", line 462, in __init__
INTERNALERROR>     _seen.add(exc_value)
INTERNALERROR> TypeError: unhashable type: 'InvalidRequestException'
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-24 12:03:26.085,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 25 05:56:29 UTC 2018,,,,,,0|i3w5vb:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,24/Jul/18 12:03;jasobrown;[~krummas] What version of c* was this against?,"24/Jul/18 12:08;krummas;[~jasobrown] 3.0, but looks similar on 2.2 and 3.11: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-2.2-dtest/120/console https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-3.11-dtest/346/console","24/Jul/18 17:29;krummas;""clean"" test run: https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/594/
dtest patch: https://github.com/krummas/cassandra-dtest/commits/marcuse/14583",24/Jul/18 19:34;jasobrown;+1,"25/Jul/18 05:53;githubbot;GitHub user krummas opened a pull request:

    https://github.com/apache/cassandra-dtest/pull/32

    compact storage when testing thrift

    Patch by marcuse; reviewed by Jason Brown for CASSANDRA-14583

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/krummas/cassandra-dtest marcuse/14583

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra-dtest/pull/32.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #32
    
----
commit 194ad8a22315f0410155ab5eb2283d006a4fdd37
Author: Marcus Eriksson <marcuse@...>
Date:   2018-07-23T08:03:17Z

    compact storage when testing thrift
    
    Patch by marcuse; reviewed by Jason Brown for CASSANDRA-14583

----
","25/Jul/18 05:53;githubbot;Github user krummas closed the pull request at:

    https://github.com/apache/cassandra-dtest/pull/32
","25/Jul/18 05:56;krummas;and committed as {{194ad8a22315f0410155ab5eb2283d006a4fdd37}} to cassandra-dtest, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] test_functional - global_row_key_cache_test.TestGlobalRowKeyCache,CASSANDRA-14599,13174868,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,jasobrown,jasobrown,26/Jul/18 14:32,12/Mar/19 14:19,13/Mar/19 22:35,30/Jul/18 14:19,,,,,Test/dtest,,,,0,dtest,,,"dtest fails all the time on 3.0, but not other branches. Error from pytest output:

{code}
test teardown failure
Unexpected error found in node logs (see stdout for full details). Errors: [WARN  [main] 2018-07-23 18:53:10,075 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class), WARN  [main] 2018-07-23 18:53:56,966 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class), WARN  [main] 2018-07-23 18:55:54,508 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class), WARN  [main] 2018-07-23 18:56:42,688 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class), WARN  [main] 2018-07-23 18:53:10,075 Uns.java:169 - Failed to load Java8 implementation ohc-core-j8 : java.lang.NoSuchMethodException: org.caffinitas.ohc.linked.UnsExt8.<init>(java.lang.Class)]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-29 05:31:56.261,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 30 14:19:04 UTC 2018,,,,,,0|i3wctr:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"29/Jul/18 05:31;jay.zhuang;Seems like it's caused by CASSANDRA-12133:
{quote}[~snazy]:
 Mike, you can safely ignore this message (see OHC issue).
 It's fixed in OHC 0.4.4 (not sure why I forgot to submit a patch for that until now)..

Using this ticket to upgrade OHC - but not in 3.0.x as it's just an ""annoying"" message.
{quote}
 I actually think we should upgrade OHC for 3.0 branch too (from {{0.4.3}} to {{0.4.4}}). As a user, we also monitor the {{ERROR/WARN}} in our production environment. The {{WARN}} message is harmless but misleading.","30/Jul/18 05:44;krummas;patch to ignore the WARN message here: https://github.com/krummas/cassandra-dtest/commits/marcuse/14599

dtest run against 3.0: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14599

[~jay.zhuang] if we want to upgrade OHC we should probably do it in a separate ticket","30/Jul/18 11:58;jasobrown;+1 to [~krummas]'s patch.

 Also, I'm -1 on upgrading OHC so late into 3.0's lifecycle - unless, of course, there's some CVE or other security/stability problem","30/Jul/18 14:19;krummas;committed as {{32b53217da060343bcb8dcd272edf46df38d90b7}}, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reconcile should not be dependent on nowInSec,CASSANDRA-14592,13174399,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,25/Jul/18 16:04,12/Mar/19 14:19,13/Mar/19 22:35,23/Aug/18 13:16,4.0,,,,,,,,0,,,,"To have the arrival time of a mutation on a replica determine the reconciliation priority seems to provide for unintuitive database behaviour.  It seems we should formalise our reconciliation logic in a manner that does not depend on this, and modify our internal APIs to prevent this dependency.
 
Take the following example, where both writes have the same timestamp:
 
Write X with a value A, TTL of 1s
Write Y with a value B, no TTL
 
If X and Y arrive on replicas in < 1s, X and Y are both live, so record Y wins the reconciliation.  The value B appears in the database.
However, if X and Y arrive on replicas in > 1s, X is now (effectively) a tombstone.  This wins the reconciliation race, and NO value is the result.
 
Note that the weirdness of this is more pronounced than it might first appear.  If write X gets stuck in hints for a period on the coordinator to one replica, the value B appears in the database until the hint is replayed.  So now we’re in a very uncertain state - will hints get replayed or not?  If they do, the value B will disappear; if they don’t it won’t.  This is despite a QUORUM of replicas ACKing both writes, and a QUORUM of readers being engaged on read; the database still changes state to the user suddenly at some arbitrary future point in time.
 
It seems to me that a simple solution to this, is to permit TTL’d data to always win a reconciliation against non-TTL’d data (of same timestamp), so that we are consistent across TTLs being transformed into tombstones.
 
4.0 seems like a good opportunity to fix this behaviour, and mention in CHANGES.txt.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-08-21 17:00:26.559,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 23 23:39:03 UTC 2018,,,,,,0|i3w9xr:,9223372036854775807,,,,,iamaleksey,iamaleksey,,,,,,,,,,,"31/Jul/18 20:14;benedict;[patch|https://github.com/belliottsmith/cassandra/tree/CASSANDRA-14592], [Circle CI|https://circleci.com/workflow-run/8eed6e83-be3d-473c-9390-8e908e84bcfd]","21/Aug/18 15:29;benedict;Pushed an update that addresses (I think, it's been a while) Aleksey's offline review comments.

We collaborated to modify the reconcile semantics a little further, so that reconciliation is as consistent as possible.  Now the only situations that might arise with inconsistent reconciliation occur when one cell is expiring, another is a tombstone, and only at the point where both are logically a tombstone.  Specifically, we now prefer:

# The most recent timestamp
# If either are a tombstone or expiring
## If one is regular, select the tombstone or expiring
## If one is expiring, select the tombstone
## The most recent deletion time
# The highest value (by raw ByteBuffer comparison)",21/Aug/18 17:00;iamaleksey;+1,23/Aug/18 13:16;benedict;Thanks; [committed to trunk|https://github.com/apache/cassandra/commit/e225c88a65f2e8091f8ea6212c291416674882a1].,23/Aug/18 23:28;bdeggleston;Looks like this broke {{org.apache.cassandra.db.CellTest#testExpiringCellReconile}}. There was also a merge mixup with CASSANDRA-10726 which I ninja'd [here|https://github.com/apache/cassandra/commit/6e35cf340a4da8409482230b170a8b7546a6569b],23/Aug/18 23:39;benedict;Thanks.  I've also ninja'd the test [here|https://github.com/apache/cassandra/commit/f5adeeb8da15db0336db741bd39da46117fa9b73],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail incremental repair prepare phase if it encounters sstables from un-finalized sessions,CASSANDRA-14763,13185903,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,18/Sep/18 21:00,12/Mar/19 14:19,13/Mar/19 22:35,21/Sep/18 17:05,4.0,,,,Consistency/Repair,,,,0,,,,"Raised in CASSANDRA-14685. If we encounter sstables from other IR sessions during an IR prepare phase, we should fail the new session. If we don't, the expectation that all data received before a repair session is consistent when it completes wouldn't always be true.

In more detail: 
We don’t have a foolproof way of determining if a repair session has hung. To prevent hung repair sessions from locking up sstables indefinitely, incremental repair sessions will auto-fail after 24 hours. During this time, the sstables for this session will remain isolated from the rest of the data set. Afterwards, the sstables are moved back into the unrepaired set.
 
During the prepare phase of an incremental repair, we isolate the data to be repaired. However, we ignore other sstables marked pending repair for the same token range. I think the intention here was to prevent a hung repair from locking up incremental repairs for 24 hours without manual intervention. Assuming the session succeeds, it’s data will be moved to repaired. _However the data from a hung session will eventually be moved back to unrepaired._ This means that you can’t use the most recent successful incremental repair as the high water mark for fully repaired data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-19 08:25:39.67,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 21 17:05:32 UTC 2018,,,,,,0|i3y89j:,9223372036854775807,,,,,krummas,krummas,,,,,,,,,,,"18/Sep/18 21:47;bdeggleston;[trunk|https://github.com/bdeggleston/cassandra/tree/14763]
[circle|https://circleci.com/workflow-run/03a36dfd-74b9-40e7-9284-b079f931f991]","19/Sep/18 08:25;krummas;a few comments;

* The error message given by the failing nodetool could be a bit better: {{Repair job has failed with the error message: [2018-09-19 10:01:51,386] null}} maybe we could add that the user should have a look in the logs for further details
* a comment about isPending() on the commit on github

wrote a dtest making sure that we throw an exception if this happens: https://github.com/krummas/cassandra-dtest/commits/marcuse/14763

also looks like a few repair dtests needs fixing","19/Sep/18 17:32;bdeggleston;Pushed up fixes and started a new circle run.

Good catch with the race condition. I realized there’s another race where another pending anti-compaction could complete after we’d filtered our sstables and before we locked the sstables, potentially leaking data from one session to another which I fixed in AcquisitionCallback.","20/Sep/18 07:04;krummas;pushed a few fixes: https://github.com/krummas/cassandra/commits/blake/14763
* re-added the range intersection check
* Iterables.filter is ""lazy"" - we would have to iterate over the sstables returned to populate {{conflictingSessions}}

tests running here: https://circleci.com/gh/krummas/cassandra/tree/blake%2F14763",20/Sep/18 08:38;krummas;seems repair_tests.py test_dead_coordinator is a legit failure,"20/Sep/18 15:59;bdeggleston;Ok dead coordinator test needed to be updated to cancel the hung repair session, that's fixed (and pulled in your dtest changes) here: [https://github.com/bdeggleston/cassandra-dtest/tree/14763]

New run here: [https://circleci.com/workflow-run/9d4564d2-acbb-4c5a-b296-aa7977d66da4]

 

 ","20/Sep/18 17:59;krummas;LGTM, +1","21/Sep/18 17:05;bdeggleston;Thanks, committed to trunk as [167ebbcf4304512fa538e9cfc18da4295511d16c|https://github.com/apache/cassandra/commit/167ebbcf4304512fa538e9cfc18da4295511d16c]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building deb packages fails on trunk,CASSANDRA-14707,13183845,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mshuler,rustyrazorblade,rustyrazorblade,08/Sep/18 18:58,12/Mar/19 14:19,13/Mar/19 22:35,09/Sep/18 02:02,4.0,,,,Build,,,,0,lhf,,,"Looks like there were some changes to either {{conf/cassandra-env.sh}} and/or bin/cassandra that's screwing up the 002cassandra_logdir_fix patch.  I think it's the result of CASSANDRA-9608.  

Here's the error I get when doing the build:

{noformat}
build-cassandra_1  | applying patch 001cassandra_yaml_dirs to ./ ... ok.
build-cassandra_1  | applying patch 002cassandra_logdir_fix to ./ ... failed.
build-cassandra_1  | /usr/share/dpatch/dpatch.make:27: recipe for target 'patch-stamp' failed
build-cassandra_1  | make: *** [patch-stamp] Error 1
build-cassandra_1  | dpkg-buildpackage: error: debian/rules build gave error exit status 2
aws_cluster_build-cassandra_1 exited with code 0
{noformat}

Seems like an easy fix, should be able to apply the same edits to those 2 files and regenerate the patch.
",,,,,,,,,,,,,,,,,,08/Sep/18 21:02;mshuler;dpatch2quilt.diff.txt;https://issues.apache.org/jira/secure/attachment/12938969/dpatch2quilt.diff.txt,08/Sep/18 21:04;mshuler;dpatch2quilt_build-and-clean.log.txt;https://issues.apache.org/jira/secure/attachment/12938971/dpatch2quilt_build-and-clean.log.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2018-09-08 21:05:26.291,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 09 02:02:28 UTC 2018,,,,,,0|i3xvo7:,9223372036854775807,,,,,rustyrazorblade,rustyrazorblade,,,,,,,,,,,"08/Sep/18 21:05;mshuler; [^dpatch2quilt.diff.txt] patch attached for trunk to switch the Debian packaging patch tool from dpatch (long deprecated) to quilt, along with patch refresh.

 [^dpatch2quilt_build-and-clean.log.txt] log attached, testing out Debian package build and clean successfully.

Pushed branch to github, if that's easier: https://github.com/mshuler/cassandra/tree/dpatch2quilt","09/Sep/18 00:36;rustyrazorblade;Built the package without issue, installed on an ubuntu AWS instance, verified location of logs & data. 

+1, ship it.

",09/Sep/18 02:02;mshuler;Thanks for the check! Committed to trunk: [f424e03|https://github.com/apache/cassandra/commit/f424e03a445080b937605515210a061061c7906b].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible corruption in compressed files with uncompressed chunks,CASSANDRA-14892,13198559,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,blambov,blambov,blambov,15/Nov/18 09:02,12/Mar/19 14:19,13/Mar/19 22:35,29/Nov/18 14:29,4.0,,,,Legacy/Local Write-Read Paths,,,,0,,,,"When deciding to switch to writing a chunk uncompressed in a compressed file (see CASSANDRA-10520) and that chunk is smaller than the full chunk size (only currently happens in the last chunk of the file), it may very rarely happen that the chunk is:
- bigger than the compression limit when compressed
- smaller than the compression limit when left uncompressed

If this happens the writer will write it uncompressed, but the reader will treat it as compressed and fail when attempting to read it.

Such chunks should be padded with 0s to the minimum uncompressed size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-29 12:50:24.724,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 29 14:28:35 UTC 2018,,,,,,0|s00is0:,9223372036854775807,,,,,snazy,snazy,,,,,,,,,,,"29/Nov/18 09:29;blambov;As requested by reviewer off-line, added extra tests for writing and reading various input and compressed buffer sizes, as well as a unit test for {{ByteBufferUtil.writeZeroes}}.","29/Nov/18 12:50;snazy;Not much to say here.

Just +1 :)","29/Nov/18 14:28;blambov;Got a clean dtest run at DataStax, and testall results match trunk. Committed as [9a7db292cc4e470cd913f5c850982a7d7300d6c8|https://github.com/apache/cassandra/commit/9a7db292cc4e470cd913f5c850982a7d7300d6c8].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In mixed 3.x/4 version clusters write tracing and repair history information without new columns,CASSANDRA-14897,13198748,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,15/Nov/18 21:48,12/Mar/19 14:19,13/Mar/19 22:35,29/Nov/18 18:22,4.0,,,,Legacy/Distributed Metadata,,,,0,4.0-pre-rc-bugs,,,"In CASSANDRA-14841 I stopped it from writing to those tables so it wouldn't generate any errors. Aleksey pointed out I could write just the old columns. 

If a user manually adds the new columns to the old version nodes before upgrade they will be able to query this information across the cluster. This is a better situation then making it completely impossible for people to run repairs or perform tracing in mixed version clusters.

This would avoid breaking repair and tracing in mixed version clusters.

The missing columns can be added by following [these instructions|https://issues.apache.org/jira/browse/CASSANDRA-14841?focusedCommentId=16684959&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16684959]
For 3.0.15 and 3.11.1 and older versions:
{noformat}
cqlsh> ALTER TABLE system_distributed.repair_history ADD coordinator_port int;
cqlsh> ALTER TABLE system_distributed.repair_history ADD participants_v2 set<text>;
{noformat}
For 3.0.16 and 3.11.2 and newer:
{noformat}
cqlsh> INSERT INTO system_schema.columns (keyspace_name , table_name , column_name , clustering_order , column_name_bytes , kind , position , type ) VALUES ( 'system_distributed', 'repair_history', 'coordinator_port', 'none', 0x636f6f7264696e61746f725f706f7274, 'regular', -1, 'int');
cqlsh> INSERT INTO system_schema.columns (keyspace_name , table_name , column_name , clustering_order , column_name_bytes , kind , position , type ) VALUES ( 'system_distributed', 'repair_history', 'participants_v2', 'none', 0x7061727469636970616e74735f7632, 'regular', -1, 'set<text>');
cqlsh> exit
$ nodetool reloadlocalschema
{noformat}
Remember that the INSERT's and nodetool reloadschema must be done on the same node.

",,,,,,,,,,,,,,,,,,15/Nov/18 21:50;aweisberg;14897.diff;https://issues.apache.org/jira/secure/attachment/12948396/14897.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-11-29 14:01:36.735,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 29 18:18:34 UTC 2018,,,,,,0|s00jy0:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"28/Nov/18 23:46;aweisberg;[Trunk code|https://github.com/apache/cassandra/compare/trunk...aweisberg:14897-trunk?expand=1], [CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14897-trunk]",29/Nov/18 14:01;jasobrown;+1 lgtm,29/Nov/18 18:18;aweisberg;Committed as [1c8d0ad333c642405537150fed2cbb8623a8fe94|https://github.com/apache/cassandra/commit/1c8d0ad333c642405537150fed2cbb8623a8fe94]. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RangeTombstoneList doesn't properly clean up mergeable or superseded rts in some cases,CASSANDRA-14894,13198729,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,15/Nov/18 20:09,12/Mar/19 14:19,13/Mar/19 22:35,28/Nov/18 17:33,3.0.18,3.11.4,4.0,,Legacy/Local Write-Read Paths,,,,0,,,,"There are a few scenarios RangeTombstoneList doesn't handle correctly.

If there are 2 overlapping range tombstones with identical timestamps, they should be merged. Instead, they're stored as 2 rts with congruent bounds and identical timestamps.

If a range tombstone supersedes multiple sequential range tombstones, instead of removing them, they cause the superseding rt to be split into multiple rts with congruent bounds and identical timestamps.

When converted to an UnfilteredRowIterator, these become extra boundary markers with the same timestamp on each side. Logically these are noops, but they do cause digest mismatches which will cause unneeded read repairs, and repair overstreaming (since they're also included in flushed sstables).

Also, not sure if this is reachable in practice, but querying RTL with an empty slice that covers a range tombstone causes an rt to be returned with an empty slice. If reachable this might cause extra read repairs as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-28 14:46:36.494,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 28 17:33:43 UTC 2018,,,,,,0|s00jts:,9223372036854775807,,,,,beobal,beobal,,,,,,,,,,,"26/Nov/18 23:58;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/14894-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14894-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/14894-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14894-3.11]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/14894-trunk]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14894-trunk]|

I cheated and didn't actually fix RangeTombstoneList, but am just filtering them out in RowAndDeletionMergeIterator instead. Fixing RangeTombstoneList to correctly handle things was not at all trivial and would have risked introducing correctness bugs.",28/Nov/18 14:46;beobal;+1,"28/Nov/18 17:33;bdeggleston;thanks, committed as [f7630e4c3af3bbcf933f0708afaac7e3e7ef6101|https://github.com/apache/cassandra/commit/f7630e4c3af3bbcf933f0708afaac7e3e7ef6101]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Website documentation search function returns broken links,CASSANDRA-14971,13208446,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,Anthony Grasso,Anthony Grasso,Anthony Grasso,09/Jan/19 01:13,12/Mar/19 14:19,13/Mar/19 22:35,09/Jan/19 05:13,,,,,Documentation/Website,,,,0,,,,"The search bar on the main page of the [Cassandra Documentation|http://cassandra.apache.org/doc/latest/] returns search [results|http://cassandra.apache.org/doc/latest/search.html?q=cache&check_keywords=yes&area=default] with broken links.

When a link from a returned search is clicked, the site returns a 404 with the message similar to this:
{quote}The requested URL /doc/latest/tools/nodetool/nodetool.rst.html was not found on this server.
{quote}
From the error, it appears that the links are pointing to pages that end in *.rst.html* in their name. The links should point to pages that end in *.html*.",,,,,,,,,,,,,,,,,,09/Jan/19 01:40;Anthony Grasso;CASSANDRA-14971_v01.patch;https://issues.apache.org/jira/secure/attachment/12954251/CASSANDRA-14971_v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2019-01-09 04:42:43.039,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 09 06:17:55 UTC 2019,,,,,,0|u00nfk:,9223372036854775807,,,,,michaelsembwever,michaelsembwever,,,,,,,,,,,"09/Jan/19 01:28;Anthony Grasso;It looks like the search results are pieced together by the [searchtools.js|https://svn.apache.org/repos/asf/cassandra/site/src/js/searchtools.js] file that lives in the _js_ directory in the SVN [repository|https://svn.apache.org/repos/asf/cassandra/site]. Specifically the {{displayNextItem()}} function walks through the returned results and generates the HTML output. This function generates the filenames using the data in the returned results.

The search results are generated by the {{performObjectSearch}} and {{performTermsSearch}} functions. These functions obtain the file information from the search index. In this case, it is the search index file ([searchindex.js|https://svn.apache.org/repos/asf/cassandra/site/src/doc/4.0/searchindex.js] which is generated by Sphinx.

It appears that we are referencing the documents in the {{filenames}} list property of the search index. These documents contain the *.rst* extension. We should probably be referencing the documents in the {{docnames}} list property of the search index.",09/Jan/19 01:40;Anthony Grasso;Attached {{svn diff}} patch,"09/Jan/19 04:42;michaelsembwever;Solid write up, thanks [~Anthony Grasso].

Patch is +1 from me. Going to test it.",09/Jan/19 05:13;michaelsembwever;Committed as r1850821,09/Jan/19 06:17;Anthony Grasso;Awesome! Thanks [~michaelsembwever].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Anticompaction should throw exceptions on errors, not just log them",CASSANDRA-14936,13204474,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,14/Dec/18 13:06,12/Mar/19 14:19,13/Mar/19 22:35,19/Dec/18 17:52,4.0,,,,,,,,0,,,,"Anticompaction currently catches any exceptions and just logs them instead of rethrowing, this can cause us to overstream and leave sstables unrepaired.

This was made more likely to happen with CASSANDRA-14397 (before that anticompactions could not be stopped at all).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-12-17 23:25:21.783,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 19 17:52:33 UTC 2018,,,,,,0|s01iyw:,9223372036854775807,,,,,bdeggleston,bdeggleston,,,,,,,,,,,"14/Dec/18 13:51;krummas;patch: https://github.com/krummas/cassandra/commits/marcuse/14936
tests: https://circleci.com/workflow-run/bf69001c-756b-4c43-a754-652066dca07f

Note that this also makes anticompactions unstoppable again to make sure that starting a conflicting AC fails by timing out when trying to cancel compactions. CASSANDRA-14935 will improve this behaviour by failing immediately if any conflicting AC is active and make them stoppable again.",14/Dec/18 14:03;krummas;cancelling patch - we need to abort any non-finished transactions as well,"14/Dec/18 15:02;krummas;nope, missed that the transaction is closed [here|https://github.com/apache/cassandra/blob/a41b861fa4d4acfbcce13dd62b1e8f48be22f8ed/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L751]

pushed a byteman test which makes sure we don't have any compacting sstables after a failed AC","17/Dec/18 23:25;bdeggleston;This addresses this ticket fine, but it seems like there's some stuff related to CASSANDRA-14935 in here as well. Could you put that in a separate branch?","18/Dec/18 07:28;krummas;bq. there's some stuff related to CASSANDRA-14935 in here as well
Do you mean making anticompaction unstoppable? I added that to avoid cancelling ongoing anticompactions until we have CASSANDRA-14935 finished, it might be unnecessary since we should fix 14935 soon anyway, so I could remove those parts",18/Dec/18 15:50;bdeggleston;Ah ok I guess that part is complete. +1 then,"19/Dec/18 14:55;krummas;while fixing this up for commit I realised that we would be running the PendingAntiCompactionTests twice with the Byteman tests I added

tiny refactor: https://github.com/krummas/cassandra/commit/87c87a6a5554cfc82bc4fd10ea1d6dc5e67a20b2
https://circleci.com/workflow-run/7e383aaf-e96a-42d2-bbf9-866ced074df7

[~bdeggleston] could you just sanity check that before I commit?",19/Dec/18 16:14;bdeggleston;looks good to me,"19/Dec/18 17:52;krummas;and committed as {{0a79f9f5c970dcb8265814cd5dc361eb2d4bec6b}}, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests failure on trunk,CASSANDRA-15014,13214436,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,djoshi3,djoshi3,07/Feb/19 19:05,12/Mar/19 14:19,13/Mar/19 22:35,25/Feb/19 13:27,2.2.x,3.0.x,3.11.x,4.0,Test/unit,,,,0,,,,"Currently org.apache.cassandra.distributed.test.DistributedReadWritePathTest is failing on trunk with the following error -
{code:java}
[junit-timeout] Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
[junit-timeout] Testsuite: org.apache.cassandra.distributed.test.DistributedReadWritePathTest
[junit-timeout] Exception in thread ""main"" java.lang.OutOfMemoryError: Metaspace
[junit-timeout] Testsuite: org.apache.cassandra.distributed.test.DistributedReadWritePathTest
[junit-timeout] Testsuite: org.apache.cassandra.distributed.test.DistributedReadWritePathTest Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
[junit-timeout] 
[junit-timeout] Testcase: org.apache.cassandra.distributed.test.DistributedReadWritePathTest:readWithSchemaDisagreement: Caused an ERROR
[junit-timeout] Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
[junit-timeout] junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
[junit-timeout] at java.util.Vector.forEach(Vector.java:1275)
[junit-timeout] at java.util.Vector.forEach(Vector.java:1275)
[junit-timeout] at java.lang.Thread.run(Thread.java:748)
[junit-timeout] 
[junit-timeout] 
[junit-timeout] Test org.apache.cassandra.distributed.test.DistributedReadWritePathTest FAILED (crashed)
[junitreport] Processing /tmp/cassandra/build/test/TESTS-TestSuites.xml to /tmp/null1041131060
[junitreport] Loading stylesheet jar:file:/usr/share/ant/lib/ant-junit.jar!/org/apache/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl
[junitreport] Transform time: 277ms
[junitreport] Deleting: /tmp/null1041131060{code}
I have noticed sporadic failures in the org.apache.cassandra.distributed.test.* suite.",,,,,,,,,,,,,,,,,,11/Feb/19 13:04;ifesdjeen;Screen Shot 2019-02-11 at 12.30.19.png;https://issues.apache.org/jira/secure/attachment/12958243/Screen+Shot+2019-02-11+at+12.30.19.png,13/Feb/19 14:47;ifesdjeen;Screen Shot 2019-02-13 at 15.46.28.png;https://issues.apache.org/jira/secure/attachment/12958583/Screen+Shot+2019-02-13+at+15.46.28.png,13/Feb/19 02:12;jolynch;threads_stuck_waiting.png;https://issues.apache.org/jira/secure/attachment/12958491/threads_stuck_waiting.png,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2019-02-11 11:35:26.515,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 25 13:26:58 UTC 2019,,,,,,0|yi0rcw:,9223372036854775807,,,,,jolynch,jolynch,,,,,,,,,,,07/Feb/19 19:07;djoshi3;[~benedict] [~jolynch] [~ifesdjeen] could you please check whats going on?,"11/Feb/19 11:35;ifesdjeen;In summary, there was a problem with retention resulting from schema change: since it was executed on the main thread, we were retaining a bunch of thread locals. Attaching a screenshot showing that we retain mere 12mb after full test run. 

I did test it on several environments and it seems that circleci is still timing out because of GC pressure. I've decided to simplify runs and just have each test executing in its own JVM to avoid GC contention alltogether. 

I've also added an ant task to conveniently run distributed tests without test list and have improved a shutdown process. 

|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...ifesdjeen:CASSANDRA-15014-2.2]|[test run|https://circleci.com/workflow-run/8b1aae41-e279-405b-b5e5-12808179daa2]|
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...ifesdjeen:CASSANDRA-15014-3.0]|[test run|https://circleci.com/workflow-run/53ad7477-7d5e-4c23-8634-be4def9e95b9]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...ifesdjeen:CASSANDRA-15014-3.11]|[test run|https://circleci.com/workflow-run/e41d800f-e39d-4f1d-a2bd-99e61c8a0675]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15014-trunk]|[test run|https://circleci.com/workflow-run/2cfdb4b1-b52d-4108-b2cb-4899382ac880]|[without high capacity|https://circleci.com/workflow-run/2cfdb4b1-b52d-4108-b2cb-4899382ac880]|","11/Feb/19 23:46;jolynch;Heads up that non trunk links are missing the cassandra before the branch name (e.g. https://github.com/apache/cassandra/compare/2.2...ifesdjeen:CASSANDRA-15014-2.2 instead of https://github.com/apache/cassandra/compare/cassandra-2.2...ifesdjeen:CASSANDRA-15014-2.2). Working on review now.

","13/Feb/19 02:29;jolynch;[~ifesdjeen] I think that this is a pretty reasonable workaround, and from my testing it only appears about 2x slower then running them all in one JVM (My testing indicated about 1 min 10s vs 30s). I think long term we'll need to figure out how to run these either with a fork per method or with a test cluster per test class or something, but I recognize we're trying to mitigate the trunk unit test runs here and we can iterate on making them faster next as we need to.

Feedback:
 * If we go generated script direction can we wrap up the gen list -> chmod -> execute [logic|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15014-trunk#diff-1d37e48f9ceff6d8030570cd36286a61R194] into an ant target, the only drawback I see is that our stdout will have ""[exec]"" prepended. Maybe something like [this|https://github.com/apache/cassandra/commit/88da841585d4fb310bbac80b03601d74919fa507]
 * Is [test-distributed|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15014-trunk#diff-2cccd7bf48b7a9cc113ff564acd802a8R1890] dead code?
 * When I run `ant testclasslist -Dtest.classlistfile=/home/josephl/pg/cassandra/testlist.txt -Dtest.classlistprefix=distributed` to try to test if we need to split by method and I attach yourkit it either fails with a class not found exception or if I do attach I see a bunch of threads waiting on ""MigrationManager.announce"" forever (screenshot attached). Can you run the test like that or do you hang as well? I think this might indicate that the wait for logic in AbstractCluster.java may not be working right
 * Unused imports in [Instance.java|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15014-trunk#diff-7c02c337a482a2dc284c2e67bbb44dc1R28]
 * Calling System.runFinalization() is interesting, did you try doing that without the method separation and it didn't work?

For trunk:
 * Can you run it without the high capacity machines? Usually it's the unit tests that fail","13/Feb/19 14:50;ifesdjeen;[~jolynch] thank you for review

bq. ""MigrationManager.announce"" forever (screenshot attached)

I've just tried running tests myself and for me those threads stop, so I'm not 100% sure what's going on.

bq. Is test-distributed dead code?

It's just a more convenient runner, so that you wouldn't have to create a testiest file.

bq. Unused imports in Instance.java

Fixed.

bq. Calling System.runFinalization() is interesting, did you try doing that without the method separation and it didn't work?

I did try and it sometimes runs fine but isn't stable on non-high-capacity env. I did runs on both environments before pushing. Also your run seems to have passed.

bq. the only drawback I see is that our stdout will have ""[exec]"" prepended.

This is great, let's switch to what you propose.


","25/Feb/19 07:24;jolynch;Alright, finally figured out why attaching a debugger was causing the tests to crash, looks like the {{InstanceClassLoader}} switched from a blacklist to a whitelist where we only use the shared class loader for whitelisted classes, I just added a whitelist for ""com.yourkit"" and I was able to attach again.

Just a few comments, +1 after fixing first one.
 * I don't think right now if a jvm-dtest fails the build actually fails (I inserted an exception into one of the tests and {{ant test-jvm-dtest}} still passed). Perhaps we need {{#!/bin/bash}} and {{set -e}} at the top of the file written by {{TestLocator.java}} and {{failonerror}} set on the ant {{exec}} line.
 * Do you find that [setting|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15014-trunk#diff-2cccd7bf48b7a9cc113ff564acd802a8R1340] {{MetaspaceSize}} or {{MaxMetaspaceExpansion}} does anything? In my experience they just cause a metaspace OOM before the GC can collect classes. Let's leave them out if we don't find they help?
 * After you applied my patch now there is a test-jvm-dtest and a test-distributed target. They do different things so maybe we should have both, but should we unify the names e.g. ""test-jvm-dtest-forking"" and ""test-jvm-dtest"" maybe?","25/Feb/19 13:26;ifesdjeen;Thank you for the review! Fixed tests not showing up, example failure: [here|https://circleci.com/gh/ifesdjeen/cassandra/1366#tests/containers/2].

Committed to [2.2|https://github.com/apache/cassandra/commit/a7d8ba7b10a441f9710724e65a939a46add0ae78] and merged up to [3.0|https://github.com/apache/cassandra/commit/b27cc37abdef959c599440edc9fb85e0bc567249], [3.11|https://github.com/apache/cassandra/commit/7b462ec46753943281b3b4d4b106bcc4625bfae7] and [trunk|https://github.com/apache/cassandra/commit/5c4c75cac70dfb885a9905b7fc33a910f83ac989].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index summaries fail when their size gets > 2G and use more space than necessary,CASSANDRA-14649,13179350,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,blambov,blambov,blambov,16/Aug/18 13:13,12/Mar/19 14:19,13/Mar/19 22:35,21/Aug/18 08:58,,,,,,,,,0,,,,"After building a summary, {{IndexSummaryBuilder}} tries to trim the memory writers by calling {{SafeMemoryWriter.setCapacity(capacity())}}. Instead of trimming, this ends up allocating at least as much extra space and failing the {{Buffer.position()}} call when the size is greater than {{Integer.MAX_VALUE}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-08-16 14:09:47.536,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 21 08:58:39 UTC 2018,,,,,,0|i3x467:,9223372036854775807,3.0.x,3.11.x,4.x,,benedict,benedict,,,,2.2.0,,,,,,,16/Aug/18 13:24;blambov;Patch uploaded here: https://github.com/blambov/cassandra/tree/14649,"16/Aug/18 14:09;benedict;I think this patch is also broken.

Previously, the logical trim invoked {{setCapacity(length())}}, which I can see is buggy for a size > 2GiB (but is otherwise consistent).  Now, it seems to be invoking {{setCapacity(capacity())}}, which is surely a no-op?

It seems that there's a bunch of bugs here, and that really we should be:
# fix {{length}} to work for sizes > 2GiB
# implement {{trim}} as {{resizeTo(length())}}
# rename {{reallocate}} to something like {{ensureCapacity}}, to avoid this kind of misuse mistake in future

","16/Aug/18 14:45;blambov;Right, I messed it up too... I can't see any problems with {{length}}, though.

Updated patch to address the other comments and also added size checking to the test.","16/Aug/18 16:19;benedict;You're right, I was just assuming {{length()}} was inherited from {{DataOutputBuffer}}

I realise {{ensureCapacity}} was a terrible suggestion, but {{ensureHeadroom}} might be clearer than {{expandToFit}} since it might be that you're trying to fit N total, not N extra.

But no strong feeling, and +1 either way.","16/Aug/18 19:18;jjirsa;Safe to relate to CASSANDRA-12014 (haven't read the patch, just sounds familiar)? 
","20/Aug/18 14:36;blambov;CircleCI doesn't seem to like the new test but AFAICT is otherwise fine: [2.2|https://circleci.com/gh/blambov/workflows/cassandra/tree/14649-2.2] [3.0|https://circleci.com/gh/blambov/workflows/cassandra/tree/14649-3.0] [3.11|https://circleci.com/gh/blambov/workflows/cassandra/tree/14649-3.11] [trunk|https://circleci.com/gh/blambov/workflows/cassandra/tree/14649-trunk]

Should I remove the >2G test, or is there something I need to set up to be able to run tests needing more memory?","20/Aug/18 15:36;benedict;I think you need to set yourself up on a CircleCI account that supports larger instances, then modify the .circleci/config.yml

[For example|https://github.com/belliottsmith/cassandra/commit/b1cbd819274e3095f348402bca257ad4e6765f22]","21/Aug/18 08:27;blambov;I realized we still have a problem if the size grows by over 2G, i.e. if it becomes >4G and needs to grow. Pushed another commit to fix and test this and limit the test size if there isn't enough memory: [new commit|https://github.com/blambov/cassandra/commit/65798672eff79bd1c97b960ed965f0e908f6c23e] [branch|https://github.com/blambov/cassandra/tree/14649-trunk] [test|https://circleci.com/gh/blambov/workflows/cassandra/tree/14649-trunk]","21/Aug/18 08:50;benedict;+1

I do wonder if we should revisit requiring linear memory for all of this, but really we should probably instead revisit if such huge sstables are a good idea.",21/Aug/18 08:58;blambov;Committed as [49adbe7e0f0c8a83f3b843b65612528498b5c9a5|https://github.com/apache/cassandra/commit/49adbe7e0f0c8a83f3b843b65612528498b5c9a5].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
built_views entries are not removed after dropping keyspace,CASSANDRA-14646,13179260,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,16/Aug/18 06:03,12/Mar/19 14:19,13/Mar/19 22:35,16/Aug/18 17:08,4.0,,,,Feature/Materialized Views,Legacy/Distributed Metadata,,,0,,,,"If we restore view schema after dropping keyspace, view build won't be triggered because it was marked as SUCCESS in {{built_views}} table.

| patch | CI | 
| [trunk|https://github.com/jasonstack/cassandra/commits/mv_drop_ks] | [utest|https://circleci.com/gh/jasonstack/cassandra/739] |
| [dtest|https://github.com/apache/cassandra-dtest/pull/36]|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-08-16 17:08:00.147,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 18 05:19:44 UTC 2018,,,,,,0|i3x3m7:,9223372036854775807,,,,,pauloricardomg,pauloricardomg,,,,,,,,,,,"16/Aug/18 17:08;pauloricardomg;LGTM, commited dtest as {{e426ce1daa1d52983b8823388e568e5097254c0c}} to master and patch as {{d3e6891ec33a6e65cf383ad346c452293cfe50ec}} to trunk.

ftr, dtest looks good on private CI and also double checked locally.

Thanks!",18/Aug/18 05:19;jasonstack;Thanks for reviewing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't skip entire sstables when reading backwards with mixed clustering column order,CASSANDRA-14910,13200244,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,23/Nov/18 12:54,12/Mar/19 14:17,13/Mar/19 22:35,10/Dec/18 18:28,2.2.14,,,,Legacy/Local Write-Read Paths,,,,0,,,,"In 2.x, if a table has clustering columns in {{DESC}} order, any SSTable that doesn’t have any static rows in it will be skipped altogether when iterated in reverse.

This occurs due to the logic in {{ColumnSlice.compare()}} errorneusly sorting any empty {{ByteBuffer}} after non-empty values due to the way {{ReversedType}} operates. In case that empty {{ByteBuffer}} is coming from a static {{Composite}}, however, the logic breaks down. Static {{Composite}} components must *always* sort before any non-empty value, no matter the table’s comparator.

2.0, 2.1, and 2.2 are all affected. 3.0 isn’t, but only because we skip slice intersection logic entirely if static rows are present in the filter.

Introduced by CASSANDRA-8502.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-27 10:49:11.705,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 10 18:28:35 UTC 2018,,,,,,0|s00t28:,9223372036854775807,2.1.20,2.2.13,,,,ifesdjeen,,,,2.0.16,,,,,,,"23/Nov/18 13:00;iamaleksey;Code [here|https://github.com/iamaleksey/cassandra/commits/14910-2.2], CI [here|https://circleci.com/workflow-run/0c915d7b-e94a-4c47-b604-9f938e659b28].","26/Nov/18 14:23;iamaleksey;As spotted by [~ifesdjeen] during review, fixing comparison logic with static slices exposes another breakage - specifically the scenario when a static row is present in the sstable. With reversed comparator, when trying to read just the static row, such sstables would now be skipped.

Pushed the fix to the same branch.","27/Nov/18 10:49;ifesdjeen;The new version looks much better, the only thing is that current tests do not cover [this|https://github.com/apache/cassandra/compare/trunk...iamaleksey:14910-2.2#diff-82e58a7c5bec8818f2e88a982725690fR113] part (e.g. {{compare}} is now not called on static bounds since we're returning {{true}} before calling {{intersect}} which would invoke comparator), so while I'm not saying that the change is wrong, I'm not 100% sure if we should keep it unless we have a test or at least an example scenario where this one is required. Can you comment on where we need it?

UPDATE: I've ran some more tests and so far could only land into {{comparison != null}} clause but couldn't do it with static bounds.","27/Nov/18 21:06;iamaleksey;Force-pushed an updated version that just avoids making any decisions regarding skipping sstables based on static slice bounds - start or end. Can't rely on them, can't trust them, they are as dodgy as our recording of min/max cell names if static rows are involved.","03/Dec/18 18:08;ifesdjeen;Thank you for the patch, 

Latest version looks good! +1",10/Dec/18 18:28;iamaleksey;Committed to 2.2 as [afa4563864889c78569e29466047b411cd866b38|https://github.com/apache/cassandra/commit/afa4563864889c78569e29466047b411cd866b38] and merged (the unit tests) upwards. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming needs to synchronise access to LifecycleTransaction,CASSANDRA-14554,13169701,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,Stefania,djoshi3,djoshi3,03/Jul/18 06:00,12/Mar/19 14:17,13/Mar/19 22:35,10/Dec/18 15:33,3.0.18,3.11.4,4.0,,Legacy/Streaming and Messaging,,,,0,,,,"When LifecycleTransaction is used in a multi-threaded context, we encounter this exception -
{quote}java.util.ConcurrentModificationException: null
 at java.util.LinkedHashMap$LinkedHashIterator.nextNode(LinkedHashMap.java:719)
 at java.util.LinkedHashMap$LinkedKeyIterator.next(LinkedHashMap.java:742)
 at java.lang.Iterable.forEach(Iterable.java:74)
 at org.apache.cassandra.db.lifecycle.LogReplicaSet.maybeCreateReplica(LogReplicaSet.java:78)
 at org.apache.cassandra.db.lifecycle.LogFile.makeRecord(LogFile.java:320)
 at org.apache.cassandra.db.lifecycle.LogFile.add(LogFile.java:285)
 at org.apache.cassandra.db.lifecycle.LogTransaction.trackNew(LogTransaction.java:136)
 at org.apache.cassandra.db.lifecycle.LifecycleTransaction.trackNew(LifecycleTransaction.java:529)
{quote}
During streaming we create a reference to a {{LifeCycleTransaction}} and share it between threads -

[https://github.com/apache/cassandra/blob/5cc68a87359dd02412bdb70a52dfcd718d44a5ba/src/java/org/apache/cassandra/db/streaming/CassandraStreamReader.java#L156]

This is used in a multi-threaded context inside {{CassandraIncomingFile}} which is an {{IncomingStreamMessage}}. This is being deserialized in parallel.

{{LifecycleTransaction}} is not meant to be used in a multi-threaded context and this leads to streaming failures due to object sharing. On trunk, this object is shared across all threads that transfer sstables in parallel for the given {{TableId}} in a {{StreamSession}}. There are two options to solve this - make {{LifecycleTransaction}} and the associated objects thread safe, scope the transaction to a single {{CassandraIncomingFile}}. The consequences of the latter option is that if we experience streaming failure we may have redundant SSTables on disk. This is ok as compaction should clean this up. A third option is we synchronize access in the streaming infrastructure.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-07 06:33:31.478,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 10 15:08:38 UTC 2018,,,,,,0|i3vh93:,9223372036854775807,,,,,,benedict,snazy,,,,,,,,,,"07/Nov/18 06:33;Stefania;We had a related issue where one of our customers ended up with a corrupt txn log file during streaming, with an ADD record following an ABORT record. We couldn't look at the logs as they weren't available any longer, since the customer only noticed the problem when the node would not restart 22 days later. However, it's pretty obvious in my opinion that one thread aborted the streaming session whilst the receiving thread was adding a new sstable. So this seems the same root cause as reported in this ticket, which is that streaming is using the txn in a thread unsafe way. In my opinion, the problem exists since 3.0. However it becomes significanlty more likely with the Netty streaming refactoring. Our customer was on a branch based on 3.11.

We took a very conservative approach with the fix, in that we didn't want to fully synchronize abstract transactional and the lifecycle transaction on released branches. We could consider synchronizing these classes for 4.0 however, or reworking streaming.

Here are the 3.11 changes, if there is interest in this approach I can create patches for 3.0 and trunk as well:

[https://github.com/apache/cassandra/compare/cassandra-3.11...stef1927:db-2633-3.11]

We simply extracted a new interface, the [sstable tracker|https://github.com/apache/cassandra/compare/cassandra-3.11...stef1927:db-2633-3.11#diff-9d71c7ad9ad16368bd0429d3b34e2b21R15], which is also [implemented|https://github.com/apache/cassandra/compare/cassandra-3.11...stef1927:db-2633-3.11#diff-1a464da4a62ac4a734c725059cbc918bR144] by {{StreamReceiveTask}} by synchronizing the access to the txn, just like it does for all its other accesses to the txn. Whilst it's not ideal to have an additional interface, the change should be quite safe for released branches.","07/Nov/18 16:49;benedict;Hi [~Stefania]. Thanks for the patch!

I haven't reviewed it, but just skimming it, I wonder if you had considered (and potentially discarded) what might be a slightly simpler approach of allocating a separate {{LifecycleTransaction}} for each operation, and atomically transferring their contents as they ""complete"" to the shared {{LivecycleTransaction}}?

Semantically the behaviour remains the same as today, but we should need to minimally change existing code - just encapsulate the offending {{LifecycleTransaction}} to avoid future temptation for unsafe access, and introduce a {{transferTo}} method (or equivalent) to update the shared state.

What do you think?","08/Nov/18 02:47;Stefania;You're welcome [~benedict] !

bq.  I wonder if you had considered (and potentially discarded) what might be a slightly simpler approach of allocating a separate LifecycleTransaction for each operation, and atomically transferring their contents as they ""complete"" to the shared LivecycleTransaction?

No I hadn't considered it. It sounds elegant in principle but in order to atomically transfer child transactions to their parent, we'd have to add some complexity to transactions that I'm not sure we need. Obviously, the state of the parent transaction could change at any time (due to an abort), including whilst a child transaction is trying to transfer its state. So this would require some form of synchronization or CAS. The same is true for two child transactions transferring their state simultaneously. The state on disk should be fine as long as child transactions are never committed but only transferred. Child transaction should be allowed to abort independently though. So different rules for child and parent transactions would apply. 

I'm not sure we need this additional complexity because the txn state only changes rarely. {{LifecycleTransaction}} exposes a large API, but many methods are probably only used during compaction. Extracting a more comprehensive interface that can be implemented with a synchronized wrapper may be an easier approach.

I submitted a safe patch that fixes a known problem with streaming and that is safe for branches that will not undergo a major release testing cycle. Unfortunately, I do not have the time to work on a more comprehensive solution, at least not right now. I could however review whichever approach we choose.","08/Nov/18 17:00;benedict;I was actually thinking of something very simple.  Child transactions would not have any direct relationship to parents, there would just be a method to transfer their contents, and this method would be synchronised.  The other methods on a {{LifecycleTransaction}} could simply be marked synchronised as well.  I don't think there would be any major problem with this?  It's not a high-traffic object, so the cost would be low even without extracting a synchronised interface, particularly as this object requires regular fsyncs.

I completely understand that you may be too busy to try this alternative approach.  I think it would be _preferable_ for somebody to have a brief try at the alternative, just to see if we can isolate the complexity, but if we find we don't have time I think your patch looks good too (modulo a proper review).  

Perhaps we should wait and see how things pan out with finding time for review, as I know [~djoshi3] had been planning to take a crack at this too.","09/Nov/18 03:17;Stefania;bq.  The other methods on a LifecycleTransaction could simply be marked synchronised as well.

If we are synchronizing the LifecycleTransaction methods anyway, I'm not sure I understand why we need child transactions. Even in 4.0, where Netty threads call {{trackNew}}, I don't think we're adding sstables so frequently to introduce contention on a shared, synchronized txn. Considering {{trackNew}} performs a file sync as you correctly reminded me, surely this blocks Netty threads more than a synchronized {{trackNew}}. Maybe if many sstables are created concurrently during streaming, child transactions would make sense. I'm still not totally sure.

It's fine with me if we prefer to try a different alternative, the patch is available at any time. This code is not changing much so there is little risk of the patch getting stale. For info, internally [~snazy] already reviewed the patch.
","12/Nov/18 14:13;benedict;bq. If we are synchronizing the LifecycleTransaction methods anyway, I'm not sure I understand why we need child transactions.

The only reason would be simplifying analysis of the code's behaviour.  For instance, it's not clear to me how we either would (or should) behave in the stream writers actively working (and creating sstable files) but for whom the transaction has already been cancelled.  Does such a scenario even arise?  Is it possible it would leave partially written sstables?

A separate transaction is very easy to reason about, so we have only to consider what happens when we transfer ownership.

I agree that there is no sensible reason to worry about blocking behaviour specifically, and perhaps synchronising the transaction object is a simple first step we can follow-up later (we could even do it with a delegating SynchronizedLifecycleTransaction, which would seem to be equivalent to your patch, but with the changes isolated to a couple of classes, I think?)","13/Nov/18 02:03;Stefania;{quote}The only reason would be simplifying analysis of the code's behaviour. For instance, it's not clear to me how we either would (or should) behave in the stream writers actively working (and creating sstable files) but for whom the transaction has already been cancelled. Does such a scenario even arise? Is it possible it would leave partially written sstables?
{quote}
I'm not sure if this scenario may arise when a streaming transaction is aborted, it depends on streaming details which I've forgotten, but let's step through it:
 - The new sstables are recorded as new records before the files are created. If the recording fails, because the transaction was aborted, the streamer will abort with an exception. Fine.
 - So long as the sstables are recorded, the transaction tidier will delete the files on disk and so the contents will be removed from disk as soon as the streamer finishes writing. Also fine.
 - We may however have a race is if the streamer has added a new record to a txn that is about to be aborted, and the streamer hasn't created sstable files when the transaction tidier is running. This could leave files on disk. It's an extremely small window, but it's not impossible.

We keep a reference to the txn only for obsoleted readers of existing files, we should also keep a reference to the txn until all new files are at least created and the directory has been synced. Child transactions would solve this without the need for this extra reference, but we would need to enforce them for all multi-threaded code (the presence of synchronized methods may lure people on sharing transactions). The alternative to child transactions is to force writers to reference the txn.
{quote}we could even do it with a delegating SynchronizedLifecycleTransaction, which would seem to be equivalent to your patch
{quote}
This was exactly the starting point of my patch. I did not implement a fully synchronized transaction because the API is quite large. I thought it may need some cleanup in order to extract the methods related to the transaction behavior. I did not have the time to look into this, and also cleaning up the API is not an option on our released branches, due to the risk of introducing problems, so I extracted the three methods that are used by the writers and implemented the easiest and safest approach.","13/Nov/18 14:40;benedict;I think the situation would actually be, essentially, undefined behaviour?  Since even with this patch we've got a race for modifying the various collections between {{abort()}} and {{trackNew()}}. Even if this window is narrow, this is probably not acceptable for the fix?

We could perhaps create a synchronised transaction object that wraps the {{LifecycleTransaction}} and exposes largely the API you have exposed, but also synchronises its prepare/commit/abort phases? This would avoid the possibility of corrupting the internal state and producing undefined behaviour.

But we're still at least broken on Windows platforms (unless their FS semantics have changed), because we abort the shared transaction before we have closed our in-progress file handles, and so we cannot delete all of the sstables. Not sure what state that leaves us in, but hopefully it would be recovered on restart.

Ultimately, the child transaction approach still feels easier to reason about for me.  With the right comments, I don't see why we should worry about people misusing it for concurrent scenario - and we could only synchronise the internal methods (and only synchronise externally the transfer method, for clarity).

On a bikeshedding note, I'm very unconvinced by the name {{SSTableTracker}}. It's generic, and very close to {{Tracker}} which is a much more global thing. Perhaps {{LifecycleTransactionNewTracker}} or something, to make clear its scope? The variables wouldn't need to be renamed, also. I don't see us ever wanting to back this by something other than a {{LifecycleTransaction}}?

Probably we should have originally called {{LifecycleTransaction}} {{LifecycleTxn}}, or simply {{SSTableTxn}}","14/Nov/18 01:53;Stefania;Looking at the 3.11 code, {{StreamReceiveTask}} does synchronize all access to the txn: update/abort/finish. With the proposed patch the same lock is used for track/untrackNew.  So this patch takes care of the txn status at least up to 3.11. Looking briefly at the code on trunk, it seems the txn has been moved to {{CassandraStreamReceiver}}. I think the intention was still to synchronize access to it, but I haven't checked in detail whether that was achieved correctly.

Ultimately we also need to fix the problem of writers still not having created files when the txn is cleared, or still having files open on Windows.

If you are confident that child transactions are the best solution I'm not opposed but the same can be achieved with a synchronized transaction and a reference to the txn kept by the writers.
","14/Nov/18 08:45;benedict;{quote}If you are confident that child transactions are the best solution I'm not opposed but the same can be achieved with a synchronized transaction and a reference to the txn kept by the writers.
{quote}
Well, we've seen a few edge cases now where the synchronised version was difficult to reason about and left problematic behaviours, and it might be more brittle in future.

But if you're confident you can resolve the remaining issues through synchronisation (and we can establish if trunk is (effectively) synchronised, or should be), that has the advantage of already being (almost) done.","19/Nov/18 03:58;Stefania;For trunk, we should probably either introduce a synchronized transaction and add references in the writers, or look into child transactions.

The patch I proposed is suitable for released branches. I see this ticket is now classified for 4.0 only. In my opinion, there is an issue also for 3.0 and 3.11. The explanation is in my original [comment|https://issues.apache.org/jira/browse/CASSANDRA-14554?focusedCommentId=16677739&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16677739]. Perhaps we should create a different ticket for this problem and commit the patch only to 3.0 and 3.11?

I also would like to follow up with a more comprehensive approach for trunk but I don't know when I'll have time to work on this. I'll post another update if I do start work.

 ","19/Nov/18 05:41;djoshi3;Hi [~Stefania] thanks for submitting a patch. When I originally created this ticket, I had scoped it for 4.0. I did not investigate it for previous versions. It is fine to use this ticket to address previous versions as well.",20/Nov/18 02:25;Stefania;Thanks [~djoshi3] !,"05/Dec/18 21:56;benedict;[~krummas]: Could you take a look at [this line|https://github.com/apache/cassandra/compare/cassandra-3.11...stef1927:db-2633-3.11#diff-5d2d6808d387dcbd1937ee23496ac10aR95]?  It looks like it was introduced in CASSANDRA-6696, but I'm not sure why?

I'll be posting a 3.0, 3.11 and trunk patch tomorrow.

Stefania, are you comfortable with calling the new interface {{LifecycleNewTracker}}?  Or do you have another proposal?  I'd simply prefer a term that doesn't clash so strongly with {{Tracker}} which is a much more general concept.","06/Dec/18 02:01;Stefania;[~benedict], I'm fine with renaming the new interface {{LifecycleNewTracker}}, or with any other change that you see fit.","06/Dec/18 09:43;krummas;bq. Marcus Eriksson: Could you take a look at this line? It looks like it was introduced in CASSANDRA-6696, but I'm not sure why?
with 6696 we might create sstablewriters but not write anything to them - say you have 2 disks but flush a single partition, the empty writer is then .abort:ed, but the full transaction is not, so we make sure the files are deleted by untrack:ing them. What I can't remember is why {{SSTableWriter#abort}} is not removing the files though, that feels like it would be the correct solution to the problem...

edit: We do the same in [SSTableRewriter|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/SSTableRewriter.java#L311] when we encounter an empty writer","06/Dec/18 11:19;benedict;Thanks.  I was just a little confused as to why it was in {{abort}} - in {{SSTableRewriter}} we do this on commit / switchWriter.  I can see now that there's an invocation of {{SSTableMultiWriter.abortOrDie}}  (which invokes {{abort}}) in {{RangeAwareSSTableWriter}} on commit).  

Probably this should all be rejigged slightly as you say, moving both of these to {{SSTableWriter}}, perhaps with a parameter to {{abort}} to indicate you want to do this (because in the case of a full abort it's preferable to leave cleanup to the {{LogFile}}'s abort).","07/Dec/18 12:45;benedict;||3.0||3.11||trunk||
|[patch|https://github.com/belliottsmith/cassandra/tree/14554-3.0]|[patch|https://github.com/belliottsmith/cassandra/tree/14554-3.11]|[patch|https://github.com/belliottsmith/cassandra/tree/14554-trunk]|
|[CI|https://circleci.com/workflow-run/3e615cf7-a985-448b-9ba0-6d52f9b87eec]|[CI|https://circleci.com/workflow-run/a74915a5-ea21-49ed-ab0a-389964b606f4]|[CI|https://circleci.com/workflow-run/64915295-ab06-4946-b736-85b799765003]|

AFAICT the CI failures are related to CASSANDRA-14921.  There was a [brief single unit test failure|https://circleci.com/gh/belliottsmith/cassandra/1032#tests/containers/2] for one run, but probably environment or timing related.

This patch is simply a slightly modified and ported version of Stefania's patch above.  As discussed, this doesn't necessarily solve the problem perfectly, but it is close enough that it's worth applying now and worrying about that later.

[~bdeggleston]: I would appreciate it if you could take a quick look at [this|https://github.com/belliottsmith/cassandra/commit/fd54c420da81ee6ebfa1d29f45b8edc4922c8bdb#diff-374d64d7ac810fe7be021a2ef356c071R216], as it's not clear to me why there is a separate synchronised {{finishTransaction}} method.  Was there anticipated to be some kind of potential deadlock?
",07/Dec/18 17:49;bdeggleston;[~benedict] I'm afraid I don't have a great explanation other than that's just how it was in StreamReceiveTask before the refactor.,"07/Dec/18 18:00;benedict;Thanks.  I had a suspicion it might have simply been migrated, but I wasn't quite sure where form.  Looks like it goes all the way back to 2016; I don't think [~yukim] is around anymore?

I guess we'll just leave it be; I'm not terribly thrilled at the asymmetry between fully synchronising {{abort}} (and hence accesses to {{sstables}}), and only synchronising {{finishTransaction}} in {{finish}} - though this *should* be fine, the asymmetry suggests maybe there's something we're missing.  Anyway, it certainly maintains the prior behaviour.","08/Dec/18 07:04;djoshi3;[~benedict] thanks, I have assigned the ticket to you. I can review it and [~Stefania] please feel free to add yourself as a reviewer too.","08/Dec/18 11:09;benedict;The bulk of the work was Stefania's, so I'll leave myself reviewer.","10/Dec/18 01:44;Stefania;Thanks for the pull requests [~benedict], they LGTM. Do put both names as authors, since you've done quite a fair bit of work too.",10/Dec/18 15:08;benedict;Thanks [~Stefania].  I've committed to [84ffcb82a74667b957201f2cdae2d6b308956549|https://github.com/apache/cassandra/commit/84ffcb82a74667b957201f2cdae2d6b308956549] on 3.0 and merged upwards.,,,,,,,,,,,,,,,,,,,,,,,,
thrift_test.py failures on 3.0 and 3.x branches,CASSANDRA-14921,13202318,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,beobal,beobal,04/Dec/18 17:35,12/Mar/19 14:16,13/Mar/19 22:35,07/Dec/18 15:55,3.0.18,3.11.4,,,Legacy/Testing,,,,0,,,,"{{putget_test::TestPutGet::test_wide_slice}} fails on CircleCI since the docker image was updated for CASSANDRA-14713. The reason for this is that the {{fastbinary}} extension used by {{TBinaryProtocolAccelerated}} is not compatible with thrift 0.10.0 (according to [this bug report against Pycassa|https://github.com/pycassa/pycassa/issues/245]). The offending binary is present in the filesystem of the [current docker image|https://hub.docker.com/r/spod/cassandra-testing-ubuntu18-java11/], but wasn't in [the previous image |https://hub.docker.com/r/kjellman/cassandra-test/], which meant that thrift would fallback to the standard protocol implementation (silently).

As this is the only test which uses {{TBinaryProtocolAccelerated}} it's easy enough to switch it to {{TBinaryProtocol}}, which also fixes things. We might want consider removing the binary next time the image is updated though (cc [~spodxx@gmail.com]).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-12-06 20:12:46.856,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 07 15:55:57 UTC 2018,,,,,,0|s015rs:,9223372036854775807,,,,,,spodxx@gmail.com,,,,,,,,,,,"04/Dec/18 17:40;beobal;dtest branch: [https://github.com/beobal/cassandra-dtest/commits/fix_putget_test]

3.0 CI run using updated dtest: [https://circleci.com/workflow-run/cc252860-46a5-42b5-bf7a-fc8292bbbbac]

 ","06/Dec/18 20:12;spodxx@gmail.com;Can't we just downgrade to 0.9.3 in dtest's requirements.txt? If there's a compatibility issue with our tests and 0.10, shouldn't we either use a different thrift version, or fix the issue on our side (which your patch does)? ","07/Dec/18 10:03;beobal;bq. Can't we just downgrade to 0.9.3 in dtest's requirements.txt?

I seem to recall that there is some other incompatibility between thrift 0.9 and python3, which was why the dependency got bumped in CASSANDRA-14134. I didn't make that change though, so I'll have to check. 

Fixing the issue on our side is fine with me, hence the patch. I just wanted to note the presence of the problematic binary so we can maybe remove next time we have to do some maintenance on the docker image.
 

 ","07/Dec/18 10:05;beobal;bq. yes, pycassa isn't python 3 compatible and we needed new thrift bindings to get python 3 support for the remianing thrift tests we still have. Jeff Jirsa did this work.

from: [this comment | https://issues.apache.org/jira/browse/CASSANDRA-14134?focusedCommentId=16314023&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16314023] on CASSANDRA-14134. ","07/Dec/18 11:47;spodxx@gmail.com;+1 on the patch

 

Do you have the path of the binary that should be removed?","07/Dec/18 15:55;beobal;Thanks, committed as {{3d069f713ef2bd61a32863b29a6f160b74ecd89c}}

The binary that causes the issue is: {{/home/cassandra/env/lib/python3.6/site-packages/thrift/protocol/fastbinary.cpython-36m-x86_64-linux-gnu.so}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Message Flusher scheduling fell off the event loop, resulting in out of memory",CASSANDRA-14855,13194748,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sumanth.pasupuleti,sumanth.pasupuleti,sumanth.pasupuleti,29/Oct/18 05:49,12/Mar/19 14:16,13/Mar/19 22:35,06/Dec/18 15:59,3.0.18,,,,Messaging/Client,,,,0,pull-request-available,,,"We recently had a production issue where about 10 nodes in a 96 node cluster ran out of heap. 

From heap dump analysis, I believe there is enough evidence to indicate `queued` data member of the Flusher got too big, resulting in out of memory.
Below are specifics on what we found from the heap dump (relevant screenshots attached):
* non-empty ""queued"" data member of Flusher having retaining heap of 0.5GB, and multiple such instances.
* ""running"" data member of Flusher having ""true"" value
* Size of scheduledTasks on the eventloop was 0.

We suspect something (maybe an exception) caused the Flusher running state to continue to be true, but was not able to schedule itself with the event loop.
Could not find any ERROR in the system.log, except for following INFO logs around the incident time.


{code:java}
INFO [epollEventLoopGroup-2-4] 2018-xx-xx xx:xx:xx,592 Message.java:619 - Unexpected exception during request; channel = [id: 0x8d288811, L:/xxx.xx.xxx.xxx:7104 - R:/xxx.xx.x.xx:18886]
io.netty.channel.unix.Errors$NativeIoException: readAddress() failed: Connection timed out
 at io.netty.channel.unix.Errors.newIOException(Errors.java:117) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.unix.Errors.ioResult(Errors.java:138) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.unix.FileDescriptor.readAddress(FileDescriptor.java:175) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.AbstractEpollChannel.doReadBytes(AbstractEpollChannel.java:238) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:926) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:397) [netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:302) [netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) [netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) [netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}

I would like to pursue the following proposals to fix this issue:
# ImmediateFlusher: Backport trunk's ImmediateFlusher ( [CASSANDRA-13651|https://issues.apache.org/jira/browse/CASSANDRA-13651] https://github.com/apache/cassandra/commit/96ef514917e5a4829dbe864104dbc08a7d0e0cec)  to 3.0.x and maybe to other versions as well, since ImmediateFlusher seems to be more robust than the existing Flusher as it does not depend on any running state/scheduling.
# Make ""queued"" data member of the Flusher bounded to avoid any potential of causing out of memory due to otherwise unbounded nature.


",,"Github user sumanth-pasupuleti commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/293#discussion_r235090149
  
    --- Diff: conf/cassandra.yaml ---
    @@ -1003,3 +1003,9 @@ windows_timer_interval: 1
     # An interval of 0 disables any wait time, which is the behavior of former Cassandra versions.
     #
     # otc_backlog_expiration_interval_ms: 200
    +
    +# Define use of immediate flusher for replies to TCP connections. This is an alternate simplified flusher that does not
    +# depend on any event loop scheduling. Details around why this has been backported from trunk: CASSANDRA-14855.
    +# Default is false.
    +# native_transport_use_immediate_flusher: false
    --- End diff --
    
    other more descriptive name I had in mind was ""native_transport_use_immediate_flusher_in_place_of_batches_flusher"", but felt it could be too long a name
;20/Nov/18 17:05;githubbot;600","Github user belliottsmith commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/293#discussion_r235729649
  
    --- Diff: conf/cassandra.yaml ---
    @@ -1003,3 +1003,9 @@ windows_timer_interval: 1
     # An interval of 0 disables any wait time, which is the behavior of former Cassandra versions.
     #
     # otc_backlog_expiration_interval_ms: 200
    +
    +# Define use of immediate flusher for replies to TCP connections. This is an alternate simplified flusher that does not
    +# depend on any event loop scheduling. Details around why this has been backported from trunk: CASSANDRA-14855.
    +# Default is false.
    +# native_transport_use_immediate_flusher: false
    --- End diff --
    
    perhaps native_transport_flush_messages_immediately ?
;22/Nov/18 13:46;githubbot;600",,0,1200,,,0,1200,,,,,,,,29/Oct/18 05:48;sumanth.pasupuleti;blocked_thread_pool.png;https://issues.apache.org/jira/secure/attachment/12945991/blocked_thread_pool.png,29/Oct/18 05:48;sumanth.pasupuleti;cpu.png;https://issues.apache.org/jira/secure/attachment/12945990/cpu.png,29/Oct/18 05:48;sumanth.pasupuleti;eventloop_scheduledtasks.png;https://issues.apache.org/jira/secure/attachment/12945989/eventloop_scheduledtasks.png,29/Oct/18 05:48;sumanth.pasupuleti;flusher running state.png;https://issues.apache.org/jira/secure/attachment/12945988/flusher+running+state.png,29/Oct/18 05:48;sumanth.pasupuleti;heap.png;https://issues.apache.org/jira/secure/attachment/12945986/heap.png,29/Oct/18 05:48;sumanth.pasupuleti;heap_dump.png;https://issues.apache.org/jira/secure/attachment/12945987/heap_dump.png,29/Oct/18 05:48;sumanth.pasupuleti;read_latency.png;https://issues.apache.org/jira/secure/attachment/12945985/read_latency.png,,,,,7.0,,,,,,,,,,,,,,,,,,,2018-11-09 16:46:40.154,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 06 16:46:41 UTC 2018,,,,,,0|i3zqkf:,9223372036854775807,,,,,,benedict,,,,,,,,,,,"01/Nov/18 05:35;sumanth.pasupuleti;Backport of ImmediateFlusher to 3.0 https://github.com/sumanth-pasupuleti/cassandra/tree/3.0_backport_immediate_flusher

Passing UTs: https://circleci.com/gh/sumanth-pasupuleti/cassandra/139",09/Nov/18 16:46;zznate;[~sumanth.pasupuleti] Thanks for the detailed analysis and patch. Was this the first time your team has seen this or has it potentially manifested in other clusters previously?,"09/Nov/18 21:26;sumanth.pasupuleti;[~zznate] Yes, this was the first time we saw this (and there was a second incident that followed with similar characteristics on the same cluster). This happened only on one cluster (this is our most read heavy 3.0 CQL cluster), and following are the characteristics:
* 3.0.17 C* version
* CQL
* Relatively high read traffic (~60k rps at peak at coordinator level)
* Has client side wire compression (LZ4) enabled
* Total outbound traffic of ~4Gbps across the cluster","09/Nov/18 23:36;benedict;Thanks [~sumanth.pasupuleti] for the diagnosis and patch.

I haven't as yet reviewed it, but I think we need to consider whether changing the default is acceptable.  We have to weigh the chance of this bug versus the chance of detrimental performance impact to any existing users.  Unfortunately we cannot easily put a number to either risk (at least, not without significant effort).

In an ideal world, we would modify the existing flusher to avoid accumulating messages (introducing a bound on the number of waiting messages, or their size), but that might be overkill when we anticipate retiring the old flusher.  It shouldn't be too onerous, however.

I would personally find any of the options somewhat acceptable, including introducing this change, or doing so without changing the default property.  This would leave users exposed to this (presumably uncommon) bug, but with an easy option for fixing it by switching the config property.

It's not entirely clear to me which is the superior option, so input from anyone else would be welcome.","19/Nov/18 23:56;sumanth.pasupuleti;Appreciate your thoughts [~benedict] . Trying to figure out a way forward since there have not been inputs from anyone else.

I also like the suggestion of keeping the existing flusher ON by default, and making ImmediateFlusher usage optional (through yaml property like native_transport_flush_immediate which is set to false by default) - I can work on a patch for that. Let me know.","20/Nov/18 10:53;benedict;That seems like a reasonable path forward to me, yes.  I'll review the patch once you have it prepared.

I'd propose using a more descriptive name for the property, though, that's prefixed by 'native_transport'","20/Nov/18 17:10;sumanth.pasupuleti;[~benedict] Here is the updated patch for review - [https://github.com/apache/cassandra/pull/293/files], Passing UTs: [https://circleci.com/gh/sumanth-pasupuleti/cassandra/185]","06/Dec/18 15:59;benedict;Committed as [fff6eec2903ee85f648535dd051c9bc72631f524|https://github.com/apache/cassandra/commit/fff6eec2903ee85f648535dd051c9bc72631f524] to 3.0 and merged upwards.

[~sumanth.pasupuleti]: On reflection, the parameter names should ideally remain the same for both 3.0 and 3.11, so I have reverted to the names in Michael's original patch.  I have simply changed the default value to {{true}} until trunk.

I also removed the text from the cassandra.yaml; not all options are listed in the yaml explicitly, and it's probably not worth including this until it's truly legacy (i.e. in 4.0).  Anybody can fill in the parameter without the prompt if they need it.

Let me know if you disagree with any of this, but wanted to not delay any further the commit.","06/Dec/18 16:46;sumanth.pasupuleti;I agree with you [~benedict]. Thanks for committing. I was going to update this JIRA with yet another finding we had in production a few days ago (waiting on collecting relevant screenshots of heap dumps, etc), on the same cluster, where CPU got hogged by ""something"", and messages were not getting flushed from ImmediateFlusher either (my theory is lack of CPU) resulting in OOM. As you were suggesting earlier in this JIRA, will look into adding a bound to the queue of items waiting to be flushed - this would apply to trunk as well, so will spin off a different JIRA for that.
In parallel, looking into automating taking of flamegraphs during such incident, so that we would have leads into what was actually hogging the CPU.

Let me know if you have any thoughts/suggestions around this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catch CorruptSSTableExceptions and FSErrors in ALAExecutorService,CASSANDRA-14993,13210966,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,spodxx@gmail.com,spodxx@gmail.com,spodxx@gmail.com,22/Jan/19 12:22,12/Mar/19 14:16,13/Mar/19 22:35,04/Feb/19 15:48,3.0.19,3.11.5,4.0,,,,,,0,,,,"Actively handling CorruptSSTableExceptions and FSErrors currently only happens during opening of sstables and in the default exception handler. What's missing is to catch these in AbstractLocalAwareExecutorService as well. Therefor I propose to add calls to FileUtils.handleCorruptSSTable/handleFSError there, too, so we don't miss invoking the disk failure policy in that case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-01-23 20:44:41.685,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 04 15:48:24 UTC 2019,,,,,,0|yi0614:,9223372036854775807,,,,,aweisberg,aweisberg,,,,,,,,,,,"23/Jan/19 20:44;aweisberg;Do we ever want to skip JVMStability inspector? It checks all the causes so while the top level might be FSError it may contain something else.

Should we be inspecting nested exceptions for FSError?","24/Jan/19 13:30;spodxx@gmail.com;inspectThrowable is called in any case, either in DefaultFSErrorHandler.handleCorruptSSTable/handleFSError(), or in the else statement.

Should we inspect nested exceptions for FSError? Not 100% sure, but I'd probably first start fixing the logging statement, so we get proper stack traces and see what we get.","31/Jan/19 17:00;aweisberg;+1

I just noticed the switch from WARN to ERROR. Seems appropriate since we have bubbled up to such a high level exception handler that if it wasn't an ERROR it would have been handled already.","04/Feb/19 15:48;spodxx@gmail.com;Thanks, Ariel!

Merged as c94a6aa7e5dd6d to cassandra-3.0

Tests CircleCI:
[3.0|https://circleci.com/workflow-run/f8154177-162a-416e-ad79-4c3c86f66906]
[3.11|https://circleci.com/workflow-run/eaaf37f8-ea9e-4a0a-b138-c26855e44309]
[trunk|https://circleci.com/workflow-run/76cab84e-33f8-45b8-ba4a-2d5ba8fa37a3]

 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid leaking threads when anticompaction fails,CASSANDRA-15002,13212868,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,30/Jan/19 17:49,12/Mar/19 14:16,13/Mar/19 22:35,31/Jan/19 08:16,4.0,,,,Consistency/Repair,,,,0,,,,"If anticompaction fails on a node, a message is sent to all repair participants that this session is failed. If the other participants successfully finish their anticompactions we will not shut down the executor in `LocalSessions` since we can't change the state from ""FAILED"" to ""PREPARED"" and throw exception before calling shutdown.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-01-31 00:12:56.326,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 31 08:16:18 UTC 2019,,,,,,0|yi0hps:,9223372036854775807,,,,,bdeggleston,bdeggleston,,,,,,,,,,,"30/Jan/19 18:02;krummas;Patch: https://github.com/krummas/cassandra/commits/marcuse/15002
Tests: https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15002

The branch contains 2 commits, first one fixes the thread leak and retries acquiring sstables for 1 minute, the second commit adds rate limiting to anticompaction and makes anticompactions stoppable again (should have been done in CASSANDRA-14935).

The reasoning behind adding retries when trying to acquire sstables is that if there is an sstable [0, 100] and the user submits a repair on [0, 50], we will anticompact the sstable in to two new sstables [0, 50], [51, 100] - if a user submits a repair on [51, 100] while the first anticompaction is executing, we would fail immediately, since the sstable the new repair requires is busy anticompacting. By retrying to acquire sstables we give the first anticompaction a chance to finish and the new repair would be able to grab the new [51, 100] sstable - it does not need to wait for the whole first repair to finish.",31/Jan/19 00:12;bdeggleston;+1,"31/Jan/19 08:16;krummas;And committed as {{7f634feb7cf1fdb135133946ffd75efa681b8cb7}}, thanks

Ran the failing test in a loop locally successfully, don't think this patch caused the test failure",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anti-compaction briefly corrupts sstable state for reads,CASSANDRA-15004,13213220,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,benedict,bdeggleston,bdeggleston,01/Feb/19 01:33,12/Mar/19 14:16,13/Mar/19 22:35,06/Feb/19 13:52,3.0.x,3.11.x,4.0,,,,,,0,,,,"Since we use multiple sstable rewriters in anticompaction, the first call to prepareToCommit will remove the original sstables from the tracker view before the other rewriters add their sstables. This creates a brief window where reads can miss data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-02-01 09:01:30.081,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 06 13:53:44 UTC 2019,,,,,,0|yi0jvk:,9223372036854775807,,,,,,krummas,,,,,,,,,,,"01/Feb/19 01:42;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/15004-3.0]|[3.11|https://github.com/bdeggleston/cassandra/tree/15004-3.11]|[trunk|https://github.com/bdeggleston/cassandra/tree/15004-trunk]|
|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F15004-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F15004-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F15004-trunk]|","01/Feb/19 09:01;krummas;+1

pushed a unit test: [3.0|https://github.com/krummas/cassandra/commits/blake/15004-3.0] [3.11|https://github.com/krummas/cassandra/commits/blake/15004-3.11] [trunk|https://github.com/krummas/cassandra/commits/blake/15004-trunk]

edit: not sure what is going on with the dtests though, probably need a restart","01/Feb/19 09:06;benedict;Nice catch, and it looks like a good fix to me.

(+1)","04/Feb/19 13:41;benedict;Blake realised there was an issue with the patch he posted, so I have put together an alternative patch with input from [~krummas].

[3.0|https://github.com/belliottsmith/cassandra/tree/15004-3.0] [3.11|https://github.com/belliottsmith/cassandra/tree/15004-3.11] [4.0|https://github.com/belliottsmith/cassandra/tree/15004-4.0]

These patches extract an interface for {{LifecycleTransaction}} and no-op the relevant calls ({{prepareToCommit}} and {{obsoleteOriginals}}) so that {{SSTableRewriter.prepareToCommit}} does not update the tracker - these are then invoked directly once each rewriter has finished its other preparatory work.

It's a bit ugly and still finicky, but probably better/safer than more invasive surgery at this point in time.",04/Feb/19 15:13;krummas;updated unit tests [3.0|https://github.com/krummas/cassandra/tree/15004-3.0] [3.11|https://github.com/krummas/cassandra/tree/15004-3.11] [trunk|https://github.com/krummas/cassandra/tree/15004-trunk] also adds checks that the files on disk are what we expect,"05/Feb/19 15:27;krummas;lgtm, just need a few comments explaining what is going on and the comment mentioning {{permitRedundantTransitions}} needs to be removed/updated",05/Feb/19 15:54;benedict;Thanks.  I've pushed branches with updated comments.,05/Feb/19 17:08;krummas;+1,"06/Feb/19 13:53;benedict;Thanks, committed to [3.0|https://github.com/apache/cassandra/commit/44785dd2eec5697eec7e496ed3a73d2573f4fe6a], [3.11|https://github.com/apache/cassandra/commit/9199e591c6148d14f3d12784af8ce5342f118161] and [4.0|https://github.com/apache/cassandra/commit/df62169d1b6a5bfff2bc678ffbeb0883a3a576b5]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue a CQL native protocol warning if SASI indexes are enabled on a table,CASSANDRA-14866,13196254,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,adelapena,adelapena,adelapena,05/Nov/18 13:52,12/Mar/19 14:16,13/Mar/19 22:35,19/Feb/19 16:17,3.11.x,4.0,4.x,,Feature/SASI,,,,0,pull-request-available,,,"If someone enables SASI indexes then we should return a native protocol warning that will be printed by cqlsh saying that they are beta quality still and you need to be careful with using them in production.

This is motivated not only by [the existing bugs and limitations|https://issues.apache.org/jira/browse/CASSANDRA-12674?jql=project%20%3D%20CASSANDRA%20AND%20status%20%3D%20Open%20AND%20component%20%3D%20sasi] but for the fact that they haven't been extensively tested yet.",,"adelapena commented on pull request #44: Adapt tests to materialized views disabled by default (CASSANDRA-14866)
URL: https://github.com/apache/cassandra-dtest/pull/44
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Feb/19 15:27;githubbot;600","adelapena commented on pull request #44: Adapt tests to materialized views disabled by default (CASSANDRA-14866)
URL: https://github.com/apache/cassandra-dtest/pull/44
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Feb/19 16:16;githubbot;600",,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-05 19:44:31.049,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 06 05:47:10 UTC 2019,,,,,,0|s004l4:,9223372036854775807,,,,,snazy,snazy,,,,,,,,,,,"05/Nov/18 18:28;adelapena;Patch: 
||[3.11|https://github.com/adelapena/cassandra/tree/CASSANDRA-14866-3.11]||[trunk|https://github.com/adelapena/cassandra/tree/CASSANDRA-14866-trunk]||",05/Nov/18 19:44;snazy;+1,"06/Nov/18 05:17;jjirsa;Seems like an odd time for this, but other retroactively ""experimental"" features came with a way to toggle them off in the yaml rather than a per-user warning on CQL statements and discussion on the mailing list before they were committed.

Also, do we REALLY need to send the client warning on {{SELECT}} ? Seems like a big hammer there. ","07/Nov/18 10:31;adelapena;[~jjirsa] We don't really need the client warning on {{SELECT}}, probably the warning on {{CREATE INDEX}} is enough. I'm not so sure about the usefulness of the {{cassandra.yaml}} flag, but indeed it would be more aligned with the MV behaviour.

Here is an alternative version of the patch warning only on {{CREATE INDEX}}, and with a new {{enable_sasi_indexes}} property in {{cassandra.yaml}} (enabled by default):
||[3.11|https://github.com/adelapena/cassandra/tree/CASSANDRA-14866-mv-3.11]||[trunk|https://github.com/adelapena/cassandra/tree/CASSANDRA-14866-mv-trunk]||

[Here|https://github.com/adelapena/cassandra-dtest/commit/f0eeb1140678f3fe48129441b6dde4b36f8901eb] are a couple of dtests for the warnings and the property. These are our very first SASI dtests so I have put them in a separate {{sasi_test.py}} file.

[~snazy] WDYT?","19/Nov/18 19:46;adelapena;I'm adding a couple of unit tests for the client warning and the config flag, they are very similar to the dtests above.",09/Jan/19 13:33;snazy;+1 on the latest changes (haven't noticed the update on this ticket).,11/Jan/19 13:28;adelapena;[~snazy] I have just updated the patch set SASI as disabled by default on trunk. This has involved a few minor changes in unit tests. I think that with the new unit tests we don't require the extra dtests anymore. CI looks good to me.,"11/Jan/19 14:15;jjirsa;As already mentioned in a previous comment, this deserves a mailing list thread. That’s doubly true if you’re proposing changing the default
","11/Jan/19 14:21;iamaleksey;It's one thing to issue a warning here - which, to be clear, I'm not against at all - but an altogether qualitatively different thing to disable the feature by default.

I would argue that the latter, in 2019 Cassandra universe, should require a discussion on the dev list first.",14/Jan/19 19:23;adelapena;I have just sent [a mail|https://lists.apache.org/thread.html/280722c7f4b8f4a7a78b7413dc41b09f649a43d4fb0d760c44d08e4c@%3Cdev.cassandra.apache.org%3E] to dev list.,"04/Feb/19 15:18;adelapena;I've updated the patch to disable both {{enable_materialized_views}} and {{enable_sasi_indexes}} by default in trunk, as discussed in the mail list:
||[3.11|https://github.com/adelapena/cassandra/tree/CASSANDRA-14866-3.11]||[trunk|https://github.com/adelapena/cassandra/tree/CASSANDRA-14866-trunk]||[dtest|https://github.com/adelapena/cassandra-dtest/commits/CASSANDRA-14866]||

I've moved {{enable_materialized_views}}, {{enable_sasi_indexes}} and {{enable_transient_replication}} to an experimental features section in cassandra.yaml.

Each dtest creating materialized views is modified to activate the flag, I don't know if there is a better way to do this.",18/Feb/19 20:28;snazy;+1 Thanks for the patch!,"19/Feb/19 16:16;adelapena;Thanks for reviewing.

Committed to 3.11 as [e6a61be8c857106d5d99a270b2d17de9f84c4d67|https://github.com/apache/cassandra/commit/e6a61be8c857106d5d99a270b2d17de9f84c4d67] and merged to trunk.

Dtests committed as [227ae5e7aca25c668340e467a35dfb0f7e1546fa|https://github.com/apache/cassandra-dtest/commit/227ae5e7aca25c668340e467a35dfb0f7e1546fa].","06/Mar/19 05:47;jolynch;[~adelapena] I think that this may have broken 2.2 dtests in test teardown. Can you try running 2.2 dtests on circle and see if they pass for you?

For me I get a lot of ""test teardown failure"" because of the enable_materialized_views included in the yaml now:
{noformat}
test teardown failure
Unexpected error found in node logs (see stdout for full details). Errors: [ERROR [main] 2019-03-06 05:05:04,639 CassandraDaemon.java:670 - Exception encountered during startup
org.apache.cassandra.exceptions.ConfigurationException: Invalid yaml. Please remove properties [enable_materialized_views] from your cassandra.yaml
	at org.apache.cassandra.config.YamlConfigurationLoader$MissingPropertiesChecker.check(YamlConfigurationLoader.java:146) ~[main/:na]
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:113) ~[main/:na]
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:85) ~[main/:na]
	at org.apache.cassandra.config.DatabaseDescriptor.loadConfig(DatabaseDescriptor.java:151) ~[main/:na]
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:133) ~[main/:na]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:531) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:657) [main/:na], ERROR [main] 2019-03-06 05:05:04,639 CassandraDaemon.java:670 - Exception encountered during startup
org.apache.cassandra.exceptions.ConfigurationException: Invalid yaml. Please remove properties [enable_materialized_views] from your cassandra.yaml
	at org.apache.cassandra.config.YamlConfigurationLoader$MissingPropertiesChecker.check(YamlConfigurationLoader.java:146) ~[main/:na]
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:113) ~[main/:na]
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:85) ~[main/:na]
	at org.apache.cassandra.config.DatabaseDescriptor.loadConfig(DatabaseDescriptor.java:151) ~[main/:na]
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:133) ~[main/:na]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:531) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:657) [main/:na]]
{noformat}

For example, a run after your dtest change: [73 fails|https://circleci.com/gh/jolynch/cassandra/524#tests/containers/9]
Versus a run without your dtest change: [1 fail|https://circleci.com/gh/jolynch/cassandra/528#tests/containers/74]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot perform slice reads in reverse direction against tables with clustering columns in mixed order,CASSANDRA-14899,13198891,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,16/Nov/18 12:14,12/Mar/19 14:16,13/Mar/19 22:35,20/Nov/18 15:31,2.2.14,,,,Legacy/CQL,,,,0,,,,"CASSANDRA-11196 accidentally broke reading from tables with mixed clustering column order in the opposite direction.

{{ReversedPrimaryKeyRestrictions::boundsAsComposites}} method attempts to reverse the list returned from {{PrimaryKeyRestrictionSet::boundsAsComposites}} and fails, as Guava’s {{Lists::transform}} method returns a {{List}} that doesn’t support {{set()}}.

Reproduction:

{code}
CREATE TABLE test.test (
    a int,
    b int,
    c int,
    PRIMARY KEY (a, b, c)
) WITH CLUSTERING ORDER BY (b ASC, c DESC);

SELECT * FROM test.test WHERE a = 0 AND (b, c) > (0, 0) ORDER BY b DESC, c ASC;

> ServerError: java.lang.UnsupportedOperationException
{code}

{code}
java.lang.UnsupportedOperationException: null
	at java.util.AbstractList.set(AbstractList.java:132) ~[na:1.8.0_181]
	at java.util.Collections.swap(Collections.java:497) ~[na:1.8.0_181]
	at java.util.Collections.reverse(Collections.java:378) ~[na:1.8.0_181]
	at org.apache.cassandra.cql3.restrictions.ReversedPrimaryKeyRestrictions.boundsAsComposites(ReversedPrimaryKeyRestrictions.java:63) ~[main/:na]
	at org.apache.cassandra.cql3.restrictions.StatementRestrictions.getClusteringColumnsBoundsAsComposites(StatementRestrictions.java:580) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.makeFilter(SelectStatement.java:418) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.getSliceCommands(SelectStatement.java:359) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.getPageableCommand(SelectStatement.java:191) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:172) ~[main/:na]
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-20 14:26:43.928,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 20 15:31:20 UTC 2018,,,,,,0|s00ktk:,9223372036854775807,2.2.13,,,,,ifesdjeen,,,,2.2.6,,,,,,,"16/Nov/18 12:20;iamaleksey;Code [here|https://github.com/iamaleksey/cassandra/commits/14899-2.2], CI [here|https://circleci.com/workflow-run/b6db8bff-2aa2-49e7-8a8e-d01079c85872].","20/Nov/18 14:26;ifesdjeen;+1 with two minor optional nits for your consideration: 

  * {{ArrayList -> List}} [here|https://github.com/iamaleksey/cassandra/commit/29b7f064d69aff55123dcfc282c3c44ab733734f#diff-f2feb48017f94269b5fbb96ee38b5816R204]
  * you can short-circuit [here|https://github.com/iamaleksey/cassandra/commit/29b7f064d69aff55123dcfc282c3c44ab733734f#diff-f2feb48017f94269b5fbb96ee38b5816R199]

If you think these modifications are unnecessary, please feel free to commit as-is.","20/Nov/18 15:31;iamaleksey;Committed as [cf6f7920f7742bb9a17a23ad37499d9213807d81|https://github.com/apache/cassandra/commit/cf6f7920f7742bb9a17a23ad37499d9213807d81] to 2.2, and merged upwards (just the unit tests bit). Cheers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop/add column name with different Kind can result in corruption,CASSANDRA-14843,13193916,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,24/Oct/18 16:44,12/Mar/19 14:16,13/Mar/19 22:35,29/Nov/18 14:56,3.0.18,,,,CQL/Semantics,Local/SSTable,,,0,,,,"While we have always imposed that the type of any column name remains consistent, we have not done the same for its kind.  If a column’s kind is changed via Drop/Add, the SerializationHeader’s column sets will be interpreted incorrectly, and may either lead to CorruptSSTableException or silent data corruption.

The problem occurs in SerializationHeader.Component.toHeader().  In this method, we lookup columns from the metadata only by name, ignoring the static status.  If a column of the same name but different kind to our dropped column exists in the schema, we will only add this column to the list of fields we expect.  So we will be missing a column from our set of expected static columns.  We will also add the regular variant to the set of regular columns present, which it may not be.

There are a number of ways this can play out:

1) There are no other static columns in the affected sstables.  In this case we will incorrectly treat the table as containing no static data, so we will not attempt to read any static row.  This leaves a static row to be read as the first regular row, and will throw the exception in the description.
1a) If for some reason the static row is absent - say, it has expired - then depending on lexicographical ordering, and if the sstable did not contain any instance of the regular variant, we may misattribute column data.  This will probably result in corruption exceptions, but if the columns were of the same type it could be undetectable.
2) There are other static columns on the table, and in the affected sstables.  In this case, the row will be correctly treated as a static row, but depending on the dropped column’s lexicographical position relative to these, it may result in column data being misattributed to other columns.  This *could* be undetectable corruption, if the column types were the same.
3) Either of these above scenario could be replayed with the static/regular relations replaced.

CASSANDRA-14591 would protect against 1a and 2.

Probably the safest and correct fix is to prevent adding a column back with a different kind to the original, however this would require storing more metadata about dropped columns.  This is probably viable for 4.0, but for 3.0 perhaps too invasive. 

It is quite easy to make this particular method safe against this particular corruption.  However it is hard to be certain there aren’t other places where assumptions were made about a name being of only one kind.  There are only a handful of places that *should* need to be of concern, specifically those places that invoke CFMetaData.getDroppedColumn, so we should be fairly confident that - if these call sites are safe - we are now insulated.  This might suffice for 3.0.  Thoughts?
",,,,,,,,,,,,,,,CASSANDRA-14948,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-10-24 17:12:45.943,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 29 14:56:33 UTC 2018,,,,,,0|i3zlfr:,9223372036854775807,,,,,,beobal,,,,3.0.0,,,,,,,"24/Oct/18 17:12;iamaleksey;Yep.

Too invasive for 3.0, but trivial to prevent re-addition of columns with a different kind in 4.0, as we do store the kind of dropped columns in 4.0 since CASSANDRA-9425.","27/Nov/18 19:39;benedict;[3.0 patch|https://github.com/belliottsmith/cassandra/tree/14843], [CI|https://circleci.com/gh/belliottsmith/cassandra/tree/14843]","29/Nov/18 11:18;beobal;+1 - there's an additional/unexpected dtest failure in {{putget_test.py::TestPutGet::test_wide_slice}}, but I've observed this failing on 3.0 yesterday, so I'm planning to investigate that separately.",29/Nov/18 14:56;benedict;Thanks.  Committed as [4b1f40d5382638bf3913293b713d5d22b57c844d|https://github.com/apache/cassandra/commit/4b1f40d5382638bf3913293b713d5d22b57c844d] to 3.0 and up.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid CME when cancelling compactions for anticompaction,CASSANDRA-15036,13218143,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,26/Feb/19 17:58,12/Mar/19 14:16,13/Mar/19 22:35,04/Mar/19 13:10,4.0,,,,,,,,0,,,,When iterating over a set created with {{Collections.synchronizedSet}} we need to manually synchronize on the set,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-03-04 12:56:40.46,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 04 13:10:21 UTC 2019,,,,,,0|z003s0:,9223372036854775807,,,,,,beobal,,,,,,,,,,,"26/Feb/19 18:02;krummas;patch to just synchronize on the active compactions;

patch: https://github.com/krummas/cassandra/commits/marcuse/15036
circle: https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15036","27/Feb/19 09:13;krummas;not sure what is going on with circleci - seems to randomly fail, even for trunk: https://circleci.com/workflow-run/1c33a46e-a551-47ef-8872-6a0091041566",04/Mar/19 12:56;beobal;+1 LGTM after the [ccm patch|https://github.com/riptano/ccm/pull/694] got circleci looking a bit more healthy.,04/Mar/19 13:10;krummas;and committed as {{6b3ea1e1a480c9f9a2f6f813443daa4c6866e2ff}} - thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LegacyLayout errors on collection tombstones from dropped columns,CASSANDRA-14912,13200622,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,beobal,beobal,26/Nov/18 17:04,12/Mar/19 14:16,13/Mar/19 22:35,28/Nov/18 16:15,3.0.18,3.11.4,,,Legacy/Local Write-Read Paths,,,,0,,,,"When reading legacy sstables in 3.0, the dropped column records in table metadata are not checked when a collection tombstone is encountered. This means that if a collection column was dropped and a new column with the same name but a non-collection type subsequently added prior to upgrading to 3.0, reading any sstables containing the collection data will error. This includes reads done by upgradesstables, which makes recovery from this situation without losing data impossible. Scrub will clean the affected tables, but any valid data will also be discarded.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-27 14:08:49.45,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 28 16:15:57 UTC 2018,,,,,,0|s00ve0:,9223372036854775807,,,,,,iamaleksey,,,,,,,,,,,"27/Nov/18 12:43;beobal;||branch||CI||
|[14912-3.0|https://github.com/beobal/cassandra/tree/14912-3.0]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14912-3.0]|
|[14912-3.11|https://github.com/beobal/cassandra/tree/14912-3.11]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14912-3.11]|

 ","27/Nov/18 14:08;benedict;I think this is related to CASSANDRA-14749; basically I didn't properly fix it for a drop and re-add of different type.

Should a fix also go [here|https://github.com/apache/cassandra/blob/06c55f779ae68de98cce531e0b78be5716849003/src/java/org/apache/cassandra/db/LegacyLayout.java#L217]?  We could simply check that the column definition is a collection, and perhaps also (if we wanted to be super paranoid) that the type is compatible.

Checking the dropped time is also a great additional check, but it doesn't necessarily seem as robust, given clock drift.","27/Nov/18 16:12;beobal;Adding the check to {{LegacyLayout::decodeBound}} seems reasonable as an extra safety check, but it wouldn’t make a difference in this case as we’d then fetch the dropped column definition and proceed as before to construct the bound. We could also check that the dropped column def was a collection and throw if not, but that’s a slightly different bug IMO.

The dropped time check is what's really required otherwise we don't properly filter data from sstables written before the schema changes happened. Maybe I'm misunderstanding what you mean here about it not being terribly robust, but this is the mechanism by which we filter such atoms (or simple/complex columns) in both the legacy and non-legacy cases. I agree that it’s not perfect wrt to clock drift btw, just that it’s the only method we have AFAIK for handling dropped columns.","27/Nov/18 16:13;iamaleksey;[~beobal] +1

[~benedict] Sam is about to reply, but I think we are actually good there.","27/Nov/18 16:23;iamaleksey;bq. Adding the check to LegacyLayout::decodeBound seems reasonable as an extra safety check, but it wouldn’t make a difference in this case as we’d then fetch the dropped column definition and proceed as before to construct the bound. We could also check that the dropped column def was a collection and throw if not, but that’s a slightly different bug IMO.

Additionally, it can easily and legally be {{BytesType}}, as we used to not to bother to record dropped types in 2.1, so we cannot really reliably tell. But also we don't need to, as this should work out just fine either way.","27/Nov/18 16:53;benedict;{quote}this is the mechanism by which we filter such atoms (or simple/complex columns) in both the legacy and non-legacy cases
{quote}
So, I agree that this bug probably also exists for our non-legacy handling of complex tombstones.

But our regular atom dropping is only expected to be a 'best effort' to prevent dropped data making it to the client.  This bug seems to be about preventing data being entirely unavailable because the {{ColumnDefinition}} is incorrect (i.e. not a complex column)?

So surely specifically checking the condition that causes the failure, i.e. that we're building a tombstone with a non-complex column, is the robust solution to preventing this unavailable data?

I agree that filtering it downstream based on dropped time is also beneficial, but in preventing this specific failure it seems to be either superfluous or insufficient in some cases?","27/Nov/18 17:13;beobal;bq. So surely specifically checking the condition that causes the failure, i.e. that we're building a tombstone with a non-complex column, is the robust solution to preventing this unavailable data?

Do you mean changing {{decodeBound}} to do something like return null or a special value in such cases and having its callers handle that? It can't throw in situations like this or else we're no better off than we are currently as the row containing the collection tombstone would still be unreadable.","27/Nov/18 17:30;benedict;I _think_ the easiest fix for preventing the exception is to simply return a RT bound where the {{ColumnDefinition}} is definitely complex?

This might mean we return a RT that covers a non-existent column, but with your change we *should* filter that out.  But if we don't, that's fine, it's only a best effort, and it won't actually cause us any problems.

FWIW, I've tried this change to confirm it works [here|https://github.com/belliottsmith/cassandra/tree/14912-3.0-suggest].","28/Nov/18 13:44;beobal;Adding the first {{isComplex}} check (as per your branch) does help here because the dropped definition is complex. Adding the second {{isComplex}} check on that and constructing a fake definition if it isn't is probably worth it to deal with the rare case where a column has been dropped and re-added multiple times. Pushed a second commit with those additions & the test extended to cover them.

Although we *could* live without the {{isDroppedComplexDeletion}} check, I've left it in and extended the test to check both when it fails and succeeds. Though this does make the verification of results somewhat finicky, it's maybe useful in documenting the expected behaviour in these edge cases.",28/Nov/18 13:50;benedict;+1,"28/Nov/18 16:15;beobal;Thanks, committed to 3.0 in {{0a7fbee43f25b6ad3172825cd29bae455223ab33}} and merged to 3.11 and trunk (with {{-s ours}})",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable min/max metadata can cause data loss,CASSANDRA-14861,13195507,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,31/Oct/18 22:05,12/Mar/19 14:16,13/Mar/19 22:35,06/Nov/18 19:26,3.0.18,3.11.4,4.0,,Local/SSTable,,,,0,,,,"There’s a bug in the way we filter sstables in the read path that can cause sstables containing relevant range tombstones to be excluded from reads. This can cause data resurrection for an individual read, and if compaction timing is right, permanent resurrection via read repair. 

We track the min and max clustering values when writing an sstable so we can avoid reading from sstables that don’t contain the clustering values we’re looking for in a given read. The min max for each clustering column are updated for each row / RT marker we write. In the case of range tombstones markers though, we only update the min max for the clustering values they contain, which is almost never the full set of clustering values. This leaves a min/max that are above/below (respectively) the real ranges covered by the range tombstone contained in the sstable.

For instance, assume we’re writing an sstable for a table with 3 clustering values. The current min clustering is 5:6:7. We write an RT marker for a range tombstone that deletes any row with the value 4 in the first clustering value so the open marker is [4:]. This would make the new min clustering 4:6:7 when it should really be 4:. If we do a read for clustering values of 4:5 and lower, we’ll exclude this sstable and it’s range tombstone, resurrecting any data there that this tombstone would have deleted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-01 21:12:21.637,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 06 19:26:04 UTC 2018,,,,,,0|i3zv8n:,9223372036854775807,,,,,,benedict,beobal,,,,,,,,,,"01/Nov/18 17:00;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/14861-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14861-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/14861-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14861-3.11]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/14861-trunk]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/14861-trunk]|

This adds a minor sstable version to 3.x and changes 2 behaviors. First, when reading metadata for pre-md sstables, -only the first clustering value is loaded into the min/max values and the rest are discarded- min max values are discarded. When writing new sstables, the size of the min/max values written are limited by the length of the shortest RT clustering.

edit: min max values from legacy sstables need to be discarded, otherwise open ended RTs (ie: DELETE WHERE c < 100) would still have this problem.","01/Nov/18 17:52;bdeggleston;[~benedict], [~beobal], [~iamaleksey] would one or more of you be interested in reviewing?","01/Nov/18 21:12;benedict;Sure, happy to","02/Nov/18 11:18;benedict;Really nice catch.  Unfortunately, I *think* this still leaves some incidences of this problem unhandled.  If I understand the patch correctly, and the way we do sstable filtering (it's been a while since I looked closely at it), we would still permit the following sequence of events to slip by:
 # RT[(a, b)..(x, x)]
 # Query for clustering (b, a)

 ","02/Nov/18 15:10;beobal;Benedict's understanding is correct, the issue is that when we check whether a slice intersects with the min/max values, each component is compared in isolation. We need to also remove the ""minor optimization"" from Slice::intersects or switch to min/max clusterings, rather than components.","02/Nov/18 18:03;bdeggleston;Good catch, pushed up the fix for that",06/Nov/18 18:16;beobal;+1,06/Nov/18 19:07;benedict;+1,"06/Nov/18 19:26;bdeggleston;committed as {{d60c78358b6f599a83f3c112bfd6ce72c1129c9f}}, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range.subtractContained produces incorrect results when used on full ring,CASSANDRA-14869,13196753,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,Gerrrr,Gerrrr,Gerrrr,07/Nov/18 07:10,12/Mar/19 14:16,13/Mar/19 22:35,19/Nov/18 08:03,3.0.x,3.11.x,4.0,,Legacy/Core,,,,0,,,,"Currently {{Range.subtractContained}} returns incorrect results if minuend range covers full ring and:
* subtrahend range wraps around. For example, {{(50, 50] - (10, 100]}} returns {{\{(50,10], (100,50]\}}} instead of {{(100,10]}}
* subtrahend range covers the full ring as well. For example {{(50, 50] - (0, 0]}} returns {{\{(0,50], (50,0]\}}} instead of {{\{\}}}",,,,,,,,,,,,,,,,,,07/Nov/18 07:09;Gerrrr;range bug.jpg;https://issues.apache.org/jira/secure/attachment/12947191/range+bug.jpg,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-11-12 09:50:34.626,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 19 08:02:40 UTC 2018,,,,,,0|s007nk:,9223372036854775807,,,,,,ifesdjeen,,,,,,,,,,,"07/Nov/18 07:14;Gerrrr;Patches:
* [3.0 | https://github.com/Gerrrr/cassandra/tree/14869-3.0]
* [3.11 | https://github.com/Gerrrr/cassandra/tree/14869-3.11]
* [4.0 | https://github.com/Gerrrr/cassandra/tree/14869-4.0]","12/Nov/18 09:50;ifesdjeen;Several remarks: 
  * I would extract [this check|https://github.com/Gerrrr/cassandra/commit/f92047ab378062e58d02d7f57e0694ba2e3c90a7#diff-b6aa8cb091f4de56555d650df9db6ca6R282] to {{isFull}} method.
  * if you're already using {{return}} there's no need for {{else if}}
  * Maybe add tests for {{subtract}} also not only {{subtractAll}}
  * Make sure full range subtraction is covered, like {{range(0,0).subtract(range(1,1))}} should be empty
  * Still mention of ArrayList [here|https://github.com/Gerrrr/cassandra/commit/f92047ab378062e58d02d7f57e0694ba2e3c90a7#diff-b6aa8cb091f4de56555d650df9db6ca6R277] 

In summary, there are two cases that are broken right now: subtracting one full range from another {{range(0,0).subtract(1,1)}} does not yield an empty range, and subtracting a non-wrapping range from wrapping one: {{range(0,0).subtract(-1, 1)}} is yielding two ranges which wrap incorrectly. Even though patch does fix both issues description does not fully elaborate on issue and added if cases might use a small elaboration comment (same as issue description).","13/Nov/18 17:32;Gerrrr;Thanks for the review! I've fixed the patches according to your suggestions.

{quote}
Make sure full range subtraction is covered, like range(0,0).subtract(range(1,1)) should be empty
{quote}

I believe this case is covered by [that test|https://github.com/Gerrrr/cassandra/commit/9e4afefa1dd892a22197c7296ec1d9cd13ad5ae0#diff-5b7f5ad57ef5c28db2566ce381294b0dR404].



",14/Nov/18 09:30;ifesdjeen;Did you run dtests results for this change across all branches?,"14/Nov/18 21:43;jjirsa;([~ifesdjeen], you could just push it into a branch and run it through circleci just to have a passing dtest link for posterity)


","15/Nov/18 18:46;ifesdjeen;[~jjirsa] sure; I did trigger tests as I wrote the comment.

[~Gerrrr] looks like there's a failure in unit tests: https://circleci.com/gh/ifesdjeen/cassandra/782",16/Nov/18 14:51;Gerrrr;[~ifesdjeen] Thanks for running the builds! I've tested all failing tests locally on {{cassandra-3.0}} and {{trunk}} right before my commit and they still fail in the same way.,"16/Nov/18 20:08;ifesdjeen;Builds: 

|[3.0|https://circleci.com/workflow-run/dec6ad52-121f-4bd6-92f9-95d52b041ec9]|[3.11|https://circleci.com/workflow-run/02340ed3-5b7f-4e8e-a51f-2192cdd93612]|[trunk|https://circleci.com/workflow-run/7de63da3-3977-4ace-aeef-be5430b9bc37]|","19/Nov/18 08:02;ifesdjeen;[~Gerrrr] thank you for the patch!

Committed to [3.0|https://git1-us-west.apache.org/repos/asf?p=cassandra.git;a=commit;h=c6f822c2a07e0e7c8e4af72523fe62d181c71e56] and merged up to [3.11|https://git1-us-west.apache.org/repos/asf?p=cassandra.git;a=commit;h=78c7d57ebb28ac688cd287d7d8b8f483a99d0135] and [trunk|https://git1-us-west.apache.org/repos/asf?p=cassandra.git;a=commit;h=13108037177a30e103a84bca5dadb38d1c090453].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paged Range Slice queries with DISTINCT can drop rows from results,CASSANDRA-14956,13208002,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,beobal,beobal,07/Jan/19 11:45,12/Mar/19 14:16,13/Mar/19 22:35,17/Jan/19 13:12,2.1.21,2.2.14,,,CQL/Interpreter,,,,0,,,,"If we have a partition where the first CQL row is fully deleted (possibly via TTLs), and that partition happens to fall on the page boundary of a paged range query which is using SELECT DISTINCT, the next live partition *after* it is omitted from the result set. This is due to over fetching of the pages and a bug in trimming those pages where overlap occurs.

This does not affect 3.0+.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-01-15 14:49:21.218,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 04 11:41:19 UTC 2019,,,,,,0|u00kow:,9223372036854775807,,,,,,krummas,,,,,,,,,,,"07/Jan/19 11:59;beobal;Patch to change the page trimming for DISTINCT range queries to simply remove the first row from the page.

 
||branch||CI||
|[14956-2.2|https://github.com/beobal/cassandra/tree/14956-2.2]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14956-2.2]|","08/Jan/19 10:39;beobal;Pushed a 2.1 branch after discussion on dev@ about one last 2.1 release before EOL. Unfortunately, CircleCI no longer supports v1.0 job configuration so a CI run is going to need the v2.0 config backporting (which we may want to do before a release anyway).

[14956-2.1|https://github.com/beobal/cassandra/tree/14956-2.1]","15/Jan/19 14:49;krummas;+1, just change CASSANDRA-XYZ in the unit test comment","17/Jan/19 13:12;beobal;Thanks, committed to 2.1 in {{dd228d4581b020fb2fb788858481c81357d7fa72}} and merged (test only to 3.0+).","31/Jan/19 18:03;jolynch;[~beobal] I think that the merge up to trunk broke trunk test runs (e.g. [7d138e20|https://circleci.com/gh/jolynch/cassandra/415]). I think it has to do with re-using the data directory from {{cassandra-murmur.yaml}}, debugging it now but do you think we need a separate ticket for fixing that test or I can just submit a fixup patch here?

Confirmed it's a pollution issue with the {{EmbeddedCassandraService}} by running two tests that both use it:

{noformat}
$ cat testlist.txt                                                                                                                                                                                                            
org/apache/cassandra/audit/AuditLoggerTest.java
org/apache/cassandra/cql3/PagingTest.java

$ ant testclasslist -Dtest.classlistfile=$(pwd)/testlist.txt
...
[junit-timeout] Testsuite: org.apache.cassandra.cql3.PagingTest
[junit-timeout] Testsuite: org.apache.cassandra.cql3.PagingTest Tests run: 0, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 5.241 sec
[junit-timeout] 
[junit-timeout] Testcase: org.apache.cassandra.cql3.PagingTest: Caused an ERROR
[junit-timeout] Unable to gossip with any peers
[junit-timeout] java.lang.RuntimeException: Unable to gossip with any peers
[junit-timeout]         at org.apache.cassandra.gms.Gossiper.doShadowRound(Gossiper.java:1546)
[junit-timeout]         at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:553)
[junit-timeout]         at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:841)
[junit-timeout]         at org.apache.cassandra.service.StorageService.initServer(StorageService.java:699)
[junit-timeout]         at org.apache.cassandra.service.StorageService.initServer(StorageService.java:650)
[junit-timeout]         at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:379)
[junit-timeout]         at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:501)
[junit-timeout]         at org.apache.cassandra.service.EmbeddedCassandraService.start(EmbeddedCassandraService.java:50)
[junit-timeout]         at org.apache.cassandra.cql3.PagingTest.setup(PagingTest.java:63)
[junit-timeout] 
[junit-timeout] 
[junit-timeout] Testcase: org.apache.cassandra.cql3.PagingTest: Caused an ERROR
[junit-timeout] null
[junit-timeout] java.lang.NullPointerException
[junit-timeout]         at org.apache.cassandra.cql3.PagingTest.tearDown(PagingTest.java:81)
[junit-timeout] 
[junit-timeout] 
...
{noformat}","31/Jan/19 19:31;jolynch;Turns out it was just that in trunk we don't include the port in the [cassandra-murmur3.yaml|https://github.com/apache/cassandra/blob/7f634feb7cf1fdb135133946ffd75efa681b8cb7/test/conf/cassandra-murmur.yaml#L27] so the check [here|https://github.com/apache/cassandra/blob/7f634feb7cf1fdb135133946ffd75efa681b8cb7/src/java/org/apache/cassandra/service/StorageService.java#L556] fails and we actually try to gossip with peers. A quick patch to just include the port appears to fix the issue. Other tests are broken but PagingTest is now fixed:

 
||trunk||
|[bb4b0626|https://github.com/jolynch/cassandra/commit/bb4b06265cfc7a9cf915e8022feb9ccd83390d5d]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14956-fix-unit.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14956-fix-unit]|

Test failures:
{{org.apache.cassandra.distributed.DistributedReadWritePathTest#readRepairTest}}: I believe this is CASSANDRA-14922, debugging that separately","04/Feb/19 11:41;beobal;Thanks [~jolynch], good catch. I've pushed your fix to trunk in {{27b35799a46dd5b649c4a172f4f8316b48615304}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Forbid re-adding static columns as regular and vice versa,CASSANDRA-14913,13200641,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,26/Nov/18 18:12,12/Mar/19 14:16,13/Mar/19 22:35,27/Nov/18 14:01,4.0,,,,Cluster/Schema,,,,0,,,,"Re-adding a dropped column with an incompatible kind (dropped regular re-added as static, or dropped static re-added as regular) can ultimately result in corruption (see CASSANDRA-14843 for more context). In 3.x, unfortunately, we don’t persist enough context when dropping a column. In trunk, however, we do, and it’s trivial to forbid this operation, as we should.",,,,,,,,,,,,,,,CASSANDRA-14948,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-26 20:20:04.458,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 27 14:09:40 UTC 2018,,,,,,0|s00vi0:,9223372036854775807,,,,,,benedict,,,,,,,,,,,"26/Nov/18 18:15;iamaleksey;[Code|https://github.com/iamaleksey/cassandra/commits/14913], [CI|https://circleci.com/workflow-run/f88826a9-62c0-42bb-84ec-7e403ef80747].",26/Nov/18 20:20;benedict;+1,"27/Nov/18 07:37;ifesdjeen;This is great, +1. 

Should we back port this and re-add multi-cell as single cell or vice versa to 3.0/3.11? These are pretty serious issues I'd say as they can lead to corruption.","27/Nov/18 14:00;iamaleksey;Thanks, committed as [8aec742a1f86a825028f4a1267e111cd1a4aea40|https://github.com/apache/cassandra/commit/8aec742a1f86a825028f4a1267e111cd1a4aea40] to trunk. ","27/Nov/18 14:08;iamaleksey;[~ifesdjeen] I wouldn't mind it, personally. The change would be more involved: would have to add an extra column ({{kind}}) to the schema table in 3.0 (that already exists in 4.0, making this change super trivial). Also extend {{DroppedColumn}} a little. But that doesn't sound prohibitively involved to me.

If you are interested in doing the backport, please do. Otherwise I'll do it eventually.

While at it, might also backport 4.0 prohibition on re-adding columns with a different type from the dropped one if nobody objects.","27/Nov/18 14:09;benedict;{quote}While at it, might also backport 4.0 prohibition on re-adding columns with a different type from the dropped one if nobody objects.
{quote}
+1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Protocol frame checksumming options should not be case sensitive,CASSANDRA-14716,13184390,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,beobal,beobal,11/Sep/18 15:52,12/Mar/19 14:16,13/Mar/19 22:35,27/Nov/18 14:22,4.0,,,,Legacy/CQL,,,,0,,,,"Protocol v5 adds support for checksumming of native protocol frame bodies. The checksum type is negotiated per-connection via the \{{STARTUP}} message, with two types currently supported, Adler32 and CRC32. The mapping of the startup option value requested by the client to a \{{ChecksumType}} should not be case sensitive, but currently it is.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-26 20:20:38.627,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 27 14:22:28 UTC 2018,,,,,,0|i3xyyn:,9223372036854775807,,,,,iamaleksey,iamaleksey,,,,,,,,,,,"11/Sep/18 16:05;beobal;Patch + unit test: [here|https://github.com/beobal/cassandra/commit/f9cda7d55cc2b57ba5905f0635275f569d56d4c9]
||utests||dtests||
|[utests|https://circleci.com/gh/beobal/cassandra/437]|[vnodes|https://circleci.com/gh/beobal/cassandra/436] / [no vnodes|https://circleci.com/gh/beobal/cassandra/438]|","26/Nov/18 20:20;iamaleksey;I might be missing it, and probably am, but what is the reason for not simply doing {{ChecksumType.valueOf(name.toUpperCase)}}, and rethrowing {{IllegalArgumentException}} as {{ProtocolException}}?

EDIT: Ok, I see, it's {{Adler32}} not being low or upper case itself. Is it too late to change {{Adler32}} to {{ADLER32}} at this point? Would it be desirable?","26/Nov/18 20:30;iamaleksey;That said, +1 either way.","27/Nov/18 06:49;djoshi3;Another option would be to use a static, immutable map of upper cased values to {{ChecksumType}} and use that.","27/Nov/18 09:58;beobal;{quote}Is it too late to change {{Adler32}} to {{ADLER32}} at this point? Would it be desirable?{quote}

No, I think that's acceptable and desirable, if only for consistency. This is the only usage of {{ChecksumType}} where the algo is configurable, every other usage is hardcoded to {{CRC32}}. We included support for {{Adler32}} in CASSANDRA-13304, rather than just removing that {{ChecksumType}} to enable future potential perf improvements to be taken advantage of (https://issues.apache.org/jira/browse/CASSANDRA-13304?focusedCommentId=15923191&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15923191). 

Before committing I'll rename the enum value, re-run CI and post a message to dev list to be sure nobody objects to this post-freeze change. Thanks.","27/Nov/18 12:51;beobal;Rebased and updated branch to uppercase the enum value.
||branch||CI||
|[14716-trunk|https://github.com/beobal/cassandra/tree/14716-trunk]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14716-trunk]|
","27/Nov/18 13:41;iamaleksey;[~djoshi3] yeah, that would also work.

[~beobal] +1","27/Nov/18 14:22;beobal;Thanks, committed to trunk in {{cdeac4992bdb1f569c3a04b628ded7e5351364ee}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix 14861 related test failures,CASSANDRA-14889,13198120,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,13/Nov/18 18:28,12/Mar/19 14:16,13/Mar/19 22:35,26/Nov/18 19:55,3.0.18,3.11.4,4.0,,Legacy/Testing,,,,0,,,,CASSANDRA-14861 broke a few tests unfortunately,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 26 19:55:02 UTC 2018,,,,,,0|s00g2g:,9223372036854775807,,,,,,,,,,,,,,,,,"26/Nov/18 19:55;bdeggleston;Turned out to be a simple one liner, so I just ninja'd as [60a8cfe115b78cee7e4d8024984fa1f8367685db |https://github.com/apache/cassandra/commit/60a8cfe115b78cee7e4d8024984fa1f8367685db]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Don't write to system_distributed.repair_history, system_traces.sessions, system_traces.events in mixed version 3.X/4.0 clusters",CASSANDRA-14841,13193805,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,aweisberg,tommy_s,tommy_s,24/Oct/18 09:36,12/Mar/19 14:16,13/Mar/19 22:35,01/Nov/18 16:13,4.0,,,,Legacy/Distributed Metadata,,,,0,,,,"When upgrading from 3.x to 4.0 I get exceptions in the old nodes once the first 4.0 node starts up. I have tested to upgrade from both 3.0.15 and 3.11.3 and get the same problem.

 
{noformat}
2018-10-22T11:12:05.060+0200 ERROR [MessagingService-Incoming-/10.216.193.244] CassandraDaemon.java:228 Exception in thread Thread[MessagingService-Incoming-/10.216.193.244,5,main]
java.lang.RuntimeException: Unknown column coordinator_port during deserialization
at org.apache.cassandra.db.Columns$Serializer.deserialize(Columns.java:452) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.db.filter.ColumnFilter$Serializer.deserialize(ColumnFilter.java:482) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:760) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:697) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.net.MessageIn.read(MessageIn.java:123) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:192) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:180) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:94) ~[apache-cassandra-3.11.3.jar:3.11.3]{noformat}
I think it was introduced by CASSANDRA-7544.

 ",,,,,,,,,,,,,,CASSANDRA-14897,CASSANDRA-14864,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-10-26 19:43:51.698,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 13 09:51:44 UTC 2018,,,,,,0|i3zkr3:,9223372036854775807,4.0,,,,djoshi3,djoshi3,,,,,,,,,,,24/Oct/18 09:37;tommy_s;Server encryption was disabled when I tested this.,"26/Oct/18 19:43;aweisberg;[~iamaleksey] in IRC
{noformat}
10:29 AM nope, that's safe, assuming you mean 'system'; 'system_schema' is also fine. things in system_distributed and system_traces is in the danger zone
10:29 AM ok, I see the problem
10:33 AM don't necessarily need new tables, but you can't just add a column to one of those, no
10:33 AM what I'm not seeing is what is doing the read
10:33 AM nothing in the code does it as far as I can see
10:33 AM code only writes
10:34 AM which would also be a problem if a writes comes from a 4.0 node in the mixed-mode
10:35 AM also do: read the comment for StorageService.maybeAddOrUpdateKeyspace()
10:35 AM and MigrationManager.forceAnnounceNewTable() that it calls
10:37 AM on startup 4.0 detects that the keyspace and the table exist, but the definitions are different, so it tries to force the migration with the new definition; that however will not be propagated to 3.x nodes because there is a schema barrier in place between different major-version nodes
10:38 AM that works as intended more or less, just annoying. although at least one problem is that the same 0 timestamp is reused, and for conflicting table params the new definition might never actually be applied depending on old/new value
10:39 AM that timestamp needs to be incremented on every major
10:39 AM I mentioned this before in one of the JIRAs
10:40 AM aweisberg: TL;DR is that reads and writes involving the new column in 4.0 will fail on 3.0 side. not much you can do about it here
10:40 AM could avoid using those columns until the whole cluster is on the latest major
{noformat}

We don't support cross version streaming so people can't run repair anyways. At least not unless they are fairly sophisticated about how they invoke repair and what order they bounce nodes. They also probably can't be using vnodes. Maybe it's fine if this generates an error until the cluster is upgraded?

[~tommy_s] is it possible you have an external process that is reading from the repair history table that is causing this error to appear?

WRT to tracing. Creating a new table and writing to both seems a little sketchy since it will double the overhead of tracing even when not upgrading. Upgrade would also need to do a migration of data from the old table into the new table, and we can only perform the migration once all nodes are upgraded. I suppose after the cluster is not mixed version we could stop writing to the old table.

I'm not sure if the fix here should be documentation in NEW.TXT or documentation + changes to avoid generating errors in mixed version clusters by not doing the writes.","26/Oct/18 19:54;jjirsa;Would personally prefer not generating errors in mixed versions. A lot of people log or alert on errors, and we know that we CAN avoid this, so we SHOULD. We've done far more impactful workarounds in the past (see: 2.x -> 3.0 schema upgrades and 2.1 -> 2.2 system_auth changes), so going down that path seems like the right approach.","26/Oct/18 20:21;tommy_s;[~aweisberg] We have an external repair scheduler that reads the repair history, I think I forgot to disable that before starting to upgrade. I didn't make the connection when I saw the exception but your probably right.","29/Oct/18 19:32;aweisberg;[trunk code|https://github.com/apache/cassandra/compare/trunk...aweisberg:14841-trunk?expand=1]
[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14841-trunk]",31/Oct/18 22:58;djoshi3;+1 LGTM,"01/Nov/18 14:21;tommy_s;But this means I can't read the repair history while I have mixed versions, It seams to work from the old version but using cqlsh on the 4.0 node I get this:
{noformat}
cassandra@cqlsh> select keyspace_name , columnfamily_name, coordinator FROM system_distributed.repair_history LIMIT 1 ;
OperationTimedOut: errors={'10.216.193.242': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=10.216.193.242:12742
cassandra@cqlsh> select * FROM system_distributed.repair_history LIMIT 1 ;
OperationTimedOut: errors={'10.216.193.242': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=10.216.193.242:12742{noformat}
and in the log on the old version I get the exception:
{noformat}
2018-11-01T14:28:23.623+0100 [MessagingService-Incoming-/10.216.193.242] ERROR o.a.c.service.CassandraDaemon$2:223 uncaughtException Exception in thread Thread[MessagingService-Incoming-/10.216.193.242,5,main]
java.lang.RuntimeException: Unknown column coordinator_port during deserialization
at org.apache.cassandra.db.Columns$Serializer.deserialize(Columns.java:433) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.db.filter.ColumnFilter$Serializer.deserialize(ColumnFilter.java:447) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:661) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:598) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.17.jar:3.0.17]
at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.17.jar:3.0.17]{noformat}",01/Nov/18 14:52;aweisberg;Yes you won't be able to get repair_history while it is mixed version. We don't support cross version streaming so in a mixed version cluster you can't run repair anyways. Will that be an issue?,"01/Nov/18 15:13;tommy_s;Maybe I'm just picky. I agree we can't run repair in mixed version but does that mean I should not read repair history?

Will this be an issue? Maybe not. In our case we have a system for monitoring the status of the cluster which among other things reads from the repair history but I guess we could make it possible to disable that during upgrade. Monitoring the state of repairs while having mixed versions doesn't make much sens anyway.","01/Nov/18 15:40;aweisberg;[~tommy_s] I polled [~iamaleksey] and [~jjirsa] and neither was in favor of having the read path automatically recognize this scenario and remove the column.

The recommendation is to avoid using ""select *"" which is dangerous if you are accessing by index since columns may get silently re-ordered as new columns are added. I will add a comment to NEWS.txt warning people about this.",01/Nov/18 16:13;aweisberg;Committed as [877b08eaf0e02542c9f6d9f8cd457a8e44b4febf|https://github.com/apache/cassandra/commit/877b08eaf0e02542c9f6d9f8cd457a8e44b4febf]. Thanks!,"01/Nov/18 19:47;tommy_s;[~aweisberg] What you say makes good sens, I'm satisfied. Thanks.","02/Nov/18 15:27;aweisberg;Tommy pointed out that due to CASSANDRA-6588 even if you select specific columns the queries will fail. I don't think that changes the outcome we will aim for, but I will have to update the warning in NEWS.txt.","06/Nov/18 11:49;tommy_s;I think there is another scenario we have to consider, if we have several DC's its possible to do repair with in one dc and we would only need the nodes within that DC to be on the same version, if there are another version in another DC it should not be a problem.

And its also possible to create the new columns manually before starting the upgrade, in this case both reads and writes will work.

So just blocking the writes while having mixed versions might not be so good. Its only in the DC I'm currently upgrading that I can't run repairs but I would not be able to monitor the repair status in DC's with the new version.

I think we should make it possible to override the blocking with some configuration so you can decide to write repair history despite having mixed versions. Or remove the blocking of writes and document in the NEWS.txt how to create the new columns before starting to upgrade.","06/Nov/18 19:15;aweisberg;Why would this block repairs and why would it stop you from monitoring repairs? Wouldn't you use nodetool repair admin to monitor repairs?

For repair it is just a history table. It's only populated after repair has already occurred.

I'm not sure users can alter the schema of the distributed system tables. @imaleksey is that actually possible?","06/Nov/18 20:19;tommy_s;We don't use nodetool for repair, we have a repair scheduler that triggers repair jobs via jmx. The scheduling is based on repair history, checking for failed repairs that needs to be retried, how long ago something was repaired and if something is completely missing it has not been repaired at all. There is no big issue shunting down this for a while during an upgrade but I don't want to do that for a longer time then a need to, if I can do it per DC it would help.

Yes, you can alter the schema of the system distributed tables. Previously you could just do {{ALTER TABLE}} but since a few versions back its a little bit more complected, now you have to do {{INSERT}} in the system schema tables and use {{nodetool reloadlocalschema}}.",12/Nov/18 16:39;aweisberg;Aleksey pointed out that in mixed version clusters we could still write to the tables and just omit the port column. Then you could add the columns and be able to read from the table. [~tommy_s] can you add as a comment a description of the steps to add the column?,"13/Nov/18 09:51;tommy_s;[~aweisberg] here is the steps to add the new columns before you start to upgrade.

For 3.0.15 and 3.11.1 and older versions:
{noformat}
cqlsh> ALTER TABLE system_distributed.repair_history ADD coordinator_port int;
cqlsh> ALTER TABLE system_distributed.repair_history ADD participants_v2 set<text>;{noformat}
For 3.0.16 and 3.11.2 and newer:
{noformat}
cqlsh> INSERT INTO system_schema.columns (keyspace_name , table_name , column_name , clustering_order , column_name_bytes , kind , position , type ) VALUES ( 'system_distributed', 'repair_history', 'coordinator_port', 'none', 0x636f6f7264696e61746f725f706f7274, 'regular', -1, 'int');
cqlsh> INSERT INTO system_schema.columns (keyspace_name , table_name , column_name , clustering_order , column_name_bytes , kind , position , type ) VALUES ( 'system_distributed', 'repair_history', 'participants_v2', 'none', 0x7061727469636970616e74735f7632, 'regular', -1, 'set<text>');
cqlsh> exit
$ nodetool reloadlocalschema{noformat}
Remember that the INSERT's and {{nodetool reloadschema}} must be done on the same node.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test stdout capture is malfunctioning in 4.0,CASSANDRA-14974,13208989,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,11/Jan/19 12:22,12/Mar/19 14:16,13/Mar/19 22:35,15/Jan/19 19:14,4.0,,,,Test/dtest,Test/unit,,,0,,,,"In 3.x unit tests we make sure to capture stdout to the unit test files, in case tests log to stdout and not to a logger.  However, in 4.0 due to a configuration parameter that is deprecated in logback, the logic is short-circuited silently.

Once fixed, this affects the cleanup of in-jvm dtests which would register an infinite chain of System.out overrides (one for each node), so we make the functionality robust to multiple instantiations, as well as improve its startup/shutdown sequence guarantees.",,,,,,,,,,,,,,,CASSANDRA-14922,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-01-15 13:50:59.061,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 21 14:20:13 UTC 2019,,,,,,0|u00qrc:,9223372036854775807,,,,,,,,,,,,,,,,,"11/Jan/19 14:58;benedict;[patch|https://github.com/belliottsmith/cassandra/tree/14974], [CI|https://circleci.com/workflow-run/e834041e-6361-4435-a3bc-2720aa5337a9]","15/Jan/19 13:50;ifesdjeen;+1, just need to re-run {{DatabaseDescriptorRefTest}} ","15/Jan/19 19:14;benedict;Thanks, committed as [a43b651f8e35dd7081b8593057f118ed0c49cfd6|https://github.com/apache/cassandra/commit/a43b651f8e35dd7081b8593057f118ed0c49cfd6]","21/Jan/19 14:20;ifesdjeen;Not sure I should reopen it, but it looks like we might have missed something. Currently, it looks like we double-wrap subsitute logger which results into:

{code}
INFO  [AsyncAppender-Worker-ASYNC] 2019-01-21 14:41:01,193 SubstituteLogger.java:169 - DEBUG [MemtableFlushWriter:1] INSTANCE_127.0.0.3 2019-01-21 14:41:01,193 ColumnFamilyStore.java:1153 - Flushed to [BigTableReader(path='/private/var/folders/d_/t6f7wkp53g7bf1pcnm69ljwc0000gn/T/dtests8830319164832856679/node3/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/na-2-big-Data.db')] (1 sstables, 5.155KiB), biggest 5.155KiB, smallest 5.155KiB

INFO  [AsyncAppender-Worker-ASYNC] 2019-01-21 14:41:01,196 SubstituteLogger.java:169 - DEBUG [MemtableFlushWriter:1] INSTANCE_127.0.0.2 2019-01-21 14:41:01,196 ColumnFamilyStore.java:1153 - Flushed to [BigTableReader(path='/private/var/folders/d_/t6f7wkp53g7bf1pcnm69ljwc0000gn/T/dtests8830319164832856679/node2/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/na-2-big-Data.db')] (1 sstables, 5.155KiB), biggest 5.155KiB, smallest 5.155KiB

INFO  [AsyncAppender-Worker-ASYNC] 2019-01-21 14:41:01,226 SubstituteLogger.java:169 - INFO  [MigrationStage:1] INSTANCE_127.0.0.2 2019-01-21 14:41:01,226 Keyspace.java:368 - Creating replication strategy distributed_test_keyspace params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=3}}
{code}

(notice double timestamp, location and log level). If we avoid re-wrapping substitute logger, this won't happen. We need to fix it either here or in multi-JVM patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid querying “self” through messaging service when collecting full data during read repair,CASSANDRA-14807,13189735,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,05/Oct/18 15:39,12/Mar/19 14:16,13/Mar/19 22:35,12/Oct/18 17:05,,,,,Legacy/Coordination,Legacy/Local Write-Read Paths,,,0,,,,"Currently, when collecting full requests during read-repair, we go through the messaging service instead of executing the query locally.

||[patch|https://github.com/apache/cassandra/pull/278]||[dtest-patch|https://github.com/apache/cassandra-dtest/pull/39]||

|[utest|https://circleci.com/gh/ifesdjeen/cassandra/641]|[dtest-vnode|https://circleci.com/gh/ifesdjeen/cassandra/640]|[dtest-novnode|https://circleci.com/gh/ifesdjeen/cassandra/639]|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-10-11 16:38:08.456,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 11 16:53:42 UTC 2018,,,,,,0|i3yvtj:,9223372036854775807,,,,,aweisberg,aweisberg,,,,,,,,,,,11/Oct/18 16:38;aweisberg;+1. My review comments are in the PRs.,"11/Oct/18 16:53;ifesdjeen;Thank you for the review!

[Latest test run|https://circleci.com/workflow-run/68b29a4d-e72e-4664-a2dc-92b392b59a1c]

Committed to trunk with [f24e23c5f42f27cf74297e5c12de370fc6a724bc|https://github.com/apache/cassandra/commit/f24e23c5f42f27cf74297e5c12de370fc6a724bc]

[dtest commit|https://github.com/apache/cassandra-dtest/commit/74f578abe03fd004f9ffe26868e76f63949cedec]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid calling iter.next() in a loop when notifying indexers about range tombstones,CASSANDRA-14794,13187833,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,27/Sep/18 09:03,12/Mar/19 14:15,13/Mar/19 22:35,28/Sep/18 10:57,3.0.18,3.11.4,4.0,,Legacy/Local Write-Read Paths,,,,0,,,,In [SecondaryIndexManager|https://github.com/apache/cassandra/blob/914c66685c5bebe1624d827a9b4562b73a08c297/src/java/org/apache/cassandra/index/SecondaryIndexManager.java#L901-L902] - avoid calling {{.next()}} in the {{.forEach(..)}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-27 16:58:47.964,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 28 10:57:25 UTC 2018,,,,,,0|i3yk4n:,9223372036854775807,,,,,,beobal,ifesdjeen,,,,,,,,,,"27/Sep/18 16:17;krummas;https://github.com/krummas/cassandra/commits/marcuse/14794
tests: https://circleci.com/workflow-run/8413c7d8-bd59-4c78-8ae8-7d529c55ab1f","27/Sep/18 16:58;ifesdjeen;+1, thank you for the patch!",28/Sep/18 08:31;beobal;+1,"28/Sep/18 10:57;krummas;and committed as {{30d2835809e119173b1124b3eecb134e3a8c19b6}} to 3.0 and merged up, thanks!

tests for the other branches: [3.11|https://circleci.com/workflow-run/6af0ef35-4be8-4f7c-8a8a-30ccaddc7b1f] [trunk|https://circleci.com/workflow-run/ff02648a-a795-4af1-a742-9673acccde41]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming failure during bootstrap makes new node into inconsistent state,CASSANDRA-14525,13166262,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,15/Jun/18 04:42,12/Mar/19 14:15,13/Mar/19 22:35,31/Dec/18 06:19,2.2.14,3.0.18,3.11.4,4.0,Legacy/Core,,,,0,,,,"If bootstrap fails for newly joining node (most common reason is due to streaming failure) then Cassandra state remains in {{joining}} state which is fine but Cassandra also enables Native transport which makes overall state inconsistent. This further creates NullPointer exception if auth is enabled on the new node, please find reproducible steps here:

For example if bootstrap fails due to streaming errors like
{quote}java.util.concurrent.ExecutionException: org.apache.cassandra.streaming.StreamException: Stream failed
 at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[guava-18.0.jar:na]
 at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:1256) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:894) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.StorageService.initServer(StorageService.java:660) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.StorageService.initServer(StorageService.java:573) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:330) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:567) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:695) [apache-cassandra-3.0.16.jar:3.0.16]
 Caused by: org.apache.cassandra.streaming.StreamException: Stream failed
 at org.apache.cassandra.streaming.management.StreamEventJMXNotifier.onFailure(StreamEventJMXNotifier.java:85) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202) ~[guava-18.0.jar:na]
 at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:211) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:187) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:440) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:540) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:307) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{quote}
then variable [StorageService.java::dataAvailable |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L892] will be {{false}}. Since {{dataAvailable}} is {{false}} hence it will not call [StorageService.java::finishJoiningRing |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L933] and as a result [StorageService.java::doAuthSetup|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L999] will not be invoked.

API [StorageService.java::joinTokenRing |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L763] returns without any problem. After this [CassandraDaemon.java::start|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/CassandraDaemon.java#L584] is invoked which starts native transport at 
 [CassandraDaemon.java::startNativeTransport |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/CassandraDaemon.java#L478]

At this point daemon’s bootstrap is still not finished and transport is enabled. So client will connect to the node and will encounter {{java.lang.NullPointerException}} as following:
{quote}ERROR [SharedPool-Worker-2] Message.java:647 - Unexpected exception during request; channel = [id: 0x412a26b3, L:/a.b.c.d:9042 - R:/p.q.r.s:20121]
 java.lang.NullPointerException: null
 at org.apache.cassandra.auth.PasswordAuthenticator.doAuthenticate(PasswordAuthenticator.java:160) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.auth.PasswordAuthenticator.authenticate(PasswordAuthenticator.java:82) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.auth.PasswordAuthenticator.access$100(PasswordAuthenticator.java:54) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.auth.PasswordAuthenticator$PlainTextSaslAuthenticator.getAuthenticatedUser(PasswordAuthenticator.java:198) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:78) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:535) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:429) [apache-cassandra-3.0.16.jar:3.0.16]
 at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.1.0.CR6.jar:4.1.0.CR6]
 at io.netty.channel.ChannelHandlerInvokerUtil.invokeChannelReadNow(ChannelHandlerInvokerUtil.java:83) [netty-all-4.1.0.CR6.jar:4.1.0.CR6]
 at io.netty.channel.DefaultChannelHandlerInvoker$7.run(DefaultChannelHandlerInvoker.java:159) [netty-all-4.1.0.CR6.jar:4.1.0.CR6]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.16.jar:3.0.16]
 at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{quote}
At this point if we run {{nodetool status}} then it will show this new node in {{UJ}} state, however clients can connect to this node over {{CQL}} and will receive {{java.lang.NullPointerException}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-15 11:38:34.725,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 10 06:14:26 UTC 2019,,,,,,0|i3uwav:,9223372036854775807,,,,,KurtG,KurtG,,,,,,,,,,,"15/Jun/18 05:11;chovatia.jaydeep@gmail.com;In my opinion daemon should enable native transport only after successful bootstrap to avoid inconsistent state.

Please find patch with the fix here:
||trunk||3.0||2.x||
|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/76]|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/73]  |  [!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/74]|
|[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-trunk] |[patch|https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-3.0] |[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-2.2] |

Please review it and let me know your opinion.

 ","15/Jun/18 11:38;jasobrown;This is not a review, but you should have some dtests to validate your patch.","15/Jun/18 11:45;chovatia.jaydeep@gmail.com;Sure [~jasobrown] I will add a dTest, created sub-task CASSANDRA-14526","18/Jun/18 05:07;chovatia.jaydeep@gmail.com;[~jasobrown] I've added a dtest for this which passes with this fix and fails on current trunk, please find dtest here:
||dtest||
|[patch |https://github.com/apache/cassandra-dtest/compare/master...jaydeepkumar1984:14526-trunk]|

 ",18/Jun/18 11:15;KurtG;We've already had a ticket (and a _very_ similar patch) for this since November last year... CASSANDRA-14063 ,"18/Jun/18 15:43;chovatia.jaydeep@gmail.com;I see [~KurtG], sorry I missed it. In my opinion this is a bug and needs to be fixed (CASSANDRA-14063 should be considered as per FCFS priority). We also need to fix dtest along with this so CASSANDRA-14526 also needs to be landed.","19/Jun/18 05:20;KurtG;Thanks [~chovatia.jaydeep@gmail.com], I agree. No fault on your part, more a problem with the consistent lack of reviewers we have who can prioritise review work. Just unfortunate that it's wasted more time than necessary for everyone. I think [~VincentWhite] would appreciate the acknowledgement (especially after such a long time) so FCFS makes sense to me, but there's no use doing the work twice, just take into account the two patches slight discrepancies when reviewing I guess.","19/Jun/18 18:33;chovatia.jaydeep@gmail.com;I agree there are few discrepancy between two patches as following:
 My patch:
 - It was missing check before starting RPC server. I've incorporated this and my latest patch has this.

[~VincentWhite]'s patch:
 - Missing to start {{cql}} post successful {{bootstrap}}
 - Not sure if we need to use separate variable {{streamingSuccessful}} as we can directly use {{SystemKeyspace.bootstrapComplete()}}
 - We should have consistent error message _{{node is not yet bootstraped completely. Use nodetool to check bootstrap state and resume. For more, see nodetool help bootstrap}}_
 - We should make {{isSurveyMode}} volatile as it is being updated by different thread
 - Need to make sure we get patches for all branches and verify CASSANDRA-14526

[~VincentWhite] Do you want to make a complete patch by incorporating above items?

 

[~KurtG] Does this sound ok to you?","22/Jun/18 17:34;chovatia.jaydeep@gmail.com;Hi [~KurtG]

It seems [~VincentWhite] is not responding, could you please review patch from this ticket?

Jaydeep","25/Jun/18 04:38;KurtG;Sorry about that, we had a pretty busy week last week and Vince probably won't have time. I'll review.","26/Jun/18 05:27;KurtG;A few things:
 If we fail streaming and {{isSurveyMode}} is true we still get the NPE if auth is enabled when trying to connect to C* on that node. Not much we can do about this because auth isn't initialised until we join the ring, but I'm not sure why we should handle this situation differently, and also it's currently kind of broken. At the moment if you resume bootstrap after a streaming failure _while in write survey mode_, you will leave write survey mode on completion of bootstrapping (ouch).


 I think we should handle write survey bootstrapping the same as normal bootstrap, where if we get an error during streaming we don't start transports. Then, on resume, handle survey mode so that we _don't_ join the ring on completion of bootstrapping, but we do still start transports.


 On top of that, seeing as we're in this code anyway, I think it would be reasonable if we could look at handling the auth case a bit better when write survey is enabled as well. Ideally, if auth is required I see no point in starting the transports seeing as you'll always get an NPE, so maybe we can add a check for that in {{CassandraDaemon#start()?}}

{{DatabaseDescriptor.getAuthenticator().requireAuthentication()}} should be enough here I think.

 

Some things regarding the error message:
 We've got repeated information in our error:
{code:java}
WARN  [main] 2018-06-25 09:13:24,136 StorageService.java:935 - Some data streaming failed. Use nodetool to check bootstrap state and resume. For more, see `nodetool help bootstrap`. IN_PROGRESS
ERROR [main] 2018-06-25 09:13:32,190 CassandraDaemon.java:445 - Node is not yet bootstraped hence not enabling native transport. Use nodetool to check bootstrap state and resume. For more, see `nodetool help    bootstrap`
{code}
I think our new message should either be INFO or WARN (INFO is in line with other messages in {{start()}}, and I think it would make more sense if the original message in \{{StorageService}} was ERROR. We could change the message in CassandraDaemon to:
{code:java}
Not starting client transports as bootstrap has not completed.{code}
or something similar, to be more in line with the other info messages.

Finally, with your patch if we resume bootstrap we don't start thrift. As per Vince's patch, daemon.start() is desirable here over startNativeTransport so that we always start thrift and CQL.","26/Jun/18 23:33;chovatia.jaydeep@gmail.com;[~KurtG] Thanks for your review comments. I've made patch simple by not allowing transport if bootstrap is not complete irrespective of {{survey}} mode. Also incorporated other comments, please find updated patch here:

Please find patch with the fix here:
||trunk||3.0||2.x||
|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/76]|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/73]  |  [!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/74]|
|[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-trunk] |[patch|https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-3.0] |[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-2.2] |
","27/Jun/18 10:07;KurtG;Mostly looks good, however we're still leaving write_survey mode after a resume bootstrap completes when we were started in write survey mode. Also just noticed but as we hackily re-use isSurveyMode when resuming a bootstrap we always log the following message regardless of if we were in write survey mode originally or not.
{code}Leaving write survey mode and joining ring at operator request{code}
I think at this point we could solve these 2 problems by simply calling {{finishJoiningRing}} explicitly when we successfully bootstrap after a resume in {{resumeBootstrap}}, rather than indirectly through {{joinRing}, and also handle write_survey in the same place.

Also, another small nit, can we change the spelling of {{bootstraped}} to {{bootstrapped}} in the exception messages?","28/Jun/18 02:55;chovatia.jaydeep@gmail.com;[~KurtG] I think there was a bug in my previous patch in which it will not start native transport in normal scenario if {{isSurveyMode}} is {{true}}. 
 Also I've discovered another bug exists in current open source code in which if {{isSurveyMode}} is {{true}} and streaming fails (i.e. {{isBootstrapMode}} is {{true}}) then also one can call {{nodetool join}} without {{nodetool bootstrap resume}} and have that node join the ring.

 

I've taken care of this bug and your review comments,please find updated patch here:
||trunk||3.0||2.x||
|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/76]|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/73]  |  [!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/74]|
|[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-trunk] |[patch|https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-3.0] |[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-2.2] |","02/Jul/18 08:22;KurtG;{quote}Also I've discovered another bug exists in current open source code in which if isSurveyMode is true and streaming fails (i.e. isBootstrapMode is true) then also one can call nodetool join without nodetool bootstrap resume and have that node join the ring.
{quote}
Great catch. I found a couple more small issue w.r.t {{nodetool join}} as well while I was testing this.
 # If in write_survey and you join the ring after bootstrap, transports won't be enabled. can we call {{CassandraDaemon#start()}} here?
 # nodetool join fails silently if write_survey is true and we haven't completed bootstrapping, but server log prints the following
{code:java}
WARN [RMI TCP Connection(5)-127.0.0.1] 2018-06-29 12:39:49,735 StorageService.java:1008 - Some data streaming failed. Use nodetool to check bootstrap state and resume. For more, see `nodetool help bootstrap`. IN_PROGRESS
{code}
nodetool join should say something along the lines of ""{{Can't join the ring because in write_survey mode and bootstrap hasn't completed}}""

Also another minor nit w.r.t logging; you can get the following log message after successfully bootstrapping if you were in write survey mode:
{code:java}
INFO [main] 2018-06-29 12:12:39,071 CassandraDaemon.java:479 - Not starting client transports as bootstrap has not completed
{code}
Probably better to split CassandraDaemon.start() if block so that we print ""{{Not starting client transports as write_survey mode is enabled.}}""

And finally, there's still 2 occurences of ""bootstraped"" in the exception messages in {{startNativeTransport}} and {{startRPCServer}}.","02/Jul/18 22:18;chovatia.jaydeep@gmail.com;[~KurtG] Please find updated review with all of the review comments incorporated.
||trunk||3.0||2.x||
|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/76]|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/73]  |  [!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/74]|
|[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-trunk] |[patch|https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-3.0] |[patch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-2.2] |","05/Jul/18 11:31;KurtG;Thanks [~chovatia.jaydeep@gmail.com]. I figured it was unfair making you do all the work so I made some changes myself. Basically we were still silently failing with {{nodetool join}}, so I've added {{isBootstrapMode()}} to {{StorageServiceMBean}} and {{NodeProbe}} and added a check for it in {{Join.java}}. I've also tried to simplify some of the if statements (edge cases are killing me) and added a bit of documentation. I also created a 3.11 branch from our changes as there were a few conflicts from 3.0.

Finally, I extended your dtest from CASSANDRA-14526 to cover the write_survey, join and resume cases to be in line with the changes we've made, and also changed the byteman rule to trigger on {{maybeCompleted()}} as it wasn't consistently triggering on {{startStreamingFiles()}}

Now I think we're pretty much ready to go, but if you want to give my changes a once over to make sure I haven't missed anything as I'm sure you're aware the startup code is an absolute nightmare.

Also, I haven't run the whole dtest suite, but I've at least run all the bootstrap dtests and they are all passing, so I'm taking that as a good sign.
||2.2||3.0||3.11||trunk||dtests||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...kgreav:14525-2.2]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14525-3.0]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14525-3.11]|[branch|https://github.com/apache/cassandra/compare/trunk...kgreav:14525-trunk]|[branch|https://github.com/apache/cassandra-dtest/compare/master...kgreav:14526-trunk-k]|
|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/178]|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/181]|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-3.11.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/180]|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/183]|","05/Jul/18 17:47;chovatia.jaydeep@gmail.com;[~KurtG] Thanks for making the changes! 

Latest code changes looks good to me.","14/Jul/18 11:01;KurtG;So I messed up a merge to 3.11 and upon doing further dtests found some issues and inefficiencies. These have all been fixed up now and branches above should be all up to date and passing tests so going to mark this as ready to commit.
||2.2||3.0||3.11||trunk||dtests||
|[branch|https://github.com/apache/cassandra/compare/cassandra-2.2...kgreav:14525-2.2]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:14525-3.0]|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:14525-3.11]|[branch|https://github.com/apache/cassandra/compare/trunk...kgreav:14525-trunk]|[branch|https://github.com/apache/cassandra-dtest/compare/master...kgreav:14526-trunk]|
|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/202]|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/199]|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-3.11.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/201]|[!https://circleci.com/gh/kgreav/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/kgreav/cassandra/200]|",19/Jul/18 23:21;djoshi3;[~KurtG] - I pulled your branch and have run the dtests with some failures. See [here.|https://circleci.com/workflow-run/035dca04-5b2b-4fb2-aebe-3089d876f995],"20/Jul/18 04:45;KurtG;Thanks heaps [~djoshi3], I'll look into it.","28/Aug/18 07:12;chovatia.jaydeep@gmail.com;[~KurtG] I did analyze all the 9 failing dtests from [~djoshi3]'s run and found that they all expect CQL up and running w/o bootstrap, and as per our new design in this ticket, it is simply not allowed.

So I believe we simply have to remove those 9 dtests. Please check and let me know if you need my help anywhere.

Thanks [~djoshi3] for doing a dtest run for us!","28/Aug/18 12:15;KurtG;Thanks so much. I have been meaning to get back to this but since we missed the boat on 3.11.3 I figured this could wait until Sept 1st. If you want you can go ahead and fix the tests, otherwise if not I'll have a look next week :)","29/Aug/18 05:26;chovatia.jaydeep@gmail.com;[~KurtG] I've fixed the failing dtests by changing some part in Cassandra and some part in dtests. Please review them whenever you get a chance. Thank You!
||2.2||3.0||3.11||trunk||dtests||
|[branch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-2.2] |[branch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-3.0] |[branch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-3.11] |[branch |https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-trunk]|[branch |https://github.com/apache/cassandra-dtest/compare/master...jaydeepkumar1984:14526-trunk]|
|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-2.2.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/=132]|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-3.0.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/=133]  |  [!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-3.11.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/=128]|[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/14525-trunk.svg?style=svg! |https://circleci.com/gh/jaydeepkumar1984/cassandra/=129]|",19/Sep/18 04:45;KurtG;Looks good to me. Probably need another run of the dtests though :/,26/Oct/18 23:42;chovatia.jaydeep@gmail.com;[~jay.zhuang] [~djoshi3] Can one of you please help run a final round of dtest so that this ticket can be committed and closed? ,"27/Oct/18 21:03;jay.zhuang;Sure, I'll kick off the tests.","31/Oct/18 05:48;djoshi3;Sorry, did not see your comment earlier. I think we should update the dtests rather than removing them. ","21/Dec/18 22:43;jay.zhuang;Rebased the code and started the tests:
| Branch | uTest | dTest |
| [14525-2.2|https://github.com/cooldoger/cassandra/tree/14525-2.2] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14525-2.2.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14525-2.2] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/664/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/664/] |
| [14525-3.0|https://github.com/cooldoger/cassandra/tree/14525-3.0] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14525-3.0.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14525-3.0] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/665/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/665/] |
| [14525-3.11|https://github.com/cooldoger/cassandra/tree/14525-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14525-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14525-3.11] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/666/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/666/] |
| [14525-trunk|https://github.com/cooldoger/cassandra/tree/14525-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14525-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14525-trunk] | [!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/667/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/667/] |","28/Dec/18 20:04;jay.zhuang;The uTest failure is because CASSANDRA-14946. The check conditions are hard to read, how about switching it from:
{noformat}
        // We only start transports if bootstrap has completed and we're not in survey mode, OR if we are in
        // survey mode and streaming has completed but we're not using auth.
        // OR if we have not joined the ring yet.
        if (StorageService.instance.hasJoined() &&
                ((!StorageService.instance.isSurveyMode() && !SystemKeyspace.bootstrapComplete()) ||
                (StorageService.instance.isSurveyMode() && StorageService.instance.isBootstrapMode())))
        {
            logger.info(""Not starting client transports as bootstrap has not completed"");
            return;
        }
        else if (StorageService.instance.hasJoined() &&  StorageService.instance.isSurveyMode() &&
                DatabaseDescriptor.getAuthenticator().requireAuthentication())
        {
            // Auth isn't initialised until we join the ring, so if we're in survey mode auth will always fail.
            logger.info(""Not starting client transports as write_survey mode and authentication is enabled"");
            return;
        }
{noformat}
to:
{noformat}
        // Do not start the transports if we already joined the ring AND
        // if we are in survey mode, streaming has not completed or auth is enabled
        // if we are not in survey mode, bootstrap has not completed
        if (StorageService.instance.hasJoined())
        {
            if (StorageService.instance.isSurveyMode())
            {
                if (StorageService.instance.isBootstrapMode() ||
                    DatabaseDescriptor.getAuthenticator().requireAuthentication())
                {
                    logger.info(""Not starting client transports in write_survey mode as it's bootstrapping or auth is enabled"");
                    return;
                }
            }
            else
            {
                if (!SystemKeyspace.bootstrapComplete()) {
                    logger.info(""Not starting client transports as bootstrap has not completed"");
                    return;
                }
            }
        }
{noformat}","29/Dec/18 00:07;chovatia.jaydeep@gmail.com;Thanks [~jay.zhuang] for the review, I've incorporated the changes. Please find latest patch details here:
||Branch ||uTest ||
|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...jaydeepkumar1984:14525-2.2?expand=1] |[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/bqr.svg?style=svg! | https://circleci.com/gh/jaydeepkumar1984/cassandra/161]  |
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...jaydeepkumar1984:14525-3.0?expand=1] |[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/bqr.svg?style=svg! | https://circleci.com/gh/jaydeepkumar1984/cassandra/160]  |
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...jaydeepkumar1984:14525-3.11?expand=1] |[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/bqr.svg?style=svg! | https://circleci.com/gh/jaydeepkumar1984/cassandra/162]  |
|[trunk|https://github.com/apache/cassandra/compare/trunk...jaydeepkumar1984:14525-trunk?expand=1] |[!https://circleci.com/gh/jaydeepkumar1984/cassandra/tree/bqr.svg?style=svg! | https://circleci.com/gh/jaydeepkumar1984/cassandra/164]  |","29/Dec/18 01:41;jay.zhuang;Hi, just a question : {{SystemKeyspace.bootstrapComplete()}} is checked here: [https://github.com/apache/cassandra/commit/9c3fb65e697d810321936e06504de4b2f7cf633f#diff-b76a607445d53f18a98c9df14323c7ddR392]

But not here: [https://github.com/apache/cassandra/commit/9c3fb65e697d810321936e06504de4b2f7cf633f#diff-b76a607445d53f18a98c9df14323c7ddR351]

Is that expected?","30/Dec/18 22:14;chovatia.jaydeep@gmail.com;Yes [~jay.zhuang] that is expected, because {{isSurveyMode=true}} mode and {{SystemKeyspace.bootstrapComplete()=true}} does not co-exist

e.g. 
 [If in survey mode then bootstrap will not be COMPLETE |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L929]
 [If we give-up survey mode then bootstrap will be COMPLETE |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L985]

So this check {{!SystemKeyspace.bootstrapComplete()}} at [this line|https://github.com/apache/cassandra/commit/9c3fb65e697d810321936e06504de4b2f7cf633f#diff-b76a607445d53f18a98c9df14323c7ddR392] was redundant which is been removed in the latest diffs.",31/Dec/18 06:17;jay.zhuang;Thanks [~chovatia.jaydeep@gmail.com] and [~KurtG]. Committed as [{{a6196a3}}|https://github.com/apache/cassandra/commit/a6196a3a79b67dc6577747e591456328e57c314f].,31/Dec/18 19:11;chovatia.jaydeep@gmail.com;Thanks [~jay.zhuang] [~KurtG] [~djoshi3],08/Jan/19 21:49;aweisberg;This breaks bootstrap_test.py:TestBootstrap.test_resumable_bootstrap. The test expects the cluster to start the native interface when bootstrap fails.,"08/Jan/19 21:52;aweisberg;I think test_resume secondary_indexes_test.py:TestPreJoinCallback.test_resume has the same issue.
","08/Jan/19 22:48;chovatia.jaydeep@gmail.com;[~aweisberg] I've already taken care of dests as part of https://issues.apache.org/jira/browse/CASSANDRA-14526, here is the [patch for dtest|https://github.com/apache/cassandra-dtest/compare/master...jaydeepkumar1984:14526-trunk]. Not sure if [~jay.zhuang] got a chance to fire dtest, if possible could you please help me start dtest with this patch?","09/Jan/19 16:47;aweisberg;OK, it's better to avoid committing any dtest breakage. If there are 10 developers occasionally committing dtest breakage then each of them has to look at the current set of breakage and figure out if it was their changes that are causing issues or some other set of changes.

And I am very aware I am in a glass house when I say this!

If you want to break things into multiple JIRAs or commits it's fine, but it's less disruptive if you wait until everything is complete before landing any piece of it.

If Jay is reviewing the dtest changes I'll kick it off a dtest run, but let him finish the review.","10/Jan/19 02:21;chovatia.jaydeep@gmail.com;Apologize for not having both the fixes landed at the same time, will take utmost care in the future.

[~jay.zhuang] If you have sometime then could you please validate the dtest and land it if it looks fine?",10/Jan/19 06:14;jay.zhuang;I'm sorry for not committing the dtest change. Will do that ASAP (I'm still trying to confirm a few flaky tests are not introduced by the changes).,,,,,,,
Dropped columns can cause reverse sstable iteration to return prematurely,CASSANDRA-14838,13193659,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,23/Oct/18 19:45,12/Mar/19 14:15,13/Mar/19 22:35,29/Oct/18 02:45,3.0.18,3.11.4,4.0,,Legacy/Local Write-Read Paths,,,,0,,,,"CASSANDRA-14803 fixed an issue where reading legacy sstables in reverse could return early in certain cases. It's also possible to get into this state with current version sstables if there are 2 or more indexed blocks in a row that only contain data for a dropped column. Post 14803, this will throw an exception instead of returning an incomplete response, but it should just continue reading like it does for legacy sstables",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-10-26 10:41:26.234,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 29 02:45:36 UTC 2018,,,,,,0|i3zjvj:,9223372036854775807,,,,,beobal,beobal,,,,,,,,,,,"23/Oct/18 19:53;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/14838-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14838-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/14838-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14838-3.11]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/14838-trunk]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14838-trunk]|",26/Oct/18 10:41;beobal;+1. Only nit is the unnecessary throws clauses in the test setup method declarations.,"29/Oct/18 02:45;bdeggleston;committed as e4bac44a04d59d93f622d91ef40b462250dac613, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DESC order reads can fail to return the last Unfiltered in the partition in a legacy sstable,CASSANDRA-14766,13186036,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,19/Sep/18 11:26,12/Mar/19 14:15,13/Mar/19 22:35,25/Sep/18 16:08,3.0.18,3.11.4,,,Local/SSTable,,,,0,,,,"{{OldFormatDeserializer}}’s {{hasNext()}} method can and will consume two {{Unfiltered}} from the underlying iterator in some scenarios - intentionally.

But in doing that it’s losing intermediate state of {{lastConsumedPosition}}. If that last block, when iterating backwards, only has two {{Unfiltered}}, the first one will be returned, and the last one won’t as the reverse iterator would incorrectly things that the deserisalizer is past the index block, despite still having one {{Unfiltered}} unreturned.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-25 14:30:18.045,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 25 16:08:13 UTC 2018,,,,,,0|i3y92v:,9223372036854775807,,,,,,benedict,beobal,,,,,,,,,,"25/Sep/18 14:12;iamaleksey;3.0: [code|https://github.com/iamaleksey/cassandra/commits/14766-3.0], [CI|https://circleci.com/workflow-run/b86af556-5675-4a83-8de8-21941175608f]
3.11: [code|https://github.com/iamaleksey/cassandra/commits/14766-3.11], [CI|https://circleci.com/workflow-run/d23b37c9-225a-4e13-85fd-484bda81c37b]
A small dtest for illustration (in addition to a regression unit test in C* repo) can be found [here|https://github.com/iamaleksey/cassandra-dtest/commits/14766].","25/Sep/18 14:30;benedict;one tiny nit: should probably set {{couldBeStartOfPartition = false}} outside of the {{if}} branch, so we don't unnecessarily perform the RT tests.  

It shouldn't have any impact besides possibly some probably immeasurable performance cost, but probably better that way.","25/Sep/18 14:38;beobal;+1

The fact that we're now resetting {{couldBeStartOfPartition}} in {{clearState}}, which wasn't done before probably fixes an overlooked edge case in CASSANDRA-13236. We greedily read the static row in {{AbstractSSTableIterator}}'s constructor, where {{isFirst}} ({{couldBeStartOfPartition}} as was) would trigger the swapping in the presence of the required tombstone. {{IndexState::setToBlock}} attempts to skip past the static row if we end up iterating backward to block 0 so we don't re-read the static row, but without resetting this flag it would only skip past the RT, leaving the statics to be read next which would then hit the the error from 13236.",25/Sep/18 14:39;iamaleksey;Argh. You are right. Not sure how I missed it. Pushed the updates.,25/Sep/18 15:22;benedict;+1,"25/Sep/18 16:08;iamaleksey;Thanks. Committed to 3.0 as [45937def313bbb32024ae890f830e23bcc6ccae5|https://github.com/apache/cassandra/commit/45937def313bbb32024ae890f830e23bcc6ccae5] and merged to 3.11, and with {{-s ours}} into trunk.

Dtest committed as [02c1cd77439b220a09df1d53891441bb80dcf944|https://github.com/apache/cassandra-dtest/commit/02c1cd77439b220a09df1d53891441bb80dcf944].

NOTE: I did fold {{iterator.hasNext()}} check into the if-branch above on commit, to remove one extra level of nesting from the code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transient->Full movements mishandle consistency level upgrade,CASSANDRA-14759,13185829,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,18/Sep/18 15:14,12/Mar/19 14:15,13/Mar/19 22:35,03/Oct/18 13:50,4.0,,,,Legacy/Core,,,,0,Availability,correctness,transient-replication,"While we need treat a transitioning node as ‘full’ for writes, so that it can safely begin serving full data requests once it has finished, we cannot maintain it in the ‘pending’ collection else we will also increase our consistency requirements by a node that doesn’t exist.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-18 15:47:13.574,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 03 13:50:40 UTC 2018,,,,,,0|i3y7t3:,9223372036854775807,,,,,,aweisberg,ifesdjeen,,,,,,,,,,"18/Sep/18 15:21;benedict;I thought I had already committed this as one of my other patches, but it seems not.  I hope you don't mind both being tagged reviewers, since you've been reviewing my other TR follow ups, but shis bug is a proper Blocker for 4.0.

[PR|https://github.com/apache/cassandra/pull/270] [CI|https://circleci.com/workflow-run/636b2eae-2d9e-4245-ad6b-462eab583f13]

I will follow up with some tests.","18/Sep/18 15:47;aweisberg;I think I understand this. It's fixing a deficiency in write availability where transient -> full would cause that replica to be unavailable for meeting write quorum requirements. Since it would remain present in pending it would also increase group size in blockFor.

{code}
return natural.filter(r -> !r.isTransient() || !pending.contains(r.endpoint(), true));
{code}

Specifically the {{!pending.contains(r.endpoint(), true)}} (use of an enum instead of true/false might be a hair easier to follow for that API) causes it to no longer appear in natural and in pending instead.

Once you have the tests I'll do a final review.","18/Sep/18 15:56;benedict;bq. use of an enum instead of true/false might be a hair easier to follow for that API

-Agreed, I had that thought before too.  I'll update the API.-

Actually, the method is no longer used, so I'll remove it (for now)",19/Sep/18 09:06;benedict;I have added a simple test case for this functionality.,20/Sep/18 19:34;aweisberg;+1,"24/Sep/18 13:37;ifesdjeen;+1 as logic looks right, just two minor things for discussion:

  * should we get rid of generics {{<E extends Endpoints<E>> E}} there since we only have token writes and these methods are called with {{EndpointsForToken}} only?
  * semantically, wouldn't it be better to leave full pending in pending and filter natural transient out? It might be slightly confusing that natural replica flips from transient to full: it doesn't seem as we have inherent limitations that prevent us from that, or? 
","24/Sep/18 14:25;benedict;bq. semantically, wouldn't it be better to leave full pending in pending and filter natural transient out? It might be slightly confusing that natural replica flips from transient to full: it doesn't seem as we have inherent limitations that prevent us from that, or?

It might be, but unfortunately pending is used - almost exclusively - to calculate how much we should boost our CL blockFor requirements by.  The sufficiency logic is the last bit of this codepath that could do with being refactored, but until we do this, it is probably more dangerous to leave it in the pending collection and risk boosting blockFor accidentally.","24/Sep/18 14:29;benedict;bq. should we get rid of generics <E extends Endpoints<E>> E there since we only have token writes and these methods are called with EndpointsForToken only?

I've made this change, and once I get a clean CI response I'll merge the patch in.  Thanks for the review!","03/Oct/18 13:50;benedict;I've committed as [daa3619ae63bb8b06d532890e51d288c189c787c|https://github.com/apache/cassandra/commit/daa3619ae63bb8b06d532890e51d288c189c787c]

dtests are now very flaky on CircleCI, but all of the failures in the latest run have shown up in trunk runs for me on CircleCI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle failures in upgradesstables/cleanup/relocate,CASSANDRA-14657,13179999,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,20/Aug/18 16:29,12/Mar/19 14:15,13/Mar/19 22:35,14/Sep/18 06:46,3.0.18,3.11.4,4.0,,,,,,0,,,,"If a compaction in {{parallelAllSSTableOperation}} throws exception, all current transactions are closed, this can make us close a transaction that has not yet finished (since we can run many of these compactions in parallel). This causes this error:
{code}
java.lang.IllegalStateException: Cannot prepare to commit unless IN_PROGRESS; state is ABORTED
{code}
and this can get the leveled manifest (if running LCS) in a bad state causing this error message:
{code}
Could not acquire references for compacting SSTables ...
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-13 12:13:56.245,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 14 06:46:50 UTC 2018,,,,,,0|i3x867:,9223372036854775807,,,,,benedict,benedict,,,,,,,,,,,"20/Aug/18 16:32;krummas;patch here: https://github.com/krummas/cassandra/commits/marcuse/14657

needs a couple of tests, will add soon",05/Sep/18 12:52;krummas;and dtest to reproduce: https://github.com/krummas/cassandra-dtest/commits/marcuse/14657,"13/Sep/18 11:41;krummas;dtest run against the branch above:
https://circleci.com/workflow-run/315d4f3d-18b7-4af2-aa46-ef6a2a57d37e","13/Sep/18 12:13;benedict;LGTM.  Do we think it's maybe worth cancelling the existing operations?  It looks like there's unfortunately no trivial way of doing it, as we don't currently check for {{Thread.isInterrupted}} when checking {{isStopRequested}}, so perhaps not worth worrying about.  Does seem a shame to leave all that work on going after an exception, but perhaps we should simply file a follow-up ticket.",13/Sep/18 12:16;krummas;[~benedict] yeah I considered that but if we run for example `upgradesstables` it would probably be more wasteful to cancel the potentially successful upgrades,"13/Sep/18 12:27;benedict;I guess we could specify per operation if it's valuable, but this all seems out of scope for this ticket.  Ship it, +1.","14/Sep/18 06:46;krummas;committed as {{9be437064f5348fe7f8fc6665b747ad751699f49}} to 3.0 and merged up, thanks!

all test results:
3.0: https://circleci.com/workflow-run/315d4f3d-18b7-4af2-aa46-ef6a2a57d37e
3.11: https://circleci.com/workflow-run/b14718fb-474f-4a90-bf70-98af8df49820
trunk: https://circleci.com/workflow-run/ac121b2d-0f81-4198-b20c-3fd5c3bf1567",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dtests: fix pytest.raises argument names,CASSANDRA-14545,13168542,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,spodxx@gmail.com,spodxx@gmail.com,spodxx@gmail.com,27/Jun/18 07:26,12/Mar/19 14:15,13/Mar/19 22:35,25/Jul/18 07:22,,,,,Test/dtest,,,,0,dtest,,,"I've been through a couple of [dtest results|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/580/#showFailuresLink] lately and notices some interpreter errors regarding how we call pytest.raises. The [reference|https://docs.pytest.org/en/latest/assert.html#assertions-about-expected-exceptions] is pretty clear on what would be the correct arguments, but still want to make sure we're not working on different pytest versions. 
[~mkjellman] can you quickly check the following inconsistencies and look at my patch (msg->message, matches->match)?
{noformat}
git show 49b2dda4 |egrep 'raises.*, m' {noformat}",,,,,,,,,,,,,,,,,CASSANDRA-14560,27/Jun/18 07:27;spodxx@gmail.com;CASSANDRA-14545.patch;https://issues.apache.org/jira/secure/attachment/12929347/CASSANDRA-14545.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-07-24 12:09:45.77,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 25 07:22:28 UTC 2018,,,,,,0|i3va3z:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"24/Jul/18 12:09;jasobrown;[~mkjellman] isn't around these days, so I've taken a look. Double checked {{master}} for other {{msg}}/{{matches}} and this patch got them all. I also checked the pytest source to make sure {{match}}/{{message}} are the correct param names (they are).

+1",25/Jul/18 07:22;spodxx@gmail.com;Committed as f210e532e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reinstate repaired data tracking when ReadRepair == NONE,CASSANDRA-14755,13185490,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,beobal,beobal,17/Sep/18 12:11,12/Mar/19 14:15,13/Mar/19 22:35,20/Sep/18 12:56,4.0,,,,Legacy/Local Write-Read Paths,,,,0,,,,"Some of the refactoring in CASSANDRA-14698 breaks repaired data tracking when read repair is disabled as it skips wrapping the {{MergeIterator}} in {{DataResolver::wrapMergeListener}}. If repaired tracking is enabled, the iterator still needs to be extended so that it calls {{RepairedDataTracker::verify}} on close. This wasn't easy to spot as the new dtests for CASSANDRA-14145 haven't yet been merged. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-18 10:44:41.292,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 20 12:56:06 UTC 2018,,,,,,0|i3y5pz:,9223372036854775807,,,,,ifesdjeen,ifesdjeen,,,,,,,,,,,"17/Sep/18 12:12;beobal;Here's the dtest PR which contains the new tests and also turns on repaired tracking for all dtests by default: https://github.com/apache/cassandra-dtest/pull/37 

C* Patch and CI runs:
||branch||utests||dtests||
|[14755-trunk|https://github.com/beobal/cassandra/tree/14755-trunk]|[utests|https://circleci.com/gh/beobal/cassandra/444]|[vnodes|https://circleci.com/gh/beobal/cassandra/445] / [no vnodes|https://circleci.com/gh/beobal/cassandra/446]|
","18/Sep/18 10:44;ifesdjeen;[~beobal] sorry: I was looking closely at all the {{on*}} methods to ensure they do not have any effects, but overlooked the {{close}}. Thank you for fixing it.

+1, LGTM!","20/Sep/18 12:56;beobal;Thanks [~ifesdjeen], committed to trunk in {{ee9e06b5a75c0be954694b191ea4170456015b98}} and will merge the dtest PR shortly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle IR prepare phase failures less race prone by waiting for all results,CASSANDRA-15027,13216314,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,spodxx@gmail.com,spodxx@gmail.com,spodxx@gmail.com,18/Feb/19 07:49,12/Mar/19 14:15,13/Mar/19 22:35,22/Feb/19 18:57,4.0,,,,Consistency/Repair,Local/Compaction,,,0,,,,"Handling incremental repairs as a coordinator begins by sending a {{PrepareConsistentRequest}} message to all participants, which may also include the coordinator itself. Participants will run anti-compactions upon receiving such a message and report the result of the operation back to the coordinator.

Once we receive a failure response from any of the participants, we fail-fast in {{CoordinatorSession.handlePrepareResponse()}}, which will in turn completes the {{prepareFuture}} that {{RepairRunnable}} is blocking on. Then the repair command will terminate with an error status, as expected.

The issue is that in case the node will both be coordinator and participant, we may end up with a local session and submitted anti-compactions, which will be executed without any coordination with the coordinator session (on same node). This may result in situations where running repair commands right after another, may cause overlapping execution of anti-compactions that will cause the following (misleading) message to show up in the logs and will cause the repair to fail again:
 ""Prepare phase for incremental repair session %s has failed because it encountered intersecting sstables belonging to another incremental repair session (%s). This is by starting an incremental repair session before a previous one has completed. Check nodetool repair_admin for hung sessions and fix them.""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-02-21 21:32:08.954,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 22 18:57:27 UTC 2019,,,,,,0|yi12wo:,9223372036854775807,,,,,bdeggleston,,,,,,,,,,,,18/Feb/19 08:42;spodxx@gmail.com;* [ [trunk|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-15027] ][ [circleci|https://circleci.com/workflow-run/2b027f87-cf45-48ee-8eae-45a563701bc6] ],"21/Feb/19 21:32;bdeggleston;Thanks [~spodxx@gmail.com]. I’ve extended your code so that in addition to waiting for other anti-compactions to complete, the coordinator also pro-actively cancels ongoing anti-compactions on the other participants. This avoids wasting time waiting for anti-compactions on other machines. The code does 3 things:
 * Adds a session state check to the {{isStopRequested}} method in the anti-compaction iterator.
 * The coordinator now sends failure messages to all participants when it receives a failure message from one of them in the prepare phase. It does not mark these participants as having failed internally though, since that would cause the nodetool session to immediately complete. Instead, it waits until it’s received messages from all the other nodes.
 * The participants will now respond with a failed prepare message if the anti-compaction completes, but the session was failed in the mean time. This prevents a dead lock on the coordinator in the case where the participant received a failure message between the time the anti-compaction completes and the callback fires.

Let me know what you think. If everything looks ok to you, I’m +1 on committing.

[trunk|https://github.com/bdeggleston/cassandra/tree/15027-trunk]
 [circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15027-trunk]","22/Feb/19 14:15;spodxx@gmail.com;Your updates look like valuable improvement over the initial patch. I'm +1 in general as for the changes, but also fixed some additional minor issues and added a new tests:

* [CASSANDRA-15027|https://github.com/spodkowinski/cassandra/commits/CASSANDRA-15027]
* [https://circleci.com/workflow-run/2b444c33-a54c-46b5-9923-bcded8bcf465]

Please see comments with each commit in branch above for details.

Also happy to discuss any of the changes (most likely the last commit) in another jira, if you feel it's out of scope for this ticket.
 ","22/Feb/19 16:43;bdeggleston;Nice. Your follow on changes look good to me, I have 2 nits, but those can just be fixed on commit.

* We should log the session id in compaction manager when an anti-compaction is cancelled (and probably when there's an error as well)
* Some error handling should be added to the commit fixing the race between proposeFuture and hasFailure so nodetool doesn't hang if there's an error in the callback

edit: proposed fixes [here|https://github.com/bdeggleston/cassandra/commit/02d7d9e09983db0d4661486b17adc375e17be24f]",22/Feb/19 17:41;spodxx@gmail.com;LGTM +1,"22/Feb/19 18:57;bdeggleston;Committed to trunk as 9bde713ee8883f70d130efb6290ec0e6daea524f, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partitioned outbound internode TCP connections can occur when nodes restart,CASSANDRA-14358,13149263,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jolynch,jolynch,jolynch,30/Mar/18 23:51,12/Mar/19 14:15,13/Mar/19 22:35,01/Nov/18 22:37,4.0,,,,Legacy/Streaming and Messaging,,,,0,4.0-feature-freeze-review-requested,,,"edit summary: This primarily impacts networks with stateful firewalls such as AWS. I'm working on a proper patch for trunk but unfortunately it relies on the Netty refactor in 4.0 so it will be hard to backport to previous versions. A workaround for earlier versions is to set the {{net.ipv4.tcp_retries2}} sysctl to ~5. This can be done with the following:
{code:java}
$ cat /etc/sysctl.d/20-cassandra-tuning.conf
net.ipv4.tcp_retries2=5
$ # Reload all sysctls
$ sysctl --system{code}
Original Bug Report:

I've been trying to debug nodes not being able to see each other during longer (~5 minute+) Cassandra restarts in 3.0.x and 2.1.x which can contribute to {{UnavailableExceptions}} during rolling restarts of 3.0.x and 2.1.x clusters for us. I think I finally have a lead. It appears that prior to trunk (with the awesome Netty refactor) we do not set socket connect timeouts on SSL connections (in 2.1.x, 3.0.x, or 3.11.x) nor do we set {{SO_TIMEOUT}} as far as I can tell on outbound connections either. I believe that this means that we could potentially block forever on {{connect}} or {{recv}} syscalls, and we could block forever on the SSL Handshake as well. I think that the OS will protect us somewhat (and that may be what's causing the eventual timeout) but I think that given the right network conditions our {{OutboundTCPConnection}} threads can just be stuck never making any progress until the OS intervenes.

I have attached some logs of such a network partition during a rolling restart where an old node in the cluster has a completely foobarred {{OutboundTcpConnection}} for ~10 minutes before finally getting a {{java.net.SocketException: Connection timed out (Write failed)}} and immediately successfully reconnecting. I conclude that the old node is the problem because the new node (the one that restarted) is sending ECHOs to the old node, and the old node is sending ECHOs and REQUEST_RESPONSES to the new node's ECHOs, but the new node is never getting the ECHO's. This appears, to me, to indicate that the old node's {{OutboundTcpConnection}} thread is just stuck and can't make any forward progress. By the time we could notice this and slap TRACE logging on, the only thing we see is ~10 minutes later a {{SocketException}} inside {{writeConnected}}'s flush and an immediate recovery. It is interesting to me that the exception happens in {{writeConnected}} and it's a _connection timeout_ (and since we see {{Write failure}} I believe that this can't be a connection reset), because my understanding is that we should have a fully handshaked SSL connection at that point in the code.

Current theory:
 # ""New"" node restarts,  ""Old"" node calls [newSocket|https://github.com/apache/cassandra/blob/6f30677b28dcbf82bcd0a291f3294ddf87dafaac/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L433]
 # Old node starts [creating a new|https://github.com/apache/cassandra/blob/6f30677b28dcbf82bcd0a291f3294ddf87dafaac/src/java/org/apache/cassandra/net/OutboundTcpConnectionPool.java#L141] SSL socket 
 # SSLSocket calls [createSocket|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/security/SSLFactory.java#L98], which conveniently calls connect with a default timeout of ""forever"". We could hang here forever until the OS kills us.
 # If we continue, we get to [writeConnected|https://github.com/apache/cassandra/blob/6f30677b28dcbf82bcd0a291f3294ddf87dafaac/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L263] which eventually calls [flush|https://github.com/apache/cassandra/blob/6f30677b28dcbf82bcd0a291f3294ddf87dafaac/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L341] on the output stream and also can hang forever. I think the probability is especially high when a node is restarting and is overwhelmed with SSL handshakes and such.

I don't fully understand the attached traceback as it appears we are getting a {{Connection Timeout}} from a {{send}} failure (my understanding is you can only get a connection timeout prior to a send), but I think it's reasonable that we have a timeout configuration issue. I'd like to try to make Cassandra robust to networking issues like this via maybe:
 # Change the {{SSLSocket}} {{getSocket}} methods to provide connection timeouts of 2s (equivalent to trunk's [timeout|https://github.com/apache/cassandra/blob/11496039fb18bb45407246602e31740c56d28157/src/java/org/apache/cassandra/net/async/NettyFactory.java#L329])
 # Appropriately set recv timeouts via {{SO_TIMEOUT}}, maybe something like 2 minutes (in old versions via [setSoTimeout|https://docs.oracle.com/javase/8/docs/api/java/net/Socket.html#setSoTimeout-int-], in trunk via [SO_TIMEOUT|http://netty.io/4.0/api/io/netty/channel/ChannelOption.html#SO_TIMEOUT]
 # Since we can't set send timeouts afaik (thanks java) maybe we can have some kind of watchdog that ensures OutboundTcpConnection is making progress in its queue and if it doesn't make any progress for ~30s-1m, forces a disconnect.

If anyone has insight or suggestions, I'd be grateful. I am going to rule out if this is keepalive duration by setting tcp_keepalive_probes to like 1 and maybe tcp_retries2 to like 8 get more information about the state of the tcp connections the next time this happens. It's a very rare bug and when it does happen I only have 10 minutes to jump on the nodes and fix it before it fixes itself so I'll do my best.","Cassandra 2.1.19 (also reproduced on 3.0.15), running with {{internode_encryption: all}} and the EC2 multi region snitch on Linux 4.13 within the same AWS region. Smallest cluster I've seen the problem on is 12 nodes, reproduces more reliably on 40+ and 300 node clusters consistently reproduce on at least one node in the cluster.

So all the connections are SSL and we're connecting on the internal ip addresses (not the public endpoint ones).

Potentially relevant sysctls:
{noformat}
/proc/sys/net/ipv4/tcp_syn_retries = 2
/proc/sys/net/ipv4/tcp_synack_retries = 5
/proc/sys/net/ipv4/tcp_keepalive_time = 7200
/proc/sys/net/ipv4/tcp_keepalive_probes = 9
/proc/sys/net/ipv4/tcp_keepalive_intvl = 75
/proc/sys/net/ipv4/tcp_retries2 = 15
{noformat}",,,,,,,,,,,,CASSANDRA-14424,,,,,30/Mar/18 23:44;jolynch;10 Minute Partition.pdf;https://issues.apache.org/jira/secure/attachment/12917074/10+Minute+Partition.pdf,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-04-02 09:49:07.148,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 06 19:16:36 UTC 2018,,,,,,0|i3s0cn:,9223372036854775807,2.1.19,3.0.15,,,aweisberg,aweisberg,,,,,,,,,,,"30/Mar/18 23:59;jolynch;It's also worth noting that the non ssl connections have the same problem, it's just unlikely I think that the destination server get's as overloaded and drops a handshake.","02/Apr/18 09:49;djoshi3;[~jolynch], I think what you're experiencing is a classic slow consumer issue. The OS will likely buffer the socket but it looks like once this queue is full, you will see it stuck on {{java.net.SocketOutputStream.socketWrite0}}. On Linux, TCP will continue trying to retransmit your packets for roughly 15 minutes before it finally gives up. This is controlled via {{tcp_retries2 (retransmission timeout)}}. You can tune it down to be closer to 100s if that is more desirable to you (See: RFC 1122).

 ","02/Apr/18 17:07;jolynch;[~djoshi3] yea I think that you're probably right, but I think that this might not be a slow consumer so much as a never consumer. Specifically if there is a stateful firewall (e.g. security groups or vpc) in the way, the network could absolutely blackhole packets on a connection that's been reset. My plan for this week is I'm going to try to catch this happening and run the following analysis:
 # Get a netstat view of established connections and their send buffers on both sides
 # Slap a tcpdump on both sides to see if {{RESETS}} from the restarted node are even getting to the old node (VPC might be swallowing them)
 # If #1 or #2 confirm the theory that we're just in a stuck tcp connection, I will try tuning {{tcp_retries2}} down to ~5, setting {{SO_SNDBUF}} or {{wmem_default}} down to a more reasonable number (I think right now it's like 16 megs) to see if that fixes the issue

For what it's worth, Cassandra is _really_ intolerant of these kinds of network partitions. For example if I tell a stateful firewall to just drop all packets on one of Cassandra's internode connections with {{iptables -A OUPTUT -p tcp -d <ip1> --sport <ip1 local port> --dport <ssl port> -j DROP}}, Cassandra continues sending packets out on that {{OutboundTcpConnection}} until the operating system says ""hey that's probably not going to work"" 15 minutes later. Unfortunately we have two TCP connections so I can't just do a simple heartbeat mechanism.","02/Apr/18 17:15;jjirsa;Suspect you'll see a single byte in the sendq on the instance that didnt bounce. 

 ","02/Apr/18 18:14;djoshi3;[~jolynch] I understand the problem that you're describing but this is TCP specific and not Cassandra specific. If there is packet loss or slow consumer or no consumer but the connection is established, all applications will see this issue. The problem here is that the thread is stuck on making progress due to Java Sockets implementation specifically the {{write0}} call. In case of Netty or Java NIO, you simply would ignore the socket and continue processing other ""ready"" sockets allowing Cassandra's threads to make progress. I suspect you will not see this issue in trunk. Have you given that a try?","02/Apr/18 18:52;jolynch;[~djoshi3] I agree that the issue is probably a bad TCP level connection, but also I think it's a  bug that Cassandra blocks forever never making any progress and potentially causing an outage for users (until the OS kills the connection); we're supposed to be highly available right ;). In non JVM languages this would be as simple (I think) as setting {{SO_SNDTIMEO}} to a reasonable number like 30s, but I don't see how to do it without JNI in Cassandra 2.1.x/3.0.x/3.11.x. If this is the issue (I think we still need more data), I also think recommending users tune {{tcp_retries2}} to 8, 5 or even 3 would be a reasonable workaround if it's fixed in trunk.

I haven't tried in trunk (tbh we can't even build/deploy trunk right now), but unfortunately I think it would be vulnerable to same kind of issue since afaict {{[NettyFactory|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/NettyFactory.java#L358-L377]}} doesn't add any kind of {{[WriteTimeoutHandler|https://netty.io/4.0/api/io/netty/handler/timeout/WriteTimeoutHandler.html]}} or {{[ReadTimeoutHandler|https://netty.io/4.0/api/io/netty/handler/timeout/ReadTimeoutHandler.html]}} which will throw an exception (I believe) if the write/read on the socket doesn't complete in a reasonable amount of time. Admittedly I'm very new to the netty refactor in 4.0 so I might be missing the part where we set a bunch of timeouts somewhere.

I don't want to invest too much time testing trunk until I confirm the type of partition that is actually happening, but if our theory is correct it should be easy to reproduce on trunk with the above iptables rule (if trunk handles it then the tcp connection should get thrown out after ~30s or something reasonable and we re-connect).","02/Apr/18 21:18;aweisberg;We can implement timeouts with a watchdog and wake up the threads by closing the socket. This will work even if they are blocked on the various socket methods.

This sounds like a real implementation hole to me. It means MTTR for any instance where the kernel doesn't gracefully clean up connections is as long as whatever the timeout is.

Even the Netty implementation is vulnerable here because it won't attempt the reconnect for a long time. The thread won't be blocked, but that isn't the issue here the issue is that communication itself is blocked until the faulty connection is replaced.",02/Apr/18 21:33;pauloricardomg;Did you try reproducing this issue with CASSANDRA-9630 in? This has been committed relatively recently and it used to cause hanging connections on 2.1/3.X.,"02/Apr/18 22:12;jolynch;[~aweisberg] if the issue is what I think it is (and I definitely need more supporting evidence), I think we can fix it in trunk with a netty {{WriteTimeoutHandler}}/ {{ReadTimeoutHandler}} which will throw away the tcp connection if recv/write take too long. I think it's reasonable to say in earlier versions we are not going to patch it past connection and recv timeouts and just recommend {{/proc/sys/net/ipv4/tcp_retries2 = 5}}. I tested the sysctl and it leads to a much shorter partition with the iptables test, like 20s instead of 10 minutes. I'm testing it out on a large cluster with a rolling restart soon.

[~pauloricardomg] I believe that CASSANDRA-9630 is a separate issue (although +1 to a 2.1.x backport there since it's so simple and 2.1.x is actually still used places) because in this case I believe we have an established connection that is blackholing. I'll double check this next time I run the test and ensure that the tcp connection state is {{ESTABLISHED}}.","03/Apr/18 03:05;djoshi3;[~aweisberg] [~jolynch]

The underlying issue is that when a node restarts and begins listening, the peers in the cluster cause a thundering herd overwhelming the node. In 3.0 / 2.1 we accept the incoming connection and read some bytes off the socket on the same thread. This probably causes the connections to build up in the listen queue. Using a threadpool and separating the accept from rest of the code would make this better. 

If the peers kill their connections and restart them with a shorter timeout, the restarting node will get further overwhelmed. I looked at trunk and I don't think we will see the same issue crop up as we use a separate event loop group for accepting connections. Netty internally uses a separate IO thread for the channel pipeline and the request is dispatched using a separate threadpool. Unless you block the Netty accept thread or the IO thread, these symptoms won't surface.","03/Apr/18 13:38;jasobrown;For trunk, we already use {{IdleStateHandler}} [in the outbound pipeline|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/OutboundHandshakeHandler.java#L196] for the ""not making progress sending data"" case. Take a look at {{MessageOutHandler#userEventTriggered}}. iirc, {{WriteTimeoutHandler}} / {{ReadTimeoutHandler}} does not fit our use case for messaging; I'd have to look again to page it back in.

Further, [~jolynch], I thought your problem was in pre-4.0? There is a problem (or, at least, non-optimization) with {{MessagingService.SocketThread}} where the single-threaded accept thread handles receiving the first message pf the c* internode messaging protocol, which may require executing the TLS handshake before that can be received (if using internode TLS). Thus a single new establishing connection may block the accept thread for 500-1000 ms for a cross-datacenter connection. Even worse, if you are using the {{Ec2MultiregionSnitch}}, all connections for the local region will be torn down and built anew as we switch from the public IP to the region-local private IP - more traffic on the accept thread. So one slow connection interaction screws the whole accept thread.

I have an internal patch for 3.0 which, when a new socket is accepted, moves all processing (internode protocol handshake, TLS handshake) to a separate thread (via an executor group). So, if one connection runs into a problem, the other incoming connections are not held up. I've been reticent about OSS'ing this late in the 3.0/3.11 branches, even though it's not that clever or invasive. Let me know if you'd be interested in giving it shot and I can post it. If it's useful/helpful we can consider committing it, as well.",03/Apr/18 14:42;aweisberg;A node restart that is graceful (kernel closes connections) is different from power failure or network black holing packets to a host. In the latter case what in Cassandra causes the server to notice a TCP connection is no longer responding and attempt to recreate it so it can connect to the new version of the host?,"03/Apr/18 18:00;jolynch;[~djoshi3] We send RSTs on listen overflows via the {{tcp_abort_on_overflow}} sysctl, so I don't think that this is a simple listen overflow. You're absolutely right that nodes coming up can get overwhelmed in 2.1/3.0/3.11, and I think it's really great that trunk fixes that; but I don't think this is the bug I've identified here. In this case a Cassandra node that _didn't_ restart continues trying to send on a dead connection forever until the kernel steps in and cuts off the connection.

 [~jasobrown]
{quote}For trunk, we already use IdleStateHandler in the outbound pipeline for the ""not making progress sending data"" case. Take a look at MessageOutHandler#userEventTriggered. iirc, WriteTimeoutHandler / ReadTimeoutHandler does not fit our use case for messaging; I'd have to look again to page it back in.
{quote}
I am definitely not a netty expert, but I think that {{IdleStateHandler}} needs to be paired with a {{WriteTimeoutHandler}} or {{ReadTimeoutHandler}} in order to effectively identify such blackhole partitions. The app is sending data on the socket, it's just never going anywhere. My understanding is that {{IdleStateHandler}} just sends data if there is none.
{quote}Further, Joseph Lynch, I thought your problem was in pre-4.0? There is a problem (or, at least, non-optimization) with MessagingService.SocketThread where the single-threaded accept thread handles receiving the first message pf the c* internode messaging protocol, which may require executing the TLS handshake before that can be received (if using internode TLS). Thus a single new establishing connection may block the accept thread for 500-1000 ms for a cross-datacenter connection. Even worse, if you are using the Ec2MultiregionSnitch, all connections for the local region will be torn down and built anew as we switch from the public IP to the region-local private IP - more traffic on the accept thread. So one slow connection interaction screws the whole accept thread.
{quote}
Yes, our problem is with pre-4.0, but I'm 99% sure we can mitigate this by setting {{tcp_retries2}} to 3 or 5 to tell the OS to cover over the app bug. If Cassandra trunk is resilient to the (rare) bug I think the sysctl is a reasonable workaround for earlier versions, what do you think? It's also worth noting that this kind of partition probably only occurs frequently in stateful networks that swallow RSTs to unknown flows such as AWS VPC (although as [~aweisberg] points out, power failure could manifest similarly).
{quote}I have an internal patch for 3.0 which, when a new socket is accepted, moves all processing (internode protocol handshake, TLS handshake) to a separate thread (via an executor group). So, if one connection runs into a problem, the other incoming connections are not held up. I've been reticent about OSS'ing this late in the 3.0/3.11 branches, even though it's not that clever or invasive. Let me know if you'd be interested in giving it shot and I can post it. If it's useful/helpful we can consider committing it, as well.
{quote}
I don't think that would fix this particular bug, but it is awesome and fixes other issues. I volunteer to test it and review it if that helps get it merged to 3.0/3.11.","03/Apr/18 18:01;djoshi3;[~aweisberg] For cases where you have power failure (or process crashes or even restarts without properly closing its connections), the Socket will be left in a ""half-open"" state. Per TCP's spec, it will retry transmitting the packet a number of times. Each unsuccessful attempt will cause an exponential back off. Ultimately it will give up and send a RST packet. All this while your application will continue writing to the Kernel's socket buffer and will not know that the other end is dead / unresponsive. This is per TCP's design. If you want your applications to timeout faster or detect dead peers quicker you can tune TCP's parameters per your requirements.","04/Apr/18 00:24;jolynch;For what it's worth we've opened an AWS ticket to find out if this is expected behaviour in VPCs (that the network blackholes packets on unknown flows rather than resetting them). I have a feeling that answer will be ""yea that's how stateful firewalls work we can't keep flows forever"", in which case Cassandra (probably) should be resilient to it I think.

[~djoshi3] I think you've got it now. I'm proposing that we tune Cassandra's socket options (opt-in of course like most socket options we set) to tune TCP's parameters per Cassandra's requirement of high availability. In the case of blocking socket the way that I'm familiar with are connect timeouts ([unix|http://man7.org/linux/man-pages/man2/connect.2.html], [java|https://docs.oracle.com/javase/8/docs/api/java/net/Socket.html#connect-java.net.SocketAddress-int-]), receive timeouts ([SO_RCVTIMEO|https://linux.die.net/man/7/socket], [java|https://docs.oracle.com/javase/8/docs/api/java/net/Socket.html#setSoTimeout-int-]) and send timeouts which Java doesn't expose afaict ([SO_SNDTIMEO|https://linux.die.net/man/7/socket]). For {{O_NONBLOCK}} sockets I think typically the timeout is handled at the event loop level (e.g. I think that's what {{WriteTimeoutHandler}} and {{ReadTimeoutHandler}} do in Netty).

I still need to test this thoroughly, but I believe asking users to set {{/proc/sys/net/ipv4/tcp_retries2 = 5}} is a reasonable workaround. I was able to simulate pretty easily the exact error in the production logs by doing the following:
{noformat}
$ sudo lsof -p $(pgrep -f Cassandra) -n | grep OTHER_IP | grep STORAGE_PORT
java    5277 cassandra  64u     IPv4             113860      0t0        TCP LOCAL_IP:38665->OTHER_IP:STORAGE_PORT (ESTABLISHED)
java    5277 cassandra  69u     IPv4             114797      0t0        TCP LOCAL_IP:STORAGE_PORT->OTHER_IP:54875 (ESTABLISHED)

$ sudo iptables -A OUTPUT -p tcp -d OTHER_IP --sport 38665 --dport STORAGE_PORT -j DROP

$ cqlsh
cqlsh> CONSISTENCY ALL
Consistency level set to ALL.
cqlsh> select * from test_ks.test_cf WHERE key = 'key_that_lives_on_OTHER_IP';
OperationTimedOut: errors={}, last_host=<something>
... timeouts happen for next ~15 minutes

$ tail -20 system.log 
TRACE [MessagingService-Outgoing-/OTHER_IP] 2018-04-04 00:13:05,009 OutboundTcpConnection.java:365 - Socket to /OTHER_IP closed
DEBUG [MessagingService-Outgoing-/OTHER_IP] 2018-04-04 00:13:05,010 OutboundTcpConnection.java:303 - error writing to /OTHER_IP
java.net.SocketException: Connection timed out (Write failed)
	at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_152]
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) ~[na:1.8.0_152]
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155) ~[na:1.8.0_152]
	at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_152]
	at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_152]
	at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:886) ~[na:1.8.0_152]
	at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:857) ~[na:1.8.0_152]
	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:123) ~[na:1.8.0_152]
	at net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:205) ~[lz4-1.2.0.jar:na]
	at net.jpountz.lz4.LZ4BlockOutputStream.flush(LZ4BlockOutputStream.java:222) ~[lz4-1.2.0.jar:na]
	at org.apache.cassandra.io.util.DataOutputStreamPlus.flush(DataOutputStreamPlus.java:55) ~[cassandra-2.1.19.jar:2.1.19]
	at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:294) [cassandra-2.1.19.jar:2.1.19]
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:222) [cassandra-2.1.19.jar:2.1.19]
DEBUG [MessagingService-Outgoing-/OTHER_IP] 2018-04-04 00:13:49,867 OutboundTcpConnection.java:380 - attempting to connect to /OTHER_IP
INFO  [HANDSHAKE-/OTHER_IP] 2018-04-04 00:13:49,916 OutboundTcpConnection.java:496 - Handshaking version with /OTHER_IP
TRACE [MessagingService-Outgoing-/OTHER_IP] 2018-04-04 00:13:49,960 OutboundTcpConnection.java:453 - Upgrading OutputStream to be compressed

$ cqlsh
cqlsh> CONSISTENCY ALL
Consistency level set to ALL.
cqlsh> select * from test_ks.test_cf WHERE key = 'key_that_lives_on_OTHER_IP';
... results come back and everything is happy

$ sudo lsof -p $(pgrep -f Cassandra) -n | grep OTHER_IP | grep STORAGE_PORT
java    5277 cassandra  134u     IPv4             113860      0t0        TCP LOCAL_IP:33417->OTHER_IP:STORAGE_PORT (ESTABLISHED)
java    5277 cassandra   69u     IPv4             114797      0t0        TCP LOCAL_IP:STORAGE_PORT->OTHER_IP:54875 (ESTABLISHED)

{noformat}
This simulates ""blackholing"" of the {{OutboundTcpConnection}}. With the default linux value of {{/proc/sys/net/ipv4/tcp_retries2 = 15}} this takes ~15 minutes to resolve (the operating system eventually saves Cassandra). With the changed value of ~3, the partition only lasts ~15 seconds. If Cassandra trunk set (configurable) read and write timeouts we could control how long it takes to recover from such a partition rather than relying on the OS.","05/Apr/18 20:19;jolynch;A quick update, AWS has confirmed that this type of half open partition is very much possible when using VPC/security groups as they do their own [connection tracking|https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html#security-group-connection-tracking]. If the sg doesn't know about the flow because the ""timeout"" (which is very non-specific as to how long it is) occurs after close, the sg will blackhole packets. We're following up to see if they could have the more useful behavior of sending resets instead of dropping, but I imagine they'll answer that it's for security reasons.

I'm going to spend time seeing if the proposed workaround of setting {{/proc/sys/net/ipv4/tcp_retries2 = 5 }} fixes the issue satisfactorily, and if so I think that's a reasonable workaround for pre-netty, and post-netty we can use the reproduction steps above to see if Cassandra is now resilient to this kind of half-open partition. [~aweisberg] [~jasobrown] [~djoshi3] what do you guys think?","09/Apr/18 23:15;jolynch;Good news, Linux has support for a [socket option|https://patchwork.ozlabs.org/patch/62889/] since 2.6.37 called {{TCP_USER_TIMEOUT}} which implements [RFC5482|https://tools.ietf.org/html/rfc5482] and does exactly what we want (ensures HA on a kept alive TCP connection). Netty also supports [setting this option|https://github.com/netty/netty/issues/4174] for epoll sockets as of 4.0.31, so we can set that on our internode TCP connections and get really high availability networking without requiring OS tuning on Linux. The {{man tcp}} entry is educational:
{noformat}
       TCP_USER_TIMEOUT (since Linux 2.6.37)
              This  option takes an unsigned int as an argument.  When the value is greater than 0, it specifies the maximum amount of time in milliseconds that trans‐
              mitted data may remain unacknowledged before TCP will forcibly close the corresponding connection and return ETIMEDOUT to the application.  If the option
              value is specified as 0, TCP will to use the system default.

              Increasing  user  timeouts allows a TCP connection to survive extended periods without end-to-end connectivity.  Decreasing user timeouts allows applica‐
              tions to ""fail fast"", if so desired.  Otherwise, failure may take up to 20 minutes with the current system defaults in a normal WAN environment.

              This option can be set during any state of a TCP connection, but is effective only during the synchronized states  of  a  connection  (ESTABLISHED,  FIN-
              WAIT-1, FIN-WAIT-2, CLOSE-WAIT, CLOSING, and LAST-ACK).  Moreover, when used with the TCP keepalive (SO_KEEPALIVE) option, TCP_USER_TIMEOUT will override
              keepalive to determine when to close a connection due to keepalive failure.

              The option has no effect on when TCP retransmits a packet, nor when a keepalive probe is sent.

              This option, like many others, will be inherited by the socket returned by accept(2), if it was set on the listening socket.

              Further details on the user timeout feature can be found in RFC 793 and RFC 5482 (""TCP User Timeout Option"").
{noformat}
I've started working on a [trunk patch|https://github.com/jolynch/cassandra/tree/CASSANDRA-14358] which will fix this on any Linux based deployment via the {{TCP_USER_TIMEOUT}} socket option. 

I've been testing with the reproduction steps I listed above using iptables and without my patch Cassandra waits until the OS kills the connection in ~15-20 minutes, and with my patch it kills it after ~10s.","09/Apr/18 23:27;aweisberg;Nice find! The numbers you picked seem pretty aggressive. I was expecting in the 30-60 second range before giving up. If the network conditions are unreliable but working you might drop connections and then not be able to recreate them quickly due to packet loss. Granted things are probably mostly unavailable anyways in that scenario, but you don't want them looping too tightly trying to recreate connections.

The last question is if this needs to be a hot property or not.","10/Apr/18 20:54;jolynch;Yea, I am interested as to what's the right default for this setting. For normal TCP connections I think it would be reasonable to put this very low (like close to the TCP connect timeout of 2s we use right now), but for SSL ... losing those SSL handshakes is somewhat of a bummer if it's just a temporary switch failure + OSPF convergence or something. What do you think about being conservative (maybe like 30s) for SSL and we'll make it a hot property in addition to the yaml configuration?

I've been testing this option on Linux 4.4 and 4.13 with [a minimal repro|https://gist.github.com/jolynch/90033c2b10ab8280859c8cfe352503cd] and it appears that the option works well if set to a small number (e.g. 5, 10, 20s), but it seems to take about 2x as long for large settings and (I need to do further testing) on Linux 4.4 any setting greater than 30s appears to just default to the system behavior (on 4.13 it is 2x the timeout, but not the system default). So if we set it to 30s we'd get a 60s timeout in most modern Linux's, and if it's not effective we'll just get the system default.",10/Apr/18 23:20;aweisberg;30 seconds (effectively 60) and a hot prop sounds excellent.,"04/May/18 03:34;alienth;[~jolynch] One thing slightly interesting here that I found when reproducing. I can confirm that the traffic is retransmitting from the non-restarted node on the blackholed connection, yet the socket is in `CLOSE_WAIT` rather than `ESTABLISHED`. This would indicate that the non-restarted node got the FIN, but AWS blackholed the FIN,ACK. To me that suggests that the flow tracking was only lost in one direction. I saw this both times I was able to reproduce it, so I doubt it's a fluke of timing.","04/May/18 20:40;alienth;Reproduced this behaviour several times now,and in all cases the socket that cassandra is trying to Outbound on has been in stuck in `CLOSE_WAIT`.

I think that suggests that the more common case might be where AWS stops tracking the flow for a FINd connection, and an ACK being dropped somewhere resulting in a node not fully closing the connection?

Note that I'm seeing this on cassandra 3.11.2, which has the patches from CASSANDRA-9630

{{tcp    CLOSE-WAIT 1      322683 10.0.161.40:18254              10.0.109.39:7000                timer:(on,1min54sec,13) uid:115 ino:5837280 sk:1a1 -->}}","04/May/18 23:22;alienth;Captured the socket state for before, during, and after the restart, from the POV of a node which the restarted node sees as down:

Before, when nothing has been done:
 {{Netid State Recv-Q Send-Q Local Address:Port Peer Address:Port}}
 {{tcp   ESTAB 0      0      10.0.161.40:59739  10.0.107.88:7000 timer:(keepalive,4min59sec,0) uid:115 ino:5913893 sk:216e <->}}

After 10.0.107.88 has been restarted. Note the {{1}} in in the recv-q:
 {{tcp CLOSE-WAIT 1 0 10.0.161.40:59739 10.0.107.88:7000 timer:(keepalive,4min14sec,0) uid:115 ino:5913893 sk:216e -->}}

When 10.0.107.88 comes back up and 10.0.161.40 tries to respond to the EchoMessage, using the previous socket which has been in CLOSE-WAIT. You can see the outbounds piling up in the send-q:
 {{tcp CLOSE-WAIT 1 36527 10.0.161.40:59739 10.0.107.88:7000 timer:(on,1.932ms,4) uid:115 ino:5913893 sk:216e -->}}","05/May/18 02:03;jasobrown;[~alienth] nice detective work. it looks like {{10.0.161.40}} hasn't closed the previous sockets/connections. If you have the logs of {{10.0.161.40}} still handy, can you see if, during and after the bounce, there are log statements about  {{10.0.107.88}} being down or alive (or restarted). Not looking at the code to see the exact log messages, but typically they are emitted by {{Gossiper}}.

Also, how long did it take  {{10.0.107.88}} to bounce? If it went down and came back up fast enough, it's possible the failure detector on {{10.0.161.40}} didn't mark it as down.

How did you terminate {{10.0.107.88}}? A normal shutdown, or {{kill -9}}? ","05/May/18 03:25;alienth;[~jasobrown] {{.88}} was down for about 10 minutes, and was shutdown via `nodetool drain`, followed by stopping the service gracefully.


{{.40}} has no gossiper logs showing {{.88}} going down, until the socket to {{.88}} is killed about 20 minutes later. {{.88}} was shut down at 1602 and came back up at 1612:

{{INFO  [RMI TCP Connection(22)-127.0.0.1] 2018-05-04 16:02:14,055 StorageService.java:1449 - DRAINING: starting drain process}}
{{INFO  [RMI TCP Connection(22)-127.0.0.1] 2018-05-04 16:02:14,057 HintsService.java:220 - Paused hints dispatch}}
{{INFO  [RMI TCP Connection(22)-127.0.0.1] 2018-05-04 16:02:14,058 ThriftServer.java:139 - Stop listening to thrift clients}}
{{INFO  [RMI TCP Connection(22)-127.0.0.1] 2018-05-04 16:02:14,077 Server.java:176 - Stop listening for CQL clients}}
{{INFO  [RMI TCP Connection(22)-127.0.0.1] 2018-05-04 16:02:14,078 Gossiper.java:1540 - Announcing shutdown}}
<snip>
{{INFO  [main] 2018-05-04 16:12:39,746 StorageService.java:1449 - JOINING: Finish joining ring}}

And {{.40}} has no logs of this happening, and seemingly doesn't see it as down until ~20 minutes later:

{{INFO  [GossipStage:1] 2018-05-01 16:20:36,680 Gossiper.java:1034 - InetAddress /10.0.107.88 is now DOWN}}
{{INFO  [HANDSHAKE-/10.0.107.88] 2018-05-01 16:20:36,693 OutboundTcpConnection.java:560 - Handshaking version with /10.0.107.88}}
{{INFO  [HANDSHAKE-/10.0.107.88] 2018-05-01 16:20:36,822 OutboundTcpConnection.java:560 - Handshaking version with /10.0.107.88}}
{{INFO  [GossipStage:1] 2018-05-01 16:20:56,323 Gossiper.java:1053 - Node /10.0.107.88 has restarted, now UP}}
{{INFO  [GossipStage:1] 2018-05-01 16:20:56,325 StorageService.java:2292 - Node /10.0.107.88 state jump to NORMAL}}

That is.. bizarre.","05/May/18 03:28;alienth;One extra thing to note, as expected turning down `tcp_retries2` does greatly alleviate this issue. The `CLOSE-WAIT` socket is nuked by the kernel much more quickly, resulting in the partition resolving quickly.","05/May/18 03:34;alienth;Extra note: At the same second that {{.88}}'s gossiper announced shutdown, there was a newly established socket from {{.40}} to {{.88}}:

{{Fri May 4 16:02:15 PDT 2018}}
 {{tcp ESTAB 0 0 10.0.161.40:59739 10.0.107.88:7000 timer:(keepalive,4min59sec,0) uid:115 ino:5913893 sk:216e <->}}

This is the same socket that would become CLOSE-WAIT only seconds later, and remain that way for the following 18 minutes:

{{Fri May 4 16:02:20 PDT 2018}}
 {{tcp CLOSE-WAIT 1 0 10.0.161.40:59739 10.0.107.88:7000 timer:(keepalive,4min54sec,0) uid:115 ino:5913893 sk:216e -->}}",05/May/18 04:39;alienth;Why on earth didn't the failure detector on {{.40}} see anything for 20 minutes? Possibly due to blocking on an Outbound forever?,"21/May/18 19:06;jolynch;[~alienth] that is interesting and thank you for digging so deeply! If I understand correctly during a {{drain}} the other servers are responsible for noticing the change and closing their connections within the {{[shutdown_announce_in_ms|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/gms/Gossiper.java#L1497]}} period in [response|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/gms/GossipShutdownVerbHandler.java#L37] to the {{GOSSIP_SHUTDOWN}} gossip state, and then the {{[markAsShutdown|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/gms/Gossiper.java#L363-L373]}} method marks it down and forcibly convicts it. I believe that the TCP connections get closed via the {{StorageService}}'s {{onDead}} method which calls {{[onDead|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L2514]}} which calls {{[MessagingService::reset|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/net/MessagingService.java#L505]}} which calls {{[OutboundTcpConnection::closeSocket|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/net/OutboundTcpConnectionPool.java#L80], which [enqueues a sentinel|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L210]}} into the backlog and then the {{[OutboundTcpConnection::run|https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L253]}} method is actually supposed to close it. The {{drainedMessages}} queue is a local reference though so backlog could get something that was enqueued before the {{CLOSE_SENTINEL}} and after it as well. This seems very racey to me, in particular the reconnection logic might race with the closing logic from what I can tell as we have a 2 second window between when the clients start closing and when the server will actually stop accepting new connections (because it closes the listeners).

Non stateful networks would surface the RST in the {{writeConnected}} method, but AWS is like ""yea that machine isn't allowed to talk to that one"" and just blackholes the RSTs... I wonder if I can reproduce this by increasing that window significantly and just sending lots of traffic.","08/Jun/18 18:18;khuizhang;[~jolynch] currently we have an issue with gossip going one way after a node had sudden loss of storage controller. After the offending node comes back online, all the rest nodes show TCP connections on gossip, but those connections  (in bold) are not seen on the offending node. On offending node, nodetool gossipinfo shows generation 0 for all other nodes and nodetool status show DN for all other nodes. On other nodes, nodetool gossipinfo seems fine but nodetool status shows offending node down. This can be resolved by restarting cassandra on all nodes except the offending node, or wait for 2 hours after crash event (tcp_keepalive is set to 7200s on debian?). I don't know if this is related, but wondering if there is any way to verify (like TRACE logging on org.apache.cassandra.gms and/or org.apache.cassandra.net, or maybe packet capture). So far we can reproduce it at 30% chance.  Thanks in advance. 

node 10.96.105.4
*tcp 0 0 10.96.105.4:7001 10.96.105.6:55629 ESTABLISHED keepalive (729.79/0/0)*
*tcp 0 0 10.96.105.4:39219 10.96.105.6:7001 ESTABLISHED keepalive (783.04/0/0)*
*tcp 0 0 10.96.105.4:7001 10.96.105.6:60007 ESTABLISHED keepalive (729.79/0/0)*
tcp 0 0 10.96.105.4:7001 10.96.105.6:45318 ESTABLISHED keepalive (1471.16/0/0)
node 10.96.105.6
tcp 0 0 10.96.105.6:7001 0.0.0.0:* LISTEN off (0.00/0/0)
tcp 0 0 10.96.105.6:45318 10.96.105.4:7001 ESTABLISHED keepalive (1477.00/0/0)","23/Aug/18 01:42;jolynch;[~khuizhang] yea that looks similar with the half open keepalive connections. Did you try the kernel workaround and did it help?

[~aweisberg] [~jasobrown] I've got a mitigation patch so that Cassandra trunk at least heals the half open partitions faster. Please take a look if you have bandwidth for review. While testing the re-connection behavior I ran into CASSANDRA-14503 because the retry future was just getting clobbered by another message, so I couldn't test that we don't keep retrying after just connections are killed (as right now they just retry every message).
||trunk||
|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358]|
|[unit tests|https://circleci.com/gh/jolynch/workflows/cassandra/tree/CASSANDRA-14358]|

This patch makes the timeout configurable for internode connection (2s) and internode tcp user timeout (30s). The timeouts are settable via JMX (and nodetool).

I'm marking this as patch available as I think the operating system workaround is probably ok for previous releases? If it's not just let me know and I can try to figure out how to fix it for those ones as well.

I didn't have any new tests because the only way I'm aware to reproduce this behavior is by using iptables to blackhole traffic. I've been testing with a ccm cluster by trying to block just the small message channel (if gossip is blocked then the failure detector convicts it) and doing a read at ALL:
{noformat}
$ netstat -on | grep 7000 | grep 127.0.0.3:7000 | grep -v ""0 127.0.0.3""
tcp        0      0 127.0.0.1:55604         127.0.0.3:7000          ESTABLISHED keepalive (4093.37/0/0)
tcp        0      0 127.0.0.1:55610         127.0.0.3:7000          ESTABLISHED keepalive (4093.53/0/0)
tcp        0      0 127.0.0.1:58080         127.0.0.3:7000          ESTABLISHED keepalive (6601.96/0/0)

# Try to drop just the small message channel
$ sudo iptables -A OUTPUT -p tcp -d 127.0.0.3 --dport 7000 --sport 58080 -j DROP
{noformat}
Then I just check that we properly reconnect faster than 15 minutes. On an unpatched trunk I can watch the small message channel just sit there probing for 15 retries (the default tcp_retries2 value, node that this netstat is a different run than the previous one) :
{noformat}
netstat -on | grep 7000 | grep 127.0.0.3:7000 | grep -v ""0 127.0.0.3""               
tcp        0      0 127.0.0.1:34532         127.0.0.3:7000          ESTABLISHED keepalive (6823.99/0/0) 
tcp        0      0 127.0.0.1:34564         127.0.0.3:7000          ESTABLISHED keepalive (6840.36/0/0) 
tcp        0    808 127.0.0.1:34544         127.0.0.3:7000          ESTABLISHED probe (108.32/0/10) 
tcp        0      0 127.0.0.1:34554         127.0.0.3:7000          ESTABLISHED keepalive (6840.26/0/0) 
tcp        0      0 127.0.0.1:34540         127.0.0.3:7000          ESTABLISHED keepalive (6831.27/0/0) 
tcp        0      0 127.0.0.1:34534         127.0.0.3:7000          ESTABLISHED keepalive (6828.96/0/0)

{noformat}
And then with my patch it get's killed after like 5 retries instead of waiting the full 15.

If you have ideas for how to unit test it I'm open to suggestions of course.",27/Sep/18 16:23;khuizhang;[~jolynch]: thanks for the answer. I did the kernel workaround and it worked. ,"11/Oct/18 20:34;aweisberg;One argument against this approach is that this is Linux specific and maybe we should go with a solution that works on other platforms (timers and external threads timing stuff out), but to an extent that is the tail wagging the dog, and I like how simple this is.

For testing for dead connections one good reason to heartbeat at the application level is that it can detect bugs at the application level that prevent the networking code from processing messages. This won't catch a connection being wedged like that. And once you have gone to the trouble of heartbeating having the kernel check is superfluous.

I think a heartbeat mechanism is a bit much for 4.0 during the code freeze and this is a small enough change I would rather back it out later (or leave it, it's harmless) if we add heartbeating down the road.

* [Move these defaults to Config | https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-f5e4f8d3a95c98844b371ba1d1e98285R38] and use them there so they can't mismatch. 
* [Can you add these config options to the yaml with these comments, but commented out?|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-b66584c9ce7b64019b5db5a531deeda1R147]
* It would be nice if the unit of the timeout were in the name in more places. Basically add _ms everywhere you can. I would actually leave nodetool the way it is just to satisfy my OCD since units are not in the name of the other options and we don't want to change them. Although adding synonyms might be nice.

My only concern is whether we should ratchet TCP_USER_TIMEOUT timeouts down as a new default. Not because I have a concrete issue with it, but because I lack the operational experience say yeah this is a good idea in practice.

I really want to give people a good default and not cop out and leave it at 10 minutes where it was just because that is how it was. I brought it up in IRC and maybe some people will chime in.","11/Oct/18 20:50;aweisberg;Talked about it in IRC and people think I am overthinking.

I think 30 seconds should be fine. Each server would only see a few hundred incoming connections at once. A very nice benchmark number to have for C* would be the SSL accept rate. Apparently SSL accept is single threaded right now. One thing to watch out for when measuring that kind of thing is sockets being stuck in TIMED_WAIT causing you to run out of ephemeral ports. Setting ""TCP_TW_REUSE"" fixes that issue for benchmarks.

I don't think you need to do that for this issue though.","25/Oct/18 00:10;jolynch;Great! I pushed a patch incorporating your feedback and am running dtests against it. I am slightly concerned that {{OutboundConnectionParams}} now requires the {{DatabaseDescriptor}} to be loaded, but if tests pass I think we're ok?
||trunk||
|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358]|
|dtests:[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14358.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14358]|

I agree that active application level monitoring is a great idea, but to be honest I think this plus the latency probes we added as part of CASSANDRA-14459 should do a pretty good job of detecting and evicting bad connections. There is of course room for improvement but I think it's actually a pretty good start.","29/Oct/18 22:44;aweisberg;[Why add protocol version here?|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-1560ed3bf5675f8ec0b1b35198debe15R41]
[The acceptable range of values differs, but the tests are for both are -1|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-f5e4f8d3a95c98844b371ba1d1e98285R224]. I just want to confirm what the underlying tunables actually support.","29/Oct/18 23:35;jolynch;{quote}[Why add protocol version here?|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-1560ed3bf5675f8ec0b1b35198debe15R41]
{quote}
Because otherwise the test fails the {{protocolVersion}} check instead of the {{sendBufferSize}} check which the test was (I believe) trying to test. Before this patch I believe that the unit test was testing the wrong thing.
{quote}[The acceptable range of values differs, but the tests are for both are -1|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-f5e4f8d3a95c98844b371ba1d1e98285R224]. I just want to confirm what the underlying tunables actually support.
{quote}
Correct, the {{TCP_USER_TIMEOUT}} can be set to zero (which will pick up the OS level setting).
{noformat}
       TCP_USER_TIMEOUT (since Linux 2.6.37)
              This option takes an unsigned int as an argument.  When the value is greater than 0, it specifies the maximum amount of time in milliseconds that transmitted data may remain unacknowledged before TCP will forcibly close the corresponding connection and return ETIMEDOUT to the applica‐
              tion.  If the option value is specified as 0, TCP will to use the system default.
{noformat}
The connection timeout setting also [supports zero|https://netty.io/4.0/api/io/netty/channel/ChannelConfig.html#setConnectTimeoutMillis-int-] (meaning disable) but I don't (imo) think that users should ever disable connection timeouts. A user could plausibly set it to the same value as their RPC timeout, or even higher, but I don't think turning it off ever makes sense.",01/Nov/18 04:34;jolynch;I have a branch over on CASSANDRA-14862 which fixes the broken dtests. Is there any more testing or changes you want me to do on this patch?,"01/Nov/18 14:58;aweisberg;Can you update CHANGES.txt and NEW.txt and then I will commit?

+1","01/Nov/18 18:50;jolynch;I've added the CHANGES and NEWS updates, rebased and squashed down the changes into [cb82946|https://github.com/apache/cassandra/commit/cb82946b48100b06a342a02093dd3bb2c489e25b]. Tests are re-running on my branch.

||trunk||
|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358]|
|dtests:[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14358.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14358]|","01/Nov/18 18:58;aweisberg;Hey, so feedback on NEWS.txt, linking to the ticket is helpful, but saying there are two new tunables in the YAML is important.

For CHANGES.txt I think we always use the title of the JIRA. On commit I frequently change the title of the JIRA to make it more useful in CHANGES.txt. People should know either what changed, or what problem was fixed, or both.","01/Nov/18 20:36;jolynch;[~aweisberg] Ok I think I've re-factored the jira ticket name and CHANGES entry appropriately (since we're not fixing the underlying problem in this ticket, just mitigating). I've also updated the NEWS entry to list the two tunables.

Rebased patch at [ac83f0de|https://github.com/apache/cassandra/commit/ac83f0def640ab89d0a1c911e82d867f6588de4c].","01/Nov/18 20:41;aweisberg;OK, +1 I'll commit. Just a heads up you can write long beautiful commit messages, but they get replaced with a one line subject of the JIRA and a standard Patch by XYZ; Reviews by ZYX for CASSANDRA1234. We don't deviate from that format for commit messages.","01/Nov/18 21:05;jolynch;Ok, sounds reasonable :-)",01/Nov/18 22:37;aweisberg;Committed as [bfbc5274f2b3a5af2cbbe9679f0e78f1066ef638|https://github.com/apache/cassandra/commit/bfbc5274f2b3a5af2cbbe9679f0e78f1066ef638]. Thanks!,"02/Nov/18 17:20;jolynch;For those looking for a fix for earlier versions, unfortunately I was not able to determine the root cause of the close issues, but we have a follow up ticket CASSANDRA-14818 for trying to figure that out (if someone can figure that out I think it's worth fixing that).

That being said these partitions are now mitigated in 4.0 with the TCP_USER_TIMEOUT socket option.

A workaround for earlier versions is to set the {{net.ipv4.tcp_retries2}} sysctl to ~5. This can be done with the following:
{code:bash}
$ cat /etc/sysctl.d/20-cassandra-tuning.conf
net.ipv4.tcp_retries2=5
$ # Reload all sysctls
$ sysctl --system{code}","05/Nov/18 12:15;benedict;{quote}We don't deviate from that format for commit messages{quote}

Is this true?  I recall our standardising on the last line following the format ""Patch by X,Y,Z; reviewed by A,B,C for CASSANDRA-XXXXX"", and the first line approximating the JIRA title.  I don't recall any proposal to prohibit more information between those two lines?

I certainly put more information in sometimes, and should probably do it more.  I'd hate to explicitly discourage it.",06/Nov/18 19:16;aweisberg;I guess I misinterpreted the scolding I got long ago WRT to commit messages.
Bind to correct local address in 4.0 streaming,CASSANDRA-14362,13149864,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,Lerh Low,Lerh Low,Lerh Low,04/Apr/18 01:44,12/Mar/19 14:15,13/Mar/19 22:35,17/Apr/18 12:49,4.0,,,,,,,,0,,,,"I'm not sure if anybody has tried using {{trunk}} and starting an actual EC2 cluster, but with a 3 node setup that works on 3.11, it fails to work on {{trunk}}. I mistakenly stumbled into it while testing my own code changes. 

My setup is as follows:

broadcast_rpc_address: publicIP
broadcast_address: publicIP
listen_address: omitted. Ends up as privateIP. 

Works on 3.11 just fine. 

On {{trunk}} though, it never works. My node is never able to join the cluster:

{code}
Apr 03 05:57:06 ip-10-0-47-122 cassandra[13914]: INFO  [main] 2018-04-03 05:57:05,895 RangeStreamer.java:195 - Bootstrap: range (-128373781239966537,-122439194129870521] exists on 52.88.241.181:7000 for keyspace system_traces
Apr 03 05:57:06 ip-10-0-47-122 cassandra[13914]: INFO  [main] 2018-04-03 05:57:05,895 RangeStreamer.java:195 - Bootstrap: range (6968670424536541270,6973888347502882935] exists on 52.88.241.181:7000 for keyspace system_traces
Apr 03 05:57:42 ip-10-0-47-122 cassandra[13914]: WARN  [GossipTasks:1] 2018-04-03 05:57:42,298 FailureDetector.java:324 - Not marking nodes down due to local pause of 26215173446ns > 5000000000ns
Apr 03 05:57:53 ip-10-0-47-122 cassandra[13914]: WARN  [GossipTasks:1] 2018-04-03 05:57:53,035 FailureDetector.java:324 - Not marking nodes down due to local pause of 10736485907ns > 5000000000ns
Apr 03 05:58:30 ip-10-0-47-122 cassandra[13914]: WARN  [GossipTasks:1] 2018-04-03 05:58:30,790 Gossiper.java:814 - Gossip stage has 28 pending tasks; skipping status check (no nodes will be marked down)
Apr 03 05:58:33 ip-10-0-47-122 cassandra[13914]: WARN  [GossipTasks:1] 2018-04-03 05:58:33,060 Gossiper.java:814 - Gossip stage has 20 pending tasks; skipping status check (no nodes will be marked down)
Apr 03 06:04:33 ip-10-0-47-122 cassandra[13914]: WARN  [GossipTasks:1] 2018-04-03 06:04:33,826 FailureDetector.java:324 - Not marking nodes down due to local pause of 400790432954ns > 5000000000ns
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: WARN  [GossipTasks:1] 2018-04-03 06:04:49,133 Gossiper.java:814 - Gossip stage has 2 pending tasks; skipping status check (no nodes will be marked down)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: ERROR [StreamConnectionEstablisher:1] 2018-04-03 06:04:49,138 StreamSession.java:524 - [Stream #d4cd6420-3703-11e8-a6a5-e51ddc10cfe6] Streaming error occurred on session with peer 52.88.241.181:7000
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: java.io.IOException: failed to connect to 52.88.241.181:7000 (STREAM) for streaming data
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.DefaultConnectionFactory.createConnection(DefaultConnectionFactory.java:98)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.DefaultConnectionFactory.createConnection(DefaultConnectionFactory.java:57)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.async.NettyStreamingMessageSender.createChannel(NettyStreamingMessageSender.java:183)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.async.NettyStreamingMessageSender.setupControlMessageChannel(NettyStreamingMessageSender.java:165)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.async.NettyStreamingMessageSender.sendMessage(NettyStreamingMessageSender.java:222)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.async.NettyStreamingMessageSender.initialize(NettyStreamingMessageSender.java:146)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:271)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:273)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at java.lang.Thread.run(Thread.java:748)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: Caused by: io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Cannot assign requested address
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.unix.Errors.newIOException(Errors.java:117)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.unix.Socket.bind(Socket.java:266)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.epoll.AbstractEpollStreamChannel.doConnect(AbstractEpollStreamChannel.java:729)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.epoll.EpollSocketChannel.doConnect(EpollSocketChannel.java:184)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.connect(AbstractEpollStreamChannel.java:797)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1274)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:545)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:530)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:999)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:260)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.bootstrap.Bootstrap$3.run(Bootstrap.java:254)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:312)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         ... 1 common frames omitted
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: INFO  [StreamConnectionEstablisher:1] 2018-04-03 06:04:49,140 StreamResultFuture.java:197 - [Stream #d4cd6420-3703-11e8-a6a5-e51ddc10cfe6] Session with 52.88.241.181:7000 is complete
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: ERROR [StreamConnectionEstablisher:1] 2018-04-03 06:04:49,146 StreamSession.java:524 - [Stream #d4cd6420-3703-11e8-a6a5-e51ddc10cfe6] Streaming error occurred on session with peer 52.88.241.181:7000
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]: java.lang.RuntimeException: stream has been closed, cannot send Prepare SYN (3 requests,  0 files}
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.async.NettyStreamingMessageSender.sendMessage(NettyStreamingMessageSender.java:209)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.StreamSession.onInitializationComplete(StreamSession.java:495)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:272)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:273)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
Apr 03 06:04:49 ip-10-0-47-122 cassandra[13914]:         at java.lang.Thread.run(Thread.java:748)
{code}

Remote debugging it, it eventually seems to come to be due to trying to call {{socket.bind}} in {{doConnect}} in {{AbstractEpollStreamChannel}}. Local address is <publicIP>:0. I think port 0 is fine because it just means pick the next ephemeral port available, but there's no way of binding to the public IP. 

I made a commit https://github.com/apache/cassandra/commit/00222ddf0694f1c35c2bdd84fd7407174c3fc57a that seems to have fixed it so I can continue on with my purposes, and that is have {{StreamSession}} use listen address instead of broadcast address which may be public. In the event listen address is public I guess that means if it was able to bind to that interface to begin with then {{socket.bind}} should also work. 

Not sure if this is the right way though, and it seems to have been introduced by the Streaming rework to netty. https://issues.apache.org/jira/browse/CASSANDRA-12229. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-04-04 01:58:33.166,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 19 00:07:38 UTC 2018,,,,,,0|i3s413:,9223372036854775807,4.0,,,,jasobrown,jasobrown,,,,,,,,,,,04/Apr/18 01:45;Lerh Low;Any thoughts on this [~jasobrown]...? :),"04/Apr/18 01:58;jasobrown;I'll take a look tomorrow, certainly looks like me :). Which snitch are you using?","04/Apr/18 02:01;Lerh Low;I'm using EC2Snitch (mostly was just using bare bones to get a cluster working). If the commit I made seems like the appropriate fix to you (I may be missing something.... :/ ) then I'm happy to create a proper patch for it. 

(And feel free to LMK if you need any more details)","04/Apr/18 22:04;jasobrown;So it looks like when working on CASSANDRA-8457/CASSANDRA-12229, I introduced a regression of CASSANDRA-12673. The short of it is we no longer bind the local side of a outbound connection as of CASSANDRA-12673, yet I mistakenly brought that back (see [{{NettyFactory#createOutboundBootstrap()}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/NettyFactory.java#L337]).

I believe the most correct fix is just not setting the local address to bind to for outbound connections, as that's bascially what CASSANDRA-12673 does. Removing the local bind should resolve [~Lerh Low]'s error as well as eliminate the regression against CASSANDRA-12673. However, we should also incorporate [~Lerh Low]'s fix as well, to clarify the intent of which address we want to 'reference' (but not actually use) on the local side.
||14362||
|[branch|https://github.com/jasobrown/cassandra/tree/14362]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14362]|

tests are running now. 

[~aweisberg] do you mind reviewing?

UPDATE: dtests seem to be pretty unhappy. I'll investigate what's wrong and ping this ticket with updates.","08/Apr/18 11:07;djoshi3;[~jasobrown] / [~aweisberg] I think I know what the issue is. [~jasobrown] not only changed the {{OutboundConnectionIdentifier}} to prefer the local address of the node but also removed the binding to a specific local IP address during {{NettyFactory#createOutboundBootstrap}} setup. This had an unintended consequence. When you leave the {{Socket}}'s source IP (local address) unset, the kernel will choose one for you. This normally is not an issue. However, during the dtest run all nodes are local and rely on 127.0.0.x IPs. When the C* nodes try to setup the streaming connections, they get tripped up due to this change.

Say node1 (127.0.0.1) & node2 (127.0.0.2) are started up by the dtest (I know in reality its 3 nodes but for the sake of simplicity I'm limiting it to two nodes). node1 attempts to create an outbound connection for streaming with node2, the outbound socket should actually be {{127.0.0.1:ANY_PORT -> 127.0.0.2:INTERNODE_PORT}}. However, per this patch, we leave the source IP unbound. When the connection is established and node1 sends the {{FirstHandshakeMessage}}, it has a source IP set to set to an arbitrarily chose IP by the kernel - in this case it was {{127.0.0.2}} (when it should actually be {{127.0.0.1}}).

The receiving node (node2) gets this message {{InboundHandshakeHandler#handleStart}}, decodes it fine but when it goes to {{InboundHandshakeHandler#setupStreamingPipeline}}, it creates a {{StreamingInboundHandler}} with remote IP set to {{127.0.0.2}}. This messes up the streaming pipeline because now node2 is waiting on {{127.0.0.2}} (itself) for data. Hence we find it stuck in the bootstrap phase -
{noformat}
""Stream-Deserializer-127.0.0.2:7000-4004ef3b"" #133 daemon prio=5 os_prio=35 tid=0x00007ff58c601ca0 nid=0xdc03 waiting on condition [0x0000700007613000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at java.lang.Thread.sleep(Thread.java:340)
	at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386)
	at com.google.common.util.concurrent.Uninterruptibles.sleepUninterruptibly(Uninterruptibles.java:285)
	at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:174)
	at java.lang.Thread.run(Thread.java:748)

""main"" #1 prio=5 os_prio=35 tid=0x00007ff58c706000 nid=0x2803 waiting on condition [0x0000700004e5e000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007a11a3f98> (a org.apache.cassandra.streaming.StreamResultFuture)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:497)
	at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:1519)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:949)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:651)
	- locked <0x00000007a40f80b8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:586)
	- locked <0x00000007a40f80b8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:367)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:590)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:679)
{noformat}
I also noted this error in the logs which further points to the wrong IP for the peer being used -
{noformat}
ERROR [Stream-Deserializer-127.0.0.2:53830-7b781083] 2018-04-06 13:19:25,863 StreamingInboundHandler.java:210 - [Stream channel: 7b781083] stream operation from 127.0.0.2:53830 failed
java.lang.IllegalArgumentException: Unknown peer requested: 127.0.0.2:7000
        at org.apache.cassandra.streaming.StreamCoordinator.getHostData(StreamCoordinator.java:242)
        at org.apache.cassandra.streaming.StreamCoordinator.getSessionById(StreamCoordinator.java:170)
        at org.apache.cassandra.streaming.StreamResultFuture.getSession(StreamResultFuture.java:237)
        at org.apache.cassandra.streaming.StreamManager.findSession(StreamManager.java:194)
        at org.apache.cassandra.streaming.StreamManager.findSession(StreamManager.java:185)
        at org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:41)
        at org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:36)
        at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:55)
        at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:177)
        at java.lang.Thread.run(Thread.java:748)
{noformat}
I added a bit of debugging code and turned {{TRACE}} logging on and confirmed that this was indeed the case.

Below is the output of node2's debug.log in the 3 node cluster that dtest run created. node1, 2 & 3 were assigned IPs {{127.0.0.1, 127.0.0.2, 127.0.0.3}} respectively. Note that node2's actual IP is never used as we've left it unbound so it seems the OS kernel prefers the IP that it thinks is closest to the remote IP so channel local & remote as the showing up as the same IP.
{noformat}
03:07 $ tail -F /var/folders/5f/_29kl4f524l7gp__02cxby_c0000gn/T/dtest-wu9pai7a/test/node2/logs/debug.log  | grep handshake
TRACE [MessagingService-NettyOutbound-Thread-4-1] 2018-04-08 03:07:55,360 OutboundHandshakeHandler.java:107 - starting handshake with peer 127.0.0.1:7000, msg = FirstHandshakeMessage - messaging version: 12, mode: MESSAGING, compress: false, channel local = /127.0.0.1:65020 remote = /127.0.0.1:7000
TRACE [MessagingService-NettyOutbound-Thread-4-3] 2018-04-08 03:09:21,427 OutboundHandshakeHandler.java:107 - starting handshake with peer 127.0.0.3:7000, msg = FirstHandshakeMessage - messaging version: 12, mode: MESSAGING, compress: false, channel local = /127.0.0.3:65091 remote = /127.0.0.3:7000
{noformat}
I only modified the trace log line in {{OutboundHandshakeHandler#channelActive}} as below -
{noformat}
        logger.trace(""starting handshake with peer {}, msg = {}, channel local = {} remote = {}"", connectionId.connectionAddress(),
                     msg, ctx.channel().localAddress(), ctx.channel().remoteAddress());
{noformat}

What is unclear to me is why is this an issue now if CASSANDRA-12673 left the local side unbound?","17/Apr/18 12:49;jasobrown;[~Lerh Low]'s patch is correct and fixes a bug I introduced with CASSANDRA-12229. Further, there is something wrong with my handling of CASSANDRA-12673, but I don't want to hold up Lerh or anyone else looking at 4.0 in ec2. Thus, I've decided to commit Lerh's patch and will open a followup ticket to address the local address binding issue.

Committed as sha {{70d95359d2dca1c35f573776d11ed87bb9b4b441}}. Nice find and fix, Lehr.","17/Apr/18 12:53;jasobrown;Opened CASSANDRA-14389 for the local address binding regression. Thanks, [~djoshi3] for also looking at this ticket.","19/Apr/18 00:07;Lerh Low;It's not blocking me because I have that workaround, but thanks for looking into it so quickly, much appreciated :) ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C* nodetool should report the lowest of the highest CQL protocol version supported by all clients connecting to it,CASSANDRA-14335,13147045,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,djoshi3,djoshi3,djoshi3,22/Mar/18 03:29,12/Mar/19 14:15,13/Mar/19 22:35,04/May/18 18:50,4.0,,,,,,,,0,,,,"While upgrading C*, it makes it hard to tell whether any client will be affected if C* is upgraded. C* should internally store the highest protocol version of all clients connecting to it. The lowest supported version will help determining if any client will be adversely affected by the upgrade.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-05-02 01:35:10.755,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri May 04 21:40:02 UTC 2018,,,,,,0|i3rmpr:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"01/May/18 22:29;djoshi3;||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/14335-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/14335-trunk]|
||",02/May/18 01:35;zznate;[~djoshi3] You have some config changes to CircleCI as part of this branch - was that intentional? Can we put those in a separate ticket if so?,"02/May/18 01:54;jasobrown;[~zznate] the circleci changes are just for running the utests/dtests, and not part of the real commit. We make sure to not commit those.","02/May/18 05:39;djoshi3;Thank you [~jasobrown]. [~zznate] just so you have more background, I have a paid CircleCI account that supports higher quotas. In order to use it, I need to update the settings. So my patches are typically two commits. You can cherry pick just the patch and ignore the CircleCI commit. If there is a better way to do this, I'm happy to discuss it :)",02/May/18 05:41;djoshi3;I have fixed some minor issues with the patch that I spotted. I am not 100% certain that this class needs to be thread safe. Thoughts?,"02/May/18 13:27;jasobrown;We already store the {{ProtocolVersion}} along with the channel in {{Connection}}. We also store a reference to the {{Connection}} in the {{Channel}}, as a channel attribute. Thus, it seems to me you could just add a method like this to {{Server.ConnectionTracker}} (I changed the return type to make the example simpler):

{code:java}
        public Map<ProtocolVersion, Set<SocketAddress>> getClientsByProtocolVersion()
        {
            Map<ProtocolVersion, Set<SocketAddress>> result = new EnumMap<>(ProtocolVersion.class);
            for (Channel c : allChannels)
            {
                Attribute<Connection> attrConn = c.attr(Connection.attributeKey);
                Connection connection = attrConn.get();
                if (connection != null)
                {
                    ProtocolVersion version = connection.getVersion();
                    SocketAddress addr = c.remoteAddress();
                    result.computeIfAbsent(version, protocolVersion -> new HashSet<>());
                    result.get(version).add(addr);
                }
            }
            return result;
        }
{code}

(Note: {{ConnectionTracker#allChannels}} is a {{DefaultChannelGroup}}, whose {{Iterator()}} method wraps two {{ConcurrentHashMap}} s, so I think you are safe concurrency-wise.) This gives you a snapshot of everything that is currently connected. Is this sufficient, [~djoshi3]?","02/May/18 15:20;djoshi3;This will just give me the currently connected clients. When the client connects and drops off we won’t have any idea of what protocol version it used. So, my patch keeps a history of last N Clients and their protocol versions. ","03/May/18 22:35;djoshi3;[~jasobrown] I have updated my branch with an implementation that will be threadsafe as well as performant. As part of this change, I am introducing addition flags for {{nodetool clientstats}} command. We can use {{nodetool clientstats --by-protocol}} to give you the list last 100 clients that the C* process has seen for each protocol. I am deduping by IP address so you should see unique IPs. You can clear this list by running {{nodetool clientstats --clear-history}} to clear the history of all clients.

For posterity - [~jasobrown] and I had a conversation about the usefulness of having this information. Although the existing {{nodetool clientstats --all}} is sufficient to tell you what clients are _currently_ connected to the C* process including the protocol version, this information is ephemeral. There are many situations where an operator may miss intermittently connecting clients for example - batch processes. Additionally, we can script around by periodically invoking {{nodetool clientstats --all}} from external scripts but that is sub-optimal for a variety of reasons - you can still miss intermittent clients, you unnecessarily make calls over JMX. Storing this historical information in the C* process means you wont miss intermittently connecting clients and would be overall more efficient that going over JMX all the time.","04/May/18 18:50;jasobrown;+1, and committed with a few minor spacing/naming changes as sha {{a9ec46a613ae5602ced004935c9954638e83e735}}","04/May/18 19:58;jasobrown;Ughh, forgot to commit my minor changes locally before pushing up. commited those changes as {{68605cf03bdfecb11cd69c6d5260a773e4e87300}}",04/May/18 21:40;djoshi3;Thanks [~jasobrown],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The order of application of nodetool garbagecollect is broken,CASSANDRA-14870,13196765,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,blambov,blambov,blambov,07/Nov/18 08:22,12/Mar/19 14:15,13/Mar/19 22:35,13/Nov/18 11:10,3.11.4,4.0,,,Local/Compaction,,,,0,,,,"{{nodetool garbagecollect}} was intended to work from oldest sstable to newest, so that the collection in newer tables can purge tombstones over data that has been deleted.

However, {{SSTableReader.maxTimestampComparator}} currently sorts in the opposite order (the order changed in CASSANDRA-13776 and then back in CASSANDRA-14010), which makes the garbage collection unable to purge any tombstones.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-13 07:39:17.116,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 13 11:07:54 UTC 2018,,,,,,0|s007q8:,9223372036854775807,,,,,jasonstack,jasonstack,,,,,,,,,,,"07/Nov/18 12:36;blambov;Patch for 3.11 linked, applies with insignificant changes to trunk too. Incorporates CASSANDRA-14099 as that required the same change of order.
The LCS ordering test (by [~VincentWhite]) is copied from the CASSANDRA-14099 patch.",08/Nov/18 08:34;blambov;testall and dtests are clean on DataStax CI servers.,13/Nov/18 07:39;jasonstack;Sorry for delay.. Patch LGTM!,13/Nov/18 11:07;blambov;Committed to 3.11 as [a03424ef95559c9df2bb7f86e1ac1edca1436058|https://github.com/apache/cassandra/commit/a03424ef95559c9df2bb7f86e1ac1edca1436058] and trunk as [b80f6c65fb0b97a8c79f6da027deac06a4af9801|https://github.com/apache/cassandra/commit/b80f6c65fb0b97a8c79f6da027deac06a4af9801].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unbounded validation compactions on repair,CASSANDRA-14332,13146727,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,KurtG,KurtG,KurtG,21/Mar/18 04:32,12/Mar/19 14:15,13/Mar/19 22:35,17/Apr/18 21:24,3.0.17,3.11.3,,,,,,,0,,,,"After CASSANDRA-13797 it's possible to cause unbounded, simultaneous validation compactions as we no longer wait for validations to finish. Potential fix is to have a sane default for the # of concurrent validation compactions by backporting CASSANDRA-13521 and setting a sane default.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-04-13 15:56:29.187,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 17 21:24:36 UTC 2018,,,,,,0|i3rkrr:,9223372036854775807,,,,,bdeggleston,bdeggleston,,,,,,,,,,,"13/Apr/18 04:24;KurtG;|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...kgreav:13797-r-3.0]|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...kgreav:13797-r-3.11]| 
|[utests|https://circleci.com/gh/kgreav/cassandra/152]|[utests|https://circleci.com/gh/kgreav/cassandra/154]|

I think reverting this is the correct solution for 3.0 and 3.x. Changing defaults in minors while in this case is probably fine, it's not good practice. Backporting CASSANDRA-13521 completely is also a bit questionable. 

The original problem from CASSANDRA-13797 of the interrupted exceptions is only very minor, and likely not possible outside of CCM clusters.","13/Apr/18 15:56;bdeggleston;fwiw, the interrupted exception _wasn't_ from a ccm cluster, so this can happen in the wild. That said, occasionally getting interrupted exceptions when repairing inactive tables is a lot less of a problem than unbounded repair sessions.",17/Apr/18 21:24;bdeggleston;committed as {{00e5a3d508eb41944ce01c6cc96ae18cb16dad8c}} thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running repair on multiple nodes in parallel could halt entire repair ,CASSANDRA-14804,13189347,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,04/Oct/18 04:34,12/Mar/19 14:15,13/Mar/19 22:35,05/Oct/18 23:08,3.0.18,,,,Consistency/Repair,,,,0,,,,"Possible deadlock if we run repair on multiple nodes at the same time. We have come across a situation in production in which if we repair multiple nodes at the same time then repair hangs forever. Here are the details:

Time t1
 {{node-1}} has issued repair command to {{node-2}} but due to some reason {{node-2}} didn't receive request hence {{node-1}} is awaiting at [prepareForRepair |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/ActiveRepairService.java#L333] for 1 hour *with lock*

Time t2
 {{node-2}} sent prepare repair request to {{node-1}}, some exception occurred on {{node-1}} and it is trying to cleanup parent session [here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/repair/RepairMessageVerbHandler.java#L172] but {{node-1}} cannot get lock as 1 hour of time has not yet elapsed (above one)

snippet of jstack on {{node-1}}
{quote}""Thread-888"" #262588 daemon prio=5 os_prio=0 waiting on condition
 java.lang.Thread.State: TIMED_WAITING (parking)
 at sun.misc.Unsafe.park(Native Method)
 - parking to wait for (a java.util.concurrent.CountDownLatch$Sync)
 at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
 at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277)
 at org.apache.cassandra.service.ActiveRepairService.prepareForRepair(ActiveRepairService.java:332)
 - locked <> (a org.apache.cassandra.service.ActiveRepairService)
 at org.apache.cassandra.repair.RepairRunnable.runMayThrow(RepairRunnable.java:214)
 at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
 at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$9/864248990.run(Unknown Source)
 at java.lang.Thread.run(Thread.java:748)

""AntiEntropyStage:1"" #1789 daemon prio=5 os_prio=0 waiting for monitor entry []
 java.lang.Thread.State: BLOCKED (on object monitor)
 at org.apache.cassandra.service.ActiveRepairService.removeParentRepairSession(ActiveRepairService.java:421)
 - waiting to lock <> (a org.apache.cassandra.service.ActiveRepairService)
 at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:172)
 at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
 at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$9/864248990.run(Unknown Source)
 at java.lang.Thread.run(Thread.java:748){quote}
Time t3:
 {{node-2}}(and possibly other nodes {{node-3}}…) sent [prepare request |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/ActiveRepairService.java#L333] to {{node-1}}, but {{node-1}}’s AntiEntropyStage thread is busy awaiting for lock at {{ActiveRepairService.removeParentRepairSession}}, hence {{node-2}}, {{node-3}} (and possibly other nodes) will also go in 1 hour wait *with lock*. This rolling effect continues and stalls repair in entire ring.

If we totally stop triggering repair then system would recover slowly but here are the two major problems with this:
 1. Externally there is no way to decide whether to trigger new repair or wait for system to recover
 2. In this case system recovers eventually but it takes probably {{n}} hours where n = #of repair requests fired, only way to come out of this situation is either to do a rolling restart of entire ring or wait for {{n}} hours before triggering new repair request

Please let me know if my above analysis makes sense or not.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-10-05 20:07:36.59,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 05 23:08:30 UTC 2018,,,,,,0|i3ytfb:,9223372036854775807,,,,,,,,,,,,,,,,,04/Oct/18 04:35;chovatia.jaydeep@gmail.com;[~bdeggleston] [~jjirsa] Could you please see if this analysis makes sense or not?,"05/Oct/18 20:07;bdeggleston;[~chovatia.jaydeep@gmail.com] I’m not sure how we'd get to the state in t2. We wait for an hour on a semaphore we instantiate in {{prepareForRepair}}, and {{removeParentRepairSession}} is synchronized on the object monitor. One shouldn’t block the other. I think the jstack in the description is missing the thread where the {{ActiveRepairService}} monitor is being held. ","05/Oct/18 20:52;chovatia.jaydeep@gmail.com;In our branch we have {{prepareForRepair}}  *{{synchronized}}* yet, it was fixed in CASSANDRA-13849 which we missed to backport. 
Let me back port CASSANDRA-13849 to our branch and then hopefully this will fix the issue.

Thanks a lot [~bdeggleston] for your help!","05/Oct/18 23:08;bdeggleston;No problem, glad you got it figured out",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Legacy sstables with range tombstones spanning multiple index blocks create invalid bound sequences on 3.0+,CASSANDRA-14823,13191299,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,12/Oct/18 21:01,12/Mar/19 14:15,13/Mar/19 22:35,16/Oct/18 19:52,3.0.18,3.11.4,,,Legacy/Local Write-Read Paths,,,,0,,,,"During upgrade from 2.1 to 3.0, reading old sstables in reverse order would generate invalid sequences of range tombstone bounds if their range tombstones spanned multiple column index blocks. The read fails in different ways depending on whether the 2.1 tables were produced by a flush or a compaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-10-14 16:50:09.734,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 29 16:27:01 UTC 2018,,,,,,0|i3z5dr:,9223372036854775807,,,,,,beobal,iamaleksey,,,,,,,,,,"12/Oct/18 23:46;bdeggleston;[3.0 fix|https://github.com/bdeggleston/cassandra/tree/14823-3.0]
[2.1 sstable generator|https://github.com/bdeggleston/cassandra/tree/14823-2.1]
[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F14823-3.0]

This fixes a few bugs related to range tombstones and iterating over legacy sstables in reverse.

First, in 2.1, range tombstones spanning multiple indexed blocks are rewritten at the beginning of each block they partially cover. 3.0 expects open markers to be noted in the column index, so it treats these as normal range tombstones, which will lead to duplicate start bounds being emitted, and incorrect dropping of rows in some cases. When generated by memtable flush, the repeated range tombstone open marker is less than the current index block first name. When generated by compaction, the index block first name is set to the repeated tombstone's open bound, which is less than the preceding index block's last name. Fixing this complicates the logic of determining if the contents of a row span multiple index blocks and correctly handling them, so that's been modified here as well.

Next, since the open and closing bound of a range tombstone are stored together on disk, we stash the closing bound in memory to read out later. However we use the file pointer to figure out when we should stop reading an index block. So any rt closing bound that should be the last thing we emit for an index block gets dropped, leading to an open ended closing bound being emitted. That's fixed here as well.",12/Oct/18 23:47;bdeggleston;[~benedict] [~iamaleksey] would one (or both) of you mind taking a look at this as well?,"14/Oct/18 16:50;beobal;This is a nasty one. The patch essentially LGTM, though I've pushed a couple of tiny commits with minor suggestions [here|https://github.com/beobal/cassandra/commits/14823-3.0]. The first fixes a couple of typos, tweaks the comments slightly in SSTRI and changes a param name. The second switches from using Guava {{Preconditions}} to {{Verify}}, which is not a big deal at all, but seems marginally better aligned semantically. e.g.
{quote}If checking whether the _caller_ has violated your method or constructor's contract (such as by passing an invalid argument), use the utilities of the Preconditions class instead.""
{quote}
from [the Verify doc|https://google.github.io/guava/releases/18.0/api/docs/com/google/common/base/Verify.html])

One last thing to note is that the merge to 3.11 is not entirely clean but it is trivially resolvable and tests do pass once merged.","15/Oct/18 14:24;beobal;I realised that I'd overlooked one additional aspect here: that a 2.1 has the potential to have multiple repeated RTs following a block boundary. So I've updated the test data generator to do that, though I could only figure out how to do it via the compaction path. While I was at it, I changed the naming of the test tables slightly from {{...\_compact\_...}} to {{...\_compacted\_...}} to avoid potential confusion due to the existing test tables named that way to indicate that they use {{COMPACT STORAGE}}. 
[3.0 branch|https://github.com/beobal/cassandra/tree/14823-3.0]
[2.1 sstable generator|https://github.com/beobal/cassandra/commit/420457c3192952206e07276be7c2edf86aa79a7e]
[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14823-3.0]
","15/Oct/18 21:46;bdeggleston;Changes look good to me. Pulled in, merged to 3.11 and started another run of tests:
|[3.0|https://github.com/bdeggleston/cassandra/tree/14823-3.0]|[circle|https://circleci.com/workflow-run/fd588958-4771-4d4e-8569-772415523c88]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/14823-3.11]|[circle|https://circleci.com/workflow-run/37a12a6a-f2ab-4591-a5c7-83b926ba655a]|",16/Oct/18 17:00;beobal;+1,16/Oct/18 18:45;iamaleksey;LGTM,"16/Oct/18 19:52;bdeggleston;Committed as [285153f621a1e67212ef61686188eff3e9c80a4e |https://github.com/apache/cassandra/commit/285153f621a1e67212ef61686188eff3e9c80a4e], thanks",19/Oct/18 20:49;madega;Does this fix apply to 3.11.3 release?,"12/Nov/18 23:18;jjirsa;[~madega] yes, it will impact 3.11.3, and will be fixed with 3.11.4 when it's released.
","29/Nov/18 05:52;michaelsembwever;[~bdeggleston], correct me if i'm wrong please…

For this to impact a user all the following must be true:
 - upgrading from 2.1,
 - clients were performing partition or multi-row deletes, creating range tombstones,
 - partitions have multiple rows and are {{>64KB}} (ie has IndexInfo column index blocks),
 - before the post-upgrade {{`nodetool upgradesstables`}} is complete, clients perform reads with an ordering clause (that reverses the clustering column's ordering on disk).

That is a data-model that does not do {{DELETEs}} (or only does single-row or column {{DELETEs}}) will not be affected by this bug.
 Likewise a data-model that never specifies the ""{{ORDER BY""}} clause in any {{SELECTs}} will not be affected by this bug.","29/Nov/18 16:27;bdeggleston;That’s correct Mick, it almost definitely affects people upgrading from 2.2 as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race Condition in batchlog replica collection,CASSANDRA-14742,13184662,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,12/Sep/18 16:18,12/Mar/19 14:15,13/Mar/19 22:35,27/Sep/18 16:05,4.0,,,,,,,,0,pull-request-available,,,"When we collect nodes for it in {{StorageProxy#getBatchlogReplicas}}, we already filter out down replicas; subsequently they get picked up and taken for liveAndDown.

There's a possible race condition due to picking tokens from token metadata twice (once in {{StorageProxy#getBatchlogReplicas}} and second one in {{ReplicaPlan#forBatchlogWrite}})",,"GitHub user ifesdjeen opened a pull request:

    https://github.com/apache/cassandra/pull/267

    Consolidate batch write code

    for CASSANDRA-14742

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ifesdjeen/cassandra CASSANDRA-14742

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/267.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #267
    
----

----
;17/Sep/18 14:12;githubbot;600","Github user ifesdjeen closed the pull request at:

    https://github.com/apache/cassandra/pull/267
;04/Oct/18 09:20;githubbot;600",,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-26 11:19:15.958,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 27 16:05:24 UTC 2018,,,,,,0|i3y0mv:,9223372036854775807,,,,,benedict,benedict,,,,,,,,,,,"17/Sep/18 14:17;ifesdjeen;We're picking batchlog CL of {{ONE}} or {{TWO}} since the logic in {{BatchlogManager#EndpointFilter}} allows either one node (local, in case of single-node data centers), or two replicas from different racks.","26/Sep/18 11:19;benedict;Patch looks good overall, just a few nits:

# Right now, {{ReplicaPlans}} is organised into counter writes, regular writes, regular write utilities, reads, reads utilities; I think it would be cleanest to keep the batch write utilities similarly proximal to the batch writes themselves, for consistency
# {{syncWriteBatchedMutations}} and {{forBatchlogWrite}} each accept a {{localDc}} parameter - this seems a bit weird, since it's a global variable, and only ever invoked with this (but also, we obtain it inconsistently, by asking the snitch instead of the cached {{localDc}}.  Perhaps they should each just use the latter, without requiring it as a parameter?  (I realise this is pre-existing)
# Unused imports in {{ReplicaPlans}}","27/Sep/18 08:47;ifesdjeen;Thank you, have addressed all three comments and took a  liberty to introduce {{localDatacenter}} and {{localRack}} shortcuts, waiting for one more CI run.","27/Sep/18 08:59;benedict;Thanks.

I'm a little concerned about the inconsistency of our {{DatabaseDescriptor.getLocalDataCenter}} and this {{IEndpointSnitch.getLocalDataCenter}}.  I had intended to simply refer to the former (although, debatably, only the latter should exist - since the former is not consistently updated, although it should for correctness never change).

I'm honestly not clear what the best clean up of this mess is, particularly with the distinction between the per-replication strategy snitch and the global snitch (the former of which seem to simply cache the latter), and perhaps it can be deferred to a later dedicated cleanup ticket anyway.

Otherwise, +1","27/Sep/18 16:05;ifesdjeen;Thank you,

Committed to trunk as [29f83b88821c4792087df19d829ac87b5c06e9e6|https://github.com/apache/cassandra/commit/29f83b88821c4792087df19d829ac87b5c06e9e6]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column result order can change in 'SELECT *' results when upgrading from 2.1 to 3.0 causing response corruption for queries using prepared statements when static columns are used,CASSANDRA-14638,13178327,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,andrew.tolbert,andrew.tolbert,10/Aug/18 17:13,12/Mar/19 14:15,13/Mar/19 22:35,16/Aug/18 14:50,3.0.18,3.11.4,4.0,,,,,,0,,,,"When performing an upgrade from C* 2.1.20 to 3.0.17 I observed that the order of columns returned from a 'SELECT *' query changes, particularly when static columns are involved.

This may not seem like that much of a problem, however if using Prepared Statements, any clients that remain connected during the upgrade may encounter issues consuming results from these queries, as data is reordered and the client not aware of it.  The result definition is sent in the original prepared statement response, so if order changes the client has no way of knowing (until C* 4.0 via CASSANDRA-10786) without re-preparing, which is non-trivial as most client drivers cache prepared statements.

This could lead to reading the wrong values for columns, which could result in some kind of deserialization exception or if the data types of the switched columns are compatible, the wrong values.  This happens even if the client attempts to retrieve a column value by name (i.e. row.getInt(""colx"")).

Unfortunately I don't think there is an easy fix for this.  If the order was changed back to the previous format, you risk issues for users upgrading from older 3.0 version.  I think it would be nice to add a note in the NEWS file in the 3.0 upgrade section that describes this issue, and how to work around it (specify all column names of interest explicitly in query).

Example schema and code to reproduce:

 
{noformat}
create keyspace ks with replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

create table ks.tbl (p0 text,
  p1 text,
  m map<text, text> static,
  t text,
  u text static,
  primary key (p0, p1)
);

insert into ks.tbl (p0, p1, m, t, u) values ('p0', 'p1', { 'm0' : 'm1' }, 't', 'u');{noformat}
 

When querying with 2.1 you'll observe the following order via cqlsh:
{noformat}
 p0 | p1 | m            | u | t
----+----+--------------+---+---
 p0 | p1 | {'m0': 'm1'} | u | t{noformat}
 

With 3.0, observe that u and m are transposed:

 
{noformat}
 p0 | p1 | u | m            | t
----+----+---+--------------+---
 p0 | p1 | u | {'m0': 'm1'} | t{noformat}
 

 
{code:java}
import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.ColumnDefinitions;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import com.google.common.util.concurrent.Uninterruptibles;
import java.util.concurrent.TimeUnit;

public class LiveUpgradeTest {

  public static void main(String args[]) {
    Cluster cluster = Cluster.builder().addContactPoints(""127.0.0.1"").build();
    try {
      Session session = cluster.connect();
      PreparedStatement p = session.prepare(""SELECT * from ks.tbl"");

      BoundStatement bs = p.bind();

      // continually query every 30 seconds
      while (true) {
        try {
          ResultSet r = session.execute(bs);
          Row row = r.one();
          int i = 0;
          // iterate over the result metadata in order printing the
          // index, name, type, and length of the first row of data.
          for (ColumnDefinitions.Definition d : r.getColumnDefinitions()) {
            System.out.println(
                i++
                    + "": ""
                    + d.getName()
                    + "" -> ""
                    + d.getType()
                    + "" -> val = ""
                    + row.getBytesUnsafe(d.getName()).array().length);
          }
        } catch (Throwable t) {
          t.printStackTrace();
        } finally {
          Uninterruptibles.sleepUninterruptibly(30, TimeUnit.SECONDS);
        }
      }
    } finally {
      cluster.close();
    }
  }
}
{code}
To reproduce, set up a cluster, the schema, and run this script.  Then upgrade the cluster to 3.0.17 (with ccm, ccm stop; ccm node1 setdir -v 3.0.17; ccm start works) and observe after the client is able to reconnect that the results are in a different order.  i.e.:

 

With 2.x:

 
{noformat}
0: p0 -> varchar -> val = 2
1: p1 -> varchar -> val = 2
2: m -> map<varchar, varchar> -> val = 16
3: u -> varchar -> val = 1
4: t -> varchar -> val = 1{noformat}
 

With 3.x:

 
{noformat}
0: p0 -> varchar -> val = 2
1: p1 -> varchar -> val = 2
2: m -> map<varchar, varchar> -> val = 1
3: u -> varchar -> val = 16 (<-- the data for 'm' is now at index 3)
4: t -> varchar -> val = 1{noformat}
 

 

 

 ",Single C* node ccm cluster upgraded from C* 2.1.20 to 3.0.17,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-08-15 16:12:20.3,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 16 14:49:42 UTC 2018,,,,,,0|i3wxvj:,9223372036854775807,,,,,benedict,benedict,,,,,,,,,,,"15/Aug/18 16:12;iamaleksey;Unfortunately this is an accidental, lower level bug in 3.0, that I believe should be fixed in the next minor.

The internal contract for wildcard order in 2.1, and, really, everywhere since introduction of static columns is the following:
1. Partition key columns in their positional order
2. Clustering columns in their positional order
3. All static columns, ordered alphabetically by name, irregardless of whether they are simple or complex
4. All regular columns, ordered alphabetically by name, irregardless of whether they are simple or complex

To illustrate, let's say we have a table defined as
{code}
CREATE TABLE ks.tbl (p0 text, p1 text, a set<text>, b set<text> static, c text, d text static, e set<text>, f set<text> static, g text, h text static, i set<text>, j set<text> static, PRIMARY KEY (p0, p1));
{code}
And one inserted row:
{code}
INSERT INTO ks.tbl (p0, p1, a, b, c, d, e, f, g, h, i, j) VALUES ('p0', 'p1', {'a'}, {'b'}, 'c', 'd', {'e'}, {'f'}, 'g', 'h', {'i'}, {'j'});
{code}

In 2.1, performing a {{SELECT * FROM ks.tbl;}} query would return the following result set:
{code}
 p0 | p1 | b     | d | f     | h | j     | a     | c | e     | g | i
----+----+-------+---+-------+---+-------+-------+---+-------+---+-------
 p0 | p1 | {'b'} | d | {'f'} | h | {'j'} | {'a'} | c | {'e'} | g | {'i'}
{code}
3.0, in contrast, would return the following result set:
{code}
 p0 | p1 | d | h | b     | f     | j     | a     | c | e     | g | i
----+----+---+---+-------+-------+-------+-------+---+-------+---+-------
 p0 | p1 | d | h | {'b'} | {'f'} | {'j'} | {'a'} | c | {'e'} | g | {'i'}
{code}

Both correctly return {{PRIMARY KEY}} columns in their defined order first, then all the static columns (although 3.0 order for them is all wrong), followed by all regular columns in alphabetic order, correct in both 2.1 and 3.0.

What 3.0 does incorrectly is ordering the static columns it returns. Instead of merging simple and complex static columns, we are getting all simple static columns first, followed by all complex static columns.

The bug is deep in {{Columns}} class, more specifically in {{findFirstComplexIdx()}} method. To find the first complex column index we are using a surrogate {{ColumnDefinition}} value {{FIRST_COMPLEX}}, which is of kind {{REGULAR}} for {{Columns}} of both {{REGULAR}} and {{STATIC}} kinds. This quite clearly breaks {{Columns}} of kind {{STATIC}}, making the container believe that all of its columns are {{COMPLEX}}.","15/Aug/18 16:14;iamaleksey;Proposed work-in-progress fix (test coverage pending) that addresses the underlying issue can be found [here|https://github.com/iamaleksey/cassandra/commits/14638-3.0], with CI upcoming [here|https://circleci.com/workflow-run/55195e85-6d1a-4864-8c0f-ca656b843f56].","15/Aug/18 16:22;iamaleksey;bq. Unfortunately I don't think there is an easy fix for this.  If the order was changed back to the previous format, you risk issues for users upgrading from older 3.0 version.

This is true. That said, there are huge fleets still on 2.1 and 2.2 in the wild, so leaving this unfixed is also a choice that puts some users in potential danger.

Our internal contract is quite clear, and is quite clearly broken. I would prefer to address it here and now, rather than maintaining broken behaviour just for the sake of it. For the record, here are the relevant bits:
{code}
    /**
     * An iterator that returns the columns of this object in ""select"" order (that
     * is in global alphabetical order, where the ""normal"" iterator returns simple
     * columns first and the complex second).
     *
     * @return an iterator returning columns in alphabetical order.
     */
    public Iterator<ColumnDefinition> selectOrderIterator()
    {
        // In wildcard selection, we want to return all columns in alphabetical order,
        // irregarding of whether they are complex or not
        return Iterators.mergeSorted(ImmutableList.of(complexColumns(), simpleColumns()),
                                     (s, c) ->
                                     {
                                         assert !s.kind.isPrimaryKeyKind();
                                         return s.name.bytes.compareTo(c.name.bytes);
                                     });
    }
{code}

The contact has been defined this way since 2.0, when static columns where introduced, and was broken relatively recently when 3.0 came out. I would argue that the correct thing to do is to fix it in the next 3.0 and 3.11 minors, with a loud NEWS.txt entry and an email to dev@ and users@ warning users who are both on 2.1/2.2 and 3.0+ and up about the current breakage and the upcoming fix.","15/Aug/18 16:51;iamaleksey;[~slebresne] I assume you may have an opinion on this. If so, please share.","15/Aug/18 17:03;jjordan;I would think we should be able to do a protocol version bump or something and know ""when talking to X put them in this order, when talking to Y put them in this other order"", such that we do not break things with current 3.0 and 3.11 nodes on upgrade?","15/Aug/18 17:05;andrew.tolbert;{quote}
What 3.0 does incorrectly is ordering the static columns it returns. Instead of merging simple and complex static columns, we are getting all simple static columns first, followed by all complex static columns.
{quote}

Ah, that explains it!  Thanks for looking into this.  I was trying to understand why it was ordering the static columns the way it was, that makes sense.

{quote}
This is true. That said, there are huge fleets still on 2.1 and 2.2 in the wild, so leaving this unfixed is also a choice that puts some users in potential danger.
{quote}

That's a great point.  We need to balance the risk between users upgrading from 2.x to those upgrading from previous 3.x versions.  I may not have the best sense of which should win out, but I think I agree that if we can make it consistent and work the way it did pre-3.0, that is probably for the best long term.","15/Aug/18 17:24;benedict;[~jjordan]:  there is no well defined behaviour for a client version, only for a C* version, so unless we reject all connections from older client versions (which would be unacceptable) I don't think this buys us much.  

The only properly backward compatible solution I can imagine would be extremely costly, i.e. on startup, fetch all prepared statements from other nodes in the cluster, and persist the UUID and the version it was prepared against, and forever more treat this statement as that select order.  Even this would not be perfect, as there's the possibility of a statement falling out of cache.

Another option would be to roll the hash of all prepared statements, just once, so that submitting the query to an upgraded node would return NOT_PREPARED, but this would cause a lot of re-prepare traffic during upgrade and I'm not sure all clients would handle this gracefully (they may assume, for instance, that the UUID does not change when the statement is re-prepared).

I don't think there's a neat solution to this, though I'm happy to be proven wrong.","15/Aug/18 17:26;jjirsa;We're talking about this in the context of prepared statements, but even if you roll the hash to force a re-prepare, someone who's fetching by index will still be surprised if we re-order the index, and that's protocol and driver independent.","15/Aug/18 17:32;benedict;Very true, and they're basically a lost cause.  The only way to fix for them would be to cement the column order at the point of upgrade for all queries in perpetuity (i.e., if 2.1->fixed 3.0, use original; if broken 3.0->fixed 3.0, use new broken order), and even this wouldn't fix applications that were running through an upgrade from 2.1 -> broken 3.0.  It would also mean docs couldn't say what the column order was defined as!

All in all, I think the best option is to ""fix the bug"" and leave it at that.  It's dissatisfying, but it does put a clean end to the problem.",15/Aug/18 17:53;jjordan;I was just thinking for the internode part. Back to clients I can’t think of a good way either.,"15/Aug/18 18:31;iamaleksey;bq. I was just thinking for the internode part.

Luckily internode messaging is not affected by this bug.","15/Aug/18 19:13;iamaleksey;Branches with the fix for [3.0|https://github.com/iamaleksey/cassandra/commits/14638-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/14638-3.11], and [4.0|https://github.com/iamaleksey/cassandra/commits/14638-4.0]. CI, respectively: [3.0|https://circleci.com/workflow-run/6be0f2b2-292c-4c9e-81fe-a1b90a86421e], [3.11|https://circleci.com/workflow-run/a771039b-aa8f-4eea-a268-822c759a1c8e], and [4.0|https://circleci.com/workflow-run/ce2620d7-2317-43ef-8f96-90d4f53ee175].","16/Aug/18 07:28;slebresne;I agree both that this is unfortunate, and that I don't see any reasonable backward compatible option. And I'm also in favor or restoring the pre-3.0 order, partly because this is indeed the order we've used for the longest, but mainly because it doesn't make sense to have inconsistent ordering for static and normal columns.

I would also argue that relying that strongly on the order of columns of a {{SELECT *}} is a bad idea in the first place, and to the best of my knowledge, no official documentation has ever pretended that the order was guaranteed (don't get me wrong, this is a weak argument in that our documentation are historically bad and if we were to guarantee only what is documented, we wouldn't be guaranteeing much; still, it would clearly have been worth if we had ever documented the order as guaranteed), so hopefully this won't break too many users. In fact, on top of the big fat NEWS entry already mentioned, I'd be in favor of updating the documentation to explicitly mention that the order of columns is not specified and not guaranteed to be stable, and should thus not be relied on. ","16/Aug/18 10:19;benedict;The patch looks good to me, so I'd be happy to commit as-is.  

There is an alternative approach that might be simpler, or at least fewer edits; namely, to modify findFirstComplexId to auto-detect isStatic, and avoid the caller worrying about the distinction.  Since we already get the end ColumnDefinition to avoid a binary search in the case there are no complex cells, we can also extract its Kind for (almost) free.

There is an argument to be made either way, since we implicitly require that you never mix regular and static columns in one of these collections, but presently never actually impose it either semantically or via assertion.","16/Aug/18 12:37;iamaleksey;[~benedict] That's indeed better and less invasive. Force-pushed the new version, updated CI links in-place in the previous comment.

[~slebresne] Agreed on all points. Updated documentation in the v2 of the patch.",16/Aug/18 14:40;benedict;+1,"16/Aug/18 14:49;iamaleksey;Cheers, committed as [236c47e65ce95dcc7c8c75706715b5a2a88fd237|https://github.com/apache/cassandra/commit/236c47e65ce95dcc7c8c75706715b5a2a88fd237] to 3.0 and merged up to 3.11 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert 4.0 GC alg back to CMS,CASSANDRA-14636,13178265,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasobrown,jasobrown,jasobrown,10/Aug/18 12:45,12/Mar/19 14:15,13/Mar/19 22:35,10/Aug/18 17:45,4.0,,,,,,,,0,Java11,,,"CASSANDRA-9608 accidentally swapped the default GC algorithm from CMS to G1. Until further community consensus is achieved about swapping the default alg, we should switch back to CMS.

As reported by [~rustyrazorblade] on the [dev@ ML|https://lists.apache.org/thread.html/0b30f9c84457033583e9a3e0828adc603e01f1ca03ce0816098883cc@%3Cdev.cassandra.apache.org%3E] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-08-10 17:10:02.53,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 10 17:45:26 UTC 2018,,,,,,0|i3wxhr:,9223372036854775807,,,,,jrwest,jrwest,,,,,,,,,,,"10/Aug/18 15:55;jasobrown;Using the [previous, 3.11 settings|https://github.com/apache/cassandra/blob/cassandra-3.11/conf/jvm.options#L197] as a guide, reverting the java8 settings was simple. However, java11 was not as trivial. A naive reenabling of java8's GC settings yielded:

{noformat}
Unrecognized VM option 'UseParNewGC'                                                                                                                                                                               
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
{noformat}

{{UseParNewGC}} was deprecated in java 9 and [removed in java 10|https://bugs.openjdk.java.net/browse/JDK-8151084]. Once that flag is removed, cassandra can start and run properly. However, this is this log statement upon process start:

{noformat}
OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.                                                                          
{noformat}

For those who haven't been playing along with every gyration of the JDK, the [CMS collector is deprecated|https://bugs.openjdk.java.net/browse/JDK-8142518]. However, CMS [might not be removed|https://bugs.openjdk.java.net/browse/JDK-8163329], but might not be supported directly by the OpenJDK project.

Either way, it appears that simply removing {{UseParNewGC}} form the java 11 config gets us back to on-par with the java 8 config.

Patch available here:

||14636||
|[branch|https://github.com/jasobrown/cassandra/tree/14636]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14636]|
||

Failed dtests are known issues.","10/Aug/18 17:10;jrwest;+1. 

My understanding is {{UseParNewGC}} is implicit in Java 10+. Otherwise it might be worth adding a note about that specific change. ",10/Aug/18 17:45;jasobrown;Committed as sha {{ed806594e5169458d744a06c73ec224a1f37abce}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTables may not be properly removed from original strategy when repair status changed,CASSANDRA-14720,13184522,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,12/Sep/18 03:56,12/Mar/19 14:15,13/Mar/19 22:35,12/Sep/18 06:33,4.0,,,,Local/Compaction,,,,0,,,,"In {{CSM.handleRepairStatusChangedNotification()}}, CASSANDRA-14621 changed the original semantic of {{removing sstables in repaired first}} to {{adding sstables into unrepaired first}}...

In case of LCS, adding sstables may modify their levels, so they won't be removed from {{repaired}} which locates sstables by level. 

| [trunk|https://github.com/apache/cassandra/compare/trunk...jasonstack:CASSANDRA-14621-follow-up?expand=1] | [circle|https://circleci.com/gh/jasonstack/cassandra/742] |",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-12 06:33:56.67,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 12 07:52:29 UTC 2018,,,,,,0|i3xzrz:,9223372036854775807,,,,,krummas,krummas,,,,4.0,,,,,,,12/Sep/18 03:58;jasonstack;[~bdeggleston] do you mind reviewing?,"12/Sep/18 06:33;krummas;nice catch, thanks!

committed as {{f100024eb3becf53042823ce1008d3d5ec4e5f86}}",12/Sep/18 07:52;jasonstack;Thanks for the quick review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadCommandVerbHandler validateTransientStatus class cast exception ,CASSANDRA-14959,13208068,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,07/Jan/19 16:16,12/Mar/19 14:15,13/Mar/19 22:35,07/Jan/19 22:38,4.0,,,,Consistency/Coordination,,,,0,,,,"Causes a test failure, looks like it should just use instanceof.
{noformat}
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py"", line 917, in _bootstrap_inner
    self.run()
  File ""/Users/aweisberg/repos/cassandra-dtest/venv/src/ccm/ccmlib/cluster.py"", line 189, in run
    self.scan_and_report()
  File ""/Users/aweisberg/repos/cassandra-dtest/venv/src/ccm/ccmlib/cluster.py"", line 182, in scan_and_report
    on_error_call(errordata)
  File ""/Users/aweisberg/repos/cassandra-dtest/dtest_setup.py"", line 137, in _log_error_handler
    pytest.fail(""Error details: \n{message}"".format(message=message))
  File ""/Users/aweisberg/repos/cassandra-dtest/venv/lib/python3.7/site-packages/_pytest/outcomes.py"", line 97, in fail
    raise Failed(msg=msg, pytrace=pytrace)
Failed: Error details: 
Errors seen in logs for: node1
node1: ERROR [ReadStage-2] 2019-01-03 14:02:43,704 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,5,main]
java.lang.ClassCastException: org.apache.cassandra.db.PartitionRangeReadCommand cannot be cast to org.apache.cassandra.db.SinglePartitionReadCommand
	at org.apache.cassandra.db.ReadCommandVerbHandler.validateTransientStatus(ReadCommandVerbHandler.java:85)
	at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:53)
	at org.apache.cassandra.net.MessageDeliveryTask.process(MessageDeliveryTask.java:92)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:115)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

FAILED
upgrade_tests/cql_tests.py:2869 (TestCQLNodes3RF3_Upgrade_indev_3_11_x_To_indev_trunk.test_edge_2i_on_complex_pk)
{noformat}",,,,,,,,,,,,,,,CASSANDRA-14964,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-01-07 17:12:54.316,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 07 22:33:30 UTC 2019,,,,,,0|u00l3k:,9223372036854775807,,,,,ifesdjeen,ifesdjeen,,,,,,,,,,,"07/Jan/19 16:27;aweisberg;Proposed fix https://github.com/aweisberg/cassandra/commit/290f6dc2e79ff2d265fb2b232799bab0ba3a94cb
CircleCI: https://circleci.com/gh/aweisberg/cassandra/tree/14959-trunk","07/Jan/19 17:12;ifesdjeen;+1, looks good!

Since this is already caught by dtests, we should be good without adding any new tests, too.",07/Jan/19 22:33;aweisberg;Thanks! Committed as [f0494889176873b3f68ae14cc5f1d9dcbc189da9|https://github.com/apache/cassandra/commit/f0494889176873b3f68ae14cc5f1d9dcbc189da9].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collection Deletions for Dropped Columns in 2.1/3.0 mixed-mode can delete rows,CASSANDRA-14749,13184944,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,13/Sep/18 16:02,12/Mar/19 14:15,13/Mar/19 22:35,17/Sep/18 13:13,3.0.18,,,,Legacy/Core,,,,0,,,,"Similar to CASSANDRA-14568, if a 2.1 node sends a response to a 3.0 node containing a deletion for a dropped collection column, instead of deleting the collection, we will delete the row containing the collection.

 

This is an admittedly unlikely cluster state but, during such a state, a great deal of data loss could happen.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-13 16:55:09.429,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 17 10:57:22 UTC 2018,,,,,,0|i3y2cv:,9223372036854775807,,,,,,iamaleksey,slebresne,,,,,,,,,,"13/Sep/18 16:23;benedict;[3.0|https://github.com/belliottsmith/cassandra/tree/14749] [CI|https://circleci.com/workflow-run/c877e18e-ea8e-45ec-b090-4652445f487d]

Since this has already been reviewed in CASSANDRA-14568, if either of you could give a quick cursory +1 to the split, that would be great.  I felt it warranted its own CHANGES.txt entry and JIRA.","13/Sep/18 16:55;iamaleksey;Should probably pass {{isStatic}} to that {{CFMetaData.getDroppedColumnDefinition()}} call, or else it just defaults to {{false}}. May or may not have effect here, but since you have that flag in hand already, might as well use it. +1 otherwise.

On a related note, I'm not 100% certain that other calls of {{CFMetaData.getDroppedColumnDefinition()}} are oll korrect, it's something to look into later.","13/Sep/18 17:04;benedict;-Good job I did (split this out) - this time, Aleksey pointed out the {{getDroppedColumn}} invokes another method {{getDroppedColumn(name, isStatic)}} with a default parameter of {{false}} for {{isStatic}}-  (That's what I get for not refreshing the page)

I've pushed an update, but I think the defaulting variant of {{getDroppedColumn}} probably shouldn't exist.",13/Sep/18 18:10;iamaleksey;LGTM. I'll file a JIRA to follow-up on the defaulting variant audit and/or removal.,"13/Sep/18 18:13;benedict;-FWIW, I've been planning to file a ticket to audit the non-use of {{getDroppedColumn}} also, off the back of this, and perhaps we should combine the two efforts.  I'm sure most uses of {{getColumn}} should have a corresponding use of {{getDroppedColumn}} and I'm sure there have been other oversights than this.  We could keep two separate endeavours though.-

At least in trunk, it seems dropped columns have been considered in all places that we invoke {{getColumn}}, besides in test cases",14/Sep/18 10:29;benedict;Thanks.  Committed as [06c55f779ae68de98cce531e0b78be5716849003|https://github.com/apache/cassandra/commit/06c55f779ae68de98cce531e0b78be5716849003].,"17/Sep/18 10:20;snazy;Seems that this commit broke the build in cassandra-3.11:
{code:java}
build-test:
[javac] Compiling 540 source files to /home/automaton/cassandra-src/build/test/classes
[javac] /home/automaton/cassandra-src/test/unit/org/apache/cassandra/db/LegacyLayoutTest.java:279: error: Clustering is abstract; cannot be instantiated
[javac] builder.newRow(new Clustering(UTF8Serializer.instance.serialize(""a"")));
[javac] ^
[javac] /home/automaton/cassandra-src/test/unit/org/apache/cassandra/db/LegacyLayoutTest.java:280: error: no suitable method found for live(CFMetaData,ColumnDefinition,long,ByteBuffer,<null>)
[javac] builder.addCell(BufferCell.live(table, v, 0L, Int32Serializer.instance.serialize(1), null));
[javac] ^
[javac] method BufferCell.live(ColumnDefinition,long,ByteBuffer) is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] method BufferCell.live(ColumnDefinition,long,ByteBuffer,CellPath) is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] Note: Some input files use or override a deprecated API.
[javac] Note: Recompile with -Xlint:deprecation for details.
[javac] Note: Some input files use unchecked or unsafe operations.
[javac] Note: Recompile with -Xlint:unchecked for details.
[javac] 2 errors
{code}","17/Sep/18 10:57;benedict;Hmm.  Sorry about that.  I ran {{ant clean && ant}} but clearly didn't look closely at the result.  I'll ninja a fix in shortly, since it's only tests affected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate transient status on query,CASSANDRA-14704,13183662,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,07/Sep/18 15:45,12/Mar/19 14:15,13/Mar/19 22:35,12/Sep/18 15:05,,,,,,,,,0,,,,"Validate transient status on query:

|[patch|https://github.com/apache/cassandra/pull/261]|[utest|https://circleci.com/gh/ifesdjeen/cassandra/393]|[dtest-novnode|https://circleci.com/gh/ifesdjeen/cassandra/394]|[dtest-vnode|https://circleci.com/gh/ifesdjeen/cassandra/392]|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-12 09:12:05.782,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 12 15:04:54 UTC 2018,,,,,,0|i3xujj:,9223372036854775807,,,,,benedict,benedict,,,,,,,,,,,"12/Sep/18 09:12;benedict;I'm not sure it matters, but it's possibly better in {{validateTransientStatus}} to only fail the query if we received a non-transient request on a transient node.  This is what was meant to be implied by {{acceptsTransient}} - it doesn't *require* transient.  This would mean more work on the node and more network traffic, but that's maybe better than failing the query?  Not sure.

Otherwise LGTM, +1","12/Sep/18 15:04;ifesdjeen;Thank you for the review!

You're right, I've adjusted the logic and comment according to your suggestion.

Committed with [2046c30adec194fb07bc5dd1c31fc19a64e7895c|https://github.com/apache/cassandra/commit/2046c30adec194fb07bc5dd1c31fc19a64e7895c] to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing check for receiving digest response from transient node,CASSANDRA-14750,13184988,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,13/Sep/18 19:43,12/Mar/19 14:15,13/Mar/19 22:35,21/Sep/18 14:57,,,,,Legacy/Coordination,,,,0,pull-request-available,,,Range reads do not check for transient nodes returning a request. Read Command also currently allows a combination of transient node and digest query.,,"GitHub user ifesdjeen opened a pull request:

    https://github.com/apache/cassandra/pull/266

    Add a check for receiving digest response from transient node

    for CASSANDRA-14750

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ifesdjeen/cassandra CASSANDRA-14750

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/266.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #266
    
----
commit 735b8da5379c7e3556b05dc450741bf6cae09f28
Author: Alex Petrov <oleksandr.petrov@...>
Date:   2018-09-12T21:20:34Z

    Add a check for receiving digest response from transient node
    
    Patch by Alex Petrov; reviewed by Benedict Elliot Smith for CASSANDRA-14750

----
;14/Sep/18 09:38;githubbot;600","Github user ifesdjeen closed the pull request at:

    https://github.com/apache/cassandra/pull/266
;21/Sep/18 13:12;githubbot;600",,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-20 12:53:46.758,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 21 13:13:11 UTC 2018,,,,,,0|i3y2mf:,9223372036854775807,,,,,benedict,benedict,,,,,,,,,,,"20/Sep/18 12:53;benedict;Sorry, I filed this mentally under things I had reviewed.  +1","21/Sep/18 13:13;ifesdjeen;Thank you, committed to trunk with [59de353325768b6bb8f4dc18a1a2ace5071f8f84|https://github.com/apache/cassandra/commit/59de353325768b6bb8f4dc18a1a2ace5071f8f84]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transient node receives full data requests in dtests,CASSANDRA-14762,13185857,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,benedict,aweisberg,aweisberg,18/Sep/18 17:42,12/Mar/19 14:15,13/Mar/19 22:35,03/Oct/18 13:29,4.0,,,,Feature/Transient Replication,,,,0,,,,"I saw this running them on my laptop with rapid write protection disabled. Attached is a patch for disabling rapid write protection in the transient dtests.

{noformat}
.Exception in thread Thread-19:
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/Users/aweisberg/repos/cassandra-dtest/venv/src/ccm/ccmlib/cluster.py"", line 180, in run
    self.scan_and_report()
  File ""/Users/aweisberg/repos/cassandra-dtest/venv/src/ccm/ccmlib/cluster.py"", line 173, in scan_and_report
    on_error_call(errordata)
  File ""/Users/aweisberg/repos/cassandra-dtest/dtest_setup.py"", line 137, in _log_error_handler
    pytest.fail(""Error details: \n{message}"".format(message=message))
  File ""/Users/aweisberg/repos/cassandra-dtest/venv/lib/python3.6/site-packages/_pytest/outcomes.py"", line 96, in fail
    raise Failed(msg=msg, pytrace=pytrace)
Failed: Error details:
Errors seen in logs for: node3
node3: ERROR [ReadStage-1] 2018-09-18 12:28:48,344 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-1,5,main]
org.apache.cassandra.exceptions.InvalidRequestException: Attempted to serve transient data request from full node in org.apache.cassandra.db.ReadCommandVerbHandler@3c55e0ff
	at org.apache.cassandra.db.ReadCommandVerbHandler.validateTransientStatus(ReadCommandVerbHandler.java:104)
	at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:53)
	at org.apache.cassandra.net.MessageDeliveryTask.process(MessageDeliveryTask.java:92)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:110)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-20 15:48:04.67,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 03 13:28:53 UTC 2018,,,,,,0|i3y7zb:,9223372036854775807,,,,,aweisberg,aweisberg,,,,,,,,,,,"20/Sep/18 15:48;benedict;This error message has its parameters the wrong way around; it should read

""Attempted to serve full data request from transient node""

Which is obviously much worse",20/Sep/18 15:53;benedict;I'm not sure why rapid write protection should affect this?,20/Sep/18 16:29;aweisberg;It probably didn't I just didn't find out if it reproduced without it.,"20/Sep/18 18:02;benedict;I'm pretty sure the issue here is that, in SRP and AbstractReadRepair, we're not correctly setting the transient flags.  I've got a patch queued in CI that should correct it.",20/Sep/18 22:37;benedict;[4.0|https://github.com/belliottsmith/cassandra/tree/14762] [CI|https://circleci.com/workflow-run/c389a15a-c2f4-4ad7-8ea1-921266d18bf5],"25/Sep/18 22:31;aweisberg;[Is this just a clarification?|https://github.com/belliottsmith/cassandra/commit/0f8a736316feded871231e2ad571d53182d33ee2#diff-0920d7430e98bf3c6a3c4cb88056f8f5R293]

Maybe [~bdeggleston] should do a quick review as well since it is a small change. I just want to sanity check that we should be reading from transient replicas in both these cases.

It seems to me we use read repair to assemble the read after digest mismatch between full replicas and that is why we it might send messages to transient replicas? That is what it seems like to me. A minor out of scope improvement would be to use the existing response and not repeat the read?

It seems to me also that we would read from transients as part of short read protection (they are just another member of the group), and they aren't special so we should issue them the query.
","26/Sep/18 08:16;benedict;Thanks for the review.

bq. Is this just a clarification?

Yes; it is a no-effect change.

bq. It seems to me we use read repair to assemble the read after digest mismatch between full replicas and that is why we it might send messages to transient replicas? That is what it seems like to me.
Even though we don't send repair mutations after read-repair from transient replicas, on digest mismatch we still perform reads to other replicas - including those that we may not have contacted initially, which might include new transient nodes.

bq. A minor out of scope improvement would be to use the existing response and not repeat the read?
Agreed, see CASSANDRA-14733.  I considered doing that optimisation for this ticket, but given the above fact that we might issue new transient reads it seemed to unnecessarily complicate fixing this bug.

bq. It seems to me also that we would read from transients as part of short read protection (they are just another member of the group), and they aren't special so we should issue them the query.

Is this simply you reasoning out the rationale for issuing the requests, or have you spotted an issue with the patch that means we are not doing so?

It does look to me like we should be perhaps switching to {{acceptsTransient}} for the local query also, and validating transient status in {{LocalReadRunnable}} - but, for now, we don't do this.  Perhaps we could fix this also in this patch, or otherwise file a follow up.","26/Sep/18 14:59;aweisberg;bq. Is this simply you reasoning out the rationale for issuing the requests, or have you spotted an issue with the patch that means we are not doing so?
Sorry it's just socratic code review. I don't see any problems.

+1

Checking locally is nice, but I'm not sure it is as valuable just because the race is much smaller since it's not O(gossip) it's O(time to switch threads). 

If you want to do it here or in another ticket it's still good to have.","26/Sep/18 16:45;benedict;bq. I'm not sure it is as valuable just because the race is much smaller since it's not O(gossip) it's O(time to switch threads).

I was thinking of programmer error more than the race condition, but I agree it's much less impactful.  I might rustle it up anyway, while we're here.","03/Oct/18 13:28;benedict;I've committed as-is, to [467068d1e9d84e6cca1f9dd5a4eff5f80d027c2e|https://github.com/apache/cassandra/commit/467068d1e9d84e6cca1f9dd5a4eff5f80d027c2e].  

dtests are now very flaky on CircleCI, but all of the failures in the latest run have shown up in trunk runs for me on CircleCI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resolve local address binding in 4.0,CASSANDRA-14389,13152996,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,djoshi3,jasobrown,jasobrown,17/Apr/18 12:52,12/Mar/19 14:15,13/Mar/19 22:35,22/Apr/18 23:38,4.0,,,,Legacy/Streaming and Messaging,,,,0,,,,CASSANDRA-8457/CASSANDRA-12229 introduced a regression against CASSANDRA-12673. This was discovered with CASSANDRA-14362 and moved here for resolution independent of that ticket.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-04-19 15:40:55.752,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 22 23:38:38 UTC 2018,,,,,,0|i3sn9j:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"19/Apr/18 15:40;djoshi3;I found the issue. When you leave the local side of the socket unbound, the kernel will prefer the IP address that matches the remote IP. Say node1 with IP {{127.0.0.1}} wants to open a connection to node2 with IP {{127.0.0.2}}, the socket would look like {{<127.0.0.2:61002, 127.0.0.2:7000>}} on node1. This seems to confuse the streaming code. Here's how -

Say we have three nodes node1, node2 & node3 with IPs {{127.0.0.1, 127.0.0.2, 127.0.0.3}}. node1 has data and node3 is bootstrapping. It requests a stream from node1. So node3 is the `peer` in this case and node1's code execution is described below -

* node1 receives the request ({{StreamingInboundHandler#deriveSession}}) and {{StreamResultFuture#initReceivingSide}} creates a new {{StreamResultFuture}} and calls {{attachConnection()}}. At this point it has two sets of IP & Ports from the peer. They are identified by the variable `{{from}}` & expression `{{channel.remoteAddress()}}` a.k.a `{{connecting}}` ).
* {{StreamResultFuture#attachConnection calls StreamCoordinator#getOrCreateSessionById}} passing the from IP & {{InetAddressAndPort.getByAddressOverrideDefaults(connecting, from.port)}} (!!!)
* The key observation here is `from` is the IP that the peer sent in the `{{StreamMessageHeader}}` while `connecting` is the remote IP of the peer.
* {{StreamCoordinator#getOrCreateSessionById}} subsequently calls {{StreamCoordinator#getOrCreateHostData(peer)}}. So we're indexing the {{peerSessions}} by the `{{peer}}` IP address. We also end up creating a `{{StreamSession}}` in the process.
* During `{{StreamSession}}` creation, we end up passing the `{{peer}}` and `{{connecting}}` IPs. We use the `connecting` IP to establish the outbound connection to the peer. ({{NettyStreamingMessageSender}} is now connected to `{{connecting}}` IP on port {{7000}}).

In our case, since we leave the local side of the socket unbound, although the `{{peer}}` correctly sets its IP to {{127.0.0.3}} in the {{StreamMessageHeader}}, the {{localAddress}} that the kernel chooses for it is {{127.0.0.1}}. On the inbound node1 seems to think that the `peer` is {{127.0.0.3}} however the connecting IP address should be {{127.0.0.1}}. Therefore, it prefers that IP when trying to establish an outbound session. In fact it establishes a connection to itself leading to the `{{Unknown peer requested: 127.0.0.1:7000}}` exception. Note that along the way it actually drops the ephemeral port and instead uses the port returned by {{MessagingService#portFor}}.

Streaming code seems to rely on the perceived remote IP address of the host rather than the one that is set in the message header. I am not sure if preferring the IP address set in the header is the correct approach.","20/Apr/18 04:52;djoshi3;Here's a fix along with restoring the behavior from CASSANDRA-12673

||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/CASSANDRA-14389-trunk-fix-streaming]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/CASSANDRA-14389-trunk-fix-streaming]|
||","20/Apr/18 20:02;jasobrown;[~djoshi3], This looks pretty good. However, I wonder if we can dump all the places where we naively plumb the 'connecting' address though. I took a pass at it here:

||14389||
|[branch|https://github.com/jasobrown/cassandra/tree/14389]|
|[utests &amp; dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14389]|
||

The only change of interest is in {{MessagingService#getPreferredRemoteAddr()}}, I check into {{SystemKeyspace.getPreferredIP(to)}} if there was no {{OutboundMessagingPool}} set up in {[MessagingService#channelManagers}}. wdyt?","20/Apr/18 21:43;djoshi3;Hi [~jasobrown], thank you for reviewing. I have incorporated your changes. I also added a test for {{StreamSession}}.",21/Apr/18 00:55;djoshi3;I have incorporated the changes from your commit as well as added tests for \{{MessagingService#getPreferredRemoteAddr}}. Also squashed all commit into one.,"21/Apr/18 16:13;jasobrown;I didn't understand the {{NettyStreamingMessageSenderFactory}} and subclassing of {{NettyStreamingMessageSender}} in the {{StreamSessionTest}}. You weren't really taking any advantage of the subclass as all {{NSMSStub}} did was override the parent's constructor, only to call the parent's constructor. Thus I've removed {{NettyStreamingMessageSenderFactory}} (ignore the comment on the commit) and cleaned up the test. I've pushed up a commit to my branch, on top of your squash, and ran the testa again. If you are good with that, I'm +1 on the rest of the patch.",21/Apr/18 21:56;djoshi3;Yes that looks good. I was fiddling with different ways of writing the test and the constructor that I introduced in {{StreamSession}} made the factory redundant. Thank you for taking care of it.,22/Apr/18 23:38;jasobrown;committed as sha {{63945228fc0fabea2cfcf1f1b4d0a29ed3964107}}. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix argument passing for SSLContext in trunk,CASSANDRA-14314,13145277,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,djoshi3,djoshi3,djoshi3,15/Mar/18 05:09,12/Mar/19 14:15,13/Mar/19 22:35,22/Mar/18 13:42,4.0,,,,,,,,0,security,,,Argument passing has a minor bug while creating the SSLContext. Audit and make sure that the client & server SSL contexts are created at appropriate locations.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-15 18:49:42.174,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 22 13:42:11 UTC 2018,,,,,,0|i3rbtr:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"15/Mar/18 05:18;djoshi3;[~jasobrown] - please review.

||sslfactory||
|[branch|https://github.com/dineshjoshi/cassandra/tree/fix-args-to-sslfactory]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/fix-args-to-sslfactory]|
||","15/Mar/18 18:49;jasobrown;[~djoshi3] and I spoke offline about this, and we uncovered some futher incorrectness with {{SSLFactory.getSslContext()}}. We'll have an updated patch soon.","17/Mar/18 07:14;djoshi3;Hi [~jasobrown], I have updated the branch with a bunch of changes. Here's a short rundown of the changes -
 # Removed the {{serverSslContext}} and {{clientSslContext}} {{AtomicReference}}
 # Introduced a new field {{SocketType}} - so now you can create a combination of (PEER, CLIENT) \{{ConnectionType}} and (SERVER, CLIENT) {{SocketType}}
 # Netty SSL contexts are cached in a {{ConcurrentHashMap}}. I haven't currently implemented any strategy to prune or reset this map. My expectation is in the steady state this map should not grow.
 # Hot reloading is updated to use this map.","17/Mar/18 23:38;djoshi3;||sslfactory||
|[branch|https://github.com/dineshjoshi/cassandra/tree/fix-args-to-sslfactory]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/fix-args-to-sslfactory]|
||",18/Mar/18 00:22;djoshi3;Squashed the commit - b9836dc07560ffa03eb6fb3902a2d96c3ff5715e,"21/Mar/18 18:43;jasobrown;I've made a few changes to [~djoshi3]'s branch, which was a great step in the right direction, and pushed up to my repo:
||sslfactory||
|[branch|https://github.com/jasobrown/cassandra/tree/fix-args-to-sslfactory]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/fix-args-to-sslfactory]|
 - I renamed {{ConnectionType.CLIENT}} to {{ConnectionType.NATIVE_PROTOCOL}} (and {{ConnectionType.PEER}} to {{ConnectionType.INTERNODE_MESSAGING}}) as it was just confusing to me what 'client' vs 'server' really meant. The change ends up reading very well, especially at call sites.
 - as a petty readability change, I replaced {{SSLFactory.HotReloadableFile.Type}} with {{ConnectionType}}.
 - {{SSLFactory.CacheKey}} - I moved the {{equals}}/{{hashCode}} logic for {{EncryptionOptions}} fields into {{equals}}/{{hashCode}} functions on {{EncryptionOptions}} itself.
 - {{Server}} - {{ConnectionType}} should be {{CLIENT}} (now {{NATIVE_PROTOCOL}}) as this is where we start the server-side of the client-facing native protocol.
 - {{NettyFactory.OutboundInitializer.initChannel}} - {{ConnectionType}} should be {{PEER}} (now {{INTERNODE_MESSAGING}}) as this is where we start the client-side of a internode messaging connection.
 - {{SSLFactory.getSslContext}} - when {{cachedSslContexts.putIfAbsent}} is called, if the value is not null, we should return that value rather than using the instance we just created. That way we reuse the 'winning' context.
 - {{SSLFactory.createNettySslContext}} - I'm not sure we need to check if {{options.enabled}}. Actually, I'm not sure if there's a strong argument for why we should ever not load the keystore. In 3.0, we always loaded the keystore. wdyt?
 - Last, while I can't (or don't want to) rename the yaml properties ({{client_encryption_options}}/{{server_encryption_options}}), I can clean up the code that refers to them. Hence, in DD I've changed {{getServerEncryptionOptions}} to {{getInternodeMessagingEncryptonOptions}} and so on. This make the call sites to get that data more obvious, as well. This change is not a requirement for this patch, but helps clarify the code.","21/Mar/18 20:07;djoshi3;[~jasobrown] a few comments -

* {{SSLFactory}} line 327 info message should read - ""SSL certificates have been updated. Reseting the ssl contexts for new connections."". Please drop the word ""peer"".
* {{HotReloadableFile}} we can get rid of {{isServer}} and {{isClient}} methods and associated code as we're not making any distinctions any more.

Rest looks good.","22/Mar/18 13:42;jasobrown;committed, with minor nits addressed, as sha {{11496039fb18bb45407246602e31740c56d28157}}. Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader should use discovered broadcast address to connect intra-cluster,CASSANDRA-14522,13166176,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,Yarnspinner,jeromatron,jeromatron,14/Jun/18 18:47,12/Mar/19 14:14,13/Mar/19 22:35,06/Aug/18 16:12,3.0.18,3.11.4,,,Legacy/Tools,,,,0,lhf,,,"Currently, in the LoaderOptions for the BulkLoader, the user can give a list of initial host addresses.  That's to do the initial connection to the cluster but also to stream the sstables.  If you have two physical interfaces, one for rpc, the other for internode traffic, then bulk loader won't currently work.  It will throw an error such as:

{quote}
> sstableloader -v -u cassadmin -pw xxx -d 10.133.210.101,10.133.210.102,10.133.210.103,10.133.210.104 /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl
Established connection to initial hosts
Opening sstables and calculating sections to stream
Streaming relevant part of /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl/mc-1-big-Data.db /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl/mc-2-big-Data.db  to [/10.133.210.101, /10.133.210.103, /10.133.210.102, /10.133.210.104]
progress: total: 100% 0  MB/s(avg: 0 MB/s)ERROR 10:16:05,311 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Streaming error occurred
java.net.ConnectException: Connection refused
        at sun.nio.ch.Net.connect0(Native Method) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:454) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:446) ~[na:1.8.0_101]
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[na:1.8.0_101]
        at java.nio.channels.SocketChannel.open(SocketChannel.java:189) ~[na:1.8.0_101]
        at org.apache.cassandra.tools.BulkLoadConnectionFactory.createConnection(BulkLoadConnectionFactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.createConnection(StreamSession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.ConnectionHandler.initiate(ConnectionHandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-all-4.0.54.Final.jar:4.0.54.Final]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
ERROR 10:16:05,312 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Streaming error occurred
java.net.ConnectException: Connection refused
        at sun.nio.ch.Net.connect0(Native Method) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:454) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:446) ~[na:1.8.0_101]
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[na:1.8.0_101]
        at java.nio.channels.SocketChannel.open(SocketChannel.java:189) ~[na:1.8.0_101]
        at org.apache.cassandra.tools.BulkLoadConnectionFactory.createConnection(BulkLoadConnectionFactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.createConnection(StreamSession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.ConnectionHandler.initiate(ConnectionHandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-all-4.0.54.Final.jar:4.0.54.Final]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
ERROR 10:16:05,312 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Streaming error occurred
java.net.ConnectException: Connection refused
        at sun.nio.ch.Net.connect0(Native Method) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:454) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:446) ~[na:1.8.0_101]
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[na:1.8.0_101]
        at java.nio.channels.SocketChannel.open(SocketChannel.java:189) ~[na:1.8.0_101]
        at org.apache.cassandra.tools.BulkLoadConnectionFactory.createConnection(BulkLoadConnectionFactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.createConnection(StreamSession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.ConnectionHandler.initiate(ConnectionHandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-all-4.0.54.Final.jar:4.0.54.Final]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
progress: total: 100% 0  MB/s(avg: 0 MB/s)WARN  10:16:05,320 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Stream failed
Streaming to the following hosts failed:
[/10.133.210.101, /10.133.210.103, /10.133.210.102]
java.util.concurrent.ExecutionException: org.apache.cassandra.streaming.StreamException: Stream failed
        at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
        at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
        at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:122)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.datastax.bdp.tools.ShellToolWrapper.main(ShellToolWrapper.java:34)
Caused by: org.apache.cassandra.streaming.StreamException: Stream failed
        at org.apache.cassandra.streaming.management.StreamEventJMXNotifier.onFailure(StreamEventJMXNotifier.java:85)
        at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310)
        at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457)
        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)
        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145)
        at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202)
        at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:215)
        at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:191)
        at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:449)
        at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:549)
        at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:259)
        at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:212)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:745)
WARN  10:16:05,322 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Stream failed
WARN  10:16:05,322 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Stream failed
{quote}",,,,,,,,,,,,,,,,,,01/Aug/18 19:46;jblangston@datastax.com;CASSANDRA-14522.patch;https://issues.apache.org/jira/secure/attachment/12933965/CASSANDRA-14522.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-07-02 14:07:20.478,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 06 16:12:20 UTC 2018,,,,,,0|i3uvhz:,9223372036854775807,3.0.15,,,,,,,,,,,,,,,,02/Jul/18 14:07;Yarnspinner;Hello I would like to try working on this issue,"02/Jul/18 15:19;jjirsa;Assigned to you [~Yarnspinner] , good luck! Feel free to ask questions here or the dev@ list if needed. ","03/Jul/18 06:35;tommy_s;This looks similar to CASSANDRA-13639, at least configuration should be done the same way.","03/Jul/18 16:36;aweisberg;I think the solution presented in CASSANDRA-13639 is probably not what we are looking for. I am 90% confident this is the same complaint though. This is just the outgoing network interface that is chosen incorrectly I think.

Seems like this is a bit of a headache to test as well.","08/Jul/18 13:18;Yarnspinner;Sorry for any silly questions. This is my first time working on a big open source project and I have an idea or two about how to fix it but I'm not sure if it's completely off-track. Would really appreciate osme help.

I poked around in the code and it seems that the sstableloader starts out by launching org.apache.cassandra.tools.BulkLoader which then creates an ExternalClient which is a NativeSSTableLoaderClient extended with 2 fields. The only place i see the storagePort used in there is 

{code:java}
try (Cluster cluster = builder.build(); Session session = cluster.connect()){
for (TokenRange tokenRange : tokenRanges)
            {
                Set<Host> endpoints = metadata.getReplicas(Metadata.quote(keyspace), tokenRange);
                Range<Token> range = new Range<>(tokenFactory.fromString(tokenRange.getStart().getValue().toString()),
                                                 tokenFactory.fromString(tokenRange.getEnd().getValue().toString()));
                for (Host endpoint : endpoints)
                {
                    int portToUse;
                    if (allowServerPortDiscovery)
                    {
                        portToUse = endpoint.getBroadcastAddressOptPort().portOrElse(storagePort);
                    }
                    else
                    {
                        portToUse = storagePort;
                    }
                    addRangeForEndpoint(range, InetAddressAndPort.getByNameOverrideDefaults(endpoint.getAddress().getHostAddress(), portToUse));
                }
            }
}
{code}

So I think there might be 2 ways to fix it?

#
Instead of using endpoint.getAddress().getHostAddress(), use endpoint.getBroadcastAddressOptPort().address?

#
Use the session to execute a CQLSH query on the system.peers table and then parse the broadcast addresses from there?

Also, is there a way to test this on a single node? Or would I need to go get 2 AWS nodes and configure broadcast addresses that differ from the RPC address before testing it on them?

Thank you!","09/Jul/18 19:37;aweisberg;There are (almost) no silly questions.

You can run a cluster of Cassandra nodes on a single machine using https://github.com/riptano/ccm 

We write automated tests that use ccm called dtests https://github.com/apache/cassandra-dtest which I don't think you will have to work with for this.

You will need some familiarity with Python and virtual environments and setting up requirements files. We use Python 3 now for the tests but cqlsh is Python 2. Let me know if you need help.

For testing this you don't need multiple Cassandra processes, but you would need to setup up a second loopback interface that isn't reachable from your usual one so if it picks the incorrect outbound interface it fails to connect. You can probably do this with a firewall rule.

So that code you are looking at isn't the problem code I think. It's picking the correct interface to connect to based on the cluster metadata from the driver, but it's probably picking the wrong interface to connect on in BulkLoadConnectionFactory. On trunk this code has changed so I am not sure if it's still an issue https://github.com/apache/cassandra/commit/fc92db2b9b56c143516026ba29cecdec37e286bb#diff-fc44570de4b634df61bd83b639db98d4L51 and where things go wrong is buried a little deeper. https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/streaming/StreamSession.java#L197

[~jasobrown] ^ I think this may be an issue with Netty  intracluster streaming and security? Is it going to pick the wrong address just like the bulk loader? I am assuming not because the server loads the YAML and it all works out fine even in an AWS like setup.

In 3.0 where this is reported it is https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/tools/BulkLoadConnectionFactory.java#L51 and it's calling FBUtilities.getLocalAddress() to pick the outbound interface. 

Looking at the ticket I see connection refused so it's reaching the other node, but the other node isn't bound. Just be aware if you test this you will get a different result if there is no route.

TL;DR in all cases I think what we want is FBUtilities.getLocalAddress() to be used to provide a default interface, but there is a command line option to specify the outbound interface for streaming. In 3.0 and other versions forwards (3.11) and backwards (2.2, 2.1) this propagates into BulkLoadConnectionFactory. In trunk/4.0 it will have to propagate all the way into where StreamSession is calling FBUtilities.getLocalAddress(). One option might be to manually set the value in FBUtilities. You will need to create and test a patch for each version.","09/Jul/18 19:45;aweisberg;And yes AWS with a small cheap instance may be the fastest easiest way to test this if you don't already know the voodoo for firewall rules, routing, and network interfaces to test this on your local machine.",11/Jul/18 00:07;Yarnspinner;[~aweisberg] Thank you very much for the explanation! I will try out the testing setup and see how it goes.,"01/Aug/18 06:43;Yarnspinner;Hello,

I reproduced the error on my computer by setting different ip addresses for rpc_address, listen_address and local_address on my local cassandra instance. I then fixed it for 3.0 by querying system.peers and system.local to map the rpc_address to the listen_address in NativeSSTableLoaderClient.init. Is this a good way to go about it? 

Diff:
https://github.com/apache/cassandra/compare/trunk...yarnspinnered:14522-3.0

The tests pass except a few which failed due to timeout. However, these tests are unrelated and also fail on a clean copy cloned from the repo. Tests like 'testUpdateColumnNotInViewWithFlush,  testClusteringKeyMultiColumnRestrictions,  testClusteringOrder'.","01/Aug/18 19:47;jblangston@datastax.com;There is a simpler fix. The Host object being iterated over in that loop has methods to get the listen address directly. You just need to change endpoint.getAddress to endpoint.getBroadcastAddress.  I have also attached a patch.  The patch is against Cassandra 3.0 and should merge forward cleanly.

Note: there is also a Host.getListenAddress method which returns the local listen address, but we want to use the broadcast address in case the sstableloader is run in a different DC that cannot communicate with a remote node over the local listen address.  

I also noticed that you checked in your changes to cassandra.yaml that you were using to test this. That should be reverted.",01/Aug/18 21:25;djoshi3;Has someone tried reproducing this on trunk? I suspect this is fixed on trunk. See: CASSANDRA-14389,"02/Aug/18 14:45;jblangston@datastax.com;Yes, it does appear to be fixed in trunk.","02/Aug/18 16:06;aweisberg;OK, apparently I am deeply confused and this and CASSANDRA-13639 is not the same as this and you correctly fixed this issue. [~Yarnspinner] you might want to try CASSANDRA-13639 next it is also relatively straightforward and my [earlier comment|https://issues.apache.org/jira/browse/CASSANDRA-14522?focusedCommentId=16537451&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16537451] applies to it.

It looks like [~jblangston@datastax.com]'s patch should apply cleanly from 2.2-3.11 where this bug exists so I'm going to go with that.","02/Aug/18 16:37;aweisberg;In 2.2 the Java driver doesn't provide the information from {{system.peers}} I think. It only provides {{getAddress()}}. So it will need to go and query {{system.peers}}.

[~Yarnspinner] in your patch you read {{local}} and {{peers}} but with the driver's load balancing it's not guaranteed that {{local}} and {{peers}} will be read from the same node so you can still miss one. I am not sure if the {{listenAddress}} value is better? It's not broadcast address so it's not 100% correct.

It's kind of a mess because the comments in that version of the driver say this:
{code:java}
    // The listen_address (really, the broadcast one) as know by Cassandra. We use that internally because
    // that's the 'peer' in the 'System.peers' table and avoids querying the full peers table in
    // ControlConnection.refreshNodeInfo. We don't want to expose however because we don't always have the info
    // (partly because the 'System.local' doesn't have it for some weird reason for instance).
    volatile InetAddress listenAddress;
{code}

Not quite sure how to fix that in 2.2. Maybe you can research how to fix this in 2.2.","06/Aug/18 15:17;aweisberg;I'm going to commit and close this for 3.0 and 3.11 and create a separate ticket for 2.2 since it is pretty involved to figure out how to do it right there.

[3.0 CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14522-3.0]
[3.11 CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14522-3.11]

dtest failures appear unrelated.",06/Aug/18 16:12;aweisberg;Committed as [3d48cbc74cb50e02e85ed51ccf6a3a46690c3a99|https://github.com/apache/cassandra/commit/3d48cbc74cb50e02e85ed51ccf6a3a46690c3a99]. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OffsetAwareConfigurationLoader doesn't set ssl storage port causing bind errors in CircleCI,CASSANDRA-14546,13168909,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,djoshi3,djoshi3,aweisberg,28/Jun/18 16:29,12/Mar/19 14:12,13/Mar/19 22:35,28/Jun/18 16:59,4.0,,,,Legacy/Testing,,,,0,,,,"This is causing test failures in CircleCI

https://circleci.com/gh/dineshjoshi/cassandra/336#tests/containers/4

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-28 16:29:40.4,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 28 17:16:08 UTC 2018,,,,,,0|i3vcdb:,9223372036854775807,,,,,aweisberg,aweisberg,,,,,,,,,,,"28/Jun/18 16:29;aweisberg;[https://github.com/apache/cassandra/compare/trunk...dineshjoshi:messagingservice-test-failure-fix?expand=1]

[https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/messagingservice-test-failure-fix]",28/Jun/18 16:58;aweisberg;Passing unit tests https://circleci.com/gh/aweisberg/cassandra/1278,28/Jun/18 16:58;aweisberg;Commited as [4cb83cb81abe6990820f76c0addbd172d9f248a6|https://github.com/apache/cassandra/commit/4cb83cb81abe6990820f76c0addbd172d9f248a6]. Thanks!,28/Jun/18 17:16;djoshi3;Thank you [~aweisberg]!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't try to cancel 2i compactions when starting anticompaction,CASSANDRA-15024,13216038,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,15/Feb/19 17:13,12/Mar/19 14:12,13/Mar/19 22:35,18/Feb/19 08:09,4.0,,,,Consistency/Repair,Feature/2i Index,,,0,,,,"When we start an anticompaction we cancel ongoing compactions, {{runWithCompactionsDisabled}} always cancels compactions for secondary index cfs:es, this causes problem since CASSANDRA-14935 since we check for range intersection which will fail since 2i sstables are LocalPartitioner.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-02-15 18:25:30.231,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 18 08:09:13 UTC 2019,,,,,,0|yi117k:,9223372036854775807,,,,,bdeggleston,bdeggleston,,,,,,,,,,,"15/Feb/19 17:18;krummas;Attaching patch to add option to simply avoid concatenating the index cfs when calling {{runWithCompactionsDisabled}}.

patch: https://github.com/krummas/cassandra/commits/marcuse/2ianticompaction
tests: https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F2ianticompaction",15/Feb/19 18:25;bdeggleston;+1,18/Feb/19 08:09;krummas;committed as {{0706d32b0bd478160deb0143deb9811d49050b10}} - thanks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert parameterization of AuthCacheMBean interface,CASSANDRA-14687,13182683,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,beobal,beobal,beobal,03/Sep/18 18:35,12/Mar/19 14:11,13/Mar/19 22:35,04/Sep/18 11:35,4.0,,,,Feature/Authorization,,,,0,,,,"In CASSANDRA-14662, a type parameter {{<T>}} and {{invalidate<T t>}} method were added to {{AuthCacheMBean}} with the intention that this would automatically expose via JMX the {{invalidate}} method from {{AuthCache}} itself. Actually, this is not the case, as type erasure obscures the actual type of the parameter and so JMX clients like jconsole and jmc disable access to the method. 

Only {{PasswordAuthenticator.CredentialsCache}} provided method like this previously, via {{CredentialsCacheMBean extends AuthCacheMBean}}, so the most straightforward fix is to just revert the change to {{AuthCacheMBean}}. Future/alternative cache implementations can continue to specify their own MBean interfaces to add specialised methods as before.

I should've caught this before committing the CASSANDRA-14662 as it broke a couple of dtests in AuthTest, mea culpa.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-04 09:12:00.366,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 04 11:35:46 UTC 2018,,,,,,0|i3xonz:,9223372036854775807,,,,,,,,,,,,,,,,,"03/Sep/18 18:40;beobal;||branch||utests||dtests||
|[branch|https://github.com/beobal/cassandra/tree/14687-trunk]|[utests|https://circleci.com/gh/beobal/cassandra/398]|[vnodes|https://circleci.com/gh/beobal/cassandra/399] / [no vnodes|https://circleci.com/gh/beobal/cassandra/397]|
","04/Sep/18 09:12;KurtG;+1. My bad, should have checked that.","04/Sep/18 11:35;beobal;Thanks [~KurtG], committed to trunk in {{d26f142b34681d047fe010c8ec9097add0b44d2a}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DecayingEstimatedHistogramReservoir.EstimatedHistogramReservoirSnapshot returns wrong value for size() and incorrectly calculates count,CASSANDRA-14696,13183249,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,05/Sep/18 22:00,12/Mar/19 14:11,13/Mar/19 22:35,07/Sep/18 19:14,4.0,,,,Legacy/Core,,,,0,4.0-pre-rc-bugs,,,"It appears to return the wrong value for no reason. There isn't a single instance in the code base where we use size. There is an internal count that is calculated, but it is calculated over an empty array. 

Fix both of these bugs and then use the size to only populate read and write latency values used for speculation when there are actual samples being used.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-06 15:06:58.818,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 07 19:14:13 UTC 2018,,,,,,0|i3xs0f:,9223372036854775807,,,,,cnlwsu,cnlwsu,,,,,,,,,,,05/Sep/18 22:06;aweisberg;This is used to fix a more general issue with read and write latency tracking where when we have no data we end up using 0 as the value for speculation timeouts. This is different from using the default (1/2 the timeout) or the value from back when we had data.,06/Sep/18 14:13;aweisberg;[CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/cassandra-14696-trunk],"06/Sep/18 15:06;cnlwsu;Is the read_repair_test.TestSpeculativeReadRepair dtest failure related?

 

[In PR - I thought these were copied into a comment from bot?]

Since the size > 0 is really a precondition for calculateThreshold to run successfully, it seems like the check should be there instead in the policies that require it vs an implicit undocumented precondition to be checked before in only the existing calls (incase a new entry point ever created).","06/Sep/18 16:37;aweisberg;They are supposed to be? I think it may be broken. It's supposed to show up under work log.

It's unrelated, we know about that failure and I think Benedict is working on the fix.",06/Sep/18 16:49;aweisberg;Updated to fix the bug you found where it was ignoring fixed policies and the like that don't rely on the latency information.,06/Sep/18 17:40;aweisberg;At Benedict's suggestion I made the inner class static and reference the enclosing class fields via reference. Some of the fields were being shadowed so it was a bit tricky.,07/Sep/18 18:23;cnlwsu;I like the static inner class change a lot. +1 on pr now as well.,07/Sep/18 18:40;benedict;+1,07/Sep/18 19:14;aweisberg;Committed as [8d443805f06e7abb25f768f6c800b7ae71bd4a41|https://github.com/apache/cassandra/commit/8d443805f06e7abb25f768f6c800b7ae71bd4a41]. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid NPE in RepairRunnable.recordFailure,CASSANDRA-15025,13216041,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,krummas,krummas,krummas,15/Feb/19 17:27,12/Mar/19 14:11,13/Mar/19 22:35,18/Feb/19 08:09,4.0,,,,Consistency/Repair,,,,0,,,,"failureMessage parameter in {{RepairRunnable.recordFailure}} can be null, avoid this happening and make sure we log the actual exception",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-02-15 18:32:39.738,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 18 08:09:48 UTC 2019,,,,,,0|yi1188:,9223372036854775807,,,,,bdeggleston,bdeggleston,,,,,,,,,,,"15/Feb/19 17:30;krummas;patch: https://github.com/krummas/cassandra/commits/marcuse/avoid_repair_npe
tests: https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2Favoid_repair_npe

I'll submit a dtest for this early next week",15/Feb/19 18:32;bdeggleston;+1,"18/Feb/19 08:09;krummas;committed as {{98d81e409a3512fccaeab3ba89f7cf5bfa8f39ae}}, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Audit log does not include statements on some system keyspaces,CASSANDRA-14498,13164206,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vinaykumarcse,eperott,eperott,05/Jun/18 15:36,12/Mar/19 14:11,13/Mar/19 22:35,19/Nov/18 11:38,4.0,,,,Feature/Authorization,,,,0,audit,lhf,security,"Audit logs does not include statements on the ""system"" and ""system_schema"" keyspace.

It may be a common use case to whitelist queries on these keyspaces, but Cassandra should not make assumptions. Users who don't want these statements in their audit log are still able to whitelist them with configuration.",,,,,,,,,,,,,,,,,,01/Nov/18 20:26;vinaykumarcse;14498-trunk.txt;https://issues.apache.org/jira/secure/attachment/12946611/14498-trunk.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-06-06 18:27:20.18,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 19 11:38:10 UTC 2018,,,,,,0|i3ujcv:,9223372036854775807,4.0,,,,,eperott,krummas,,,4.0,,,,,,,"06/Jun/18 18:27;vinaykumarcse;Just curious, are there any use cases where you would to audit system keyspaces? fwiw auditing these generate lot of noise as C* calls system keyspaces in many places throughout its lifetime","07/Jun/18 06:30;eperott;bq. are there any use cases where you would to audit system keyspaces?

One use case would be to get audit logs on all operations from selected users.

bq. auditing these generate lot of noise as C* calls system keyspaces in many places

Internal calls in C* will not come through the audit logger. Right? I've observed that client drivers will emit some queries on their own. This typically happens when a user login or when there are schema changes. But that only represents a fraction of all operations coming from a client.

The problem I see with a hard coded filter is that it will not only filter out queries from the driver, but also any query issued by the client application on those keyspaces.

The decision should be with the administrator of the cluster and it will still be possible to whitelist these queries with configuration. We could add some documentation on this so that users will not get surprised when they see queries in the log that they didn't expect.","27/Jun/18 00:12;vinaykumarcse;{quote}The problem I see with a hard coded filter is that it will not only filter out queries from the driver, but also any query issued by the client application on those keyspaces.

The decision should be with the administrator of the cluster and it will still be possible to whitelist these queries with configuration. We could add some documentation on this so that users will not get surprised when they see queries in the log that they didn't expect.
{quote}
I buy this argument, will start working on it, we can ship with default excluding system keyspaces and let administrator tweak it as needed.",03/Jul/18 12:27;eperott;Thanks! I'm happy to review.,"17/Oct/18 06:26;eperott;[~vinaykumarcse], any progress on this ticket?

Would be nice to get this into 4.0.","18/Oct/18 07:56;vinaykumarcse;[~eperott] Attached the patch to remove {{system}}, {{system_schema}} keyspaces exclusion from AuditLogManager. This patch allows the user to enable the audit log for system keyspaces. ","19/Oct/18 14:58;eperott;Thanks! The patch looks mostly good to me.

There is one corner case that I'm not able to cover properly though; that is having audit logs on _all_ keyspaces. This is not possible to configure, I think, since it is not possible to have an empty {{excluded_keyspaces}} list in the yaml, and anything listed in {{included_keyspaces}} will be overruled by things in the {{excluded_keyspaces}} list. The only way around it would be to configure a single non-existing keyspace in the {{excluded_keyspaces}} list, but that doesn't feel quite right.

Also, most out of curiosity, what's the reasoning for adding {{system_virtual_schema}} to the default exclude-list?

 ","19/Oct/18 21:45;vinaykumarcse;Thanks for reviewing the patch [~eperott]
{quote}There is one corner case that I'm not able to cover properly though; that is having audit logs on all keyspaces.
{quote}
You could have an empty {{excluded_keyspaces}} in yaml without mentioning any keyspaces. 
e.g.,
{code:java}
excluded_keyspaces: """"
{code}
{quote}Also, most out of curiosity, what's the reasoning for adding system_virtual_schema to the default exclude-list?
{quote}
{{system_virtual_schema}} was introduced as part of CASSANDRA-7622, these column families tend to get queries from tools which are often at high frequency and generates a lot of audit messages from internal tools. However, if an operator decides to audit this keyspace they can do that from {{cassandra.yaml}} file

Let me know if this does not satisfy your requirements.","22/Oct/18 10:51;eperott;{quote}You could have an empty {{excluded_keyspaces}} in yaml without mentioning any keyspaces.
{quote}
Ahh, right! That works for me. And same approach is applicable for the nodetool options.

I'm +1 on this patch!","31/Oct/18 16:03;eperott;[~krummas], since you're looking into releated parts in CASSANDRA-14772, would you be able to review (and merge) this patch?","02/Nov/18 15:33;krummas;sure, I'll try to have a look next week","16/Nov/18 14:12;krummas;this lgtm, with a tiny nit: https://github.com/krummas/cassandra/commit/ae103bfef73abdaa5f91bb7a0be75cbcbcd3ae62

running tests here: https://circleci.com/workflow-run/1d78e8cc-98a2-4acd-af47-505064f94c6f - will commit if they look ok","16/Nov/18 15:19;krummas;seems a bunch of unit tests fail:
https://circleci.com/gh/krummas/cassandra/1031#tests/containers/13","16/Nov/18 22:36;vinaykumarcse;Thanks for reviewing, I am looking into those failed tests. ","17/Nov/18 07:24;vinaykumarcse;
[~krummas] Updated the patch to fix the tests (attached patch worked on trunk without CASSANDRA-13668 changes). Below is the branch and CircleCI unit tests, there is one unit test failing which is being addressed in CASSANDRA-14889


||trunk||Circle CI||
|[trunk_CASS-14498|https://github.com/vinaykumarchella/cassandra/tree/trunk_CASS-14498]|[utests|https://circleci.com/gh/vinaykumarchella/cassandra/321#tests/containers/68]|","19/Nov/18 11:38;krummas;tests look good, committed as {{f46762eeca9f5d7e32e731573a8c3e521b70fc05}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to 4.0 fails with NullPointerException,CASSANDRA-14820,13191171,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,aweisberg,tommy_s,tommy_s,12/Oct/18 11:07,12/Mar/19 14:09,13/Mar/19 22:35,16/Oct/18 22:51,4.0,,,,,,,,0,,,,"I tested to upgrade an existing cluster to latest 4.0 but it fails with a NullPointerException, I upgraded from 3.0.15 but upgrading from any 3.0.x or 3.11.x to 4.0 will give the same fault.
{noformat}
 
2018-10-12T11:27:02.261+0200 ERROR [main] CassandraDaemon.java:251 Error while loading schema: 
java.lang.NullPointerException: null
 at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:156)
 at org.apache.cassandra.serializers.AbstractTextSerializer.deserialize(AbstractTextSerializer.java:41)
 at org.apache.cassandra.serializers.AbstractTextSerializer.deserialize(AbstractTextSerializer.java:28)
 at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:116)
 at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:267)
 at org.apache.cassandra.schema.SchemaKeyspace.createTableParamsFromRow(SchemaKeyspace.java:997)
 at org.apache.cassandra.schema.SchemaKeyspace.fetchTable(SchemaKeyspace.java:973)
 at org.apache.cassandra.schema.SchemaKeyspace.fetchTables(SchemaKeyspace.java:927)
 at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:886)
 at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesWithout(SchemaKeyspace.java:877)
 at org.apache.cassandra.schema.SchemaKeyspace.fetchNonSystemKeyspaces(SchemaKeyspace.java:865)
 at org.apache.cassandra.schema.Schema.loadFromDisk(Schema.java:102)
 at org.apache.cassandra.schema.Schema.loadFromDisk(Schema.java:91)
 at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:247)
 at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:590)
 at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:679)
{noformat}
 The problem seams to be line 997 in SchemaKeyspace.java

 
{noformat}
.speculativeWriteThreshold(SpeculativeRetryPolicy.fromString(row.getString(""speculative_write_threshold""{noformat}
speculative_write_threshold is a new table option introduced in CASSANDRA-14404, when upgrading the table option is missing and we get a NullPointerException on this line.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-10-12 16:29:38.483,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 18 14:37:16 UTC 2018,,,,,,0|i3z4lb:,9223372036854775807,4.0,,,,,djoshi3,,,,,,,,,,,"12/Oct/18 16:29;aweisberg;[trunk changes|https://github.com/apache/cassandra/compare/trunk...aweisberg:14820-trunk?expand=1]
 [dtest changes|https://github.com/apache/cassandra-dtest/compare/master...aweisberg:14820?expand=1]
 [CircleCI|https://circleci.com/gh/aweisberg/cassandra/tree/14820-trunk] [!https://circleci.com/gh/aweisberg/cassandra/tree/14820-trunk.png?circle-token=d9f905fd5057cf16c947a84cfdf9b10a4f58252a! | https://circleci.com/gh/aweisberg/cassandra/tree/14820-trunk]",12/Oct/18 18:31;aweisberg;Along with fixing this issue I am doing the rename from CASSANDRA-14671 so that when people upgrade it doesn't eventually break due to the rename.,16/Oct/18 05:17;djoshi3;I found one minor issue in {{ddl.rst#L466}} - {{additional_write_policy}} description seems to be duplicated from the earlier row. Also minor nit on the white space. Please fix on commit. Thanks!,"16/Oct/18 14:27;aweisberg;ddl.rst isn't a mistake per se we haven't renamed  speculative_retry yet. That's going to be a separate issue. We are documenting them together because they accept the same parameters.

What is the whitespace change you are looking for?",16/Oct/18 21:39;djoshi3;+1 Ariel & I spoke offline about the spacing issue.,"16/Oct/18 22:51;aweisberg;Committed to trunk as [4ae229f5cd270c2b43475b3f752a7b228de260ea|https://github.com/apache/cassandra/commit/4ae229f5cd270c2b43475b3f752a7b228de260ea] and dtests as [104835d880b4ace131e341235359606347783102|https://github.com/apache/cassandra-dtest/commit/104835d880b4ace131e341235359606347783102].

Thanks!","18/Oct/18 14:37;cscotta;Thanks [~aweisberg] and [~djoshi3].

[~tommy_s], thank you very much for this report!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix missing rows when reading 2.1 SSTables with static columns in 3.0,CASSANDRA-14873,13196918,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,iamaleksey,iamaleksey,iamaleksey,07/Nov/18 19:41,12/Mar/19 14:08,13/Mar/19 22:35,13/Nov/18 15:34,3.0.18,3.11.4,,,Legacy/Local Write-Read Paths,,,,0,,,,"If a partition has a static row and is large enough to be indexed, then {{firstName}} of the first index block will be set to a static clustering. When deserializing the column index we then incorrectly deserialize the {{firstName}} as a regular, non-{{STATIC}} {{Clustering}} - a singleton array with an empty {{ByteBuffer}} to be exact. Depending on the clustering comparator, this can trip up binary search over {{IndexInfo}} list and cause an incorrect resultset to be returned.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-13 01:14:34.329,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 13 15:34:10 UTC 2018,,,,,,0|s008o0:,9223372036854775807,,,,,,bdeggleston,,,,,,,,,,,"12/Nov/18 19:36;iamaleksey;Code: [3.0|https://github.com/iamaleksey/cassandra/commits/14873-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/14873-3.11]. CI: [3.0|https://circleci.com/workflow-run/81d0530a-3d53-4831-ac9a-7051283caadf], [3.11|https://circleci.com/workflow-run/8a0efd5e-2a1e-455e-ba2c-b0bc1a2c29c6].","13/Nov/18 01:14;bdeggleston;+1 on the fix, could you take a look at the failing dtests though? The only dtest I've seen fail recently in 3.0 and 3.11 is the HSHA one.","13/Nov/18 15:34;iamaleksey;Thanks.

Had a look, yes. Just the usual flaky suspects.

Committed as [9eee7aa7874c17ab28d43ff58c97da889c87e397|https://github.com/apache/cassandra/commit/9eee7aa7874c17ab28d43ff58c97da889c87e397] to 3.0, merged fully to 3.11, and to trunk with {{-s ours}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSL Cert Hot Reloading should check for sanity of the new keystore/truststore before loading it,CASSANDRA-14991,13210456,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,djoshi3,djoshi3,djoshi3,18/Jan/19 18:53,12/Mar/19 14:07,13/Mar/19 22:35,08/Feb/19 16:57,4.0,,,,Feature/Encryption,,,,0,security,,,"SSL Cert Hot Reloading assumes that the keystore & truststore are valid. However, a corrupt store or a password mismatch can cause Cassandra to fail accepting new connections as we throw away the old {{SslContext}}. This patch will ensure that we check the sanity of the certificates during startup and during hot reloading. This should protect against bad key/trust stores. As part of this PR, I have cleaned up the code a bit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-01-22 21:58:17.014,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 08 16:57:58 UTC 2019,,,,,,0|yi02vs:,9223372036854775807,,,,,aweisberg,aweisberg,,,,,,,,,,,"18/Jan/19 18:57;djoshi3;||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/14991-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/14991-trunk]|
||",18/Jan/19 21:13;djoshi3;dtest with vnodes have some failures but they're unrelated to this change. dtests without vnodes and utests are good.,19/Jan/19 01:11;djoshi3;The latest run is clean. I had to rebase the branch on latest trunk.,"22/Jan/19 21:58;aweisberg;Looks pretty good.

[This won't return an error to nodetool|https://github.com/apache/cassandra/compare/trunk...dineshjoshi:14991-trunk?expand=1#diff-3514653a59c886f2106d3099124f03bbR315]
[Maybe don't bind the encryption option references in case they ever become hot swappable|https://github.com/apache/cassandra/compare/trunk...dineshjoshi:14991-trunk?expand=1#diff-3514653a59c886f2106d3099124f03bbR355].
[Should buildTrustStore always be true for server options?|https://github.com/apache/cassandra/compare/trunk...dineshjoshi:14991-trunk?expand=1#diff-3514653a59c886f2106d3099124f03bbR374]","23/Jan/19 02:26;djoshi3;Hi [~aweisberg]. Thanks for looking at the PR. I have addressed comments 1 & 2. Regarding #3, the issue is that when we do optional client auth, we build the truststore based on the value in the configuration so I left it to the value of the client auth variable. When someone sets client auth to off, it is ok to have invalid values for the truststore configuration. If we try to enforce valid values it will deviate from its current behavior. Let me know if you have any other comments.","23/Jan/19 17:18;aweisberg;RE #3, but it's always hardcoded to true when the server actually goes to build the SSL certs?
https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/NettyFactory.java#L295
https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/NettyFactory.java#L372
Why should we test with different parameters then are used when we actually go to construct the SSL context? Is the SSL context being constructed with invalid parameters?",23/Jan/19 19:23;djoshi3;{{OptionalSecureInitializer}} inherits from {{AbstractSecureInitializer}}. It passes in {{require_client_auth}} which is sourced from the configuration. So we could generate a {{SslContext}} without instantiating the truststore. See [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/transport/Server.java#L409],23/Jan/19 19:41;aweisberg;That is for the client -> server not server -> server. Server -> server always hard codes it to true. So why wouldn't we test the server context the way it would be retrieved when we actually need to retrieve them?,23/Jan/19 20:56;djoshi3;You're right Ariel. I have pushed an update. Hopefully that fixes the confusion :),"31/Jan/19 17:14;aweisberg;+1
Can you add CHANGES.txt, squash, rebase, remove the CircleCI changes? I'll merge it then.","01/Feb/19 01:03;djoshi3;Thanks, [~aweisberg]. Here's the commit https://github.com/dineshjoshi/cassandra/commit/307717b307e2f244f6cf5cdb0f4e764e73a72734",08/Feb/19 16:57;aweisberg;Committed as [367cdc95514d4550db57054c90fb794fc29179d1|https://github.com/apache/cassandra/commit/367cdc95514d4550db57054c90fb794fc29179d1]. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress write hangs with default options,CASSANDRA-14616,13175905,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,cnlwsu,cnlwsu,31/Jul/18 17:57,12/Mar/19 14:07,13/Mar/19 22:35,07/Dec/18 03:04,3.0.18,3.11.4,4.0,,Tool/stress,,,,0,,,,"Cassandra stress sits there for incredibly long time after connecting to JMX. To reproduce {code}./tools/bin/cassandra-stress write{code}

If you give it a -n its not as bad which is why dtests etc dont seem to be impacted. Does not occur in 3.0 branch but does in 3.11 and trunk",,,,,,,,,,,,,CASSANDRA-14890,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-08-13 09:46:37.49,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 07 03:04:21 UTC 2018,,,,,,0|i3wj7z:,9223372036854775807,3.11.0,4.0,,,Stefania,Stefania,,,,3.11.0,,,,,,,"13/Aug/18 09:46;Yarnspinner;Hello, I would like to try solving this issue.

I have done some preliminary testing and it appears that it is caused by cassandra-stress waiting for uncertainty to stabilize, the trace from jstack is included below.

{quote}
java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000076d873e20> (a java.util.concurrent.CountDownLatch$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
        at org.apache.cassandra.stress.util.Uncertainty$WaitForTargetUncertainty.await(Uncertainty.java:56)
        at org.apache.cassandra.stress.util.Uncertainty.await(Uncertainty.java:85)
        at org.apache.cassandra.stress.report.StressMetrics.waitUntilConverges(StressMetrics.java:135)
        at org.apache.cassandra.stress.StressAction.run(StressAction.java:269)
        at org.apache.cassandra.stress.StressAction.warmup(StressAction.java:121)
        at org.apache.cassandra.stress.StressAction.run(StressAction.java:70)
        at org.apache.cassandra.stress.Stress.run(Stress.java:143)
        at org.apache.cassandra.stress.Stress.main(Stress.java:62)
{quote}

I also did some printlns for debugging in 3.11.
{quote}
uncertainty: NaN targetUncertainty: 0.020000 
measurements: 1 minMeasurements: 30 
measurements: 1 maxMeasurements: 200 
uncertainty: NaN targetUncertainty: 0.020000 
measurements: 2 minMeasurements: 30 
measurements: 2 maxMeasurements: 200 
...
uncertainty: NaN targetUncertainty: 0.020000 
measurements: 200 minMeasurements: 30 
measurements: 200 maxMeasurements: 200 
{quote}

In the warmup phase, the program aims for either uncertainty to fall below 0.02 with at least 30 measurements or to hit 200 measurements. It ends up waiting for 200 measurements since the uncertainty is always NaN. The same problem doesn't occur in 3.0 because the Runnable (https://github.com/apache/cassandra/blob/cassandra-3.0/tools/stress/src/org/apache/cassandra/stress/StressMetrics.java#L86) calls wakeAll after 2 iterations. However, uncertainty is still always NaN in 3.0.

 The problem arises in 3.11 and trunk as that runnable loop was refactored  into reportingLoop which waited for all 200 tries first. https://github.com/apache/cassandra/blob/cassandra-3.11/tools/stress/src/org/apache/cassandra/stress/report/StressMetrics.java#L154

Here's what it looks like for 3.0.
{quote}
Warming up WRITE with 0 iterations...
_______________________
Updated value: NaN 
uncertainty: NaN targetUncertainty: 0.020000 
measurements: 1 minMeasurements: 30 
measurements: 1 maxMeasurements: 200 
_______________________
Updated value: NaN 
uncertainty: NaN targetUncertainty: 0.020000 
measurements: 2 minMeasurements: 30 
measurements: 2 maxMeasurements: 200 
latch counted down via wakeall
wakeAll via line 123 in stressmetrics
WARNING: uncertainty mode (err<) results in uneven workload between thread runs, so should be used for high level analysis only
Running with 4 threadCount
{quote}

I think this is being caused by having 0 iterations for warmup. The number of iterations is decided at the start by 
 {{Math.min(50000, (int) (settings.command.count * 0.25)) * settings.node.nodes.size();}}. 
https://github.com/apache/cassandra/blob/cassandra-3.11/tools/stress/src/org/apache/cassandra/stress/StressAction.java#L108 . 

When {{./tools/bin/cassandra-stress write}} is called without any arguments, settings.command.count evaluates to -1 and  {{Math.min(50000, (int) (settings.command.count * 0.25)) * settings.node.nodes.size();}} evaluates to 0 so we always end up with 0 iterations. 

One proposed fix is to choose a minimum nonzerovalue for iterations in the warmup phase. Something like https://github.com/yarnspinnered/cassandra/commit/33cf059f63b56ac17a3f66869615d3d7cc52f8a9 . I tried this and it no longer hangs but I'm not sure on the exact value or if there is a better way to fix this.","15/Nov/18 00:05;jay.zhuang;Hi [~Yarnspinner], the fix looks good. I had the similar fix which re-enables {{warm-up}} to 50k as before ([{{StressAction.java}}|https://github.com/apache/cassandra/commit/6a1b1f26b7174e8c9bf86a96514ab626ce2a4117#diff-fd2f2d2364937fcb1c0d73c8314f1418L90])

|Branch|uTest|dTest|
|[14890-3.0|https://github.com/cooldoger/cassandra/tree/14890-3.0]|[!https://circleci.com/gh/cooldoger/cassandra/tree/14890-3.0.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14890-3.0]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/661/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/661/]|
|[14890-3.11|https://github.com/cooldoger/cassandra/tree/14890-3.11]|[!https://circleci.com/gh/cooldoger/cassandra/tree/14890-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14890-3.11]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/662/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/662/]|
|[14890-trunk|https://github.com/cooldoger/cassandra/tree/14890-trunk]|[!https://circleci.com/gh/cooldoger/cassandra/tree/14890-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14890-trunk]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/663/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/663/]|

Here is a dTest to reproduce the problem:
|[14890|https://github.com/cooldoger/cassandra-dtest/tree/14890]|",15/Nov/18 01:07;jay.zhuang;The failed the utest is because of CASSANDRA-14891,"15/Nov/18 02:19;Stefania;[~jay.zhuang] , [~Yarnspinner] , thanks for fixing this problem and writing the test.

I checked both patches, they are both good. Perhap's Jay's approach of assuming 50,000 iterations for duration tests is slightly preferable since that was the old behavior.

 ","19/Nov/18 01:58;Stefania;[~Yarnspinner], [~jay.zhuang] are you OK with committing Jay's approach? I don't mind too much which approach, they are both OK, it's just a matter of picking a default value.

Jay you are a committer correct? So if Jeremey is OK committing your patch, I assume you prefer to merge it yourself?",07/Dec/18 03:04;jay.zhuang;Thank you [~Stefania] for the review. Committed as [{{bbf7dac}}|https://github.com/apache/cassandra/commit/bbf7dac87cdc41bf8e138a99f630e7a827ad0d98]. The dTest is committed as [{{325ef3f}}|https://github.com/apache/cassandra-dtest/commit/325ef3fa063252e6dad88473613abbd829e8c24d].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect log entry during startup in 4.0,CASSANDRA-14836,13193536,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tommy_s,tommy_s,tommy_s,23/Oct/18 10:52,12/Mar/19 14:21,13/Mar/19 22:35,23/Oct/18 20:45,,,,,Local/Startup and Shutdown,,,,0,,,,"When doing some testing on 4.0 I found this in the log:
{noformat}
2018-10-12T14:06:14.507+0200  INFO [main] StartupClusterConnectivityChecker.java:113 After waiting/processing for 10005 milliseconds, 1 out of 3 peers (0.0%) have been marked alive and had connections established{noformat}
1 out of 3 is not 0%:)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-10-23 20:17:54.369,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 23 20:45:00 UTC 2018,,,,,,0|i3zj4f:,9223372036854775807,4.0,,,,aweisberg,aweisberg,,,,,,,,,,,23/Oct/18 11:13;tommy_s;Patch availible here: [cassandra-14836|https://github.com/tommystendahl/cassandra/commit/bcaa224dec3939dcc883ded250fb9849bfbfa992],"23/Oct/18 20:17;aweisberg;+1, I made some small tweaks. I removed the extra float cast and I added string formatting so we only print two digits after the decimal point.

Running the tests now. 

https://github.com/apache/cassandra/compare/trunk...aweisberg:14836-trunk?expand=1
https://circleci.com/gh/aweisberg/cassandra/tree/14836-trunk
",23/Oct/18 20:45;aweisberg;Committed as [c3ef43d45e5c9c44f22dbeb8a58232aa6f0cfd15|https://github.com/apache/cassandra/commit/c3ef43d45e5c9c44f22dbeb8a58232aa6f0cfd15] thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress throws NPE if insert section isn't specified in user profile,CASSANDRA-14426,13155923,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,alexott,alexott,alexott,28/Apr/18 18:02,12/Mar/19 14:21,13/Mar/19 22:35,28/Apr/18 21:04,4.0,,,,Tool/stress,,,,0,,,,"When user profile file is used, and insert section isn't specified, then cassandra-stress is using default values instead.

Since support for LWTs was added, absence of the insert section lead to throwing of NullPointerException when generating inserts:

{noformat}
java.lang.NullPointerException
	at org.apache.cassandra.stress.StressProfile.getInsert(StressProfile.java:546)
	at org.apache.cassandra.stress.StressProfile.printSettings(StressProfile.java:126)
	at org.apache.cassandra.stress.settings.StressSettings.lambda$printSettings$1(StressSettings.java:311)
	at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684)
	at org.apache.cassandra.stress.settings.StressSettings.printSettings(StressSettings.java:311)
	at org.apache.cassandra.stress.Stress.run(Stress.java:108)
	at org.apache.cassandra.stress.Stress.main(Stress.java:63)
{noformat}

Fix is trivial, and will be provided as PR",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-04-28 18:05:19.863,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 28 21:12:29 UTC 2018,,,,,,0|i3t55z:,9223372036854775807,,,,,jjirsa,jjirsa,,,,,,,,,,,"28/Apr/18 18:05;githubbot;GitHub user alexott opened a pull request:

    https://github.com/apache/cassandra/pull/221

    fix for CASSANDRA-14426

    This commit fixes NPE when the `insert` section is missing from user-defined profile

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/alexott/cassandra CASSANDRA-14426

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/cassandra/pull/221.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #221
    
----
commit 403e463a81333dec232d353feb28f31611e4bb08
Author: Alex Ott <alexott@...>
Date:   2018-04-28T18:04:13Z

    fix for CASSANDRA-14426
    
    This commit fixes NPE when the `insert` section is missing from user-defined profile

----
",28/Apr/18 18:09;jjirsa;Is trunk/4.x sufficient?,"28/Apr/18 18:12;alexott;Yes, I think that trunk is sufficient - this is more edge case...","28/Apr/18 18:21;jjirsa;Appreciate the debugging and the fix. I think your fix is fine, but I'd like to change it a bit to move the:

{code}
        if (insert == null)
            insert = new HashMap<>();
        lowerCase(insert);
{code}

From below the {{if (!isKeyOnlyTable)}} if/else block to above, making your check irrelevant.  Are you OK with that change? ","28/Apr/18 18:22;jjirsa;Explicitly, this is what I propose (and I'm happy to do it on commit for you):

{code}
diff --git a/tools/stress/src/org/apache/cassandra/stress/StressProfile.java b/tools/stress/src/org/apache/cassandra/stress/StressProfile.java
index 6c44ff298a..2338873a8d 100644
--- a/tools/stress/src/org/apache/cassandra/stress/StressProfile.java
+++ b/tools/stress/src/org/apache/cassandra/stress/StressProfile.java
@@ -499,6 +499,10 @@ public class StressProfile implements Serializable
                         }
                     }

+                    if (insert == null)
+                        insert = new HashMap<>();
+                    lowerCase(insert);
+
                     //Non PK Columns
                     StringBuilder sb = new StringBuilder();
                     if (!isKeyOnlyTable)
@@ -543,7 +547,7 @@ public class StressProfile implements Serializable

                         //Put PK predicates at the end
                         sb.append(pred);
-                        if (insert != null && insert.containsKey(""condition""))
+                        if (insert.containsKey(""condition""))
                         {
                             sb.append("" "" + insert.get(""condition""));
                             insert.remove(""condition"");
@@ -563,10 +567,6 @@ public class StressProfile implements Serializable
                         sb.append("") "").append(""values("").append(value).append(')');
                     }

-                    if (insert == null)
-                        insert = new HashMap<>();
-                    lowerCase(insert);
-
                     partitions = select(settings.insert.batchsize, ""partitions"", ""fixed(1)"", insert, OptionDistribution.BUILDER);
                     selectchance = select(settings.insert.selectRatio, ""select"", ""fixed(1)/1"", insert, OptionRatioDistribution.BUILDER);
                     rowPopulation = select(settings.insert.rowPopulationRatio, ""row-population"", ""fixed(1)/1"", insert, OptionRatioDistribution.BUILDER);
{code}
","28/Apr/18 21:04;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/221
","28/Apr/18 21:04;jjirsa;Going to assume you're ok with my changes (if you're not, let me know), committed as {{71a27ee2b93a47e177edc16571f91bb5d592899e}} ","28/Apr/18 21:12;alexott;Thank you Jeff!
Your solution is more generic - I simply wasn't sure that it would be a good style to introduce some kind of ""defaults"" there.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SafeMemoryWriterTest doesn't compile on trunk,CASSANDRA-14681,13182174,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,snazy,snazy,snazy,30/Aug/18 16:27,12/Mar/19 14:21,13/Mar/19 22:35,30/Aug/18 17:13,4.0,,,,,,,,0,Java11,,,"{{SafeMemoryWriterTest}} references {{sun.misc.VM}}, which doesn't exist in Java 11, so the build fails.

Proposed patch makes the test work against Java 8 + 11.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-08-30 16:31:42.704,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 30 17:13:35 UTC 2018,,,,,,0|i3xljj:,9223372036854775807,,,,,,,,,,4.0,,,,,,,"30/Aug/18 16:29;snazy;[branch|https://github.com/snazy/cassandra/tree/14681-fix-SafeMemoryWriterTest-trunk]

Verified that it both builds against Java 8 and 11+8 and passes against 8 + 11.",30/Aug/18 16:31;blambov;LGTM,"30/Aug/18 17:13;snazy;Thanks for the review!

 

Committed as [8b1a6247ec5aabad92a664ff2cbf6d6529d8ceb7|https://github.com/apache/cassandra/commit/8b1a6247ec5aabad92a664ff2cbf6d6529d8ceb7] to [trunk|https://github.com/apache/cassandra/tree/trunk]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CircleCI config has dtests enabled but not the correct resources settings,CASSANDRA-14614,13175668,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jrwest,jrwest,jrwest,30/Jul/18 20:30,12/Mar/19 14:20,13/Mar/19 22:35,31/Jul/18 11:30,,,,,Test/dtest,,,,0,,,,"The commit for  -CASSANDRA-9608- enabled the {{with_dtests_jobs}} configuration in {{.circleci/config.yml}} but not the necessary env var settings. We should revert this, unless we planned to start running dtests with the correct resources on every master commit, in which case we should fix the resources.

(cc [~snazy] [~jasobrown])",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-31 02:23:18.75,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 31 11:30:14 UTC 2018,,,,,,0|i3whrj:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,30/Jul/18 20:31;jrwest;I'll have a patch up later this afternoon if no one wants to take this. Just need to run to a few meetings. ,30/Jul/18 23:44;jrwest;[branch|https://github.com/jrwest/cassandra/tree/14614-trunk]  | [tests|https://circleci.com/gh/jrwest/cassandra/tree/14614-trunk],31/Jul/18 02:23;jasobrown;+1. Commit later tonight.,31/Jul/18 11:30;jasobrown;Committed as sha {{e663ccd98dbb0696db0f511c5f3428c13e7f8c73}}. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If SizeEstimatesRecorder misses a 'onDropTable' notification, the size_estimates table will never be cleared for that table.",CASSANDRA-14905,13199553,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,Gerrrr,Gerrrr,Gerrrr,20/Nov/18 09:04,12/Mar/19 14:06,13/Mar/19 22:35,15/Jan/19 16:14,3.0.18,3.11.4,4.0,,Observability/Metrics,,,,0,,,,"if a node is down when a keyspace/table is dropped, it will receive the schema notification before the size estimates listener is registered, so the entries for the dropped keyspace/table will never be cleaned from the table. ",,,,,,,,,,,,,,,,,,21/Dec/18 16:22;Gerrrr;14905-3.0-dtest.png;https://issues.apache.org/jira/secure/attachment/12952697/14905-3.0-dtest.png,21/Dec/18 16:22;Gerrrr;14905-3.0-testall.png;https://issues.apache.org/jira/secure/attachment/12952698/14905-3.0-testall.png,21/Dec/18 16:22;Gerrrr;14905-3.11-dtest.png;https://issues.apache.org/jira/secure/attachment/12952699/14905-3.11-dtest.png,21/Dec/18 16:22;Gerrrr;14905-3.11-testall.png;https://issues.apache.org/jira/secure/attachment/12952700/14905-3.11-testall.png,21/Dec/18 16:22;Gerrrr;14905-4.0-dtest.png;https://issues.apache.org/jira/secure/attachment/12952701/14905-4.0-dtest.png,21/Dec/18 16:22;Gerrrr;14905-4.0-testall.png;https://issues.apache.org/jira/secure/attachment/12952702/14905-4.0-testall.png,,,,,,6.0,,,,,,,,,,,,,,,,,,,2018-12-31 18:52:53.097,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 15 16:14:04 UTC 2019,,,,,,0|s00ovk:,9223372036854775807,,,,,aweisberg,aweisberg,,,,,,,,,,,"20/Nov/18 09:53;Gerrrr;Patches:
* [3.0|https://github.com/Gerrrr/cassandra/tree/14905-3.0]
* [3.11|https://github.com/Gerrrr/cassandra/tree/14905-3.11]
* [4.0|https://github.com/Gerrrr/cassandra/tree/14905-4.0]
* [dtest|https://github.com/Gerrrr/cassandra-dtest/tree/14905]","21/Dec/18 16:26;Gerrrr;CI:

||3.0||3.11||4.0||
|[dtest|https://issues.apache.org/jira/secure/attachment/12952697/14905-3.0-dtest.png]|[dtest|https://issues.apache.org/jira/secure/attachment/12952699/14905-3.11-dtest.png]|[dtest|https://issues.apache.org/jira/secure/attachment/12952701/14905-4.0-dtest.png]|
|[testall|https://issues.apache.org/jira/secure/attachment/12952698/14905-3.0-testall.png]|[testall|https://issues.apache.org/jira/secure/attachment/12952700/14905-3.11-testall.png]|[testall|https://issues.apache.org/jira/secure/attachment/12952702/14905-4.0-testall.png]|
","31/Dec/18 18:52;aweisberg;LGTM. There is some whitespace churn in the 3.11 patch that isn't in the other patches. I would rather do a dedicated whitespace ticket and tackle more files then sneak it in here since we aren't touching that code.

Who is getting the author credit here? Multiple author credits?",08/Jan/19 10:46;Gerrrr;Thanks for the review! It'll be great if both authors can get the credit. Otherwise please give it to Joel.,"15/Jan/19 16:14;aweisberg;Ran the dtests [here|https://circleci.com/gh/aweisberg/cassandra/2502#tests/containers/11] and [here|https://circleci.com/gh/aweisberg/cassandra/2503#tests/containers/32] as well as the utests.

https://circleci.com/gh/aweisberg/cassandra/2500
https://circleci.com/gh/aweisberg/cassandra/2498
https://circleci.com/gh/aweisberg/cassandra/2496

Committed as [ddbcff3363c5ad13bd8975e80b3f28ae8149a459|https://github.com/apache/cassandra/commit/ddbcff3363c5ad13bd8975e80b3f28ae8149a459] in Cassandra and [7a6d9002709628de2bc6af9d987a189b302e4472|https://github.com/apache/cassandra-dtest/commit/7a6d9002709628de2bc6af9d987a189b302e4472] in the dtests.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resume compresed hints delivery broken,CASSANDRA-14419,13155345,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tommy_s,tommy_s,tommy_s,26/Apr/18 09:08,12/Mar/19 14:06,13/Mar/19 22:35,03/Jul/18 13:25,3.0.17,,,,Consistency/Hints,,,,0,,,,"We are using Cassandra 3.0.15 and are using compressed hints, but if hint delivery is interrupted resuming hint delivery is failing.

{code}

2018-04-04T13:27:48.948+0200 ERROR [HintsDispatcher:14] CassandraDaemon.java:207 Exception in thread Thread[HintsDispatcher:14,1,main]
java.lang.IllegalArgumentException: Unable to seek to position 1789149057 in /var/lib/cassandra/hints/9592c860-1054-4c60-b3b8-faa9adc6d769-1522838912649-1.hints (118259682 bytes) in read-only mode
        at org.apache.cassandra.io.util.RandomAccessReader.seek(RandomAccessReader.java:287) ~[apache-cassandra-clientutil-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsReader.seek(HintsReader.java:114) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatcher.seek(HintsDispatcher.java:83) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.deliver(HintsDispatchExecutor.java:263) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:248) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:226) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.run(HintsDispatchExecutor.java:205) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_152]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_152]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_152]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_152]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [apache-cassandra-3.0.15.jar:3.0.15]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_152]

{code}

 I think the problem is similar to CASSANDRA-11960.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-12 11:30:20.462,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 03 13:34:25 UTC 2018,,,,,,0|i3t1ov:,9223372036854775807,3.0.15,3.0.16,,,iamaleksey,iamaleksey,,,,3.0.3,,,,,,,"26/Apr/18 10:26;tommy_s;I think the fix is similar as the fix for CASSANDRA-11960 so I have tried to backport that pach to the 3.0 branch: [cassandra-14419-30|https://github.com/tommystendahl/cassandra/tree/cassandra-14419-30].
But the unit tests are broken, HintsCompressionTest failse with:
{code}
java.io.IOException: Digest mismatch exception
    FSReadError in /tmp/1524648911701-0/f3647df0-486b-11e8-a9fe-05db90a7d16a-1524648911695-1.hints
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:199)
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:164)
    at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
    at org.apache.cassandra.hints.HintsCompressionTest.multiFlushAndDeserializeTest(HintsCompressionTest.java:124)
    at org.apache.cassandra.hints.HintsCompressionTest.lz4Compressor(HintsCompressionTest.java:143)
    Caused by: java.io.IOException: Digest mismatch exception
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNextInternal(HintsReader.java:216)
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:190)
{code}","02/May/18 12:16;tommy_s;Backporting the fix for CASSANDRA-11960 to the 3.0 branch might not be as easy as I thought, changes introduced by CASSANDRA-5863 messed up things for me, thats why the unit tests failed. But I still think the solution is similar to CASSANDRA-11960.","08/May/18 11:39;tommy_s;I have found some conflicts from CASSANDRA-5863 and fixed them, but I'm not sure I have found everything yet but at least the unit tests are are working now.

My latest version of the commit is here: [cassandra-14419-30|https://github.com/tommystendahl/cassandra/tree/cassandra-14419-30]

I had to remove {{HintsServiceTest.java}} since it depended on CASSANDRA-12016 which is not availible on the 3.0 branch but I have a local branch where I have that test working. I have also modified it to test with compression on and it works fine.","16/May/18 12:29;tommy_s;I have convinced myself that my patch is complete now and I have done testing using ccm and and it seams to be working now so I this should be ready for review now.

[cassandra-14419-30|https://github.com/tommystendahl/cassandra/tree/cassandra-14419-30]","12/Jun/18 10:49;tommy_s;I looked over my patch again and realized that I had included some unnecessary changes so I removed them to reduce the size of the patch, hopefully a bit easier to review.

[cassandra-14419-30|https://github.com/tommystendahl/cassandra/tree/cassandra-14419-30]","12/Jun/18 11:30;iamaleksey;Will try to review this week, or at worst next one.

Thanks for the patch.","16/Jun/18 20:24;JoshuaMcKenzie;""Anonymous"" needs to stop setting things to ""Ready to Commit"" once every two months. ;)","28/Jun/18 19:56;iamaleksey;[~tommy_s] the patch looks good to me. There are some things I'd do slightly differently, but it's really not worth introducing a delta between 3.0 and 3.11/4.0 here if avoidable, so I'm fine with the backport as is.

That said, can you please backport {{AlteredHints}} and, yes, {{HintsServiceTest}}? I'm not comfortable committing this otherwise.

Thanks!","02/Jul/18 14:29;tommy_s;[~iamaleksey] thanks for the review.

I don't think we need the base class {{AlteredHints}} so I moved the relevant changes into {{HintsCompressionTest}}. I updated my branch with a commit for this, [cassandra-14419-30|https://github.com/tommystendahl/cassandra/tree/cassandra-14419-30].

For {{HintsServiceTest}} do you think I should backport the complete patch for CASSANDRA-12016 or should I copy the bare minimum needed for {{HintsServiceTest}}? It's relatively easy to backport the complete patch and I can't see any side effects except that part of it isn't needed for {{HintsServiceTest}}.",02/Jul/18 15:38;iamaleksey;[~tommy_s] I don't have a strong opinion here. It is a safe commit to backport wholesale - so do whichever you feel like in this case.,03/Jul/18 11:12;tommy_s;[~iamaleksey] I have created a new branch [cassandra-14419-30-v2|https://github.com/tommystendahl/cassandra/tree/cassandra-14419-30-v2] with two commits. One for the backport of the complete CASSANDRA-12016 and one for backport of the complete CASSANDRA-11960 incl the updates in {{HintsCompressionTest}}.,"03/Jul/18 13:25;iamaleksey;[~tommy_s] cheers. I ended up essentially cherry-picking CASSANDRA-12016 commit - to retain authorship, but committed your backport of CASSANDRA-11960 as is (except added the missing ASF license  header to {{InputPosition}}).

Committed as {{c4982587bfe3cb6946daa2912fe46146edef7fbf}} to 3.0 only and merged upwards with -s ours.","03/Jul/18 13:34;tommy_s;[~iamaleksey] Thats great, thanks a lot!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using nodetool status after enabling Cassandra internal auth for JMX access fails with currently documented permissions,CASSANDRA-14481,13162924,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dataindataout,dataindataout,dataindataout,30/May/18 14:53,12/Mar/19 14:06,13/Mar/19 22:35,21/Jun/18 03:08,4.0,,,,Legacy/Documentation and Website,,,,2,security,,,"Using the documentation here:

[https://cassandra.apache.org/doc/latest/operating/security.html#cassandra-integrated-auth]

Running `nodetool status` on a cluster fails as follows:
{noformat}
error: Access Denied
-- StackTrace --
java.lang.SecurityException: Access Denied
at org.apache.cassandra.auth.jmx.AuthorizationProxy.invoke(AuthorizationProxy.java:172)
at com.sun.proxy.$Proxy4.invoke(Unknown Source)
at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1468)
at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309)
at java.security.AccessController.doPrivileged(Native Method)
at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1408)
at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829)
at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:357)
at sun.rmi.transport.Transport$1.run(Transport.java:200)
at sun.rmi.transport.Transport$1.run(Transport.java:197)
at java.security.AccessController.doPrivileged(Native Method)
at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:573)
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:835)
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:688)
at java.security.AccessController.doPrivileged(Native Method)
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:687)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:283)
at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:260)
at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:161)
at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
at javax.management.remote.rmi.RMIConnectionImpl_Stub.invoke(Unknown Source)
at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.invoke(RMIConnector.java:1020)
at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:298)
at com.sun.proxy.$Proxy7.effectiveOwnership(Unknown Source)
at org.apache.cassandra.tools.NodeProbe.effectiveOwnership(NodeProbe.java:489)
at org.apache.cassandra.tools.nodetool.Status.execute(Status.java:74)
at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:255)
at org.apache.cassandra.tools.NodeTool.main(NodeTool.java:169) {noformat}
Permissions on two additional mbeans were required:
{noformat}
GRANT EXECUTE ON MBEAN 'org.apache.cassandra.db:type=StorageService' TO jmx;
GRANT EXECUTE ON MBEAN 'org.apache.cassandra.db:type=EndpointSnitchInfo' TO jmx;
{noformat}
I've updated the documentation in my fork here and would like to do a pull request for the addition:

[https://github.com/dataindataout/cassandra/blob/docs_operating_security/doc/source/operating/security.rst]

 ","Apache Cassandra 3.11.2

Centos 6.9",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-20 10:00:17.834,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 21 03:08:56 UTC 2018,,,,,,0|i3ubg7:,9223372036854775807,,,,,eperott,eperott,,,,,,,,,,,"30/May/18 14:54;dataindataout;This is my first ""PR"" to this project. I read the contributions document, but please let me know if I've missed any required tags or otherwise need to edit my submission. Thank you very much.","20/Jun/18 10:00;eperott;Thanks for your contribution.

I've verified the issue and the proposed fix on 3.11.2 and trunk.

Some comments on your proposed changes.
{quote}GRANT EXECUTE ON MBEAN ‘org.apache.cassandra.db:type=EndpointSnitchInfo’ TO jmx;
 GRANT SELECT, EXECUTE ON MBEAN ‘org.apache.cassandra.db:type=StorageService’ TO jmx;
{quote}
Please update your patch such that it will contain relevant lines only. Right now you're duplicating parts of the example. 

Make sure to use a straight quotes {{'}}, not curved quotes {{’}} around the mbean names.

There is no need to grant both {{SELECT}} and {{EXECUTE}} on the {{StorageService}} as {{SELECT}} is granted to {{ALL MBEANS}} already in the example. And CQL don't let you grant two permissions in one statement anyway.

For small fixes like this, committers seem to prefer to get proposed fixes [like this|https://cassandra.apache.org/doc/latest/development/documentation.html#github-based-work-flow]. It is not clear to me how documentation is maintained and published on different branches/versions of Cassandra, but perhaps someone else can give advice on that.","20/Jun/18 13:58;dataindataout;Thank you very much for this information, [~eperott]. I will edit the PR as you have suggested and resubmit.","20/Jun/18 18:55;dataindataout;I've edited the file to support this PR in the following ways:

- changed curly quotes to flat quotes
- removed repeated lines
- moved changes to a branch called docs_operating_security","21/Jun/18 03:08;jjirsa;Thanks! Committed as {{4f02db5c45ece38dda48e0d19667888e9f46536e}} with [~eperott] as reviewer. The site won't update immediately, but will take effect on next rebuild.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableReaderTest#testOpeningSSTable fails on macOS,CASSANDRA-14387,13152775,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,djoshi3,djoshi3,djoshi3,16/Apr/18 17:17,12/Mar/19 14:06,13/Mar/19 22:35,16/Apr/18 18:38,3.0.17,3.11.3,4.0,,,,,,0,,,,"I ran into an issue with {{SSTableReaderTest#testOpeningSSTable}} test failure on macOS. The reason for failure seems that on macOS, the file modification timestamps are at a second granularity (See: https://stackoverflow.com/questions/18403588/how-to-return-millisecond-information-for-file-access-on-mac-os-x-in-java and https://developer.apple.com/legacy/library/technotes/tn/tn1150.html#HFSPlusDates). The fix is simple - bumping up the sleep time to 1 second instead of 10ms.


{noformat}
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testOpeningSSTable(org.apache.cassandra.io.sstable.SSTableReaderTest):	FAILED
    [junit] Bloomfilter was not recreated
    [junit] junit.framework.AssertionFailedError: Bloomfilter was not recreated
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.testOpeningSSTable(SSTableReaderTest.java:421)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.io.sstable.SSTableReaderTest FAILED
{noformat}

Related issue: CASSANDRA-11163",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-04-16 17:36:03.266,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 30 18:32:25 UTC 2018,,,,,,0|i3slwv:,9223372036854775807,,,,,cnlwsu,cnlwsu,,,,,,,,,,,"16/Apr/18 17:19;djoshi3;||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/sstable-junit-failure]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/sstable-junit-failure]|
||",16/Apr/18 17:36;cnlwsu;+1,"16/Apr/18 18:38;jjirsa;Committed to trunk as 8a5e1cbe293ee7c83efba0d0101ada0a80cfaf00
",30/Apr/18 13:34;iamaleksey;Can you please commit to 3.0 and 3.11 too? Because tests are failing on Jenkins :\ Cheers.,"30/Apr/18 18:14;iamaleksey;Cherry-plicked into 3.0 as [e16f0ed0698c5cb47ab2bb0a0b04966d5bdbcde0|https://github.com/apache/cassandra/commit/e16f0ed0698c5cb47ab2bb0a0b04966d5bdbcde0] and merged upwards, replacing the somewhat silly millis to millis conversion with {{TimeUnit.sleep()}} in the process.",30/Apr/18 18:32;djoshi3;Thank you for taking care of this [~iamaleksey],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speculative retry parsing breaks on non-english locale,CASSANDRA-14374,13151469,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,10/Apr/18 19:28,12/Mar/19 14:06,13/Mar/19 22:35,17/Apr/18 18:51,4.0,,,,,,,,0,,,,"I was getting the following error when running unit tests on my machine:
{code:none}
Error setting schema for test (query was: CREATE TABLE cql_test_keyspace.table_32 (a int, b int, c text, primary key (a, b)))
java.lang.RuntimeException: Error setting schema for test (query was: CREATE TABLE cql_test_keyspace.table_32 (a int, b int, c text, primary key (a, b)))
	at org.apache.cassandra.cql3.CQLTester.schemaChange(CQLTester.java:819)
	at org.apache.cassandra.cql3.CQLTester.createTable(CQLTester.java:632)
	at org.apache.cassandra.cql3.CQLTester.createTable(CQLTester.java:624)
	at org.apache.cassandra.cql3.validation.operations.DeleteTest.testDeleteWithNonoverlappingRange(DeleteTest.java:663)
Caused by: org.apache.cassandra.exceptions.ConfigurationException: Specified Speculative Retry Policy [99,00p] is not supported
	at org.apache.cassandra.service.reads.SpeculativeRetryPolicy.fromString(SpeculativeRetryPolicy.java:135)
	at org.apache.cassandra.schema.SchemaKeyspace.createTableParamsFromRow(SchemaKeyspace.java:1006)
	at org.apache.cassandra.schema.SchemaKeyspace.fetchTable(SchemaKeyspace.java:981)
	at org.apache.cassandra.schema.SchemaKeyspace.fetchTables(SchemaKeyspace.java:941)
	at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:900)
	at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspaces(SchemaKeyspace.java:1301)
	at org.apache.cassandra.schema.Schema.merge(Schema.java:608)
	at org.apache.cassandra.schema.MigrationManager.announce(MigrationManager.java:425)
	at org.apache.cassandra.schema.MigrationManager.announceNewTable(MigrationManager.java:239)
	at org.apache.cassandra.schema.MigrationManager.announceNewTable(MigrationManager.java:224)
	at org.apache.cassandra.schema.MigrationManager.announceNewTable(MigrationManager.java:204)
	at org.apache.cassandra.cql3.statements.CreateTableStatement.announceMigration(CreateTableStatement.java:88)
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.executeInternal(SchemaAlteringStatement.java:120)
	at org.apache.cassandra.cql3.CQLTester.schemaChange(CQLTester.java:814)
{code}
It turns out that my machine is configured with {{pt_BR}} locale, which uses comma instead of dot for decimal separator, so the speculative retry option parsing introduced by CASSANDRA-14293, which assumed {{en_US}} locale was not working.

To reproduce on Linux:
{code:none}
export LC_CTYPE=pt_BR.UTF-8
ant test -Dtest.name=""DeleteTest""
ant test -Dtest.name=""SpeculativeRetryParseTest""
{code}",,,,,,,,,,,,,,,,,,10/Apr/18 19:30;pauloricardomg;0001-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch;https://issues.apache.org/jira/secure/attachment/12918444/0001-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch,10/Apr/18 20:29;pauloricardomg;0002-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch;https://issues.apache.org/jira/secure/attachment/12918453/0002-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch,10/Apr/18 20:43;pauloricardomg;0003-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch;https://issues.apache.org/jira/secure/attachment/12918458/0003-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch,10/Apr/18 20:47;pauloricardomg;0004-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch;https://issues.apache.org/jira/secure/attachment/12918460/0004-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2018-04-10 19:41:39.872,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 17 18:51:47 UTC 2018,,,,,,0|i3sdwf:,9223372036854775807,4.0,,,,iamaleksey,iamaleksey,,,,,,,,,,,10/Apr/18 19:33;pauloricardomg;Attached patch that forces {{US}} locale when generating {{PercentileSpeculativeRetryPolicy}} representation. Mind reviewing [~iamaleksey] or [~mkjellman] ?,"10/Apr/18 19:41;iamaleksey;[~pauloricardomg] Sure. Do you mind going one step further and changing that {{toString()}} to
{code}
return String.format(""%sp"", new DecimalFormat(""#.####"").format(percentile));
{code}
?

Because the previous patch introduced a minor annoying regression, in that 99p for example is being serialized as {{99.00p}} (instead of {{99p}}). And check it in pt_BR locale as well as en_US?","10/Apr/18 20:31;pauloricardomg;{quote}Because the previous patch introduced a minor annoying regression, in that 99p for example is being serialized as 99.00p (instead of 99p). And check it in pt_BR locale as well as en_US?
{quote}
Even if we use a {{DateFormatter}} we still need to specify the US locale to ensure the dot decimal separator (and not comma) is used, so I changed the toString to:
{code:none}
return String.format(""%sp"", new DecimalFormat(""#.####"", new DecimalFormatSymbols(Locale.ENGLISH)).format(percentile));
{code}
I also updated the {{SpeculativeRetryParseTest}} to test round trip parsing with the Brazilian locale which uses comma as the decimal separator. Patch is attached.",10/Apr/18 20:44;pauloricardomg;Oh you guys were faster with CASSANDRA-14352. :) Attached 0003-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch rebasing on top of that.,"10/Apr/18 20:48;pauloricardomg;Attached 0004-Use-Locale.US-on-PercentileSpeculativeRetryPolicy.to.patch with typo fix, sorry for the spam.. (:",13/Apr/18 12:41;iamaleksey;+1,"13/Apr/18 12:51;burmanm;Yep, patch works fine with Finnish locale.",17/Apr/18 18:51;pauloricardomg;Committed as {{f165e72bf19e8d12457b8f569517012628513d24}} to trunk. Thanks for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
data_file_directories config - update documentation in cassandra.yaml,CASSANDRA-14372,13151197,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,n.v.harikrishna,n.v.harikrishna,n.v.harikrishna,09/Apr/18 21:22,12/Mar/19 14:06,13/Mar/19 22:35,11/Apr/18 04:23,4.0,,,,Legacy/Documentation and Website,,,,0,,,,"If ""data_file_directories"" configuration is enabled with multiple directories, data is partitioned by token range so that data gets distributed evenly. But the current documentation says that ""Cassandra will spread data evenly across them, subject to the granularity of the configured compaction strategy"". Need to update this comment to reflect the correct behavior.",,,,,,,,,,,,,,,CASSANDRA-6696,,,10/Apr/18 20:30;n.v.harikrishna;14372-trunk.txt;https://issues.apache.org/jira/secure/attachment/12918454/14372-trunk.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-04-11 04:23:18.655,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 11 04:23:18 UTC 2018,,,,,,0|i3sc87:,9223372036854775807,,,,,jjirsa,jjirsa,,,,3.2,,,,,,,09/Apr/18 21:28;n.v.harikrishna;I can submit a patch for this. Can someone assign this ticket to me?,10/Apr/18 20:30;n.v.harikrishna;Attached patch with the changes. Please review it.,"11/Apr/18 04:23;jjirsa;Thanks! Committed as 42827e6a6709c4ba031e0a137a3bab257f88b54f

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix scheduling of speculative retry threshold recalculation,CASSANDRA-14338,13147465,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,iamaleksey,iamaleksey,iamaleksey,23/Mar/18 13:07,12/Mar/19 14:06,13/Mar/19 22:35,23/Mar/18 13:59,4.0,,,,,,,,0,,,,"Realized after committing CASSANDRA-14293 that it's slightly broken: when updating from a static to a dynamic speculative retry policy, the refresher will never be scheduled.

Also realised that the whole thing is a bit dumb. We don't need a latency calculator runnable per {{ColumnFamilyStore}}, we should have one per node, that does in batch refresh thresholds for every table. It's one fewer thing to manage, and allows to get rid of {{isDynamic()}} method in {{SpeculativeRetryPolicy}}, which I thought was clever at the time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-23 13:43:00.349,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 23 13:58:59 UTC 2018,,,,,,0|i3rpav:,9223372036854775807,,,,,beobal,beobal,,,,,,,,,,,23/Mar/18 13:09;iamaleksey;4.0 branch: https://github.com/iamaleksey/cassandra/tree/14338-4.0,"23/Mar/18 13:43;beobal;+1 LGTM.

I would just wrap the calculation in {{CFS::updateSpeculationThreshold}} in a try/catch so that an exception there doesn't cause future executions of the task to be suppressed.","23/Mar/18 13:46;iamaleksey;Good call. I'd say in 3.0 this also should be fixed, so that a single exception there doesn't stop recalculating percentiles for the affected table.

Thanks for the review. Will commit once Circle is happy.","23/Mar/18 13:58;iamaleksey;Committed as [8b7e96761f968b346aed08c0c201a8d40d801b19|https://github.com/apache/cassandra/commit/8b7e96761f968b346aed08c0c201a8d40d801b19], thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
toDate() CQL function is instantiated for wrong argument type,CASSANDRA-14502,13164639,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,blerer,Sarna,Sarna,07/Jun/18 08:44,12/Mar/19 14:06,13/Mar/19 22:35,31/Jul/18 13:03,4.x,,,,Legacy/CQL,,,,0,,,,"{{toDate()}} function is instantiated to work for {{timeuuid}} and {{date}} types passed as an argument, instead of {{timeuuid}} and {{timestamp}}, as stated in this documentation: [http://cassandra.apache.org/doc/latest/cql/functions.html#datetime-functions]

As a result it's possible to convert a {{date}} into {{date}}, but not a {{timestamp}} into {{date}}, which is probably what was meant.",,,,,,,,,,,,,,,,,,30/Jul/18 16:09;blerer;CASSANDRA-14502.txt;https://issues.apache.org/jira/secure/attachment/12933609/CASSANDRA-14502.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-07-31 07:28:12.012,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 31 13:03:41 UTC 2018,,,,,,0|i3um0v:,9223372036854775807,,,,,snazy,snazy,,,,,,,,,,,31/Jul/18 07:28;snazy;+1,31/Jul/18 13:03;blerer;Committed in trunk as 76c7d02c94c735bc5e7ef6ff45d9ddc777b176dd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] test_system_auth_ks_is_alterable - auth_test.TestAuth,CASSANDRA-14600,13174874,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jasobrown,jasobrown,26/Jul/18 14:45,12/Mar/19 14:06,13/Mar/19 22:35,30/Jul/18 17:36,,,,,Test/dtest,,,,0,dtest,,,"Test 'fails' on 3.0 and 3.11 with this error from pytest:

 
{noformat}
test teardown failure

Unexpected error found in node logs (see stdout for full details). Errors: [ERROR [Native-Transport-Requests-1] 2018-07-23 18:14:34,585 Message.java:629 - Unexpected exception during request; channel = [id: 0x0ffc99f5, L:/127.0.0.3:9042 - R:/127.0.0.1:54516]
java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
	at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:518) ~[main/:na]
	at org.apache.cassandra.auth.CassandraRoleManager.canLogin(CassandraRoleManager.java:312) ~[main/:na]
	at org.apache.cassandra.service.ClientState.login(ClientState.java:281) ~[main/:na]
	at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:80) ~[main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [main/:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_154-cassandra]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_154-cassandra]
Caused by: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
	at org.apache.cassandra.db.ConsistencyLevel.assureSufficientLiveNodes(ConsistencyLevel.java:334) ~[main/:na]
	at org.apache.cassandra.service.AbstractReadExecutor.getReadExecutor(AbstractReadExecutor.java:162) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$SinglePartitionReadLifecycle.<init>(StorageProxy.java:1779) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:1741) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy.readRegular(StorageProxy.java:1684) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:1599) ~[main/:na]
	at org.apache.cassandra.db.SinglePartitionReadCommand$Group.execute(SinglePartitionReadCommand.java:1176) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:315) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:285) ~[main/:na]
	at org.apache.cassandra.auth.CassandraRoleManager.getRoleFromTable(CassandraRoleManager.java:526) ~[main/:na]
	at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:508) ~[main/:na]
	... 13 common frames omitted, ERROR [Native-Transport-Requests-3] 2018-07-23 18:14:35,759 Message.java:629 - Unexpected exception during request; channel = [id: 0x3bf15467, L:/127.0.0.3:9042 - R:/127.0.0.1:54528]
{noformat}

Not sure if we need just another log error exclude, or if this is legit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-29 02:21:36.85,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 30 17:36:44 UTC 2018,,,,,,0|i3wcv3:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"29/Jul/18 02:21;jay.zhuang;Seems the issue because the client tries to re-connect while the server is stopping. When node-1, node-2 are down, but node-3 is still up, QUORUM cannot meet. That's why it's always node-3 has the exception. Here is a patch, please review:
|[14600|https://github.com/cooldoger/cassandra-dtest/tree/14600]|

Verified locally with {{3.0 / 3.11}} and works fine:
{noformat}
pytest --count 10 -p no:flaky --cassandra-dir=~/ws/cassandra auth_test.py::TestAuth::test_system_auth_ks_is_alterable
{noformat}
","30/Jul/18 13:26;jasobrown;I verified locally, as well, and test passes. +1",30/Jul/18 17:36;jay.zhuang;Thanks [~jasobrown]. Committed as [{{17e0656}}|https://github.com/apache/cassandra-dtest/commit/17e0656a948d52cc55a40e8bf6dc139dd5fa6920].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] test_sstableofflinerelevel - offline_tools_test.TestOfflineTools,CASSANDRA-14602,13174881,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,jasobrown,jasobrown,26/Jul/18 14:59,12/Mar/19 14:06,13/Mar/19 22:35,30/Jul/18 14:18,,,,,Test/dtest,,,,0,dtest,,,"consistently failing dtest on 3.0 (no other branches). Output from pytest:

{noformat}
        output, _, rc = node1.run_sstableofflinerelevel(""keyspace1"", ""standard1"")
>       assert re.search(""L0=1"", output)
E       AssertionError: assert None
E        +  where None = <function search at 0x7f99afffbe18>('L0=1', 'New leveling: \nL0=0\nL1 10\n')
E        +    where <function search at 0x7f99afffbe18> = re.search

offline_tools_test.py:160: AssertionError
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-27 16:17:08.532,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 30 14:18:34 UTC 2018,,,,,,0|i3wcwn:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"27/Jul/18 16:17;krummas;patch to make sure we only get a single sstable in the first assert here: https://github.com/krummas/cassandra-dtest/commits/marcuse/14602 - ran 30 times locally and passed every time

test run against 3.0: https://circleci.com/gh/krummas/cassandra/tree/marcuse%2F14602",28/Jul/18 16:31;jasobrown;+1,"30/Jul/18 14:18;krummas;committed as {{c13a78c533c9e025c6ea705fc010f67cf2224d55}}, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unfiltered.isEmpty conflicts with Row extends AbstractCollection.isEmpty,CASSANDRA-14588,13174395,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,benedict,benedict,25/Jul/18 15:55,12/Mar/19 14:06,13/Mar/19 22:35,29/Nov/18 16:01,3.0.18,3.11.4,4.0,,Legacy/Local Write-Read Paths,,,,0,,,,"The isEmpty() method’s definition for a Row is incompatible with that for a Collection.  The former can return false even if there is no ColumnData for the row (i.e. the collection is of size 0).
 
This currently, by chance, doesn’t cause us any problems.  But if we ever pass a Row as a Collection to a method that invokes isEmpty() and then expects (for correctness) that the _collection_ portion is not empty, it will fail.
 
We should probably have an asCollection() method to obtain a collection from a Row, and not implement Collection directly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-27 16:37:57.556,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 29 16:01:50 UTC 2018,,,,,,0|i3w9wv:,9223372036854775807,,,,,bdeggleston,bdeggleston,,,,,,,,,,,31/Jul/18 16:25;benedict;[3.0 patch|https://github.com/belliottsmith/cassandra/tree/CASSANDRA-14588-3.0]; [Circle CI|https://circleci.com/workflow-run/345bec8d-f6eb-4654-b33b-2942752d1ba6],27/Nov/18 16:37;bdeggleston;+1,"28/Nov/18 00:29;bdeggleston; 

In my initial review I missed that AbstractCollection has a useful toString method, and it would be nice if it was duplicated it in AbstractRow to help with debugging. That can just be added on commit though.","28/Nov/18 00:51;benedict;Ah, good catch.  Thanks, I'll sneak that in before I commit tomorrow.",29/Nov/18 16:01;benedict;Committed as [8404260f1640efd14613c4591e5e918786fcde10|https://github.com/apache/cassandra/commit/8404260f1640efd14613c4591e5e918786fcde10] to 3.0 and up.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix incorrect sorting of replicas in SimpleStrategy.calculateNaturalReplicas,CASSANDRA-14862,13195548,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jolynch,jolynch,jolynch,01/Nov/18 03:49,12/Mar/19 14:06,13/Mar/19 22:35,08/Nov/18 02:47,4.0,,,,Cluster/Membership,Consistency/Coordination,,,0,4.0-QA,,,"The sorting of natural replicas in {{SimpleStrategy.calculateNaturalReplicas}} committed as part of [e645b917|https://github.com/apache/cassandra/commit/e645b9172c5d50fc2af407de724e46121edfe109#diff-0e1563a70b49cd81e9e11b4ddad15cf2L68] for CASSANDRA-14726 has broken the {{TestTopology.test_size_estimates_multidc}} dtest ([example run|https://circleci.com/gh/jolynch/cassandra/245#tests/containers/48]) as the ""primary"" ranges have now changed. I'm actually surprised only a single dtest fails as I believe we've broken multi-dc {{SimpleStrategy}} reasonably badly.

In particular the {{SimpleStrategy.calculateNaturalReplicas}} method cannot sort the endpoints by datacenter first. It has to leave them in the order that it found them else change which replicas are considered ""primary"" replicas (which mostly impacts repair and size estimates and the such).

I have written a regression unit test for the SimpleStrategy and am running it through circleci now. Will post the patch shortly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-01 09:44:32.978,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 08 02:47:07 UTC 2018,,,,,,0|s0008g:,9223372036854775807,,,,,benedict,benedict,,,,,,,,,,,"01/Nov/18 03:59;jolynch;||trunk||
|[patch|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14862]|
|dtests:[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14862.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14862]|

Patch is up, tests are running. [~benedict] do you think you can review?","01/Nov/18 09:44;benedict;Can we not break this dependency?  It seems that simply selecting the endpoint with the minimum token in ({{getPrimaryRangesForEndpoint}} and {{getPrimaryRangeForEndpointWithinDC}}) for any returned by {{calculateNaturalReplicas}} would suffice?

I'm OK with also rolling back the change, as I'm not sure I even intended to leave it in (it's a very limited impact optimisation, that might yield dividends later). 

But, I would rather we also found these dependencies and broke them.  Failing that, we need to document them.  This is an unnecessarily brittle state of affairs.","01/Nov/18 11:10;benedict;Ah, nevermind - that's not going to be easy to do.  I guess let's simply comment {{calculateNaturalEndpoints}} to specify that the output order is defined as the token order of the replicas, and that these two methods depend on this.

+1","01/Nov/18 17:36;jolynch;Ok, I just pushed an updated commit ([0d9e09fc|https://github.com/apache/cassandra/commit/0d9e09fccc0acacc1b52e8d00dd0b8bfe86c81b0]) to my branch which clears up the {{calculateNaturalReplicas}} docstring and lists the implicit dependence. dtests are re-running.

","02/Nov/18 18:52;jolynch;From Benedict on IRC:
{noformat}
> I might change the language slightly, as I don't think it's 100% clear they should be returned in the sequence they occur following the search token (like, 95% clear but you need to think about it), but the important point (that it's depended on) is plenty clear.
> I'd probably personally have written something like ""Calculate the natural endpoints for the given token. They are returned in the order they occur in the ring following the search token, as defined by the replication strategy"", but I don't think it's terribly important{noformat}

Based on this feedback I have pushed an amended commit to ([cafd44c8|https://github.com/jolynch/cassandra/commit/cafd44c8d9ae24c953a8d82746fc89bfe2465641].","08/Nov/18 02:47;iamaleksey;Committed to trunk as [2adfa92044381aa9093104f3a105f3dbd7dda94c|https://github.com/apache/cassandra/commit/2adfa92044381aa9093104f3a105f3dbd7dda94c], thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SEPExecutor does not fully shut down,CASSANDRA-14815,13190994,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,11/Oct/18 17:24,12/Mar/19 14:06,13/Mar/19 22:35,15/Jan/19 19:20,4.0,,,,Local/Startup and Shutdown,,,,0,,,,"When trying to shut down an SEP Executor, a parked worked will still be parked on:
{code}
sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:88)
io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
java.lang.Thread.run(Thread.java:748)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-30 16:54:15.4,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 15 19:20:28 UTC 2019,,,,,,0|i3z3hz:,9223372036854775807,,,,,benedict,benedict,,,,,,,,,,,11/Oct/18 17:31;ifesdjeen;|[patch|https://github.com/apache/cassandra/pull/281/files]|[tests|https://circleci.com/workflow-run/8022c393-5c08-4643-84ae-a778b2a53656]|,"30/Nov/18 16:54;benedict;I've pushed a modified approach [here|https://github.com/belliottsmith/cassandra/tree/14815]

It's a rare possible race condition, but unilaterally invoking {{set(Work.DEAD)}} would not have guaranteed that the state was set to {{DEAD}}, as there are places in the state machine where a worker may also unilaterally self-assign its state, knowing that nobody else would interfere with it.  In this case the state change would be lost.

Perhaps this wouldn't be the end of the world, but if instead we introduce a {{pool.shuttingDown}} boolean, and we check this only in the two places we might need to - namely when we cannot assign ourselves work (which will be the case if all executors have been shutdown), and when stopping ourselves (in case we have somehow been very delayed in stopping, and so the original termination did not signal us), we can also avoid introducing a new {{COWArrayList}} to manage all of the worker threads.

Oh, also, a spelling nit in the {{MAGIC}} thread name indicator.","30/Nov/18 18:18;benedict;There is a downside to the approach I've taken, which is that the threads are not guaranteed to have stopped by the time shutdown exits.  I've added a follow-up commit that reduces this window, but this is still not a guarantee.  I think this commit is probably better to exclude, but I'll await your feedback.  I have a final commit that just has the test method invoke {{join()}} on each matching thread with a short wait parameter, so that they have the opportunity to stop themselves.","04/Dec/18 12:05;ifesdjeen;[~benedict] Thank you for the review,

You're right, I haven't noticed there are other calls to {{set}} that would overwrite this state; CAS this used only during transition between executors, and all the other time worker is an exclusive owner. I did have a similar version to yours, but the way I implemented it initially (atomic boolean for stop signal and waiting twice on descheduled and once on spinning queues to make sure all tasks are drained), but thought we can do better. I like your version, however as it improves in terms of how we signal shutdown without having to spin.

I've pushed two minor suggestions: one is to take break out of the nested if and second - remove (seemingly) redundant call to {{terminateWorkers}}, let me know what you think.","04/Dec/18 12:22;benedict;Yes, these changes are good.  +1. 

But: we should update the comments in {{terminateWorkers}}, and we should add a comment to {{STOP_SIGNALLED}} that it may ONLY be assigned in {{maybeStop}} as we now depend on this for correctness of termination of the threads.  We should perhaps also put a comment under {{doWaitSpin}} that we *must* continue here to re-check {{isShuttingDown}}.  Neither of these are strict requirements of the design, so whilst they shouldn't change, we should document the dependencies.","04/Dec/18 13:32;ifesdjeen;I've pushed two more small changes: modify two of three suggested comments and revert wait-related code as unless we can wait for threads to _really_ stop, it seems to add not enough to justify the complexity. Let me know what you think.","04/Dec/18 17:39;benedict;[here|https://github.com/ifesdjeen/cassandra/blob/cd7235d004d5ec99cb937fe213dd99b044782083/src/java/org/apache/cassandra/concurrent/SharedExecutorPool.java#L134] I meant for ""enter the SPINNING state,"" to become something like ""are runnable"", because we no longer require they enter the SPINNING state to check the condition (although we do enter the spinning state, it is no longer a requirement).

[here|https://github.com/ifesdjeen/cassandra/commit/cd7235d004d5ec99cb937fe213dd99b044782083#diff-d9febddfa9880b152f1b86242708b862R83] something slightly more descriptive about why would be useful, such as ""if the pool is terminating, but we have been assigned STOP_SIGNALLED, if we do not continue to re-check pool.shuttingDown this thread will block forever""

Otherwise, +1 LGTM","05/Dec/18 14:44;ifesdjeen;Thank you for the review, committed as [eea68a2cfeb0134510deaaa5540afdf6d0c6ee7e|https://github.com/apache/cassandra/commit/eea68a2cfeb0134510deaaa5540afdf6d0c6ee7e] to trunk.",05/Dec/18 15:03;jjordan;I assume fixver here was 4.0?  If not please correct it.,"05/Dec/18 15:03;benedict;Actually, it would be great to backport this to 3.0, so that we can utilise it for mixed-version in-jvm dtests once they are backported.",05/Dec/18 15:12;ifesdjeen;Good point; I will reopen for back port.,15/Jan/19 19:20;benedict;The backport work for this is being tracked/handled in CASSANDRA-14931.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In JVM dtests need to clean up after instance shutdown,CASSANDRA-14922,13202340,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jolynch,jolynch,jolynch,04/Dec/18 18:29,12/Mar/19 14:06,13/Mar/19 22:35,09/Jan/19 14:07,4.0,,,,Test/dtest,,,,0,,,,"Currently the unit tests are failing on circleci ([example one|https://circleci.com/gh/jolynch/cassandra/300#tests/containers/1], [example two|https://circleci.com/gh/rustyrazorblade/cassandra/44#tests/containers/1]) because we use a small container (medium) for unit tests by default and the in JVM dtests are leaking a few hundred megabytes of memory per test right now. This is not a big deal because the dtest runs with the larger containers continue to function fine as well as local testing as the number of in JVM dtests is not yet high enough to cause a problem with more than 2GB of available heap. However we should fix the memory leak so that going forwards we can add more in JVM dtests without worry.

I've been working with [~ifesdjeen] to debug, and the issue appears to be unreleased Table/Keyspace metrics (screenshot showing the leak attached). I believe that we have a few potential issues that are leading to the leaks:

1. The [{{Instance::shutdown}}|https://github.com/apache/cassandra/blob/f22fec927de7ac291266660c2f34de5b8cc1c695/test/distributed/org/apache/cassandra/distributed/Instance.java#L328-L354] method is not successfully cleaning up all the metrics created by the {{CassandraMetricsRegistry}}
 2. The [{{TestCluster::close}}|https://github.com/apache/cassandra/blob/f22fec927de7ac291266660c2f34de5b8cc1c695/test/distributed/org/apache/cassandra/distributed/TestCluster.java#L283] method is not waiting for all the instances to finish shutting down and cleaning up before continuing on
3. I'm not sure if this is an issue assuming we clear all metrics, but [{{TableMetrics::release}}|https://github.com/apache/cassandra/blob/4ae229f5cd270c2b43475b3f752a7b228de260ea/src/java/org/apache/cassandra/metrics/TableMetrics.java#L951] does not release all the metric references (which could leak them)

I am working on a patch which shuts down everything and assures that we do not leak memory.",,,,,,,,,,,,,CASSANDRA-14946,,CASSANDRA-14969,,,08/Dec/18 02:35;jolynch;AllThreadsStopped.png;https://issues.apache.org/jira/secure/attachment/12951080/AllThreadsStopped.png,08/Dec/18 02:35;jolynch;ClassLoadersRetaining.png;https://issues.apache.org/jira/secure/attachment/12951079/ClassLoadersRetaining.png,23/Jan/19 22:37;jolynch;LeakedNativeMemory.png;https://issues.apache.org/jira/secure/attachment/12956048/LeakedNativeMemory.png,04/Dec/18 18:28;jolynch;Leaking_Metrics_On_Shutdown.png;https://issues.apache.org/jira/secure/attachment/12950585/Leaking_Metrics_On_Shutdown.png,12/Dec/18 00:11;jolynch;MainClassRetaining.png;https://issues.apache.org/jira/secure/attachment/12951436/MainClassRetaining.png,08/Jan/19 21:42;jolynch;MemoryReclaimedFix.png;https://issues.apache.org/jira/secure/attachment/12954229/MemoryReclaimedFix.png,13/Dec/18 03:46;jolynch;Metaspace_Actually_Collected.png;https://issues.apache.org/jira/secure/attachment/12951598/Metaspace_Actually_Collected.png,08/Dec/18 02:35;jolynch;OnlyThreeRootsLeft.png;https://issues.apache.org/jira/secure/attachment/12951078/OnlyThreeRootsLeft.png,30/Jan/19 14:49;ifesdjeen;Screen Shot 2019-01-30 at 15.46.35.png;https://issues.apache.org/jira/secure/attachment/12956909/Screen+Shot+2019-01-30+at+15.46.35.png,30/Jan/19 14:48;ifesdjeen;Screen Shot 2019-01-30 at 15.47.13.png;https://issues.apache.org/jira/secure/attachment/12956908/Screen+Shot+2019-01-30+at+15.47.13.png,12/Dec/18 06:41;jolynch;no_more_references.png;https://issues.apache.org/jira/secure/attachment/12951468/no_more_references.png,11.0,,,,,,,,,,,,,,,,,,,2018-12-04 19:41:13.009,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 04 13:31:43 UTC 2019,,,,,,0|s015w8:,9223372036854775807,4.0,,,,ifesdjeen,ifesdjeen,,,,,,,,,,,"04/Dec/18 19:41;ifesdjeen;[~jolynch] thank you for reporting it and investing time to fix it! Great investigation, looking forward to see the full patch!

{{close}} is not waiting for full instance shutdown (same as instance shutdown does not wait for executor shutdown) wasn't an oversight. It makes shutdown significantly faster and allows shutting down a cluster much quicker. 

I do agree we should fix leaks and make sure things shutdown properly and we definitely have to do what we can to make it happen. But I also think we should keep it performant. If there's no way and clusters churn quicker than we can shutdown them, we can make a switch. 

Another thing we might want to consider is reusing cluster instance for multiple tests as I think this will most frequently be the case. We can make it in a follow-up ticket, since it is, however helpful, also orthogonal. ","04/Dec/18 23:23;jolynch;Definitely have to keep the shutdown fast, I think we just have a deadlock or something like that in the cleanups where we're not actually cleaning up. I tried adding in {{Keyspace::clear}} calls in the shutdown hook but that deadlocked on the single STPE in {{nonPeriodicTasks}}, still trying to figure out why we can't drop all the CFs and release all the metrics but making progress :)","08/Dec/18 02:35;jolynch;Alright, I think I'm narrowing in on this. I've managed to get all the threads to die over in [a branch|https://github.com/jolynch/cassandra/tree/CASSANDRA-14922] but we're still leaking all the static state through the {{InstanceClassLoader}} s. I think I've narrowed it down to just three remaining references (and I _think_ only one of them is a strong reference), details attached in the screenshots.

We basically just need to kill that last strong reference and I believe that the whole {{InstanceClassLoader}} should become collectible at that point (even with all the static state and self references to the classloaders should be ok since it'll be cut off at the root, I think).","12/Dec/18 00:11;jolynch;Nailed the MessageSink leak and properly shuts down logback now, and we clean up ThreadLocal variables which afaict means that there are no additional threads that could hold references ... and at least now [my branch|https://github.com/jolynch/cassandra/tree/CASSANDRA-14922] can pass unit tests in [circleci|https://circleci.com/workflow-run/6fb24842-bbb8-4aac-b137-4007729bf39a] so that's good.

Unfortunately I think we're still leaking, according to my heap dump analysis the _main_ thread ends up with a strong reference to the {{InstanceClassLoaders}}, which is really odd since the InstanceClassLoader doesn't have any static state so I don't see how those don't go out of scope when the test methods finish.

[~ifesdjeen] tbh at this point I'm pretty stuck. My understanding is that the {{InstanceClassLoaders}} should go out of scope after each test method, which should allow everything to GC now that there are no more live references from threads to the {{InstanceClassLoaders}}, but that's not happening. I think it might be related to passing in the current threads classloader [here|https://github.com/jolynch/cassandra/blob/1ca6ad3c41f4456b674e13883e0df0091f638564/test/distributed/org/apache/cassandra/distributed/TestCluster.java#L241] but I'm not sure how to achieve what we need there without that.","12/Dec/18 06:40;jolynch;I think I figured it out, it turns out that our version of {{jna.Native}} holds a static map called [{{options}}|https://github.com/java-native-access/jna/blob/4bcc6191c5467361b5c1f12fb5797354cc3aa897/src/com/sun/jna/Native.java#L104] which holds noncollectable references to the [{{InstanceClassLoader}}|https://github.com/java-native-access/jna/blob/4bcc6191c5467361b5c1f12fb5797354cc3aa897/src/com/sun/jna/Native.java#L1528]; I believe this leak is what the last two references were talking about. I tried cleanly unloading or unregistering those but that still doesn't remove the reference to the ClassLoader in the {{options}} map so I just hacked some reflection based solution together (similar to what I did for the {{ThreadLocal}} variables) in  [6573176|https://github.com/jolynch/cassandra/commit/6573176ef3ed9601ce7f02602c964d478c6a5741]. According to Yourkit there are now no more strong references to the {{InstanceClassLoader}} instances (attached).

 !no_more_references.png! 

We leak a lot less memory with my latest patches, but ... for some reason the class loaders aren't going away and are still retaining some heap, just a lot less ... There is still something missing, maybe something like {{CMSClassUnloadingEnabled}} or some such?","12/Dec/18 13:34;benedict;[~jolynch]: nice work, somehow I hadn't spotted this thread until now.

Unfortunately I don't have anything useful to add, except a shot-in-the-dark that maybe a full GC is needed?  This is apparently the case for G1GC; not sure about other GCs, or which you are using.  It's probably not helpful, but you could also look at {{-XX:+TraceClassUnloading}}

Mostly just wanted to say thanks for taking the time to track all of this down.","13/Dec/18 03:42;jolynch;I found a better workaround (in [bfd8c328|https://github.com/apache/cassandra/commit/bfd8c328f92e6bd2c82269a048f6b868179f484a]) for the jna {{Native}} Classloader leak is to just call the [register|https://github.com/java-native-access/jna/blob/365f6b343a92427e7890cae0c16df7b6c4c254d4/src/com/sun/jna/Native.java#L1446] API that doesn't add the {{InstanceClassLoader}} to a global static map in {{Native}}. This still loads the libraries and doesn't leak the ClassLoader into a global static map. If we're concerned about the changes to the {{NativeLibrary}} I can pursue other option. I also found another thread that leaks only during ant test runs which is fixed in [512d15b5f|https://github.com/apache/cassandra/commit/512d15b5f08bda1b0dc8a952ce950b5c4341f992].

Furthermore I believe I know why the JVM was still not releasing the classloaders even after they are no longer referenced: [SoftReferences|https://docs.oracle.com/javase/8/docs/api/java/lang/ref/SoftReference.html] are apparently just ... not collected. It looks like Metaspace just grows without bound until we run the machine OOM (not the JVM, the machine).

I tried the following after eliminating all strong references:
 1. No JVM tuning. This _did not_ work as the metaspace would grow without bound and eventually OOM the machine (not the JVM, the machine).
 2. {{-XX:SoftRefLRUPolicyMSPerMB=0}} and or {{-XX:MaxMetaspaceSize=256M}}. This *worked* and caused the ClassLoaders to be released. With MaxMetaspaceSize we also successfully limit the offheap Metaspace which is nice.
 3. {{-XX:MaxMetaspaceSize=256M}} and or {{-XX:MetaspaceSize=100M}}. This _did not_ work. Setting the Max size would cause an {{java.lang.OutOfMemoryError: Metaspace}}, setting just the size didn't do anything (machine would just OOM itself anyways)
 4. {{-XX:UseConcMarkSweepGC}} and or {{-XX:CMSClassUnloadingEnabled}}. This _did not_ work. Again the metaspace just grew without bound and ran out of memory.

I think that running all unit tests under #2 is a pretty reasonable thing to do (we shouldn't be leaking Metaspace imo). I've pursued that option in [86f982b1|https://github.com/apache/cassandra/commit/86f982b19ba23fc5efcd7421449af4a84d93711b] and it appears to work looking at my [profiler|https://issues.apache.org/jira/secure/attachment/12951598/Metaspace_Actually_Collected.png].

[~ifesdjeen]/[~benedict] Do you guys think it's acceptable to run all unit tests with the off-heap Metaspace limited to 256 megabytes and {{SoftRefLRUPolicyMSPerMB}} set to 0 to tell the GC to actually collect soft references? It's different than the status quo but I think we probably shouldn't leak metaspace (aka permgen from java7/6) and we probably shouldn't rely on soft references for things to work generally... I checked and both options are available in openjdk8+, but I'm not sure if there is a better way.

Other than the ThreadLocal reflection hacks I think the branch is almost ready to squash and merge, what do you think?","13/Dec/18 12:35;benedict;Do you know where the soft references originate?  I wonder if there's anything we can do to simply eliminate them.

Short of that, I'm comfortable setting the SoftRefLRUPolicyMSPerMB - the default seems pretty strange (of 1s per MB), and probably does not seem to play well with Metaspace, since this probably doesn't contribute to the input to the SoftRefLRUPolicyMSPerMB.  So we could have Gigabytes of free space, and judge that we need the referent to have been unused for hours before we collect it.

It's been a while since I sperlunked in the JDK source, but it might be worth taking a look to find out exactly what number if provides, but honestly I don't really see a problem with setting the value to zero.  The worst impact of soft references being freed too-eagrerly is performance, it should never be functional.  We don't depend on them directly in C*, so it would only be via the JDK.  Worst case scenario, we need to investigate again at some later date an issue with the tests.

Nice catch on the Native API also.

Overall this looks like excellent work (Though I'll leave a full review to Alex, since he's marked himself reviewer)","08/Jan/19 12:36;ifesdjeen;The patch looks good, and I'd say [~jolynch] let's merge it, since tests have been failing for a while now, unless there's something else you wanted to include in the patch immediately. 

I've had a couple of minor suggestions. All of the issues are easier to see / reproducible with a very small heap, ~256Mb: 
  * Hints are leaking direct memory 
  * Threadlocals are leaked 
  * FastThreadLocalThread thread locals are leaked (sorry for a tongue-twister)

I've put together a small [demo|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-14922] just for demonstration purposes if you wanted to see the impact of suggested changes.","08/Jan/19 21:42;jolynch;{quote}The patch looks good, and I'd say [~jolynch] let's merge it,
{quote}
Ok, yea I agree let's merge what we have so that the unit tests can pass on trunk again. I've put up a patch against trunk with what we have so far (including your changes from the demo branch which as far as I can tell remove the need for the ThreadLocal clearing).

||trunk||
|[024e6943|https://github.com/apache/cassandra/commit/024e69436e89bb79cdbf4e136a1f6d9c2747275d]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14922.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14922]|

If I attach a profiler during an intellij ""run this test until it fails"" mode I can see that the memory is indeed getting cleaned up:

!MemoryReclaimedFix.png!","08/Jan/19 21:49;benedict;Marking 'Ready to Commit' given [~ifesdjeen]'s comments.  I'll give it another quick once over then commit, so I can rebase CASSANDRA-14931 and CASSANDRA-14937.","08/Jan/19 22:54;jolynch;[~benedict],

Awesome, can we wait for Alex to see the latest diff though with the reflection removed in favor of his proposed fast local thread pool cleanup method? I've changed the patch a bit since he last looked.

Regarding the backport, I am slightly concerned about the NativeLibrary changes being backported in their current form. From my reading of the JNA source code in version 4.2.2 in trunk we're just skipping the cache by using [NativeLibrary::getInstance|https://github.com/java-native-access/jna/blob/4bcc6191c5467361b5c1f12fb5797354cc3aa897/src/com/sun/jna/NativeLibrary.java#L341] directly and passing it to [Native::register(NativeLibrary)|https://github.com/java-native-access/jna/blob/4bcc6191c5467361b5c1f12fb5797354cc3aa897/src/com/sun/jna/Native.java#L1260] instead of having [Native::register(String)|https://github.com/java-native-access/jna/blob/4bcc6191c5467361b5c1f12fb5797354cc3aa897/src/com/sun/jna/Native.java#L1251] do that for us and cache the classloader along the way [here|https://github.com/java-native-access/jna/blob/4bcc6191c5467361b5c1f12fb5797354cc3aa897/src/com/sun/jna/NativeLibrary.java#L363]. But, if I'm wrong it's unlikely we'd know, as while our tests cover Linux pretty thoroughly, darwin/windows are less covered.

Also I forgot to respond to your question about SoftReferences here, did it on IRC but not here.
{quote}Do you know where the soft references originate? I wonder if there's anything we can do to simply eliminate them.
{quote}
I think the Soft references are coming from {{java.io.ObjectStreamClass$Caches.localDescs}}, but the object serder we're doing in {{InvokableInstance}} is a bit beyond my JVM skills I'm afraid. I don't know how we can prevent the object serializations from caching the class descriptions... Perhaps the JVM option is sufficient for now and if we don't like that going forward we can dive in more?","09/Jan/19 01:11;benedict;bq.  can we wait for Alex to see the latest diff though... I've changed the patch a bit since he last looked.

Sure thing.  I'll start the rebase tomorrow in that case.  In that case, also, I've pushed my one nit from a quick look through [here|https://github.com/belliottsmith/cassandra/tree/14922] for Alex to look at, that I would have simply ninja'd in (with comment here, of course).  This is just using the {{HintsBuffer.free}} method instead of directly invoking {{DirectByteBuffer.cleaner().clean()}}.

bq. Regarding the backport, I am slightly concerned about the NativeLibrary changes being backported in their current form.

Thanks for highlighting this.  I'll be sure to take a close look at the behaviour on each version we backport to.  I expect there will be other places that need similar treatment to what you've done here, as well, so I need to double check anyway.

bq. I think the Soft references are coming from java.io.ObjectStreamClass$Caches.localDescs, but the object serder we're doing in InvokableInstance is a bit beyond my JVM skills I'm afraid.

No worries at all, thanks very much for reproducing this information here for posterity.  If we ever want to clean this up, it would probably be easiest to simply avoid ser/deser entirely (or use custom ser/deser), but your approach is a much more suitable compromise for now.  Thanks again also for all the investigative work to plug these gaps.","09/Jan/19 01:30;jolynch;{quote}
Sure thing. I'll start the rebase tomorrow in that case. In that case, also, I've pushed my one nit from a quick look through here for Alex to look at, that I would have simply ninja'd in (with comment here, of course). This is just using the HintsBuffer.free method instead of directly invoking DirectByteBuffer.cleaner().clean().
{quote}
Ah cool, yea that appears to still work (and then we can leave the slab private in {{HintsBuffer}} as well.)","09/Jan/19 14:06;ifesdjeen;Committed to trunk with [d5005627b02b4e716947fa05a40473368017c0f9|https://github.com/apache/cassandra/commit/d5005627b02b4e716947fa05a40473368017c0f9].

[CI run|https://circleci.com/workflow-run/eed85c4b-3a55-46bb-bad8-93c4c1e820f9]

[~jolynch] thank you for the investigation & patch. 
[~benedict] thank you for input as well!","09/Jan/19 19:44;benedict;In back porting this to 3.0, I wondered if it was worth opening a brief bit of discussion around alternative approaches to the {{ThreadLocal}} issue, for which the Netty approach does not anyway work in 3.0 (since we use regular {{ThreadLocal}}).

The fix introduced here assumes that we only access any cluster behaviour from the {{main}} thread.  An alternative approach would be to isolate all tests to a thread owned by the {{TestCluster}} (which we already have available to us, and we shutdown on {{close}}).

If this were the standard pattern for implementing any tests, we should not have an issue with the {{main}} thread retaining any references inside its {{ThreadLocal}}, but we also will create a pattern that extends to any tests written to utilise multi-threading, and hence not run against the {{main}} thread.

I've implemented this in my backport branch, and it is quite straight forward, however I am still chasing down other 3.0-era leaks (right now around our custom log4j integrations)","09/Jan/19 19:46;benedict;On a related topic, it might be nice to eventually migrate to assigning a {{ThreadGroup}} to all our threads, so we can reliably manage them en masse.","11/Jan/19 16:21;benedict;I've pushed a branch [here|https://github.com/belliottsmith/cassandra/tree/14922-followup] that removes the cleanup of thread locals, and removes the cluster-wide executor, instead introducing a node-specific executor on which all invocations to that node happen.  This might introduce some slight penalty for evaluating tests, as we have multiple synchronisation points where control-flow is handed between threads, but it guarantees that all state remains isolated without ever burdening the user of the API to restrict their own program design.

We can elaborate on this later to make it ergonomic to use the executor, or perhaps to skip the executor, in order to provide efficiency where it matters.

I think it makes sense to classify this as part of the work for this ticket, and I will then backport it alongside the original patch for this ticket and the jvm-dtests, however it's primarily necessary for _future_ reasons:
 * 3.0 and earlier use {{ThreadLocal}}, not {{FastThreadLocal}}, so we need the earlier hackier reflection to fix on these
 * The current approach only cleans up the main thread - any other threads may themselves retain state indefinitely
 * Most importantly, when we begin stopping and starting nodes, and/or upgrading them, the cluster-wide executor service will retain thread local state from the stopped nodes, potentially indefinitely, probably leading to a steady leak of memory.

It seems easiest to introduce the future-proof approach now and port it to all the versions at once.

On a related note, there is CASSANDRA-14974, which simultaneously fixes a bug in our stdout capture, _and_ the impact this has to the cleanup of a {{TestCluster}} that occurs once this bug is fixed.  If somebody watching this ticket fancies reviewing that ticket, it would also be appreciated.","14/Jan/19 10:11;ifesdjeen;[~benedict] thank you for the patch. It does look like an improvement. 

I think that using node-local executor will work better for scenarios different than ""fire up cluster, shut down the cluster"". 

Just to confirm my understanding is correct: we can remove thread-local works in this case since we're passing the right class loader, making references unreachable. Is that right? I did test it and tests do confirm that passing ""root"" class loader would cause heap problems again, but maybe you have some more insight on that.

+1 to commit it in the scope of this ticket as a follow-up. However, there are some test failures in distributed tests, seemingly caused by different exception wrapping in the new version, we should fix them before committing.","14/Jan/19 13:15;benedict;Thanks for the review.  I'll push an update shortly, with the test failures handled, and a slight tweak to not misleadingly return a SerializableX when it is wrapped with an executor (since this cannot be serialized).

bq. we can remove thread-local works in this case since we're passing the right class loader, making references unreachable. Is that right?

By 'passing' do you mean to the thread factory?  In which case, no, that shouldn't have an impact (I pass it only because it seems to make sense, it should work still without doing so, and seems to if I try).  All we're really doing is ensuring that any thread that evaluates anything inside one of the classes loaded by the instance's classloader is shutdown when the node is shutdown, by passing the work to a thread on this executor.","14/Jan/19 16:07;benedict;I've pushed an update with the suggested changes, and also reintroduced some sequencing to the executor shutdowns.","15/Jan/19 13:40;ifesdjeen;+1 for the latest version, huge improvement!","15/Jan/19 19:13;benedict;Thanks, committed follow-up as [00fff3ee6e6c0142529de621bcaeee5790a0c235|https://github.com/apache/cassandra/commit/00fff3ee6e6c0142529de621bcaeee5790a0c235]","23/Jan/19 22:38;jolynch;I'm getting trunk failures again as of e871903d, and after an [IRC discussion|https://wilderness.apache.org/channels/?f=cassandra-dev/2019-01-23] with Benedict it looks like we may be leaking:
 1. Off heap memory via some combination of the {{HintsBuffer}}, {{CommitLogs}} and the {{BufferPool}}
 2. File descriptors are potentially leaked and it's unclear that we clean those up

What is odd is that according to a profiler attached while running one of the dtests in a for loop, most of the leaked native memory is either pending finalization or unreachable from GC roots:

!LeakedNativeMemory.png!

Afaict both {{HintsBuffer}} and {{CommitLogs}} should be getting cleaned in the {{Instance::shutdown}} methods, although I don't think we clean the {{BufferPool}}.

Continuing to investigate this so that we can have green runs on trunk again.

*Edit:*

The test I'm running is applying this diff:
{noformat}
--- a/test/distributed/org/apache/cassandra/distributed/DistributedReadWritePathTest.java
+++ b/test/distributed/org/apache/cassandra/distributed/DistributedReadWritePathTest.java
@@ -27,6 +27,15 @@ import static org.apache.cassandra.net.MessagingService.Verb.READ_REPAIR;
 
 public class DistributedReadWritePathTest extends DistributedTestBase
 {
+    @Test
+    public void manyCoordinatedReads() throws Throwable
+    {
+        for (int i = 0; i < 20; i ++)
+        {
+            coordinatorRead();
+        }
+    }
+
     @Test
     public void coordinatorRead() throws Throwable
     {
{noformat}
Then I run the test suite and measure OS memory usage, for example if I re-wind trunk to the first patch (3dcde082) I see only 1.6GB allocated:
{noformat}
/usr/bin/time -f ""mem=%K RSS=%M elapsed=%E cpu.sys=%S .user=%U"" ant testclasslist -Dtest.classlistfile=/tmp/java_dtests_1_final.txt -Dtest.classlistprefix=distributed

testclasslist:
     [echo] Number of test runners: 1
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 103.473 sec
[junit-timeout] 
[junitreport] Processing /home/josephl/pg/cassandra/build/test/TESTS-TestSuites.xml to /tmp/null554485593
[junitreport] Loading stylesheet jar:file:/usr/share/ant/lib/ant-junit.jar!/org/apache/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl
[junitreport] Transform time: 269ms
[junitreport] Deleting: /tmp/null554485593

BUILD SUCCESSFUL
Total time: 1 minute 47 seconds
mem=0 RSS=1606332 elapsed=1:47.37 cpu.sys=10.95 .user=159.99
{noformat}
But, if I use latest trunk (e871903d), I get 5GB:
{noformat}
testclasslist:
     [echo] Number of test runners: 1
    [mkdir] Created dir: /home/josephl/pg/cassandra/build/test/cassandra
    [mkdir] Created dir: /home/josephl/pg/cassandra/build/test/output
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 103.098 sec
[junit-timeout] 
[junitreport] Processing /home/josephl/pg/cassandra/build/test/TESTS-TestSuites.xml to /tmp/null126756458
[junitreport] Loading stylesheet jar:file:/usr/share/ant/lib/ant-junit.jar!/org/apache/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl
[junitreport] Transform time: 330ms
[junitreport] Deleting: /tmp/null126756458

BUILD SUCCESSFUL
Total time: 2 minutes 15 seconds
mem=0 RSS=4962924 elapsed=2:15.93 cpu.sys=16.55 .user=284.28
{noformat}
Since the heap is 1GB, and we allocate about 256MB for the off-heap metaspace, 1.6GB is much closer to what we expect than 5GB.

So ,.. something about the new executor system may be contributing. Continuing to dig in.",24/Jan/19 11:19;ifesdjeen;[~jolynch] to be honest I do not see OOMs so far. I'm running up to 100 3-node cluster instances and it works fine. Which JDK are you using? Could that be a difference?,"24/Jan/19 18:15;jolynch;[~ifesdjeen] they are no longer JVM level OOMs; I believe from the circleci ssh dmesg output this is a OS oomkill:
{noformat}
[32619.267150] Task in /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40/390b809d1f78090fda35dcf8b56612302f71bee5632fd911b2ef94558023610b killed as a result of limit of /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40
[32619.267155] memory: usage 4194304kB, limit 4194304kB, failcnt 28170
[32619.267156] memory+swap: usage 0kB, limit 9007199254740988kB, failcnt 0
[32619.267157] kmem: usage 0kB, limit 9007199254740988kB, failcnt 0
[32619.267158] Memory cgroup stats for /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40: cache:16KB rss:9444KB rss_huge:0KB mapped_file:0KB dirty:0KB writeback:0KB inactive_anon:8KB active_anon:9452KB inactive_file:0KB active_file:0KB unevictable:0KB
[32619.267168] Memory cgroup stats for /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40/390b809d1f78090fda35dcf8b56612302f71bee5632fd911b2ef94558023610b: cache:64956KB rss:4119888KB rss_huge:0KB mapped_file:17712KB dirty:32988KB writeback:0KB inactive_anon:60KB active_anon:4120272KB inactive_file:32116KB active_file:31896KB unevictable:0KB
[32619.267176] [ pid ]   uid  tgid total_vm      rss nr_ptes nr_pmds swapents oom_score_adj name
[32619.267245] [30017]     0 30017    31256     5620      22       4        0         -1000 circleci
[32619.267248] [31296] 166536 31296     1157      198       8       3        0           200 sh
[32619.267251] [31416] 166536 31416    31064     5075      22       4        0           200 circleci-agent
[32619.267253] [33206] 166536 33206     5012      745      15       3        0           200 bash
[32619.267351] [32251] 166536 32251  5984655   127267     432      11        0           200 java
[32619.267353] [32391] 166536 32391  8094234   904842    6190      34        0           200 java
[32619.267355] Memory cgroup out of memory: Kill process 32391 (java) score 1068 or sacrifice child
[32619.271649] Killed process 32391 (java) total-vm:32376936kB, anon-rss:3603092kB, file-rss:16276kB

# Try it manually
cassandra@390b809d1f78:/tmp/cassandra$ /usr/bin/time -f ""mem=%K RSS=%M elapsed=%E cpu.sys=%S .user=%U"" ant testclasslist -Dtest.classlistfile=/tmp/java_dtests_${CIRCLE_NODE_INDEX}_final.txt -Dtest.classlistprefix=distributed
...
testclasslist:
     [echo] Number of test runners: 1
[junit-timeout] Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest
[junit-timeout] Testsuite: org.apache.cassandra.distributed.DistributedReadWritePathTest Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
[junit-timeout] 
[junit-timeout] Testcase: org.apache.cassandra.distributed.DistributedReadWritePathTest:readRepairTest: Caused an ERROR
[junit-timeout] Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
[junit-timeout] junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
[junit-timeout]         at java.util.Vector.forEach(Vector.java:1275)
[junit-timeout]         at java.util.Vector.forEach(Vector.java:1275)
[junit-timeout]         at java.lang.Thread.run(Thread.java:748)
[junit-timeout] 
[junit-timeout] 
[junit-timeout] Test org.apache.cassandra.distributed.DistributedReadWritePathTest FAILED (crashed)
[junitreport] Processing /tmp/cassandra/build/test/TESTS-TestSuites.xml to /tmp/null1789186966
[junitreport] Loading stylesheet jar:file:/usr/share/ant/lib/ant-junit.jar!/org/apache/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl
[junitreport] Transform time: 2223ms
[junitreport] Deleting: /tmp/null1789186966

BUILD FAILED
/tmp/cassandra/build.xml:1861: The following error occurred while executing this line:
/tmp/cassandra/build.xml:1751: Some test(s) failed.

Total time: 25 seconds
Command exited with non-zero status 1
mem=0 RSS=3667504 elapsed=0:26.40 cpu.sys=17.58 .user=52.25

# This was another oomkill
[33152.855977] Task in /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40/390b809d1f78090fda35dcf8b56612302f71bee5632fd911b2ef94558023610b killed as a result of limit of /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40 
[33152.855982] memory: usage 4194304kB, limit 4194304kB, failcnt 50983 
[33152.855984] memory+swap: usage 0kB, limit 9007199254740988kB, failcnt 0 
[33152.855985] kmem: usage 0kB, limit 9007199254740988kB, failcnt 0 
[33152.855986] Memory cgroup stats for /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40: cache:16KB rss:9144KB rss_huge:0KB mapped_file:0KB dirty:0KB writeback:0KB inactive_anon:8KB active_anon:9152KB inactive_file:0KB active_file:0KB unevictable:0KB 
[33152.856001] Memory cgroup stats for /docker/db2d2aa36ea5a82fa51c91a99d32de688d45610c74f496288015ad82845ebf40/390b809d1f78090fda35dcf8b56612302f71bee5632fd911b2ef94558023610b: cache:2332KB rss:4182812KB rss_huge:0KB mapped_file:500KB dirty:944KB writeback:0KB inactive_anon
:52KB active_anon:4183236KB inactive_file:808KB active_file:632KB unevictable:0KB 
[33152.856012] [ pid ]   uid  tgid total_vm      rss nr_ptes nr_pmds swapents oom_score_adj name 
[33152.856086] [30017]     0 30017    31256     5515      23       4        0         -1000 circleci 
[33152.856090] [31296] 166536 31296     1157       16       8       3        0           200 sh 
[33152.856093] [31416] 166536 31416    31640     5323      24       4        0           200 circleci-agent 
[33152.856132] [  708] 166536   708     5070      143      15       3        0           200 bash 
[33152.856251] [28288] 166536 28288     1096       16       7       3        0           200 time 
[33152.856254] [28289] 166536 28289  5984655   125056     437      11        0           200 java 
[33152.856262] [28645] 166536 28645  8143453   914706    6291      35        0           200 java 
[33152.856310] Memory cgroup out of memory: Kill process 28645 (java) score 1078 or sacrifice child 
[33152.860694] Killed process 28645 (java) total-vm:32573812kB, anon-rss:3658488kB, file-rss:336kB{noformat}
For example on the latest trunk I see three test fails, the in-jvm dtests and two PagingTests: [7d138e20|https://circleci.com/gh/jolynch/cassandra/413#tests]. I'm also getting these failures in other bug fix branches e.g. [CASSANDRA-14096-trunk|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14096-trunk].

My java version running locally is 8u191:
{noformat}
$ java -version
java version ""1.8.0_191""
Java(TM) SE Runtime Environment (build 1.8.0_191-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)
{noformat}
 ","24/Jan/19 22:44;benedict;FWIW, I don't suspect anything.  I have had terminations due to file handle exhaustion locally, but I absolutely believe that we are likely to have some native memory leaks too.","04/Feb/19 13:31;ifesdjeen;I've investigated this one a bit further, but to be honest could not find any proof of leaks. I've also tried to make shutdown process more synchronous to reduce amount of in-flight memory, but this hasn't helped. I did also check the possible native memory leak. You are right there native byte buffer instances hanging after instance shutdown, but all of them as far as I can tell were pending finalization (see !Screen Shot 2019-01-30 at 15.47.13.png!).

I've tried running tests in non-constrained environment in loop [here|https://circleci.com/gh/ifesdjeen/cassandra/1197], and it seems to be passing fine after 10x100 runs. 

I made a small follow-up patch (however, these improvements are minor and do not have any impact on the end result) [here|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:oom-improvements]. In this patch I also disable in-jvm dtests for resource constrained environment.

If this looks ok, I suggest we do that and merge [~benedict]'s multi-version patch and continue writing tests.",,,,,,,,,,,,,,,,,,,,
Netty IOExceptions caused by unclean client disconnects being logged at INFO instead of TRACE,CASSANDRA-14909,13200137,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,sumanth.pasupuleti,sumanth.pasupuleti,sumanth.pasupuleti,22/Nov/18 17:37,12/Mar/19 14:06,13/Mar/19 22:35,29/Nov/18 18:55,3.0.x,3.11.x,4.0,,Legacy/Streaming and Messaging,,,,1,,,,"Observed spam logs on 3.0.17 cluster with redundant Netty IOExceptions caused due to client-side disconnections.

{code:java}
INFO  [epollEventLoopGroup-2-28] 2018-11-20 23:23:04,386 Message.java:619 - Unexpected exception during request; channel = [id: 0x12995bc1, L:/xxx.xx.xxx.xxx:7104 - R:/xxx.xx.xxx.xxx:33754]
io.netty.channel.unix.Errors$NativeIoException: syscall:read(...)() failed: Connection reset by peer
	at io.netty.channel.unix.FileDescriptor.readAddress(...)(Unknown Source) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}


{code:java}
INFO  [epollEventLoopGroup-2-23] 2018-11-20 13:16:33,263 Message.java:619 - Unexpected exception during request; channel = [id: 0x98bd7c0e, L:/xxx.xx.xxx.xxx:7104 - R:/xxx.xx.xx.xx:33350]
io.netty.channel.unix.Errors$NativeIoException: readAddress() failed: Connection timed out
	at io.netty.channel.unix.Errors.newIOException(Errors.java:117) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.unix.Errors.ioResult(Errors.java:138) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.unix.FileDescriptor.readAddress(FileDescriptor.java:175) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.AbstractEpollChannel.doReadBytes(AbstractEpollChannel.java:238) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:926) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:397) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:302) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) [netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}

[CASSANDRA-7849|https://issues.apache.org/jira/browse/CASSANDRA-7849] addresses this for JAVA IO Exception like ""java.io.IOException: Connection reset by peer"", but not for Netty IOException since the exception message in Netty includes method name.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-29 14:35:36.078,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 29 18:55:00 UTC 2018,,,,,,0|s00seg:,9223372036854775807,3.0.17,4.0,,,jasobrown,jasobrown,,,,,,,,,,,"22/Nov/18 17:39;sumanth.pasupuleti;Code in question has not changed in trunk, so submitting a patch against trunk
https://github.com/apache/cassandra/pull/294",29/Nov/18 14:35;jasobrown;[~sumanth.pasupuleti] added wrt use of the Java stream API on the PR,"29/Nov/18 18:55;jasobrown;+1

committed as sha {{e4d0ce6ba2d6088c7edf8475f02462e1606f606d}}. Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[utest] tests unable to write system tmp directory,CASSANDRA-14791,13187460,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,25/Sep/18 22:35,12/Mar/19 14:06,13/Mar/19 22:35,08/Oct/18 15:51,4.0,,,,Legacy/Testing,,,,0,,,,"Some tests are failing from time to time because it cannot write to directory {{/tmp/}}:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-test/lastCompletedBuild/testReport/org.apache.cassandra.streaming.compression/CompressedInputStreamTest/testException/

{noformat}
java.lang.RuntimeException: java.nio.file.AccessDeniedException: /tmp/na-1-big-Data.db
	at org.apache.cassandra.io.util.SequentialWriter.openChannel(SequentialWriter.java:119)
	at org.apache.cassandra.io.util.SequentialWriter.<init>(SequentialWriter.java:152)
	at org.apache.cassandra.io.util.SequentialWriter.<init>(SequentialWriter.java:141)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.<init>(CompressedSequentialWriter.java:82)
	at org.apache.cassandra.streaming.compression.CompressedInputStreamTest.testCompressedReadWith(CompressedInputStreamTest.java:119)
	at org.apache.cassandra.streaming.compression.CompressedInputStreamTest.testException(CompressedInputStreamTest.java:78)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.AccessDeniedException: /tmp/na-1-big-Data.db
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:177)
	at java.nio.channels.FileChannel.open(FileChannel.java:287)
	at java.nio.channels.FileChannel.open(FileChannel.java:335)
	at org.apache.cassandra.io.util.SequentialWriter.openChannel(SequentialWriter.java:100)
{noformat}

 I guess it's because some Jenkins slaves don't have proper permission set. For slave {{cassandra16}}, the tests are fine:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-trunk-test/723/testReport/junit/org.apache.cassandra.streaming.compression/CompressedInputStreamTest/testException/history/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-27 10:47:54.865,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 08 15:51:53 UTC 2018,,,,,,0|i3yhtr:,9223372036854775807,,,,,krummas,krummas,,,,,,,,,,,"26/Sep/18 16:23;jay.zhuang;Hi [~mshuler], [~spodxx@gmail.com], any idea if there's a permission setting we could set for the Jenkins Job/Slave?","27/Sep/18 10:47;KurtG;All the hosts are meant to be set up in the same way... However INFRA use puppet, which I have absolutely no faith in. The slaves are all completely managed by INFRA so it's probably a case for an INFRA ticket :/",27/Sep/18 11:50;krummas;should we move unit tests to run in docker to get more control?,27/Sep/18 16:42;jay.zhuang;[~mshuler] talked about the docker option in the last NGCC: https://github.com/ngcc/ngcc2017/blob/master/Help_Test_Apache_Cassandra-NGCC_2017.pdf . Any idea how we can move forward with this?,"08/Oct/18 03:28;jay.zhuang;The root cause of this test failure is not because {{/tmp/}} directory is not writable. But because the unittest generated tmp files {{/tmp/na-1-big-Data.db}} and {{/tmp/na-1-big-CompressionInfo.db}} are not deleted after the test. So I guess on these nodes, the test was run by other user, which left the tmp files that the current user cannot override. I'm able to reproduce the same error message by:
{noformat}
sudo chown root:root /tmp/na-1-big-Data.db
{noformat}

Here is a patch for trunk:
| Branch | uTest |
| [14791|https://github.com/cooldoger/cassandra/tree/14791] | [!https://circleci.com/gh/cooldoger/cassandra/tree/14791.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/14791] |

Passed the tests in Jenkins:
https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/36/testReport/org.apache.cassandra.streaming.compression/CompressedInputStreamTest/","08/Oct/18 04:43;krummas;lgtm, +1",08/Oct/18 15:51;jay.zhuang;Thanks [~krummas] for the review. Committed as [{{73ebd20}}|https://github.com/apache/cassandra/commit/73ebd200c04335624f956e79624cf8494d872f19].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NamedThreadLocalFactory unnecessarily wraps runnable into thread local deallocator ,CASSANDRA-15008,13213896,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,05/Feb/19 13:06,12/Mar/19 14:03,13/Mar/19 22:35,11/Feb/19 17:22,,,,,,,,,0,,,,FastThreadLocalThread already does wrapping of runnable by calling [FastThreadLocalRunnable.wrap in constructor in Netty code|https://github.com/netty/netty/blob/netty-4.1.18.Final/common/src/main/java/io/netty/util/concurrent/FastThreadLocalThread.java#L60]. Second call is unnecessary and incurs unnecessary additional wrapping.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-02-05 17:50:47.015,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 11 17:21:55 UTC 2019,,,,,,0|yi0o1k:,9223372036854775807,,,,,,djoshi3,,,,,,,,,,,"05/Feb/19 13:10;ifesdjeen;|[patch|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15008-trunk]|[tests|https://circleci.com/workflow-run/4a966220-3274-40ea-b543-cf5148cf121d]|

Change was introduced in 4.1.18, so 3.0 and 3.11 are not susceptible.",05/Feb/19 17:50;djoshi3;LGTM +1,"11/Feb/19 17:21;ifesdjeen;Thank you for the review!

Committed to trunk with [c49d42f318c735676d1cb8984c1dee8ae46b3c0d |https://github.com/apache/cassandra/commit/c49d42f318c735676d1cb8984c1dee8ae46b3c0d].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect rf validation in SimpleStrategy,CASSANDRA-15007,13213878,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,djoshi3,Apozyan,Apozyan,05/Feb/19 11:48,12/Mar/19 14:03,13/Mar/19 22:35,19/Feb/19 00:30,4.x,,,,CQL/Semantics,,,,0,,,,"Getting uninformative ConfigurationException when trying to create a keyspace with SimpleStrategy and no replication factor.

{{cqlsh> create keyspace test with replication = \{'class': 'SimpleStrategy'};}}
{{ConfigurationException:}}",,,,,,,,,,,,,,,,,,05/Feb/19 11:49;Apozyan;15007.patch;https://issues.apache.org/jira/secure/attachment/12957626/15007.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2019-02-13 07:08:16.852,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 19 00:36:04 UTC 2019,,,,,,0|yi0nxk:,9223372036854775807,,,,,bdeggleston,bdeggleston,,,,,,,,,,,"05/Feb/19 11:50;Apozyan;Here is a simple fix

{{cqlsh> create keyspace test with replication = \{'class': 'SimpleStrategy'};}}
{{ConfigurationException: SimpleStrategy requires a replication_factor strategy option.}}","13/Feb/19 07:08;djoshi3;Hi [~Apozyan], thanks for the fix. Will this work in the general case?","13/Feb/19 07:58;djoshi3;Nevermind, looking at it more deeply, I noticed this is a recent regression. We already validate options and throw a meaningful message in 3.x. If you notice there is a method called {{validateOptions}} that does exactly what you tried doing in the constructor. It looks like this broke on trunk. So now {{SimpleStrategy}} creation fails with an exception before we have a chance to validate the options.

I have fixed the regression and added a test to avoid future regressions.

||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/15007-trunk]|
|[utests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/15007-trunk]|","13/Feb/19 09:58;Apozyan;Thanks [~djoshi3] for looking into this. You are right, validation broke on trunk, and {{validateOptions}} now is never called if constructor throws an exception.

I compared {{SimpleStrategy.java}} and {{NetworkTopologyStrategy.java}} and I saw that in the latter case validation code is duplicated both in constructor and {{validateOptions}} method. So I think its fine to do the same for SimpleStrategy.",19/Feb/19 00:30;bdeggleston;Looks good to me. Committed to trunk as {{47d4971b56d97ba8a528f7c17bfd6b11f1ababa3}},"19/Feb/19 00:36;djoshi3;Thanks, [~bdeggleston]!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when SELECTing token() on only one part of a two-part partition key,CASSANDRA-14989,13210158,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,djoshi3,manuelkiessling,manuelkiessling,17/Jan/19 13:47,12/Mar/19 14:03,13/Mar/19 22:35,28/Jan/19 12:37,4.0,,,,CQL/Interpreter,,,,0,,,,"I have the following schema:

{code}
CREATE TABLE query_tests.cart_snapshots (
    cart_id uuid,
    realm text,
    snapshot_id timeuuid,
    state text,
    PRIMARY KEY ((cart_id, realm), snapshot_id)
) WITH CLUSTERING ORDER BY (snapshot_id DESC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{code}

In cqlsh, I try the following query:

{code}select token(cart_id) from cart_snapshots ;{code}

This results in cqlsh returning {{ServerError: java.lang.NullPointerException}}, and the following error in the server log:

{code}
DC1N1_1  | ERROR [Native-Transport-Requests-1] 2019-01-16 12:17:52,075 QueryMessage.java:129 - Unexpected error during query
DC1N1_1  | java.lang.NullPointerException: null
DC1N1_1  |       at org.apache.cassandra.db.marshal.CompositeType.build(CompositeType.java:356) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.db.marshal.CompositeType.build(CompositeType.java:349) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.config.CFMetaData.serializePartitionKey(CFMetaData.java:805) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.functions.TokenFct.execute(TokenFct.java:59) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.ScalarFunctionSelector.getOutput(ScalarFunctionSelector.java:61) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.Selection$SelectionWithProcessing$1.getOutputRow(Selection.java:666) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.getOutputRow(Selection.java:492) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.newRow(Selection.java:458) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.processPartition(SelectStatement.java:860) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:790) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:438) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:416) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:289) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:117) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_181]
DC1N1_1  |       at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181]
DC1N1_1  | ERROR [Native-Transport-Requests-1] 2019-01-16 12:17:52,076 ErrorMessage.java:384 - Unexpected exception during request
DC1N1_1  | java.lang.NullPointerException: null
DC1N1_1  |       at org.apache.cassandra.db.marshal.CompositeType.build(CompositeType.java:356) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.db.marshal.CompositeType.build(CompositeType.java:349) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.config.CFMetaData.serializePartitionKey(CFMetaData.java:805) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.functions.TokenFct.execute(TokenFct.java:59) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.ScalarFunctionSelector.getOutput(ScalarFunctionSelector.java:61) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.Selection$SelectionWithProcessing$1.getOutputRow(Selection.java:666) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.getOutputRow(Selection.java:492) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.newRow(Selection.java:458) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.processPartition(SelectStatement.java:860) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:790) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:438) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:416) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:289) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:117) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_181]
DC1N1_1  |       at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181]
{code}
",Using {{cqlsh 5.0.1}} on a Mac OS X host system with Cassandra 3.11.3 running via Docker for Mac from the official {{cassandra:3.11.3}} image.,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-01-17 15:38:04.153,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 28 12:38:14 UTC 2019,,,,,,0|yi0128:,9223372036854775807,3.0.17,3.11.3,4.0,,,iamaleksey,jmeredithco,,,,,,,,,,"17/Jan/19 15:38;jmeredithco;Reproducible for me with a smaller reproducer. The token function expects the number of arguments (and probably the argument types) to match the partition key, however in the failing example only a partial key is passed. 
{code:java}
cqlsh:query_tests> DROP TABLE repro14989;
cqlsh:query_tests> CREATE TABLE repro14989(pk1 uuid, pk2 text, PRIMARY KEY ((pk1, pk2)));
cqlsh:query_tests> INSERT INTO repro14989(pk1,pk2) VALUES (uuid(),'pk2');
cqlsh:query_tests> SELECT token(pk1) FROM repro14989;
ServerError: java.lang.NullPointerException
cqlsh:query_tests> SELECT token(pk1,pk2) FROM repro14989;

 system.token(pk1, pk2)
------------------------
    7705645267149106563

(1 rows)
{code}

In the example above, this query should work
{code}
select token(cart_id, realm) from cart_snapshots ;
{code}

As a proposed fix, when preparing the query, Cassandra should check the arguments for {{token}} are suitable for serializing a partition key before executing the function.","22/Jan/19 07:21;djoshi3;||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/14989-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/14989-trunk]|
||","23/Jan/19 00:18;jmeredithco;Thanks for the patch.  I like the refactor to clean up FunctionResolver.get. The only real comment I have on it is the name for maybeNativeFunction - I think the other functions are native functions too, the thing that's special about token/toJson/fromJson is they support polymorphic types and so don't fit in the Candidate structure something like maybeSpecialFunction or maybePolymorphicFunction would be more descriptive.

After that, +1 from me (not that I can commit it)","23/Jan/19 00:30;djoshi3;Thanks, [~jmeredithco] I am not too fussy about naming. I can update it if you feel strongly about it. [~iamaleksey] could you please take a look at the trunk patch? I can backport it on 3.0 & 3.11 as it'll not apply cleanly.","23/Jan/19 13:32;iamaleksey;The patch is fine, but while reviewing it, I noticed a closely related pre-existing bug in the logic that has been preserved by this patch.

Specifically, if there is a UDF that's also named {{token}}, and the correct keyspace is set via {{USE}} command, then invoking {{token()}} function on a table, with all the right arguments, would fail to resolve to the UDF unless fully qualified.

{code}
create function test.token(val double) returns null on null input returns int language java as 'return 0.0;';
create table test(id int primary key, col double);

select token(col) from test;
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Type error: col cannot be passed as argument 0 of function system.token of type int""

select token(1.0) from test;
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Type error: 1.0 cannot be passed as argument 0 of function system.token of type int""
{code}

Using fully-qualified name returns the expected result:
{code}
cqlsh:test> select test.token(1.0) from test;

 test.token(1.0)
-----------------
               0

(1 rows)
cqlsh:test> select test.token(col) from test;

 test.token(col)
-----------------
               0
{code}

However I'm failing to see a reason why non-qualified invocation shouldn't follow the general logic of filtering candidate functions for the best match. Short-circuiting here, and forcing validation that early seems suboptimal to me.

EDIT: I reckon the same applies to {{fromJson}} and {{toJson}} functions.","24/Jan/19 01:46;djoshi3;[~iamaleksey] based on your feedback, I have changed the patch. It now not only fixes the NPE but also finds the closest matching function if the same function name exists in current & system keyspace. I've added a test to capture this behavior so we wont regress in the future.","28/Jan/19 12:37;iamaleksey;The first two unit tests were exact duplicates of each other, so I removed one, and added a missing test for mismatched argument counts.

Committed to trunk as [174cf761f7897443080b8a840b649b7eab17ae25|https://github.com/apache/cassandra/commit/174cf761f7897443080b8a840b649b7eab17ae25], thanks.","28/Jan/19 12:38;iamaleksey;Didn't commit to 3.0 or 3.11, but don't mind backporting, in principle, despite the issue not being a major one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up javadoc errors,CASSANDRA-14995,13211136,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,djoshi3,djoshi3,djoshi3,23/Jan/19 00:01,12/Mar/19 14:03,13/Mar/19 22:35,05/Feb/19 03:54,4.0,,,,Documentation/Javadoc,,,,0,,,,There are approximately 100 javadoc errors related to obsolete or incorrect pointers. Let's fix them.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-02-05 03:54:59.559,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 05 05:56:52 UTC 2019,,,,,,0|yi072o:,9223372036854775807,,,,,,jjirsa,,,,,,,,,,,"23/Jan/19 00:08;djoshi3;||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/14995-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/14995-trunk]|
||","05/Feb/19 03:54;jjirsa;Thanks. Committed as {{ff73c33ab78f70cd0e70280c89e8d8a46f5536d8}}
","05/Feb/19 05:56;djoshi3;Thanks, [~jjirsa]!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation claims copyright for future years,CASSANDRA-15039,13219001,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bmwiedemann,bmwiedemann,bmwiedemann,01/Mar/19 19:58,12/Mar/19 14:03,13/Mar/19 22:35,08/Mar/19 08:57,3.11.5,4.0,,,Documentation/Javadoc,,,,0,,,,"See attached patch for details and fix.

 
See also on this topic:
[https://stackoverflow.com/questions/2390230/do-copyright-dates-need-to-be-updated]
 ",,,,1800,1800,,0%,1800,1800,,,,,,,,,01/Mar/19 19:55;bmwiedemann;cassandra.patch;https://issues.apache.org/jira/secure/attachment/12960822/cassandra.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2019-03-08 08:42:01.418,,,no_permission,,,,,Patch,,,,,,,,9223372036854775807,,,Fri Mar 08 09:32:48 UTC 2019,,,,,,0|z0091s:,9223372036854775807,3.11.4,,,,michaelsembwever,,,,,,,,,,,,"08/Mar/19 08:42;michaelsembwever;It's not just the generated apidocs but the NOTICE file is wrong as well.
I will commit your fix, and correct the NOTICE file (according to https://www.apache.org/legal/src-headers.html )",08/Mar/19 09:32;bmwiedemann;Thanks. For later reference: Fix is in commit 84fc68ce3f77e88a542dd2443e560cb291109198,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"LOCAL_QUORUM may speculate to non-local nodes, resulting in Timeout instead of Unavailable",CASSANDRA-14735,13184626,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,benedict,benedict,12/Sep/18 14:32,12/Mar/19 14:03,13/Mar/19 22:35,26/Sep/18 09:57,4.0,,,,,,,,0,,,,"This issue applies to all of: rapid read protection, read repair's rapid read protection and read repair's rapid write protection.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-20 23:03:42.211,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 26 09:57:08 UTC 2018,,,,,,0|i3y0ev:,9223372036854775807,,,,,,aweisberg,ifesdjeen,,,,,,,,,,"20/Sep/18 09:49;benedict;Patch available [here|https://github.com/belliottsmith/cassandra/tree/14735]

This patch begins by cleaning up {{ConsistencyLevel}} to remove (most of) the functionality that doesn't logically belong there:

* DC locality testing (to {{InOurDcTester}})
* Replica counting per DC and replication type (to {{Replicas}})
* Endpoint sufficiency testing / filtering to {{ReplicaPlans}}

The last commit makes the only functional changes, reworking the filtering code into separate {{candidatesForRead}} and {{contactForRead}} methods, as well as moving from {{HashMap<String, Integer>}} to {{ObjectIntOpenHashMap}} in the replica counting methods.","20/Sep/18 09:50;benedict;[~ifesdjeen], [~aweisberg], would either (or both) of you mind reviewing?","20/Sep/18 23:03;aweisberg;I ran out of time today. The thing I couldn't convince myself of was how this fixes rapid read protection and read repair. I need to look up how they assemble their reads.

I guess I'm confused how it was broken before because we filter based on DC locality even without this change?","20/Sep/18 23:18;benedict;It’s possible there were too many extraneous cleanup changes; the fix, fundamentally, is splitting filterForQuery into candidatesForRead and contactsForRead; the former now also filters to DC local, whereas previously candidates were not DC-local.","21/Sep/18 00:33;aweisberg;Oh I get it now. It previously selected the candidates and then filtered after resulting in a candidates collection with too many incorrect options. Other DCs are not candidates for LOCAL_*.

+1","26/Sep/18 09:57;benedict;Thanks, committed as [8554d6b35dcc5eec46ed7edc809a36c1f7fa588f|https://github.com/apache/cassandra/commit/8554d6b35dcc5eec46ed7edc809a36c1f7fa588f]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid creating empty compaction tasks after truncate,CASSANDRA-14780,13186656,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,krummas,krummas,21/Sep/18 16:36,12/Mar/19 14:03,13/Mar/19 22:35,21/Sep/18 18:04,4.0,,,,Local/Compaction,,,,0,,,,There is a chance we create empty {{RepairFinishedCompactionTask}} after a table is truncated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-21 16:51:47.549,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 21 18:04:25 UTC 2018,,,,,,0|i3ycw7:,9223372036854775807,,,,,bdeggleston,bdeggleston,,,,,,,,,,,"21/Sep/18 16:41;krummas;https://github.com/krummas/cassandra/commits/marcuse/14780
tests: https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F14780",21/Sep/18 16:51;bdeggleston;+1 on successful circle run,"21/Sep/18 18:04;bdeggleston;committed to trunk as [44cffc0b16a1b55f26996d9aee2d3ffa63bb0512|https://github.com/apache/cassandra/commit/44cffc0b16a1b55f26996d9aee2d3ffa63bb0512], thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove ""audit"" entry from .gitignore",CASSANDRA-14758,13185799,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,krummas,krummas,18/Sep/18 12:53,12/Mar/19 14:03,13/Mar/19 22:35,18/Sep/18 13:24,4.0,,,,Build,,,,0,,,,"Seems there was a ""audit"" entry added to the .gitignore file in CASSANDRA-9608, it makes it kind of hard to work with files in the {{org.apache.cassandra.audit}} package",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-18 13:03:27.63,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 18 13:24:31 UTC 2018,,,,,,0|i3y7mf:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"18/Sep/18 12:56;krummas;https://github.com/krummas/cassandra/commits/marcuse/14758
",18/Sep/18 13:03;jasobrown;+1,"18/Sep/18 13:24;krummas;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing dependencies airline and ohc-core-j8 for pom-all,CASSANDRA-14422,13155508,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,shichao.an,shichao.an,shichao.an,26/Apr/18 19:01,12/Mar/19 14:03,13/Mar/19 22:35,31/May/18 05:12,3.0.17,3.11.3,4.0,,Build,,,,0,,,,"I found two missing dependencies for pom-all (cassandra-all):
 * airline
 * ohc-core-j8

 

This doesn't affect current build scheme because their jars are hardcoded in the lib directory. However, if we depend on cassandra-all in our downstream projects to resolve and fetch dependencies (instead of using the official tarball), Cassandra will have problems, e.g. airline is required by nodetool, and it will fail our dtests.

I will attach the patch shortly",,,,,,,,,,,,,,,,,,30/May/18 09:02;spodxx@gmail.com;deps-tree-311-no_patch.txt;https://issues.apache.org/jira/secure/attachment/12925704/deps-tree-311-no_patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-05-29 22:37:20.318,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu May 31 18:02:11 UTC 2018,,,,,,0|i3t2ov:,9223372036854775807,,,,,jay.zhuang,jay.zhuang,,,,,,,,,,,"26/Apr/18 19:09;shichao.an;Here's the patch for trunk, 3.0 and 3.11:
||Branch||uTest||
|[14422-trunk|https://github.com/shichao-an/cassandra/commits/14422-trunk]|[!https://circleci.com/gh/shichao-an/cassandra/tree/14422-trunk.svg?style=svg!|https://circleci.com/gh/shichao-an/cassandra/tree/14422-trunk]|
|[14422-3.0|https://github.com/shichao-an/cassandra/commits/14422-3.0]|[!https://circleci.com/gh/shichao-an/cassandra/tree/14422-3.0.svg?style=svg!|https://circleci.com/gh/shichao-an/cassandra/tree/14422-3.0]|
|[14422-3.11|https://github.com/shichao-an/cassandra/commits/14422-3.11]|[!https://circleci.com/gh/shichao-an/cassandra/tree/14422-3.11.svg?style=svg!|https://circleci.com/gh/shichao-an/cassandra/tree/14422-3.11]|","29/May/18 22:37;jay.zhuang;The change looks good to me. @[~mshuler], @[~spodxx@gmail.com], what do you guys think?","30/May/18 00:53;mshuler;1) I stopped looking for what changed in 3.0 diff after scrolling on lots of whitespace changes.

2) What bug does this fix with dtest, exactly?","30/May/18 04:28;jjirsa;Should definitely minimize the whitespace changes.
","30/May/18 05:17;jay.zhuang;Sure, I split the change for easy review:

||Diff||
|[14422-3.0|https://github.com/cooldoger/cassandra/commit/31c4724c10830700356ed9cbfee24dab4601e0de]|
|[14422-3.11|https://github.com/cooldoger/cassandra/commit/7d2abcd2c29fa7cbcd114fc7e3152e39ed1016f6]|

The patch is to fix a dependency management issue, it won't change any cassandra binaries or adding/removing any JARs. It only changes the metadata pom. There're missing dependencies in the pom, for example: http://central.maven.org/maven2/org/apache/cassandra/cassandra-all/3.11.2/cassandra-all-3.11.2.pom, it's missing {{airline}} and {{ohc-core-j8}} in {{project->dependencies}} list. If there's a project depends on [{{cassandra-all-3.11.2.jar}}|http://central.maven.org/maven2/org/apache/cassandra/cassandra-all/3.11.2/cassandra-all-3.11.2.jar], the dependency management tool (maven, gradle, etc.) won't be able to download {{ariline}}/{{ohc-core-j8}} which are needed by {{cassandra-all-3.11.2.jar}}.","30/May/18 09:02;spodxx@gmail.com;The dependencies are listed in the parent pom, but not as direct dependencies in cassandra-all. As a result, both artifacts won't become a transitive dependencies by projects depending on cassandra-all and must get explicitly pulled in by such projects. See {{deps-tree-311-no_patch.txt}} on how the current dependency tree for 3.11 cassandra-all pom looks like (you won't find them there).

Now the question is, will any downstream projects actually need these dependencies. But we should probably just be consistent by adding all known runtime dependencies to cassandra-all, whether used by nodetool or any other place.","30/May/18 20:43;jay.zhuang;Thanks [~spodxx@gmail.com]. That's exactly the problem that transitive dependencies are not set correctly. As we published the JAR, we should set that right. I'd like to commit the change later today if there's no objection.",30/May/18 21:59;mshuler;Thanks for the info and no-whitespace diff. Looks good to me!,"30/May/18 22:22;shichao.an;You can ignore whitespace GitHub by adding a ?w=1 in the URL, for example:

 

[https://github.com/shichao-an/cassandra/commit/c1962e32e0a3bf1dde8973855f108ec1a4aeb5d6?w=1]

 ",31/May/18 05:12;jay.zhuang;Thanks [~shichao.an] for the fix. The change is committed as [38096da|https://github.com/apache/cassandra/commit/38096da25bd72346628c001d5b310417f8f703cd].,"31/May/18 16:15;mshuler;For future reference, here is the code style ""policy""/suggestion on whitespace:

[https://cassandra.apache.org/doc/latest/development/code_style.html#whitespace]

Apologies if my suggestion was vague on the removal of whitespace changes to the particular branch diffs, also seconded by Jeff. I assumed the subsequent diffs posted for 3.0/3.11 were the ones that would be committed.

{{git blame build.xml}} forever more will have an irrelevant commit on many lines, when only the code changes should have been committed to those branches. Personally, I use blame frequently when looking for changes, so github trickery means nothing and code reviews should follow the documented code style suggestions.",31/May/18 17:57;jay.zhuang;Thanks [~mshuler] for the information. Will follow the code style policy and have the tailing space change as a separate commit in future.,31/May/18 18:02;shichao.an;[~mshuler] Thanks for your explanation on the code style. I will remember that for future contributions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CREATE TABLE fails if there is a column called ""default"" with Cassandra 3.11.2",CASSANDRA-14359,13149384,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,adelapena,aklages,aklages,01/Apr/18 21:50,12/Mar/19 14:03,13/Mar/19 22:35,18/Apr/18 12:58,3.0.17,3.11.3,4.0,,Legacy/CQL,Legacy/Documentation and Website,,,0,,,,"My project is upgrading from Cassandra 2.1 to 3.11. We have a table whose column name is ""default"". The Cassandra 3.11.2 is rejecting it. I don't see ""default"" as a keyword in the CQL spec. 

To reproduce, try adding the following:
{code:java}
CREATE TABLE simple (
    simplekey text PRIMARY KEY,
    default text // THIS IS REJECTED
);
{code}
I get this error:
{code:java}
SyntaxException: line 3:4 mismatched input 'default' expecting ')' (...    simplekey text PRIMARY KEY,    [default]...)
{code}",This is using Cassandra 3.11.2. This syntax was accepted in 2.1.20.,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-04-06 13:35:48.575,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 24 09:09:11 UTC 2018,,,,,,0|i3s13j:,9223372036854775807,,,,,blerer,blerer,,,,,,,,,,,"02/Apr/18 21:57;aklages;Even though ""default"" is not in the CQL spec, I was able to create the table if I surrounded default with double-quotes. So ""default"" is an undocumented keyword. So for this JIRA, the documentation just needs to be updated to include ""default"".","06/Apr/18 13:35;adelapena;{{DEFAULT}} keyword was added by CASSANDRA-11424 and included in [{{ReservedKeywords}}|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/cql3/ReservedKeywords.java#L89] by CASSANDRA-14205.

That keyword and some others are indeed missed in the CQL documentation:
* cassandra-3.0: {{IS}}, {{MATERIALIZED}}, {{VIEW}}
* cassandra-3.11 and trunk: {{IS}}, {{CAST}}, {{DEFAULT}}, {{DURATION}}, {{GROUP}}, {{LIKE}}, {{MATERIALIZED}}, {{MBEAN}}, {{MBEANS}}, {{PER}}, {{PARTITION}}, {{UNSET}}, {{VIEW}}

This patch adds them to the {{CQL.textile}}:
||[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...adelapena:14359-3.0]||[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...adelapena:14359-3.11]||[trunk|https://github.com/apache/cassandra/compare/trunk...adelapena:14359-trunk]||",18/Apr/18 07:45;blerer;Thanks for the patch. +1,"18/Apr/18 12:58;adelapena;Thanks for reviewing, committed as [9b5ba6ca51e6e35116fbac715cb0e1d3b7eb94f3|https://github.com/apache/cassandra/commit/9b5ba6ca51e6e35116fbac715cb0e1d3b7eb94f3].","24/Apr/18 06:43;krummas;seems this wasn't merged properly from 3.0 -> 3.11, conflicts in docs/cql3/CQL.textile - I merged 3.0 in to 3.11 with {{-s ours}} could you verify that that is correct?","24/Apr/18 09:09;adelapena;[~krummas] it seems correct to me, thanks for fixing the merge",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra NodeTool clientstats should show SSL Cipher,CASSANDRA-14322,13146007,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,djoshi3,djoshi3,djoshi3,18/Mar/18 01:11,12/Mar/19 14:03,13/Mar/19 22:35,19/Mar/18 16:55,4.0,,,,,,,,0,security,,,Currently nodetool prints out some information that does not include the SSL Cipher being used by the client. It would be helpful to add this in for better visibility.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-19 12:19:50.968,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 19 17:09:44 UTC 2018,,,,,,0|i3rgc7:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"18/Mar/18 01:13;djoshi3;||nodetool-cipher-suite||
|[branch|https://github.com/dineshjoshi/cassandra/tree/nodetool-cipher-suite]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/nodetool-cipher-suite]|
||","19/Mar/18 12:19;jasobrown;two minor points:

- I'm not a fan of printing {{""undefined""}} when the connection is not using ssl. Maybe {{""--""}} is better? 
- As long as we're adding the cipher suite, should we print out the protocol, as well?
","19/Mar/18 16:07;djoshi3;Hi [~jasobrown], I have addressed #2 and updated my commit. [~iamaleksey] and I settled on using {{undefined}} for missing driver information. So keep things consistent, I used {{undefined}} here.",19/Mar/18 16:38;djoshi3;[~iamaleksey] gave me some feedback over IRC that I have incorporated in the PR. Looks a lot better now.,19/Mar/18 16:54;djoshi3;I have rebased my patch on trunk since there were some overlapping changes that would've caused conflicts.,19/Mar/18 16:55;jasobrown;+1. committed as sha {{59814db54375d4002eb11403c72861765d9eb356}}. Thanks!,19/Mar/18 17:09;djoshi3;[~jasobrown] thank you!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up repair path after Transient Replication ,CASSANDRA-14698,13183440,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,06/Sep/18 16:12,12/Mar/19 14:03,13/Mar/19 22:35,12/Sep/18 14:58,,,,,,,,,0,pull-request-available,,,"Clean up repair path after Transient Replication since TR doesn't do RR: 

|[patch|https://github.com/apache/cassandra/pull/259]|[dtest|https://circleci.com/gh/ifesdjeen/cassandra/367]|[dtest-vnode|https://circleci.com/gh/ifesdjeen/cassandra/366]|[utest|https://circleci.com/gh/ifesdjeen/cassandra/368]|",,"Github user ifesdjeen commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/265#discussion_r217012379
  
    --- Diff: src/java/org/apache/cassandra/service/reads/DigestResolver.java ---
    @@ -93,16 +93,14 @@ public PartitionIterator getData()
             {
                 // This path can be triggered only if we've got responses from full replicas and they match, but
                 // transient replica response still contains data, which needs to be reconciled.
    -            DataResolver<E, L> dataResolver = new DataResolver<>(command,
    -                                                                 replicaLayout,
    -                                                                 (ReadRepair<E, L>) NoopReadRepair.instance,
    -                                                                 queryStartNanoTime);
    +            DataResolver<E, P> dataResolver
    +                    = new DataResolver<>(command, replicaPlan, (ReadRepair<E, P>) NoopReadRepair.instance, queryStartNanoTime);
     
                 dataResolver.preprocess(dataResponse);
                 // Forward differences to all full nodes
    --- End diff --
    
    I think this one should be already fixed in CASSANDRA-14698
;12/Sep/18 12:39;githubbot;600",,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-06 17:26:51.977,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 12 13:50:36 UTC 2018,,,,,,0|i3xt6f:,9223372036854775807,,,,,bdeggleston,aweisberg,bdeggleston,,,,,,,,,,"06/Sep/18 17:26;aweisberg;+1, Love it! Much clearer now without all that dead code misdirecting people to think that we do read repair with transient replicas.",07/Sep/18 19:06;bdeggleston;+1,"12/Sep/18 13:50;ifesdjeen;[~aweisberg] [~bdeggleston] I've found a small issue with creating merge listener from the wrong layout. It's fixed in the new patch [here|https://github.com/ifesdjeen/cassandra/tree/CASSANDRA-14698] but the change was tiny.

Committed with [8a73427c6543c94ce49da0ed1f833ec5b8ed4f18|https://github.com/apache/cassandra/commit/8a73427c6543c94ce49da0ed1f833ec5b8ed4f18] to trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogReplayer.handleReplayError should print stack traces ,CASSANDRA-14589,13174396,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,benedict,benedict,benedict,25/Jul/18 15:59,12/Mar/19 14:03,13/Mar/19 22:35,30/Nov/18 12:17,3.0.18,,,,Legacy/Observability,,,,0,,,,"handleReplayError does not accept an explicit Throwable parameter, so callers only integrate the exception’s message text into the log entry.  This means a loss of debug information for operators.

Note, this was fixed by CASSANDRA-8844 for 3.x+, only 3.0.x is affected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-11-27 19:08:09.173,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 30 12:17:38 UTC 2018,,,,,,0|i3w9x3:,9223372036854775807,,,,,,djoshi3,,,,,,,,,,,"31/Jul/18 14:52;benedict;[patch|https://github.com/belliottsmith/cassandra/tree/CASSANDRA-14589-3.0]; [CircleCI|https://circleci.com/workflow-run/64b1b22f-c20f-4cd3-8d36-3e3e845aac16]

dtest failures appear to be pre-existing and unrelated.",27/Nov/18 19:08;djoshi3;This looks good. Just a minor whitespace issue [CommitLogReplayer.java#L411|https://github.com/apache/cassandra/compare/trunk...belliottsmith:CASSANDRA-14589-3.0#diff-348a1347dacf897385fb0a97116a1b5eR411] that you can fix on commit. +1.,"30/Nov/18 12:17;benedict;Thanks, committed as [0c97908b2f185615c0134572c4f276cd2c5a3f55|https://github.com/apache/cassandra/commit/0c97908b2f185615c0134572c4f276cd2c5a3f55] to 3.0 (only)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
skip TestRepair.test_dead_coordinator dtest in 4.0,CASSANDRA-14792,13187462,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bdeggleston,bdeggleston,bdeggleston,25/Sep/18 22:57,12/Mar/19 14:03,13/Mar/19 22:35,26/Sep/18 23:46,4.0,,,,Test/dtest,,,,0,,,,"CASSANDRA-14763 changed the coordinator behavior to not cleanup old repair sessions, so this test doesn't really make sense anymore. We should just skip it in 4.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-09-26 17:31:16.45,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 26 23:46:26 UTC 2018,,,,,,0|i3yhu7:,9223372036854775807,,,,,ifesdjeen,ifesdjeen,,,,,,,,,,,"25/Sep/18 23:01;bdeggleston;[dtest|https://github.com/bdeggleston/cassandra-dtest/tree/14792]
[circle|https://circleci.com/workflow-run/bfc92bc4-5d31-4303-865b-b27515f0de00]","26/Sep/18 17:31;ifesdjeen;+1, thank you for fixing this one! 

For completeness, the test was failing with 

{code}
java.lang.RuntimeException: java.lang.IllegalArgumentException: Invalid state transition FINALIZED -> FAILED
        at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:214)
        at org.apache.cassandra.net.MessageDeliveryTask.process(MessageDeliveryTask.java:92)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:54)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)
{code}

This happens only every once in a while, since a completed task is racing with cancel. We could improve a failure message on cancelation if we had a distinction between failed state and cancelled one, but this might be not worth it.","26/Sep/18 23:46;bdeggleston;committed as f4888c8976c2012e9de3b92dedb0ae1a3c984a4b, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] snapshot_test.TestArchiveCommitlog,CASSANDRA-14597,13174862,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,spodxx@gmail.com,jasobrown,jasobrown,26/Jul/18 14:16,12/Mar/19 14:03,13/Mar/19 22:35,13/Aug/18 16:32,,,,,,,,,0,dtest,,,"All TestArchiveCommitLog dtests fail on 3.0, but no other branches. Output from pytest error:

{noformat}
            assert (
                time.time() <= stop_time), ""It's been over a {s}s and we haven't written a new "" + \
>               ""commitlog segment. Something is wrong."".format(s=timeout)
E           AssertionError: It's been over a {s}s and we haven't written a new commitlog segment. Something is wrong.

tools/hacks.py:61: AssertionError
{noformat}
",,,,,,,,,,,,,,,,,,31/Jul/18 11:57;spodxx@gmail.com;2_2.txt;https://issues.apache.org/jira/secure/attachment/12933753/2_2.txt,31/Jul/18 11:57;spodxx@gmail.com;3_0.txt;https://issues.apache.org/jira/secure/attachment/12933754/3_0.txt,31/Jul/18 11:57;spodxx@gmail.com;3_11_3.txt;https://issues.apache.org/jira/secure/attachment/12933755/3_11_3.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2018-07-31 11:59:15.053,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 13 16:32:31 UTC 2018,,,,,,0|i3wcsf:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"31/Jul/18 11:59;spodxx@gmail.com;Looks like the test itself works fine, but the segment allocation behaviour is changed across different versions and 3.0 doesn't allocate a new segment within the timeout to make the test pass.","31/Jul/18 12:37;spodxx@gmail.com;2.2 seems to consume space in segments much faster compared to 3.x, so it's allocating a new segment within shorter time and the test passes.

3.x allocates a new segment early, while still writing into an existing one. Maybe this behaviour has been changed in CASSANDRA-10202?

3.0 will slowly continue to write to the segment and eventually will allocate a new segment if the timeout is sufficiently long. I'd therefor propose to change the timeout to 120 seconds to make the test pass.","10/Aug/18 17:40;jasobrown;+1 on this solution. We could, alternatively, set the timeout only on the call to [advance_to_next_cl_segment in TestCommitlogArchive.run_archive_commitlog()|https://github.com/riptano/cassandra-dtest/blob/master/snapshot_test.py#L252], rather than change the default for the function in hacks.py.

Either way is fine with me, let's just get this one knocked out.

Thanks for looking into this one, [~spodxx@gmail.com].",13/Aug/18 16:32;spodxx@gmail.com;Merged as bd419a7ae,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Returning invalid JSON for NaN and Infinity float values,CASSANDRA-14377,13151647,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,blerer,Sarna,Sarna,11/Apr/18 13:11,12/Mar/19 14:03,13/Mar/19 22:35,02/Aug/18 09:36,2.2.14,3.0.18,3.11.4,4.0,Legacy/CQL,,,,0,,,,"After inserting special float values like NaN and Infinity into a table:

{{CREATE TABLE testme (t1 bigint, t2 float, t3 float, PRIMARY KEY (t1));}}
{{INSERT INTO testme (t1, t2, t3) VALUES (7, NaN, Infinity);}}

and returning them as JSON...

{{cqlsh:demodb> select json * from testme;}}
{{ [json]}}
{{--------------------------------------}}
{{ \{""t1"": 7, ""t2"": NaN, ""t3"": Infinity}}}

 

... the result will not be validated (e.g. with [https://jsonlint.com/|https://jsonlint.com/)] ) because neither NaN nor Infinity is a valid JSON value. The consensus seems to be returning JSON's `null` in these cases, based on this article [https://stackoverflow.com/questions/1423081/json-left-out-infinity-and-nan-json-status-in-ecmascript] and other similar ones.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-04-17 08:38:56.295,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 02 09:36:02 UTC 2018,,,,,,0|i3sezz:,9223372036854775807,2.2.12,3.11.2,,,,,,,,,,,,,,,"17/Apr/18 08:38;blerer;I pushed a patch [here|https://github.com/apache/cassandra/compare/trunk...blerer:14377-2.2]. CI results look good.

[~fcofdezc] Could you review? ",02/Aug/18 07:10;fcofdezc;The patch LGTM.,02/Aug/18 09:35;blerer;Thanks for the review.,"02/Aug/18 09:36;blerer;Committed in to 2.2 at 2bd733264ea0a30f2d62f62195a9bb7860904f83 and merged into 3.0, 3.11 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Website can be built without nodetool documentation by accident,CASSANDRA-14955,13207936,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jolynch,jolynch,jolynch,07/Jan/19 03:09,12/Mar/19 14:03,13/Mar/19 22:35,07/Jan/19 07:08,4.0,,,,Documentation/Website,,,,0,,,,"While [~mick@thelastpickle.com] was generating docs today we accidentally pushed empty nodetool docs because the {{make website}} target doesn't fail if nodetool fails to run. We believe that this is due to the line in [gen-nodetool-docs.py|https://github.com/apache/cassandra/blob/trunk/doc/gen-nodetool-docs.py#L39] which uses subprocess.call instead of check_call.

Let's make it so that if you try to build docs without nodetool being available the build should fail so that we cant make the same mistake again.",,,,,,,,,,,,,,,,,,07/Jan/19 05:00;jolynch;14955-cassandra-site-svn.patch;https://issues.apache.org/jira/secure/attachment/12953933/14955-cassandra-site-svn.patch,07/Jan/19 04:59;jolynch;14955-trunk.txt;https://issues.apache.org/jira/secure/attachment/12953932/14955-trunk.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2019-01-07 07:08:05.366,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 07 07:08:05 UTC 2019,,,,,,0|u00ka8:,9223372036854775807,,,,,michaelsembwever,michaelsembwever,,,,,,,,,,,"07/Jan/19 05:01;jolynch;I attached a patch to trunk ([^14955-trunk.txt] ) which will fail the {{gen-nodetool-docs}} script if the nodetool command fails, tested by running {{ant realclean}} and then:
{noformat}
cassandra-site/src » make add-doc                                                                                                                  2 ↵
make[1]: Entering directory '/home/josephl/pg/cassandra_trunk/doc'
rm -rf build/*
rm -f source/configuration/cassandra_config_file.rst
python convert_yaml_to_rst.py ../conf/cassandra.yaml source/configuration/cassandra_config_file.rst
python gen-nodetool-docs.py
Error: Could not find or load main class org.apache.cassandra.tools.NodeTool
ERROR: Nodetool failed to run, you likely need to build cassandra using ant jar from the top level directory
Traceback (most recent call last):
  File ""gen-nodetool-docs.py"", line 64, in <module>
    create_help_file()
  File ""gen-nodetool-docs.py"", line 48, in create_help_file
    raise cpe
subprocess.CalledProcessError: Command '['../bin/nodetool', 'help']' returned non-zero exit status 1
Makefile:72: recipe for target 'website' failed
make[1]: *** [website] Error 1
make[1]: Leaving directory '/home/josephl/pg/cassandra_trunk/doc'
Makefile:22: recipe for target '.build-doc' failed
make: *** [.build-doc] Error 2
{noformat}
The other changes are fixing the warning that {{make add-doc}} was emitting about the nodetool docs title lengths being wrong, now there are only the TOC errors left.

I've also attached a patch to the svn cassandra-site repo ( [^14955-cassandra-site-svn.patch]) which will run {{ant jar}} for you automatically before building the docs so that the user doesn't have to know they need the main repo built first.","07/Jan/19 07:08;michaelsembwever;Committed as 1850613 and [c68b0fe|https://github.com/apache/cassandra/commit/c68b0fec6f7034aa74e64abd9859ee1d481b4f62].

Thanks [~jolynch]!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] test_failure_threshold_deletions - paging_test.TestPagingWithDeletions,CASSANDRA-14601,13174879,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jasobrown,jasobrown,26/Jul/18 14:56,12/Mar/19 14:00,13/Mar/19 22:35,30/Jul/18 17:30,,,,,Test/dtest,,,,0,dtest,,,"failing dtest on 3.11 only. Error output from pytest:

{noformat}
        except ReadFailure as exc:
            if supports_v5_protocol:
>               assert exc.error_code_map is not None
E               assert None is not None
E                +  where None = ReadFailure('Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 r...d 2 failures"" info={\'consistency\': \'ALL\', \'required_responses\': 2, \'received_responses\': 0, \'failures\': 2}',).error_code_map

paging_test.py:3447: AssertionError
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-29 04:33:34.169,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 30 17:30:20 UTC 2018,,,,,,0|i3wcw7:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"29/Jul/18 04:33;jay.zhuang;The problem is because [{{supports_v5_protocol}}|https://github.com/apache/cassandra-dtest/blob/master/paging_test.py#L3422] is enabled in the test, but not in the client session: [{{dtest_setup.py:333}}|https://github.com/apache/cassandra-dtest/blob/master/dtest_setup.py#L333], so the {{exc.error_code_map}} is not set. Here is a patch to make the check consistent, please review:
|[14601|https://github.com/cooldoger/cassandra-dtest/tree/14601]|

Technically, v5_protocol is supported from {{3.10}}, but changing [{{dtest_setup.py:333}}|https://github.com/apache/cassandra-dtest/blob/master/dtest_setup.py#L333] to {{3.10}} will have the same problem as CASSANDRA-14596: unable to parse the response from prepare statement.",30/Jul/18 12:37;jasobrown;tested locally and passed multiple times. +1,30/Jul/18 17:30;jay.zhuang;Thanks [~jasobrown] for the review. Committed as [{{79d8ddd}}|https://github.com/apache/cassandra-dtest/commit/79d8ddd4a418c52d913ccd5f535f75228877301d].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[dtest] test_mutation_v5 - write_failures_test.TestWriteFailures,CASSANDRA-14596,13174858,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jay.zhuang,jasobrown,jasobrown,26/Jul/18 14:08,12/Mar/19 14:00,13/Mar/19 22:35,10/Aug/18 22:42,,,,,Test/dtest,,,,0,dtest,,,"dtest fails with the following pytest error:

{noformat}
s = b'\x00\x00'

>   unpack = lambda s: packer.unpack(s)[0]
E   struct.error: unpack requires a buffer of 4 bytes
{noformat}

Test fails on 3.11 (was introduced for 3.10), but succeeds on trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-07-28 18:18:47.125,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 10 22:42:24 UTC 2018,,,,,,0|i3wcrj:,9223372036854775807,,,,,jasobrown,jasobrown,,,,,,,,,,,"28/Jul/18 18:18;jay.zhuang;The problem is because the [{{result_metadata_id}}|https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v5.spec#L414] is introduced in protocol V5 but not included in 3.11 branch: CASSANDRA-10786. It's a backward incompatible change (which I think is fine for a beta protocol). So the [newer driver is trying to read|https://github.com/datastax/python-driver/blob/master/cassandra/protocol.py#L679] the {{result_metadata_id}}, which causes failure to parse prepare statement response.

It's impacting the customer who is:
1. Using 3.11 and
2. Using V5 protocol (Beta) and
3. Newer driver after CASSANDRA-10786

Here are a few solutions I can think of:
1. backport the feature to 3.11;
2. add a dummy {{result_metadata_id}} in the prepare statement response just to make it compatible with the latest v5 protocol, so we don't have to backport the full feature (may have other consequence);
3. Disable the test and only run it for 4.0. For the customer using 3.11 + V5 protocol, they have to use the older driver before CASSANDRA-10786.

cc. [~ifesdjeen]","28/Jul/18 18:30;jasobrown;If a user has to go out of their way to enable v5 when running against 3.11 (meaning, it's not the default behavior), I'd say we punt on the dtest for 3.11 and just run it on 4.0. (Option 3)

Like [~jay.zhuang], let's see what [~ifesdjeen] thinks.","01/Aug/18 23:14;jay.zhuang;As we're already only test v5 protocol for {{4.0}}: [{{dtest_setup.py:333}}|https://github.com/apache/cassandra-dtest/blob/master/dtest_setup.py#L333], I also prefer option 3, here is a patch, please review:
|[{{14596}}|https://github.com/cooldoger/cassandra-dtest/tree/14596]|",10/Aug/18 16:29;jasobrown;+1,10/Aug/18 22:42;jay.zhuang;Thanks [~jasobrown] for the review. Committed as [{{2572ddc}}|https://github.com/apache/cassandra-dtest/commit/2572ddce6c9a33ae81e1543195bfae084f835d6d].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Infinity ms Commit Log Sync,CASSANDRA-14451,13160049,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,ozzieisaacs,ozzieisaacs,17/May/18 15:08,12/Mar/19 14:00,13/Mar/19 22:35,05/Jun/18 20:56,3.0.17,3.11.3,4.0,,,,,,0,,,,"Its giving commit log sync warnings where there were apparently zero syncs and therefore gives ""Infinityms"" as the average duration

{code:java}
WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:11:14,294 NoSpamLogger.java:94 - Out of 0 commit log syncs over the past 0.00s with average duration of Infinityms, 1 have exceeded the configured commit interval by an average of 74.40ms 
WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:16:57,844 NoSpamLogger.java:94 - Out of 0 commit log syncs over the past 0.00s with average duration of Infinityms, 1 have exceeded the configured commit interval by an average of 198.69ms 
WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:24:46,325 NoSpamLogger.java:94 - Out of 0 commit log syncs over the past 0.00s with average duration of Infinityms, 1 have exceeded the configured commit interval by an average of 264.11ms 
WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:29:46,393 NoSpamLogger.java:94 - Out of 32 commit log syncs over the past 268.84s with, average duration of 17.56ms, 1 have exceeded the configured commit interval by an average of 173.66ms{code}",3.11.2 - 2 DC,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-05-17 16:03:16.085,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 05 20:56:44 UTC 2018,,,,,,0|i3tu5j:,9223372036854775807,,,,,jrwest,jrwest,,,,,,,,,,,"17/May/18 16:03;jasobrown;Is this on a new cluster, or after an upgrade from a previous version?",17/May/18 16:06;ozzieisaacs;Upgraded from 3.10,"17/May/18 17:25;jasobrown;This is probably me, related to CASSANDRA-14108. I'll take a look into it.","22/May/18 15:47;aby786;all,

 

even i recently upgraded from 3.10 to 3.11.2 and recieving similar errros PERIODIC-COMMIT-LOG-SYNCER.

One thing i observed is even cqlsh login takes over a minute when these erros are reported., also my lowest non prod with 4 nodes 1 DC setup is showing this error and memory was configured as 16GB, the usage went on to 13 to 14 GB. seems to be related .

Is this bug with 3.11.2 ?
 ","01/Jun/18 12:14;jasobrown;As I suspected, this is just a bug with logging (due to CASSANDRA-14108), and no real behavior or correctness is impacted. I have a patch for 3.11, but need to port to 3.0 and trunk.","01/Jun/18 21:55;jasobrown;Below are branches that resolve the problem
||3.0||3.11||trunk||
|[branch|https://github.com/jasobrown/cassandra/tree/14451-3.0]|[branch|https://github.com/jasobrown/cassandra/tree/14451-3.11]|[branch|https://github.com/jasobrown/cassandra/tree/14451-trunk]|
|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14451-3.0]|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14451-3.11]|[utests & dtests|https://circleci.com/gh/jasobrown/workflows/cassandra/tree/14451-trunk]|

The bug was using {{markerIntervalNanos}} in the {{wakeUpAt}} variable as basis for determining if we've lagged in the actual flush to disk. The code should use {{syncIntervalNanos}} for that determination. Once again, the only problem here is around determining if we should log about the commitlog flushing falling behind, not that the commitlog is actually falling behind (it ins't, at least as far as the overlogging here is concerned).

Most of the change was moving the logging code out of the primary {{AbstractCommitLogService.SyncRunnable#sync()}} and into a subroutine. This allowed me to add unit tests, as well as clean up/clarify the {{sync()}} method.","03/Jun/18 00:20;aby786;Can this cause Memory usage to go high ?

 after 3.11.2 upgrade i am getting high memory usage alerts and nothing in errorlog sometime , but sometime i see same PERIODIC-COMMIT-LOG-SYNCER message","03/Jun/18 02:09;jasobrown;[~aby786] I would highly doubt the logging thing would cause your memory problems. You should probably open a separate ticket and describe all the symptoms you are seeing, with as much detail as possible.","05/Jun/18 02:42;jrwest;[~jasobrown] change LGTM. A few questions and minor comments:
 * Are the ArchiveCommitLog dtest failures expected on the 3.0 branch? 
 * The “sleep any time we have left” comment would be more appropriate above the assignment of {{wakeUpAt}}. 
 * Mark {{maybeLogFlushLag}} and {{getTotalSyncDuration}} as {{@VisibleForTesting}}
 * Just wanted to check that the change in behavior of updating {{totalSyncDuration}} is intentional. It makes sense to me that we only increment it if a sync actually occurs but that wasn’t the case before. 
 * Is there are reason you opted for the “excessTimeToFlush” approach in 3.0 but the “maxFlushTimestamp” approach on 3.11 and trunk? The only difference I see is the unit of time. ","05/Jun/18 12:35;jasobrown;bq. Are the ArchiveCommitLog dtest failures expected on the 3.0 branch? 

Yes, 3.0 dtests consistently have about 14 failures, including {{ArchiveCommitLog}}. So, unfortunately, this is expected.

bq. The “sleep any time we have left” comment would be more appropriate above the assignment of wakeUpAt.

I left it where it was previously located, but can move it to the more logical spot.

bq. change in behavior of updating totalSyncDuration is intentional

lol, it wasn't intentional, but it now does the correct thing! You are right that in CASSANDRA-14108 I was adding time to mark the headers (without flushing) to {{totalSyncDuration}}, which is incorrect.

bq. Is there are reason you opted for the “excessTimeToFlush” approach in 3.0 but the “maxFlushTimestamp” approach on 3.11 and trunk?

I wanted to keep the logic as close to the original as possible, since 3.0 is far along in it's age. I suppose it doesn't matter that much, though, and can change if you think it's worthwhile. wdyt?","05/Jun/18 15:07;jrwest;{quote} I left it where it was previously located, but can move it to the more logical spot.
{quote}
I don't find it very useful where it is now. Would vote to move it or remove it (the code is pretty clear).
{quote}I wanted to keep the logic as close to the original as possible, since 3.0 is far along in it's age. I suppose it doesn't matter that much, though, and can change if you think it's worthwhile. wdyt?
{quote}
From the review perspective it was just a second implementation to check for correctness and it seems like either implementation could be used. Would vote for them to be the same but fine as is if you prefer.

 

Otherwise, +1","05/Jun/18 20:56;jasobrown;Made all the last recs from [~jrwest], and committed as sha {{214a3abfcc25460af50805b543a5833697a1b341}}. Thanks, all!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up SSL Cert reloading,CASSANDRA-15018,13215160,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,djoshi3,djoshi3,djoshi3,12/Feb/19 06:05,12/Mar/19 13:59,13/Mar/19 22:35,14/Feb/19 08:10,4.0,,,,Feature/Encryption,,,,0,,,,Minor clean up for SSL Cert reloading code path.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2019-02-14 08:10:18.451,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 14 08:10:18 UTC 2019,,,,,,0|yi0vtk:,9223372036854775807,,,,,ifesdjeen,ifesdjeen,,,,,,,,,,,"12/Feb/19 06:14;djoshi3;||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/15018-trunk]|
|[utests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/15018-trunk]|","14/Feb/19 08:10;ifesdjeen;+1, thank you for the patch! Great that we're getting rid of sleeps in tests!

Committed to trunk with [cbf4da4397c2cec34d6a240b0e917a847c46b3d0|https://github.com/apache/cassandra/commit/cbf4da4397c2cec34d6a240b0e917a847c46b3d0]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
