Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Duplicate),Outward issue link (Incorporates),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Reviewer),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Deleted columns are resurrected after a flush,CASSANDRA-1837,12492744,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,gdusbabek,brandon.williams,brandon.williams,12/8/2010 15:25,3/12/2019 14:20,3/13/2019 22:23,12/15/2010 15:24,0.7.0 rc 3,,,,1,,,,,,"Easily reproduced with the cli:

{noformat}
[default@unknown] create keyspace testks;
2785d67c-02df-11e0-ac09-e700f669bcfc
[default@unknown] use testks;
Authenticated to keyspace: testks
[default@testks] create column family testcf;
2fbad20d-02df-11e0-ac09-e700f669bcfc
[default@testks] set testcf['test']['foo'] = 'foo';
Value inserted.
[default@testks] set testcf['test']['bar'] = 'bar';
Value inserted.
[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test
=> (column=626172, value=626172, timestamp=1291821869120000)
=> (column=666f6f, value=666f6f, timestamp=1291821857320000)

1 Row Returned.
[default@testks] del testcf['test'];
row removed.
[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test

1 Row Returned.
{noformat}

Now flush testks and look again:

{noformat}

[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test
=> (column=626172, value=626172, timestamp=1291821869120000)
=> (column=666f6f, value=666f6f, timestamp=1291821857320000)

1 Row Returned.
{noformat}",,,,,,,,,,,,,,,,,,,,14/Dec/10 17:52;gdusbabek;1837-0.6-unit-test.diff;https://issues.apache.org/jira/secure/attachment/12466228/1837-0.6-unit-test.diff,14/Dec/10 17:29;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-unit-tests-that-show-bug.txt;https://issues.apache.org/jira/secure/attachment/12466225/ASF.LICENSE.NOT.GRANTED--v1-0001-unit-tests-that-show-bug.txt,14/Dec/10 17:29;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0002-copy-deletion-time-to-reduced-column-families.txt;https://issues.apache.org/jira/secure/attachment/12466226/ASF.LICENSE.NOT.GRANTED--v1-0002-copy-deletion-time-to-reduced-column-families.txt,14/Dec/10 22:26;jbellis;v2-0002.txt;https://issues.apache.org/jira/secure/attachment/12466259/v2-0002.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,40:44.9,,,no_permission,,,,,,,,,,,,20337,,,Wed Jan 19 01:21:24 UTC 2011,,,,,,0|i0g7pr:,92680,jbellis,jbellis,,,,,,,,,"08/Dec/10 15:36;brandon.williams;I had a suspicion it was related to CASSANDRA-1780, but I backed it out and the problem persists.",08/Dec/10 20:40;jbellis;can you port to TableTest?  it has reTest to check for same behavior before-and-after-flush.,"10/Dec/10 20:10;gdusbabek;This is happening because of bug in the way deleted rows are [not] interpreted once they leave the memtable in the CFS.getRangeSlice code.  CFS.getColumnFamily doesn't exhibit the bug, but the codepath is pretty different.

I've got a fix that addresses standard columns, but it breaks a few of the nosetests for super columns.  Looking into that now.",14/Dec/10 17:31;gdusbabek;This might be a problem in 0.6 too.  Porting the unit test over isn't trivial though...,14/Dec/10 17:52;gdusbabek;Ported unit test to 0.6. Verifies non-breakage there.,"14/Dec/10 22:26;jbellis;Thanks for tracking that down, Gary!

I think it makes sense to centralize the lastReducedAt logic in getReduced to avoid having a field that is initialized in one method and cleared in another (which looks like a bug in v1, that it doesn't get cleared between rows).  v2 attached w/ this approach.","14/Dec/10 22:47;gdusbabek;bq. I think it makes sense to centralize the lastReducedAt logic in getReduced...
Right. My mistake.",15/Dec/10 15:12;jbellis;+1,15/Dec/10 15:24;gdusbabek;committed.,"15/Dec/10 15:44;hudson;Integrated in Cassandra-0.7 #84 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/84/])
    track row deletions when merging cols to form a row. patch by gdusbabek and jbellis. CASSANDRA-1837
","15/Dec/10 16:15;hudson;Integrated in Cassandra #631 (See [https://hudson.apache.org/hudson/job/Cassandra/631/])
    changes for CASSANDRA-1837
track row deletions when merging cols to form a row. patch by gdusbabek and jbellis. CASSANDRA-1837
unit tests that show bug. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1837
",19/Jan/11 01:19;dave111;just wanted to take credit for originally discovering and reporting this to driftx in the chat. my original example- http://pastie.org/1358812.  thanks for the fix and quick turnaround.,"19/Jan/11 01:21;brandon.williams;Thanks for finding this, Dave.  (I am driftx on irc)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Booting fails on Windows 7/Windows 2003 because of a file rename failure,CASSANDRA-1790,12491618,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,rockxwre,rockxwre,11/30/2010 8:16,3/12/2019 14:20,3/13/2019 22:24,11/30/2010 21:32,0.7.0 rc 2,,,,1,,,,,,"Cassandra 0.7.0 rc will not boot on Windows 7 and Windows 2003 because of a file rename failure. The logging:

{noformat}
 INFO [FlushWriter:1] 2010-11-25 17:12:43,796 Memtable.java (line 155) Writing Memtable-LocationInfo@691789110(435 bytes, 8 operations)
ERROR [FlushWriter:1] 2010-11-25 17:12:43,993 AbstractCassandraDaemon.java (line 90) Fatal exception in thread Thread[FlushWriter:1,5,main]
java.io.IOError: java.io.IOException: rename failed of d:\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:214)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:184)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:167)
	at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:161)
	at org.apache.cassandra.db.Memtable.access$000(Memtable.java:49)
	at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:174)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: rename failed of d:\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.utils.FBUtilities.renameWithConfirm(FBUtilities.java:359)
	at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:210)
	... 12 more
 INFO [main] 2010-11-29 09:16:36,219 AbstractCassandraDaemon.java (line 73) Heap size: 1067253760/1067253760
{noformat}
","Windows 7 (64-bit), Windows 2003 (32-bit)",,,,,,,,,,,,,,,,,,,30/Nov/10 16:26;jbellis;1790.txt;https://issues.apache.org/jira/secure/attachment/12464967/1790.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,26:33.4,,,no_permission,,,,,,,,,,,,20316,,,Sat Dec 11 07:35:11 UTC 2010,,,,,,0|i0g7fb:,92633,mdennis,mdennis,,,,,,,,,"30/Nov/10 16:26;jbellis;patch to close file handle used for post-flush truncate, which will allow the rename to complete on Windows",30/Nov/10 20:04;mdennis;+1,30/Nov/10 21:32;jbellis;committed,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping is broken,CASSANDRA-1574,12475799,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,nickmbailey,nickmbailey,nickmbailey,10/4/2010 20:06,3/12/2019 14:20,3/13/2019 22:24,10/4/2010 22:08,0.7 beta 3,,,,0,,,,,,Bootstrap doesn't block for streaming requests which means nodetool move isn't blocking.  More importantly bootstrap fails to call finishBootstrapping if no stream requests are ever made. This means its impossible to perform moves if you have no keyspaces.,,,,,,,,,,,,,,,,,,,,04/Oct/10 20:24;nickmbailey;0001-Updated-bootstrapping-to-block.patch;https://issues.apache.org/jira/secure/attachment/12456314/0001-Updated-bootstrapping-to-block.patch,04/Oct/10 21:14;jbellis;1574-v2.txt;https://issues.apache.org/jira/secure/attachment/12456324/1574-v2.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,14:36.2,,,no_permission,,,,,,,,,,,,20208,,,Tue Oct 05 13:25:31 UTC 2010,,,,,,0|i0g62f:,92413,jbellis,jbellis,,,,,,,,,04/Oct/10 20:24;nickmbailey;Updated to block with a latch. If nothing is going to be streamed waiting on the latch will return immediately and finish the bootstrap.,"04/Oct/10 21:14;jbellis;v2 removes the unnecessary bootstrapNodes variable, renames startBootstrap to bootstrap to emphasize it is blocking now, and updates storageService to be aware of this blocking nature","04/Oct/10 21:55;nickmbailey;You have a comment in StorageService that spells ""finished"" as ""finishec"". Besides that looks good to me.",04/Oct/10 22:08;jbellis;fixed + committed,"05/Oct/10 13:25;hudson;Integrated in Cassandra #556 (See [https://hudson.apache.org/hudson/job/Cassandra/556/])
    fix moving nodes with no keyspaces defined
patch by Nick Bailey and jbellis for CASSANDRA-1574
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
drop/recreate column family race condition,CASSANDRA-1477,12473494,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,gdusbabek,btoddb,btoddb,9/7/2010 21:21,3/12/2019 14:20,3/13/2019 22:24,9/15/2010 20:39,0.7 beta 2,,,,0,,,,,,"using 0.7 latest from trunk as of few minutes ago.  1 client, 1 node

i have the scenario where i want to drop a column family and recreate it 
- unit testing for instance, is a good reason you may want to do this 
(always start fresh).

the problem i observe is that if i do the following:

1 - drop the column family
2 - recreate it
3 - read data from a key that existed before dropping, but doesn't exist now

if those steps happen fast enough, i will get the old row - definitely 
no good.

if they happen slow enough, get_slice throws:

""org.apache.thrift.TApplicationException: Internal error processing 
get_slice""

.. and on the server i see:

2010-09-07 13:53:48,086 ERROR 
[org.apache.cassandra.thrift.Cassandra$Processor] (pool-1-thread-4:) - 
Internal error processing get_slice
java.lang.RuntimeException: java.util.concurrent.ExecutionException: 
java.io.IOError: java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at 
org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:275)
     at 
org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:218)
     at 
org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:114)
     at 
org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:220)
     at 
org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:299)
     at 
org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:260)
     at 
org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:2795)
     at 
org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2651)
     at 
org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
     at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
     at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
     at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: 
java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
     at java.util.concurrent.FutureTask.get(FutureTask.java:83)
     at 
org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:271)
     ... 11 more
Caused by: java.io.IOError: java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at 
org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:68)
     at 
org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:509)
     at 
org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:49)
     at 
org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:65)
     at 
org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:76)
     at 
org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:961)
     at 
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:856)
     at 
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:826)
     at org.apache.cassandra.db.Table.getRow(Table.java:321)
     at 
org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
     at 
org.apache.cassandra.service.StorageProxy$weakReadLocalCallable.call(StorageProxy.java:737)
     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
     ... 3 more
Caused by: java.io.FileNotFoundException: 
cassandra-data/data/Queues/test_1283892789285_Waiting-e-1-Data.db (No 
such file or directory)
     at java.io.RandomAccessFile.open(Native Method)
     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
     at 
org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
     at 
org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:62)
     ... 15 more

","1 Node cluster, latest code from 0.7 trunk",,,,,,,,,,,,,,,,,,,15/Sep/10 19:34;gdusbabek;ASF.LICENSE.NOT.GRANTED--v4-0001-revert-r996974.-fix-by-adding-a-switch-to-files-to-all.txt;https://issues.apache.org/jira/secure/attachment/12454689/ASF.LICENSE.NOT.GRANTED--v4-0001-revert-r996974.-fix-by-adding-a-switch-to-files-to-all.txt,15/Sep/10 19:34;gdusbabek;ASF.LICENSE.NOT.GRANTED--v4-0002-move-directory-scrubbing-to-startup.txt;https://issues.apache.org/jira/secure/attachment/12454690/ASF.LICENSE.NOT.GRANTED--v4-0002-move-directory-scrubbing-to-startup.txt,15/Sep/10 19:34;gdusbabek;ASF.LICENSE.NOT.GRANTED--v4-0003-avro-system-tests.txt;https://issues.apache.org/jira/secure/attachment/12454691/ASF.LICENSE.NOT.GRANTED--v4-0003-avro-system-tests.txt,13/Sep/10 23:28;btoddb;RaceConditionTest.java;https://issues.apache.org/jira/secure/attachment/12454499/RaceConditionTest.java,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,27:02.4,,,no_permission,,,,,,,,,,,,20159,,,Fri Sep 17 13:55:34 UTC 2010,,,,,,0|i0g5av:,92289,jbellis,jbellis,,,,,,,,,09/Sep/10 12:27;gdusbabek;On the ML you indicated you had a unit tests that was producing this failure.  Is it possible for you to attach it to this ticket?  I would like to include it in our tests.,09/Sep/10 17:05;btoddb;it uses Pelops at the moment.  lemme see if i can rework it for raw thrift.,"13/Sep/10 23:28;btoddb;using latest trunk code as of now

The attached JUnit (method testRaceTooFast) illustrates the problem of data still existing even though the column family has been dropped and recreated, but no data inserted.

However, I cannot make the test fail by throwing ""org.apache.thrift.TApplicationException: Internal error processing get_slice"" as i indicate before because the first problem above never corrects itself.  The data file never goes away no matter how long i wait.  not sure what i was doing different in my app, or possibly code changed.  i no longer do it this way in my app so not sure.

in addition, i tried to drop the keyspace between tests to make sure the keyspace was ""clear"" and this returns ""InvalidRequestException(why:java.io.IOException: Unable to create compaction marker)"" when the keyspace does exist, but i guess i dropped the column family during the previous test and caused it grief.  not sure.

good luck!","14/Sep/10 15:14;gdusbabek;I see now (I can reproduce this fwiw).  We changed the way to cleanup after dropped CFs at the end of August.  Prior to that, we blocked on deletion, but now just mark the CF compacted and wait for normal cleanup to do it's thing.

In hindsight, I'm not sure if this was the best approach.  If we allow creating, dropping, then recreating in rapid succession we should support it better.",14/Sep/10 15:24;jbellis;why do sstables-marked-compacted have any effect on anything?  isn't that a relatively simple fix?,14/Sep/10 15:29;gdusbabek;I'm not sure.  Is it as simple as changing CFS.files() to ignore those that have compacted markers?,"14/Sep/10 15:35;jbellis;either that, or have it clean out the compacted ones before doing its filtering","14/Sep/10 15:35;jbellis;(ignoring is probably safer since it's possible for a file to be marked compacted, but still open for a reader in progress)",14/Sep/10 16:03;jbellis;+1,"14/Sep/10 16:49;btoddb;the patch fixes the first problem, but the second method, testRaceSlowEnoughToCauseException, in the attached unit test still throws the following from get_slice:

InvalidRequestException(why:java.io.IOException: Unable to create compaction marker)
    at org.apache.cassandra.thrift.Cassandra$system_drop_column_family_result.read(Cassandra.java:22872)
    at org.apache.cassandra.thrift.Cassandra$Client.recv_system_drop_column_family(Cassandra.java:1342)
    at org.apache.cassandra.thrift.Cassandra$Client.system_drop_column_family(Cassandra.java:1317)
    at RaceConditionTest.testRaceSlowEnoughToCauseException(RaceConditionTest.java:87)

on the server side i see this:

10/09/14 09:46:32 ERROR thrift.CassandraDaemon: Uncaught exception in thread Thread[MIGRATION_STAGE:1,5,main]
java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create compaction marker
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOError: java.io.IOException: Unable to create compaction marker
	at org.apache.cassandra.io.sstable.SSTableReader.markCompacted(SSTableReader.java:484)
	at org.apache.cassandra.io.sstable.SSTableTracker.replace(SSTableTracker.java:76)
	at org.apache.cassandra.db.ColumnFamilyStore.removeAllSSTables(ColumnFamilyStore.java:711)
	at org.apache.cassandra.db.Table.dropCf(Table.java:271)
	at org.apache.cassandra.db.migration.DropColumnFamily.applyModels(DropColumnFamily.java:94)
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:157)
	at org.apache.cassandra.thrift.CassandraServer$1.call(CassandraServer.java:644)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	... 2 more
Caused by: java.io.IOException: Unable to create compaction marker
	at org.apache.cassandra.io.sstable.SSTableReader.markCompacted(SSTableReader.java:480)
	... 11 more
","14/Sep/10 17:01;btoddb;i should point out that testRaceSlowEnoughToCauseException runs fine in isolation, but run the junit class as a whole and it will fail.","14/Sep/10 20:14;gdusbabek;Setting this to blocker. The bug is such that valid sstables can be deleted when the compaction markers stick around after a CF is dropped and recreated.

The fix is to check for a compaction marker when a sstable is created and delete it if it's there.",14/Sep/10 20:58;jbellis;somehow we're getting into a situation where there is X.compacted but no other sign of X?,"14/Sep/10 21:48;gdusbabek;The exception happens because the compaction marker exists but the deletion executor hasn't done its thing.  It's easy to visualize:
* A keyspace is created along with a column family.  sstables are written.
* That keyspace is dropped. compacted markers are written.
* That keyspace and column family are recreated.  Meanwhile, no files have been deleted.
* As part of re-Table.open()ing it, CFS.scrubDataDirectories() is called but doesn't scrub the compacted markers because of those are now excluded by files() * (I'm pretty sure this wasn't working, or wasn't working like we expect, in the first place, since B.Todd was getting the error before applying the patch).
* Newly created sstables match the names of the compaction markers and will be possibly deleted if timing is bad.

Two fixes come to mind:
1. keep the first patch that has already been committed and make AddKeyspace ensure that data directories, if they exist, are empty.
2. revert the last patch and make AddColumnFamily ensure that its data directory, if it exists, is empty.

I favor option 2.","14/Sep/10 21:56;jbellis;this seems like a race-prone thing to do in the first place since it's not valid to remove a data file that has a reader open to it...

verifying empty data directories on KS creation is fine but what is the analogue for CFs?  and how do you tell the client ""sorry, you're SOL"" if it's not empty?","15/Sep/10 13:53;gdusbabek;You're right.  I was trying to take a shortcut.  

The real problem is that CFS.files() is being used for two things: determine valid files to read from and also count the files to determine generation.  It can't determine the right generation if it doesn't include the invalid files (compacted), but if it includes the invalid files, we read data we shouldn't.",15/Sep/10 13:56;jbellis;I think you've nailed it.,"15/Sep/10 15:17;hudson;Integrated in Cassandra #536 (See [https://hudson.apache.org/hudson/job/Cassandra/536/])
    ensure that compacted sstables are excluded from newly instantiated readers. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1477
",15/Sep/10 15:19;jbellis;don't we also need to change scrub to only run at server start to avoid race-with-reader?,15/Sep/10 18:33;gdusbabek;Yes. v3 addresses that.,"15/Sep/10 18:42;jbellis;+1

(in patch 2, CFS.all() may offer a small simplification.)",15/Sep/10 19:37;gdusbabek;Your suggestion on patch 2 made me realize that the scrubbing should happen pior to populating the instances in CFS.  v4 fixes that.,15/Sep/10 20:14;btoddb;seems like these patches are fixing the issues i found,15/Sep/10 20:31;jbellis;+1,15/Sep/10 20:39;gdusbabek;new and improved fix committed.,"17/Sep/10 13:55;hudson;Integrated in Cassandra #538 (See [https://hudson.apache.org/hudson/job/Cassandra/538/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bytesToHex broken for small byte values,CASSANDRA-602,12442497,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,stuhood,stuhood,stuhood,12/4/2009 19:23,3/12/2019 14:20,3/13/2019 22:22,12/4/2009 20:19,0.5,,,,0,,,,,,"FBUtilities.bytesToHex uses Integer.toHexString, which does not zero-pad its output. We then assert in hexToBytes that each byte string is 2 characters long.

The assertion will occasionally fail, but oftentimes, we simple end up with a different byte value than we started with.",,,,,,,,,,,,,,,,,,,,04/Dec/09 19:24;stuhood;fb-bytestohex.diff;https://issues.apache.org/jira/secure/attachment/12426947/fb-bytestohex.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,00:55.4,,,no_permission,,,,,,,,,,,,19773,,,Thu Dec 17 22:03:02 UTC 2009,,,,,,0|i0fzxz:,91421,,,,,,,,,,,04/Dec/09 19:24;stuhood;Patch adding a more complete testcase and a fix.,04/Dec/09 20:00;gdusbabek;Fix looks good.  +1.,"04/Dec/09 20:19;jbellis;committed (changing "" < 16"" to "" <= 0xF"")","05/Dec/09 12:34;hudson;Integrated in Cassandra #278 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/278/])
    zero-pad single hex digits in bytesToHex.  patch by Stu Hood; reviewed by gdusbabek for 
","17/Dec/09 22:03;hudson;Integrated in Cassandra #291 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/291/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Files added with missing license headers,CASSANDRA-2016,12496139,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,stephenc,stephenc,stephenc,1/20/2011 15:35,3/12/2019 14:20,3/13/2019 22:24,1/20/2011 15:57,0.7.1,,,,0,,,,,,"       src/java/org/apache/cassandra/utils/BloomFilterSerializer.java
       src/java/org/apache/cassandra/utils/LegacyBloomFilterSerializer.java
       src/java/org/apache/cassandra/service/RepairCallback.java
       src/java/org/apache/cassandra/io/util/ColumnSortedMap.java

are all missing license headers... ASLv2 I assume",,,,,,,,,,,,,,,,,,,,20/Jan/11 15:37;stephenc;CASSANDRA-2016-missing-license-headers.patch;https://issues.apache.org/jira/secure/attachment/12468855/CASSANDRA-2016-missing-license-headers.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,57:22.9,,,no_permission,,,,,,,,,,,,20401,,,Thu Jan 20 16:44:05 UTC 2011,,,,,,0|i0g8sv:,92856,,,,,,,,,,,20/Jan/11 15:37;stephenc;attached patch adds the ASL headers which I assume are missing by mistake and not intentionally,"20/Jan/11 15:57;urandom;There is an ant target, {{rat-write}}, which I use when creating release artifacts (as someone has usually forgotten).","20/Jan/11 16:44;hudson;Integrated in Cassandra-0.7 #182 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/182/])
    added missing license headers

Patch eevans; reported by Stephen Connolly for CASSANDRA-2016
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction regression,CASSANDRA-80,12422729,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,4/13/2009 20:58,3/12/2019 14:20,3/13/2019 22:24,4/14/2009 5:13,,,,,0,,,,,,The partitioner refactor caused a regression in compaction with RandomPartitioner.  This adds a test to expose the bug and a patch to fix it.,,,,,,,,,,,,,,,,,,,,13/Apr/09 22:00;jbellis;0001-add-test-showing-compaction-regression.patch;https://issues.apache.org/jira/secure/attachment/12405358/0001-add-test-showing-compaction-regression.patch,13/Apr/09 22:00;jbellis;0002-fix-regression.patch;https://issues.apache.org/jira/secure/attachment/12405359/0002-fix-regression.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,42:54.3,,,no_permission,,,,,,,,,,,,19537,,,Tue Apr 14 05:13:14 UTC 2009,,,,,,0|i0fwr3:,90904,,,,,,,,,,,"14/Apr/09 00:42;tlipcon;One comment:

Do you actually need random values in testCompaction? I prefer that tests be deterministic wherever possible (even if that means using a constant random seed)",14/Apr/09 02:35;junrao;The patch looks fine to me. The testcase only tests that keys are inserted to SSTables in ascending order. The values associated with the keys are not used. ,14/Apr/09 05:13;jbellis;committed w/ unnecessary random data removed (use byte[0] instead).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool move is broken,CASSANDRA-1829,12492542,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,nickmbailey,nickmbailey,nickmbailey,12/6/2010 21:49,3/12/2019 14:20,3/13/2019 22:24,12/9/2010 16:59,0.7.0 rc 3,,,,0,,,,,,The code from finishBootstrapping that finishes a move was removed. This means a move will leave a node stuck in a bootstrapping state forever.,,,,,,,,,,,,,,,,,,,,08/Dec/10 08:02;nickmbailey;0001-Update-token-after-bootstrapping.patch;https://issues.apache.org/jira/secure/attachment/12465783/0001-Update-token-after-bootstrapping.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,59:47.0,,,no_permission,,,,,,,,,,,,20329,,,Mon Dec 13 20:16:13 UTC 2010,,,,,,0|i0g7nz:,92672,,,,,,,,,,,08/Dec/10 08:02;nickmbailey;Still need to test this on a couple vms but this should be right.,08/Dec/10 17:23;nickmbailey;tested on a 3 node cluster,09/Dec/10 16:59;brandon.williams;Committed.,09/Dec/10 23:17;jbellis;committed r1044161 to fix regression of setBootstrapped treatment,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ","13/Dec/10 20:16;hudson;Integrated in Cassandra #625 (See [https://hudson.apache.org/hudson/job/Cassandra/625/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anti-entropy might attempt to repair the system table,CASSANDRA-616,12442820,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,,stuhood,stuhood,12/9/2009 6:19,3/12/2019 14:20,3/13/2019 22:24,12/9/2009 6:31,0.5,,,,0,,,,,,"It is currently possible for AntiEntropyService to decide to 'repair' the system table, which would cause pandemonium, with one node losing its stored token.",,,,,,,,,,,,,,,,,,,,09/Dec/09 06:21;stuhood;616-omg-aes.diff;https://issues.apache.org/jira/secure/attachment/12427430/616-omg-aes.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,31:51.8,,,no_permission,,,,,,,,,,,,19783,,,Wed Dec 09 12:42:41 UTC 2009,,,,,,0|i0g013:,91435,,,,,,,,,,,09/Dec/09 06:21;stuhood;Patch.,09/Dec/09 06:31;jbellis;committed,"09/Dec/09 12:42;hudson;Integrated in Cassandra #282 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/282/])
    avoid anti-entropy checks on the System table.  patch by Stu Hood; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTables limited to (2^31)/15 keys,CASSANDRA-790,12456197,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,stuhood,stuhood,stuhood,2/12/2010 22:27,3/12/2019 14:20,3/13/2019 22:24,2/16/2010 4:01,0.5,,,,0,,,,,,"The current BloomFilter implementation requires a BitSet of (bucket_count * num_keys) in size, and that calculation is currently performed in an integer, which causes overflow for around 140 million keys in one SSTable.

Short term fix: perform the calculation in a long, and cap the value to the maximum size of a BitSet.
Long term fix: begin partitioning BitSets, perhaps using Linear Bloom Filters.",,,,,,,,,,,,,,,,,,,,15/Feb/10 22:01;stuhood;0001-Change-parameters-to-BloomCalculations-in-order-to-c.patch;https://issues.apache.org/jira/secure/attachment/12435910/0001-Change-parameters-to-BloomCalculations-in-order-to-c.patch,14/Feb/10 04:19;stuhood;0001-Change-parameters-to-BloomCalculations-in-order-to-c.patch;https://issues.apache.org/jira/secure/attachment/12435802/0001-Change-parameters-to-BloomCalculations-in-order-to-c.patch,15/Feb/10 22:01;stuhood;0002-Add-timeouts-to-forceBlockingFlush-during-tests.patch;https://issues.apache.org/jira/secure/attachment/12435911/0002-Add-timeouts-to-forceBlockingFlush-during-tests.patch,14/Feb/10 04:19;stuhood;0002-Add-timeouts-to-forceBlockingFlush-during-tests.patch;https://issues.apache.org/jira/secure/attachment/12435803/0002-Add-timeouts-to-forceBlockingFlush-during-tests.patch,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,41:53.4,,,no_permission,,,,,,,,,,,,19863,,,Tue Feb 16 04:01:01 UTC 2010,,,,,,0|i0g13b:,91607,,,,,,,,,,,12/Feb/10 22:35;stuhood;Here is the short term solution.,"12/Feb/10 22:41;jbellis;rather than just cap the filter size, you need to reduce bucketsPerElement too or you will get suboptimal number-of-hashes calculation, since you haven't told it it can't have as big a filter as it wanted.","14/Feb/10 04:19;stuhood;Better implementation of the short term solution. Rather than capping the total number of buckets, this patch caps the number of buckets per element.

The second patch adds timeouts to blocking flushes, to minimize hanging tests.",14/Feb/10 22:02;kingryan;Thanks for the work on this. I'll try and test it soon in the situation that brought up the problem.,"15/Feb/10 19:12;jbellis;patch 01 comments:

refactoring:
 - please split the refactoring into a separate patch; it's hard to tell what is part of the actual fix here
 - BF constructors that do not chain is a design smell; one of them only being called from tests is also a smell
 - instead of using min/max to force values into acceptable ranges, assert that they are sane
 - I feel part of the BF problems here is that BF is trying to be too high-level.  Wouldn't we be better served by having a low-level BF constructor taking hash & bucket counts, and then factories to do the high level things?

fix:
 - this feels like we're trading an obvious problem (BF constructor throws) for a more subtle one (BF is a no-op when we exceed the spec, as noted by TODO).  wouldn't it be better to log a warning, create the largest BF possible, and degrade gracefully?  This would be easier if the BF constructor were sane as mentioned above.","15/Feb/10 19:21;stuhood;> please split the refactoring into a separate patch
Which refactoring? I don't think I did anything that wasn't necessary in order to cap the number of available buckets.

> BF constructors that do not chain is a design smell; one of them only being called from tests is also a smell
The 'maxFalsePosProb' constructor has never been called anywhere but tests, but it was very elegant, and someone spent a lot of time on it, so I wasn't sure whether to remove it.

> low-level BF constructor taking hash & bucket counts, and then factories to do the high level things
Agreed... that would be much better. I'll add factories with warnings.","15/Feb/10 19:25;jbellis;> Which refactoring

the method renaming and constructor rearrangement","15/Feb/10 22:01;stuhood;Updated to use factory functions rather than constructors. Also, for the factory function that takes a bucket/elem target, if the target can't be achieved, a warning will be logged, and the filter will degrade as gracefully as possible by using a minimum of 1 bucket/elem, and the maximum size filter.","16/Feb/10 04:01;jbellis;committed 01 to 0.5 and trunk, with some changes (primarily using real exceptions for critical checks instead of asserts, and asserts instead of min() calls for others)

I'm not seeing any problems with flush that 02 is supposed to mitigate -- if you are, we should fix the code (or the test) to not be so sucky instead of not actually blocking for the requested op.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests fail on Hudson,CASSANDRA-22,12421509,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,johanoskarsson,johanoskarsson,johanoskarsson,3/29/2009 18:56,3/12/2019 14:20,3/13/2019 22:24,3/31/2009 14:42,,,,,0,,,,,,"When running ""ant test"" a number of tests fail.
For example testSingeColumn, testManyColumns, testOpen etc.

Part of stacktrace:
java.lang.NoClassDefFoundError: Could not initialize class org.apache.cassandra.config.DatabaseDescriptor
   [testng]     at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:449)

See: http://hudson.zones.apache.org/hudson/job/Cassandra/4/",Ubuntu,,,,,,,,,,,,,,,,,,,31/Mar/09 10:13;johanoskarsson;CASSANDRA-22.patch;https://issues.apache.org/jira/secure/attachment/12404222/CASSANDRA-22.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:28.1,,,no_permission,,,,,,,,,,,,19520,,,Tue Mar 31 14:42:37 UTC 2009,,,,,,0|i0fwev:,90849,,,,,,,,,,,29/Mar/09 19:58;jbellis;it's failing b/c the expected directories do not exist.  applied patch to make that more clear.,29/Mar/09 21:19;johanoskarsson;What directories are missing? Should the build file create them?,"30/Mar/09 13:37;jbellis;the ones from confifg/server.conf.  /var/cassandra/*.

seems kind of weird to have build script create those.  what if you wanted to use a different config file?",30/Mar/09 13:44;johanoskarsson;How about adding a test config dir with values that make sense when running ant test? Something like build/test/cassandra/* etc,31/Mar/09 10:13;johanoskarsson;This patch should fix the broken tests. It adds a test configuration and creates the needed directories in build/test,31/Mar/09 13:23;jbellis;i am not a build guru but shouldn't the mkdir be done in cleanup rather than hardcoding it in build.xml where it will break if test/storage-conf is changed?,"31/Mar/09 14:05;johanoskarsson;Discussed this on irc with Jonathan Ellis, it turns out the ServerTest class he was referring to gets the directories we need to create from DatabaseDescriptor. That class have a huge static block that amongst other things try to write to the directories we create in the build.xml file. So it's quite tricky to create them in ServerTest right now, we decided to leave it in the build.xml for now until we can untangle some of the staticness in Cassandra.",31/Mar/09 14:42;jbellis;applied,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLog does not flush on writes,CASSANDRA-367,12433134,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,lenn0x,lenn0x,8/15/2009 0:04,3/12/2019 14:20,3/13/2019 22:24,8/18/2009 22:49,0.4,,,,0,,,,,,"When you write data to CommitLog, we are not calling a flush() in class LogRecordAdder.

Is this acceptable? We added flush() and its working now. This bug was introduced when things were consolidated in r799942",,,,,,,,,,,,,,,,,,,,15/Aug/09 00:05;lenn0x;0001-CASSANDRA-367-Added-flush.patch;https://issues.apache.org/jira/secure/attachment/12416628/0001-CASSANDRA-367-Added-flush.patch,18/Aug/09 22:46;jbellis;367-3.patch;https://issues.apache.org/jira/secure/attachment/12416928/367-3.patch,17/Aug/09 19:16;jbellis;367-v2.patch;https://issues.apache.org/jira/secure/attachment/12416794/367-v2.patch,17/Aug/09 18:54;jbellis;367.patch;https://issues.apache.org/jira/secure/attachment/12416793/367.patch,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,42:12.9,,,no_permission,,,,,,,,,,,,19654,,,Wed Aug 19 14:08:32 UTC 2009,,,,,,0|i0fyi7:,91188,,,,,,,,,,,"15/Aug/09 01:42;jbellis;I'd rather not call flush every add if possible.  What problems are you seeing?

(To be pedantic: the change in the revision in question moved from using an unbuffered writer, to using a buffered one.  Not removing a pre-existing flush.)","17/Aug/09 18:54;jbellis;patch to always sync commitlog, either as a batch pre-ack (current CLSync of true) or a new periodic mode defaulting to ever 1000ms",17/Aug/09 19:15;lenn0x;Thread.sleep(600) should be pulling in getCommitLogSyncPeriod(),17/Aug/09 19:16;jbellis;v2 w/ config option actually used :),17/Aug/09 23:31;lenn0x;+1,17/Aug/09 23:37;jbellis;committed,18/Aug/09 22:46;jbellis;forgot while loop :),"18/Aug/09 22:47;lenn0x;Oops! Yes, this looks better now. +1",18/Aug/09 22:49;jbellis;committed,"19/Aug/09 14:08;hudson;Integrated in Cassandra #172 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/172/])
    add missing while loop on periodic commitlog sync thread.
patch by jbellis; reviewed by Chris Goffinet for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trunk build set to wrong version,CASSANDRA-209,12426727,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,johanoskarsson,johanoskarsson,johanoskarsson,5/30/2009 7:45,3/12/2019 14:20,3/13/2019 22:24,6/1/2009 15:38,0.4,,Legacy/Tools,,0,,,,,,The version property in build.xml is still set to 0.3.0-dev in trunk when it should be 0.4.0-dev,,,,,,,,,,,,,,,,,,,,30/May/09 07:45;johanoskarsson;CASSANDRA-209.patch;https://issues.apache.org/jira/secure/attachment/12409455/CASSANDRA-209.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,38:18.3,,,no_permission,,,,,,,,,,,,19595,,,Tue Jun 02 03:08:43 UTC 2009,,,,,,0|i0fxjb:,91031,,,,,,,,,,,01/Jun/09 15:38;jbellis;applied,"02/Jun/09 03:08;hudson;Integrated in Cassandra #95 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/95/])
    bump version to 0.4-dev.  patch by johano; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace incubator site with a redirect,CASSANDRA-895,12459070,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,,johanoskarsson,johanoskarsson,3/14/2010 9:10,3/12/2019 14:20,3/13/2019 22:24,3/14/2010 23:27,,,Legacy/Documentation and Website,,0,,,,,,The incubator website is still up with broken links. We should make it redirect to the new TLP version of the website.,,,,,,,,,,,,,,,,,,,,14/Mar/10 09:14;johanoskarsson;index.html;https://issues.apache.org/jira/secure/attachment/12438744/index.html,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:26.2,,,no_permission,,,,,,,,,,,,19908,,,Sun Mar 14 23:27:26 UTC 2010,,,,,,0|i0g1qn:,91712,,,,,,,,,,,14/Mar/10 09:10;johanoskarsson;It seems I don't have access to change anything in the incubator version of the website.,14/Mar/10 09:14;johanoskarsson;Snippet of html that redirects to the new site.,"14/Mar/10 23:27;iholsman;updated website with HTML.
will be live when it gets synced.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commitlog.recover can delete the commitlog segment instance() just created,CASSANDRA-1644,12478025,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,10/21/2010 19:51,3/12/2019 14:20,3/13/2019 22:24,10/21/2010 23:14,0.7 beta 3,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,21/Oct/10 19:53;jbellis;1644.txt;https://issues.apache.org/jira/secure/attachment/12457778/1644.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,29:49.1,,,no_permission,,,,,,,,,,,,20235,,,Fri Oct 22 12:52:28 UTC 2010,,,,,,0|i0g6hz:,92483,gdusbabek,gdusbabek,,,,,,,,,"21/Oct/10 20:09;jbellis;As the comment says:

+                // we used to try to avoid instantiating commitlog (thus creating an empty segment ready for writes)
+                // until after recover was finished.  this turns out to be fragile; it is less error-prone to go
+                // ahead and allow writes before recover(), and just skip active segments when we do.
",21/Oct/10 20:29;gdusbabek;+1,21/Oct/10 23:14;jbellis;committed,"22/Oct/10 12:52;hudson;Integrated in Cassandra #573 (See [https://hudson.apache.org/hudson/job/Cassandra/573/])
    fix commitlog recovery deleting the newly-created segment as well as the old ones
patch by jbellis; reviewed by gdusbabek for CASSANDRA-1644
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatacenterShardStrategyTest.testProperties fails,CASSANDRA-1107,12464898,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,johanoskarsson,johanoskarsson,5/19/2010 14:58,3/12/2019 14:20,3/13/2019 22:24,5/19/2010 15:21,,,,,0,,,,,,"Stacktrace
junit.framework.AssertionFailedError
	at org.apache.cassandra.locator.DatacenterShardStrategyTest.testProperties(DatacenterShardStrategyTest.java:36)

Standard Error
ERROR 12:39:35,968 Could not find end point information for 127.0.0.1, will use default.

http://hudson.zones.apache.org/hudson/job/Cassandra/440/testReport/junit/org.apache.cassandra.locator/DatacenterShardStrategyTest/testProperties/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,21:01.3,,,no_permission,,,,,,,,,,,,19995,,,Wed May 19 15:21:01 UTC 2010,,,,,,0|i0g31j:,91923,,,,,,,,,,,19/May/10 15:21;jbellis;already fixed in r946192,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamOut fails to start an empty stream,CASSANDRA-1573,12475792,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,nickmbailey,nickmbailey,10/4/2010 18:53,3/12/2019 14:20,3/13/2019 22:24,10/4/2010 22:06,0.7 beta 3,,,,0,,,,,,StreamOut only starts a stream if there are actually files to transfer. This means callbacks will never get called for streams that don't actually have anything to transfer.,,,,,,,,,,,,,,,,,,,,04/Oct/10 18:54;nickmbailey;0001-Remove-empty-list-check-from-StreamOut.patch;https://issues.apache.org/jira/secure/attachment/12456306/0001-Remove-empty-list-check-from-StreamOut.patch,04/Oct/10 20:58;jbellis;1573-v2.txt;https://issues.apache.org/jira/secure/attachment/12456321/1573-v2.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,57:56.8,,,no_permission,,,,,,,,,,,,20207,,,Tue Oct 05 13:25:32 UTC 2010,,,,,,0|i0g627:,92412,nickmbailey,nickmbailey,,,,,,,,,04/Oct/10 18:54;nickmbailey;There is an empty check in begin() anyway so no need for the check in StreamOut.,"04/Oct/10 20:57;jbellis;There is no reason to go through the target node, which is what session.begin will do, when there is nothing to transfer to it.  Attached is an alternate patch that simply skips to session.close when there is nothing to do.",04/Oct/10 21:55;nickmbailey;Looks good to me.,04/Oct/10 22:06;jbellis;committed,"05/Oct/10 13:25;hudson;Integrated in Cassandra #556 (See [https://hudson.apache.org/hudson/job/Cassandra/556/])
    fix unbootstrap when no data is present in a transfer range
patch by jbellis; reviewed by Nick Bailey for CASSANDRA-1573
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get Range Slices is broken,CASSANDRA-1442,12472820,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,stuhood,molezam,molezam,8/29/2010 20:18,3/12/2019 14:07,3/13/2019 22:24,10/9/2010 1:37,0.6.6,0.7 beta 3,,,0,,,,,,"HI,
We just recently tried to use 0.6.4 and 0.6.5 in our production environment and
had some serious problem.
The getRangeSlices functionality is broken.
We have a cluster of 5 machines.
We use getRangeSlices to iterate over all of the keys in a cf (2062 keys total).
We are using OrderPreservingPartitioner.
We use getRangeSlices with KeyRange using keys (not tokens).
If we set the requestBlockCount (aka: KeyRange.setCount()) to a number
greater than 2062 we get all keys in one shot (all is good).
If we try to fetch the keys in smaller blocks (requestBlockCount=100)
we get BAD RESULTS.
We get only 800 unique keys back.
We start with (startKey="""" and endKey="""") then, after each iteration, we use the lastKey to set the startKey for the next page.
Except on first page, we always skip the first item of the page (knowing that it is a repeat, the last one, of the prior page).
To get the lastKey we tried two strategies: [1] set the lastKey to the last item in the page, and [2] use String.compareTo to get the largest ley. Neither strategy worked.
Our keys are strings (obviously the only option in 0.6) that represent numbers.
Some Sample keys are: (in correct lexi order)
-1
11113
11457
6831
7035
8060
8839
------
This code (without any changes) was working correctly under 0.6.3 (we
got same response from getRangeSlices if using requestBlockCounts of
10,000 or 100).
We tried it under 0.6.4 and 0.6.5 and it stopped working.
We reverted back to 0.6.3 and (again, without changing the code) it
started working again.
------
I tried inserting all the keys into a test cluster of one (1 machine) and it worked fine.
So this must be related to how the page is build in a cluster of more than 1 nodes.
We have a cluster of 5 nodes with replication factor of 3.",Linux - CentOs,,,,,,,,,,,,,,,,,,,02/Oct/10 07:14;stuhood;0001-Add-tests-for-StorageProxy.getRestrictedRanges.patch;https://issues.apache.org/jira/secure/attachment/12456179/0001-Add-tests-for-StorageProxy.getRestrictedRanges.patch,02/Oct/10 07:14;stuhood;0002-Allow-ringIterator-to-include-the-minimum-token-and-.patch;https://issues.apache.org/jira/secure/attachment/12456180/0002-Allow-ringIterator-to-include-the-minimum-token-and-.patch,02/Oct/10 07:14;stuhood;0003-Split-the-queryRange-by-ring-and-minimum-tokens.patch;https://issues.apache.org/jira/secure/attachment/12456181/0003-Split-the-queryRange-by-ring-and-minimum-tokens.patch,02/Oct/10 07:17;stuhood;0004-Remove-restrictTo-and-unwrap.patch;https://issues.apache.org/jira/secure/attachment/12456182/0004-Remove-restrictTo-and-unwrap.patch,08/Oct/10 19:56;stuhood;for-trunk-0001-Add-tests-for-StorageProxy.getRestrictedRanges.patch;https://issues.apache.org/jira/secure/attachment/12456723/for-trunk-0001-Add-tests-for-StorageProxy.getRestrictedRanges.patch,08/Oct/10 19:56;stuhood;for-trunk-0002-Allow-ringIterator-to-include-the-minimum-token-and-.patch;https://issues.apache.org/jira/secure/attachment/12456724/for-trunk-0002-Allow-ringIterator-to-include-the-minimum-token-and-.patch,08/Oct/10 19:56;stuhood;for-trunk-0003-Split-the-queryRange-by-ring-and-minimum-tokens.patch;https://issues.apache.org/jira/secure/attachment/12456725/for-trunk-0003-Split-the-queryRange-by-ring-and-minimum-tokens.patch,08/Oct/10 19:57;stuhood;for-trunk-0004-Remove-AbstractBounds.restrictTo.patch;https://issues.apache.org/jira/secure/attachment/12456726/for-trunk-0004-Remove-AbstractBounds.restrictTo.patch,,,,,,8,,,,,,,,,,,,,,,,,,,17:51.5,,,no_permission,,,,,,,,,,,,20140,,,Sat Oct 09 01:37:06 UTC 2010,,,,,,0|i0g533:,92254,jbellis,jbellis,,,,,,,,,"02/Oct/10 07:17;stuhood;0001 Adds tons of unit tests for StorageProxy.getRestrictedRanges
0002 Adds a flag to TokenMetadata.ringIterator to include the minimum token, to allow for cleaner iteration over split points
0003 Implement getRestrictedRanges using ringIterator and a new AbstractBounds.split method
0004 Remove old implementations","02/Oct/10 07:19;stuhood;Patch applies to 0.6, but the tests should be forward ported.",04/Oct/10 15:42;jbellis;02 seems like it shouldn't be necessary for a correct solution.  thoughts?,"04/Oct/10 15:54;stuhood;> 02 seems like it shouldn't be necessary for a correct solution. thoughts?
I don't see a cleaner alternative... finding the correct place to insert the minimum token without the support of the iterator implementation would mean peeking on it to see whether the next token wrapped. This solution is much shorter, and likely to have applications elsewhere in the codebase.",08/Oct/10 19:57;stuhood;Attaching a port of this patch to trunk.,"09/Oct/10 01:37;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consistency QUORUM does not work anymore (hector:Could not fullfill request on this host),CASSANDRA-2081,12497245,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,amorton,tbritz,tbritz,1/31/2011 19:40,3/12/2019 14:07,3/13/2019 22:24,2/7/2011 16:06,0.6.12,0.7.1,,,0,,,,,,"I'm using apache-cassandra-2011-01-28_20-06-01.jar and hector 7.0.25.

Using consistency level Quorum won't work anymore (tested it on read). Consisteny level ONE still works though

I have tried this with one dead node in my cluster.

If I restart cassandra with an older svn revision (apache-cassandra-2011-01-28_20-06-01.jar), I can access the cluster with consistency level QUORUM again, while still using apache-cassandra-2011-01-28_20-06-01.jar and hector 7.0.25 in my application.


11/01/31 19:54:38 ERROR connection.CassandraHostRetryService: Downed intr1n18(192.168.0.18):9160 host still appears to be down: Unable to open transport to intr1n18(192.168.0.18):9160 , java.net.NoRouteToHostException: No route to host
11/01/31 19:54:38 INFO connection.CassandraHostRetryService: Downed Host retry status false with host: intr1n18(192.168.0.18):9160
11/01/31 19:54:45 ERROR connection.HConnectionManager: Could not fullfill request on this host CassandraClient<intr1n11:9160-483>

intr1n11 is marked as up however and I can also access the node through the cassandra cli.


192.168.0.1     Up     Normal  8.02 GB         5.00%   0cc
192.168.0.2     Up     Normal  7.96 GB         5.00%   199
192.168.0.3     Up     Normal  8.24 GB         5.00%   266
192.168.0.4     Up     Normal  4.94 GB         5.00%   333
192.168.0.5     Up     Normal  5.02 GB         5.00%   400
192.168.0.6     Up     Normal  5 GB            5.00%   4cc
192.168.0.7     Up     Normal  5.1 GB          5.00%   599
192.168.0.8     Up     Normal  5.07 GB         5.00%   666
192.168.0.9     Up     Normal  4.78 GB         5.00%   733
192.168.0.10    Up     Normal  4.34 GB         5.00%   7ff
192.168.0.11    Up     Normal  5.01 GB         5.00%   8cc
192.168.0.12    Up     Normal  5.31 GB         5.00%   999
192.168.0.13    Up     Normal  5.56 GB         5.00%   a66
192.168.0.14    Up     Normal  5.82 GB         5.00%   b33
192.168.0.15    Up     Normal  5.57 GB         5.00%   c00
192.168.0.16    Up     Normal  5.03 GB         5.00%   ccc
192.168.0.17    Up     Normal  4.77 GB         5.00%   d99
192.168.0.18    Down   Normal  ?               5.00%   e66
192.168.0.19    Up     Normal  4.78 GB         5.00%   f33
192.168.0.20    Up     Normal  4.83 GB         5.00%   ffffffffffffffff




","linux, hector + cassandra",,,,,,,,,,,,,,,,,,,07/Feb/11 13:18;amorton;2081-2.txt;https://issues.apache.org/jira/secure/attachment/12470443/2081-2.txt,02/Feb/11 21:26;amorton;2081-logging.patch;https://issues.apache.org/jira/secure/attachment/12470072/2081-logging.patch,02/Feb/11 22:29;jbellis;2081.txt;https://issues.apache.org/jira/secure/attachment/12470078/2081.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,55:45.3,,,no_permission,,,,,,,,,,,,20431,,,Mon Feb 07 16:06:12 UTC 2011,,,,,,0|i0g973:,92920,jbellis,jbellis,,,,,,,,,"31/Jan/11 19:55;jbellis;What kind of ""doesn't work"" are you seeing?","31/Jan/11 20:01;tbritz;My application hangs/blocks forever as I catch all the Hector exceptions and retry when there was an error.

Above log file messages will repeat itself again and again.

There are also no error messages in the cassandra log file.

Also ""Could not fullfill request on this host CassandraClient"" is an error message I have never seen before. ","31/Jan/11 20:38;jbellis;Is this RF=3?

What do you see in the Cassandra log when you set log level to debug, for the queries that Hector gives up on?

What are the versions you tried that works/doesn't work?  (In description above both versions are given as apache-cassandra-2011-01-28_20-06-01.jar.)","31/Jan/11 21:00;tbritz;RF=3

I will enable the debug log level tomorrow for cassandra, switch back to apache-cassandra-2011-01-28_20-06-01.jar and post you the results.

The last version that I tried that worked was apache-cassandra-2011-01-24_06-01-26.jar. apache-cassandra-2011-01-28_20-06-01.jar doesn't work anymore.
","31/Jan/11 21:22;brandon.williams;I'm not able to reproduce with contrib/stress, can you try that?","01/Feb/11 04:46;amorton;I've sort of stumbled onto something similar with an 0.7 install. I need to go home now so cannot dig any deeper and rule out human error, but this is what I have.

5 node 0.7.0 install

1) Load data in using

python stress.py -d jb-cass1,jb-cass2,jb-cass3,jb-cass4,jb-cass5 -o insert -n 1000000 -e QUORUM -t 10 -i 1 -l 3
(use all 5 nodes, insert 1,000,000 rows with RF 3 and QUORUM and 10 threads, report progress every second)

2) Read back using 

python stress.py -d jb-cass2,jb-cass3,jb-cass4,jb-cass5 -o read -n 1000000 -e QUORUM -t 10 -i 1
(note that jb-cass1 is removed from the list)

3) make big bang

Once the read has run a few seconds I ran ""reboot -f"" on node 1. I expect the read operations to complete, output was 

11270,1315,1315,0.00839671943578,9
11631,361,361,0.00746133188792,11
11631,0,0,NaN,12
11631,0,0,NaN,13
11631,0,0,NaN,14
11631,0,0,NaN,15
11631,0,0,NaN,16
11631,0,0,NaN,17
11631,0,0,NaN,18
11631,0,0,NaN,19
Process Reader-10:
Traceback (most recent call last):
  File ""/vol/apps/python-2.6.4_64/lib/python2.6/multiprocessing/process.py"", line 232, in _bootstrap
    self.run()
  File ""stress.py"", line 279, in run
    r = self.cclient.get_slice(key, parent, p, consistency)
  File ""/local1/frameworks/cassandra/apache-cassandra-0.7.0-src/contrib/py_stress/cassandra/Cassandra.py"", line 432, in get_slice
    return self.recv_get_slice()
  File ""/local1/frameworks/cassandra/apache-cassandra-0.7.0-src/contrib/py_stress/cassandra/Cassandra.py"", line 462, in recv_get_slice
    raise result.te

All clients died. stress.py is not setting a timeout on the thrift socket, so am guessing this is server side.

I was running DEBUG on all the nodes (but had turned off the line numbers), this is from one. the 114.63 machine is obviously the one I killed. 


DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,186 StorageService.java (line org.apache.cassandra.service.StorageService) Sorted endpoints are /192.168.114.63,jb08.wetafx.co.nz/192.168.114.67,/192.168.114.64
DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,186 QuorumResponseHandler.java (line org.apache.cassandra.service.QuorumResponseHandler) QuorumResponseHandler blocking for 2 responses
DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,186 StorageProxy.java (line org.apache.cassandra.service.StorageProxy) strongread reading digest for SliceFromReadCommand(table='Keyspace1', key='30323334343534', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) from 6624@jb08.wetafx.co.nz/192.168.114.67
DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,187 StorageProxy.java (line org.apache.cassandra.service.StorageProxy) strongread reading data for SliceFromReadCommand(table='Keyspace1', key='30323334343534', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) from 6623@/192.168.114.63
DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,187 StorageProxy.java (line org.apache.cassandra.service.StorageProxy) strongread reading digest for SliceFromReadCommand(table='Keyspace1', key='30323334343534', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) from 6624@/192.168.114.64
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 0 of 5: 4330:false:34@1296532428248604
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 1 of 5: 4331:false:34@1296532428248637
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 2 of 5: 4332:false:34@1296532428248640
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 3 of 5: 4333:false:34@1296532428248642
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 4 of 5: 4334:false:34@1296532428248656
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 ReadVerbHandler.java (line org.apache.cassandra.db.ReadVerbHandler) digest is 220b82e28c2bb4be869c168243d75f01
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 ReadVerbHandler.java (line org.apache.cassandra.db.ReadVerbHandler) Read key 30323334343534; sending response to 7D8FA1FD-A2FE-6A54-7BB0-3B129206D1E1@jb08.wetafx.co.nz/192.168.114.67
DEBUG [RequestResponseStage:13] 2011-02-01 17:14:08,188 ResponseVerbHandler.java (line org.apache.cassandra.net.ResponseVerbHandler) Processing response on a callback from 7D8FA1FD-A2FE-6A54-7BB0-3B129206D1E1@jb08.wetafx.co.nz/192.168.114.67
DEBUG [RequestResponseStage:13] 2011-02-01 17:14:08,188 ReadResponseResolver.java (line org.apache.cassandra.service.ReadResponseResolver) Preprocessed digest response
DEBUG [RequestResponseStage:16] 2011-02-01 17:14:08,188 ResponseVerbHandler.java (line org.apache.cassandra.net.ResponseVerbHandler) Processing response on a callback from 7D8FA1FD-A2FE-6A54-7BB0-3B129206D1E1@/192.168.114.64
DEBUG [RequestResponseStage:16] 2011-02-01 17:14:08,188 ReadResponseResolver.java (line org.apache.cassandra.service.ReadResponseResolver) Preprocessed digest response
 INFO [ScheduledTasks:1] 2011-02-01 17:14:15,438 Gossiper.java (line org.apache.cassandra.gms.Gossiper) InetAddress /192.168.114.63 is now dead.
DEBUG [ScheduledTasks:1] 2011-02-01 17:14:15,440 MessagingService.java (line org.apache.cassandra.net.MessagingService) Resetting pool for /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:14:17,442 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [pool-1-thread-1] 2011-02-01 17:14:18,183 CassandraServer.java (line org.apache.cassandra.thrift.CassandraServer) ... timed out
DEBUG [pool-1-thread-2] 2011-02-01 17:14:18,189 CassandraServer.java (line org.apache.cassandra.thrift.CassandraServer) ... timed out
DEBUG [pool-1-thread-1] 2011-02-01 17:14:18,232 ClientState.java (line org.apache.cassandra.service.ClientState) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-2] 2011-02-01 17:14:18,232 ClientState.java (line org.apache.cassandra.service.ClientState) logged out: #<User allow_all groups=[]>
DEBUG [ScheduledTasks:1] 2011-02-01 17:14:29,811 StorageLoadBalancer.java (line org.apache.cassandra.service.StorageLoadBalancer) Disseminating load info ...
DEBUG [ScheduledTasks:1] 2011-02-01 17:15:29,814 StorageLoadBalancer.java (line org.apache.cassandra.service.StorageLoadBalancer) Disseminating load info ...
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:15:53,637 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:16:08,667 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:16:23,697 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [ScheduledTasks:1] 2011-02-01 17:16:29,816 StorageLoadBalancer.java (line org.apache.cassandra.service.StorageLoadBalancer) Disseminating load info ...
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:16:36,723 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:16:50,751 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:03,775 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:19,807 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [ScheduledTasks:1] 2011-02-01 17:17:29,818 StorageLoadBalancer.java (line org.apache.cassandra.service.StorageLoadBalancer) Disseminating load info ...
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:32,834 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:42,852 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:56,880 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63


need to leave work now so may not be able to get further into this until tomorrow. ","01/Feb/11 08:48;amorton;Had a read through the code for think I have an idea what my problem was, not sure if it applies to the previous issue and not sure if its a real bug. 

o.a.c.service.ReadCallback.response() will only signal the o.a.c.utils.SimpleCondition if the data request has been received. If the signal is not set after rpc_timeout then ReadCallback.get() will raise a j.u.c.TimeoutException() which comes out of the StorageProxy and is caught in CassandraServer and turned into a o.a.c.thrift.TimedOutException. 

So if the node that is asked for the data fails to return, the entire request will timeout even if there are enough nodes to serve the request. I think I've seen this discussed before as by design, and the client should just retry in response to the timeout. Is that correct ? ","01/Feb/11 11:04;tbritz;Brandon, I haven't yet run stress test. I can reproduce this error every single time with a single thread accessing my idle cluster.

I also reverted to an older version of hector, but this won't help. As noted before, this error doesn't occur running apache-cassandra-2011-01-24_06-01-26.jar.

Here is the debug output of one of the nodes timing out in my application and not returning an answer:

I only try to access (read/iterator) one single table  ""table_usersources"".

The application runs on node intr1n5 (192.168.0.5) and I added the debug output of intr1n19 (192.168.0.19). The node intr1n18(192.168.0.18) is down and not responding.

Please let me know if you need more information in order to fix this bug. Thanks!


Intr1n19:

 INFO [HintedHandoff:1] 2011-02-01 11:47:39,772 HintedHandOffManager.java (line 249) Finished hinted handoff of 0 rows to endpoint /192.168.0.15
 INFO [ScheduledTasks:1] 2011-02-01 11:47:40,773 Gossiper.java (line 205) InetAddress /192.168.0.10 is now dead.
DEBUG [ScheduledTasks:1] 2011-02-01 11:47:40,773 MessagingService.java (line 176) Resetting pool for /192.168.0.10
 INFO [HintedHandoff:1] 2011-02-01 11:47:40,775 HintedHandOffManager.java (line 192) Started hinted handoff for endpoint /192.168.0.10
 INFO [GossipStage:1] 2011-02-01 11:47:40,775 Gossiper.java (line 579) InetAddress /192.168.0.10 is now UP
 INFO [HintedHandoff:1] 2011-02-01 11:47:40,775 HintedHandOffManager.java (line 249) Finished hinted handoff of 0 rows to endpoint /192.168.0.10
DEBUG [WRITE-intr1n4/192.168.0.4] 2011-02-01 11:47:41,479 OutboundTcpConnection.java (line 159) attempting to connect to intr1n4/192.168.0.4
DEBUG [WRITE-intr1n20/192.168.0.20] 2011-02-01 11:47:42,729 OutboundTcpConnection.java (line 159) attempting to connect to intr1n20/192.168.0.20
DEBUG [WRITE-intr1n11/192.168.0.11] 2011-02-01 11:47:42,776 OutboundTcpConnection.java (line 159) attempting to connect to intr1n11/192.168.0.11
DEBUG [WRITE-intr1n18/192.168.0.18] 2011-02-01 11:47:46,781 OutboundTcpConnection.java (line 159) attempting to connect to intr1n18/192.168.0.18
DEBUG [WRITE-intr1n15/192.168.0.15] 2011-02-01 11:47:50,786 OutboundTcpConnection.java (line 159) attempting to connect to intr1n15/192.168.0.15
DEBUG [WRITE-intr1n10/192.168.0.10] 2011-02-01 11:47:53,698 OutboundTcpConnection.java (line 159) attempting to connect to intr1n10/192.168.0.10

DEBUG [ScheduledTasks:1] 2011-02-01 11:48:15,413 GCInspector.java (line 135) GC for ParNew: 32 ms, 124741344 reclaimed leaving 880060312 used; max is 3289776128
DEBUG [ScheduledTasks:1] 2011-02-01 11:48:24,853 FileUtils.java (line 48) Deleting LocationInfo-f-79-Index.db
DEBUG [ScheduledTasks:1] 2011-02-01 11:48:24,853 FileUtils.java (line 48) Deleting LocationInfo-f-79-Filter.db
DEBUG [ScheduledTasks:1] 2011-02-01 11:48:24,854 FileUtils.java (line 48) Deleting LocationInfo-f-79-Statistics.db
 INFO [ScheduledTasks:1] 2011-02-01 11:48:24,854 SSTable.java (line 147) Deleted /hd2/cassandra_md5/data/system/LocationInfo-f-79
DEBUG [WRITE-intr1n18/192.168.0.18] 2011-02-01 11:48:28,820 OutboundTcpConnection.java (line 159) attempting to connect to intr1n18/192.168.0.18
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,936 CassandraServer.java (line 445) range_slice
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,943 StorageProxy.java (line 514) RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,], max_keys=250}
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,943 StorageProxy.java (line 705) restricted ranges for query [,] are [[,0cc], (0cc,199], (199,266], (266,333], (333,400], (400,4cc], (4cc,599], (599,666], (666,733], (733,7ff], (7ff,8cc], (8cc,999], (999,a66], (a66,b33], (b33,c00], (c00,ccc], (ccc,d99], (d99,e66], (e66,f33], (f33,ffffffffffffffff], (ffffffffffffffff,]]
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,949 ReadCallback.java (line 58) ReadCallback blocking for 2 responses
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,949 StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,0cc], max_keys=250} from 269@/192.168.0.1
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,949 StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,0cc], max_keys=250} from 269@/192.168.0.2
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,950 StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,0cc], max_keys=250} from 269@/192.168.0.3
DEBUG [RequestResponseStage:1] 2011-02-01 11:48:28,954 ResponseVerbHandler.java (line 48) Processing response on a callback from 269@/192.168.0.1
DEBUG [ScheduledTasks:1] 2011-02-01 11:48:38,771 StorageLoadBalancer.java (line 349) Disseminating load info ...
DEBUG [pool-1-thread-1] 2011-02-01 11:48:38,950 CassandraServer.java (line 483) ... timed out
DEBUG [pool-1-thread-1] 2011-02-01 11:48:38,957 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [WRITE-intr1n18/192.168.0.18] 2011-02-01 11:48:44,835 OutboundTcpConnection.java (line 159) attempting to connect to intr1n18/192.168.0.18
DEBUG [pool-1-thread-3] 2011-02-01 11:48:56,275 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-6] 2011-02-01 11:48:56,275 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-7] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-11] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-5] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-2] 2011-02-01 11:48:56,277 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-8] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-10] 2011-02-01 11:48:56,277 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-12] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-15] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-14] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-9] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-16] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-13] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-4] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [WRITE-intr1n18/192.168.0.18] 2011-02-01 11:48:57,845 OutboundTcpConnection.java (line 159) attempting to connect to intr1n18/192.168.0.18


Application:

11:48:25,616 INFO  ~ Registering JMX me.prettyprint.cassandra.service:ServiceType=hector,MonitorType=hector
11:48:25,652 INFO  ~ get connection for table_lists: consistency: ONE
11:48:25,695 INFO  ~ get connection for table_lists: consistency: ONE
11:48:25,825 INFO  ~ Downed Host Retry service started with queue size -1 and retry delay 10s
11:48:28,887 ERROR ~ Unable to open transport to intr1n18(192.168.0.18):9160
org.apache.thrift.transport.TTransportException: java.net.NoRouteToHostException: No route to host
        at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
        at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
        at me.prettyprint.cassandra.connection.HThriftClient.open(HThriftClient.java:111)
        at me.prettyprint.cassandra.connection.ConcurrentHClientPool.<init>(ConcurrentHClientPool.java:44)
        at me.prettyprint.cassandra.connection.HConnectionManager.<init>(HConnectionManager.java:63)
        at me.prettyprint.cassandra.service.AbstractCluster.<init>(AbstractCluster.java:62)
        at me.prettyprint.cassandra.service.AbstractCluster.<init>(AbstractCluster.java:58)
        at me.prettyprint.cassandra.service.ThriftCluster.<init>(ThriftCluster.java:17)
        at me.prettyprint.hector.api.factory.HFactory.createCluster(HFactory.java:157)
        at me.prettyprint.hector.api.factory.HFactory.getOrCreateCluster(HFactory.java:136)
     ....
Caused by: java.net.NoRouteToHostException: No route to host
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
        ... 23 more
11:48:28,889 ERROR ~ Could not start connection pool for host intr1n18(192.168.0.18):9160
11:48:28,889 INFO  ~ Host detected as down was added to retry queue: intr1n18(192.168.0.18):9160
11:48:28,897 INFO  ~ get connection for table_usersources: consistency: QUORUM
11:48:38,014 ERROR ~ Unable to open transport to intr1n18(192.168.0.18):9160
org.apache.thrift.transport.TTransportException: java.net.NoRouteToHostException: No route to host
        at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
        at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
        at me.prettyprint.cassandra.connection.HThriftClient.open(HThriftClient.java:111)
        at me.prettyprint.cassandra.connection.CassandraHostRetryService$RetryRunner.verifyConnection(CassandraHostRetryService.java:116)
        at me.prettyprint.cassandra.connection.CassandraHostRetryService$RetryRunner.run(CassandraHostRetryService.java:96)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.NoRouteToHostException: No route to host
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
        ... 13 more
11:48:38,019 ERROR ~ Downed intr1n18(192.168.0.18):9160 host still appears to be down: Unable to open transport to intr1n18(192.168.0.18):9160 , java.net.NoRouteToHostException: No route to host
11:48:38,020 INFO  ~ Downed Host retry status false with host: intr1n18(192.168.0.18):9160
11:48:38,956 ERROR ~ Could not fullfill request on this host CassandraClient<intr1n19:9160-594>
11:48:38,956 ERROR ~ Exception:
me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:32)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:161)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:143)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:159)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:129)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.getRangeSlices(KeyspaceServiceImpl.java:165)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:67)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:63)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery.execute(ThriftRangeSlicesQuery.java:62)
    ...
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12104)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:732)
        at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:704)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:149)
        ... 23 more
11:48:48,961 ERROR ~ Could not fullfill request on this host CassandraClient<intr1n17:9160-577>
11:48:48,961 ERROR ~ Exception:
me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:32)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:161)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:143)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:159)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:129)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.getRangeSlices(KeyspaceServiceImpl.java:165)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:67)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:63)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery.execute(ThriftRangeSlicesQuery.java:62)
       ...
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12104)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:732)
        at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:704)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:149)
        ... 23 more
11:48:51,025 ERROR ~ Unable to open transport to intr1n18(192.168.0.18):9160
org.apache.thrift.transport.TTransportException: java.net.NoRouteToHostException: No route to host
        at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
        at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
        at me.prettyprint.cassandra.connection.HThriftClient.open(HThriftClient.java:111)
        at me.prettyprint.cassandra.connection.CassandraHostRetryService$RetryRunner.verifyConnection(CassandraHostRetryService.java:116)
        at me.prettyprint.cassandra.connection.CassandraHostRetryService$RetryRunner.run(CassandraHostRetryService.java:96)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.NoRouteToHostException: No route to host
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
        ... 13 more
11:48:51,025 ERROR ~ Downed intr1n18(192.168.0.18):9160 host still appears to be down: Unable to open transport to intr1n18(192.168.0.18):9160 , java.net.NoRouteToHostException: No route to host
11:48:51,025 INFO  ~ Downed Host retry status false with host: intr1n18(192.168.0.18):9160



Address         Status State   Load            Owns    Token
                                                       ffffffffffffffff
192.168.0.1     Up     Normal  11.26 GB        5.00%   0cc
192.168.0.2     Up     Normal  11.23 GB        5.00%   199
192.168.0.3     Up     Normal  11.58 GB        5.00%   266
192.168.0.4     Up     Normal  6.77 GB         5.00%   333
192.168.0.5     Up     Normal  6.86 GB         5.00%   400
192.168.0.6     Up     Normal  6.81 GB         5.00%   4cc
192.168.0.7     Up     Normal  6.88 GB         5.00%   599
192.168.0.8     Up     Normal  6.84 GB         5.00%   666
192.168.0.9     Up     Normal  6.52 GB         5.00%   733
192.168.0.10    Up     Normal  5.17 GB         5.00%   7ff
192.168.0.11    Up     Normal  6.75 GB         5.00%   8cc
192.168.0.12    Up     Normal  7.06 GB         5.00%   999
192.168.0.13    Up     Normal  7.27 GB         5.00%   a66
192.168.0.14    Up     Normal  7.71 GB         5.00%   b33
192.168.0.15    Up     Normal  7.46 GB         5.00%   c00
192.168.0.16    Up     Normal  6.94 GB         5.00%   ccc
192.168.0.17    Up     Normal  6.45 GB         5.00%   d99
192.168.0.18    Down   Normal  ?               5.00%   e66
192.168.0.19    Up     Normal  6.26 GB         5.00%   f33
192.168.0.20    Up     Normal  6.33 GB         5.00%   ffffffffffffffff

","01/Feb/11 20:35;amorton;My understanding here is the 0.19 node is sending read requests to the 0.1, 0.2 and 0.3 nodes and only getting a reply from the 0.1 node before timing out. The 0.1 node is the first node the request is sent to, so this is the data request the others are digest. 

The timeout is the rpc_timeout, and can be seen here...

DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,949 ReadCallback.java (line 58) ReadCallback blocking for 2 responses
...10 seconds... 
DEBUG [pool-1-thread-1] 2011-02-01 11:48:38,950 CassandraServer.java (line 483) ... timed out

Whats happening on the 0.2 and 0.3 nodes at this point? Are they logging errors or WARN messages about dropped messages ? Can you see any logs about processing messages from the 0.19 node? I'm not sure the down 0.18 node is a factor here.

The client should be retrying when it gets a timeout, which I think you said Hector was doing. 

 ","01/Feb/11 21:46;tbritz;There is absolutely no load on the cluster, so it's strange that the timeout is being triggered. I will put the other nodes into debug mode tomorrow as well and post the DEBUG output of 0.1, 0.2, and 0.3

","02/Feb/11 16:08;tbritz;Debug Output:



========================================
192.168.0.1
--------------------
2011-02-02 12:54:29,117 DEBUG [ScheduledTasks:1] StorageLoadBalancer.java (line 349) Disseminating load info ...

2011-02-02 12:54:35,016 DEBUG [ReadStage:8] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 567@/192.168.0.7
2011-02-02 12:54:45,015 DEBUG [ReadStage:9] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 600@/192.168.0.7

2011-02-02 12:54:49,679 DEBUG [pool-1-thread-5] ClientState.java (line 91) logged out: #<User allow_all groups=[]>

========================================
192.168.0.2
--------------------
2011-02-02 12:54:30,147 DEBUG [ScheduledTasks:1] StorageLoadBalancer.java (line 349) Disseminating load info ...

2011-02-02 12:54:34,789 DEBUG [MutationStage:5] RowMutationVerbHandler.java (line 52) Applying RowMutation(keyspace='table_lists', key='32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674390000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674391000!2419199,])])
2011-02-02 12:54:34,790 DEBUG [MutationStage:5] Table.java (line 397) applying mutation of row 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334
2011-02-02 12:54:34,792 DEBUG [MutationStage:5] RowMutationVerbHandler.java (line 81) RowMutation(keyspace='table_lists', key='32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674390000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674391000!2419199,])]) applied.  Sending response to 566@/192.168.0.7

2011-02-02 12:54:34,953 DEBUG [ReadStage:8] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 567@/192.168.0.7
2011-02-02 12:54:44,965 DEBUG [ReadStage:9] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 600@/192.168.0.7

2011-02-02 12:54:45,057 DEBUG [ScheduledTasks:1] GCInspector.java (line 135) GC for ParNew: 14 ms, 13086296 reclaimed leaving 2087311488 used; max is 4856348672
2011-02-02 12:54:49,613 DEBUG [pool-1-thread-11] ClientState.java (line 91) logged out: #<User allow_all groups=[]>

========================================
192.168.0.3
--------------------
2011-02-02 12:54:30,920 DEBUG [ScheduledTasks:1] StorageLoadBalancer.java (line 349) Disseminating load info ...

2011-02-02 12:54:34,368 DEBUG [MutationStage:8] RowMutationVerbHandler.java (line 52) Applying RowMutation(keyspace='table_lists', key='36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674314000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674322000!2419199,])])
2011-02-02 12:54:34,369 DEBUG [MutationStage:8] Table.java (line 397) applying mutation of row 36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165
2011-02-02 12:54:34,371 DEBUG [MutationStage:8] RowMutationVerbHandler.java (line 81) RowMutation(keyspace='table_lists', key='36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674314000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674322000!2419199,])]) applied.  Sending response to 562@/192.168.0.7
2011-02-02 12:54:34,381 DEBUG [MutationStage:9] RowMutationVerbHandler.java (line 52) Applying RowMutation(keyspace='table_lists', key='32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674390000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674391000!2419199,])])
2011-02-02 12:54:34,382 DEBUG [MutationStage:9] Table.java (line 397) applying mutation of row 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334
2011-02-02 12:54:34,383 DEBUG [MutationStage:9] RowMutationVerbHandler.java (line 81) RowMutation(keyspace='table_lists', key='32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674390000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674391000!2419199,])]) applied.  Sending response to 564@/192.168.0.7

2011-02-02 12:54:34,539 DEBUG [ReadStage:8] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 567@/192.168.0.7
2011-02-02 12:54:44,540 DEBUG [ReadStage:9] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 600@/192.168.0.7

2011-02-02 12:54:49,202 DEBUG [pool-1-thread-13] ClientState.java (line 91) logged out: #<User allow_all groups=[]>

========================================
192.168.0.7
--------------------
2011-02-02 12:54:34,059 DEBUG [ScheduledTasks:1] StorageLoadBalancer.java (line 349) Disseminating load info ...

2011-02-02 12:54:34,356 DEBUG [pool-1-thread-1] CassandraServer.java (line 355) batch_mutate
2011-02-02 12:54:34,375 DEBUG [pool-1-thread-1] StorageProxy.java (line 154) insert writing key 36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165 to 561@/192.168.0.5
2011-02-02 12:54:34,375 DEBUG [pool-1-thread-1] StorageProxy.java (line 154) insert writing key 36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165 to 562@/192.168.0.3
2011-02-02 12:54:34,381 DEBUG [pool-1-thread-1] StorageProxy.java (line 154) insert writing key 36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165 to 563@/94.242.198.13
2011-02-02 12:54:34,384 DEBUG [RequestResponseStage:3] ResponseVerbHandler.java (line 48) Processing response on a callback from 561@/192.168.0.5
2011-02-02 12:54:34,386 DEBUG [RequestResponseStage:5] ResponseVerbHandler.java (line 48) Processing response on a callback from 563@/192.168.0.4
2011-02-02 12:54:34,386 DEBUG [RequestResponseStage:4] ResponseVerbHandler.java (line 48) Processing response on a callback from 562@/192.168.0.3

2011-02-02 12:54:34,391 DEBUG [pool-1-thread-2] CassandraServer.java (line 355) batch_mutate
2011-02-02 12:54:34,393 DEBUG [pool-1-thread-2] StorageProxy.java (line 154) insert writing key 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334 to 564@/192.168.0.3
2011-02-02 12:54:34,394 DEBUG [pool-1-thread-2] StorageProxy.java (line 154) insert writing key 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334 to 565@/192.168.0.4
2011-02-02 12:54:34,394 DEBUG [pool-1-thread-2] StorageProxy.java (line 154) insert writing key 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334 to 566@/192.168.0.2
2011-02-02 12:54:34,398 DEBUG [RequestResponseStage:6] ResponseVerbHandler.java (line 48) Processing response on a callback from 564@/192.168.0.3
2011-02-02 12:54:34,398 DEBUG [RequestResponseStage:7] ResponseVerbHandler.java (line 48) Processing response on a callback from 565@/192.168.0.4
2011-02-02 12:54:34,399 DEBUG [RequestResponseStage:8] ResponseVerbHandler.java (line 48) Processing response on a callback from 566@/192.168.0.2

2011-02-02 12:54:34,531 DEBUG [pool-1-thread-3] CassandraServer.java (line 445) range_slice
2011-02-02 12:54:34,533 DEBUG [pool-1-thread-3] StorageProxy.java (line 514) RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,], max_keys=250}
2011-02-02 12:54:34,534 DEBUG [pool-1-thread-3] StorageProxy.java (line 705) restricted ranges for query [,] are [[,24], (24,49], (49,6d], (6d,92], (92,b6], (b6,db], (db,ffffffffffffffff], (ffffffffffffffff,]]
2011-02-02 12:54:34,536 DEBUG [pool-1-thread-3] ReadCallback.java (line 58) ReadCallback blocking for 2 responses
2011-02-02 12:54:34,536 DEBUG [pool-1-thread-3] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 567@/192.168.0.1
2011-02-02 12:54:34,537 DEBUG [pool-1-thread-3] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 567@/192.168.0.2
2011-02-02 12:54:34,537 DEBUG [pool-1-thread-3] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 567@/192.168.0.3
2011-02-02 12:54:34,554 DEBUG [RequestResponseStage:1] ResponseVerbHandler.java (line 48) Processing response on a callback from 567@/192.168.0.3
2011-02-02 12:54:44,537 DEBUG [pool-1-thread-3] CassandraServer.java (line 483) ... timed out
2011-02-02 12:54:44,548 DEBUG [pool-1-thread-3] ClientState.java (line 91) logged out: #<User allow_all groups=[]>

2011-02-02 12:54:44,549 DEBUG [pool-1-thread-4] CassandraServer.java (line 445) range_slice
2011-02-02 12:54:44,550 DEBUG [pool-1-thread-4] StorageProxy.java (line 514) RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,], max_keys=250}
2011-02-02 12:54:44,550 DEBUG [pool-1-thread-4] StorageProxy.java (line 705) restricted ranges for query [,] are [[,24], (24,49], (49,6d], (6d,92], (92,b6], (b6,db], (db,ffffffffffffffff], (ffffffffffffffff,]]
2011-02-02 12:54:44,550 DEBUG [pool-1-thread-4] ReadCallback.java (line 58) ReadCallback blocking for 2 responses
2011-02-02 12:54:44,552 DEBUG [pool-1-thread-4] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 600@/192.168.0.1
2011-02-02 12:54:44,552 DEBUG [pool-1-thread-4] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 600@/192.168.0.2
2011-02-02 12:54:44,552 DEBUG [pool-1-thread-4] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 600@/192.168.0.3
2011-02-02 12:54:44,554 DEBUG [RequestResponseStage:5] ResponseVerbHandler.java (line 48) Processing response on a callback from 600@/192.168.0.1
2011-02-02 12:54:54,550 DEBUG [pool-1-thread-4] CassandraServer.java (line 483) ... timed out
2011-02-02 12:54:54,552 DEBUG [pool-1-thread-4] ClientState.java (line 91) logged out: #<User allow_all groups=[]>


","02/Feb/11 21:25;amorton;There is something odd about the way the messages are been described in the logs, e.g. yours say

2011-02-02 12:54:44,554 DEBUG [RequestResponseStage:5] ResponseVerbHandler.java (line 48) Processing response on a callback from 600@/192.168.0.1

mine say

DEBUG [RequestResponseStage:16] 2011-02-01 17:14:08,188 ResponseVerbHandler.java (line org.apache.cassandra.net.ResponseVerbHandler) Processing response on a callback from 7D8FA1FD-A2FE-6A54-7BB0-3B129206D1E1@/192.168.114.6 

The MessagingService.sendRR() overload that takes an array (and is called when there are multiple messages) should be updating the message ID for all messages to a shared GUID before sending. This happens the log message about ""StorageProxy.java (line 562) reading RangeSliceCommand..."" It should be present in the ResponseVerbHandler log message.

Perhaps this means the callback for the message cannot be found. To test the theory can you apply the 2081-logging.patch to the node your are running the request on? It updates ResponseVernHandler to log when a message is lost.


",02/Feb/11 21:26;amorton;adds logging to ResponseVerbHandler to log is the message is received but there is no callback,02/Feb/11 21:31;jbellis;Thibaut is using a 0.7.1 prerelease version where the uuid messages from sendRR have been replaced with unique per-host messages (which are standard int IDs like the old non-shared messages were).,02/Feb/11 21:58;tbritz;Is there an archive of older builds from hudson? I could try out a few versions between 24th january and the 28th to pinpoint the revision that is causing this. This won't take long.,"02/Feb/11 22:17;amorton;Looking at the right code now. 

In StorageProxy.fetchRows() line 395 the digestMessage is shared for all non local digest requests. 

And I just realised the log messages above are coming from StorageProxy.getRangeSlice() where the message object is shared for all endpoints. The log messages from Thibaut show the same message ID on nodes 0.1 0.2 and 0.3.

My reading of the MessagingService and ResponseVerbHandler is that once a message with ID X has been received, if others are received with the same ID they will be ignored. Is that correct? If so this looks like a bug in the two methods above. ","02/Feb/11 22:28;jbellis;bq. In StorageProxy.fetchRows() line 395 the digestMessage is shared for all non local digest requests. 

This is fixed in CASSANDRA-2094, but shouldn't break quorum for RF=3 (since a single digest is all we need)

bq. the log messages above are coming from StorageProxy.getRangeSlice() where the message object is shared for all endpoints

Aha, I didn't notice that.  I think this is our bug.  Patch attached.","02/Feb/11 22:39;amorton;Out of interest, how about making the ExpiringMap check the return of NonBlockingHashMap.put() and assert it was the value passed in. May catch future problems. 
","03/Feb/11 10:11;mconrad;Jonathan, I tried your patch and it fixes the problem. Thanks.","03/Feb/11 12:59;tbritz;As Michel noted, this fixes the problem. Thanks :-)",03/Feb/11 16:02;jbellis;Thanks for helping track that down everyone!,"03/Feb/11 16:16;hudson;Integrated in Cassandra-0.7 #237 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/237/])
    fix range slice ConsistencyLevel > ONE
patch by jbellis; tested by Michel Conrad and Thibaut for CASSANDRA-2081
","03/Feb/11 16:24;hudson;Integrated in Cassandra-0.6 #56 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/56/])
    fix range slice ConsistencyLevel > ONE
patch by jbellis; tested by Michel Conrad and Thibaut for CASSANDRA-2081
","03/Feb/11 16:37;hudson;Integrated in Cassandra-0.7 #238 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/238/])
    add debug log message for missing callback
patch by Aaron Morton and jbellis for CASSANDRA-2081
","07/Feb/11 12:48;amorton;StorageProxy.scan() is also reusing Message objects, with the same result of requests at QUORUM timing out for get_indexed_slice()

Log messages below show a get_indexed_slice() in a 3 node cluster @ QUORUM. The new log message shows the callback has already been removed. 



DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,893 CassandraServer.java (line 509) scan
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,894 StorageProxy.java (line 703) restricted ranges for query [-1,-1] are [[-1,85070591730234615865843651857942052864], (85070591730234615865843651857942052864,95070591730234615865843651857942052864], (95070591730234615865843651857942052864,108074891939685041992920030907211891412], (108074891939685041992920030907211891412,-1]]
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,919 StorageProxy.java (line 796) scan ranges are [-1,85070591730234615865843651857942052864],(85070591730234615865843651857942052864,95070591730234615865843651857942052864],(95070591730234615865843651857942052864,108074891939685041992920030907211891412],(108074891939685041992920030907211891412,-1]
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,919 ReadCallback.java (line 58) ReadCallback blocking for 2 responses
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,923 StorageProxy.java (line 819) reading org.apache.cassandra.db.IndexScanCommand@26a1b248 from 6289@/127.0.0.3
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,923 StorageProxy.java (line 819) reading org.apache.cassandra.db.IndexScanCommand@26a1b248 from 6289@/127.0.0.2
DEBUG [RequestResponseStage:1] 2011-02-08 01:46:32,926 ResponseVerbHandler.java (line 51) Processing response on a callback from 6289@/127.0.0.3
DEBUG [RequestResponseStage:2] 2011-02-08 01:46:32,928 ResponseVerbHandler.java (line 41) Callback already removed for 6289
",07/Feb/11 12:48;amorton;Am trying to fix this now. ,"07/Feb/11 13:17;amorton;patch 2081-2.txt is against the current 0.7 branch. I've added an assert in the MessageService that looks like this when a callback is added for an existing message ID. 

RROR [pool-1-thread-1] 2011-02-08 02:07:46,763 Cassandra.java (line 2918) Internal error processing get_indexed_slices
java.lang.AssertionError
	at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:262)
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:278)
	at org.apache.cassandra.service.StorageProxy.scan(StorageProxy.java:817)
	at org.apache.cassandra.thrift.CassandraServer.get_indexed_slices(CassandraServer.java:520)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_indexed_slices.process(Cassandra.java:2910)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)


Have also modified StorageProxy.scan() to create a message for each endpoint. 

I've read through StorageProxy and think their might be an problem in .sendMessages() line 236, it looks like it sends the same message to all endpoints in the local DC. It's late/early and I'm not sure so can someone else take a look please. 
","07/Feb/11 14:47;hudson;Integrated in Cassandra-0.7 #252 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/252/])
    avoid re-using Message object in index queries
patch by Aaron Morton; reviewed by jbellis for CASSANDRA-2081
","07/Feb/11 14:48;jbellis;bq. it looks like it sends the same message to all endpoints in the local DC

It sends the same message to each entry in the list...  which is always going to be a list of size one.  So the good news is I don't think it's incorrect, but the bad news is we broke CASSANDRA-1530.",07/Feb/11 16:06;jbellis;will re-open 1530,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not restarting due to Invalid saved cache,CASSANDRA-2076,12497183,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,mdennis,tbritz,tbritz,1/31/2011 9:15,3/12/2019 14:07,3/13/2019 22:24,2/10/2011 3:29,0.7.1,,,,0,,,,,,"This occured on two nodes on me (running 0.7.1 from svn)

One node was killed by the kernel due to a OOM and the other node was haning and I had to kill it manually with kill -9 (kill didn't work). (maybe these were faulty hardware nodes, I don't know)

The saved_cache was corrupt afterwards and I couldn't start the nodes. 

After deleting the saved_caches directory I could start the nodes again. 

Instead of not starting when an error occurs, cassandra could simply delete the errornous file and continue to start?




 INFO 22:31:11,570 reading saved cache
/hd1/cassandra_md5/saved_caches/table_attributes-table_attributes-KeyCache
ERROR 22:31:11,595 Exception encountered during startup.
java.lang.RuntimeException: The provided key was not UTF8 encoded.
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
       at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:281)
       at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:218)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:458)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:440)
       at org.apache.cassandra.db.Table.initCf(Table.java:360)
       at org.apache.cassandra.db.Table.<init>(Table.java:290)
       at org.apache.cassandra.db.Table.open(Table.java:107)
       at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
       at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:312)
       at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.nio.charset.MalformedInputException: Input length = 1
       at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
       at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
       at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
       ... 11 more
Exception encountered during startup.
java.lang.RuntimeException: The provided key was not UTF8 encoded.
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
       at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:281)
       at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:218)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:458)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:440)
       at org.apache.cassandra.db.Table.initCf(Table.java:360)
       at org.apache.cassandra.db.Table.<init>(Table.java:290)
       at org.apache.cassandra.db.Table.open(Table.java:107)
       at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
       at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:312)
       at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.nio.charset.MalformedInputException: Input length = 1
       at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
       at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
       at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
       ... 11 more",linux,,7200,7200,,0%,7200,7200,,,,,,,,,,,,08/Feb/11 23:19;mdennis;2076-cassandra-0.7.txt;https://issues.apache.org/jira/secure/attachment/12470637/2076-cassandra-0.7.txt,05/Feb/11 13:56;jbellis;2076-v2.txt;https://issues.apache.org/jira/secure/attachment/12470361/2076-v2.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,46:19.8,,,no_permission,,,,,,,,,,,,20430,,,Thu Feb 10 04:34:54 UTC 2011,,,,,,0|i0g96f:,92917,jbellis,jbellis,,,,,,,,,"31/Jan/11 15:01;tbritz;This might be related:

Two other nodes (still running) also show up the ""The provided key was not UTF8 encoded."" error in the log.

I have never seen this error in 0.7.0


ERROR [MutationStage:19] 2011-01-30 21:36:16,951 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:190)
        at org.apache.cassandra.service.StorageProxy$2.runMayThrow(StorageProxy.java:288)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 8 more
ERROR [MutationStage:19] 2011-01-30 21:36:16,991 AbstractCassandraDaemon.java (line 119) Fatal exception in thread Thread[MutationStage:19,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:190)
        at org.apache.cassandra.service.StorageProxy$2.runMayThrow(StorageProxy.java:288)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 8 more
 WARN [ScheduledTasks:1] 2011-01-30 21:36:21,450 MessagingService.java (line 506) Dropped 8 MUTATION messages in the last 5000ms



","31/Jan/11 15:22;tbritz;I brought down and restarted the entire cluster. (100 nodes, 5x20 nodes)

Every single node complains of an invalid file in the saved_cache directory.",01/Feb/11 11:20;tbritz;This just happens if you kill the node with -9,"03/Feb/11 16:46;jbellis;The problem is that we're not cloning the buffer backing the key read from thrift, so we are hitting a problem similar to CASSANDRA-1743.  Matt is working on a fix.","03/Feb/11 23:35;mdennis;attached patch (depends on CASSANDRA-2102) allows C* to start when the saved caches are invalid/corrupt.

CASSANDRA-2102 prevents the saved caches from becoming corrupt in the first place.

","04/Feb/11 09:22;tbritz;I was running yesterday's version with the Consitency fix and getting a similar error messages while reading the commitlog at start. I had to delete the commitlog (data loss) to restart cassandra.

Is this also related to CASSANDRA-2102 or shall I open a new bug report?


 INFO 23:50:00,922 Replaying /hd1/cassandra_md5/commitlog/CommitLog-1296731219671.log, /hd1/cassandra_md5/commitlog/CommitLog-1296759540149.log, /hd1/cassandra_md5/commitlog/CommitLog-1296760444366.log, /hd1/cassandra_md5/commitlog/CommitLog-1296761120546.log, /hd1/cassandra_md5/commitlog/CommitLog-1296762054192.log, /hd1/cassandra_md5/commitlog/CommitLog-1296773137101.log, /hd1/cassandra_md5/commitlog/CommitLog-1296773242671.log
 INFO 23:50:02,192 Finished reading /hd1/cassandra_md5/commitlog/CommitLog-1296731219671.log
ERROR 23:50:02,235 Fatal exception in thread Thread[MutationStage:7,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:294)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 10 more
 INFO 23:50:02,245 Finished reading /hd1/cassandra_md5/commitlog/CommitLog-1296759540149.log
ERROR 23:50:02,245 Exception encountered during startup.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:455)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:301)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:159)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:166)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:307)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:451)
        ... 5 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:294)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 10 more
Exception encountered during startup.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:455)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:301)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:159)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:166)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:307)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:451)
        ... 5 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:294)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 10 more


","04/Feb/11 15:23;jbellis;It's a different bug.

Did you have any nodes down for part of the time (any hinted handoff going on?)",04/Feb/11 15:32;jbellis;I reverted CASSANDRA-1743 which is what caused CASSANDRA-2102.  I suspect it's causing this other problem too for reasons not yet understood.,"04/Feb/11 17:41;tbritz;I can't say for sure. I guess so.

Can you trigger a hudson build with the most recent fixes as well. (Don't know for sure which revision to take) So I can let it run over the weekend and check if any of the exceptions and bugs (these ones, cpu spike, consistency...) occurs again or not.


",04/Feb/11 19:41;jbellis;build scheduled,"05/Feb/11 03:03;mdennis;the commit logs have checksums in them to attempt to catch corrupted files.  C* is supposed to log that it is corrupted, skip over it and then continue.  The fact that *any* value inside a commit log can cause C* not to start is a bug of it's own.  I've opened CASSANDRA-2113 to track this.",05/Feb/11 13:56;jbellis;v2 is a less-invasive fix to the original problem.,"08/Feb/11 23:19;mdennis;I like the first patch better (recently rebased) as it adds a version field at the top and operates on an actual count instead of using in.available() which is not actually reliable in all cases.

In addition the rebase fixes a recently introduced bug that prevented the caches from getting saved where BufferedRandomAccessFile doesn't accept ""w"" as a mode and adds some comments in CacheWriter about how the totalBytes reported via ICompactionInfo is just an approximation.

If we don't want these other changes, +1 on the v2 patch.","10/Feb/11 03:29;jbellis;I'm not excited about the enumerated cache format change, because the benefit is nominal (if available() is wrong, then the cache is corrupt and we'll find out soon enough; knowing how many their SHOULD have been is of purely theoretical interest at that point) while the cost is making the cache useless for upgrades, which is the reason it was added in the first place.

Committed w/o the format change, but with the change to getTotalBytes to take the max of estimated/written.","10/Feb/11 04:34;hudson;Integrated in Cassandra-0.7 #273 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/273/])
    continue starting when invalid savedcache entries are encountered
patch by mdennis and jbellis for CASSANDRA-2076
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insert performance regression,CASSANDRA-1917,12494259,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,brandon.williams,brandon.williams,12/29/2010 17:52,3/12/2019 14:07,3/13/2019 22:24,12/29/2010 19:24,0.7.0,,,,0,,,,,,"We caused a performance regression in CASSANDRA-1780, costing us about 33% on inserts.",,,,,,,,,,,,,,,,,,,,29/Dec/10 17:58;jbellis;1917.txt;https://issues.apache.org/jira/secure/attachment/12467126/1917.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:13.4,,,no_permission,,,,,,,,,,,,20369,,,Wed Dec 29 19:24:11 UTC 2010,,,,,,0|i0g87j:,92760,brandon.williams,brandon.williams,,,,,,,,,29/Dec/10 17:58;jbellis;revert the default to without-flush mode,"29/Dec/10 19:24;jbellis;reverted CASSANDRA-1780 entirely instead.  Created CASSANDRA-1919 to provide a ""principle of least surprise"" solution for people restarting the server w/ periodic flushing configured.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Config converter fails,CASSANDRA-1655,12478153,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,stuhood,stuhood,stuhood,10/23/2010 16:27,3/12/2019 14:07,3/13/2019 22:24,10/23/2010 17:37,0.7 beta 3,,Legacy/Tools,,0,,,,,,"Trying to run the config converter for an 0.6.6 -> 0.7.0-rc1 upgrade failed with the following exception:
{code:java}
stuhood@stu-laptop:~/src/cassandra/apache-cassandra-0.7.0-rc1$ bin/config-converter storage-conf.xml cassandra.yaml
WARN : Thrift uses framed Transport by default in 0.7! Setting TFramedTransportSize to 0MB (disabled).
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:63)
	at org.apache.cassandra.config.Converter.readTablesFromXml(Converter.java:77)
	at org.apache.cassandra.config.Converter.loadPreviousConfig(Converter.java:308)
	at org.apache.cassandra.config.Converter.main(Converter.java:359)
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:389)
	... 4 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.config.KSMetaData.<init>(KSMetaData.java:50)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:349)
	... 4 more
Exception in thread ""PERIODIC-COMMIT-LOG-SYNCER"" java.lang.NoClassDefFoundError: Could not initialize class org.apache.cassandra.config.DatabaseDescriptor
	at org.apache.cassandra.db.commitlog.CommitLog$2.run(CommitLog.java:136)
	at java.lang.Thread.run(Thread.java:619)
{code}",,,,,,,,,,,,,,,,,,,,23/Oct/10 16:38;stuhood;0001-Move-static-methods-needed-for-config-conversion-to-.patch;https://issues.apache.org/jira/secure/attachment/12457904/0001-Move-static-methods-needed-for-config-conversion-to-.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,37:11.5,,,no_permission,,,,,,,,,,,,20242,,,Sun Oct 24 12:48:59 UTC 2010,,,,,,0|i0g6kn:,92495,jbellis,jbellis,,,,,,,,,23/Oct/10 16:38;stuhood;CFMetaData was using DatabaseDescriptor during construction.,"23/Oct/10 17:37;jbellis;committed, thanks!","24/Oct/10 12:48;hudson;Integrated in Cassandra #575 (See [https://hudson.apache.org/hudson/job/Cassandra/575/])
    avoid initializing DatabaseDescriptor during CFMetadata construction.  patch by Stu Hood; reviewed by jbellis for CASSANDRA-1655
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExpiringColumn wrongly inherits Column.getMarkedForDeleteAt,CASSANDRA-1539,12474925,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,9/23/2010 12:25,3/12/2019 14:07,3/13/2019 22:24,9/24/2010 15:30,0.7 beta 2,,,,0,,,,,,"An ExpiringColumn could be 'markedAsDeleted', but Column.getMarkedForDeleteAt() always throw an exception.",,,,,,,,,,,,,,,,,,,,23/Sep/10 12:26;slebresne;0001-Add-correct-getMarkedForDeleteAt-method-to-ExpiringC.patch;https://issues.apache.org/jira/secure/attachment/12455374/0001-Add-correct-getMarkedForDeleteAt-method-to-ExpiringC.patch,24/Sep/10 08:42;slebresne;0001-v2-Add-correct-getMarkedForDeleteAt-method-to-ExpiringC.patch;https://issues.apache.org/jira/secure/attachment/12455473/0001-v2-Add-correct-getMarkedForDeleteAt-method-to-ExpiringC.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,22:23.6,,,no_permission,,,,,,,,,,,,20186,,,Sat Sep 25 12:47:14 UTC 2010,,,,,,0|i0g5un:,92378,jbellis,jbellis,,,,,,,,,23/Sep/10 16:22;jbellis;is this something we can add a unit test for?,24/Sep/10 08:42;slebresne;v2 adds a unit test (with a 2 seconds sleep :( ),24/Sep/10 15:30;jbellis;committed,"25/Sep/10 12:47;hudson;Integrated in Cassandra #546 (See [https://hudson.apache.org/hudson/job/Cassandra/546/])
    treat expired columns as deleted.  patch by Sylvain Lebresne; reviewed by jbellis for CASSANDRA-1539
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException on startup after upgrade,CASSANDRA-1545,12475072,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jhermes,bterm,bterm,9/24/2010 19:33,3/12/2019 14:07,3/13/2019 22:24,9/27/2010 22:38,0.7 beta 2,,,,0,,,,,,"Running a cluster on trunk of 0.7.0beta-2 and updated to tip of trunk. On startup of node got the following NullPointerException. Was using r997774 and switched to r1000247

ERROR [main] 2010-09-22 12:30:14,110 AbstractCassandraDaemon.java (line 216) Exception encountered during startup.
java.lang.NullPointerException
    at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:373)
    at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:118)
    at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:106)
    at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:441)
    at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:109)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:54)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:199)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:133)",,,,,,,,,,,,,,,,,,,,27/Sep/10 21:21;jhermes;namechanges.txt;https://issues.apache.org/jira/secure/attachment/12455758/namechanges.txt,27/Sep/10 21:55;jhermes;nullprimitive.txt;https://issues.apache.org/jira/secure/attachment/12455763/nullprimitive.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,34:58.6,,,no_permission,,,,,,,,,,,,20192,,,Tue Sep 28 13:31:33 UTC 2010,,,,,,0|i0g5vz:,92384,stuhood,stuhood,,,,,,,,,"24/Sep/10 19:34;jhermes;Current working theory: looks like CASSANDRA-1437 rears it head, as it did in CASSANDRA-891.",24/Sep/10 22:34;jbellis;At worst blowing away your schema definitions in the system keyspace and recreating them should fix.,"25/Sep/10 00:35;jhermes;Yeah, I'm not sure what's causing the NPE after looking at the config and doing some debug testing.
If this is seen from beta1-> or from 0.6.5->, then it would be a major bug, but from an arbitrary trunk to current trunk is not so important.","25/Sep/10 00:40;jbellis;right, we've broken stuff at various revisions, and i don't think we can necessarily fix that retroactively.  as long as beta1 -> beta2 works i think we're fine.","27/Sep/10 21:21;jhermes;Going from beta1 to trunk has several problems.
The first of which is that we stored the old classnames for o.a.c.locator and are now reading them out and failing to find the matching classes.
We have special case logic in DD to fix this, so it's being pushed into CFMetaData.inflate().

Now I can repro this bug from beta1 ->.","27/Sep/10 21:55;jhermes;Was trying to jam a null into an int and not an Integer.
That was subtle.","27/Sep/10 21:56;jhermes;After both patches, on-disk from beta1 is read safely by beta2.","27/Sep/10 22:20;stuhood;+1
Works for me with the definitions in our default cassandra.yaml.",27/Sep/10 22:38;jbellis;pulled the replace ops into KSMD.convertOldStrategyName and committed,"27/Sep/10 22:38;bterm;+1 
looks good","28/Sep/10 13:31;hudson;Integrated in Cassandra #549 (See [https://hudson.apache.org/hudson/job/Cassandra/549/])
    fix reading beta1 schema from beta2.  patch by jhermes; reviewed by Stu Hood and jbellis for CASSANDRA-1545
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read failure during flush,CASSANDRA-1040,12463432,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,4/30/2010 13:33,3/12/2019 14:07,3/13/2019 22:24,5/7/2010 21:31,0.7 beta 1,,,,2,,,,,,"Joost Ouwerkerk writes:
	
On a single-node cassandra cluster with basic config (-Xmx:1G)
loop {
  * insert 5,000 records in a single columnfamily with UUID keys and
random string values (between 1 and 1000 chars) in 5 different columns
spanning two different supercolumns
  * delete all the data by iterating over the rows with
get_range_slices(ONE) and calling remove(QUORUM) on each row id
returned (path containing only columnfamily)
  * count number of non-tombstone rows by iterating over the rows
with get_range_slices(ONE) and testing data.  Break if not zero.
}

while this is running, call ""bin/nodetool -h localhost -p 8081 flush KeySpace"" in the background every minute or so.  When the data hits some critical size, the loop will break.",,,,,,,,,,,,,,,,,,,,07/May/10 21:14;jbellis;1040.txt;https://issues.apache.org/jira/secure/attachment/12443998/1040.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,32:56.9,,,no_permission,,,,,,,,,,,,19969,,,Fri May 07 21:31:52 UTC 2010,,,,,,0|i0g2mv:,91857,,,,,,,,,,,"06/May/10 03:28;jbellis;Brandon's code to reproduce:

{code}


#!/usr/bin/python
from telephus.protocol import ManagedCassandraClientFactory
from telephus.client import CassandraClient
from twisted.internet import defer

HOST = 'cassandra-6'
PORT = 9160
KEYSPACE = 'Keyspace1'
CF = 'Standard1'
SCF = 'Super1'
colname = 'foo'
scname = 'bar'

@defer.inlineCallbacks
def dostuff(client):
    while True:
        print ""inserting""
        for i in xrange(5000):
            yield client.insert(str(i), CF, 'test', column=colname)
        print ""removing""
        res = yield client.get_range_slice(CF, count=10000)
        for ks in res:
            if len(ks.columns) > 0:
                yield client.remove(ks.key, CF)
        print ""checking""
        res = yield client.get_range_slice(CF, count=10000)
        for ks in res:
            assert len(ks.columns) == 0
        print ""ok""

if __name__ == '__main__':
    from twisted.internet import reactor
    from twisted.python import log
    import sys
    log.startLogging(sys.stdout)

    f = ManagedCassandraClientFactory()
    c = CassandraClient(f, KEYSPACE)
    dostuff(c)
    reactor.connectTCP(HOST, PORT, f)
    reactor.run()
{code}
","06/May/10 03:41;jbellis;Also:

{code}
for x in seq `1 1000`; do bin/nodetool -h `hostname` flush Keyspace1; sleep 5; done
{code}","07/May/10 18:32;stuhood;Independent of (but related to) this issue, ColumnFamilyStore.getRangeRows has a race condition in memtable handling. The order of operations that might trigger the problem is:
# Copy memtablesPendingFlush
# (new memtable becomes pending)
# Copy reference to current Memtable

Swapping 3. with 1. would prevent new memtables from being ignored, but would mean we might scan one memtable twice. Making 1. and 3. atomic would remove the race, but is a longer time to hold the lock than we are use to.

EDIT: this description only applies to trunk","07/May/10 21:14;jbellis;Most of this was caused by the bug Stu found for CASSANDRA-1063, which has been committed separately.  Here is a patch to fix the trunk-only part explained above.  (We take the ""allow the original memtable to scanned twice occasionally"" approach, which is the one taken by getTopLevelColumns.)",07/May/10 21:26;stuhood;+1 for 1040.txt. Thanks!,07/May/10 21:31;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in consistency manager after a failed node rejoins,CASSANDRA-124,12424315,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,markr,markr,4/30/2009 16:32,3/12/2019 14:07,3/13/2019 22:24,5/5/2009 20:07,0.3,,,,0,,,,,,"ERROR [CONSISTENCY-MANAGER:2] 2009-04-30 18:22:38,946 DebuggableThreadPoolExecutor.java (line 89) Error in ThreadPoolExecutor
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.service.ConsistencyManager.run(ConsistencyManager.java:168)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more

Plus other similar ones.

Config:

    <ReplicationFactor>2</ReplicationFactor>
    <Tables>
        <Table Name=""Messages"">
            <ColumnFamily ColumnSort=""Name"" Name=""base""/>
            <ColumnFamily ColumnSort=""Name"" Name=""extra""/>
            <ColumnFamily ColumnSort=""Time"" Name=""StandardByTime1""/>
            <ColumnFamily ColumnSort=""Time"" Name=""StandardByTime2""/>
            <ColumnFamily ColumnType=""Super"" ColumnSort=""Name"" Name=""Super1""/>
            <ColumnFamily ColumnType=""Super"" ColumnSort=""Name"" Name=""Super2""/>
        </Table>
    </Tables>


I inserted some data using insert method on another node while one node had failed (of three), then brought the failed node back","Centos 5.0 java version ""1.6.0_13""
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,26:34.5,,,no_permission,,,,,,,,,,,,19557,,,Tue May 05 20:07:55 UTC 2009,,,,,,0|i0fx0v:,90948,,,,,,,,,,,"02/May/09 16:26;nk11;Shouldn't ConsistencyManager() constructor contain the following line?
this.replicas_ = replicas_;","04/May/09 14:49;jbellis;FWIW this is not a regression, it's only showing up in the logs now after I added a change that logs exceptions from our executors instead of ignoring them.","04/May/09 14:52;jbellis;Oops, I take it back.  It's a regression from CASSANDRA-95.  nk11 is right, the constructor got broken.","05/May/09 13:43;hudson;Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])
    do not leave variables uninitialized in ConsistencyManager constructor.  fixes regression from #95.  patch by jbellis for 
",05/May/09 20:07;jbellis;looks fixed in my testing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interrupted recovery requires manual intervention to fix,CASSANDRA-78,12422699,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,4/13/2009 15:02,3/12/2019 14:07,3/13/2019 22:24,5/5/2009 19:59,0.3,,,,0,,,,,,"Originally reported by Alexander Staubo: ""If you kill the server while it is going through its initial ""row recovery"" phase, you risk ending up with a database that's corrupt and will fail with ""negative seek"" exceptions and similar.""

Prashant replied:

""The commit logs are only deleted after a successful recovery. You should still have teh commit log if u killed the server while recovering ? When u restart the server it should generate a new file , for compactions we name intermediate files with a .tmp and only on successful dump do we place them as usable files , this same logic is required at recovery and there is a fix coming up which will do it .

""So with the state that u have today there will  be no data loss ass commit logs still exist but its a round about process to recover it since now u haave to delete the intermediate file and then do teh recovery again.""",,,,,,,,,,,,,,,,,,,,01/May/09 19:18;jbellis;0001-clean-up-anticompaction-code-a-little.patch;https://issues.apache.org/jira/secure/attachment/12407032/0001-clean-up-anticompaction-code-a-little.patch,01/May/09 19:18;jbellis;0002-use-getTempFileName-closeRename-to-avoid-problems.patch;https://issues.apache.org/jira/secure/attachment/12407033/0002-use-getTempFileName-closeRename-to-avoid-problems.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,52:33.7,,,no_permission,,,,,,,,,,,,19536,,,Tue May 05 19:59:09 UTC 2009,,,,,,0|i0fwqn:,90902,,,,,,,,,,,01/May/09 18:47;jbellis;Prashant's memory seems to be wrong here -- the only place getTempFileName is in the anticompaction (bootstrap) code.  Everything else uses getNextFileName directly.,"01/May/09 19:50;jbellis;while writing these patches I checked and closeRename does fsync (via FileChannel.force).  Have not audited commitlog similarly.

Additionally, the bootstrap code uses the potentially unsafe CFS.getFileName instead of getTempFileName.  Not sure if there's actually a problem there.  (Just a note to self to come back to this after 0.3)",04/May/09 15:52;urandom;+1,"05/May/09 13:43;hudson;Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])
    use getTempFileName / closeRename to avoid problems w/ half-written sstables.
patch by jbellis; reviewed by Eric Evans for 
clean up anticompaction code a little.
patch by jbellis; reveiewed by Eric Evans for 
",05/May/09 19:59;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra silently loses data when a single row gets large,CASSANDRA-7,12416995,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,3/16/2009 19:40,3/12/2019 14:07,3/13/2019 22:24,3/20/2009 21:29,,,,,0,,,,,,"When you insert a large number of columns in a single row, Cassandra silently loses some of these inserts.
This does not happen until the cumulative size of the columns in a single row exceeds several megabytes.

Say each value is 1MB large, 

insert(""row"", ""col0"", value, timestamp)
insert(""row"", ""col1"", value, timestamp)
insert(""row"", ""col2"", value, timestamp)
...
...
insert(""row"", ""col100"", value, timestamp)

Running: 
get_column(""row"", ""col0"")
get_column(""row"", ""col1"")
...
..
get_column(""row"", ""col100"")

The sequence of get_columns will fail at some point before 100. This was a problem with the old code in code.google also.
I will attach a small program that will help you reproduce this. 

1. This only happens when the cumulative size of the row exceeds several megabytes. 
2. In fact, the single row should be large enough to trigger an SSTable flush to trigger this error.
3. No OutOfMemory errors are thrown, there is nothing relevant in the logs.




","code in trunk, Red Hat 4.1.2-33,  Linux version 2.6.23.1-42.fc8, java version ""1.7.0-nio2""",,,,,,,,,,,,,,,,,,,16/Mar/09 19:44;sandeep_tata;BigReadWriteTest.java;https://issues.apache.org/jira/secure/attachment/12402305/BigReadWriteTest.java,16/Mar/09 20:06;sandeep_tata;dirty_bit_patch.txt;https://issues.apache.org/jira/secure/attachment/12402307/dirty_bit_patch.txt,16/Mar/09 20:22;sandeep_tata;dirty_bit_patch_v2.txt;https://issues.apache.org/jira/secure/attachment/12402308/dirty_bit_patch_v2.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,37:13.1,,,no_permission,,,,,,,,,,,,19511,,,Fri Mar 20 21:29:33 UTC 2009,,,,,,0|i0fwbr:,90835,,,,,,,,,,,"16/Mar/09 19:44;sandeep_tata;This program simply writes a bunch of data first and tries to read it all back.
If the write phase spans multiple SSTables, you will notice that the read phase fails with missing values.
The ""--numColumns"" needs to be large enough -- try something like 6000. It might take a couple of minutes to run the test.



","16/Mar/09 20:00;sandeep_tata;Another way to check that this is really a bug is to list the columns in the serialized SSTable. You will notice a large contiguous range of missing columns. The trunk does not have a ""show SSTable"" utility -- mine depends on a bunch of other code, I'll try and put one up soon.

Opening up the SSTable in a binary file viewer might be enough -- you'll see a large swath of zeroes in the middle where real data should be.

","16/Mar/09 20:06;sandeep_tata;This is a patch to the data-loss bug.

This seemingly simple fix caused a whole lot of headache :-)

The bug is in io.BufferedRandomAccess.writeAtMost

If it takes more than 2 invocations to write out the data, except for the first and last calls, the data is not flushed out to disk because the dirty bit is not set correctly.

BufferedRandomAccess.write(byte[] b, int off, int len) will work correctly if 1 or 2 iterations happen in the loop because the bit is set to true before first invocation and the bit is set to true in the end. The fix simply sets this bit to true after the call to  System.arraycopy.
","16/Mar/09 20:22;sandeep_tata;Also patches the element comment to say :

/* Write at most ""len"" bytes *from* ""b"" starting at .... */

Thanks to jbellis for pointing this out!","16/Mar/09 21:37;neophytos;The issue still persists. I remember sending a bug report about this (old repository). IIRC, the issue was that the ColumnIndexer would raise a java.util.ConcurrentModificationException when it tried to read the sortedSet_ from EfficientBidiMap while flushing. No exception is raised using the new repository  you still get zero-size Data files when you batch insert lots of simple and super columns in the same row without any throttle. 

The basic test I am using for this is as follows: I have a collection of 100000 content items. For each of the content items I batch insert (in the same row) 5000-10000 supercolumns each time for a total of about  a million different supercolumn names before I get the first zero-sized data file.  I have tried this with different MemtableSizeInMB, MemTableObjectCountInMillions (also tried with a smaller threshold), MemtableLifetimeInDays, and with your patch. Always, the same behavior (except when I use a throttle).

PS. The old repository had an issue with addColumn in ColumnFamily.java but that was not it, see: 
http://groups.google.com/group/cassandra-user/browse_thread/thread/329b7700ebda3072/2da827df755eb168","16/Mar/09 21:48;sandeep_tata;Neo:

This issue only deals with the serialization bug while flushing the memtable to disk. It has nothing to do with batch_insert, or supercolumn sizes, or thread-unsafe access of the sortedSet in EfficientBidiMap. 

Can you open a new ticked and attach a driver program that might help us reproduce the problem you're describing?","16/Mar/09 21:58;neophytos;(a) It happens when you insert a large number of columns in a single row
(b) Cassandra silently loses some of these inserts (batch inserts are also inserts). 
(c) This DOES happen when the threshold is violated (the cumulative size is only one of the reasons for the threshold to be violated)
(d) It is also while flushing the memtable to disk.

Yes, I can open a new ticket but it seemed relevant to this issue.",20/Mar/09 21:29;sandeep_tata;Fixed by svn commit: r756155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when a supercolumn is marked deleted, need to check for newer data in subcolumns",CASSANDRA-583,12441641,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,11/25/2009 5:50,3/12/2019 14:07,3/13/2019 22:24,12/3/2009 21:04,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,01/Dec/09 22:05;jbellis;583.patch;https://issues.apache.org/jira/secure/attachment/12426584/583.patch,25/Nov/09 05:52;jbellis;dumbfix.patch;https://issues.apache.org/jira/secure/attachment/12426072/dumbfix.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,39:15.9,,,no_permission,,,,,,,,,,,,19765,,,Sat Dec 05 12:34:26 UTC 2009,,,,,,0|i0fztr:,91402,,,,,,,,,,,"25/Nov/09 05:52;jbellis;brute-force fix (removeDeleted makes sure we don't return redundant data, but this could be inefficient)","01/Dec/09 22:05;jbellis;Here is the ""right"" fix, with a test case modification that catches the bug.",03/Dec/09 17:39;junrao;+1.,03/Dec/09 21:04;jbellis;committed,"05/Dec/09 12:34;hudson;Integrated in Cassandra #278 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/278/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mutation replies are not correctly deserialized by originator,CASSANDRA-120,12424213,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,4/29/2009 20:23,3/12/2019 14:07,3/13/2019 22:24,5/4/2009 19:05,0.3,,,,0,,,,,,"these lines in WriteResponseResolver

			Object[] body = response.getMessageBody();
			WriteResponse writeResponse = (WriteResponse) body[0];

cause this exception

java.lang.ClassCastExceptionException: [B cannot be cast to org.apache.cassandra.db.WriteResponse
	at org.apache.cassandra.service.WriteResponseResolver.resolve(WriteResponseResolver.java:50)
	at org.apache.cassandra.service.WriteResponseResolver.resolve(WriteResponseResolver.java:31)
	at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:101)
	at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:132)

because of course only byte[] is sent over the wire",,,,,,,,,,,CASSANDRA-34,,,,,,,,,30/Apr/09 19:58;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-120-demonstrate-the-problem-by-checking-inse.txt;https://issues.apache.org/jira/secure/attachment/12406946/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-120-demonstrate-the-problem-by-checking-inse.txt,30/Apr/09 19:58;jbellis;ASF.LICENSE.NOT.GRANTED--0002-add-constructor-to-DTPE-for-most-commonly-used-values.txt;https://issues.apache.org/jira/secure/attachment/12406947/ASF.LICENSE.NOT.GRANTED--0002-add-constructor-to-DTPE-for-most-commonly-used-values.txt,30/Apr/09 19:58;jbellis;ASF.LICENSE.NOT.GRANTED--0003-refactor-HttpConnection-to-use-its-own-executor-instea.txt;https://issues.apache.org/jira/secure/attachment/12406948/ASF.LICENSE.NOT.GRANTED--0003-refactor-HttpConnection-to-use-its-own-executor-instea.txt,30/Apr/09 19:58;jbellis;ASF.LICENSE.NOT.GRANTED--0004-r-m-DataImporter-it-s-not-useful-outside-of-FB.txt;https://issues.apache.org/jira/secure/attachment/12406949/ASF.LICENSE.NOT.GRANTED--0004-r-m-DataImporter-it-s-not-useful-outside-of-FB.txt,30/Apr/09 19:58;jbellis;ASF.LICENSE.NOT.GRANTED--0005-change-Message-body-to-byte-.-this-reveals-where-the.txt;https://issues.apache.org/jira/secure/attachment/12406950/ASF.LICENSE.NOT.GRANTED--0005-change-Message-body-to-byte-.-this-reveals-where-the.txt,,,,,,,,,5,,,,,,,,,,,,,,,,,,,31:14.3,,,no_permission,,,,,,,,,,,,19555,,,Tue May 05 13:43:26 UTC 2009,,,,,,0|i0fwzz:,90944,,,,,,,,,,,"30/Apr/09 19:59;jbellis;05
    change Message body to byte[].  this reveals where there are problems: everything thatis using any of the send messages already needs to be doing that or they're broken.  the good ones I just unwrap; the broken ones I fixed except for MoveMessage which doesn't even have a byte[] serializer yet, so I left that one broken.  writeResponseResolver is the specific case that caused the bug report.

04
    r/m DataImporter; it's not useful outside of FB

03
    refactor HttpConnection to use its own executor instead of abusing MessagingService.  This will let us refactor Message body to a byte[].

02
    add constructor to DTPE for most commonly used values (single thread, no timeout, LinkedBlockingQueue)

01
    CASSANDRA-120 demonstrate the problem by checking insert return values (see CASSANDRA-102 for how to run the system tests)
","04/May/09 18:31;nk11;Patches seem ok.
Tested also for one node only and error did not reproduce.",04/May/09 19:05;jbellis;thanks for the review!  committed.,"05/May/09 13:43;hudson;Integrated in Cassandra #59 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/59/])
    change Message body to byte[].  this reveals where there are problems: everything that is using any of the send messages already needs to be doing that or they're broken.  the good ones I just unwrap; the broken ones I fixed except for MoveMessage which doesn't even have a byte[] serializer yet, so I left that one broken.  writeResponseResolver is the specific case that caused the bug report.
patch by jbellis; reviewed by nk11 for 
r/m DataImporter; it's not useful outside of FB
patch by jbellis; reviewed by nk11 for 
refactor HttpConnection to use its own executor instead of abusing MessagingService.  This will let us refactor Message body to a byte[].
patch by jbellis; reviewed by nk11 for 
add constructor to DTPE for most commonly used values (single thread, no timeout, LinkedBlockingQueue)
patch by jbellis; reviewed by nk11 for 
demonstrate problem with _blocking methods by checking insert return values.
patch by jbellis; reviewed by nk11 for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple migrations might run at once,CASSANDRA-1292,12469498,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,gdusbabek,stuhood,stuhood,7/16/2010 21:58,3/12/2019 14:07,3/13/2019 22:24,7/28/2010 18:02,0.7 beta 1,,,,0,,,,,,"The service.MigrationManager class manages a MIGRATION_STAGE where nodes should execute db.migration.Migration instances.

The problem is that the node that a client connects to via Thrift or Avro initiates the migration in their client thread (calls migration.apply). Instead, the Thrift and Avro clients should ensure that the migration occurs in MIGRATION_STAGE, and should block until the migration is applied by the stage.",,,,,,,,,,,,,,,,,,,,28/Jul/10 16:12;gdusbabek;0001-run-thrift-and-jmx-migrations-on-to-migration-stage.patch;https://issues.apache.org/jira/secure/attachment/12450713/0001-run-thrift-and-jmx-migrations-on-to-migration-stage.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,56:04.6,,,no_permission,,,,,,,,,,,,20063,,,Thu Jul 29 13:14:13 UTC 2010,,,,,,0|i0g467:,92106,,,,,,,,,,,27/Jul/10 20:56;gdusbabek;same thing goes for loadSchemaFromYaml(),28/Jul/10 17:23;stuhood;+1,"29/Jul/10 13:14;hudson;Integrated in Cassandra #503 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/503/])
    run thrift and jmx migrations on migration stage. patch by gdusbabek, reviewed by stuhood. CASSANDRA-1292
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BytesType and batch mutate causes encoded bytes of non-printable characters to be dropped,CASSANDRA-1235,12468005,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,messi,tnine,tnine,6/28/2010 3:02,3/12/2019 14:07,3/13/2019 22:24,8/13/2010 17:26,0.6.5,,,,0,,,,,,"When running the two tests, individual column insert works with the values generated.  However, batch insert with the same values causes an encoding failure on the key.  It appears bytes are dropped from the end of the byte array that represents the key value.  See the attached unit test","Java 1.6 sun JDK 
Java(TM) SE Runtime Environment (build 1.6.0_20-b02)
Java HotSpot(TM) 64-Bit Server VM (build 16.3-b01, 

Ubuntu 10.04 64 bit",,,,,,,,,,CASSANDRA-767,,,,,,,,,12/Aug/10 19:56;univ;TestByteKeys.py;https://issues.apache.org/jira/secure/attachment/12451944/TestByteKeys.py,28/Jun/10 03:03;tnine;TestEncodedKeys.java;https://issues.apache.org/jira/secure/attachment/12448170/TestEncodedKeys.java,31/Jul/10 16:28;messi;rowmutation-key-trimming.patch;https://issues.apache.org/jira/secure/attachment/12450963/rowmutation-key-trimming.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,30:11.6,,,no_permission,,,,,,,,,,,,20041,,,Fri Aug 13 18:29:49 UTC 2010,,,,,,0|i0g3tj:,92049,,,,,,,,,,,28/Jun/10 03:03;tnine;This file demonstrates the broken input.  Notice that the first test passes with clean input.  The second one fails utilizing batch write for the same input keys.,28/Jun/10 15:30;jbellis;Please don't mess with the issue metadata.,"28/Jun/10 19:38;tnine;No worries, sorry about that, I just realized the affected version was incorrect.  Where can I look to begin fixing this? Unfortunately this issue has caused our development to a halt since we depend on the functionality of numeric range queries in Lucene/Lucandra.  Ideally I'd like to create a patch that applies to 0.6.2 so we can roll our own build with the patch and get running again.  I'm assuming it's an issue with the thrift server, but I don't want to start tweaking things without a good idea on where I should be looking for this issue.

Here's an example in hex.  The left is what I pass as bytes in UTF-8 for the key, the right is what I get back during get_range_slice.

http://pastebin.com/KM8Ze794
","08/Jul/10 15:27;jbellis;I believe that

                return new String(buffer, 0, len);

will treat buffer as UTF-16, not UTF-8.  you want

                return new String(buffer, 0, len, ""UTF8"");

I'm not at all sure that longToPrefixCoded is going to generate valid UTF-8, either.","08/Jul/10 16:15;thetaphi;buffer is char[], so there is no conversion at all, new String(char[]) only copies the char[] to the internal String's char[]. longToPrefixCoded is definitely correct, large parts of Lucene Java are based on this :-)

(from the Lucene Generics and Unicode Policeman)","08/Jul/10 19:45;tnine;While I'm in agreement with Uwe, my bigger concern is that two tests that are functionally equivalent return different results based on the mutation operations.  Performing a batch mutate with the same insertion data as a single write should insert and the same bytes.  Unfortunately batch mutate appears to be randomly dropping bytes.  If it were a true UTF8 issue, wouldn't it drop bytes on the single column writes as well batch mutate?","08/Jul/10 20:21;jbellis;That has to be a Thrift bug, then -- the insert and batch_mutate method both end up calling StorageProxy.mutate or mutateBlocking after converting the Thrift objects into RowMutations","26/Jul/10 21:47;todd@spidertracks.co.nz;I'm currently out of the office and will return on 2010-07-27.  If
this is an urgent request, please mail support@spidertracks.com.
",31/Jul/10 16:28;messi;This patch fixes the problem but I don't know if other problems will arise.,"01/Aug/10 01:42;jbellis;nice fix.  (It's possible that this would break someone relying on it, but it's clearly broken the way it is.)  committed.","12/Aug/10 19:58;univ;Still experiencing some problems with byte keys. The file ""TestByteKeys.py"" demonstrates the problem.
Tested with revision 984926.","13/Aug/10 17:26;jbellis;0.6 row keys are _strings_ which means they must be utf-8 encoded, although your version of thrift for python doesn't enforce that (see THRIFT-395).","13/Aug/10 18:29;thetaphi;For Lucandra/Lucene this is fine, too (at the moment, as all terms are strings here. Even binary numbers are correcty UTF-8 encoded terms).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM intermittently during compaction,CASSANDRA-208,12426691,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,jihuang,jihuang,5/29/2009 16:49,3/12/2019 14:07,3/13/2019 22:24,6/9/2009 17:47,0.4,,,,0,,,,,,"jvm crashes intermittently during compaction. Our test data set is not that big, less than 10 GB.
When jvm is about to crash, we see that it consumes a lot of memory (exceeding the max heap size).

The excessive memory usage during compaction is caused by the maintenance of blockIndexes_ in SSTable. this blockIndexes_ was only introduced to the apache version.","arch: x86_64
os: Linux version 2.6.18-92.1.22.el5 
java: nio2-ea-bin-b99-linux-x64-05_feb_2009
",,,,,,,,,,,,,,,,,,,05/Jun/09 03:19;jbellis;0004-fix-ColumnReader-add-tests.patch;https://issues.apache.org/jira/secure/attachment/12409922/0004-fix-ColumnReader-add-tests.patch,05/Jun/09 17:11;jbellis;0005-fix-test-order-dependent-failures.patch;https://issues.apache.org/jira/secure/attachment/12410003/0005-fix-test-order-dependent-failures.patch,08/Jun/09 21:56;jbellis;208-2.patch;https://issues.apache.org/jira/secure/attachment/12410180/208-2.patch,09/Jun/09 15:39;jbellis;208-3.patch;https://issues.apache.org/jira/secure/attachment/12410228/208-3.patch,08/Jun/09 21:55;jbellis;208.patch;https://issues.apache.org/jira/secure/attachment/12410179/208.patch,04/Jun/09 22:30;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-208-cleanup.txt;https://issues.apache.org/jira/secure/attachment/12409910/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-208-cleanup.txt,04/Jun/09 22:30;jbellis;ASF.LICENSE.NOT.GRANTED--0002-r-m-touch.txt;https://issues.apache.org/jira/secure/attachment/12409911/ASF.LICENSE.NOT.GRANTED--0002-r-m-touch.txt,04/Jun/09 22:30;jbellis;ASF.LICENSE.NOT.GRANTED--0003-split-sstable-into-data-index-and-bloom-filter-files.txt;https://issues.apache.org/jira/secure/attachment/12409912/ASF.LICENSE.NOT.GRANTED--0003-split-sstable-into-data-index-and-bloom-filter-files.txt,,,,,,8,,,,,,,,,,,,,,,,,,,31:04.6,,,no_permission,,,,,,,,,,,,19594,,,Wed Jun 10 13:12:06 UTC 2009,,,,,,0|i0fxj3:,91030,,,,,,,,,,,"29/May/09 17:31;junrao;In your data set, do you have a large row? Today, during compaction, a full row may have to be buffered in memory. See https://issues.apache.org/jira/browse/CASSANDRA-16 for more on this.","29/May/09 19:43;jihuang;we don't have a large row, definitely not to a size which cannot fit in memory.","29/May/09 23:25;jbellis;blockindexes_ keeps the indexes in memory as the flush or compaction is performed, so they can be dumped at the end of the SSTable file.

The old code on code.google dumped each index at the end of its block which avoided this problem (only one block's index would ever be in memory at once).  I'm not sure why FB changed this behavior.  The only reason I can think of is that reading the index later won't require as many seeks, but this seems like a bad optimization to make since index reading is done once per sstable.

I couldn't tell at first why this should cause OOM -- in both the google and apache versions we have indexMetadataMap_ which contains the same block index info, right?  Wrong, the difference is that iMM only contains the first entry from each block index.  So much less memory is used there.

I guess we should switch back to the old, interleaved block indexing method.","01/Jun/09 23:22;jbellis;Or, how about this: we just stop storing multiple kinds of data in the SSTable file, and instead store the index entries and the bloom filter in separate on-disk files.

Gains (vs interleaving):
 - simplicity (don't have to skip non-data keys, EOF is really EOF)
 - no more hard-coded ""keys"" in the sstable that will result in very weird-ass bugs if a client every uses one
 - behaves more like the FS cache expects since each section (index, data, bloom) is homogeneous and the ""if I read block A, I'm more likely to need the next block"" assumption holds more often
 - retains goal of loading index on server start w/o seeking

Losses:
 - some seeking during flush/compaction to switch between writing data blocks and index blocks

Although the ""no seeks on writes, at all"" claim is a cool one, in practice the amount of seeks we'll be doing is still negligible when buffering is done, i.e., still a huge win over traditional B-tree design where every update requires a seek.

Thoughts?","01/Jun/09 23:24;jbellis;IMO we need to address the hard-coded-""keys"" problem sooner or later, and having separate files is really the only sane solution.","02/Jun/09 01:22;junrao;Even if we take out the row index and BF, data is still mixed with column index.

Interestingly, HBase's sstable format started with MapFile (1 file for index, 1 file for BF and 1 file for data) and has recently moved to the TFile (a spec can be found at https://issues.apache.org/jira/browse/HADOOP-3315) format (1 file for everything). Although HBase probably did that mainly for reducing # of files (problem for HDFS master), it's probably worth while to take a look at their new design (https://issues.apache.org/jira/browse/HBASE-61).","02/Jun/09 01:41;jbellis;Thanks for the links, that is interesting context.","02/Jun/09 02:06;jbellis;Another related link: http://wiki.apache.org/hadoop/Hbase/NewFileFormat

My take is that the designs are different enough that their reasons for moving to a single file don't really apply to cassandra.

 - the old MapFile has a bunch of properties that make it general enough for Hadoop core but inefficient for hbase (e.g. storing the CF name once per key, keys appearing multiple times in the index)
 - they only index one key per block, so their index is much much smaller than ours, and they can get away with storing the index at the end of the file as cassandra currently does

> Even if we take out the row index and BF, data is still mixed with column index. 

Not at the SSTable key/value level.  To sstable the value is just byte[] so the fact that CF serializes with indexes is an implementation detail.  (To the degree that SSTable or SF does care, that is an encapsulation violation -- one of the reasons this code is one of the less pleasant parts of cassandra to work in.)

I will get a patch together that will implement the file splitting I proposed and we will see how that looks.  I think that's going to get us to a stable 0.3 fastest; if we want to radically re-think how indexing works (so we can go back to index-at-the-end-of-one-file) then I think that is a change to make in 0.4.  (The non-sparse index Cassandra uses may be necessary if you want to support large CF rows, or you will waste too much time scanning through those rows looking for keys when you only get to within 128 keys from the index.)

One thing that piqued my curiosity: what is the hbase ""row index?""  Looks like their ""key index"" is like our sstable indexes (with the difference mentioned above, that it only indexes one key per block).","02/Jun/09 15:34;junrao;When HBase was using the old format, rows are stored as <key,value> pairs in MapFile, where a key is rowkey:columnName:timestamp. The keys for every 128 rows are promoted to the row index. The benefit is that it's simple. There is only a single-level index (compared with row index and column index within a row in cassandra) and it can be used to efficiently look up a full row, a column within a row, or a version of a column in a row. On the other hand, if you make the index dense, the row keys are duplicated for columns within the same row.","03/Jun/09 21:39;jbellis;03
    split sstable into data, index, and bloom filter files

02
    r/m 'touch' code that populates a cache that is never used (and never updated on compaction either, so it's buggy too)

01
    cleanup.  mostly just moving code around so its position makes more sense to me
","03/Jun/09 23:03;junrao;can't apply patch to trunk. could you rebase?
","03/Jun/09 23:35;junrao;just realize this is off 0.3 branch, never mind.","04/Jun/09 15:51;jbellis;Jiansheng: can you give us more details about your data set?  We are trying to figure out how likely it is for others to run into this, leaning towards moving this patch to trunk/0.4.

The relevant data are
 - how many keys in the CF that fails to compact
 - average / max row size (column size x count) in that CF
 - jvm memory settings, and how much of the heap is free (according to e.g. jconsole) before compaction starts","04/Jun/09 17:59;junrao;Just browsed through the patch for now and found the following bug.

In SSTable.afterAppend, the position added to indexEntries should be the position in the index file, not the position in the data file.
","04/Jun/09 18:14;jbellis;Good catch.

I will add a test for this in the version for trunk.","04/Jun/09 18:24;jihuang;To answer Jonathan's questions: (1) we have tested around several million keys, (2) avg row size ~ 2k, max row size ~ 10k. (3) tried several Xmx settings, max tried is 5G. Before compaction starts, pretty much most of the heap is free. I think the problem is easy to run into if the system is run with continuous traffic for over a week or so given that our test dataset has been fairly small.","04/Jun/09 18:53;junrao;Jiansheng, what's the average key size of a row?
","04/Jun/09 19:18;jihuang;Jun, our key size is about 20 characters.","04/Jun/09 20:18;junrao;Hmm, with a key size of 40 bytes, even with 10 million keys, the space needed for all row index entries is about 400MB. That's way smaller than 5G. So, not sure whether the new SSTable format really solves this particular OOME problem.
","04/Jun/09 20:54;jbellis;You're forgetting the BlockMetadata longs (extra 16 bytes per key) and Map overhead (32 bytes per key or more), which would be 880MB for 10M keys.

Still smaller than 5G, though.  I guess we can get Jiansheng to test after applying the patch. :)","04/Jun/09 22:31;jbellis;removed patches against 0.3 (to avoid confusion, i still have copies should they become necessary).

added patches against trunk.","04/Jun/09 23:07;jihuang;Ok, then scale up the number of keys by 5 times, you will run into the problem. :)","04/Jun/09 23:35;junrao;Some comments:
1. In SSTable.loadIndexFile, indexEntries could be empty if there are fewer than indexInterval keys. I don't see code dealing with empty indexEntries. A simple solution is to add the last index key to indexEntries when indexEntries is empty. Similarly, you have to fix the same case when an SSTable is first created. You have to remember the last index key being appended.

2. In SSTable.next, need to deal with the case that getPosition() returns -1. 

3. In SSTable.getColumnGroupReader, change getNearestPostion() to getPostion(). Similar to 2, need to deal with returned position being -1.


Also, we need to remember opening another issue to get rid of partitioner from KeyPosition.

","05/Jun/09 01:08;jbellis;1. empty memtables are not flushed, so there will be at least one key, and the first key will always be in indexEntries since 0 % interval == 0 for any interval.  lots of the tests have only one row.  I don't think there is a problem here.

2. ok

3. Hmm, if you have columns B C and D in a row, and you do a slice_from starting with A, don't you want B C D to be returned?  getNearestPosition will start the scan at B, but vanilla getPosition will return -1 if there is not an exact match.","05/Jun/09 01:23;junrao;3. That's true, but at the column level. At the row level, get_slice_from only cares about exact row match.","05/Jun/09 01:26;jbellis;Ah, thanks.  I got mixed up. :)","05/Jun/09 03:19;jbellis;2. is handled by FileReader.next being a no-op if position == -1

3. attached 04 patch to fix, and add more tests.  Also, makes sliceFrom able to handle """" as start column.","05/Jun/09 15:47;junrao;Were you able to run unit tests successfully? I saw a bunch of failures in tests like CompactionsTest and NameSortTest.
","05/Jun/09 17:11;jbellis;I did have NameSortTest and TimeSortTest failing because they were extending CFST and hence running those tests as well, which caused problems since those sets of tests are supposed to be run independently.  Making NST and TST not inherit from CFST (which was only done for convenience) fixes that.   Here is the patch.

I haven't seen compactions et al ever fail, no.","05/Jun/09 18:03;junrao;My ant test succeeds on linux, but fails under cygwin on windows. Here are some of the failures that I get. Those tests succeed on windows without your patch.

 [junit] Testcase: testCompactions(org.apache.cassandra.db.CompactionsTest): FAILED
 [junit] attempted to delete non-existing file Table1-Standard1-14-Filter.db
 [junit] junit.framework.AssertionFailedError: attempted to delete non-existing file Table1-Standard1-14-Filter.db
 [junit]     at org.apache.cassandra.io.SSTable.deleteWithConfirm(SSTable.java:103)
 [junit]     at org.apache.cassandra.io.SSTable.delete(SSTable.java:98)
 [junit]     at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:1373)
 [junit]     at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:849)
 [junit]     at org.apache.cassandra.db.CompactionsTest.testCompactions(CompactionsTest.java:63)

[junit] Testcase: testNameSort10(org.apache.cassandra.db.NameSortTest):     FAILED
[junit] null
[junit] junit.framework.AssertionFailedError
[junit]     at org.apache.cassandra.db.NameSortTest.validateNameSort(NameSortTest.java:111)
[junit]     at org.apache.cassandra.db.NameSortTest.testNameSort(NameSortTest.java:89)
[junit]     at org.apache.cassandra.db.NameSortTest.testNameSort10(NameSortTest.java:43)",08/Jun/09 16:17;jbellis;do those tests pass after applying just 01 and 02?,08/Jun/09 17:50;junrao;The tests pass with just 01 and 02. They start failing with 03.,08/Jun/09 19:45;jbellis;committed 01 to avoid rebase hell.  (02 was merged via CASSANDRA-222 from 0.3.),"08/Jun/09 21:55;jbellis;208.patch integrates the old 03, 04, and 05 for convenience","08/Jun/09 21:56;jbellis;208-2 fixes syncing of the bloom filter files.  this fixes the compactions failure and all the others except timesort, which is not actually a new bug.

CASSANDRA-223 shows that the timesort failure is caused by a bug in TimeFilter that dates back to the FB import.","08/Jun/09 22:25;junrao;Ok, now all tests pass except timesort.
","09/Jun/09 12:34;hudson;Integrated in Cassandra #103 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/103/])
    cleanup SSTable-related code.  patch by jbellis; reviewed by Jun Rao for 
","09/Jun/09 15:39;jbellis;208-3 patches TST to only test what it was intended to test, not CASSANDRA-223.",09/Jun/09 17:21;junrao;Patch 208-3 looks fine. All tests pass.,"09/Jun/09 17:47;jbellis;committed 208 and 208-2 as a single commit, and 208-3 separately.

jiansheng, does this resolve your OOM problems?","10/Jun/09 13:12;hudson;Integrated in Cassandra #104 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/104/])
    apply rows atomically, rather than one-column-at-a-time.  this avoids exposing the bug in time-sorted
columns discussed in #223.
patch by jbellis; reviewed by Jun Rao for 
split sstable into data, index, and bloom filter files.  this allows us to avoid saving up index chunks
in memory until the sstable data is completely written, while still allowing fast scanning of the index
on startup.  it also simplifies the sstable/sequencefile code considerably.
patch by jbellis; reviewed by Jun Rao for  
",,,,,,,,,,,,,,,,,,,,
AES makes Streaming unhappy,CASSANDRA-1169,12466310,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,gdusbabek,jbellis,jbellis,6/7/2010 3:41,3/12/2019 14:07,3/13/2019 22:24,6/14/2010 18:40,0.6.3,0.7 beta 1,,,0,,,,,,"Streaming service assumes there will only be one stream from S to T at a time for any nodes S and T.  For the original purpose of node movement, this was a reasonable assumption (any node T can only perform one move at a time) but AES throws off streaming tasks much more frequently than that given the right conditions, which will de-sync the fragile file ordering that Streaming assumes (that T knows which files S is going to send, in what order).  Eventually T is expecting file F1 but S sends a smaller file F2, leading to an infinite loop on T while it waits for F1 to finish, and T waits for S to acknowledge F2, which it never will.

For 0.6 maybe the best solution is for AES to manually wait for one of its streaming tasks to finish, before it allows itself to create another.  For 0.7 it would be nice to make Streaming more robust.  The whole 4-stage-ack process seems very fragile, and poking around in parent objects via inetaddress keys makes reasoning about small pieces impossible b/c of encapsulation violations.",,,,,,,,,,,,,,,,,,,,14/Jun/10 17:55;gdusbabek;1169-2.txt;https://issues.apache.org/jira/secure/attachment/12447046/1169-2.txt,11/Jun/10 14:33;gdusbabek;1169.txt;https://issues.apache.org/jira/secure/attachment/12446866/1169.txt,09/Jun/10 14:32;appodictic;aes.txt;https://issues.apache.org/jira/secure/attachment/12446693/aes.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,04:13.3,,,no_permission,,,,,,,,,,,,20014,,,Mon Jun 14 18:40:07 UTC 2010,,,,,,0|i0g3f3:,91984,,,,,,,,,,,07/Jun/10 12:04;gdusbabek;Is this the root cause of the problems being experienced at Digg and by Lu Ming on the ML?,07/Jun/10 14:17;jbellis;I think so.  That's what prompted this ticket.,"07/Jun/10 16:53;lenn0x;As mentioned on IRC, we actually saw T sending to S, while anticompaction was running on S using the exact filename as T. We should probably break streaming out to support `stream/<source>/<sstables>`. ",09/Jun/10 14:19;appodictic;I have this problem as well. I have a 5 node cluster. A simple repair on keyspace1 (which has nothing but some test data) streams never complete.,"09/Jun/10 14:32;appodictic;I have upgraded to 6.2 because 6.1 streaming would randomly timeout on me. Now, I am still having issues with move, join, repair. Since I was having so many streaming problems I tuned this up in some logs. Over the past few weeks I have spent a lot of time managing my clusters, I try to do these type of operations in the AM so they are less performance impacting, but I have a very low sucess rate with any move,join,repair. I have a building list of nodes to join and ring management that I keep having to put off due to failures. So anything to make these processes less brittle would be a big big deal. Attached is ooutput.
 ",11/Jun/10 14:33;gdusbabek;Ensures that AES streaming happens on the streaming stage and waits for each transfer to complete.,"11/Jun/10 15:56;jbellis;you don't need to do that exception dance with futures, it will throw an exception that happened in the background as a wrapped ExecutionException on get (and all our executors are DebuggableTPE, which makes sure the exception gets logged even if get() is never called)

LGTM otherwise","12/Jun/10 08:21;albert_e;StreamOutManager.waitForStreamCompletion() can't block the AES streaming thread if StreamOutManager.condition is signaled once and StreamOutManager has not been removed from streamManagers map. 

Make StreamOutManager.addFilesToStream() synchronized and block the thread if StreamOutManager.files.size() > 0 may be more efficient.","12/Jun/10 08:29;xluke;I have applied your patch to cassandra.
According to my log on StreamOutManager.addFilesToStream() , the function is still called when StreamOutManager.files.size() > 0 
I think the problem is maybe not fixed yet.","12/Jun/10 10:57;xluke;After I applied the above patch,
StreamOutManager.waitForStreamCompletion()  return immediately and StreamOut.transferSSTables do Not  wait for its streaming tasks to finish

27938- INFO [STREAM-STAGE:1] 2010-06-12 17:08:48,810 StreamOut.java (line 132) Sending a stream initiate message to /121.1.1.1...
27939: INFO [STREAM-STAGE:1] 2010-06-12 17:08:48,810 StreamOut.java (line 137) Waiting for transfer to /121.1.1.1 to complete
27940- INFO [STREAM-STAGE:1] 2010-06-12 17:08:48,810 StreamOut.java (line 141) Done with transfer to /121.1.1.1
27941- INFO [AE-SERVICE-STAGE:1] 2010-06-12 17:08:48,811 AntiEntropyService.java (line 641) Finished streaming repair to /121.1.1.1 for (GroupDataStore,Group)
..................................
27982- INFO [STREAM-STAGE:1] 2010-06-12 17:19:22,066 StreamOut.java (line 132) Sending a stream initiate message to /222.222.2.2 ...
27983: INFO [STREAM-STAGE:1] 2010-06-12 17:19:22,066 StreamOut.java (line 137) Waiting for transfer to /222.222.2.2 to complete
27984- INFO [STREAM-STAGE:1] 2010-06-12 17:19:22,066 StreamOut.java (line 141) Done with transfer to /222.222.2.2
27985- INFO [AE-SERVICE-STAGE:1] 2010-06-12 17:19:22,067 AntiEntropyService.java (line 641) Finished streaming repair to /222.222.2.2 for (GroupChat,Topic)
..................................",14/Jun/10 17:55;gdusbabek;Instruct AES to remove active StreamManager when it's finished and reset the condition in SOM any time files are added so that it is a bit more reentrant.,"14/Jun/10 17:56;gdusbabek;albert_e: patch 2 adjusts addFilesToStream to reset the condition so that future waiters do wait.
Lu Ming: I believe you were experiencing that problem.","14/Jun/10 18:04;jbellis;won't removing the active SOM bork things, if another stream to that target is going on?",14/Jun/10 18:20;gdusbabek;SOM.remove() checks for that.  (I'm not saying the code is perfect--it's not--but I don't think removing the SOM is going to mess things up.),"14/Jun/10 18:20;stuhood;Rather than an 'endpoint->StreamManager' map, we really should have a 'session_id->StreamManager' map.",14/Jun/10 18:26;gdusbabek;Stu: Right.  But I don't think we want to introduce it in 0.6.  I'm hoping just to get things to the point of working and then fix it all in 0.7. ,14/Jun/10 18:34;jbellis;+1 on this patch and do-what-we-can-in-0.6-and-eviscerate-this-crap-in-0.7 in general,14/Jun/10 18:40;gdusbabek;both patches are committed and merged.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Addition of internode buffering broke Streaming,CASSANDRA-1943,12494805,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,,stuhood,stuhood,1/6/2011 3:18,3/12/2019 14:07,3/13/2019 22:24,1/6/2011 23:34,0.6.10,0.7.1,,,0,,,,,,Adding internode buffering broke StreamingTransferTest in the 0.7 branch. Bisected to r1055313,,,,,,,,,,,,,,,,,,,,06/Jan/11 22:58;stuhood;0.6-0001-Don-t-begin-buffering-a-connection-until-we-ve-determi.txt;https://issues.apache.org/jira/secure/attachment/12467684/0.6-0001-Don-t-begin-buffering-a-connection-until-we-ve-determi.txt,06/Jan/11 04:33;stuhood;0001-Don-t-begin-buffering-a-connection-until-we-ve-determi.txt;https://issues.apache.org/jira/secure/attachment/12467613/0001-Don-t-begin-buffering-a-connection-until-we-ve-determi.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,45:08.8,,,no_permission,,,,,,,,,,,,20378,,,Fri Jan 07 01:38:13 UTC 2011,,,,,,0|i0g8cv:,92784,gdusbabek,gdusbabek,,,,,,,,,"06/Jan/11 04:33;stuhood;Streaming connections don't use the InputStream implementation to read their data: they bypass all buffering and use the SocketChannel directly. By buffering immediately after opening the connection, we were buffering in the beginning of the streamed file.",06/Jan/11 04:36;stuhood;Also affects trunk.,"06/Jan/11 07:09;stuhood;Actually, the IOError we used to throw in the constructor should probably become a log statement like the other exceptions in run().",06/Jan/11 21:45;gdusbabek;+1,06/Jan/11 22:58;stuhood;And a version rebased for 0.6,06/Jan/11 23:34;brandon.williams;Committed.,"07/Jan/11 01:38;hudson;Integrated in Cassandra-0.6 #48 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/48/])
    Don't begin buffering a connection until we've determined the type.
Patch by Stu Hood, reviewed by gdusbabek for CASSANDRA-1943
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming should hold a reference to the source SSTR to prevent GC races,CASSANDRA-1749,12479993,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,stuhood,stuhood,stuhood,11/15/2010 20:20,3/12/2019 14:07,3/13/2019 22:24,11/18/2010 1:37,0.7.0 rc 1,,,,0,,,,,,"An SSTable waiting to be streamed will be GC'd and deleted from disk if there are no references being held to its SSTableReader. While streaming an SSTable, we should hold an SSTR reference.",,,,,,,,,,,,,,,,,,,,15/Nov/10 21:10;stuhood;0001-Add-SSTable-reference-to-PendingFile-to-prevent-GC-of-.txt;https://issues.apache.org/jira/secure/attachment/12459636/0001-Add-SSTable-reference-to-PendingFile-to-prevent-GC-of-.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,03:03.0,,,no_permission,,,,,,,,,,,,20290,,,Thu Nov 18 03:16:35 UTC 2010,,,,,,0|i0g75j:,92589,jbellis,jbellis,,,,,,,,,15/Nov/10 22:03;jbellis;doesn't passing sstable make the descriptor param redundant?,"15/Nov/10 22:25;stuhood;> doesn't passing sstable make the descriptor param redundant?
There is another constructor that passes only the descriptor.",18/Nov/10 01:37;jbellis;committed,"18/Nov/10 03:16;hudson;Integrated in Cassandra-0.7 #14 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/14/])
    retain reference to PendingFile sstables
patch by Stu Hood; reviewed by jbellis for CASSANDRA-1749
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using QUORUM and replication factor of 1 now causes a timeout exception,CASSANDRA-1646,12478042,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,tnine,tnine,10/22/2010 2:56,3/12/2019 14:07,3/13/2019 22:24,10/22/2010 13:59,0.7 beta 3,,,,0,,,,,,See the attached path to the python thrift tests.  On the source from 2010-10-14 20:00 UTC this passed.  From the latest HEAD as of today this fails. ,,,,,,,,,,,,,,,,,,,,22/Oct/10 03:01;tnine;test_quorum.patch;https://issues.apache.org/jira/secure/attachment/12457814/test_quorum.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,59:53.6,,,no_permission,,,,,,,,,,,,20237,,,Fri Oct 22 13:59:53 UTC 2010,,,,,,0|i0g6if:,92485,,,,,,,,,,,22/Oct/10 03:01;tnine;Patch file to update function tests.,22/Oct/10 13:59;jbellis;caused by bad merge from CASSANDRA-1622.  thanks for the test case!  fixed in r1026327.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replayed log data is not flushed before logs are wiped,CASSANDRA-204,12426618,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,5/28/2009 19:36,3/12/2019 14:07,3/13/2019 22:24,5/28/2009 21:10,0.3,,,,0,,,,,,"The memtable created by replaying commit logs on startup is supposed to be flushed as a SSTable before the commitlog is removed, but this is not happening.  So you can lose data by doing the following:

1. insert data
2. restart cassandra (using kill, to force replay)
3. restart cassandra again
",,,,,,,,,,,,,,,,,,,,28/May/09 19:46;jbellis;204.patch;https://issues.apache.org/jira/secure/attachment/12409297/204.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,01:21.8,,,no_permission,,,,,,,,,,,,19593,,,Thu May 28 21:10:27 UTC 2009,,,,,,0|i0fxi7:,91026,,,,,,,,,,,"28/May/09 19:46;jbellis;Sorry, cleanup got mixed in with the bugfix here.

The fix is, call MT.put instead of putOnRecover.  pOR() didn't set dirty, so it was basically a broken put().  Removed it.  (Client code goes through apply(), which schedules put() on the executor.  So recover calling put directly is exactly what we want.)

The rest is removing comments that weren't actually accurate, and making startup messages more signal than noise (which is what made the problem clear).",28/May/09 20:35;jbellis;s/schedules on the executor/locks and checks for flush/,"28/May/09 21:01;junrao;Patch looks fine to me.
",28/May/09 21:10;jbellis;committed to 0.3 and merged to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError during initial compaction,CASSANDRA-425,12434898,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,9/4/2009 22:52,3/12/2019 14:07,3/13/2019 22:24,9/7/2009 17:18,0.4,0.5,,,0,,,,,,"Note that the FutureTask the compaction is wrapped in hides the error from the logs, since get() is never called on the task.",,,,,,,,,,,,,,,,,,,,05/Sep/09 02:35;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-425-log-errors-in-futuretasks-in-DScheduledT.txt;https://issues.apache.org/jira/secure/attachment/12418692/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-425-log-errors-in-futuretasks-in-DScheduledT.txt,05/Sep/09 02:35;jbellis;ASF.LICENSE.NOT.GRANTED--0002-fix-compaction-bug-only-one-of-the-SSTR-constructor.txt;https://issues.apache.org/jira/secure/attachment/12418693/ASF.LICENSE.NOT.GRANTED--0002-fix-compaction-bug-only-one-of-the-SSTR-constructor.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,04:25.0,,,no_permission,,,,,,,,,,,,19682,,,Tue Sep 08 12:35:49 UTC 2009,,,,,,0|i0fyun:,91244,,,,,,,,,,,"05/Sep/09 02:37;jbellis;02
    fix compaction bug -- only one of the SSTR constructors was adding to openedFiles

01
    log errors in futuretasks in DScheduledTPE like in DTPE (and clean out unused threadlocal hack).

01 should be applied to 0.4 and 0.5.  02 is only needed on 0.5 (fixes regression caused by CASSANDRA-413)","07/Sep/09 16:04;urandom;Looks good, +1.",07/Sep/09 17:18;jbellis;committed as described above,"08/Sep/09 12:35;hudson;Integrated in Cassandra #191 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/191/])
    fix compaction bug -- only one of the SSTR constructors was adding to openedFiles.  (fixes regression from #413).
patch by jbellis; reviewed by Eric Evans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOFException in LazilyCompactedRow,CASSANDRA-1299,12469630,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,stuhood,stuhood,7/19/2010 16:18,3/12/2019 14:07,3/13/2019 22:24,7/19/2010 17:34,0.7 beta 1,,,,0,,,,,,"Post CASSANDRA-270, 'ant clean long-test' fails with an EOFException in LazilyCompactedRow.

{code}java.io.IOError: java.io.EOFException
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:103)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:32)
	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)
	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:604)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.ColumnIndexer.serializeInternal(ColumnIndexer.java:76)
	at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:50)
	at org.apache.cassandra.io.LazilyCompactedRow.<init>(LazilyCompactedRow.java:62)
	at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:135)
	at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:107)
	at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:46)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
	at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
	at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:334)
	at org.apache.cassandra.db.LongCompactionSpeedTest.testCompaction(LongCompactionSpeedTest.java:101)
	at org.apache.cassandra.db.LongCompactionSpeedTest.testCompactionWide(LongCompactionSpeedTest.java:49)
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
	at java.io.RandomAccessFile.readLong(RandomAccessFile.java:758)
	at org.apache.cassandra.db.TimestampClockSerializer.deserialize(TimestampClock.java:128)
	at org.apache.cassandra.db.TimestampClockSerializer.deserialize(TimestampClock.java:119)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:90)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:31)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:99)
{code}",,,,,,,,,,,,,,,,,,,,19/Jul/10 17:25;jbellis;1299.txt;https://issues.apache.org/jira/secure/attachment/12449859/1299.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,25:34.5,,,no_permission,,,,,,,,,,,,20068,,,Wed Jul 21 12:50:26 UTC 2010,,,,,,0|i0g47r:,92113,,,,,,,,,,,"19/Jul/10 17:25;jbellis;CASSANDRA-270 exposed a long-standing bug in SSTableUtils where it was writing garbage data while bypassing memtable to assemble its own SSTables by hand.  It needs to limit the data passed to SSTableWriter to the actual valid data in the DataOutputBuffer.  Attached is a patch that takes the simplest approach to this by copying out only the valid data into a separate byte[].  Ideally I'd prefer to remove the append(key, byte[]) method entirely but that will have to wait until we have a BMT replacement.","19/Jul/10 17:28;stuhood;Ha, yea... that would do it. Thanks!

+1",19/Jul/10 17:34;jbellis;committed,"21/Jul/10 12:50;hudson;Integrated in Cassandra #496 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/496/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
keys are now byte[] but hashmap cannot have byte[] as keys so they need to be fixed,CASSANDRA-1020,12462918,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,4/24/2010 4:54,3/12/2019 14:07,3/13/2019 22:24,4/27/2010 15:15,0.7 beta 1,,,4/26/2010 0:00,0,,,,,,Thrift client calls use hashmap and needs this fix,Linux/Mac Cassandra,,,,,,,,,,,,,,,,,,,26/Apr/10 20:57;vijay2win@yahoo.com;1020-v1.txt;https://issues.apache.org/jira/secure/attachment/12442896/1020-v1.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,52:03.9,,,no_permission,,,,,,,,,,,,19959,,,Tue Apr 27 15:15:07 UTC 2010,,,,,,0|i0g2if:,91837,,,,,,,,,,,"24/Apr/10 13:52;jbellis;as discussed on IRC, it looks like the only places we use byte[] keys in maps are in CassandraServer where using object identity is OK since we don't need to worry about two different byte[] w/ the same contents","24/Apr/10 16:07;vijay2win@yahoo.com;Thanks Jonathan, but cfamilies.get(command.key) will be fixed in cassandraserver?","24/Apr/10 16:43;jbellis;you're right, that one is still a potential bug.",26/Apr/10 20:57;vijay2win@yahoo.com;This Solves the problem which i noticed earlier... will this work?,"27/Apr/10 15:15;jbellis;committed, with a similar patch for avro.CassandraServer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove JDK-derived code from project,CASSANDRA-37,12421788,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,4/1/2009 15:42,3/12/2019 14:07,3/13/2019 22:24,4/27/2009 18:24,0.3,,,,0,,,,,,"Some classes appear to have been copied from the JDK source and altered to improve them for use in Cassandra.  Unfortunately neither the classic JDK nor the OpenJDK license is compatible with the Apache license that Cassandra is operating under.  See https://issues.apache.org/jira/browse/LEGAL-46

Modified JDK classes include:

 - BitSet [JDK: BitSet]
 - FastByteArrayInputStream [ByteArrayInputStream]
 - FastByteArrayOutputStream [ByteArrayOutputStream]
",,,,,,,,,,,,,,,,,,,,14/Apr/09 15:12;jbellis;0001-add-SSTable-tests.-in-particular-we-want-to-be-sure.patch;https://issues.apache.org/jira/secure/attachment/12405424/0001-add-SSTable-tests.-in-particular-we-want-to-be-sure.patch,14/Apr/09 15:12;jbellis;0002-r-m-copied-JDK-code.patch;https://issues.apache.org/jira/secure/attachment/12405425/0002-r-m-copied-JDK-code.patch,14/Apr/09 17:40;jbellis;37.tgz;https://issues.apache.org/jira/secure/attachment/12405440/37.tgz,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,02:30.4,,,no_permission,,,,,,,,,,,,19523,,,Wed May 06 03:37:59 UTC 2009,,,,,,0|i0fwhz:,90863,,,,,,,,,,,14/Apr/09 15:14;jbellis;the only nontrivial change here is to change BitSet serialization to use the JDK's hooks for that since we can't access private fields directly anymore.  the tests make sure to exercise that part of the code (verified by cobertura).  Performance impact should be minimal since these are only read once in the process's lifetime.,"14/Apr/09 15:15;jbellis;note that to cleanly apply this, CASSANDRA-65 must be applied first.",14/Apr/09 17:40;jbellis;rebased to HEAD + 65,"14/Apr/09 19:02;urandom;The only nit I have is that in SSTable, dataFile_ has been made implicitly public in order to make it accessible from SSTableTest. Perhaps it would be better to keep it private and use a getter instead?

Other than that, +1","14/Apr/09 19:05;jbellis;thanks for the feedback.

it's package-local, which means classes in the same package (like its associated test) can access it.  to java that is quite different than public. :)

my understanding (and I could be wrong) is that this is a reasonably idiomatic use of package local.",15/Apr/09 20:40;tlipcon;Agreed that package-local is idiomatic for accessing semi-private stuff from within unit tests,27/Apr/09 18:24;jbellis;applied,"28/Apr/09 13:47;hudson;Integrated in Cassandra #51 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/51/])
    r/m copied JDK code
patch by jbellis; reviewed by Eric Evans for 
add SSTable tests.  in particular we want to be sure to test bloomfilter
[de]serialize since that's going to change a lot.
patch by jbellis; reviewed by Eric Evans for 
","06/May/09 03:37;iholsman;Hey JB.

lucene/solr have a 'OpenBitSet' class in their code. would this be suitable as a replacement for BitSet ?

http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/util/OpenBitSet.html
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avro Schema Swap,CASSANDRA-1351,12470732,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,stuhood,stuhood,stuhood,8/3/2010 15:58,3/12/2019 14:06,3/13/2019 22:24,8/3/2010 17:21,0.7 beta 1,,,,0,,,,,,"Due to misreading Avro's docs, I swapped the Schema parameters to Avro's schema resolver.

The schema resolver allows for backwards compatibility by accepting a writer's (ser.) and reader's (deser.) schema, and resolving them to determine which fields to add or ignore. The reader's schema was not being set correctly, which was breaking backwards compatibility (although not the disk format).",,,,,,,,,,,,,,,,,,,,03/Aug/10 15:58;stuhood;0001-Specify-the-readers-schema-as-different-from-the-wri.patch;https://issues.apache.org/jira/secure/attachment/12451130/0001-Specify-the-readers-schema-as-different-from-the-wri.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:45.8,,,no_permission,,,,,,,,,,,,20097,,,Wed Aug 04 13:25:33 UTC 2010,,,,,,0|i0g4iv:,92163,,,,,,,,,,,"03/Aug/10 16:17;stuhood;Also, I opened AVRO-603 to handle clarifying these docs.",03/Aug/10 17:21;gdusbabek;+1 committed (I left testKSMetaDataSerialization() in),"04/Aug/10 13:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    specify schema used to read serialized migrations. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1351
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
multiget_slice calls do not close files resulting in file descriptor leak,CASSANDRA-1188,12466908,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,mdennis,wr0ngway,wr0ngway,6/14/2010 15:12,3/12/2019 14:06,3/13/2019 22:24,6/14/2010 17:58,0.7 beta 1,,,,0,,,,,,"Insert  1000 rows into a super column family. Read them back in a loop using multiget_slice. Note leaked file descriptors with lsof:
lsof -p `ps ax | grep [C]assandraDaemon | awk '{print $1}'` | awk '{print $9}' | sort | uniq -c | sort -n | tail -n 5

Looks like SSTableNamesIterator is never closing the files it creates via the sstable ...?

This is similar to CASSANDRA-1178 except for use of multiget_slice instead of get_slice

",Ubuntu 10.04,,,,,,,,,,,,,,,,,,,14/Jun/10 17:17;mdennis;0001-trunk-1188.patch;https://issues.apache.org/jira/secure/attachment/12447042/0001-trunk-1188.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:00.1,,,no_permission,,,,,,,,,,,,20028,,,Tue Jun 15 12:51:22 UTC 2010,,,,,,0|i0g3jb:,92003,,,,,,,,,,,"14/Jun/10 17:43;wr0ngway;I ran my tests against this patch and they pass, looks like this cleared up the leak.  Thanks! 

BTW, oddly enough, my tests never fail (i.e. lsof never balloons up) on OS X even before the fix for CASSANDRA-1178, any ideas why that might be?  I'm guessing implementation details of GC or underlying OS are to blame, but don't know enough to say for sure.  Not important, just curious.
","14/Jun/10 17:58;jbellis;committed, with formatting cleanup.

yes, the jdk declares finalizers on file streams to close them if the GC runs in time; as you note, it's implementation-dependent how often that actually happens.","14/Jun/10 18:44;mdennis;not all GC implementations close file descriptors when the objects are collected.  Apparently the one you're using on OSX does.  Curious though, you didn't see *any* ballooning?  If OSX was releasing the descriptors when the objects were collected, I would expect to see a few hundred open files while reading in a loop on OSX and only a handful after the patch.

random thought: It may also be a difference in memmapped IO versus not - perhaps OSX doesn't use it (or isn't configured to)?","14/Jun/10 19:49;wr0ngway;I wasn't really watching it while it was running, just checking the count of open files after.

No idea about memmapped IO in OS X, I didn't explicitly set anything up w.r.t. it anyway...

","15/Jun/10 12:51;hudson;Integrated in Cassandra #466 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/466/])
    close file in SSTableNamesIterator when opened locally.  patch by mdennis; reviewed by jbellis for CASSANDRA-1188
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction can echo data which breaks upon sstable format changes,CASSANDRA-2216,12499332,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,2/22/2011 12:17,3/12/2019 14:06,3/13/2019 22:24,2/22/2011 17:32,0.7.3,,,,0,compaction,,,,,"While compaction, if for a row we have only 1 sstable holding data, we echo this data. This breaks when we change the data format, creating mixed (corrupted) sstable.

(I suspect this is the cause of CASSANDRA-2195, but opening a new ticket until we can confirm that hunch)",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,22/Feb/11 12:24;slebresne;0001-Don-t-echo-data-during-compaction.patch;https://issues.apache.org/jira/secure/attachment/12471609/0001-Don-t-echo-data-during-compaction.patch,22/Feb/11 15:35;slebresne;2216_v2.patch;https://issues.apache.org/jira/secure/attachment/12471625/2216_v2.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,40:34.9,,,no_permission,,,,,,,,,,,,20512,,,Tue Feb 22 18:57:55 UTC 2011,,,,,,0|i0ga13:,93055,jbellis,jbellis,,,,,,,,,"22/Feb/11 12:24;slebresne;Attached patch completely remove the echoing of data when we have only one row. We could easily, as in CASSANDRA-2211, echo data if the sstable we are echoing from is at the last version.

However, not doing so will allow potentially corrupted sstable to get repaired by compaction (in the case of corruption from the bloom filter change).

We could add back the echoing optimisation later on.",22/Feb/11 15:15;slebresne;Attaching v2 that does the optimisation of checking for last version. Implies we repair the inconsistencies introduced outside of compaction.,"22/Feb/11 16:40;jbellis;bq. We could easily echo data if the sstable we are echoing from is at the last version

Let's do that, and introduce a separate command to force deserialization.  (Telling people ""compact to fix it"" is not something we want to do since that leaves you with One Big SSTable and all the problems associated w/ that.)",22/Feb/11 17:32;jbellis;committed v2,"22/Feb/11 18:57;hudson;Integrated in Cassandra-0.7 #304 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/304/])
    fix compaction echoing old-style data into new sstable version
patch by slebresne; reviewed by jbellis for CASSANDRA-2216
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"restarting node crashes with NPE when, while replaying the commitlog, the cfMetaData is requested",CASSANDRA-995,12462287,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,gdusbabek,tzz,tzz,4/16/2010 20:31,3/12/2019 14:06,3/13/2019 22:24,4/21/2010 13:51,0.7 beta 1,,,,0,,,,,,"Removing the commitlog directory completely fixes this.   I can reliably reproduce it by 1) starting and configuring a schema with one keyspace, one super CF with LongType supercolumns; 2) inserting data; 3) shutting down and restarting the node.

Here's my schema expressed in cassidy.pl, should be obvious what the parameters are:
./cassidy.pl -server X -port Y -keyspace system 'kdefine test org.apache.cassandra.locator.RackUnawareStrategy 2 org.apache.cassandra.locator.EndPointSnitch'
./cassidy.pl -server X -port Y -keyspace test 'fdefine Status Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'

The problem seems to be related to CASSANDRA-44 as it happens when the CF metadata is requested but I don't know what's causing it.

10/04/16 15:25:11 INFO commitlog.CommitLog: Replaying /home/cassandra/commitlog/CommitLog-1271449410100.log, /home/cassandra/commitlog/CommitLog-1271449378151.log, /home/cassandra/commitlog/CommitLog-1271449415800.log
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.<init>(Table.java:261)
        at org.apache.cassandra.db.Table.open(Table.java:102)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:233)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:104)
        at org.apache.cassandra.thrift.CassandraDaemon.init(CassandraDaemon.java:151)
        ... 5 more
",SVN rev 935070,,,,,,,,,,,,,,,,,,,20/Apr/10 14:23;gdusbabek;0001-include-all-keyspaces-when-creating-the-schema-migra.patch;https://issues.apache.org/jira/secure/attachment/12442298/0001-include-all-keyspaces-when-creating-the-schema-migra.patch,20/Apr/10 14:23;gdusbabek;0002-Use-RackUnawareStrategy-in-unit-tests-because-it-doe.patch;https://issues.apache.org/jira/secure/attachment/12442299/0002-Use-RackUnawareStrategy-in-unit-tests-because-it-doe.patch,16/Apr/10 21:11;tzz;ASF.LICENSE.NOT.GRANTED--crashlog-995;https://issues.apache.org/jira/secure/attachment/12442006/ASF.LICENSE.NOT.GRANTED--crashlog-995,16/Apr/10 22:29;gdusbabek;ASF.LICENSE.NOT.GRANTED--run_1.txt;https://issues.apache.org/jira/secure/attachment/12442014/ASF.LICENSE.NOT.GRANTED--run_1.txt,16/Apr/10 22:29;gdusbabek;ASF.LICENSE.NOT.GRANTED--run_2.txt;https://issues.apache.org/jira/secure/attachment/12442015/ASF.LICENSE.NOT.GRANTED--run_2.txt,19/Apr/10 16:46;tzz;Tester.java;https://issues.apache.org/jira/secure/attachment/12442199/Tester.java,,,,,,,,6,,,,,,,,,,,,,,,,,,,39:38.6,,,no_permission,,,,,,,,,,,,19946,,,Tue Apr 20 17:00:59 UTC 2010,,,,,,0|i0g2cv:,91812,,,,,,,,,,,"16/Apr/10 20:39;jbellis;If this is just saying that ""I can't replay a commitlog written before -44, after applying 44 and restarting,"" then that is expected.  Use nodetool drain before upgrading (or simply r/m the commitlog as you say).","16/Apr/10 20:45;tzz;I wouldn't report it as a bug if it wasn't on a commitlog written by the same version of the software.  All the steps (1,2,3) are on the same SVN revision I stated in the Environment section.",16/Apr/10 21:11;tzz;This is the complete log.  First start is fresh without commitlog or data.  Second start is after shutting down completely by killing jsvc (through /etc/init.d/cassandra restart on a Debian system).,"16/Apr/10 21:19;tzz;My complete schema, although only Status and Property are used.  Sorry I edited the original but it was not substantially different.

./cassidy.pl -server X -port 9170 -keyspace system 'kdefine HB3-prod org.apache.cassandra.locator.RackUnawareStrategy 2 org.apache.cassandra.locator.EndPointSnitch'
./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'fdefine Status Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'
./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'fdefine Property Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'
./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'fdefine Analysis Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'
./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'fdefine Relationships Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'
./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'fdefine Knowledge Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'

./cassidy.pl -server X -port 9170 -keyspace system 'kdefine CM org.apache.cassandra.locator.RackUnawareStrategy 2 org.apache.cassandra.locator.EndPointSnitch'
./cassidy.pl -server X -port 9170 -keyspace CM 'fdefine Inventory Super LongType BytesType comment=statuschanges,row_cache_size=0,key_cache_size=20000'

When I tried to trigger the error manually with cassidy with just a few inserts like this (the below inserts test=abc into SC 0 in the Status CF):

./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'ins Status testkey 0 test=abc'
./cassidy.pl -server X -port 9170 -keyspace HB3-prod 'get Status testkey 0'

the problem doesn't happen.  I don't know how much data causes it.  My typical writes that trigger this, over a minute, are about 20-30 columns per supercolumn, about 2500 keys, about 5 SCs per key.  I can reliably trigger it with a one-minute population run.

Thanks and I hope this is enough information to replicate the bug.  If not I'll try to give you more including a better test case.","16/Apr/10 22:29;gdusbabek;I couldn't reproduce this.  run_1.txt is a log from defining a KS with a SC and then inserting 100 rows.  run_2.txt is what happens after a restart.

Ted, can you reproduce this outside of cassidy.pl?",19/Apr/10 15:12;tzz;I'm working on replicating the inserts I do from Perl but so far haven't been able to replicate in Java.  I'll keep trying.,"19/Apr/10 16:43;tzz;OK, I can prepare a proper test if you need it but the atteched Tester.java definitely causes the NPE and should work with minor changes for you (makeOpenLoginClient and getLongAsBytes are really simple helper methods).  I do ""sudo /etc/init.d/cassandra stop; rm -rf *; sudo /etc/init.d/cassandra start"", run the test, then ""sudo /etc/init.d/cassandra restart"" and get the NPE I showed earlier.  This is with today's SVN (r935659).

It definitely is a combination of the number of keyspaces and CFs together with the number of inserts.  If I do less that 1000 iterations or if I define fewer CFs and keyspaces, the problem doesn't happen on restart.  So it took me a long time to hunt it down.
",19/Apr/10 19:34;gdusbabek;Thanks Ted.  I can reproduce this problem now.,20/Apr/10 14:23;gdusbabek;Migration code was only storing the most recently modified keyspace when recording a migration instead of all of them.,20/Apr/10 16:01;jbellis;+1 except don't commit the .iml :),20/Apr/10 17:00;tzz;I tested and the fix works for me. Thank you.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
commitlog recover bug,CASSANDRA-1297,12469585,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jftan,jftan,jftan,7/19/2010 9:02,3/12/2019 14:06,3/13/2019 22:24,7/19/2010 12:42,0.6.4,,,,0,,,,,,"class CommitLog.java
when recover log files;
 if one log  file have no dirty , process is break;
{quote}
199  int lowPos = CommitLogHeader.getLowestPosition(clHeader);
 200 if (lowPos == 0)
 201    break;
{quote}

why not continue and read next log file
{quote}
199  int lowPos = CommitLogHeader.getLowestPosition(clHeader);
200 if (lowPos == 0)\{
201   reader.close();
202   continue;
203  \}
{quote}

i am not very sure about that. how can answer?


",,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,42:13.1,,,no_permission,,,,,,,,,,,,20066,,,Mon Jul 19 12:42:13 UTC 2010,,,,,,0|i0g47b:,92111,,,,,,,,,,,"19/Jul/10 12:42;jbellis;you're right, it should continue to the next segment.  fixed in r965457.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"commitlog may consider writes flushed, that are not yet",CASSANDRA-445,12435716,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,junrao,jbellis,jbellis,9/15/2009 14:39,3/12/2019 14:06,3/13/2019 22:24,9/18/2009 3:21,0.5,,,,0,,,,,,"Jun Rao explains:

Suppose there are 3 updates u1, u2, and u3. They are written to commit log in that order. If u1 and u3 are applied to memtable first and at that point, a flush is triggered. After the flush completes, it will move the commit log restarting position based on the log for u3. However, u2 hasn't been persisted on disk yet. This means that if the node dies now, the recovery logic won't replay u2 from the log.",,,,,,,,,,,,,,,,,,,,16/Sep/09 16:25;junrao;issue445.patchv1;https://issues.apache.org/jira/secure/attachment/12419789/issue445.patchv1,17/Sep/09 20:16;junrao;issue445.patchv2;https://issues.apache.org/jira/secure/attachment/12419925/issue445.patchv2,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,03:00.4,,,no_permission,,,,,,,,,,,,19691,,,Fri Sep 18 13:03:42 UTC 2009,,,,,,0|i0fyz3:,91264,,,,,,,,,,,"15/Sep/09 14:43;jbellis;One solution would be to have a map<thread, int> of most-recent commitlog writes per columnfamily.  Then we could use the lowest most-recent-write on flush.  Weak referenced keys would keep threads that don't exist anymore from causing trouble.

Dare I ask what does hbase does?","15/Sep/09 23:03;junrao;hbase actually behaves correctly. Here is the logic. Every update first grabs a read lock, then gets a log sequence num (increasing) and appends to log, and finally updates memtable and releases the read lock. The flusher first grabs a write lock, then gets the next LSN (used as the starting position for log reply), starts the flush and releases the write lock. So, the flusher waits until all outstanding writes to memtable complete and prevents new writes while initiating a flush. Updates to memtable are not necessarily in LSN order though.
","16/Sep/09 02:31;jbellis;Thanks for looking into that.  Using a rwlock is definitely the easiest way to enforce a ""wait for everyone to finish writing before flushing"" policy.
","16/Sep/09 16:25;junrao;Submit a quick patch. Use the approach in hbase to drain updaters before flushing. If this looks good, we can polish the code a bit more. For example, CFS.apply() no longer needs commitLogContext.","17/Sep/09 00:54;junrao;Did a quick performance test : inserting 1milllion single-column keys to a 1-node cassandra (key + value < 20 bytes), with 8 client threads, on an 8-core machine.

with patch
run1: 2m28s
run2: 2m36s

w/o patch
run1: 2m29s
run2: 2m32s

There is hardly any performance difference.","17/Sep/09 16:46;jbellis;I think this approach will work, so +1.

Two comments:

1)
            memtable_.put(key, columnFamily);

should change to

            initialMemtable.put(key, columnFamily);

to emphasize that apply always puts it into the memtable that was live at the start of the call, and the switch is now done by Table.

2)
there seems to be a high degree of overlap between memtableLock_ and flusherLock_ -- both are serializing access to a flush-related activity (that is, post-CASSANDRA-444).  I think memtableLock could be removed now with a little work.","17/Sep/09 20:16;junrao;Attach v2.
1. make flusherlock static to guard updates on all tables.
2. fix periodic flushers
3. force a sync on header updates to log files (called on flush)

Performance didn't change in the same test above.","18/Sep/09 01:23;jbellis;+1, but i think we'll still need some cleanup of memtableLock post-444",18/Sep/09 03:21;junrao;committed to trunk.,"18/Sep/09 13:03;hudson;Integrated in Cassandra #201 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/201/])
    commitlog may consider writes flushed, that are not yet; patched by junrao; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More schema migration race conditions,CASSANDRA-1715,12479261,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,gdusbabek,jbellis,jbellis,11/5/2010 21:12,3/12/2019 14:06,3/13/2019 22:24,11/19/2010 17:04,0.7.0 rc 1,,,,0,,,,,,"Related to CASSANDRA-1631.

This is still a bug with schema updates to an existing CF, since reloadCf is doing a unload/init cycle. So flushing + compaction is an issue there as well. Here is a stacktrace from during an index creation where it stubbed its toe on an incomplete sstable from an in-progress compaction (path names anonymized):
{code}
INFO [CompactionExecutor:1] 2010-11-02 16:31:00,553 CompactionManager.java (line 224) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-6-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-7-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-8-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-9-Data.db')]
...
ERROR [MigrationStage:1] 2010-11-02 16:31:10,939 ColumnFamilyStore.java (line 244) Corrupt sstable Standard1-tmp-e-10-<>=[Data.db, Index.db]; skipped
java.io.EOFException
        at org.apache.cassandra.utils.FBUtilities.skipShortByteArray(FBUtilities.java:308)
        at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:231)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:286)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:202)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:235)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:443)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:431)
        at org.apache.cassandra.db.Table.initCf(Table.java:335)
        at org.apache.cassandra.db.Table.reloadCf(Table.java:343)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.applyModels(UpdateColumnFamily.java:89)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:158)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:672)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
...
 INFO [CompactionExecutor:1] 2010-11-02 16:31:31,970 CompactionManager.java (line 303) Compacted to Standard1-tmp-e-10-Data.db.  213,657,983 to 213,657,983 (~100% of original) bytes for 626,563 keys.  Time: 31,416ms.
{code}

There is also a race between schema modification and streaming.",,,,,,,,,,,,,,,,,,,,15/Nov/10 23:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0001-take-drop-off-CompactionManager.txt;https://issues.apache.org/jira/secure/attachment/12459653/ASF.LICENSE.NOT.GRANTED--v3-0001-take-drop-off-CompactionManager.txt,15/Nov/10 23:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0002-compaction-lock.txt;https://issues.apache.org/jira/secure/attachment/12459654/ASF.LICENSE.NOT.GRANTED--v3-0002-compaction-lock.txt,15/Nov/10 23:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0003-migration-uses-locks.txt;https://issues.apache.org/jira/secure/attachment/12459655/ASF.LICENSE.NOT.GRANTED--v3-0003-migration-uses-locks.txt,15/Nov/10 23:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0004-handle-moved-dropped-CF-prior-to-pending-compaction-st.txt;https://issues.apache.org/jira/secure/attachment/12459656/ASF.LICENSE.NOT.GRANTED--v3-0004-handle-moved-dropped-CF-prior-to-pending-compaction-st.txt,15/Nov/10 23:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0005-CFS.reload-assumes-metadata-is-mutable.txt;https://issues.apache.org/jira/secure/attachment/12459657/ASF.LICENSE.NOT.GRANTED--v3-0005-CFS.reload-assumes-metadata-is-mutable.txt,15/Nov/10 23:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0007-updateColumnFamily-uses-reload-remove-unneccesary-stru.txt;https://issues.apache.org/jira/secure/attachment/12459659/ASF.LICENSE.NOT.GRANTED--v3-0007-updateColumnFamily-uses-reload-remove-unneccesary-stru.txt,15/Nov/10 23:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0008-perform-index-maintenance-outside-of-migration-locks-d.txt;https://issues.apache.org/jira/secure/attachment/12459660/ASF.LICENSE.NOT.GRANTED--v3-0008-perform-index-maintenance-outside-of-migration-locks-d.txt,15/Nov/10 23:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0009-use-avro-structures-inside-UpdateColumnFamily.txt;https://issues.apache.org/jira/secure/attachment/12459661/ASF.LICENSE.NOT.GRANTED--v3-0009-use-avro-structures-inside-UpdateColumnFamily.txt,15/Nov/10 23:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0010-remove-unused-fields-in-DropColumnFamily-DropKeyspace.txt;https://issues.apache.org/jira/secure/attachment/12459662/ASF.LICENSE.NOT.GRANTED--v3-0010-remove-unused-fields-in-DropColumnFamily-DropKeyspace.txt,18/Nov/10 10:05;jbellis;v3-0006-replace-modifiable-CFM-members-with-private-fields-a.patch;https://issues.apache.org/jira/secure/attachment/12459906/v3-0006-replace-modifiable-CFM-members-with-private-fields-a.patch,18/Nov/10 10:06;jbellis;v3-0011-make-addIndex-asynchronous-and-race-proof.patch;https://issues.apache.org/jira/secure/attachment/12459907/v3-0011-make-addIndex-asynchronous-and-race-proof.patch,19/Nov/10 14:23;jbellis;v3-0012-remove-locks-from-UpdateColumnFamily.patch;https://issues.apache.org/jira/secure/attachment/12460004/v3-0012-remove-locks-from-UpdateColumnFamily.patch,,12,,,,,,,,,,,,,,,,,,,25:56.6,,,no_permission,,,,,,,,,,,,20269,,,Tue Nov 23 02:15:03 UTC 2010,,,,,,0|i0g6xz:,92555,,,,,,,,,,,"08/Nov/10 23:25;gdusbabek;Be prepared to throw up a little.

This approach puts a lock around CompactionManager. Migrations have to acquire it and Table.flushLock before proceeding.  Special care had to be taking when reloading a CFS since it places blocking jobs on the compaction manager.  This patch should handle the streaming race as well (when the directory specified in a Descriptor is no longer valid).","08/Nov/10 23:33;jbellis;Also, out of curiosity, what were the main complications w/ mutable CFS.metadata?","08/Nov/10 23:48;gdusbabek;bq. what were the main complications w/ mutable CFS.metadata?

There were a couple things. A new memtable would need to know about the updated meta settings for thresholds.  The timing here is tricky because of flushing (chances are you would have just flushed and have an empty memtable in anyway, but one can't be too sure).  Other things... Make sure secondary indexes are dealt with properly on updates (e.g.: not reloaded needlessly).  Efficiently dealing with SSTableReader instances--certain classes up updates wouldn't require messing with them at all, but others would (when files move).  Ideally, it would be nice to repoint a few instances of SSTable at new data files and have all caches, stats, etc. remain intact.","09/Nov/10 02:02;jbellis;UpdateColumnFamily doesn't acquireLocks().  (Shouldn't Migration do that so the subclasses don't have to?)

bq. A new memtable would need to know about the updated meta settings for thresholds. The timing here is tricky because of flushing (chances are you would have just flushed and have an empty memtable in anyway, but one can't be too sure).

This gets a little messy code-wise (because we allow overriding memtable settings at runtime) but not too bad.  At worst we just set the CFS values to the new migration values during application.  I don't see any timing issues (Memtable.isThresholdViolated checks w/ the CFS each time, it doesn't cache locally).

bq. Make sure secondary indexes are dealt with properly on updates (e.g.: not reloaded needlessly).
 
Writing code to detect when indexes are added/dropped is a pain compared to just rebuilding it from scratch, but efficiency-wise it seems like a win. At least mutating you can avoid redoing the index sampling every time.  Stopping updates in their tracks while we reload, to change read_repair_chance, is really brutal.  (If UpdateCF doesn't actually need to acquireLocks then never mind, but I think it does.)

bq. Efficiently dealing with SSTableReader instances--certain classes up updates wouldn't require messing with them at all, but others would (when files move). 

What is making files move here?
","09/Nov/10 02:10;gdusbabek;bq. What is making files move here?
Renaming. Sorry.  I know we'll need to implement it eventually, so I can't stop thinking about it. That one doesn't matter in this context.","09/Nov/10 14:03;gdusbabek;bq. UpdateColumnFamily doesn't acquireLocks().
It does on line 82 in my checkout.
bq. Shouldn't Migration do that so the subclasses don't have to?
It doesn't make sense to lock on the Add* methods, but I agree it might be easier just to do the locking in the superclass. What do you think?","10/Nov/10 16:21;gdusbabek;v2 shows what the CFS/CFM reload approach would probably look like for UpdateColumnFamily.  Unfortunately it touches a lot of code and might not be warranted at this late stage in the beta cycle.  

If left for 0.7.1, I need to explain that it changes the serialization format for Migrations in a non-backwards compatible way, which is not desirable.  The same kind of work would have to be done for the other Migration subclasses.

This is mainly a demonstration, but one thing I'd definitely change is to use the avro CfDef in the UpdateColumnFamily constructor instead of the thrift version.","14/Nov/10 21:25;jbellis;The v2 approach looks great.  I think the main improvement we need is to not do blocking flushes while the locks are held.  For the purposes of creating a new memtable a nonblocking flush is fine.  For creating indexes we'll need to set up a callback to do the index building after the flush completes.  (We used to have code that took a callback arg as part of the flush call, I think I took it out but it should be relatively easy to resurrect.)  I agree that it touches a lot of code, but the core changes (i.e. not one-line things like encapsulating gcgraceseconds that are messy but not dangerous) aren't much larger than v1.  The huge improvement over waiting to re-sample indexes after UpdateCF is worth it imo.

I'm also fine with saying that changing the CFS will blow away any JMX-applied changes and reset values to what the new CFM says the should be.  But if you are happy with the Default* approach I am too.

bq. If left for 0.7.1, I need to explain that it changes the serialization format for Migrations in a non-backwards compatible way, which is not desirable

Is this saying that we'd need to tell beta3 users to rebuild their schemas if this goes in?  I am fine with that, I just want to make sure I understand.","15/Nov/10 13:35;gdusbabek;bq. Is this saying that we'd need to tell beta3 users to rebuild their schemas if this goes in? I am fine with that, I just want to make sure I understand.
Yes, exactly.

I'll go ahead and finish this patch in the v2 direction.",15/Nov/10 23:27;gdusbabek;0008 addresses the flushes-within-locks brought up by jonathan.  0009 and 0010 are cleanup.,"18/Nov/10 10:08;jbellis;new version of 06 to rebase.

11 makes addIndex asynchronous which unclogs the post-flush executor (which is single-threaded for safety, so a long index build would mean we don't clean up any commitlogs until it's done) and also makes restarting when a new index is not yet complete friendlier.

I don't think we need the locks on Update anymore since we're keeping the same Tracker and so forth so 12 removes that.  locks are still needed for Drop.

What do you think?","18/Nov/10 19:49;gdusbabek;I think update still needs to acquire the locks for the case when secondary indexes are dropped.  The locking could conceivably be pushed down to the point in CFS when the indexes are dropped though, at which point we'd need to remove the assert from the beginning of CFS.reload() and make the members that get reset in reload() volatile (minCompactionThreshold, maxCompactionThreshold, etc.).",18/Nov/10 19:59;gdusbabek;0012 causes several unit tests to fail.  You might want to take a closer look at it.,"19/Nov/10 14:23;jbellis;new 0012.

bq. the locking could conceivably be pushed down to the point in CFS when the indexes are dropped 

right, this is what 0011 does.  more accurately, it uses CSLM to avoid an explicit lock.

bq. at which point we'd need to remove the assert from the beginning of CFS.reload() and make the members that get reset in reload() volatile (minCompactionThreshold, maxCompactionThreshold, etc.). 

done.

bq. 0012 causes several unit tests to fail

one was the assert, one was an unrelated commitlog bug that got exposed by something unrelated.  fixed.","19/Nov/10 14:57;gdusbabek;bq. right, this is what 0011 does. more accurately, it uses CSLM to avoid an explicit lock.
My argument was that it wasn't safe to call indexCfs.removeAllSSTables() without the flush lock, but it looks like SSTableTracker is properly synchronized to avoid any problems with flushing.  No problem here.

+1 I'll commit this shortly.",19/Nov/10 17:04;gdusbabek;committed.,"19/Nov/10 20:05;hudson;Integrated in Cassandra-0.7 #19 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/19/])
    ","20/Nov/10 12:48;hudson;Integrated in Cassandra #602 (See [https://hudson.apache.org/hudson/job/Cassandra/602/])
    remove locks from UpdateColumnFamily. patch by jbellis, reviewed by gdusbabek. CASSANDRA-1715
make addIndex asynchronous and race proof. patch by jbellis, reviewed by gdusbabek. CASSANDRA-1715
remove unused fields in DropColumnFamily, DropKeyspace. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
use avro structures inside UpdateColumnFamily. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
perform index maintenance outside of migration locks during CF update. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
updateColumnFamily uses reload, remove unneccesary structures, fix bugs. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
replace modifiable CFM members with private fields and public getters. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
CFS.reload() assumes metadata is mutable. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
handle moved/dropped CF prior to pending compaction/streams. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
migration uses locks. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
compaction lock. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
take drop off CompactionManager. patch by gdusbabek, reviewe by jbellis. CASSANDRA-1715
","23/Nov/10 01:32;tjake;During testing I hit this section of code:

CFMetaData.java:662 
{code}
  // remove the ones leaving.
        for (ByteBuffer indexName : toRemove)
            column_metadata.remove(indexName);
{code}

but column_metadata is defined as:

{code}
        this.column_metadata = Collections.unmodifiableMap(column_metadata);
{code}

So remove() will throw an exception.
",23/Nov/10 02:07;jbellis;can you open a new ticket for that?,23/Nov/10 02:15;tjake;CASSANDRA-1768,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit log replay issues,CASSANDRA-370,12433347,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,jamesgolick,jamesgolick,8/18/2009 16:41,3/12/2019 14:06,3/13/2019 22:24,8/24/2009 22:03,0.4,,,,0,,,,,,"I've been having a bunch of trouble replaying commit logs. I've seen various exceptions, including:

java.lang.NegativeArraySizeException
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:273)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:63)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:95)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:156)
Exception encountered during startup.
java.lang.NegativeArraySizeException
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:273)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:63)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:95)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:156)

I also got this:

java.lang.RuntimeException: Unable to load comparator class 'org.apache.cassandra.db.marshal.UTF8Typ'.  probably this means you have obsolete sstables lying around
        at org.apache.cassandra.db.ColumnFamily$ColumnFamilySerializer.readComparator(ColumnFamily.java:525)
        at org.apache.cassandra.db.ColumnFamily$ColumnFamilySerializer.deserializeEmpty(ColumnFamily.java:535)
        at org.apache.cassandra.db.ColumnFamily$ColumnFamilySerializer.deserialize(ColumnFamily.java:500)
        at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:225)
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:284)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:63)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:95)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:156)
Caused by: java.lang.ClassNotFoundException: org/apache/cassandra/db/marshal/UTF8Typ
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:169)
        at org.apache.cassandra.db.ColumnFamily$ColumnFamilySerializer.readComparator(ColumnFamily.java:521)
        ... 7 more

and this:

java.io.EOFException
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
        at java.io.DataInputStream.readUTF(DataInputStream.java:572)
        at java.io.DataInputStream.readUTF(DataInputStream.java:547)
        at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:218)
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:284)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:63)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:95)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:156)
Exception encountered during startup.

Not sure if any of them are related.",OS X 10.5.7,,,,,,,,,,,,,,,,,,,24/Aug/09 21:30;jbellis;01-fixed.diff;https://issues.apache.org/jira/secure/attachment/12417530/01-fixed.diff,18/Aug/09 17:41;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-370-avoid-opening-multiple-writers-for-the-c.txt;https://issues.apache.org/jira/secure/attachment/12416896/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-370-avoid-opening-multiple-writers-for-the-c.txt,18/Aug/09 17:41;jbellis;ASF.LICENSE.NOT.GRANTED--0002-read-bytes-doesn-t-automatically-throw-EOFException-i.txt;https://issues.apache.org/jira/secure/attachment/12416897/ASF.LICENSE.NOT.GRANTED--0002-read-bytes-doesn-t-automatically-throw-EOFException-i.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,43:59.8,,,no_permission,,,,,,,,,,,,19655,,,Tue Aug 25 14:21:27 UTC 2009,,,,,,0|i0fyiv:,91191,,,,,,,,,,,"18/Aug/09 17:43;jbellis;02
    read(bytes) doesn't automatically throw EOFException if it reads less than asked for, so we need to check for that maniually

01
    avoid opening multiple writers for the current file; the buffered nature of the global
    logWriter_ could cause problems

james, in all likelihood the problem fixed by 02 is what you were running into, and that patch should fix replaying your old commitlog as-is (magic! :)","24/Aug/09 20:12;sammy.yu;+1 on both 0001 and 0002 patches
",24/Aug/09 20:33;jbellis;committed,24/Aug/09 21:14;jbellis;patch 01 causes issues.  reverted.  (02 is still applied.),"24/Aug/09 21:30;jbellis;fixed -- wasn't seeking back to current-end-of-CL in the header update code, so it would start clobbering the hell out of itself as soon as that code path ran.","24/Aug/09 21:57;sammy.yu;01-fixed.diff +1   Tested it for real this time.  Unit test passes and our production system no longer exhibits this bad behavior.


",24/Aug/09 22:03;jbellis;committed,"25/Aug/09 14:21;hudson;Integrated in Cassandra #177 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/177/])
    [fixed version] avoid opening multiple writers for the current file; the buffered nature of the global logWriter_ could cause problems
patch by jbellis; reviewed for  by Sammy Yu
Revert "" avoid opening multiple writers for the current file; the buffered nature of the global logWriter_ could cause problems""
read(bytes) doesn't automatically throw EOFException if it reads less than asked for, so we need to check for that.
patch by jbellis; reviewed by Sammy Yu for 
 avoid opening multiple writers for the current file; the buffered nature of the global logWriter_ could cause problems
patch by jbellis; reviewed by Sammy Yu for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop streaming -> Cassandra ColumnFamilyOutputFormat not respecting partitioner,CASSANDRA-1455,12473166,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,stuhood,mrflip,mrflip,9/2/2010 17:42,3/12/2019 14:06,3/13/2019 22:24,9/4/2010 18:20,,,,,0,,,,,,"The Hadoop streaming shim (hadoop streaming client => avro => ColumnFamilyOutputFormat => cassandra ring) is only connecting to one or a couple clients on the ring.  With 24 hadoop clients launched,  ` sudo netstat -antp | grep 9160 | wc -l ` gave 24 on one machine, and only 1-3 on any other node.

I'll attach the script and runner I used.","Ubuntu lucid ; Hadoop 0.20.1 CDH3b1 ; Cassandra 0.7beta1 + #1368, 1358, 1322, 1315 + patch to allow flat StreamingMutation avro schema",,,,,,,,,,,,,,,,,,,02/Sep/10 17:45;mrflip;avromapper.rb;https://issues.apache.org/jira/secure/attachment/12453703/avromapper.rb,02/Sep/10 17:45;mrflip;streamer.sh;https://issues.apache.org/jira/secure/attachment/12453704/streamer.sh,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,20:41.9,,,no_permission,,,,,,,,,,,,20148,,,Sat Sep 04 18:20:06 UTC 2010,,,,,,0|i0g55z:,92267,,,,,,,,,,,02/Sep/10 17:45;mrflip;Note that in its current state requires a patch to use flat avro schema.,03/Sep/10 06:20;stuhood;Flip: could you try out the fix from CASSANDRA-1434?,"03/Sep/10 12:20;mrflip;I tried it with the first rev of the patches, no success; I see there was a second rev and I'll give that a shot in a bit.","03/Sep/10 14:38;mrflip;I had a look at the code. I'm not really set up to debug Java atm, but did a finger trace thru.

In storeDescribedRing in RingCache, line 37 reads
 host = range.endpoints.get(0)
AFAICT, it's outside the loop over endpoints. Is it possible that host needs to be set inside the loop from the endpoints map?

flip
----
http://infochimps.org
Find any dataset in the world




","03/Sep/10 17:26;stuhood;> host = range.endpoints.get(0)
Aha... good eye Flip. Looks like I broke that in 1322: so it is only collecting the primary endpoints. That makes the ""alternate through replicas of a range"" fix on 1434 moot, but you should still be seeing an even number of connections to all the nodes in your cluster.

I'll roll another 1434, but I don't have any other ideas here at the moment: in tests against a RF=1 12 node cluster (with OPP even), I saw an equal number of connections to all nodes, even with the above bug.",04/Sep/10 18:20;mrflip;The combination of patches now shows inbound connections on all hosts in the ring. Thanks Stu!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix consistencylevel during bootstrap,CASSANDRA-833,12457318,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,2/24/2010 17:44,3/12/2019 14:16,3/13/2019 22:24,9/21/2012 15:13,1.2.0 beta 2,,,,0,,,,,,"As originally designed, bootstrap nodes should *always* get *all* writes under any consistencylevel, so when bootstrap finishes the operator can run cleanup on the old nodes w/o fear that he might lose data.

but if a bootstrap operation fails or is aborted, that means all writes will fail until the ex-bootstrapping node is decommissioned.  so starting in CASSANDRA-722, we just ignore dead nodes in consistencylevel calculations.

but this breaks the original design.  CASSANDRA-822 adds a partial fix for this (just adding bootstrap targets into the RF targets and hinting normally), but this is still broken under certain conditions.  The real fix is to consider consistencylevel for two sets of nodes:

  1. the RF targets as currently existing (no pending ranges)
  2.  the RF targets as they will exist after all movement ops are done

If we satisfy CL for both sets then we will always be in good shape.

I'm not sure if we can easily calculate 2. from the current TokenMetadata, though.",,,,,,,,,,,,,,,,,,,,06/May/11 17:23;slebresne;0001-Increase-CL-with-boostrapping-leaving-node.patch;https://issues.apache.org/jira/secure/attachment/12478450/0001-Increase-CL-with-boostrapping-leaving-node.patch,17/May/11 21:21;jbellis;833-v2.txt;https://issues.apache.org/jira/secure/attachment/12479519/833-v2.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,10:05.4,,,no_permission,,,,,,,,,,,,19884,,,Fri Sep 21 15:13:10 UTC 2012,,,,,,0|i0g1cv:,91650,slebresne,slebresne,,,,,,,,,"03/Mar/10 03:09;jbellis;To clarify: the #722 fix breaks the design because a bootstrapping node that goes ""down"" temporarily but completes bootstrap will not actually have all the writes that happened during bootstrap on it.","05/Mar/10 14:10;jaakko;This issue is not only related to bootstrapping, since nodes leaving the ring will also cause pending ranges. If a node does not complete leaving operation properly, obsolete pending ranges will be left in metadata.

(2) above is actually almost exactly how pending ranges is calculated. All current move operations are finished and pending ranges is calculated according to what are the new natural endpoints for the ranges in question.

This is not directly related to bootstrapping IMHO, but to the fact that node movement increases quorum and due to node movement being uncertain, there is bigger possibility that something goes wrong and quorum nodes cannot be reached.
","03/May/11 13:56;jbellis;Consider the case of CL=1, RF=3 to replicas A, B, C. We begin bootstrapping node D, and write a row K to the range being moved from C to D.

If the cluster is heavily loaded, it's possible that we write one copy to C, all the other writes get dropped, and once bootstrap completes we lose the row. Or if we write one copy to D, and cancel bootstrap, we again lose the row.

As said above, we want to satisfy CL for both the pre- and post-bootstrap nodes (in case bootstrap aborts).  This requires treating the old/new range owner as a unit: both D *and* C need to accept the write for it to count towards CL. So rather than considering {A, B, C, D} we should consider {A, B, (C, D)}.

This is a lot of complexity to introduce. A simplification that preserves correctness is to continue treating nodes independently but require *one more node* than normal CL. So CL=1 would actually require 2 nodes; CL=Q would require 3 (for RF=3), and so forth.  (Note that Q(3) + 1 is the same as Q(4), which is what the existing code computes; that is one reason I chose a CL=1 example to start with, since those are *not* the same even for the simple case of RF=3.)

This would mean we may fail a few writes unnecessarily (a write to A or B is actually sufficient to satisfy CL=1, but this scheme would time that out) but never allow a write to succeed that would leave CL unsatisfied post-bootstrap (or if bootstrap is cancelled).","06/May/11 17:23;slebresne;Attaching patch that implements the ""simplification"" idea. The case of {LOCAL|EACH}_QUORUM requires some care but I think that by considering DC separately we are fine.","17/May/11 21:21;jbellis;v2 tweaks getWriteEndpoints to avoid new Collection creation where possible, instead using Iterables.concat.

otherwise lgtm.",08/Jun/11 15:37;slebresne;+1,08/Jun/11 15:46;jbellis;committed,"09/Jun/11 02:50;hudson;Integrated in Cassandra-0.8 #158 (See [https://builds.apache.org/job/Cassandra-0.8/158/])
    fix inconsistency window duringbootstrap
patch by slebresne; reviewed by jbellis for CASSANDRA-833

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1133443
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/DatacenterSyncWriteResponseHandler.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/locator/SimpleStrategyTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/LeaveAndBootstrapTest.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/MoveTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/AbstractReplicationStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AbstractWriteResponseHandler.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/ConsistencyLevelTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/WriteResponseHandler.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/DatacenterWriteResponseHandler.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/TokenMetadata.java
","20/Sep/12 17:40;jbellis;Well, WTF.

{noformat}
commit 063c8f6cf7b12e976b0d7067037c52c548c6c0db
Author: Jonathan Ellis <jbellis@apache.org>
Date:   Thu Jun 9 00:16:27 2011 +0000

    revert 1133443
    
    git-svn-id: https://svn.apache.org/repos/asf/cassandra/branches/cassandra-0.8@1133610 13f79535-47bb-0310-9956-ffa450edef68

commit 31f0ee95e927c09183dca77be7739305ba2eeab0
Author: Jonathan Ellis <jbellis@apache.org>
Date:   Wed Jun 8 15:45:54 2011 +0000

    fix inconsistency window duringbootstrap
    patch by slebresne; reviewed by jbellis for CASSANDRA-833
    
    git-svn-id: https://svn.apache.org/repos/asf/cassandra/branches/cassandra-0.8@1133443 13f79535-47bb-0310-9956-ffa450edef68
{noformat}

I have no memory of this. :)

Maybe it caused a regression?",21/Sep/12 01:11;jbellis;Pushed rebase to https://github.com/jbellis/cassandra/branches/833-3,21/Sep/12 01:21;jbellis;(Not exactly a pure rebase since I split out pendingRangesFor instead of cramming everything into getWriteEndpoints.),"21/Sep/12 08:39;slebresne;In counterWriteTask, sendToHintedEndpoints should be called with remotes, not targets.

But other than that it lgtm. I don't remember either why it was reverted and I don't remember any specific problem with that. But in any case, you reverted it almost right away, so if that wasn't accidental the regression was likely easy to spot, so we'll see soon enough :).",21/Sep/12 15:13;jbellis;committed w/ that fix,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
removetoken drops node from ring before re-replicating its data is finished,CASSANDRA-1216,12467577,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,nickmbailey,jbellis,jbellis,6/22/2010 14:28,3/12/2019 14:16,3/13/2019 22:24,9/28/2010 5:56,0.7 beta 2,,,,0,,,,,,this means that if something goes wrong during the re-replication (e.g. a source node is restarted) there is (a) no indication that anything has gone wrong and (b) no way to restart the process (other than the Big Hammer of running repair),,,,,,,,,,,,,,,,,,,,22/Sep/10 20:24;nickmbailey;0001-Modify-removeToken-to-be-similar-to-decommission.patch;https://issues.apache.org/jira/secure/attachment/12455298/0001-Modify-removeToken-to-be-similar-to-decommission.patch,22/Sep/10 20:24;nickmbailey;0002-Additional-tests-for-removeToken.patch;https://issues.apache.org/jira/secure/attachment/12455299/0002-Additional-tests-for-removeToken.patch,27/Sep/10 15:54;nickmbailey;0003-Fixes-from-review.patch;https://issues.apache.org/jira/secure/attachment/12455666/0003-Fixes-from-review.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,16:54.6,,,no_permission,,,,,,,,,,,,20035,,,Tue Sep 28 13:31:34 UTC 2010,,,,,,0|i0g3pj:,92031,gdusbabek,gdusbabek,,,,,,,,,02/Jul/10 15:16;nickmbailey;A possible solution I see for this is to keep nodes in the justRemovedEndpoints map in Gossiper until we can verify that replication has completed.  I think we could accomplish verification through a callback on the replicate request.  I'm unsure about what data gets persisted so I don't know if a restart would wipe out the justRemovedEndpoints map,02/Jul/10 16:12;nickmbailey;So clearly that solution would fail in the case of the node that is attempting to retrive the data failing.  Perhaps a better solution is simply not removing the node until replication and done.  Perhaps marking it with a new state?,"06/Jul/10 19:41;nickmbailey;It seems like this should follow a pattern similar to decommissioning a node.

* If nodeA has removeToken called on it, it becomes responsible for nodeB, the node to remove
* nodeA sets the MOVE_STATE of nodeB to STATE_REMOVING
* This is gossipped throughout the ring.
* Nodes see this change and fetch any ranges they are becoming responsible for
** After this is complete they will need to notify nodeA somehow that this is complete
* Once nodeA sees all replications have finished, change state of nodeB to STATE_REMOVED
* All nodes then remove nodeB from their ring.",06/Jul/10 19:42;jbellis;agreed,06/Jul/10 21:46;nickmbailey;A side effect of this approach may be that you would need to call removeToken on a node that had seen the token previously.,"06/Jul/10 23:19;jbellis;since all tokens will be propagated to all nodes (even ones brought up after the dead node went down), that's not a problem","20/Jul/10 22:37;nickmbailey;* 0001 - changes to make removeToken behave similarly to decomission
* 0002 - fixes to existing tests since the state for STATE_LEFT changed

I am still working on some good unit tests for these changes but these are the changes so far.

The new process for removeToken is basically the one outlined above. One change is that instead of a STATE_REMOVED state it seemed like tokens that are removed should just go into STATE_LEFT similar to nodes that are decommissioned.

One thing I'm not sure of is the timeout values for waiting for replications to stream and for waiting for replication notifications. Currently they are just set arbitrarily in that patch. Need to determine good values for these.
","27/Jul/10 20:53;nickmbailey;Some fixes and tests added.

There is one thing that still needs to be fixed.
 * Currently the call to removeToken blocks either:
 ** until all nodes confirm that they have replicated the data for the dead node.
 ** or a timeout is reached
 * I'm not sure what the timeout for this should be. Additionally when nodes throughout the ring attempt to replicate data there should be a similar timeout before they give up on a source and retry.
 * Also clients may timeout before the timeout is even reached or all the data is replicated. I'm not sure how the user will be able to determine if the remove finished correctly or repair should be run.  
",27/Jul/10 21:39;nickmbailey;Updated 0001 patch. It was missing a class before. Oops.,"13/Aug/10 14:36;gdusbabek;Nick, can you rebase?",13/Aug/10 20:11;nickmbailey;Rebased.,13/Aug/10 22:18;nickmbailey;Re-rebased.,"17/Aug/10 18:08;gdusbabek;RemoveTest needs some cleanup.
* ReplicationSink doesn't need callCount
* NotificationSink doesn't need hitList
* testRemoveToken and testStartRemoving abuse Gossiper.start().  Consider adding a method to Gossiper that initializes the epstate for a given node.  E.g.: initializeNodeUnsafe(InetAddr addr, int generation).
* (minor nit) I wish there were a way to assert that tmd.getLeavingNodes() actually has nodes in it.
* all the methods throw UnknownHostException, but don't need to (IOException covers it)
* testStartRemoving should assert preconditions before calling ss.onChange (it also makes the same assertion twice).

StorageService:
* (minor nit) a comment describing the distinction between the leaving and removing constants.
* SS.removeToken() shouldn't throw a RuntimeException, as the client won't know what to make of it.  Declare an exception in the interface and throw it in the impl.  I imagine this will be a fairly common case (e.g.: when a node is down).
* SS.setReplicatingNodes and clearReplicatingNodes can be inlined into removeToken. It saves a few lines and obviates a local var.
* SS.replicateTables should probably be merged into SS.restoreReplicaCount.

Was the intent that SS.replicateTables block until the files are transferred?  Because it doesn't.  AFAICT it blocks until the first ack comes back from each source node, which is a good indication that streaming has started, but not that it is finished.

I couldn't verify that the callbacks are ever called.  That happens on the READ_RESPONSE stage and afaict, none of the streaming code path ever puts a task there.  That's a painful interface to follow though, so I might be wrong.","18/Aug/10 19:51;nickmbailey;Yeah I wasn't really understanding that streaming/messaging code at all.

The current StreamOut implementation has a callback concept however.  I think this should be moved into the StreamContext object and then both StreamOut and StreamIn can perform callbacks on actual stream completion.  ","18/Aug/10 20:15;gdusbabek;The StreamOut callback works differently than the MessagingService callback.  Your approach sounds workable.  I don't think it matters where you push the callback to, so long as you make sure it gets executed after the stream is finished.","23/Aug/10 21:12;nickmbailey;bq. (minor nit) I wish there were a way to assert that tmd.getLeavingNodes() actually has nodes in it.

This is what tmd.isLeaving() does

bq. testStartRemoving should assert preconditions before calling ss.onChange (it also makes the same assertion twice).

I'm not sure what preconditions you mean. I added an assertion to make sure there are no endpoints already leaving.

bq. SS.removeToken() shouldn't throw a RuntimeException,

Do you think the UnsupportedOperationExceptions should be removed as well? These existed previously.

I modified the callback support for streaming so that the code should wait for all streams to finish before confirming. I also added a reply to the ReplicationFinishedHandler so the IAsyncResult will be updated.  

Thoughts?

The timeout values for waiting on the latches still need to be updated.
","25/Aug/10 14:51;gdusbabek;> Do you think the UnsupportedOperationExceptions should be removed as well? These existed previously.
My bad; I didn't notice that.  RTE was probably ok.

> I modified the callback support for streaming so that the code should wait for all streams to finish before confirming. I also added a reply to the ReplicationFinishedHandler so the IAsyncResult will be updated
First glance tells me this will work.  I'll run some tests after I'm done reviewing.

> The timeout values for waiting on the latches still need to be updated.
Is this coming in another patch?","25/Aug/10 16:30;gdusbabek;I see this in RemoveTest:


 [junit] Testsuite: org.apache.cassandra.service.RemoveTest
    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 1.97 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 11:27:58,277 Did not find matching ranges on /127.0.0.6
    [junit] ERROR 11:27:58,279 Did not find matching ranges on /127.0.0.6
    [junit] ERROR 11:27:58,280 Did not find matching ranges on /127.0.0.5
    [junit] ERROR 11:27:58,280 Did not find matching ranges on /127.0.0.4
    [junit] ERROR 11:27:58,280 Did not find matching ranges on /127.0.0.2
    [junit] ERROR 11:27:59,264 Did not find matching ranges on /127.0.0.6
    [junit] ERROR 11:27:59,272 Did not find matching ranges on /127.0.0.6
    [junit] ERROR 11:27:59,276 Did not find matching ranges on /127.0.0.5
    [junit] ERROR 11:27:59,279 Did not find matching ranges on /127.0.0.4
    [junit] ERROR 11:27:59,283 Did not find matching ranges on /127.0.0.2
    [junit] ------------- ---------------- ---------------

Is that ok?","25/Aug/10 16:38;nickmbailey;Re: timeouts

Yes I'm just not sure how to approach determining the right values for these.  Depends mostly on the amount of data and network bandwidth.

Re: RemoveTest

Yeah. The message sink in the test immediately responds to the stream request saying there are no files to stream.  This makes the StreamInManager think the data didn't exist remotely.  Doing it that way seems much easier than trying to make the test actually stream something.","25/Aug/10 16:42;gdusbabek;Some questions about the coordinator...  I see that removeToken() is quasi-blocking now, like unbootstrap() (it was fire-and-forget before).  What are the consequences of the coordinator node going down?  Assuming a dead coordinator, would it be Bad for another node to remove-token on the same token while the transfers initiated by the original failed coordinator were in process?  Or assuming the transfers were finished, would a remove-token on a new coordinator generally do little other than get the state to LEFT?

I think I'm of the opinion that removeToken should either block until the transfer is complete (or failed), or should return instantly, and that we need to make sure that subsequent removeToken calls do not upset existing transfers.  Having it return error after a timeout (which is possible in the case of LOTS of data) makes me think we should be doing differently.

Or is the only recourse to repair?","25/Aug/10 17:06;nickmbailey;I believe the only consequences of calling removeToken on another node when the coordinator goes down would be that the entire operation would be repeated. So any data that was transferred before would be transferred again.  I think this is the right behavior since there is no way of knowing what was transferred before the coordinator went down.  

It might be useful to add a 'force' option though.  If the coordinator goes down and the token gets stuck in a REMOVING state you may want to force removal rather than redoing the entire operation. 

It should be possible to remove the timeout so that removeToken blocks until the transfer is completely finished.  The code for streaming in the remote data blocks until all streams are complete and the code for sending a confirmation to the coordinator will keep retrying until it is received or the coordinator dies.  

I think this would work if a check was added so that you can only call removeToken a second time if the coordinator is down.  It wouldn't handle two calls that occurred before the state made its way through gossip though.  

","27/Aug/10 19:06;nickmbailey;After some more thinking I think there are two problems here.

 * The timeout for waiting on a stream to complete - An arbitrary timeout here is not the right way to do this. What we really need is the concept of stream progress. We should be able to verify that a stream is progressing or not and based on that retry it.  CASSANDRA-1438 kind of relates to this problem and could be modified to implement this.  

 * The timeout waiting for nodes to confirm replication - Ideally there could be no timeout here. The problem though is if a node that should be grabbing data goes down permanently, removeToken will wait forever.  I think it's reasonable to have some sort of timeout in this case. A log message/error can indicate which machines were being waited on for replication. An administrator should know if that machine went down or is still streaming. That will determine if repair needs to be run.  The alternative to this I guess would be periodically waking up and checking that the nodes we are waiting on are still alive.  That wouldn't be particularly hard to implement

I don't think returning immediately from the call is the right approach.  That is part of the reason why this ticket is created. In the case that replication fails somewhere, there is no feedback to the user.  At least timing out eventually provides information about which machines we think failed to replicate data.  

As far as multiple remove calls and the coordinator going down.  I think there should be a 'force' option in the case the coordinator goes down and you believe the rest of the nodes completed the operation.  To prevent multiple calls to removeToken there should just be a check to make sure the coordinator is dead before another call can be performed.

So besides those few changes above, I think we should either implement this part way with a time out for stream replication or postpone completion here until we add the concept of stream progress.
","21/Sep/10 19:43;nickmbailey;Patches:
 * 0001
 ** Modifies the removeToken operation to follow a pattern of NORMAL->REMOVING->LEFT, rather than the current pattern of a coordinator node setting its own status to a special cased version of NORMAL.
 ** Fixes a small bug in StreamHeader serialization
 ** Adds the ability to either get the status of a remove operation taking place or force a remove operation to finish immediately
 * 0002
 ** Tests for removing tokens
 ** Move shared code for creating a ring to Util class


Removal Process:
 * Normal Case
 *# Coordinator sets status of failed node to REMOVING
 *# Coordinator blocks on confirmation from other nodes
 *# Any newly responsible nodes stream data
 *# Newly responsible nodes send confirmation once all data has streamed
 *# Coordinator updates status of failed node to LEFT
 *# Done
 * Failure Cases
 ** Coordinator failure
 *** If the coordinator fails the remove operation will need to be retried
 *** This can be done on any node in the cluster.  
 **  Newly responsible node failure
 *** If a newly responsible node fails but comes back up, it should see the REMOVING status in gossip and restart the operation
 *** If a newly responsible node fails permanently or a streaming operation fails and the node stays up, the coordinator will block forever while waiting for confirmation.  The best solution is to force the remove operation to complete and then run repair on the failed node.",22/Sep/10 16:01;nickmbailey;Bah.  Gossip marks the node alive when it receives an updated application state. Reverting it to modifying the coordinator nodes state.,"22/Sep/10 20:29;nickmbailey;Ok this should be ready for review now.  The process is:

# Coordinator node modifies its own status to NORMAL - REMOVING to indicate which node is being removed
# Coordinator blocks on removal confirmaton from other nodes
# Newly responsible nodes see this status and begin fetching new data
# Newly responsible nodes notify coordinator they have replicated all data
# Coordinator node updates its own status to NORMAL - REMOVED to indicate the removal is complete
# This causes all nodes to remove the node from gossip/tokenmetadata. 
# Done

Tested this with a 3 node cluster in the cloud, as well as testing the new getStatus and forceRemoval operations.","27/Sep/10 07:58;gdusbabek;This looks good.

1.  There were a few unused local variables in SS.retoreReplicationCount().  Was this just leftovers from a rebase?
2.  SS.handleStateRemoving removes a null check that previously existed for epThatLeft (renamed removeEndpoint).  Was the original null-check pointless or was something missed in the change?
3.  You made a change to StreamHeader that made me think you were running into cases where SH.pendingFiles == null.  Is that true?  Tracing the codepaths makes me think this is not possible.

Don't bother with the cleanup in 1.  I'm more curious about 2 and 3.","27/Sep/10 15:54;nickmbailey;1 and 2 are just errors on my part. I changed 3 because I was under the impression that a stream request to an endpoint that doesn't contain any of the ranges requested would create a header with null for pendingFiles.  I at first wrote one of the tests to behave like that, and got the NPE.  Looks like it changed or was never like that.

Fixed all that in a quick patch and attached it.",28/Sep/10 05:56;gdusbabek;committed.,"28/Sep/10 13:31;hudson;Integrated in Cassandra #549 (See [https://hudson.apache.org/hudson/job/Cassandra/549/])
    changes update for CASSANDRA-1216
modify removetoken so that the coordinator relies on replicating nodes for updates. patch by Nick Bailey, reviewed by Gary Dusbabek. CASSANDRA-1216
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Store AccessLevels externally to IAuthenticator,CASSANDRA-1237,12468077,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,6/28/2010 18:29,3/12/2019 14:16,3/13/2019 22:24,8/25/2010 20:43,0.7 beta 2,,,,0,,,,,,"Currently, the concept of authentication (proving the identity of a user) is mixed up with permissions (determining whether a user is able to create/read/write databases). Rather than determining the permissions that a user has, the IAuthenticator should only be capable of authenticating a user, and permissions (specifically, an AccessLevel) should be stored consistently by Cassandra.

EDIT: Adding summary

----

In summary, there appear to be 3 distinct options for how to move forward with authorization. Remember that this ticket is about disconnecting authorization (permissions) from authentication (user/group identification), and its goal is to leave authentication pluggable.

Options:
# Leave authentication and authorization in the same interface. If we choose this option, this ticket is invalid, and CASSANDRA-1271 and CASSANDRA-1320 will add-to/improve IAuthenticator
** Pros:
*** Least change
** Cons:
*** Very little actually implemented by Cassandra: burden is on the backend implementors
*** Each combination of authz and authc backends would require a new implementation (PAM for authc + permissions keyspace for authz, for instance), causing an explosion of implementations
# Separate out a pluggable IAuthority interface to implement authorization
## IAuthenticator interface would be called at login time to determine user/groups membership
## IAuthority would be called at operation time with the user/groups determined earlier, and the required permission for the operation
** Pros:
*** Provides the cleanest separation of concerns
*** Allows plugability for permissions
** Cons:
*** Pluggability of permissions gains limited benefit
*** IAuthority would need to support callbacks for keyspace/cf creation and removal to keep existing keyspaces in sync with their permissions (although technically, option 1 suffers from this as well)
# Separate authorization, but do not make it pluggable
** This option is implemented by the existing patchset by attaching permissions to metadata, but could have an alternative implementation that stores permissions in a permissions keyspace.
** Pros:
*** Cassandra controls the scalability of authorization, and can ensure it does not become a bottleneck
** Cons:
*** Would need to support callbacks for user creation and removal to keep existing users in sync with their permissions",,,,,,,,,,,,,,,,,,,,28/Jul/10 16:58;stuhood;0001-Consolidate-KSMetaData-mutations-into-copy-methods.patch;https://issues.apache.org/jira/secure/attachment/12450717/0001-Consolidate-KSMetaData-mutations-into-copy-methods.patch,28/Jul/10 16:58;stuhood;0002-Thrift-and-Avro-interface-changes.patch;https://issues.apache.org/jira/secure/attachment/12450718/0002-Thrift-and-Avro-interface-changes.patch,28/Jul/10 16:58;stuhood;0003-Add-user-and-group-access-maps-to-Keyspace-metadata.patch;https://issues.apache.org/jira/secure/attachment/12450719/0003-Add-user-and-group-access-maps-to-Keyspace-metadata.patch,28/Jul/10 16:58;stuhood;0004-Remove-AccessLevel-return-value-from-login-and-retur.patch;https://issues.apache.org/jira/secure/attachment/12450720/0004-Remove-AccessLevel-return-value-from-login-and-retur.patch,28/Jul/10 16:58;stuhood;0005-Move-per-thread-state-into-a-ClientState-object-1-pe.patch;https://issues.apache.org/jira/secure/attachment/12450721/0005-Move-per-thread-state-into-a-ClientState-object-1-pe.patch,28/Jul/10 16:58;stuhood;0006-Apply-access.properties-to-keyspaces-during-an-upgra.patch;https://issues.apache.org/jira/secure/attachment/12450722/0006-Apply-access.properties-to-keyspaces-during-an-upgra.patch,25/Aug/10 02:29;stuhood;sample-usage.patch;https://issues.apache.org/jira/secure/attachment/12453007/sample-usage.patch,29/Jul/10 18:23;messi;simple-jaas-authenticator.patch;https://issues.apache.org/jira/secure/attachment/12450850/simple-jaas-authenticator.patch,25/Aug/10 02:21;stuhood;v2-0001-Remove-AccessLevel-return-value-from-login-since-aut.patch;https://issues.apache.org/jira/secure/attachment/12453002/v2-0001-Remove-AccessLevel-return-value-from-login-since-aut.patch,25/Aug/10 02:29;stuhood;v2-0002-Add-IAuthority-and-split-login-into-authenticate-aut.patch;https://issues.apache.org/jira/secure/attachment/12453005/v2-0002-Add-IAuthority-and-split-login-into-authenticate-aut.patch,25/Aug/10 02:21;stuhood;v2-0003-Factor-out-reflection-based-class-construction.patch;https://issues.apache.org/jira/secure/attachment/12453004/v2-0003-Factor-out-reflection-based-class-construction.patch,25/Aug/10 02:21;stuhood;v2-0004-Add-configuration-for-IAuthority-and-handle-SimpleAu.patch;https://issues.apache.org/jira/secure/attachment/12453003/v2-0004-Add-configuration-for-IAuthority-and-handle-SimpleAu.patch,25/Aug/10 02:29;stuhood;v2-0005-Separate-authentication-and-authorization-into-Clien.patch;https://issues.apache.org/jira/secure/attachment/12453006/v2-0005-Separate-authentication-and-authorization-into-Clien.patch,13,,,,,,,,,,,,,,,,,,,40:21.3,,,no_permission,,,,,,,,,,,,20043,,,Wed Aug 25 20:43:17 UTC 2010,,,,,,0|i0g3tz:,92051,urandom,urandom,,,,,,,,,20/Jul/10 23:24;stuhood;Patchset to separate permissions from authentication.,"20/Jul/10 23:25;stuhood;The set currently uses our old-school KSMetaData serialization, so it will need a rebase when CASSANDRA-1186 makes it in.",21/Jul/10 19:43;stuhood;Rebased post 1186.,"21/Jul/10 19:46;stuhood;This should be ready for review.

One thing that might block it getting committed, though, is that we probably need to automate the conversion from 'access.properties' to per-keyspace access maps. Perhaps during loadFromYaml, we could check for the existence of an 'access.properties' file, and apply the properties found there to the new keyspaces?","21/Jul/10 20:40;messi;Failing to authenticate correctly is not an exception. OTOH, accessing a keyspace you're not authorized for is. All RPC methods except ""login"" should throw AuthorizationException.

While you're at it, please look at CASSANDRA-974. I suggest renaming ""login"" to ""authenticate"" and have it return Map<String,String> to make it future-proof for SASL authentication schemes, like DIGEST-MD5.","22/Jul/10 17:15;stuhood;> Failing to authenticate correctly is not an exception.
(Keeping in mind that I didn't change any of the Exception handling/throwing in this patch) I think I agree with the original decision. A failed authentication should be extremely rare, and therefore exceptional.

> All RPC methods except ""login"" should throw AuthorizationException.
Agreed, but out of the scope of this ticket.

> While you're at it, please look at CASSANDRA-974. I suggest renaming ""login"" to ""authenticate""
I'm thinking of the IAuthenticator interface and login() methods as stopgaps until Avro adds support for SASL in their custom protocol (AVRO-341), so I don't think breaking the Thrift API right now is worth it.","22/Jul/10 20:12;stuhood;Rebased for trunk, and added 0006 to handle upgrades by parsing the deprecated 'access.properties' when users call readTablesFromYaml.

This should be ready for review.",26/Jul/10 19:55;stuhood;Rebased for trunk.,"27/Jul/10 02:19;messi;Attached patch shows a simple IAuthenticator for JAAS. With JAAS you can configure LoginModules for Unix users or PAM, for LDAP or Kerberos and you can also write your own LoginModule reading passwd.properties files or even column families. In fact, I have a SimpleLoginModule (similar to SimpleAuthenticator) half ready.

This authenticator is also not finished, yet. I submitted it because I hope it's not too late to urge you to make/keep IAuthenticator as lightweight as possible.
The proposed defaultUser() would make IAuthenticators somewhat stateful. Not good.

If you're interested I can open a new issue and submit my JAAS classes there including sample config files and a programmatic JAAS configuration.","27/Jul/10 02:21;messi;By the way, I think the constants USERNAME_KEY and PASSWORD_KEY should go into IAuthenticator because these are common keys used by all authenticators.","27/Jul/10 18:37;gdusbabek;Folke, I'd be interested in seeing your classes (on a separate ticket).","27/Jul/10 18:38;gdusbabek;0003: unavronateAccessMap is checking for null on the wrong variable.
0004: is there a way to not have a default user? I think it adds some noise to the interface.  

Feel free to make breaking changes (no need to support access.properties) if it simplifies things.  Our authentication API has been explicitly 'experimental' from day one.","27/Jul/10 21:29;messi;In AuthenticatedUser.toString(): String.format() is a static method that takes the format as its first parameter.

Again, please drop defaultUser() and put the ""super admin"" status directly into AuthenticatedUser or use a not configurable pseudo group for super admins.","28/Jul/10 16:58;stuhood;* 0003: Fixed unavronateAccessMap null check (good eye!)
* 0004: Moved 'isSuper' onto AuthenticatedUser
* 0004: Fixed static/instance String.format usage
* 0005: Fixed an error where calling set_keyspace before login would fail

----

I can't think of a good way to remove defaultUser: it replaced lots of (authenticator instanceof AllowAllAuthenticator) calls, which existed to check whether it was necessary for the user to login.","28/Jul/10 19:03;messi;I see.

A few suggestions then:
- rename ""AuthenticatedUser"" to just ""User"".
- add ""isAuthenticated"" state to User.
- add package private getter(s) and setter(s) that allow Authenticators to manipulate their User objects.
- add ""User"" as argument to ""login"" (and ""logout""). Authenticators use the credentials to fill in the details.
- rename ""defaultUser"" to ""createUser"" to make it clear that it's a factory method that must always return a (new) User who may or may not already be authenticated.

I know. It's probably too much.",28/Jul/10 21:05;gdusbabek;+1 committed.,29/Jul/10 03:36;jbellis;committing to a users-and-groups approach seems very wrongheaded to me.  the approach of pluggable authenticators to PAM etc seems much better to me (although it's hard to say now since that patch was deleted -- bad practice there...),"29/Jul/10 03:50;jbellis;similarly, the AccessLevel change is moving 180 degrees in the wrong direction -- we _want_ to push that into the authenticator rather than hard-coding it somewhere :(","29/Jul/10 13:14;hudson;Integrated in Cassandra #503 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/503/])
    apply access.properties to KSM during loadSchemaFromYaml. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1237
move CS threadlocals into single object. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1237
new IAuthenticator interface. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1237
push access structures into KSM. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1237
put access structures in KsDef. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1237
move KSM modification code into copy methods. patch by stuhood, reviewed by gdusbabek. CASSANDRA-1237
","29/Jul/10 16:29;urandom;On the one hand, decoupling access levels seems like a Good Move, and from that stand-point this is an improvement over the status quo, but I disagree with the decision to bundle the access levels in keyspace definitions.

Wouldn't it be reasonable to create another interface (IAuthority or whatever) and off-load how access levels are persisted to implementations (similar to how it is done with IAuthenticator)?

-1 (in the ASF-sense), I believe this should be rolled back.

EDIT: and I do apologize for coming into this late, as opposed to when it was actively being discussed.","29/Jul/10 16:32;urandom;
{quote}
Attached patch shows a simple IAuthenticator for JAAS. With JAAS you can configure LoginModules for Unix users or PAM, for LDAP or Kerberos and you can also write your own LoginModule reading passwd.properties files or even column families. In fact, I have a SimpleLoginModule (similar to SimpleAuthenticator) half ready.
This authenticator is also not finished, yet. I submitted it because I hope it's not too late to urge you to make/keep IAuthenticator as lightweight as possible.
The proposed defaultUser() would make IAuthenticators somewhat stateful. Not good.

If you're interested I can open a new issue and submit my JAAS classes there including sample config files and a programmatic JAAS configuration.
{quote}

What happened to this? This sounds quite interesting.","29/Jul/10 17:01;stuhood;> the approach of pluggable authenticators to PAM
> we want to push that into the authenticator rather than hard-coding it somewhere
I feel like you're mixing up 'authentication' with 'permissions/authorization'... the reasoning behind this ticket is that backends like PAM aren't designed to provide storage for permissions. Folke's JAAS example is a great example of authentication (and what he coded will still apply post 1237), but I haven't seen any JAAS/PAM backends that implement permissions storage.

> Wouldn't it be reasonable to create another interface (IAuthority or whatever) and off-load how access levels are persisted
I think that would be a mistake, without a working backend that filled the interface. Then, imagine how that backend might fill that interface, and I expect you'll come up with either storing the permissions in their own Keyspace, storing them in a backend specific manner, or attaching them as metadata to the keyspace. The first option is an alternative that should only be implemented once, and therefore shouldn't have an interface. The second could be handled by the IAuthenticator, and is what we have now. The third option is what is implemented here.","29/Jul/10 17:39;urandom;bq. ...the reasoning behind this ticket is that backends like PAM aren't designed to provide storage for permissions.

PAM is a bad example here, and there are plenty of other (more relevant) services and frameworks that do, (think LDAP, radius, tacacs, etc).

bq. I think that would be a mistake, without a working backend that filled the interface.

Right, you'd need at least the equivalent of SimpleAuthenticator.

bq. Then, imagine how that backend might fill that interface, and I expect you'll come up with either storing the permissions in their own Keyspace, storing them in a backend specific manner, or attaching them as metadata to the keyspace. The first option is an alternative that should only be implemented once, and therefore shouldn't have an interface. The second could be handled by the IAuthenticator, and is what we have now. The third option is what is implemented here.

If authorization should be pluggable (I've argued that is should be), then ""backend specific"" is the only option that makes sense.

","29/Jul/10 18:23;messi;I'm sorry I deleted my patch too early. I wanted to open a new ticket but when I looked over my code I thought that it's not ready and I want to work on it over the weekend.

The attached patch adds several classes:
- +JAASAuthenticator:+ an IAuthenticator implemention that uses a LoginContext to request authentication against configured LoginModules. Very simple stuff.
- +SimpleLoginModule:+ LoginModule impl that could replace SimpleAuthenticator.
- +SimpleLoginModuleConfiguration:+ programmatic configuration of LoginModules.
- +User/Group:+ implementations of java.security.Principal added by SimpleLoginModule to the Subject that is passed from the LoginContext to all configured LoginModules.

This is only proof-of-concept code. I hope I have something cleaner and more robust ready by Sunday.","29/Jul/10 18:45;jbellis;I agree that committing this was premature, and also apologize for not looking at it earlier.","29/Jul/10 19:09;stuhood;> If authorization should be pluggable (I've argued that is should be)
I'd be interested in seeing the reasons for making permissions pluggable, if you know where I can find the thread.","29/Jul/10 19:15;gdusbabek;>> If authorization should be pluggable (I've argued that is should be)
>I'd be interested in seeing the reasons for making permissions pluggable, if you know where I can find the thread.

Separation of concerns.  I tried to make the argument yesterday that mixing KS definitions with KS permissions was not the right design.","29/Jul/10 19:25;stuhood;> Separation of concerns. I tried to make the argument yesterday that mixing KS definitions with KS permissions was not the right design.
Separation of concerns is not an argument for making it pluggable (pluggable implies multiple implementations), although it is an argument for storing the permissions somewhere other than in the metadata for the keyspace.","29/Jul/10 19:43;urandom;{quote}
> If authorization should be pluggable (I've argued that is should be)
I'd be interested in seeing the reasons for making permissions pluggable, if you know where I can find the thread.
{quote}

Directory services like LDAP and Active Directory seem like prominent examples of existing systems that people might want to integrate with for authorization, (as well as authentication). And, I'm sure there are plenty of people with existing databases, web services, etc that would appreciate the opportunity to integrate instead of duplicating that information.","29/Jul/10 20:22;messi;I don't think you can make authorization pluggable: authorization is very specific, it requires predefined permissions and/or group/role names, and authz is done at many places in the code. I recommend an authorization infrastructure that does not authorize users directly but groups or roles. An authenticator must return a list of groups a user belongs to and the authz infrastructure renders a list of permissions. In Cassandra you just ask if UserX.isAllowedTo(READ, ""Keyspace1"");","30/Jul/10 13:37;hudson;Integrated in Cassandra #504 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/504/])
    revert 980215, 980217, 980220, 980222, 980225, 980226. CASSANDRA-1237
","16/Aug/10 19:49;stuhood;In summary, there appear to be 3 distinct options for how to move forward with authorization. Remember that this ticket is about disconnecting authorization (permissions) from authentication (user/group identification), and its goal is to leave authentication pluggable.

Options:
# Leave authentication and authorization in the same interface. If we choose this option, this ticket is invalid, and CASSANDRA-1271 and CASSANDRA-1320 will add-to/improve IAuthenticator
** Pros:
*** Least change
** Cons:
*** Very little actually implemented by Cassandra: burden is on the backend implementors
*** Each combination of authz and authc backends would require a new implementation (PAM for authc + permissions keyspace for authz, for instance), causing an explosion of implementations
# Separate out a pluggable IAuthority interface to implement authorization
## IAuthenticator interface would be called at login time to determine user/groups membership
## IAuthority would be called at operation time with the user/groups determined earlier, and the required permission for the operation
** Pros:
*** Provides the cleanest separation of concerns
*** Allows plugability for permissions
** Cons:
*** Pluggability of permissions gains limited benefit
*** IAuthority would need to support callbacks for keyspace/cf creation and removal to keep existing keyspaces in sync with their permissions (although technically, option 1 suffers from this as well)
# Separate authorization, but do not make it pluggable
** This option is implemented by the existing patchset by attaching permissions to metadata, but could have an alternative implementation that stores permissions in a permissions keyspace.
** Pros:
*** Cassandra controls the scalability of authorization, and can ensure it does not become a bottleneck
** Cons:
*** Would need to support callbacks for user creation and removal to keep existing users in sync with their permissions","16/Aug/10 20:24;urandom;The pressing problem is that the current implementation predates dynamically created keyspaces/column families and as a result it's not possible to add/remove keyspaces with auth enabled.  It's also quite late in the 0.7 cycle so ""least change"" from #1 seems quite appealing.","16/Aug/10 20:37;stuhood;> The pressing problem is that the current implementation predates dynamically created keyspaces/column families
Correct.. CASSANDRA-1271 addresses that issue. I think it is too late to make any of these changes in 0.7, so I'm interested in what you think we can/should do for 0.8.","16/Aug/10 22:15;toulmean;I am in favor of 3. I like that it keeps scalability over checking permissions, and I like it is made pluggable for people to integrate with LDAP or their own framework.","16/Aug/10 22:22;urandom;bq. I am in favor of 3. I like that it keeps scalability over checking permissions, and I like it is made pluggable for people to integrate with LDAP or their own framework.

3 is decidedly _not_ pluggable with respect to authorization (only authentication).   Also, there is also nothing to prevent you from implementing a back-end that stored credentials and/or permissions in Cassandra. ","17/Aug/10 20:07;rnirmal;Here's some probable use cases for Authz:
* User A / Group B -> read/write KS1, KS2: read KS3
* User C / Group D -> create/rename/drop KS/CF
* User E -> read/write KS1-CF1, KS1-CF2: read KS1-CF3
* Admin -> add/modify/delete Users/groups/permissions","23/Aug/10 19:27;stuhood;I think it should be possible to implement option #2 before 0.7 final, if we defer implementing the 'callbacks' until we have a backend that can support modifying its permissions (SimpleAuthenticator cannot).

Split {{AccessLevel IAuthenticator.login(String keyspace, Map credentials)}} into:
* {{AuthenticatedUser IAuthenticator.authenticate(Map credentials)}}
** Where AuthenticatedUser is similar to the implementation in the existing patchset, but without a 'isSuper' flag, since the IAuthority should make that decision
* {{AccessLevel IAuthority.authorize(AuthenticatedUser user,  String keyspace)}}
** In CASSANDRA-1320, AccessLevel will be replaced with a Set<Permission>
** In CASSANDRA-1271, keyspace will be replaced with a generic 'resource' hierarchy

Does this sound reasonable?","23/Aug/10 20:04;urandom;bq. Does this sound reasonable?

It sounds reasonable to me.","25/Aug/10 02:29;stuhood;Implementation of option 2:

v2-0001 - Removes AccessLevel return value from login in the client APIs
v2-0002 - Splits IAuthority out of IAuthenticator: SimpleAuthority and SimpleAuthenticator are backwards compatible
v2-0003 - Removes some reflection-centered duplication
v2-0004 - Allows configuration of an IAuthority, and specifies SimpleAuthority for upgraders who have configured SimpleAuthenticator
v2-0005 - Splits the authc/z ThreadLocals out of the CassandraServers and into a ClientState object",25/Aug/10 02:31;stuhood;Optimistically targetting back to 0.7-beta2 (sorry for the flapping).,25/Aug/10 20:43;urandom;committed; thanks!,,,,,,,,,,,,,,,,,,,
Cassandra AVRO api is missing system_add_column_family,CASSANDRA-1238,12468180,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,toulmean,next2you,next2you,6/29/2010 17:45,3/12/2019 14:16,3/13/2019 22:24,8/20/2010 21:37,0.7 beta 2,,,,0,,,,,,"The cassandra.avpr does contain the method system_add_keyspace but is missing the system_add_column_family.

Workaround: have to revert to the thrift API to create column families.
","Mac OSX 10.6.4, Cassandra Trunk, Rev. 958403, java 1.6.0_20",,,,,,,,,,,,,,,,,,,14/Aug/10 00:56;toulmean;001-CASSANDRA-1238.patch;https://issues.apache.org/jira/secure/attachment/12452073/001-CASSANDRA-1238.patch,20/Aug/10 21:34;urandom;002-CASSANDRA-1238-errata.patch;https://issues.apache.org/jira/secure/attachment/12452668/002-CASSANDRA-1238-errata.patch,20/Aug/10 06:45;toulmean;002-CASSANDRA-1238.patch;https://issues.apache.org/jira/secure/attachment/12452609/002-CASSANDRA-1238.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,08:38.1,,,no_permission,,,,,,,,,,,,20044,,,Sat Aug 21 11:14:35 UTC 2010,,,,,,0|i0g3u7:,92052,urandom,urandom,,,,,,,,,"29/Jun/10 18:08;urandom;Avro support is in-progress and considered experimental. There are still a number of unimplemented RPC methods.

That said, you can create column families and keyspaces at once using {{system_add_keyspace}} but supplying a list of CfDefs (see the system tests for an example of this).","14/Aug/10 00:56;toulmean;Here is a patch that adds this method. It adds the method to the Avro API by patching the Avro protocol and implements the method on CassandraServer, mostly by copy/pasting the code from add_system_keyspace, and using the AddColumnFamily migration to apply the change.

I could not find the tests you were referring to. I would be much more comfortable if that patch came with tests, but I could not spot them in the source code. If you would be so kind as to give me their location, I'd happily contribute tests for this change.","14/Aug/10 01:48;urandom;First off, thanks for taking the time to work on this.

As for the patch, it would be better to model this method after the corresponding one in the Thrift version of CassandraServer, instead of add_system_keyspace().

The tests for this are written in Python and can be found in test/system/test_avro_server.py (let me know if you need help with this).","14/Aug/10 14:27;toulmean;Thanks Eric. I will rework the patch to follow Thrift, and will look at the python tests.","20/Aug/10 06:45;toulmean;Here is a second patch that brings a few new methods to the Avro API. It should cover quite a bit. All tests pass.

This patch is made against the current trunk and ready for review. Enjoy!",20/Aug/10 21:34;urandom;002-CASSANDRA-1238-errata.patch is just here to document the changes I made ontop of 002-CASSANDRA-1238.patch.  This has been committed as one changeset.,"20/Aug/10 21:37;urandom;This has been committed with a few changes.  Some of these changes were for conformance with our coding conventions (brace placement, etc), some where normative, but all of them were minor. 

See 002-CASSANDRA-1238-errata.patch for the specific edits I made.

Thanks Antoine!","21/Aug/10 11:14;hudson;Integrated in Cassandra #518 (See [https://hudson.apache.org/hudson/job/Cassandra/518/])
    several new avro rpc method implementations

Patch by Antoine Toulme w/ some changes by eevans for CASSANDRA-1238
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading with CL > ONE returns multiple copies of the same column per key.,CASSANDRA-1145,12465813,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jeromatron,ajslater,ajslater,5/31/2010 23:23,3/12/2019 14:16,3/13/2019 22:24,8/5/2010 14:09,0.6.5,,,,0,,,,,,"Testing with 0.6-trunk today:

Reading with CL > ONE returns multiple copies of the same column per key consistent with the replicas queried before return. i.e, for RC=3, a QUORUM read yields 2 copies and an ALL read returns 3.
This is with pycassa get_range() which is using get_range_slice()

I see the same behavior with 0.6.1 and 0.6.2 debs

If my experience is not unique, anyone using get_range_slice is now deluged with duplicate data.
",ubuntu jaunty,,,,,,,,,,,,,,,,,,,04/Aug/10 20:44;jeromatron;0001-Added-a-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12451265/0001-Added-a-unit-test.patch,04/Aug/10 23:45;jeromatron;0001-Sort-keys.patch;https://issues.apache.org/jira/secure/attachment/12451281/0001-Sort-keys.patch,05/Aug/10 12:54;jbellis;1145-v2.txt;https://issues.apache.org/jira/secure/attachment/12451327/1145-v2.txt,31/May/10 23:28;ajslater;bugtest.py;https://issues.apache.org/jira/secure/attachment/12445975/bugtest.py,04/Aug/10 22:03;messi;demo.patch;https://issues.apache.org/jira/secure/attachment/12451271/demo.patch,31/May/10 23:28;ajslater;storage-conf.xml;https://issues.apache.org/jira/secure/attachment/12445976/storage-conf.xml,,,,,,,,6,,,,,,,,,,,,,,,,,,,31:24.7,,,no_permission,,,,,,,,,,,,20006,,,Thu Aug 05 14:09:20 UTC 2010,,,,,,0|i0g39r:,91960,jbellis,jbellis,,,,,,,,,"31/May/10 23:28;ajslater;attached is a storage-conf and example python program. 
it requires the python thrift bindings and pycassa: http://github.com/vomjom/pycassa

usage:  ./bugtest.py [TEXT1 TEXT2..]

arguments on the command line are inserted into cassandra and then range_get is used to get and pretty print them, with timestamps.

it counts the multiple copies of the same rows at the end.","31/May/10 23:31;jeromatron;Hi AJ, could you try to replicate it with the latest 0.6 branch just to see if it wasn't fixed by another fix (cassandra-1042)?  Just wondering if it's a the same bug but showing itself in more than one place.

http://svn.apache.org/repos/asf/cassandra/branches/cassandra-0.6/",31/May/10 23:34;ajslater;I saw the same behavior with cassandra-0.6 svn revision 949892,"31/May/10 23:40;jeromatron;Okay yeah - that fix was in revision 948934, so it looks like it's a different issue.  Tx.","16/Jun/10 21:18;joosto;We're getting duplicate rows from get_range_slice when NOT using ConsistencyLevel.ONE.  That is to say, when using QUORUM or ALL, we're getting duplicate rows, but when using ONE, we are not. Is this case misnamed?","16/Jun/10 21:25;jbellis;Joost: :) no, it really is a bug

Jeremy: could you have a look?","06/Jul/10 23:45;jeromatron;Hi AJ, taking another look at this.  I'm getting errors when I try to put ('localhost:9160') in the SERVERS variable in bugtest.py.  Specifically, I'm seeing that pycassa is giving a NoServerAvailable exception when trying to connect.  So I'm unable to reproduce the problem.

Can you see if you can get it to run and hopefully replicate the problem just with the localhost?  I'm not sure if I'm just configuring that incorrectly either. ","07/Jul/10 00:00;ajslater;Looks like pycassa has a problem with tuples with one element, which is another problem. So to fix, use a list:

SERVERS= ['localhost:9160']

Also I ran into your NoServerAvailable problem, using localhost:9160.
Cassandra is likely not really listening on 'localhost:9160', try 'myServerName:9160'  or use the exact IP you specify to thrift in storage-conf.xml
","07/Jul/10 02:18;jeromatron;Sweet sassafras - thanks AJ, that was it - matching the localhost in the SERVERS variable to localhost in the rpc address in storage-conf.xml.  Looking at it more now.",04/Aug/10 20:44;jeromatron;Added a unit test that will expose the problem.,04/Aug/10 22:03;messi;CollatingIterator expects both collections to be sorted. See demo.patch.,"04/Aug/10 22:26;jeromatron;Folke - you're right.  I'll focus on the creation of the list of rows.  I had thought we looked at the ordering, but I think since they were in the same order, we didn't think twice.  Good catch.","04/Aug/10 23:45;jeromatron;Adding a fix patch that will sort the keys before getRangeSlice uses them to get their associated rows.  That will keep them sorted when they are sent back to the client, for the CollatingIterator.","05/Aug/10 12:54;jbellis;The order we read them in CFS [token order] is the ""right"" order.  Stomping on that will take as back to CASSANDRA-1042 type bugs.  

Does v2 (attached) fix the problem?","05/Aug/10 13:50;jeromatron;Yes - I wondered if what I had done would affect other things, such as reverting other changes.  Thanks.  I could see where the problem was, but wasn't 100% sure where to make the change.  Sounds like they were sorted correctly wrt to token, which our CollatingIterator needed to take into account.

I ran the same tests again that could formerly reproduce the problem (with several permutations) and it fixes the problem.

I also ran `ant test` and `nosetests` to make sure everything else was happy.

+1 from me.","05/Aug/10 14:09;jbellis;committed, with a slight modification to allow the unit test to pass in a different partitioner than the global one",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OrderPreservingPartitioner with type validated indexed columns causes ClassCastException,CASSANDRA-1373,12471158,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,thobbs,thobbs,8/10/2010 1:28,3/12/2019 14:16,3/13/2019 22:24,8/17/2010 17:06,0.7 beta 2,,,,0,,,,,,"If OrderPreservingPartitioner is used and you have an indexed column with a type validator, using batch_mutate to insert column values (like pycassa does) on the same key and indexed column causes a ClassCastException to be thrown the *second* time you execute it.  That is, the first batch_mutate succeeds, but the following ones fail.  CollatedOrderPreservingPartitioner seems to avoid this problem.  Also, it appears that the row key is being compared to the column value at some point using the validator's Comparator class (such as LongType) which is where the actual exception is thrown.

Stack trace below:
{noformat}
java.lang.RuntimeException: java.lang.ClassCastException: java.lang.String cannot be cast to [B
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to [B
	at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:27)
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:82)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:37)
	at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:878)
	at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
	at org.apache.cassandra.db.Memtable.resolve(Memtable.java:127)
	at org.apache.cassandra.db.Memtable.put(Memtable.java:119)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:508)
	at org.apache.cassandra.db.Table.applyCF(Table.java:452)
	at org.apache.cassandra.db.Table.apply(Table.java:409)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
	at org.apache.cassandra.service.StorageProxy$2.runMayThrow(StorageProxy.java:276)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}",Cassandra Trunk,,,,,,,,,,,,,,,,,,,16/Aug/10 22:35;jbellis;1373.txt;https://issues.apache.org/jira/secure/attachment/12452221/1373.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,35:15.2,,,no_permission,,,,,,,,,,,,20108,,,Wed Aug 18 13:13:48 UTC 2010,,,,,,0|i0g4nr:,92185,thobbs,thobbs,,,,,,,,,16/Aug/10 22:35;jbellis;correct patch attached.,"16/Aug/10 23:01;thobbs;New stacktrace with patch 1373.txt (2010-08-16 06:35 PM) applied:

\\
{noformat}
java.lang.RuntimeException: java.lang.ClassCastException: java.lang.String cannot be cast to [B
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to [B
	at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:28)
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:82)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:37)
	at java.util.concurrent.ConcurrentSkipListMap.findPredecessor(ConcurrentSkipListMap.java:685)
	at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:864)
	at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
	at org.apache.cassandra.db.Memtable.resolve(Memtable.java:127)
	at org.apache.cassandra.db.Memtable.put(Memtable.java:119)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:508)
	at org.apache.cassandra.db.Table.applyCF(Table.java:452)
	at org.apache.cassandra.db.Table.apply(Table.java:409)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
	at org.apache.cassandra.service.StorageProxy$1.runMayThrow(StorageProxy.java:194)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}","17/Aug/10 17:03;thobbs;Patch was not exercised correctly last time.  The patch does fix the issue, now.",17/Aug/10 17:06;jbellis;committed,"18/Aug/10 13:13;hudson;Integrated in Cassandra #517 (See [https://hudson.apache.org/hudson/job/Cassandra/517/])
    fix updating index when value is changed.  patch by jbellis; tested by Tyler Hobbs for CASSANDRA-1373
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
commit log recovery is broken when the CL contains mutations to CFs that have been dropped,CASSANDRA-1353,12470756,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,8/3/2010 19:51,3/12/2019 14:16,3/13/2019 22:24,8/4/2010 14:37,0.7 beta 1,,,,0,,,,,,"This was working, so the fix should include a RecoveryManager test that checks for regressions.",,,,,,,,,,,,,,,,,,,,04/Aug/10 13:34;gdusbabek;0001-skip-CL-rows-that-have-unrecoverable-row-mutations.patch;https://issues.apache.org/jira/secure/attachment/12451222/0001-skip-CL-rows-that-have-unrecoverable-row-mutations.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,13:27.8,,,no_permission,,,,,,,,,,,,20099,,,Sun Aug 08 13:52:26 UTC 2010,,,,,,0|i0g4jb:,92165,mdennis,mdennis,,,,,,,,,"03/Aug/10 20:01;gdusbabek;Connected to the target VM, address: '127.0.0.1:49379', transport: 'socket'
 INFO 14:55:18,246 [main] Loading settings from /Users/gary.dusbabek/codes/apache/git-trunk/out/production/conf1/cassandra.yaml
DEBUG 14:55:18,658 [main] Syncing log with a period of 10000
 INFO 14:55:18,658 [main] DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
DEBUG 14:55:18,683 [main] setting auto_bootstrap to true
DEBUG 14:55:18,913 [main] Starting CFS Statistics
DEBUG 14:55:18,939 [main] Starting CFS Schema
DEBUG 14:55:19,028 [main] Starting CFS Migrations
DEBUG 14:55:19,042 [main] Starting CFS LocationInfo
DEBUG 14:55:19,056 [main] Starting CFS HintsColumnFamily
 INFO 14:55:19,133 [main] Loading schema version b72d969e-9f37-11df-903b-e700f669bcfc
DEBUG 14:55:19,153 [main] collecting 0 of 2147483647: Avro/Schema:false:1368@1280864748965
DEBUG 14:55:19,154 [main] collecting 1 of 2147483647: Keyspace1:false:62@1280864748965
 WARN 14:55:19,600 [main] Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
 INFO 14:55:19,615 [main] Replaying /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280862145661.log, /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280864632989.log, /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280864746190.log, /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280864747315.log, /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280864748249.log, /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280864748880.log, /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280864749601.log, /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280864798401.log
DEBUG 14:55:19,642 [main] Replaying /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280862145661.log starting at 276
DEBUG 14:55:19,642 [main] Reading mutation at 276
DEBUG 14:55:19,788 [main] replaying mutation for system.[B@7848fbc0: {ColumnFamily(LocationInfo [B:false:1@1280862146155,])}
DEBUG 14:55:19,835 [main] Reading mutation at 424
DEBUG 14:55:19,836 [main] replaying mutation for system.[B@764d2b11: {ColumnFamily(LocationInfo [","03/Aug/10 20:39;gdusbabek;It's easy to detect when a given cfid is gone, but without the old metadata, it is difficult to know how many bytes to skip over (since clock serialization length will vary, as will the column serialization length).","03/Aug/10 20:58;gdusbabek;We'd generate even more garbage if we pre-serialized the CF and them framed it with a length field (so we could skip over the data).  

One alternative I can think of is to keep and 'attic' of dropped column family definitions in the system table so that it can be consulted in order to read old row mutations.  This is ugly though.","03/Aug/10 21:13;jbellis;we are including the length of the entire serialized record in CLS.write already.  (this is the value that recover reads in as serializedSize).  so we should just be able to skip to the next one w/o any extra changes (possibly by having RowMutation deserialize throw a ColumnFamilyNotFoundException, or something like that).","04/Aug/10 00:25;jbellis;WARN freaks people out, especially dozens of WARNs which is what's likely to happen here.

It's more work, by if you make UCFE take just a CFID as a parameter and expose it, then recover can collect a count of rows skipped per CFID and log one message (i would vote for INFO but WARN is ok) at the end of the process.",04/Aug/10 12:46;gdusbabek;Updated patch to log less.,"04/Aug/10 13:34;gdusbabek;No really, I uploaded the file this time.  This patch makes the changes Jonathan suggested.","04/Aug/10 13:54;jbellis;+1

the log message might be more helpful as something like

Skipped %d mutations from unknown (probably removed) CF id %d","08/Aug/10 13:52;hudson;Integrated in Cassandra #510 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/510/])
    skip CL rows that have unrecoverable row mutations. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1353
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Static CFMetaData objects are using the wrong constructor.,CASSANDRA-1354,12470764,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,8/3/2010 21:29,3/12/2019 14:16,3/13/2019 22:24,8/4/2010 12:39,0.7 beta 1,,,,0,,,,,,"This means they are getting assigned ids > 1000.  Static CFMs should be using the private constructor that uses a specific cfid that is <1000

I'm pretty sure they just need to be getting a readRepairChance=0.",,,,,,,,,,,,,,,,,,,,03/Aug/10 21:42;gdusbabek;0001-use-correct-constructor-when-creating-static-CFM-ins.patch;https://issues.apache.org/jira/secure/attachment/12451166/0001-use-correct-constructor-when-creating-static-CFM-ins.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:58.2,,,no_permission,,,,,,,,,,,,20100,,,Wed Aug 04 13:25:35 UTC 2010,,,,,,0|i0g4jj:,92166,stuhood,stuhood,,,,,,,,,"04/Aug/10 00:27;jbellis;+1, but for the record, what badness happens when the static CFs get part of the ""public pool"" of ids?",04/Aug/10 12:38;gdusbabek;The next time we create a static CFM (like we did with StatisticsCf) it would use an ID that is already used by a regular column family that just hasn't been loaded yet.  ,"04/Aug/10 13:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    use correct constructor when creating static CFM instances. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1354
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove automatic repair sessions,CASSANDRA-1190,12466935,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,6/14/2010 21:46,3/12/2019 14:16,3/13/2019 22:24,9/6/2010 18:42,0.7 beta 2,,,,0,,,,,,"Currently both manual and automatic repair sessions use the same timeout value: TREE_STORE_TIMEOUT. This has the very negative effect of setting a maximum time that compaction can take before a manual repair will fail.

For automatic/natural repairs (triggered by two nodes autonomously finishing major compactions around the same time), you want a relatively low TREE_STORE_TIMEOUT value, because trees generated a long time apart will cause a lot of unnecessary repair. The current value is 10 minutes, to optimize for this case.

On the other hand, for manual repairs, TREE_STORE_TIMEOUT needs to be significantly higher. For instance, if a manual repair is triggered for a source node A storing 2 TB of data, and a destination node B with an empty store, then node B needs to wait long enough for node A to finish compacting 2 TB of data, which might take > 12 hours. If a node B times out the local tree before node A sends its tree, then the repair will not occur.",,,,,,,,,,,,,,,,,,,,25/Jun/10 00:00;stuhood;0001-Remove-natural-repair-throttling-in-preparation-for-.patch;https://issues.apache.org/jira/secure/attachment/12448001/0001-Remove-natural-repair-throttling-in-preparation-for-.patch,25/Jun/10 00:00;stuhood;0002-Rename-readonly-compaction-to-validation-and-make-it.patch;https://issues.apache.org/jira/secure/attachment/12448002/0002-Rename-readonly-compaction-to-validation-and-make-it.patch,25/Jun/10 00:00;stuhood;0003-Add-session-info-to-RPCs-to-handle-concurrent-repair.patch;https://issues.apache.org/jira/secure/attachment/12448003/0003-Add-session-info-to-RPCs-to-handle-concurrent-repair.patch,25/Jun/10 19:37;stuhood;for-0.6-0001-Remove-natural-repair-throttling-in-preparation-for-.patch;https://issues.apache.org/jira/secure/attachment/12448087/for-0.6-0001-Remove-natural-repair-throttling-in-preparation-for-.patch,25/Jun/10 19:37;stuhood;for-0.6-0002-Rename-readonly-compaction-to-validation-and-make-it.patch;https://issues.apache.org/jira/secure/attachment/12448088/for-0.6-0002-Rename-readonly-compaction-to-validation-and-make-it.patch,,,,,,,,,5,,,,,,,,,,,,,,,,,,,07:48.0,,,no_permission,,,,,,,,,,,,20029,,,Sun Aug 29 19:01:18 UTC 2010,,,,,,0|i0g3jr:,92005,jbellis,jbellis,,,,,,,,,"14/Jun/10 21:52;stuhood;Differentiating manual repairs from automatic repairs will require a network protocol change, so for 0.6, I think we should just bump TREE_STORE_TIMEOUT/NATURAL_REPAIR_FREQUENCY to values like 12/24 hours.

For trunk/0.7, it would make sense to remove the timeout entirely for manual repairs, and to make the timeout for automatic repairs a calculated value based on the size of the column family.",14/Jun/10 22:07;jbellis;I think I'd prefer simply removing automatic repairs.  The probability of it happening by chance are low enough that having it happen once in a while is more confusing than useful.,"15/Jun/10 20:02;stuhood;Removing automatic repairs is inline with some other improvements I have in mind, so I'm fine with that.","25/Jun/10 00:00;stuhood;0001 and 0002 remove automatic repairs, and should be safe to apply to 0.6.

0003 adds a session id to tree requests/responses, and should only be applied to 0.7.

Should be ready for review now!","25/Jun/10 14:43;jbellis;committed 0001 to 0.6, but 0002 fails to apply","25/Jun/10 18:38;jbellis;reverted 0001, bumping to 0.6.4","25/Jun/10 19:37;stuhood;Sorry for the delay: 'for-0.6' is rebased to apply to 0.6, and the original set can be applied to trunk.",25/Jun/10 20:35;jbellis;committed,"26/Jun/10 13:07;hudson;Integrated in Cassandra #477 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/477/])
    allow multiple repair sessions per node.  patch by Stu Hood; reviewed by jbellis for CASSANDRA-1190
","28/Aug/10 06:01;stuhood;> Removing automatic repairs is inline with some other improvements I have in mind, so I'm fine with that.
The other improvements I had in mind fell through, and I haven't seen any good alternatives to major compactions for repairs. I'm wondering if disabling this was the wisest course of action.","29/Aug/10 19:01;jbellis;Since the automatic repairs couldn't be relied upon, i.e., if you wanted to guarantee repair, you needed to schedule it anyway, I don't see what has really been lost.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyOutputFormat performs blocking writes for large batches,CASSANDRA-1434,12472542,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,stuhood,stuhood,8/25/2010 23:20,3/12/2019 14:16,3/13/2019 22:24,9/27/2010 22:30,0.7 beta 2,,,,0,,,,,,"By default, ColumnFamilyOutputFormat batches {{mapreduce.output.columnfamilyoutputformat.batch.threshold}} or {{Long.MAX_VALUE}} mutations, and then performs a blocking write.",,,,,,,,,,,,,,,,,,,,08/Sep/10 00:16;stuhood;0001-Switch-away-from-Multimap-and-fix-regression-introdu.patch;https://issues.apache.org/jira/secure/attachment/12454059/0001-Switch-away-from-Multimap-and-fix-regression-introdu.patch,08/Sep/10 00:16;stuhood;0002-Improve-concurrency-and-add-basic-retries-by-attempt.patch;https://issues.apache.org/jira/secure/attachment/12454060/0002-Improve-concurrency-and-add-basic-retries-by-attempt.patch,19/Sep/10 00:42;stuhood;0003-Switch-RingCache-back-to-multimap.patch;https://issues.apache.org/jira/secure/attachment/12454961/0003-Switch-RingCache-back-to-multimap.patch,19/Sep/10 00:42;stuhood;0004-Replace-Executor-with-map-of-threads.patch;https://issues.apache.org/jira/secure/attachment/12454962/0004-Replace-Executor-with-map-of-threads.patch,19/Sep/10 15:44;jbellis;1434-v3.txt;https://issues.apache.org/jira/secure/attachment/12454975/1434-v3.txt,22/Sep/10 15:00;jbellis;1434-v4.txt;https://issues.apache.org/jira/secure/attachment/12455273/1434-v4.txt,22/Sep/10 20:51;jbellis;1434-v5.txt;https://issues.apache.org/jira/secure/attachment/12455309/1434-v5.txt,27/Sep/10 20:43;jbellis;1434-v6.txt;https://issues.apache.org/jira/secure/attachment/12455749/1434-v6.txt,27/Sep/10 21:18;jbellis;1434-v7.txt;https://issues.apache.org/jira/secure/attachment/12455757/1434-v7.txt,,,,,9,,,,,,,,,,,,,,,,,,,31:15.1,,,no_permission,,,,,,,,,,,,20136,,,Mon Sep 27 22:30:29 UTC 2010,,,,,,0|i0g51b:,92246,stuhood,stuhood,,,,,,,,,"30/Aug/10 07:31;mrflip;The blocking behavior is causing 'broken pipe' errors (even with relatively small batch sizes) when cassandra latency is high. (This is afaict not network latency but response latency due to a compaction or flush, etc.)

It also makes the whole cluster resonate: one slow node blocks many writers, which then all unblock at the same time, write bursts of enough size to cause a compaction or GC, etc simultaneously on every node. This means adding more writers doesn't work around the blocking write","03/Sep/10 06:19;stuhood;0001 and 0003 are minor fixes, but 0002:
* Avoids blocking processing for writes (but only 2 * batchSize mutations may be in memory at a time, so we may still block)
* Changes the default batchSize to 2^14
* Rotates through possible endpoints for a range per flush, which should more evenly distribute client connections when there are small numbers of keys in play

One issue we haven't tackled yet is how to handle failures: I've reopened CASSANDRA-1264 to handle that.",03/Sep/10 06:24;stuhood;Doh... this applies atop CASSANDRA-1368.,"03/Sep/10 17:43;stuhood;0004 collects all replicas for each range in RingCache, which I broke in 1322 (previously, we were completely rebuilding the tokenmap using a replication strategy, which would have recreated the lost information).",06/Sep/10 16:13;jbellis;why the double-flushing in close()?  can you add a comment for that?,06/Sep/10 16:18;jbellis;committed 01.  declining to apply 03; in general refactoring out a method that is called in a single place obscures control flow rather than clarifies it.  04 looks ok but i'm not sure to what degree it depends on 02 (see above) so leaving alone for now.,"06/Sep/10 20:10;mrflip;Right now the code does { buffer n mutations, holding each  acc. to its endpoint. After n writes, check that all endpoint writes are finished, and dispatch to each endpoint its share of the n mutations }

This is non-blocking at the socket level but ends up being blocking at the app level, and the wide variance in size has bad effects on gc at the cassandra end.

I think the ColumnFamilyRecordWriter would see a speedup & improved stability with  { buffer mutations, holding each acc. to its endpoint. When an endpoint has seen n writes, check that any previous write has finished, and dispatch to this endpoint a full buffer of N mutations }.","08/Sep/10 00:16;stuhood;0001 is changes to the RingCache that survived from v1: it fixes the bug in ringcache that was handled by pre-0004, and removes the multimap.

0002 is a completely revamped ColumnFamilyRecordWriter: nothing from the original patch survived.
* Launches a client thread per unique range, which is responsible for communicating with endpoint replicas for that range.
** The client threads receives mutations for the range from the parent thread on a bounded queue.
** Client threads will attempt to send a full batch of mutations to its replicas in order: this means that each batch gets up to RF retries before failing, but without any failures, connections will always be made to the first replica.
* The parent thread loops trying to offer to queues for client threads, and checks that they are still alive (and fails if they aren't).
* For a N node cluster, up to (2 * N * batchSize) mutations will be in memory at once, so the default batchSize was lowered to 4096.

Fairly well tested against a 12 node cluster: no obvious races or bottlenecks.","08/Sep/10 15:16;jbellis;why is switching from Multimap<Range, InetAddress> to Map<Range, List<InetAddress>> an improvement?","16/Sep/10 14:50;stuhood;Sent via e-mail while I was on vacation:
{quote}
I wanted to dodge object creation, but I guess I assumed that Multimap created Set and Collection facades for every call. Also, there didn't appear to be a way to iterate over unique keys without a facade.
{quote}","18/Sep/10 13:16;jbellis;Had a look at 02. Still don't understand your objection to using Multimap -- use ListMultimap if you want to preserve ordering.  It's noticeably cleaner than Map<X, List<Y>>, and the Guava guys are very careful about performance.

Also, 02 kind of abuses an executor when a map of threads would be clearer as to what is going on, while not requiring much more code.  ""Send this message"" is a good task to submit to an executor; ""run an infinite loop pulling messages off a public queue"" is not.",19/Sep/10 00:42;stuhood;Adding 0003 and 0004 with the requested changes: exception handling was also improved a bit.,"19/Sep/10 15:44;jbellis;I squashed and added code to keep CFRW from slamming Cassandra with spikes of load: it keeps a pooled connection, and sends mutations one at a time over that.  This is only a trivial amount of overhead compared to using a large batch, since we're not reconnecting for each message.  (The main advantage of using a larger batch is that it gives you an idempotent group of work to replay if necessary, which doesn't matter here.  Under the hood it takes the same code path.)

Also attempted to distinguish between recoverable errors and non- in the exception handling.","19/Sep/10 17:45;stuhood;* ArrayBlockingQueue.isEmpty will kill client threads if their queue is ever empty
* Interrupt handling doesn't seem like a clearer solution for killing client threads: what happens when an interrupt in received during a mutation?
* I don't like the idea of indefinite retries: pretending that the cluster is never unavailable sidesteps Hadoop's own retry system
* As mentioned in IRC, batchSize == 1 does not seem like a good value to hardcode. Any amount of overhead becomes measurable when you are sending small enough values: mutations containing a single integer might increase in size X fold for instance","19/Sep/10 18:17;jbellis;bq. ArrayBlockingQueue.isEmpty will kill client threads if their queue is ever empty

it's while (run || isEmpty).  am i missing something?

bq. what happens when an interrupt in received during a mutation

nothing. InterruptedException is only thrown at well-defined points (one of the few times checked exceptions have done me a favor), and blocking socket send is not one of them.  the JDK uses this pattern to shut down threadpoolexecutors.

bq. I don't like the idea of indefinite retries

the idea is it tries each endpoint, then throws if they all fail. (if !iter.hasnext() then throw)

bq. batchSize == 1 does not seem like a good value to hardcode

as described above, batching > 1 is a misfeature that has been demonstrated to cause badness in practice.","19/Sep/10 18:28;stuhood;> it's while (run || isEmpty). am i missing something?
Ah, sorry.

> the idea is it tries each endpoint, then throws if they all fail. (if !iter.hasnext() then throw)
Gotcha. I missed that part because there doesn't appear to be a way for the parent thread to figure out that a client died, so I assumed that the clients never died. Does it need an UncaughtExceptionHandler that alerts the parent thread? This was what was accomplished by using offer() rather than put() in the previous version.

> as described above, batching > 1 is a misfeature that has been demonstrated to cause badness in practice.
In Cassandra, or in general?","19/Sep/10 18:33;jbellis;> Does it need an UncaughtExceptionHandler that alerts the parent thread?

Probably.  What should the parent thread do?

> In Cassandra, or in general?

In Cassandra.  Flip spent several days in the user IRC channel trying to deal with the load spikes.","19/Sep/10 18:44;stuhood;> Probably. What should the parent thread do?
Probably what the previous version did.

> In Cassandra. Flip spent several days in the user IRC channel trying to deal with the load spikes.
Does changing this patch solve his problem, or are we assuming that?",22/Sep/10 15:00;jbellis;v4 attached to throw IOException on put or stopNicely if the thread has errored out,"22/Sep/10 16:35;stuhood;* There is a race condition in put() between {{!run}} and the put itself
* Exceptions thrown by child threads will be logged, but not reported to the Hadoop frontend, since they aren't what kill the parent thread

I'm -0 on v3 and v4: but I'll add a 0005 to separate queue size from batch size, so that we can tune down the batch size for Flip.",22/Sep/10 16:46;jbellis;i'm -1 on batching at all.,"22/Sep/10 20:51;jbellis;v5 uses a small batch size and eagerly sends out ""incomplete"" batches if the reducer falls behind","23/Sep/10 01:22;stuhood;* ColumnFamilyOutputFormat.createAuthenticatedClient calls socket.open, so the second open in RangeClient is getting {{TTransportException: Socket already connected}}
* Logging a NPE for the first batch is pretty ugly
* The default batchSize was increased back up to Long.MAX_VALUE: it should probably be significantly lower (32~128) for the reasons you've mentioned","27/Sep/10 20:43;jbellis;v6.

bq. There is a race condition in put() between !run and the put itself

not really.  the check in put is just an attempt to abort earlier if possible.

bq. Exceptions thrown by child threads will be logged, but not reported to the Hadoop frontend

saved actual exceptions.

bq. the second open in RangeClient is getting TTransportException: Socket already connected

fixed

bq. Logging a NPE for the first batch is pretty ugly

nothing is logged.  the alternatives strike me as uglier.

bq. The default batchSize was increased back up to Long.MAX_VALUE

fixed","27/Sep/10 21:10;stuhood;This patch includes a change CompactionManager.java.

>> There is a race condition in put() between !run and the put itself
> not really. the check in put is just an attempt to abort earlier if possible.
put() is called from the parent thread: it isn't interrupted by the child thread, so it will block indefinitely if an exception occurs between lastException != null and the blocking put(). Unlikely, but...

----

Other than those two nitpicks, +1: tested against a 12 node cluster and saw smooth network utilization.","27/Sep/10 21:18;jbellis;v7

bq. This patch includes a change CompactionManager.java

fixed

bq.  it will block indefinitely if an exception occurs between lastException != null and the blocking put()

you're right.  fixed",27/Sep/10 22:00;stuhood;+1,27/Sep/10 22:30;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Memtable flush causes bad ""reversed"" get_slice",CASSANDRA-1450,12472952,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,thobbs,thobbs,8/31/2010 17:14,3/12/2019 14:16,3/13/2019 22:24,9/5/2010 3:31,0.7 beta 2,,,,2,,,,,,"If columns are inserted into a row before and after a memtable flush, a get_slice() after the flush with reversed=True will return incorrect results.  See attached patch to reproduce.",Cassandra Trunk,,,,,,,,,,,,,,,,,,,03/Sep/10 14:52;jbellis;1450.txt;https://issues.apache.org/jira/secure/attachment/12453781/1450.txt,31/Aug/10 17:14;thobbs;reversed_slice_reproduce.txt;https://issues.apache.org/jira/secure/attachment/12453524/reversed_slice_reproduce.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,52:57.7,,,no_permission,,,,,,,,,,,,20146,,,Sun Sep 05 03:31:40 UTC 2010,,,,,,0|i0g54v:,92262,brandon.williams,brandon.williams,,,,,,,,,03/Sep/10 14:52;jbellis;patch applies reversed flag during collation from different data sources and makes QF.getColumnComparator package-private to make it more difficult to make the same mistake again.  unit test added to TableTest.,"03/Sep/10 16:05;brandon.williams;+1, though maybe the flush in the test should be explicit since it's not immediately obvious it will occur after 20 ops.","05/Sep/10 03:31;jbellis;committed, w/ test adjusted as suggested",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"replication factor exceeds number of endpoints, when attempting to join a new node (but cluster has enough running nodes to fulfill RF)",CASSANDRA-1467,12473291,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,blanquer,blanquer,9/3/2010 23:24,3/12/2019 14:16,3/13/2019 22:24,9/13/2010 16:11,0.6.6,0.7 beta 2,,,0,,,,,,"What happens here the following:
* given a healthy running cluster of 2 nodes (it used to be 3 but I manually killed one down)
* with a Keyspace having a ReplicationFactor of 2
* (this cluster is operational, loaded with data and working well)

 as soon as I want to bring up a new 3rd node:
* the node is detected by the current cluster
* but as soon as it tries to initiate bootstrap sequence it dies with:  replication factor (2) exceeds number of endpoints (1)

I believe this is similar to #1343, but not quite the same, since the keyspace and everything is already created, and I'm not attempting any modification. This is purely bringing a new node up.

One extra tidbit of information, in case it's important...maybe it is not: 
I have only set 1 seed node configured (this is a test setup). And when the new node starts coming up (before the crash) its nodetool ring only reports the seed one.

From the new node coming up (before it crashes):
root@node1-3:~# nodetool -h localhost -p 8080 ring
Address         Status State   Load            Token                                       
10.250.106.111  Up     Normal  116.3 GB        0          < = this the only configured seed node

From one of the other running nodes (this is the real status of the cluster):
root@node1-2:~# nodetool -h localhost -p 8080 ring
Address         Status State   Load            Token                                       
                                       113416112894748789872342756657008344878    
10.250.106.111  Up     Normal  116.3 GB        0                                           
10.215.195.81   Up     Normal  116.31 GB       56713727820156410577229101238628035242      
10.246.65.221   Up     Joining 7.63 KB         113416112894748789872342756657008344878     


Here's the full boot trace of the node is trying to join:
Java HotSpot(TM) Client VM warning: Can't detect initial thread stack location - find_vma failed
Create RMI registry on port 8081
Get the platform's MBean server
Initialize the environment map
Create an RMI connector server
Start the RMI connector server on port 8081
 INFO 22:32:50,448 Loading settings from /etc/cassandra/cassandra.yaml
DEBUG 22:32:50,537 Syncing log with a period of 10000
 INFO 22:32:50,538 DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
DEBUG 22:32:50,548 setting auto_bootstrap to true
DEBUG 22:32:50,702 Starting CFS Statistics
DEBUG 22:32:50,710 key cache capacity for Statistics is 1
DEBUG 22:32:50,711 Starting CFS Schema
DEBUG 22:32:50,712 key cache capacity for Schema is 1
DEBUG 22:32:50,712 Starting CFS Migrations
DEBUG 22:32:50,713 key cache capacity for Migrations is 1
DEBUG 22:32:50,713 Starting CFS LocationInfo
DEBUG 22:32:50,713 key cache capacity for LocationInfo is 1
DEBUG 22:32:50,714 Starting CFS HintsColumnFamily
DEBUG 22:32:50,714 key cache capacity for HintsColumnFamily is 1
 INFO 22:32:50,736 Couldn't detect any schema definitions in local storage.
 INFO 22:32:50,737 Found table data in data directories. Consider using JMX to call org.apache.cassandra.service.StorageService.loadSchemaFromYaml().
DEBUG 22:32:50,738 opening keyspace system
 INFO 22:32:50,757 Cassandra version: 
 INFO 22:32:50,757 Thrift API version: 10.0.0
 INFO 22:32:50,758 Saved Token not found. Using 113416112894748789872342756657008344878
 INFO 22:32:50,758 Saved ClusterName not found. Using SOMETHING 
 INFO 22:32:50,763 Creating new commitlog segment /mnt/cassandra/commitlog/CommitLog-1283553170763.log
DEBUG 22:32:50,767 Estimating compactions for LocationInfo
DEBUG 22:32:50,768 Estimating compactions for HintsColumnFamily
DEBUG 22:32:50,768 Estimating compactions for Migrations
DEBUG 22:32:50,768 Estimating compactions for Schema
DEBUG 22:32:50,768 Estimating compactions for Statistics
DEBUG 22:32:50,769 Checking to see if compaction of LocationInfo would be useful
DEBUG 22:32:50,769 Checking to see if compaction of HintsColumnFamily would be useful
DEBUG 22:32:50,769 Checking to see if compaction of Migrations would be useful
DEBUG 22:32:50,769 Checking to see if compaction of Schema would be useful
DEBUG 22:32:50,769 Checking to see if compaction of Statistics would be useful
 INFO 22:32:50,779 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=276)
 INFO 22:32:50,782 Enqueuing flush of Memtable-LocationInfo@18721294(192 bytes, 4 operations)
 INFO 22:32:50,783 Writing Memtable-LocationInfo@18721294(192 bytes, 4 operations)
 INFO 22:32:50,897 Completed flushing /mnt/ebs/data/system/LocationInfo-e-1-Data.db
DEBUG 22:32:50,898 Checking to see if compaction of LocationInfo would be useful
DEBUG 22:32:50,898 Discarding 0
DEBUG 22:32:50,899 discard completed log segments for CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=276), column family 0.
DEBUG 22:32:50,899 Marking replay position 276 on commit log CommitLogSegment(/mnt/cassandra/commitlog/CommitLog-1283553170763.log)
 INFO 22:32:50,908 Starting up server gossip
 INFO 22:32:50,928 Joining: getting load information
 INFO 22:32:50,928 Sleeping 90000 ms to wait for load information...
DEBUG 22:32:50,940 attempting to connect to node1-1.domain.com/10.250.106.111
DEBUG 22:32:51,044 attempting to connect to node1-1.domain.com/10.250.106.111
DEBUG 22:32:51,045 attempting to connect to /10.215.195.81
 INFO 22:32:51,051 Node /10.215.195.81 is now part of the cluster
DEBUG 22:32:51,051 Resetting pool for /10.215.195.81
DEBUG 22:32:51,052 Token 113416112894748789872342756657008344877 removed manually (endpoint was unknown)
 INFO 22:32:51,052 Node /10.250.106.111 is now part of the cluster
DEBUG 22:32:51,053 Resetting pool for /10.250.106.111
DEBUG 22:32:51,053 Node /10.250.106.111 state normal, token 0
DEBUG 22:32:51,053 clearing cached endpoints
DEBUG 22:32:51,171 Applying AddKeyspace from /10.250.106.111
DEBUG 22:32:51,188 Applying migration 77e97d6c-b625-11df-8596-318df8b646e8
 INFO 22:32:51,189 switching in a fresh Memtable for Migrations at CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=7417)
 INFO 22:32:51,189 Enqueuing flush of Memtable-Migrations@18093512(4938 bytes, 1 operations)
 INFO 22:32:51,189 Writing Memtable-Migrations@18093512(4938 bytes, 1 operations)
 INFO 22:32:51,194 switching in a fresh Memtable for Schema at CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=7417)
 INFO 22:32:51,194 Enqueuing flush of Memtable-Schema@27402470(1784 bytes, 3 operations)
 INFO 22:32:51,274 Completed flushing /mnt/ebs/data/system/Migrations-e-1-Data.db
DEBUG 22:32:51,274 Checking to see if compaction of Migrations would be useful
DEBUG 22:32:51,275 Discarding 2
DEBUG 22:32:51,275 discard completed log segments for CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=7417), column family 2
.
DEBUG 22:32:51,275 Marking replay position 7417 on commit log CommitLogSegment(/mnt/cassandra/commitlog/CommitLog-1283553170763.log)
 INFO 22:32:51,275 Writing Memtable-Schema@27402470(1784 bytes, 3 operations)
 INFO 22:32:51,388 Completed flushing /mnt/ebs/data/system/Schema-e-1-Data.db
DEBUG 22:32:51,389 Checking to see if compaction of Schema would be useful
DEBUG 22:32:51,389 Discarding 3
DEBUG 22:32:51,389 discard completed log segments for CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=7417), column family 3
.
DEBUG 22:32:51,389 Marking replay position 7417 on commit log CommitLogSegment(/mnt/cassandra/commitlog/CommitLog-1283553170763.log)
DEBUG 22:32:51,406 Starting CFS MyColumnFamily
DEBUG 22:32:51,406 key cache capacity for MyColumnFamily is 200000
 INFO 22:32:51,407 Creating new commitlog segment /mnt/cassandra/commitlog/CommitLog-1283553171407.log
DEBUG 22:32:51,604 attempting to connect to node1-1.domain.com/10.250.106.111
 INFO 22:32:51,622 InetAddress /10.215.195.81 is now UP
 INFO 22:32:51,623 InetAddress /10.250.106.111 is now UP
 INFO 22:32:51,623 Started hinted handoff for endpoint /10.215.195.81
 INFO 22:32:51,631 Finished hinted handoff of 0 rows to endpoint /10.215.195.81
 INFO 22:32:51,632 Started hinted handoff for endpoint /10.250.106.111
 INFO 22:32:51,632 Finished hinted handoff of 0 rows to endpoint /10.250.106.111
DEBUG 22:32:51,918 GC for ParNew: 14 ms, 20561696 reclaimed leaving 149257720 used; max is 902627328
DEBUG 22:32:51,919 GC for ConcurrentMarkSweep: 73 ms, 4178480 reclaimed leaving 8520856 used; max is 902627328
DEBUG 22:32:52,924 Disseminating load info ...
DEBUG 22:32:53,929 attempting to connect to /10.215.195.81
DEBUG 22:32:54,925 GC for ConcurrentMarkSweep: 73 ms, 212034048 reclaimed leaving 15214608 used; max is 902627328
DEBUG 22:33:52,931 Disseminating load info ...
DEBUG 22:34:20,929 ... got load info
 INFO 22:34:20,929 Joining: getting bootstrap token
DEBUG 22:34:20,931 token manually specified as 113416112894748789872342756657008344878
 INFO 22:34:20,932 Joining: sleeping 30000 ms for pending range setup
 INFO 22:34:50,935 Bootstrapping
DEBUG 22:34:50,935 Beginning bootstrap process
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1)
        at org.apache.cassandra.locator.RackUnawareStrategy.calculateNaturalEndpoints(RackUnawareStrategy.java:57)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getRangeAddresses(AbstractReplicationStrategy.java:195)
        at org.apache.cassandra.dht.BootStrapper.getRangesWithSources(BootStrapper.java:155)
        at org.apache.cassandra.dht.BootStrapper.startBootstrap(BootStrapper.java:73)
        at org.apache.cassandra.service.StorageService.startBootstrap(StorageService.java:467)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:408)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:134)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:57)
        ... 5 more
Cannot load daemon
Service exit with a return value of 3
",I'm using 0.7 beta1. Built from: http://www.apache.org/dyn/closer.cgi?path=/cassandra/0.7.0/apache-cassandra-0.7.0-beta1-src.tar.gz  using dpkg-buildpackage from it.,,,,,,,,,,,,,,,,,,,09/Sep/10 21:09;gdusbabek;1467-0.6.txt;https://issues.apache.org/jira/secure/attachment/12454241/1467-0.6.txt,09/Sep/10 21:07;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-expose-endpoint-states-to-jmx.txt;https://issues.apache.org/jira/secure/attachment/12454238/ASF.LICENSE.NOT.GRANTED--v1-0001-expose-endpoint-states-to-jmx.txt,09/Sep/10 21:07;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0002-remove-unused-code-from-SLB.txt;https://issues.apache.org/jira/secure/attachment/12454239/ASF.LICENSE.NOT.GRANTED--v1-0002-remove-unused-code-from-SLB.txt,09/Sep/10 21:07;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0003-broadcast-removetoken-using-MOVE-NORMAL-to-preserve-th.txt;https://issues.apache.org/jira/secure/attachment/12454240/ASF.LICENSE.NOT.GRANTED--v1-0003-broadcast-removetoken-using-MOVE-NORMAL-to-preserve-th.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,58:46.5,,,no_permission,,,,,,,,,,,,20157,,,Mon Sep 13 16:11:48 UTC 2010,,,,,,0|i0g58n:,92279,jbellis,jbellis,,,,,,,,,03/Sep/10 23:58;gdusbabek;Did the node you were trying to bootstrap already have schema defined?,"04/Sep/10 00:23;blanquer;No, it was a blank node. 
I've just wiped the data and commit log dirs clean, and restarted cassandra again with the same result.

Here's the start of the system.log:
 INFO [main] 2010-09-04 00:19:19,744 DatabaseDescriptor.java (line 120) Loading settings from /etc/cassandra/cassandra.yaml
DEBUG [main] 2010-09-04 00:19:19,923 DatabaseDescriptor.java (line 163) Syncing log with a period of 10000
 INFO [main] 2010-09-04 00:19:19,923 DatabaseDescriptor.java (line 171) DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
DEBUG [main] 2010-09-04 00:19:19,933 DatabaseDescriptor.java (line 311) setting auto_bootstrap to true
DEBUG [main] 2010-09-04 00:19:20,122 ColumnFamilyStore.java (line 149) Starting CFS Statistics
DEBUG [main] 2010-09-04 00:19:20,188 SSTableTracker.java (line 110) key cache capacity for Statistics is 1
DEBUG [main] 2010-09-04 00:19:20,189 ColumnFamilyStore.java (line 149) Starting CFS Schema
DEBUG [main] 2010-09-04 00:19:20,190 SSTableTracker.java (line 110) key cache capacity for Schema is 1
DEBUG [main] 2010-09-04 00:19:20,191 ColumnFamilyStore.java (line 149) Starting CFS Migrations
DEBUG [main] 2010-09-04 00:19:20,191 SSTableTracker.java (line 110) key cache capacity for Migrations is 1
DEBUG [main] 2010-09-04 00:19:20,191 ColumnFamilyStore.java (line 149) Starting CFS LocationInfo
DEBUG [main] 2010-09-04 00:19:20,192 SSTableTracker.java (line 110) key cache capacity for LocationInfo is 1
DEBUG [main] 2010-09-04 00:19:20,193 ColumnFamilyStore.java (line 149) Starting CFS HintsColumnFamily
DEBUG [main] 2010-09-04 00:19:20,193 SSTableTracker.java (line 110) key cache capacity for HintsColumnFamily is 1
 INFO [main] 2010-09-04 00:19:20,223 DatabaseDescriptor.java (line 442) *Couldn't detect any schema definitions in local storage.*
 INFO [main] 2010-09-04 00:19:20,224 DatabaseDescriptor.java (line 468) Found table data in data directories. Consider using JMX to call org.apache.cassandra.service.StorageService.loadSchemaFromYaml().
DEBUG [main] 2010-09-04 00:19:20,227 CassandraDaemon.java (line 115) opening keyspace system
 INFO [main] 2010-09-04 00:19:20,247 StorageService.java (line 342) Cassandra version:
 INFO [main] 2010-09-04 00:19:20,247 StorageService.java (line 343) Thrift API version: 10.0.0
 INFO [main] 2010-09-04 00:19:20,248 SystemTable.java (line 202) Saved Token not found. Using 113416112894748789872342756657008344878
 INFO [main] 2010-09-04 00:19:20,249 SystemTable.java (line 208) Saved ClusterName not found. ......
","07/Sep/10 21:30;gdusbabek;It took some trying, but I can reproduce this now.","09/Sep/10 16:21;gdusbabek;Reproduced in 0.6 (mostly).  The bootstrap still happens because we don't enforce the RF restriction that we do now.  When the fourth node is brought up and bootstrapped, it sees the the ring minus the coordinator node from the removetoken operation.

I've got a patch worked up for trunk.  I'll port it to 0.6 and attach shortly.","09/Sep/10 18:31;gdusbabek;The first two patches are housekeeping.  Exposing the endpoint states via jmx was helpful for troubleshooting.
The third patch changes gossip so that the removetoken information is broadcast as part of a MOVE,NORMAL instead of MOVE,LEFT.  This means that we no longer have to distinguish between MOVE,LEFT,leaving and MOVE,LEFT,removetoken, so I removed that distinction.  

Valid MOVE operations are now these:
MOVE,NORMAL,token[,<other command>,<other token>]
MOVE,LEFT,token
MOVE,BOOTSTRAPPING,token
MOVE,LEAVING,token

If this patch goes into 0.6 as is, it alters gossip enough so that if a removetoken, decommission or loadbalance are in progress during the rolling upgrade, bad things might happen (the wrong handlers could be called).",09/Sep/10 21:09;gdusbabek;comments MOVE state transitions.,13/Sep/10 14:25;jbellis;+1,13/Sep/10 16:11;gdusbabek;committed 0.6 and trunk (0.7),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Failed bootstrap can cause NPE in batch_mutate on every node, taking down the entire cluster",CASSANDRA-1463,12473282,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,ketralnis,ketralnis,9/3/2010 20:50,3/12/2019 14:16,3/13/2019 22:24,9/4/2010 20:34,0.6.6,0.7 beta 2,,,0,,,,,,"In adding a node to the cluster, the bootstrap failed (still investigating the cause). An hour later, the entire cluster failed, preventing any writes from being accepted. This exception started being printed to the logs:

{quote}
 INFO [Timer-0] 2010-09-03 12:23:33,282 Gossiper.java (line 402) FatClient /10.251.243.191 has been silent for 3600000ms, removing from gossip
ERROR [Timer-0] 2010-09-03 12:23:33,318 Gossiper.java (line 99) Gossip error
java.util.ConcurrentModificationException
        at java.util.Hashtable$Enumerator.next(Hashtable.java:1048)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:383)
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:93)
        at java.util.TimerThread.mainLoop(Timer.java:534)
        at java.util.TimerThread.run(Timer.java:484)
ERROR [pool-1-thread-69153] 2010-09-03 12:23:33,857 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
        at org.apache.cassandra.gms.FailureDetector.isAlive(FailureDetector.java:135)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:85)
        at org.apache.cassandra.service.StorageProxy.mutateBlocking(StorageProxy.java:204)
        at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:415)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:1651)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1166)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR [pool-1-thread-69154] 2010-09-03 12:23:33,869 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
        at org.apache.cassandra.gms.FailureDetector.isAlive(FailureDetector.java:135)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:85)
        at org.apache.cassandra.service.StorageProxy.mutateBlocking(StorageProxy.java:204)
        at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:415)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:1651)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1166)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{quote}

After a large number of iterations of that (at least thousands), the printed exception was shortened (this shortening is what made me mistakenly file #1462) to

{quote}
ERROR [pool-1-thread-68869] 2010-09-03 12:39:22,857 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68869] 2010-09-03 12:39:22,883 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68869] 2010-09-03 12:39:22,894 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68970] 2010-09-03 12:39:22,985 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [pool-1-thread-68970] 2010-09-03 12:39:23,084 Cassandra.java (line 1659) Internal error processing batch_mutate
java.lang.NullPointerException
{quote}

Rolling a restart over the cluster fixed it, but every node had to be restarted before it started accepting writes again.",,,,,,,,,,,,,,,,,,,,04/Sep/10 04:14;jbellis;1463-v2.txt;https://issues.apache.org/jira/secure/attachment/12453859/1463-v2.txt,04/Sep/10 15:02;jbellis;1463-v3.txt;https://issues.apache.org/jira/secure/attachment/12453867/1463-v3.txt,03/Sep/10 21:10;jbellis;1463.txt;https://issues.apache.org/jira/secure/attachment/12453827/1463.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,56:12.6,,,no_permission,,,,,,,,,,,,20154,,,Mon Dec 27 23:30:11 UTC 2010,,,,,,0|i0g57r:,92275,brandon.williams,brandon.williams,,,,,,,,,"03/Sep/10 20:56;brandon.williams;Fixed in 0.7 by CASSANDRA-757, but the approach we took for 0.6 was CASSANDRA-1289.  My guess is so many batch_mutate errors were being logged, logging consumed all the cpu before the gossiper timer could run again, which would have solved it.  I'm not sure how to solve this in 0.6 in a less invasive way than the 0.7 approach.","03/Sep/10 21:10;jbellis;the CME is a red herring, the real problem is the NPE (caused by the IP being cleared out of the gossip records as indicated in the log, but not out of the pending ranges)

attached patch should fix the NPE, looking at how much of a bitch it would be to fix the root cause (the PR orphan)","04/Sep/10 03:30;jbellis;v2 adds an onRemove callback to the gossiper interface, to do the remove from TokenMetadata.","04/Sep/10 03:31;jbellis;patch is against 0.6, i will handle rebase to 0.7 afterwards","04/Sep/10 04:01;brandon.williams;v2 fails to compile:     [javac] /srv/cassandra/src/java/org/apache/cassandra/service/StorageLoadBalancer.java:49: org.apache.cassandra.service.StorageLoadBalancer is not abstract and does not override abstract method onRemove(java.net.InetAddress) in org.apache.cassandra.gms.IEndPointStateChangeSubscriber
","04/Sep/10 04:14;jbellis;corrected v2.  (is it just me or is ""ant"" w/o ""clean"" getting worse at dependency discovery as we add more classes?)","04/Sep/10 05:32;brandon.williams;v2 produces this:


 INFO 05:26:36,245 FatClient /10.179.65.102 has been silent for 3600000ms, removing from gossip
ERROR 05:26:36,247 Uncaught exception in thread Thread[Timer-0,5,main]
java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.removeEndpoint(TokenMetadata.java:192)
        at org.apache.cassandra.service.StorageService.onRemove(StorageService.java:879)
        at org.apache.cassandra.gms.Gossiper.removeEndPoint(Gossiper.java:221)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:407)
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:93)
        at java.util.TimerThread.mainLoop(Timer.java:534)
        at java.util.TimerThread.run(Timer.java:484)

But that is all I see, with constant insert pressure.",04/Sep/10 15:02;jbellis;v3 removes obsolete assertion and adds call to calculatePendingRanges in onRemove.,04/Sep/10 18:28;brandon.williams;Looks good now. +1,04/Sep/10 20:34;jbellis;committed,"27/Dec/10 23:30;hudson;Integrated in Cassandra-0.7 #121 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/121/])
    Java-based stress util.  Patch by Pavel Yaskevich, reviewed by
brandonwilliams for CASSANDRA-1463
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogHeader raises an AssertionError during  startup,CASSANDRA-1435,12472560,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,amorton,amorton,8/26/2010 6:52,3/12/2019 14:16,3/13/2019 22:24,8/31/2010 14:58,0.7 beta 2,,,,0,,,,,,"On a cluster that was pretty sick due to CASSANDRA-1416 and CASSANDRA-1432 I got the error below when starting up a node. The node failed to start.

After retrying the node started. 
ERROR [main] 2010-08-26 14:59:22,315 AbstractCassandraDaemon.java (line 107) Exception encountered during startup.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:549)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:339)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:174)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:545)
        ... 5 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:408)
        at org.apache.cassandra.db.ColumnFamilyStore$2.runMayThrow(ColumnFamilyStore.java:445)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:400)
        ... 8 more
Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.commitlog.CommitLogHeader$CommitLogHeaderSerializer.serialize(CommitLogHeader.java:157)
        at org.apache.cassandra.db.commitlog.CommitLogHeader.writeCommitLogHeader(CommitLogHeader.java:124)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.writeHeader(CommitLogSegment.java:70)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:450)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:75)
        at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:394)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:52)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
",,,,,,,,,,,,,,CASSANDRA-1376,,,,,,30/Aug/10 22:30;jbellis;ASF.LICENSE.NOT.GRANTED--0001-avoid-attempting-to-keep-CL-header-constant-size-schem.txt;https://issues.apache.org/jira/secure/attachment/12453471/ASF.LICENSE.NOT.GRANTED--0001-avoid-attempting-to-keep-CL-header-constant-size-schem.txt,30/Aug/10 22:30;jbellis;ASF.LICENSE.NOT.GRANTED--0002-r-m-forceNewSegment.txt;https://issues.apache.org/jira/secure/attachment/12453472/ASF.LICENSE.NOT.GRANTED--0002-r-m-forceNewSegment.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,08:39.8,,,no_permission,,,,,,,,,,,,20137,,,Sun Sep 12 19:39:05 UTC 2010,,,,,,0|i0g51j:,92247,gdusbabek,gdusbabek,,,,,,,,,"26/Aug/10 22:08;jbellis;a write during a schema change could cause adding an entry to the CLH dirty map, which the assert was dutifully noticing.

attached solution is to simply allow the map to grow; we can do this because we previously moved the header to a separate file, so restricting it to the same amount of bytes is no longer important.

does this mean we can drop all the forcenewsegment calls in the migration code?","27/Aug/10 13:31;gdusbabek;>does this mean we can drop all the forcenewsegment calls in the migration code?
If the CLH is no longer bounded, yes.","27/Aug/10 13:59;jbellis;-2 removes forcenewsegment.  (shouldn't be necessary for drain, either)",31/Aug/10 13:58;gdusbabek;+1,31/Aug/10 14:55;jbellis;committed,"12/Sep/10 19:39;hudson;Integrated in Cassandra #533 (See [https://hudson.apache.org/hudson/job/Cassandra/533/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Upgrade from 0.6.3 to 0.6.5, exception when replay commitlog",CASSANDRA-1492,12473815,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,apache.zli,apache.zli,9/10/2010 15:28,3/12/2019 14:16,3/13/2019 22:24,9/13/2010 13:30,0.6.6,,,,0,,,,,,"When start cassandra 0.6.5 with commitlog of 0.6.3, got exception when replay the commitlog. 

ERROR 15:14:16,174 Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: invalid UTF8 bytes [-84, 16, 10, -105]
	at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:43)
	at org.apache.cassandra.db.Column.getString(Column.java:215)
	at org.apache.cassandra.db.marshal.AbstractType.getColumnsString(AbstractType.java:85)
	at org.apache.cassandra.db.ColumnFamily.toString(ColumnFamily.java:334)
	at org.apache.commons.lang.ObjectUtils.toString(ObjectUtils.java:241)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3073)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3133)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:241)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:173)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:114)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:214)
","Linux, jdk 1.6",,,,,,,,,,,,,,,,,,,11/Sep/10 04:04;jbellis;1492.txt;https://issues.apache.org/jira/secure/attachment/12454356/1492.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,49:13.2,,,no_permission,,,,,,,,,,,,20164,,,Mon Sep 13 13:30:27 UTC 2010,,,,,,0|i0g5e7:,92304,gdusbabek,gdusbabek,,,,,,,,,10/Sep/10 15:49;gdusbabek;Commit logs are typically not compatible across minor versions.  You should drain your node before upgrading and restarting it.,"10/Sep/10 16:38;apache.zli;Tested again. Drained 0.6.3 commitlog with Cassandra 0.6.3, then start Cassandra 0.6.5 and works. 
Then stop Cassandra 0.6.5 and start Cassandra 0.6.5 again. Same exception. 

I shouldn't remove all data file and recreate a new node with minor version upgrade. Right?

","10/Sep/10 19:27;jbellis;commitlog is supposed to be compatible w/in minor versions.  this looks like one of those ""something is declared UTF8Type, that should be bytestype"" things.  STATUS_CF is UTF8Type in current 0.6, is that a bug?",10/Sep/10 20:52;gdusbabek;Probably. It's a BytesType in trunk.,"11/Sep/10 04:04;jbellis;patch to switch to bytestype.

agreed that if you have this in your commitlog already, deleting the commitlog file is the best fix.",13/Sep/10 12:46;gdusbabek;+1,"13/Sep/10 13:30;jbellis;committed.

(blowing away the CL isn't a permanent fix if you have a node in your cluster w/ a non-UTF8 IP.  you'll probably need to apply this patch.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in MessagingService.receive for READ_REPAIR verb,CASSANDRA-1493,12473820,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,johanoskarsson,johanoskarsson,johanoskarsson,9/10/2010 16:11,3/12/2019 14:16,3/13/2019 22:24,9/12/2010 13:58,0.7 beta 2,,,,0,,,,,,"Read repair messages are causing an assertion error in MessagingService. Looks like the enum introduced in CASSANDRA-1465 is missing a verb?

Added two lines of debug output, so lines are a bit off:
DEBUG [pool-1-thread-1] 2010-09-10 15:39:23,555 MessagingService.java (line 373) Verb: READ_REPAIR
DEBUG [pool-1-thread-1] 2010-09-10 15:39:23,555 MessagingService.java (line 374) MessageType: null
ERROR [pool-1-thread-1] 2010-09-10 15:39:23,555 Cassandra.java (line 1744) Internal error processing get
java.lang.AssertionError
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:376)
        at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:285)
        at org.apache.cassandra.service.ReadResponseResolver.maybeScheduleRepairs(ReadResponseResolver.java:163)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:116)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:43)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:89)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:430)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:266)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:113)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:317)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:1734)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1634)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,,,,,10/Sep/10 21:53;johanoskarsson;CASSANDRA-1493.patch;https://issues.apache.org/jira/secure/attachment/12454338/CASSANDRA-1493.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,48:35.0,,,no_permission,,,,,,,,,,,,20165,,,Sun Sep 12 13:58:16 UTC 2010,,,,,,0|i0g5ef:,92305,jbellis,jbellis,,,,,,,,,10/Sep/10 21:53;johanoskarsson;Adding the read repair verb seems to have fixed it.,10/Sep/10 23:48;jbellis;+1,12/Sep/10 13:58;johanoskarsson;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFMetaData.convertToThrift makes subcomparator_type empty string instead of null,CASSANDRA-1480,12473524,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jhermes,jeromatron,jeromatron,9/8/2010 3:27,3/12/2019 14:16,3/13/2019 22:24,9/13/2010 19:22,0.7 beta 2,,Legacy/CQL,,0,,,,,,"As a result of CASSANDRA-891 adding a CFMetaData.convertToThrift method, the values such as subcomparator_type are defaulted to empty string instead of null.  That makes it so, for example, in ColumnFamilyRecordReader, in its RowIterator, the check for only null is insufficient.  It also needs to check for a blank value.

After a discussion about it in IRC, Jonathan said it was probably easier to just change the creation to give a null value instead of empty string.",,,,,,,,,,,,,,,,CASSANDRA-891,CASSANDRA-1425,,,13/Sep/10 18:56;jhermes;1480.txt;https://issues.apache.org/jira/secure/attachment/12454470/1480.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,19:16.5,,,no_permission,,,,,,,,,,,,20160,,,Mon Sep 13 19:22:51 UTC 2010,,,,,,0|i0g5bj:,92292,jeromatron,jeromatron,,,,,,,,,"13/Sep/10 16:19;jhermes;(This code is only touched in describe keyspace(s) in the API.)
Attaching a debugger shows the subcomparator_type of the result of the call to be null (as expected) until it gets sent, at which point the result is then converted to the empty string.
I'm not sure when this behaviour changed, but it appears to be systemic and not a simple error.

It would be less time to change the code to accept the empty string (which may come up naturally anyway) than it would be to continue debugging thrift.","13/Sep/10 16:24;jeromatron;I kind of wondered if it would be complicated like that based on how thrift handled it.  It's a one line change to fix it in ColumnFamilyRecordReader.  I didn't know if it would affect anything else similarly.  Hopefully system/unit tests catch a lot of that.  We could do some simple searches in the code for null checks I suppose - and in those cases, if they're strings, do a o.a.commons.lang.StringUtils.isNotBlank instead.  Not sure if it's worth it though since that could have other side effects.","13/Sep/10 16:41;jhermes;To pose a related question: assume the user sets the subcomparator_type (or the comparator or the validator or the default_validator) to the empty string. All perfectly legal, as the empty string is a valid string.
What should they expect to see? Right now it's a stack trace.","13/Sep/10 17:41;jbellis;This is not a bug; subcomparator_type has a default of """", so if you leave it null (i.e., unspecified), it gets changed to its default.

Removing the default should restore the old behavior.

Defaults on reconciler and comment should also be removed.","13/Sep/10 17:52;jhermes;Massive palm to the face.
Testing now.","13/Sep/10 18:56;jhermes;convertToCFMetaData had to be changed as well, it was expecting subcomparator to default to empty string. I don't think this was a recent change; instead it looks more like we changed our assumption about the default.

In any event, subcomp, reconciler, and comment all default to null now and all tests/methods are working correctly.",13/Sep/10 19:14;jeromatron;Ran the word count again with the 1480.txt patch.  It no longer gets the exception and completes properly now.,13/Sep/10 19:22;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in calculating QUORUM,CASSANDRA-1487,12473700,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jigneshdhruv,jigneshdhruv,jigneshdhruv,9/9/2010 16:34,3/12/2019 14:16,3/13/2019 22:24,9/11/2010 1:00,0.7 beta 2,,,,0,,,,,,"Hello,

It seems that there is a bug in calculating QUORUM in src/java/org/apache/cassandra/service/QuorumResponseHandler.java

Currently the QUORUM formula in place will return correct QUORUM if replication factor <= 3. However if you have a Replication Factor > 3, it will return incorrect result.

-----------------------
--- src/java/org/apache/cassandra/service/QuorumResponseHandler.java    (revision 995482)
+++ src/java/org/apache/cassandra/service/QuorumResponseHandler.java    (working copy)
@@ -109,7 +109,7 @@
             case ANY:
                 return 1;
             case QUORUM:
-                return (DatabaseDescriptor.getQuorum(table)/ 2) + 1;
+                return DatabaseDescriptor.getQuorum(table);
             case ALL:
                 return DatabaseDescriptor.getReplicationFactor(table);
             default:
-------------------
In QuorumResponseHandler:determineBlockFor()
DatabaseDescriptor.getQuorum(table) is already returning a quorum value which is further divided by 2 and a one is added.

So say if your RF=6, it is suppose to check 4 replicas, (6/2)+1=4 but it ends up checking only 3 replicas as DatabaseDescriptor.getQuorum returns 4, so determineBlockFor will return (4/2)+1=3.

Let me know if you have any questions.

Jignesh",,,,,,,,,,,,,,,,,,,,09/Sep/10 20:20;jigneshdhruv;QuorumResponseHandler.java.patch;https://issues.apache.org/jira/secure/attachment/12454235/QuorumResponseHandler.java.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,34:31.8,,,no_permission,,,,,,,,,,,,20163,,,Sat Sep 11 01:00:11 UTC 2010,,,,,,0|i0g5d3:,92299,jbellis,jbellis,,,,,,,,,09/Sep/10 19:34;kingryan;You should attach patches rather than pasting them.,09/Sep/10 20:20;jigneshdhruv;QuoromResponseHandler Patch.,09/Sep/10 20:21;jigneshdhruv;Attached patch for QuorumResponseHandler.java.,"11/Sep/10 01:00;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The dynamic snitch can't be used with network topology strategies,CASSANDRA-1429,12472473,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,slebresne,slebresne,8/25/2010 13:27,3/12/2019 14:16,3/13/2019 22:24,8/25/2010 16:00,0.6.6,0.7 beta 2,,,0,,,,,,also ported to 0.6 and committed there,,,,,,,,,,,,,,,,,,,,25/Aug/10 13:27;slebresne;0001-Make-DynamicEndpointSnitch-a-subclass-of-AbstractNet.patch;https://issues.apache.org/jira/secure/attachment/12453033/0001-Make-DynamicEndpointSnitch-a-subclass-of-AbstractNet.patch,25/Aug/10 14:06;jbellis;1429-v2.txt;https://issues.apache.org/jira/secure/attachment/12453040/1429-v2.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,05:41.9,,,no_permission,,,,,,,,,,,,20131,,,Wed Aug 25 16:00:32 UTC 2010,,,,,,0|i0g507:,92241,slebresne,slebresne,,,,,,,,,"25/Aug/10 14:05;jbellis;this breaks DES, the whole point is that it uses a different sort than ANTS.

the right fix is to get rid of the obsolete casts in the Strategy classes, the methods they want are all on IES now.

attached.","25/Aug/10 15:25;slebresne;I agree the second patch is a better alternative. But I think that the casts to AbstractNetworkTopologySnitch in DatacenterWriteResponseHandler.java, /DatacenterSyncWriteResponseHandler.java and DatacenterQuorumResponseHandler.java should also go away.","25/Aug/10 16:00;jbellis;(for the record, as discussed in IRC, I was wrong about Sylvain's original patch breaking DES)

Committed v2 w/ above changes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError: discard at CommitLogContext(file=...) is not after last flush at  ...,CASSANDRA-1330,12470364,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,vilda,vilda,7/28/2010 17:26,3/12/2019 14:16,3/13/2019 22:24,10/2/2010 5:05,0.6.6,0.7 beta 3,,,0,,,,,,"Looks related to CASSANDRA-936?

ERROR [MEMTABLE-POST-FLUSHER:1] 2010-07-28 11:39:36,909 CassandraDaemon.java (line 83) Uncaught exception in thread Thread[MEMTABLE-POST-FLUSHER:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        ... 2 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:373)
        at org.apache.cassandra.db.ColumnFamilyStore$1.runMayThrow(ColumnFamilyStore.java:371)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:365)
        ... 8 more
Caused by: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:394)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:70)
        at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:359)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:52)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
",Java 1.6 / Linux,,,,,,,,,,,,,,,,,,,01/Oct/10 17:25;jbellis;1330.txt;https://issues.apache.org/jira/secure/attachment/12456141/1330.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,25:27.2,,,no_permission,,,,,,,,,,,,20083,,,Sat Oct 02 05:05:47 UTC 2010,,,,,,0|i0g4ef:,92143,mdennis,mdennis,,,,,,,,,"01/Oct/10 17:25;jbellis;This really is essentially the same scenario as CASSANDRA-936.  (See my 3rd-from-the-bottom comment for background and a diagram.)

The part my analysis in 936 is missing is that the CL holds mutations from many columnfamilies, so the position of the first turnOn in a given CF may be arbitrarily high depending on how many mutations happened to other CFs first.  (Similarly, the flush context may also be arbitrarily high, although it's more likely to be low because this scenario can't occur once each CF has been flushed once in the new segment.)

Given the turnOn-for-first-write-to-CF-in-new-segment behavior (which is what keeps us from having to replay the entirety of any not-completely-flushed segement), I don't think we can usefully assert anything about the flush context vs the dirty position.  This patch removes it -- and changes CLS.lastFlushedAt to CLS.cfDirtiedAt (which is what it has already been renamed to in 0.7) to make it more clear that flushing isn't the only thing that affects the header positions.",02/Oct/10 02:55;mdennis;+1,02/Oct/10 05:05;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary Indexes aren't updated when removing whole row,CASSANDRA-1571,12475744,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,xodut,xodut,10/4/2010 9:17,3/12/2019 14:16,3/13/2019 22:24,10/6/2010 17:19,0.7 beta 3,,Feature/2i Index,,0,,,,,,"When I remove a whole row in a CF
del SomeColumnFamily['row']

SI is not updated and get_indexed_slices still returns the deleted row.",,,,,,,,,,,,,,,,,,,,05/Oct/10 16:02;jbellis;1571.txt;https://issues.apache.org/jira/secure/attachment/12456398/1571.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,05:59.7,,,no_permission,,,,,,,,,,,,20206,,,Wed Oct 06 17:19:39 UTC 2010,,,,,,0|i0g61r:,92410,stuhood,stuhood,,,,,,,,,"05/Oct/10 17:05;stuhood;* {{EDIT: Ignore this line: noticed that you try it both ways.}} ColumnFamilyStoreTest uses deletion.value() to confirm that the value was actually removed, but a column that has been deleted and GCd does not contain the original column: the original column contained 'x0000000000000001' and the deletion contains 'x4cab5aad'
* In Table.ignoreObsoleteMutations, is there a way to avoid cloning the CF for each column? Perhaps adding all columns to a single clone, and then collecting them back out?
* Can you add a quickie Javadoc to explain the readCurrent, ignoreObsolete and applyIndex steps?",06/Oct/10 17:19;jbellis;committed w/ suggested improvements,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Detect use of secondary indexes with TTL'd columns,CASSANDRA-1536,12474897,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,9/23/2010 2:55,3/12/2019 14:16,3/13/2019 22:24,9/28/2010 4:26,0.7 beta 2,,Feature/2i Index,,0,,,,,,"We don't currently make any provisions for using TTL'd columns with secondary indexes. A reasonable temporary solution would be to fail inserts of TTL'd entries for indexed columns.

A longer term solution might be to record/detect the presence of expiring columns for an expression, and use that information to require post filtering in CFS.scan.",,,,,,,,,,,,,,,,,,,,28/Sep/10 02:48;stuhood;0001-Use-TTL-d-index-entries-for-TTL-d-values.patch;https://issues.apache.org/jira/secure/attachment/12455792/0001-Use-TTL-d-index-entries-for-TTL-d-values.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,26:35.0,,,no_permission,,,,,,,,,,,,20184,,,Tue Sep 28 13:31:32 UTC 2010,,,,,,0|i0g5tz:,92375,jbellis,jbellis,,,,,,,,,"23/Sep/10 21:42;stuhood;In KEYS indexes, the entry could be made to the index with a TTL as well, and KEYS_BITMAP indexes require post filtering anyway, so this might actually be an easy fix.",28/Sep/10 02:48;stuhood;Patch to use the TTL from an indexed value for its index entry.,28/Sep/10 04:26;jbellis;committed,"28/Sep/10 13:31;hudson;Integrated in Cassandra #549 (See [https://hudson.apache.org/hudson/job/Cassandra/549/])
    support TTL'd index values.
patch by Stu Hood; reviewed by jbellis for CASSANDRA-1536
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
errors reading while bootstrapping,CASSANDRA-1534,12474870,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,9/22/2010 20:56,3/12/2019 14:16,3/13/2019 22:24,9/23/2010 19:28,0.7 beta 2,,,,0,,,,,,"I loaded a 4 node cluster with 1M rows from stress.py, decommissioned a node, and then began bootstrapping it while performing constant reads against the others with stress.py.  After sleeping for 90s, the bootstrapping node started throwing many errors like this:

ERROR 16:51:48,667 Fatal exception in thread Thread[READ_STAGE:1270,5,main]
java.lang.RuntimeException: Cannot service reads while bootstrapping!
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:67)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

And I began receiving timeout errors with stress.py.",,,,,,,,,,,,,,,,,,,,23/Sep/10 16:21;jbellis;1534.txt;https://issues.apache.org/jira/secure/attachment/12455385/1534.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:17.7,,,no_permission,,,,,,,,,,,,20182,,,Fri Sep 24 12:47:05 UTC 2010,,,,,,0|i0g5tj:,92373,brandon.williams,brandon.williams,,,,,,,,,22/Sep/10 20:58;jbellis;has bootstrap-after-decom ever worked?  i think we leave data in the system table that's going to confuse things,22/Sep/10 21:06;brandon.williams;I rm'd everything after decom.,22/Sep/10 21:21;jbellis;does bs against a vanilla no-decom cluster work?,"22/Sep/10 21:33;brandon.williams;No, same problem.",23/Sep/10 16:21;jbellis;patch attached that makes new node properly announce its bootstrap status.,23/Sep/10 18:46;brandon.williams;+1,23/Sep/10 19:28;jbellis;committed,"24/Sep/10 12:47;hudson;Integrated in Cassandra #545 (See [https://hudson.apache.org/hudson/job/Cassandra/545/])
    fix setting bootstrap status on startup.
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1534
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted handoff does not replay data,CASSANDRA-1656,12478159,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/23/2010 18:21,3/12/2019 14:16,3/13/2019 22:24,10/26/2010 11:00,0.6.7,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,23/Oct/10 18:22;jbellis;1656.txt;https://issues.apache.org/jira/secure/attachment/12457906/1656.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,47:08.6,,,no_permission,,,,,,,,,,,,20243,,,Tue Oct 26 11:01:19 UTC 2010,,,,,,0|i0g6kv:,92496,brandon.williams,brandon.williams,,,,,,,,,23/Oct/10 19:47;brandon.williams;+1,26/Oct/10 11:00;jbellis;committed,"26/Oct/10 11:01;jbellis;(as noted on the mailing list, this does not affect 0.7)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system_rename_* methods need to be removed until we can solve compaction and flush races.,CASSANDRA-1630,12477742,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,10/19/2010 13:36,3/12/2019 14:16,3/13/2019 22:24,10/19/2010 14:26,0.7 beta 3,,Legacy/CQL,,0,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1585,,,,19/Oct/10 14:09;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-remvove-system_rename-methods-from-API.-thift-avro-cha.txt;https://issues.apache.org/jira/secure/attachment/12457550/ASF.LICENSE.NOT.GRANTED--v1-0001-remvove-system_rename-methods-from-API.-thift-avro-cha.txt,19/Oct/10 14:09;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0002-remove-system_rename-methods-from-API.txt;https://issues.apache.org/jira/secure/attachment/12457551/ASF.LICENSE.NOT.GRANTED--v1-0002-remove-system_rename-methods-from-API.txt,19/Oct/10 14:09;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0003-disable-system_renam-ing-in-the-cli.txt;https://issues.apache.org/jira/secure/attachment/12457552/ASF.LICENSE.NOT.GRANTED--v1-0003-disable-system_renam-ing-in-the-cli.txt,19/Oct/10 14:09;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0004-Deprecate-RenameColumnFamily-and-RenameKeyspace.txt;https://issues.apache.org/jira/secure/attachment/12457553/ASF.LICENSE.NOT.GRANTED--v1-0004-Deprecate-RenameColumnFamily-and-RenameKeyspace.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,21:11.0,,,no_permission,,,,,,,,,,,,20225,,,Wed Oct 20 15:14:26 UTC 2010,,,,,,0|i0g6ev:,92469,jbellis,jbellis,,,,,,,,,19/Oct/10 14:21;jbellis;+1,"20/Oct/10 15:14;hudson;Integrated in Cassandra #571 (See [https://hudson.apache.org/hudson/job/Cassandra/571/])
    remvove system_rename* methods from API. thift/avro changes. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1630
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Endpoint cache for a token should be part of AbstractReplicationStrategy and not Snitch,CASSANDRA-1643,12478014,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,khichrishi,khichrishi,10/21/2010 18:31,3/12/2019 14:16,3/13/2019 22:24,10/21/2010 20:55,0.7 beta 3,,,,0,,,,,,"There is a single DynamicEndpointSnitch object for all ReplicationStrategy objects. This DynamicEndpointSnitch object contains a single IEndpointSnitch subsnitch object. This subsnitch object contains the Endpoint cache for a token. Thus there is a single endpoint cache for all ReplicationStrategy objects. This implies that replica nodes for a Token as returned by the Cache would be same irrespective of the ReplicationStrategy object. This is a bug, the Endpoint cache should be a part of ""AbstractReplicationStrategy"" object rather than the IEndpointSnitch object.
",,,86400,86400,,0%,86400,86400,,,,,,,,,,,,21/Oct/10 20:37;jbellis;1643-v2.txt;https://issues.apache.org/jira/secure/attachment/12457780/1643-v2.txt,21/Oct/10 19:28;jbellis;1643.txt;https://issues.apache.org/jira/secure/attachment/12457775/1643.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,19:56.6,,,no_permission,,,,,,,,,,,,20234,,,Fri Oct 22 12:52:29 UTC 2010,,,,,,0|i0g6hr:,92482,brandon.williams,brandon.williams,,,,,,,,,"21/Oct/10 20:19;brandon.williams;SimpleStrategyTest is broken:

    [junit] Testsuite: org.apache.cassandra.locator.SimpleStrategyTest
    [junit] Tests run: 4, Failures: 1, Errors: 0, Time elapsed: 0.591 sec
    [junit]
    [junit] Testcase: tryBogusTable(org.apache.cassandra.locator.SimpleStrategyTest):   FAILED
    [junit] No replica strategy configured for SomeBogusTableThatDoesntExist
    [junit] junit.framework.AssertionFailedError: No replica strategy configured for SomeBogusTableThatDoesntExist
    [junit]     at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:254)
    [junit]     at org.apache.cassandra.locator.SimpleStrategyTest.tryBogusTable(SimpleStrategyTest.java:48)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.locator.SimpleStrategyTest FAILED
",21/Oct/10 20:37;jbellis;fixed test,21/Oct/10 20:44;brandon.williams;+1,21/Oct/10 20:55;jbellis;committed,"22/Oct/10 12:52;hudson;Integrated in Cassandra #573 (See [https://hudson.apache.org/hudson/job/Cassandra/573/])
    move endpoint cache from snitch to strategy
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1643
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cli not picking up type annotation on set,CASSANDRA-1635,12477874,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,10/20/2010 14:40,3/12/2019 14:16,3/13/2019 22:24,10/20/2010 21:11,0.7 beta 3,,Legacy/Tools,,0,,,,,,"{code}
[default@Keyspace1] create column family Users
create column family Users
737c7a71-dc56-11df-8240-e700f669bcfc
[default@Keyspace1] set Users[jsmith][first] = 'John'
set Users[jsmith][first] = 'John'
Value inserted.
[default@Keyspace1] set Users[jsmith][last] = 'Smith'
set Users[jsmith][last] = 'Smith'
Value inserted.
[default@Keyspace1] set Users[jsmith][age] = long(42)
set Users[jsmith][age] = long(42)
Value inserted.
[default@Keyspace1] get Users[jsmith]
get Users[jsmith]
=> (column=6c617374, value=Smith, timestamp=1287584999695000)
=> (column=6669727374, value=John, timestamp=1287584990126000)
=> (column=616765, value=^@^@^@^@^@^@^@*, timestamp=1287585014593000)
Returned 3 results.
{code}",,,,,,,,,,,,,,,,,,,,20/Oct/10 20:44;xedin;CASSANDRA-1635-v2.patch;https://issues.apache.org/jira/secure/attachment/12457707/CASSANDRA-1635-v2.patch,20/Oct/10 17:14;xedin;CASSANDRA-1635.patch;https://issues.apache.org/jira/secure/attachment/12457681/CASSANDRA-1635.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,14:12.6,,,no_permission,,,,,,,,,,,,20228,,,Thu Oct 21 14:55:52 UTC 2010,,,,,,0|i0g6fz:,92474,jbellis,jbellis,,,,,,,,,20/Oct/10 17:14;xedin;Please check after 1615 committed.,"20/Oct/10 19:24;jbellis;post-1615 I get

{code}
=> (column=626972746864617465, value=?, timestamp=1287602501548000)
{code}

Also, I noticed that it's actually sending

{code}
        thriftClient_.system_update_column_family(columnFamily);
{code}

when I use a conversion function.  this is not right, we should only issue system_update_column_family for ""update column family"" statements, anything else we infer should stay local.","20/Oct/10 20:00;xedin;my test (after applied CASSANDRA-1635.patch to the latest trunk):
{code}
[default@KS1] create column family Users with comparator=UTF8Type and column_metadata=[{column_name:first, validation_class:UTF8Type}, {column_name:last, validation_class:UTF8Type}]
412367d6-dc83-11df-bf47-e700f669bcfc
[default@KS1] set Users[jsmith][first] = 'John'
Value inserted.
[default@KS1] set Users[jsmith][last] = 'Smith'
Value inserted.
[default@KS1] set Users[jsmith][age] = long(42)
Value inserted.
[default@KS1] get Users[jsmith]
=> (column=last, value=Smith, timestamp=1287604215498000)
=> (column=first, value=John, timestamp=1287604214111000)
=> (column=age, value=42, timestamp=1287604216661000)
Returned 3 results.
{code}

I use UTF8Type here manually because default comparator for column names is BytesType. Results you get are very weird... 

if you mean convertValueByFunction method then it has boolean withUpdate argument which used to determine if system update is need, I have also added polymorphic convertValueByFunction 3 arguments: columnFamily, columnName and tree argument, which calls convertValueByFunction with withUpdate set to false.
","20/Oct/10 20:16;jbellis;bq. my test (after applied CASSANDRA-1635.patch to the latest trunk): 

Ah, I thought when you said ""Please check after 1615 committed"" that 1615 was supposed to fix it.  I do see value working correctly after 1635 patch.  committed.

bq. convertValueByFunction method then it has boolean withUpdate argument which used to determine if system update is need, I have also added polymorphic convertValueByFunction 3 arguments: columnFamily, columnName and tree argument, which calls convertValueByFunction with withUpdate set to false.

something is definitely buggy, when I watch the server log I can see it applying the schema migration after the long() set.

fundamentally side effects to thriftclient have no business in the convertValue method.  update local schema? yes. update server schema? no.","20/Oct/10 20:44;xedin;Sorry for my misleading comment :/ now updates for column defs are performed only locally except ""update column family"" statement.","20/Oct/10 21:11;jbellis;perfect, committed.","21/Oct/10 14:55;hudson;Integrated in Cassandra #572 (See [https://hudson.apache.org/hudson/job/Cassandra/572/])
    avoid updating server schema except for explicit 'update column family'.  patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-1635
fix cli value conversion, update readme
patch by Pavel Yaskevich and jbellis for CASSANDRA-1635
cli support for index queries
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-1635
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster restart re-adds removed tokens,CASSANDRA-1609,12477154,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,nickmbailey,nickmbailey,10/12/2010 16:49,3/12/2019 14:16,3/13/2019 22:24,10/13/2010 15:57,0.7 beta 3,,,,0,,,,,,"After a cluster restart one of our nodes began reporting tokens that had been removed a good while ago (week or more) in it's nodetool ring output.  This probably has something to do with our change to persist the ring in CASSANDRA-1518 and removetoken changes in CASSANDRA-1216. The node didn't actually gossip the removed tokens so they showed up in TMD but not gossip.

Additionally all nodes began reporting a node that had been removed maybe an hour ago.  ",,,,,,,,,,,,,,,,,,,,12/Oct/10 21:54;jbellis;1609.txt;https://issues.apache.org/jira/secure/attachment/12457019/1609.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,54:12.9,,,no_permission,,,,,,,,,,,,20215,,,Thu Oct 14 12:49:47 UTC 2010,,,,,,0|i0g6a7:,92448,nickmbailey,nickmbailey,,,,,,,,,12/Oct/10 17:24;nickmbailey;So the first 4 tokens were removed using decomission and the last one using removetoken. It doesn't look like either of those processes remove nodes from the saved state. The nodes then show up in nodetool ring but not gossip since they don't actually have an application state to begin with.,"12/Oct/10 19:09;nickmbailey;So it looks like we don't remove tokens from the system table when we decomission which leads to the following scenario:

# Node A has token 1
# Node A loadbalance to token 2
# Node A dies
# Node A removed
# Cluster restart
# Node A reappears with token 1 since that was never removed/overwritten in the system table.

At least i think thats how its happening. Not sure about one of our nodes seeing the four decomissioned tokens and the others not.","12/Oct/10 21:54;jbellis;ring state management is a mess.  token removal happens in 3 places:

 1) node receives decommission notice (STATE_LEFT)
 2) node receives removetoken notice, piggy-backed on STATE_NORMAL
 3) node coordinates removetoken (gossiper will not trigger notifications for state changes that initiated locally, so this needs to be handled separately from 2)

2) was updating the SystemTable w/ the removal but the others were not.

patch attached to move this logic into excise() method and call from all 3 places.","13/Oct/10 03:38;nickmbailey; * handleStateLeft wasn't actually removing the endpoint from gossip before. Was that an oversight or intended?
 * you removed a check from handleStateLeft to make sure the token was a member before removing it.  Intentional?
","13/Oct/10 15:14;jbellis;bq. handleStateLeft wasn't actually removing the endpoint from gossip before

right, that was a bug.

bq. you removed a check from handleStateLeft to make sure the token was a member

yes, we skip the check everywhere else because either way the result is the same.  there's no meaningful action we can take if the check fails so it's redundant.",13/Oct/10 15:53;nickmbailey;+1,13/Oct/10 15:57;jbellis;committed,"14/Oct/10 12:49;hudson;Integrated in Cassandra #565 (See [https://hudson.apache.org/hudson/job/Cassandra/565/])
    fix removing tokens from SystemTable on decommission and removetoken.
patch by jbellis; reviewed by Nick Bailey for CASSANDRA-1609
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
statistics not created after streaming data,CASSANDRA-1620,12477420,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/15/2010 4:25,3/12/2019 14:16,3/13/2019 22:24,10/18/2010 17:43,0.7 beta 3,,,,0,,,,,,"after loadbalance operation, cfstats NPEs:

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamilyStore.getMinRowSize(ColumnFamilyStore.java:332)
        at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)",,,,,,,,,,,,,,,,,,,,16/Oct/10 16:34;jbellis;1620-2.txt;https://issues.apache.org/jira/secure/attachment/12457344/1620-2.txt,16/Oct/10 02:09;jbellis;1620.txt;https://issues.apache.org/jira/secure/attachment/12457318/1620.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,26:41.2,,,no_permission,,,,,,,,,,,,20220,,,Tue Oct 19 14:15:04 UTC 2010,,,,,,0|i0g6cn:,92459,brandon.williams,brandon.williams,,,,,,,,,15/Oct/10 23:24;jbellis;root cause is SSTableWriter.Builder not creating the stats file post-stream.,"16/Oct/10 02:09;jbellis;patch to avoid NPEs when stats file does not exist (upgrading from 0.6).

should still add stats to post-stream build operation since we're iterating through the data file to build the index anyway.",16/Oct/10 16:34;jbellis;-2 (applies on top of original) adds stats build post-stream.  please test :),18/Oct/10 17:26;brandon.williams;+1,18/Oct/10 17:43;jbellis;committed,"19/Oct/10 14:15;hudson;Integrated in Cassandra #570 (See [https://hudson.apache.org/hudson/job/Cassandra/570/])
    compute next row position before changing the file pointer.  patch by jbellis for CASSANDRA-1620
build stats post-stream
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1620
avoid null SSTable stat histograms
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1620
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUG: secondaryIndexes AND multiple index expressions can cause timesouts,CASSANDRA-1623,12477501,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jasontanner,jasontanner,10/15/2010 19:59,3/12/2019 14:16,3/13/2019 22:24,10/18/2010 20:42,0.7 beta 3,,,,0,,,,,,"1. Given this Column Family definition

    Column Family Name: Requests
      Column Family Type: Standard
      Column Sorted By: org.apache.cassandra.db.marshal.UTF8Type
      Column Metadata:
        Column Name: requested
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Type: KEYS
        Column Name: requestor
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Type: KEYS

If I have an entry that has the following column/value pairs:

""request-uuid1"" : [  { ""requested"",""person-uuid1"" }, { ""requestor"",""person-uuid2""}, { ""is_confirmed"",""true"" } ]

If I do an index lookup (pseudo coded) :

get_index_slices( Connection,
                                 ColumnParent.column_family=""Requests"",
                                 [ { ""requested"",""eq"", ""person-uuid1"" }, { ""is_confirmed"",""eq"", ""false"" } ],      % Index Expressions
                                 """",100,   % StartKey, KeyCount
                                 """","""",false,100   % StartCol, EndCol, Reversed, ColCount )

for ""requested"" = ""person-uuid1"" and ""is_confirmed"" = false 

then I get the following entries in my log and the request times out along with all other requests on all clients.

DEBUG [pool-1-thread-10] 2010-10-15 19:00:27,878 CassandraServer.java (line 531) scan
DEBUG [pool-1-thread-10] 2010-10-15 19:00:27,897 StorageProxy.java (line 563) restricted single token match for query [0,0]
DEBUG [pool-1-thread-10] 2010-10-15 19:00:27,897 StorageProxy.java (line 649) scan ranges are [0,0]
DEBUG [pool-1-thread-10] 2010-10-15 19:00:27,925 StorageProxy.java (line 669) reading org.apache.cassandra.db.IndexScanCommand@42a6eb from 52@localhost/127.0.0.1
DEBUG [ReadStage:2] 2010-10-15 19:00:27,931 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,933 SliceQueryFilter.java (line 121) collecting 0 of 2147483647: is_confirmed:false:4@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,934 SliceQueryFilter.java (line 121) collecting 1 of 2147483647: request_type:false:6@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,935 SliceQueryFilter.java (line 121) collecting 2 of 2147483647: requested:false:58@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,935 SliceQueryFilter.java (line 121) collecting 3 of 2147483647: requested_network:false:57@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,936 SliceQueryFilter.java (line 121) collecting 4 of 2147483647: requestor:false:58@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,937 SliceQueryFilter.java (line 121) collecting 5 of 2147483647: requestor_network:false:57@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,942 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,943 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,945 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
DEBUG [ReadStage:2] 2010-10-15 19:00:27,946 SliceQueryFilter.java (line 121) collecting 0 of 1: null:false:0@1287103291
 this last line repeats forever until I stop the server.

If instead I do the lookup where both terms match or just the last term matches then nothing goes wrong, I get a valid (empty or otherwise) result set.

It only seems to happen if the 2nd expression does not match.

I am using the very latest code from trunk.


Jason
                                       ",centos 5.5,,,,,,,,,,,,,,,,,,,18/Oct/10 15:05;jbellis;1623.txt;https://issues.apache.org/jira/secure/attachment/12457452/1623.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,05:14.9,,,no_permission,,,,,,,,,,,,20221,,,Tue Oct 19 14:15:04 UTC 2010,,,,,,0|i0g6db:,92462,jasontanner,jasontanner,,,,,,,,,18/Oct/10 15:05;jbellis;Patch attached with fix and unit test demonstrating the problem.,18/Oct/10 20:26;jasontanner;Patch tested on my install and it resolved my issue.,18/Oct/10 20:42;jbellis;committed,"19/Oct/10 14:15;hudson;Integrated in Cassandra #570 (See [https://hudson.apache.org/hudson/job/Cassandra/570/])
    fix potential infinite loop in 2ary index queries
patch by jbellis; tested by Jason Tanner for CASSANDRA-1623
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
auto-guessed memtable sizes are too high,CASSANDRA-1641,12477951,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/21/2010 5:22,3/12/2019 14:16,3/13/2019 22:24,10/21/2010 17:10,0.7 beta 3,,,,0,,,,,,"I've seen two cases now of the memtable sizes being too large, causing OOMing.  Too-small memtables hurt performance, but too-large hurts worse when you start GC storming.",,,,,,,,,,,,,,,,,,,,21/Oct/10 05:26;jbellis;1641.txt;https://issues.apache.org/jira/secure/attachment/12457722/1641.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,04:41.6,,,no_permission,,,,,,,,,,,,20232,,,Fri Oct 22 12:52:28 UTC 2010,,,,,,0|i0g6hb:,92480,brandon.williams,brandon.williams,,,,,,,,,"21/Oct/10 05:26;jbellis;I'd like to introduce a dependency on number of user CFs, but that's not available until after we've picked a value.  So this cuts the size by half, which feels scientific because on the old default 1GB heap size this gives us memtable throughput of 64MB, which is also the old default.",21/Oct/10 17:04;brandon.williams;+1,21/Oct/10 17:10;jbellis;committed,"22/Oct/10 12:52;hudson;Integrated in Cassandra #573 (See [https://hudson.apache.org/hudson/job/Cassandra/573/])
    reduce automaticallychosen memtablesizes by 50%
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1641
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
removetoken is broken,CASSANDRA-1650,12478109,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,nickmbailey,brandon.williams,brandon.williams,10/22/2010 19:44,3/12/2019 14:16,3/13/2019 22:24,10/22/2010 21:14,0.7 beta 3,,,,0,,,,,,"When running removetoken on a dead node, it hangs forever.  The debug log shows:

DEBUG 19:45:53,794 Node /10.179.111.137 ranges [(115049868157599339472315320703867977321,62676456546693435176060154681903071729]]
DEBUG 19:45:53,795 Range (115049868157599339472315320703867977321,62676456546693435176060154681903071729] will be responsibility of cassandra-1/10.179.65.102
DEBUG 19:45:53,798 Pending ranges:
cassandra-1/10.179.65.102:(115049868157599339472315320703867977321,62676456546693435176060154681903071729]

DEBUG 19:45:53,798 Node /10.179.111.137 ranges [(115049868157599339472315320703867977321,62676456546693435176060154681903071729]]
DEBUG 19:45:53,799 Range (115049868157599339472315320703867977321,62676456546693435176060154681903071729] will be responsibility of cassandra-1/10.179.65.102
",,,,,,,,,,,,,,,,,,,,22/Oct/10 21:05;nickmbailey;0001-Don-t-wait-for-confirmation-when-replication-factor-.patch;https://issues.apache.org/jira/secure/attachment/12457872/0001-Don-t-wait-for-confirmation-when-replication-factor-.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,03:43.1,,,no_permission,,,,,,,,,,,,20240,,,Sat Oct 23 12:49:04 UTC 2010,,,,,,0|i0g6jb:,92489,brandon.williams,brandon.williams,,,,,,,,,"22/Oct/10 20:03;nickmbailey;So removetoken blocks for all new replicas to confirm they have the data for the node being removed.  You can actually run ""removetoken status"" to see which nodes are being waited on.  Since a dropped confirm message or stream request could cause it to block forever you can run ""removetoken force"" to finish a removal already in progress.  

There could be a bug here as well or possibly a dropped message?

There should probably be new documentation about this in the wiki.","22/Oct/10 20:06;brandon.williams;My RF is 1, so there are no replicas to block for.  Here's the status output:

RemovalStatus: Removing token (62676456546693435176060154681903071729). Waiting for replication confirmation from [cassandra-1/10.179.65.102].

It seems to be waiting for confirmation from itself.","22/Oct/10 20:07;brandon.williams;Also, force does not work:

Exception in thread ""main"" java.lang.UnsupportedOperationException: Node /10.179.111.137 is already being removed.
",22/Oct/10 21:05;nickmbailey;When replication factor is 1 it is impossible to recover data so we shouldn't wait for confirmation.,"22/Oct/10 21:14;brandon.williams;+1, committed.","23/Oct/10 12:49;hudson;Integrated in Cassandra #574 (See [https://hudson.apache.org/hudson/job/Cassandra/574/])
    Don't wait for confirmation when removing token and RF=1.  Patch by Nick Bailey, reviewed by brandonwilliams for CASSANDRA-1650
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Config converter should set framed transport by default,CASSANDRA-1659,12478166,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,10/23/2010 23:12,3/12/2019 14:16,3/13/2019 22:24,10/24/2010 1:10,0.7 beta 3,,,,0,,,,,,"Our built in clients require the framed transport, so it makes sense to enable it by default in the config converter.",,,,,,,,,,,,,,,,,,,,23/Oct/10 23:13;stuhood;1659.txt;https://issues.apache.org/jira/secure/attachment/12457913/1659.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,10:32.2,,,no_permission,,,,,,,,,,,,20244,,,Sun Oct 24 12:48:58 UTC 2010,,,,,,0|i0g6lj:,92499,jbellis,jbellis,,,,,,,,,24/Oct/10 01:10;jbellis;committed,"24/Oct/10 12:48;hudson;Integrated in Cassandra #575 (See [https://hudson.apache.org/hudson/job/Cassandra/575/])
    update Converter to switch default thrift mode to framed.  patch by Stu Hood; reviewed by jbellis for CASSANDRA-1659
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IntegerType.toString() handles ByteBuffer incorrectly,CASSANDRA-1681,12478635,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jancona,jancona,jancona,10/29/2010 1:41,3/12/2019 14:16,3/13/2019 22:24,10/29/2010 2:05,0.7.0 rc 1,,,,0,,,,,,IntegerType.getString doesn't correctly extract the byte array from the ByteBuffer passed to it. This causes incorrect results in cassandra-cli as described in [CASSANDRA-1680|https://issues.apache.org/jira/browse/CASSANDRA-1680].,,,,,,,,,,,,,,,,,,,,29/Oct/10 01:41;jancona;integer-type-byte-buffer.patch;https://issues.apache.org/jira/secure/attachment/12458304/integer-type-byte-buffer.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,05:57.6,,,no_permission,,,,,,,,,,,,20257,,,Fri Oct 29 02:05:57 UTC 2010,,,,,,0|i0g6qf:,92521,jbellis,jbellis,,,,,,,,,"29/Oct/10 02:05;jbellis;committed, with a slight change to call byteBufferToByteArray directly.  thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cli support for strategy_options,CASSANDRA-1612,12477205,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,10/13/2010 3:43,3/12/2019 14:16,3/13/2019 22:24,10/13/2010 13:52,0.7 beta 3,,Legacy/Tools,,0,,,,,,"[default@unknown] create keyspace MyCollections with placement_strategy='org.apache.cassandra.locator.NetworkTopologyStrategy' and strategy_options=[{DC1:2, DC2:2}] and replication_factor=4
No enum const class org.apache.cassandra.cli.CliClient$AddKeyspaceArgument.STRATEGY_OPTIONS
",,,,,,,,,,,,,,,,,,,,13/Oct/10 13:34;xedin;CASSANDRA-1612.patch;https://issues.apache.org/jira/secure/attachment/12457064/CASSANDRA-1612.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,34:55.8,,,no_permission,,,,,,,,,,,,20217,,,Thu Oct 14 12:49:50 UTC 2010,,,,,,0|i0g6av:,92451,jbellis,jbellis,,,,,,,,,13/Oct/10 13:34;xedin;Help is updated also.,"13/Oct/10 13:52;jbellis;committed.

but let's not litter local variables w/ unnecessary final keywords.",13/Oct/10 14:00;xedin;Understood.,"14/Oct/10 12:49;hudson;Integrated in Cassandra #565 (See [https://hudson.apache.org/hudson/job/Cassandra/565/])
    cli support for strategy_options.
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-1612
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair blocking forever when RF=1,CASSANDRA-1691,12478827,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,mbulman,mbulman,11/1/2010 19:15,3/12/2019 14:16,3/13/2019 22:24,11/1/2010 22:39,0.7.0 rc 1,,Legacy/Tools,,1,,,,,,"Tested on single and multi-node, RF=1.  Repair runs but never reports that it's complete, hence the blocking forever.",,,,,,,,,,,,,,,,,,,,01/Nov/10 22:01;stuhood;0001-Complete-the-session-early-if-it-has-no-content.patch;https://issues.apache.org/jira/secure/attachment/12458587/0001-Complete-the-session-early-if-it-has-no-content.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,49:06.1,,,no_permission,,,,,,,,,,,,20260,,,Mon Nov 01 22:39:33 UTC 2010,,,,,,0|i0g6sn:,92531,jbellis,jbellis,,,,,,,,,01/Nov/10 20:49;jbellis;pretty sure single-node repair is special cased to be a no-op.,"01/Nov/10 21:37;mbulman;It definitely does _something_ for single-node.  ""computing ranges.."" followed by ""GC for copy""",01/Nov/10 21:41;stuhood;Will post a patch soon.,01/Nov/10 22:01;stuhood;Checks for an empty neighbor list and completes early.,"01/Nov/10 22:06;stuhood;Er, this isn't tested yet... give me one more second.

EDIT: Looks good now.",01/Nov/10 22:23;jbellis;does 0.6 need this?,"01/Nov/10 22:28;stuhood;I'm not sure anything ""needs"" this, and the code is sufficiently different that I don't think backporting to 0.6 is worthwhile.",01/Nov/10 22:39;jbellis;committed to 0.7 + trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use of ByteBuffer limit() must account for arrayOffset(),CASSANDRA-1661,12478239,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,tjake,tjake,10/25/2010 15:06,3/12/2019 14:16,3/13/2019 22:24,10/26/2010 3:14,0.7 beta 3,,,,0,,,,,,"There are a few places in the code where it loops across a byte buffers backing array wrong:


        for (int i=bytes.position()+bytes.arrayOffset(); i<bytes.limit(); i++)

This is incorrect as the limit() does not account for arrayOffset()

              for (int i=bytes.position()+bytes.arrayOffset(); i<bytes.limit()+bytes.arrayOffset(); i++)

is the correct code.

There is also a few places where the unit tests would fail if we used non wrapped byte arrays.
",,,,,,,,,,,,,,,,,,,,25/Oct/10 15:07;tjake;1661_v1.txt;https://issues.apache.org/jira/secure/attachment/12457974/1661_v1.txt,25/Oct/10 18:31;tjake;1661_v2.txt;https://issues.apache.org/jira/secure/attachment/12457989/1661_v2.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,09:51.9,,,no_permission,,,,,,,,,,,,20245,,,Tue Oct 26 03:14:43 UTC 2010,,,,,,0|i0g6lz:,92501,stuhood,stuhood,,,,,,,,,25/Oct/10 15:09;stuhood;Regarding failing unit tests: a utility function that calls the allocator from 1651 would probably shorten the code and catch a lot of these.,"25/Oct/10 15:21;tjake;good idea, I'll address it with that ticket","25/Oct/10 16:24;stuhood;* A few of the loops involving direct array access (in particular the one in RandomPartitioner) could be simplified with judicious use of mark() and get()
* Adding the follow function and using it in the tests would clarify things a lot: CFSTest looks like it exploded.
{code:java}public static ByteBuffer bytes(String s) { return ByteBuffer.wrap(s.getBytes(UTF_8)); }{code}
* Converting BytesToken to ByteBuffer storage would clean up a ton of special casing (should probably be tackled in a separate issue though)
* ByteBufferTest is literally a test of ByteBuffers: I don't think it should be in C*

Thanks!","25/Oct/10 16:29;jbellis;bq. simplified with judicious use of mark() and get()

I worry that we're going to introduce very very tricky bugs if we don't treat position as immutable.  If BB/Thrift were sane about using offset to represent the start of what we're allowed to mess with, that would be another story, but it's not -- Thrift uses wrap to generate the BB it gives us, so position is the only indication we have of what a given BB is supposed to cover.

bq. ByteBufferTest is literally a test of ByteBuffers: I don't think it should be in C*

OTOH ByteBuffer is a tricky enough API (see: this ticket :) that I don't mind having BBT as an illustration of what is going on if not exactly a test.

Maybe docstring in BBUtil instead of actual tests?","25/Oct/10 16:30;jbellis;bq. Converting BytesToken to ByteBuffer storage

As background, this is an artifact of MerkleTree requiring serializable Tokens.  Using byte[] there was the easiest way to get that.  (Not necessarily the best, I agree.)","25/Oct/10 16:32;tjake;  * A few of the loops involving direct array access (in particular the one in RandomPartitioner) could be simplified with judicious use of mark() and get()
   *A previous version of the BB patch did exactly this, but Johnathan thought it would be better to treat ByteBuffers as immutable so avoid changing position and mark.*


  * Adding the follow function and using it in the tests would clarify things a lot: CFSTest looks like it exploded.
     *I agree this is a mess, if you think it should be fixed in this patch ok, but it seems like a small issue.*

  * Converting BytesToken to ByteBuffer storage would clean up a ton of special casing (should probably be tackled in a separate issue though)
     *I had also done this in a previous patch but it broke MerkleTree serialization*

  * ByteBufferTest is literally a test of ByteBuffers: I don't think it should be in C
     *I put it in there incase others have questions about how ByteBuffers work.*

 ","25/Oct/10 16:40;jbellis;bq. Adding the follow function and using it in the tests would clarify things a lot

+1 doing this.","25/Oct/10 16:43;stuhood;> better to treat ByteBuffers as immutable so avoid changing position and mark
In cases where the buffer should be immutable, you'd need to duplicate() it first.

> should be fixed in this patch ok, but it seems like a small issue
It's a small function: waiting any longer to add it isn't worth it, imo.

> I put it in there incase others have questions about how ByteBuffers work.
I'm out-numbered on this one I guess.","25/Oct/10 18:31;tjake;New version:

  * Puts BB test in BBUtil javadoc
  * Cleans up tests with new BBUtil call
  * Fixes Pig and word count","25/Oct/10 18:43;stuhood;+1 All that is left is nitpicks, which you can choose to ignore:

> Cleans up tests with new BBUtil call
A static import might be the last bit of sugar to make this a real win.

Code style dictates that there should be spaces around operators.",26/Oct/10 03:14;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read repair IndexOutOfBoundsException,CASSANDRA-1727,12479656,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,11/10/2010 23:22,3/12/2019 14:16,3/13/2019 22:24,11/11/2010 0:35,0.6.8,0.7.0 rc 1,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,11/Nov/10 00:09;jbellis;1727-v2.txt;https://issues.apache.org/jira/secure/attachment/12459302/1727-v2.txt,10/Nov/10 23:34;jbellis;1727.txt;https://issues.apache.org/jira/secure/attachment/12459296/1727.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,18:23.7,,,no_permission,,,,,,,,,,,,20275,,,Thu Nov 11 00:35:18 UTC 2010,,,,,,0|i0g70n:,92567,brandon.williams,brandon.williams,,,,,,,,,"10/Nov/10 23:22;jbellis;Originally reported on CASSANDRA-1719,

{code}
ERROR 20:14:11,591 Uncaught exception in thread Thread[CACHETABLE-TIMER-3,5,main]
java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
at org.apache.cassandra.service.ConsistencyChecker$DataRepairHandler.callMe(ConsistencyChecker.java:186)
at org.apache.cassandra.service.ConsistencyChecker$DataRepairHandler.callMe(ConsistencyChecker.java:141)
at org.apache.cassandra.utils.ExpiringMap$CacheMonitor.run(ExpiringMap.java:105)
at java.util.TimerThread.mainLoop(Timer.java:534)
at java.util.TimerThread.run(Timer.java:484)
Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
at java.util.ArrayList.rangeCheck(ArrayList.java:571)
at java.util.ArrayList.get(ArrayList.java:349)
at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:131)
at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:45)
at org.apache.cassandra.service.ConsistencyChecker$DataRepairHandler.callMe(ConsistencyChecker.java:182)
... 4 more
{code}

{code}
ERROR 20:17:08,688 Uncaught exception in thread Thread[CACHETABLE-TIMER-8,5,main]
java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
at org.apache.cassandra.service.ConsistencyChecker$DataRepairHandler.callMe(ConsistencyChecker.java:186)
at org.apache.cassandra.service.ConsistencyChecker$DataRepairHandler.callMe(ConsistencyChecker.java:141)
at org.apache.cassandra.utils.ExpiringMap$CacheMonitor.run(ExpiringMap.java:105)
at java.util.TimerThread.mainLoop(Timer.java:512)
at java.util.TimerThread.run(Timer.java:462)
Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
at java.util.ArrayList.RangeCheck(ArrayList.java:547)
at java.util.ArrayList.get(ArrayList.java:322)
at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:131)
at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:45)
at org.apache.cassandra.service.ConsistencyChecker$DataRepairHandler.callMe(ConsistencyChecker.java:182)
... 4 more
{code}",10/Nov/10 23:34;jbellis;Fix for regression from CASSANDRA-1622.,11/Nov/10 00:09;jbellis;v2 also fixes the local response,11/Nov/10 00:18;brandon.williams;+1,11/Nov/10 00:35;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableImport adds columns marked for delete incorrectly in methods addToStandardCF & addToSuperCF,CASSANDRA-1753,12480245,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bryantower,pushpinder.heer,pushpinder.heer,11/17/2010 19:50,3/12/2019 14:16,3/13/2019 22:24,11/18/2010 10:14,0.7.0 rc 1,,Legacy/Tools,,0,,,,,,"The logic for adding column families in the methods addToStandardCF & addToSuperCF appears to be backwards

            if (col.isDeleted) {
                cfamily.addColumn(path, hexToBytes(col.value), new TimestampClock(col.timestamp));
            } else {
                cfamily.addTombstone(path, hexToBytes(col.value), new TimestampClock(col.timestamp));
            }

",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,17/Nov/10 22:26;bryantower;cassandra-0.7-1753.txt;https://issues.apache.org/jira/secure/attachment/12459838/cassandra-0.7-1753.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,26:47.7,,,no_permission,,,,,,,,,,,,20292,,,Fri Nov 19 15:12:34 UTC 2010,,,,,,0|i0g76f:,92593,slebresne,slebresne,,,,,,,,,"17/Nov/10 22:26;bryantower;This patch adds a check to the SSTableImportTest to make sure that the retrieved Column is not deleted and fixes the bug in the SSTableImport.java by switching the logic on the isDeletedCheck.
The patch is for the cassandra-0.7 branch.",18/Nov/10 00:21;bryantower;This bug prevents anyone importing to SSTables from JSON format.  All of the data that was exported shows up as tombstoned when trying to do an import on the 0.7 branch.  This patch is simple and is isolated to the SSTableImport,18/Nov/10 08:23;slebresne;+1,"18/Nov/10 10:14;jbellis;committed, thanks!","19/Nov/10 15:12;hudson;Integrated in Cassandra-0.7 #16 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/16/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming never makes progress,CASSANDRA-1766,12480607,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,11/22/2010 20:25,3/12/2019 14:16,3/13/2019 22:24,12/27/2010 22:47,0.6.9,0.7.0,,,0,,,,,,"I have a client that can never complete a bootstrap.  AC finishes, streaming begins.  Stream initiate completes, and the sources wait on the transfer to finish, but progress is never made on any stream.  Nodetool reports streaming is happening, the socket is held open, but nothing happens.",,,,,,,,,,,,,,,,,,,,22/Dec/10 20:02;jbellis;1766-keepalive.txt;https://issues.apache.org/jira/secure/attachment/12466831/1766-keepalive.txt,24/Nov/10 00:25;eonnen;CASSANDRA-1766.patch;https://issues.apache.org/jira/secure/attachment/12460325/CASSANDRA-1766.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,25:05.2,,,no_permission,,,,,,,,,,,,20301,,,Mon Dec 27 23:00:45 UTC 2010,,,,,,0|i0g79b:,92606,brandon.williams,brandon.williams,,,,,,,,,"24/Nov/10 00:25;eonnen;Not sure it's exactly related but I encountered an issue where a stream failed post AE and was just wedged with the following stack trace:

""STREAM-STAGE:1"" prio=10 tid=0x00007ff2440a5800 nid=0x3c3c in Object.wait() [0x00007ff24a21f000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00007ff28884fad8> (a org.apache.cassandra.utils.SimpleCondition)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.cassandra.utils.SimpleCondition.await(SimpleCondition.java:38)
        - locked <0x00007ff28884fad8> (a org.apache.cassandra.utils.SimpleCondition)
        at org.apache.cassandra.streaming.StreamOutManager.waitForStreamCompletion(StreamOutManager.java:164)
        at org.apache.cassandra.streaming.StreamOut.transferSSTables(StreamOut.java:138)
        at org.apache.cassandra.service.AntiEntropyService$Differencer$1.runMayThrow(AntiEntropyService.java:511)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

We suspect that this occurred because the destination node was in drain state, although from reading the code it appears that any failed stream where the destination goes away would be susceptible to this issue. In this case, the StreamManager will never unblock making subsequent repairs to any node that was pending transfer impossible.

I've attached a patch that smooths out some possible streaming issues:

* Catches streaming errors. Near as I can tell, if an error occurred during streaming because the remote node went away, it would bubble all the way out of the executor and not even be logged. Worse, it would keep the current pending file wedged and never allow it to be cleared. This patch will remove the failed transfer when an IOException occurs. Could be it should be more general
* Allows for manual purging of pending files to a host via JMX which means un-sticking a wedged transfer no-longer requires a restart of that node. It also unfortunately results in removal of the file which could require anti-compaction again but this was the least painful path through the code.
* Corrects an unlikely but potentially fatal scenario where concurrent mutation/read from the file and fileMap references could result in dirty reads by making them concurrency-safe collections. Only way I could see this happening is if someone were to run repair multiple times in succession while streaming was happening. Unlikely but possible and the effects on unsafe map reads can result in a completely unresponsive JVM.


I'm not entirely sure this is the right thing to do but I though I'd float it out there for review. Whatever the correct fix, I think there needs to be a way to cancel pending streams so that they aren't stuck.","24/Nov/10 17:18;jbellis;Thanks for the patch, Erik.  Moving followup on that to CASSANDRA-1438 to leave this for the ""streaming doesn't start at all"" problem.","22/Dec/10 18:57;brandon.williams;What's happening here in my case, is there is a firewall/vpn between the bootstrapping node and the source.  The source takes a long time to anticompact, and in this time the tcp connection is killed due to being idle by the firewall.  This causes the stream initiate done message to never be received, because OutboundTcpConnection doesn't actually retry, it only buffers.",22/Dec/10 19:01;jbellis;Should we just turn on socket keepalive?,"22/Dec/10 19:15;brandon.williams;That would hack around the problem, but the real issue is SID can get lost on the wire and hang the streaming process forever.  For 0.6, maybe keepalive is the least invasive thing to do.",22/Dec/10 20:02;jbellis;keepalive patch against 0.6,23/Dec/10 22:28;brandon.williams;+1 on keepalive,"27/Dec/10 22:47;jbellis;committed.

if you want you can open a new ticket for retrying Initiate if the message gets swallowed, as well as recovering if a source is correctly/incorrectly marked dead, but IMO those have very small benefit:effort ratios.","27/Dec/10 23:00;hudson;Integrated in Cassandra-0.6 #36 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/36/])
    enable keepalive on intra-cluster sockets
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1766
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush before repair,CASSANDRA-1748,12479991,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,stuhood,stuhood,11/15/2010 20:04,3/12/2019 14:16,3/13/2019 22:24,12/19/2010 3:33,0.7.0 rc 3,,,,0,,,,,,"We don't currently flush before beginning a validation compaction, meaning that depending on the state of the memtables, we might end up with content on disk that is as different as a single memtable can make it (potentially, very different).",,,,,,,,,,,,,,,,,,,,13/Dec/10 22:23;thobbs;1748-trunk.txt;https://issues.apache.org/jira/secure/attachment/12466172/1748-trunk.txt,13/Dec/10 20:02;thobbs;1748.txt;https://issues.apache.org/jira/secure/attachment/12466166/1748.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,02:25.6,,,no_permission,,,,,,,,,,,,20289,,,Mon Dec 20 04:46:49 UTC 2010,,,,,,0|i0g75b:,92588,stuhood,stuhood,,,,,,,,,13/Dec/10 20:02;thobbs;Attached patch flushes inside of CompactionManager's doValidationCompaction() so that both the repairing node and its neighbors will flush before building Merkle trees.,"13/Dec/10 21:39;stuhood;This fails to build against trunk, but looks reasonable otherwise.",13/Dec/10 22:23;thobbs;1748-trunk.txt should apply against trunk.,"13/Dec/10 22:46;jbellis;would switching validation to use range scans be a better solution?  that way you would automatically get everything in the memtables, without having to flush-and-re-scan.","15/Dec/10 00:44;thobbs;Do you mean something like using CFS.getRangeSlice()?  The main problem I see is you can't iterate over the rows one-by-one with that in its current state as it returns a List; I'm not familiar with how that works, so I don't know how easy it would be to make an Iterator version.  Or is there something else you were referring to?",15/Dec/10 00:51;jbellis;CFS.gRS is a pretty thin layer over RowIterator.,"15/Dec/10 01:01;thobbs;Ah, right, I see that now.  I don't see why it couldn't be done, then.  Do you want me to go ahead with that (and open a new ticket)?","15/Dec/10 01:47;stuhood;> would switching validation to use range scans be a better solution?
We don't have a way to send a memtable to another node, so if we included it in what we validate, it might not actually end up being sent to the other node unless it was flushed at some point. So I don't think doing this would buy us much, and it is a significantly larger change. The goal is really just to repair at a ""point in time"", and flushing before repair gives us that, imo.",19/Dec/10 03:33;jbellis;committed,"20/Dec/10 04:46;hudson;Integrated in Cassandra-0.7 #97 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/97/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KEYS indexes recreated after first restart,CASSANDRA-1541,12474997,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,stuhood,stuhood,9/24/2010 2:09,3/12/2019 14:16,3/13/2019 22:24,10/1/2010 17:56,0.7 beta 3,,,,0,,,,,,"I made sure I waited at least {{commitlog_sync_period_in_ms}}, but the {{SystemTable.isIndexBuilt}} flag doesn't appear to be sticking after a call to system_add_column_family (via CASSANDRA-1531). During the first restart after creation, the index is recreated, logging: ""Creating index"" and ""Index _ complete"" again. The second restart works as it should.",,,,,,,,,,,,,,,,,,,,01/Oct/10 16:05;jbellis;1541.txt;https://issues.apache.org/jira/secure/attachment/12456128/1541.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,05:50.9,,,no_permission,,,,,,,,,,,,20188,,,Sat Oct 02 12:56:26 UTC 2010,,,,,,0|i0g5v3:,92380,stuhood,stuhood,,,,,,,,,01/Oct/10 16:05;jbellis;table.open is called before commitlog replay.  patch adds flush to INDEX_CF in setIndexBuilt so the flag is visible before log replay next restart.,"01/Oct/10 17:52;stuhood;+1
(required a bit of manual rebasing, but the spirit is correct).",01/Oct/10 17:56;jbellis;rebased & committed,"02/Oct/10 12:56;hudson;Integrated in Cassandra #553 (See [https://hudson.apache.org/hudson/job/Cassandra/553/])
    flush index built flag so we can read it before log replay
patch by jbellis; reviewed by Stu Hood for CASSANDRA-1541
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getRestrictedRanges bug where node owns minimum token,CASSANDRA-1901,12494015,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,jbellis,jbellis,12/23/2010 23:44,3/12/2019 14:16,3/13/2019 22:24,12/30/2010 3:38,0.6.9,0.7.0,,,0,,,,,,"From the ML, there are two RF=1 nodes, 0 for the local node (17.224.36.17) and 85070591730234615865843651857942052864 for the remote node (17.224.109.80).  Debug log shows

{code}
DEBUG [pool-1-thread-4] 2010-12-23 12:54:26,958 CassandraServer.java (line 479) range_slice
DEBUG [pool-1-thread-4] 2010-12-23 12:54:26,958 StorageProxy.java (line 412) RangeSliceCommand{keyspace='Harvest', column_family='TestCentroids', super_column=null, predicate=SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 0C 0C 00 01 0B 00 03 00 00 00 0D 54 65 73 74 43 65 6E 74 72 6F 69 64 73 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 0C 0C 00 01 0B 00 03 00 00 00 0D 54 65 73 74 43 65 6E 74 72 6F 69 64 73 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1)), range=[0,0], max_keys=11}
DEBUG [pool-1-thread-4] 2010-12-23 12:54:26,958 StorageProxy.java (line 597) restricted ranges for query [0,0] are [[0,0]]
DEBUG [pool-1-thread-4] 2010-12-23 12:54:26,959 StorageProxy.java (line 423) === endpoint: belize1.apple.com/17.224.36.17 for range.right 0
{code}

Thus, node 85070591730234615865843651857942052864 is left out.",,,,,,,,,,,,,,,,,,,,28/Dec/10 18:19;stuhood;0.6-0001-Switch-minimum-token-for-RP-to-1-for-midpoint-purposes.txt;https://issues.apache.org/jira/secure/attachment/12467053/0.6-0001-Switch-minimum-token-for-RP-to-1-for-midpoint-purposes.txt,28/Dec/10 07:48;stuhood;0001-Switch-minimum-token-for-RP-to-1-for-midpoint-purposes.txt;https://issues.apache.org/jira/secure/attachment/12467031/0001-Switch-minimum-token-for-RP-to-1-for-midpoint-purposes.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,15:00.1,,,no_permission,,,,,,,,,,,,20362,,,Thu Dec 30 03:38:32 UTC 2010,,,,,,0|i0g83z:,92744,jbellis,jbellis,,,,,,,,,23/Dec/10 23:48;jbellis;I'm guessing this is related to the special cases on minimum tokens -- should we just disallow using that in a live node token?,"24/Dec/10 23:53;jbellis;Mike reports,

bq. it looks like the workaround of using an initial token of 1 works","25/Dec/10 06:15;stuhood;The problem is that the minimum token plays double duty by being ""less than all possible tokens"", while also being a valid token for RP (not for OPP). Changing the minimum token for RP to -1 (which is impossible to generate any other way) might help minimize these bugs. I started playing around with this tonight, but need to adjust the midpoint calculation code a little bit. I'd like to avoid disallowing 0, so I'll try to attach a better solution before the end of the weekend.","28/Dec/10 07:48;stuhood;Switches the minimum token for RP to -1. Fixes the case mentioned on the mailing list, but hasn't been subjected to the same scrutiny as the OPPs get in StorageProxyTest: the singletons make this excessively difficult.","28/Dec/10 07:59;stuhood;Patch tested against trunk, but should apply cleanly to 0.6/0.7","28/Dec/10 16:34;jbellis;I get

{noformat}
patching file src/java/org/apache/cassandra/dht/RandomPartitioner.java
...
Hunk #2 FAILED at 67.
{noformat}

against 0.6",28/Dec/10 18:19;stuhood;Rebased for 0.6,"29/Dec/10 23:24;hudson;Integrated in Cassandra-0.6 #43 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/43/])
    change RandomPartitioner mintoken to -1 to avoid collision w/
tokens in the ring
patch by Stu Hood; reviewed by jbellis for CASSANDRA-1901
",30/Dec/10 03:38;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read Repair behavior thwarts DynamicEndpointSnitch at CL.ONE,CASSANDRA-1873,12493477,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,12/17/2010 0:23,3/12/2019 14:16,3/13/2019 22:24,12/21/2010 21:23,0.6.9,0.7.0 rc 3,,,0,,,,,,"When doing a CL.ONE read, the coordinator node selects the data node from the list of replicas via snitch sortByProximity.  The data node (_not_ the coordinator) then sends digest requests to the remaining replicas, and compares their answers to its own (in ConsistencyChecker).

This means that, in a multi-datacenter situation, for any given range R with replicas X in dc1 and Y in dc2, the only node with latency information for Y will be X.  Since DES falls back to subsnitch (static) order when latency information is missing for any replica it is asked to sort, DES will be unable to direct requests to Y no matter how overwhelmed X becomes.

To fix this, we should move the digest-checking code into the coordinator node (probably starting with the 0.7 ConsistencyChecker, which represents a cleanup of the 0.6 one).",,,,,,,,,,,,,,,,,,,,18/Dec/10 17:58;jbellis;1873.txt;https://issues.apache.org/jira/secure/attachment/12466542/1873.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,37:19.8,,,no_permission,,,,,,,,,,,,20353,,,Tue Dec 21 21:23:24 UTC 2010,,,,,,0|i0g7xj:,92715,brandon.williams,brandon.williams,,,,,,,,,"17/Dec/10 00:24;jbellis;Note: IMO it is okay to break RR temporarily when upgrading a cluster piecemeal -- that is, it's okay for RR to not happen; it's not okay to generate internal errors.","18/Dec/10 17:58;jbellis;the only tricky part was getting handles to both a command object and the address of the data read efficiently.  ended up handling the former w/ a Map in StorageProxy.weakRead, and the latter by adding a field to AsyncResult.

RR vs local reads continues to be handled by weakReadCallable.  that part required relatively little change.",18/Dec/10 17:58;jbellis;(patch is against 0.6),"21/Dec/10 18:37;brandon.williams;+1, no internal errors generated during a rolling restart.","21/Dec/10 20:58;hudson;Integrated in Cassandra-0.6 #31 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/31/])
    manage read repair in coordinator instead of data source, to provide latency information to dynamic snitch
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1873
",21/Dec/10 21:23;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal error processing get_range_slices,CASSANDRA-1781,12480961,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,patrik.modesto,patrik.modesto,11/26/2010 9:51,3/12/2019 14:16,3/13/2019 22:24,12/2/2010 1:20,0.6.9,0.7.0 rc 2,,,0,,,,,,"Runnig mapreduce task on two or more Cassandra nodes gives following error:

DEBUG 16:51:48,653 range_slice
DEBUG 16:51:48,653 RangeSliceCommand{keyspace='TEST', column_family='Url', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=57 lim=67 cap=177]]), range=(162950022446285318630909295651345252065,9481098247439719900692337295923514899], max_keys=4096}
DEBUG 16:51:48,653 restricted ranges for query (162950022446285318630909295651345252065,9481098247439719900692337295923514899] are [(162950022446285318630909295651345252065,9481098247439719900692337295923514899]]
DEBUG 16:51:48,653 local range slice
ERROR 16:51:48,653 Internal error processing get_range_slices
java.lang.AssertionError: (162950022446285318630909295651345252065,9481098247439719900692337295923514899]
at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1264)
at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:429)
at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:514)
at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.process(Cassandra.java:2868)
at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
DEBUG 16:51:48,838 logged out: #<User allow_all groups=[]>

You can reproduce this by just running contrib/word_count example. Mapreduce last worked with Cassandra 0.7-beta2. Important is to run more than one node.","Debian Linux 2.6.32-openvz-amd64 x86_64 GNU/Linux, Cloudera hadoop CDH3",,,,,,,,,,,,,,,,,,,02/Dec/10 01:06;stuhood;1781.txt;https://issues.apache.org/jira/secure/attachment/12465094/1781.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,22:44.8,,,no_permission,,,,,,,,,,,,20310,,,Thu Dec 02 02:46:42 UTC 2010,,,,,,0|i0g7cv:,92622,jbellis,jbellis,,,,,,,,,"27/Nov/10 19:22;chrusty;I'm seeing exactly the same behaviour on 0.7-rc1. contrib/word_count (as well as my own map-reduce code based on contrib/word_count) works fine on a single-node, but as soon as i add more nodes to the cluster i get the same errors.

This seems identical to issue #1724 (https://issues.apache.org/jira/browse/CASSANDRA-1724), but that ticket has been ""resolved"".

I can provide logs if they're of use to anybody.


Chris","02/Dec/10 01:06;stuhood;Ack! Posted this to CASSANDRA-1787 yesterday: we had another missing test case. Should be applied to 0.6, 0.7, trunk, etc.","02/Dec/10 01:20;jbellis;committed, thanks!","02/Dec/10 02:46;hudson;Integrated in Cassandra-0.6 #15 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/15/])
    fix range queries against wrapped range
patch by Stu Hood; reviewed by jbellis for CASSANDRA-1781
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
count timeouts towards dynamicsnitch latencies,CASSANDRA-1905,12494126,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,12/27/2010 15:54,3/12/2019 14:16,3/13/2019 22:24,12/28/2010 3:36,0.6.9,0.7.0,,,0,,,,,,receiveTiming is only called by ResponseVerbHandler; we need to add timing information for timed-out requests as well.,,,,,,,,,,,,,,,,,,,,27/Dec/10 21:26;jbellis;1905.txt;https://issues.apache.org/jira/secure/attachment/12467016/1905.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,02:49.1,,,no_permission,,,,,,,,,,,,20365,,,Tue Dec 28 15:00:40 UTC 2010,,,,,,0|i0g84v:,92748,brandon.williams,brandon.williams,,,,,,,,,"27/Dec/10 17:02;hudson;Integrated in Cassandra-0.6 #34 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/34/])
    convert ConsistencyChecker to use an executor as in 0.7 to ease merging of CASSANDRA-1905
patch by jbellis
","27/Dec/10 21:26;jbellis;The idea is simply, ""when a message expires from our Map without a response, count it at the maximum latency.""  ExpiringMap is thus extended with an optional postExpireHook Function.

In MessagingService we add a ""targets"" multimap (since a message can be sent to multiple recipients), and update that with the destinations for any message with a callback.  ResponseVerbHandler removes each destination from ""targets"" when it gets a reply.  We also combine the former ""callbackMap_"" and ""taskCompletionMap_"" into a single ""callbacks"" object to avoid redundancy.  IMessageCallback is introduced to mean ""IASyncCallback or IAsyncResult.""

Also, for consistency, removing objects from ""callbacks"" is defined to be the responsibility of the IMessageCallback, so I've moved that from the AsyncResult section into AR, rather than RVH.  It would be nice to do it in RVH but there is not enough information there, in the case of quorum reads, to know when it's safe to do the remove (RVH is per-message based, and quorum reads by definition span multiple messages).",27/Dec/10 23:34;brandon.williams;+1,"28/Dec/10 15:00;hudson;Integrated in Cassandra-0.6 #38 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/38/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop Integration doesn't work when one node is down,CASSANDRA-1927,12494462,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,michaelsembwever,uctopcu,uctopcu,1/3/2011 1:15,3/12/2019 14:16,3/13/2019 22:24,1/3/2011 19:43,0.7.1,,,,1,,,,,,"using the same directives in the sample code:

When I start the CFInputFormat to read a CF in a keyspace of RF=3 on a 4-node cluster:
- If all the nodes are all up, everything works fine and I don't have any problems walking through the all data in the CF, however
- If there's a node down, the hadoop job does not even start, just dies without any errors or exceptions.

So I'm really sorry for not being able to post any errors or exceptions, though it's really easy to reproduce. Just startup a cluster and take one node down and you're there :)",,,,,,,,,,,,,,,,,,,,03/Jan/11 17:41;michaelsembwever;CASSANDRA-1927.patch;https://issues.apache.org/jira/secure/attachment/12467331/CASSANDRA-1927.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,25:34.5,,,no_permission,,,,,,,,,,,,20373,,,Mon Jan 03 20:23:29 UTC 2011,,,,,,0|i0g89j:,92769,jbellis,jbellis,,,,,,,,,"03/Jan/11 08:25;michaelsembwever;Client side (hadoop job):

java.io.IOException: Could not get input splits
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSplits(ColumnFamilyInputFormat.java:127)
	at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:885)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:779)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:432)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:447)
	at no.finntech.countstats.reduce.FakeAdCounterTableReduce.run(FakeAdCounterTableReduce.java:421)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at no.finntech.countstats.reduce.FakeAdCounterTableReduce.main(FakeAdCounterTableReduce.java:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: unable to connect to server
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSplits(ColumnFamilyInputFormat.java:123)
	... 13 more
Caused by: java.io.IOException: unable to connect to server
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.createConnection(ColumnFamilyInputFormat.java:212)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSubSplits(ColumnFamilyInputFormat.java:187)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.access$200(ColumnFamilyInputFormat.java:74)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat$SplitCallable.call(ColumnFamilyInputFormat.java:160)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat$SplitCallable.call(ColumnFamilyInputFormat.java:145)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
	at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.createConnection(ColumnFamilyInputFormat.java:208)
	... 9 more
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
	at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
	at java.net.Socket.connect(Socket.java:525)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
	... 11 more

","03/Jan/11 08:27;michaelsembwever;There's a todo comment in ColumnFamilyInputFormat
 // TODO handle failure of range replicas & retry

line 198 only tries the first endpoint. a loop on the TException trying the next endpoint is needed.","03/Jan/11 10:45;michaelsembwever;Utku: are you able to test this patch?

( It didn't work for me because RF was never really set to 3. using cassandra-cli ""describe keyspace xxx"" reported ""Replication Factor: 1"" )  :-$

",03/Jan/11 11:00;uctopcu;Mck: Right now I can't access to our compilation server. However I can replace the running binaries and test them if I have the patched rc4. Can you somehow provide me the compiled package?,"03/Jan/11 11:08;michaelsembwever;Sent DM. If it doesn't work you should at minimum see the job's IOException stacktrace change from ""unable to connect to server"" to ""failed connecting to all endpoints"".",03/Jan/11 13:58;uctopcu;I'll be testing it in a few hours. I'll write down the results. something urgent came up.,03/Jan/11 15:53;michaelsembwever;After fixing my local RF problem this patch works for me.,"03/Jan/11 17:25;jbellis;It looks like this patch includes the code from CASSANDRA-1921, which is causing conflicts b/c it's already applied on 0.7 and trunk.  Can you create a patch for 1927 only?",03/Jan/11 17:27;michaelsembwever;Putting Stu as reviewer since he was for CASSANDRA-342 (which the TODO comment in question was added under).,"03/Jan/11 17:34;michaelsembwever;Yeah, the patch had a lot of crap in it. sorry. will re-apply.",03/Jan/11 17:38;michaelsembwever;correct patch & license grant,03/Jan/11 17:41;michaelsembwever;third time lucky. removed unnecessary import.,03/Jan/11 17:54;uctopcu;I've tested against the rc4+patch and it works.,"03/Jan/11 19:43;jbellis;committed, thanks!","03/Jan/11 20:23;hudson;Integrated in Cassandra-0.7 #142 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/142/])
    retry hadoop split requests on connection failure
patch by mck; reviewed by jbellis for CASSANDRA-1927
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted HandOff are not delivered correctly following change to ByteBuffer,CASSANDRA-1672,12478557,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,10/28/2010 9:20,3/12/2019 14:16,3/13/2019 22:24,10/28/2010 10:51,0.7 beta 3,,,,0,,,,,,"In trunk (rc1-snapshot), I get a Fatal Exception during hints delivery, the exception being: 
java.lang.RuntimeException: Corrupted hint name java.nio.HeapByteBuffer[pos=0 lim=22 cap=22]

This is due to a misuse of the 3rd parameter of ArrayUtils.lastIndexOf.",,,,,,,,,,,,,,,,,,,,28/Oct/10 09:21;slebresne;0001-Fix-hinted-handoff.patch;https://issues.apache.org/jira/secure/attachment/12458239/0001-Fix-hinted-handoff.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,51:42.3,,,no_permission,,,,,,,,,,,,20252,,,Thu Oct 28 10:51:42 UTC 2010,,,,,,0|i0g6of:,92512,jbellis,jbellis,,,,,,,,,28/Oct/10 10:51;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyOutputFormat only writes one column (per key),CASSANDRA-1774,12480793,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,michaelsembwever,michaelsembwever,michaelsembwever,11/24/2010 12:17,3/12/2019 14:16,3/13/2019 22:24,11/25/2010 17:24,0.7.0 rc 2,,,,0,,,,,,"From mailing list http://thread.gmane.org/gmane.comp.db.cassandra.user/10385

ColumnFamilyOutputFormat will only write out one column
per key.

Alex Burkoff also reported this nearly two months ago, but nobody ever
replied...
 http://article.gmane.org/gmane.comp.db.cassandra.user/9325

has anyone any ideas? 
should it be possible to write multiple columns out?

This is very easy to reproduce. Use the contrib/wordcount example, with
OUTPUT_REDUCER=cassandra and in WordCount.java add at line 132

>              results.add(getMutation(key, sum));
> +            results.add(getMutation(new Text(""doubled""), sum*2));

Only the last mutation for any key seems to be written.",,,,,,,,,,,,,,,,,,,,25/Nov/10 07:34;michaelsembwever;CASSANDRA-1774.patch;https://issues.apache.org/jira/secure/attachment/12460435/CASSANDRA-1774.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,24:37.5,,,no_permission,,,,,,,,,,,,20306,,,Sat Dec 11 07:35:15 UTC 2010,,,,,,0|i0g7bb:,92615,jbellis,jbellis,,,,,,,,,"25/Nov/10 07:34;michaelsembwever;The problem was the list of mutations inside the Map<ByteBuffer, Map<String, List<Mutation>>> that is sent to batch_mutate(..) isn't appended to, instead it was overridden.

This patch allows subsequent mutations (under the same columnFamily and key) to be appended to the existing list.",25/Nov/10 17:24;jbellis;Thanks for the fix!  Committed w/ minor change to emphasize that subBatch is only created once.,25/Nov/10 23:19;karthick;+1 on the patch.,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The describe_host API method is misleading in that it returns the interface associated with gossip traffic,CASSANDRA-1777,12480888,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,zznate,zznate,11/25/2010 5:06,3/12/2019 14:16,3/13/2019 22:24,8/3/2011 21:46,0.8.4,,,,3,,,,,,"If the hardware is configured to use separate interfaces for thrift and gossip, the gossip interface will be returned, given the results come out of the ReplicationStrategy eventually.

I understand the approach, but given this is on the API, it effective worthless in situations of host auto discovery via describe_ring from a client. I actually see this as the primary use case of this method - why else would I care about the gossip iface from the client perspective? It's current form should be relegated to JMX only. 

At the same time, we should add port information as well. 

describe_splits probably has similar issues.

I see the potential cart-before-horse issues here and that this will probably be non-trivial to fix, but I think ""give me a set of all the hosts to which I can talk"" is pretty important from a client perspective.",,,57600,57600,,0%,57600,57600,,,,,,,,CASSANDRA-3173,CASSANDRA-2882,,,03/Aug/11 20:12;brandon.williams;1777-v2.txt;https://issues.apache.org/jira/secure/attachment/12489239/1777-v2.txt,19/Jan/11 23:06;brandon.williams;1777.txt;https://issues.apache.org/jira/secure/attachment/12468792/1777.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,06:27.8,,,no_permission,,,,,,,,,,,,659,,,Wed Aug 03 22:19:59 UTC 2011,,,,,,0|i0g7bz:,92618,jbellis,jbellis,,,,,,,,,"19/Jan/11 23:06;brandon.williams;This is actually simple enough to apply to 0.7 if we leave out the rpc port (since that would require a thrift api change.)  describe_splits doesn't appear to be affected, just describe_ring.","20/Jan/11 18:57;jbellis;gossip is for internal cluster state, we definitely shouldn't be using it for rpc_address and rpc_port.

90% convinced the answer is ""don't use describe_ring for node discovery, that's what RRDNS is for.""","20/Jan/11 19:20;brandon.williams;You can't build a routing-aware client from RRDNS though because you'll have no way to map the IP to the token.  describe_ring really is useless if the gossip iface isn't the same as the rpc address right now (describe_ring is already using gossip for the info it returns, it's just the wrong info.)","20/Jan/11 23:31;kingryan;Unless you have a dns server that can understand cassandra membership, RRDNS is actually a rough way to do this. I'd prefer to supply something for clients that works correctly.",21/Jan/11 04:26;urandom;I don't see how you could build a non-Java routing-aware client without replicating the partitioner and replica placement client-side. Ick.,21/Jan/11 17:59;kingryan;I don't care about making it routing-aware. I just want to do discovery.,"09/May/11 21:24;nickmbailey;Besides just auto_discovery this breaks the pig storage func in contrib if you use different interfaces. That class uses describe ring to generate the input splits for map tasks to work on, which obviously doesn't work if it returns the gossip interface instead of the thrift interface.

+1 on fixing this and backporting to 0.7","09/May/11 21:29;brandon.williams;Since the problem is rooted in CFIF, this actually runs deeper than just Pig.","03/Aug/11 18:22;jbellis;Brandon, what is the status here?","03/Aug/11 18:27;brandon.williams;describe_ring, as is, is completely useless if you don't have thrift bound to the same interface as the storage proto, so I stand by the change to advertise the thrift address instead.",03/Aug/11 19:07;jbellis;is that what the attached patch does?,03/Aug/11 19:12;brandon.williams;That's what it did 8 months ago :)  It gets the rpc address/port information for each machine via gossip and returns that in describe_ring.,"03/Aug/11 19:22;jbellis;oh yeah.  the whole ""gossip is for internal cluster state, we definitely shouldn't be using it for rpc_address and rpc_port"" thing I objected to originally.

however, I don't have a better solution (asking nodes directly would mean we couldn't include nodes that are currently unreachable), so +1 I guess on the general approach.

if we're not going to expose individual rpc_port can we just require that it be the same cluster-wide and not bother gossiping it?  it's kind of a misfeature anyway to allow different ones.

the old getRangeToEndpointMap is unused and can be removed now right?

Committing to 0.8 is probably fine but let's leave 0.7 alone.","03/Aug/11 19:27;brandon.williams;bq. if we're not going to expose individual rpc_port can we just require that it be the same cluster-wide and not bother gossiping it? it's kind of a misfeature anyway to allow different ones.

I agree, I'll remove the port.

bq. the old getRangeToEndpointMap is unused and can be removed now right?

I think I originally left it in because it's exposed over JMX and I didn't want to break it if anyone was using it for something.","03/Aug/11 20:12;brandon.williams;v2 only communicate the rpc address over gossip, and fixes a problem in v1 when the rpc address is left blank.","03/Aug/11 21:08;jbellis;+1, but let's remove getRangeToEndpointMap when you merge to trunk.",03/Aug/11 21:46;brandon.williams;Committed.,"03/Aug/11 22:19;hudson;Integrated in Cassandra-0.8 #254 (See [https://builds.apache.org/job/Cassandra-0.8/254/])
    describe_ring returns the interface thrift is bound to instead of the
one the storage proto is bound to.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1777

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1153683
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/VersionedValue.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/ApplicationState.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix regression in 1968 (young gen sizing logic),CASSANDRA-2023,12496207,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,1/21/2011 0:57,3/12/2019 14:16,3/13/2019 22:24,1/24/2011 19:40,0.7.1,,Packaging,,0,,,,,,"1968 introduced a regression (there was still cleanup to do). In particular it broke when an explicit MAX_HEAP_SIZE was set. Attaching *draft* patch (needs more testing).

Allowing automatic newsize calculation in the face of a manually specified MAX_HEAP_SIZE was problematic. Either one has to duplicate JVM parsing of MAX_HEAP_SIZE or ask the user to set MAX_HEAP_SIZE_IN_MB (or similar) instead.

In this patch (consider it a draft) i opted for the latter + picking up MAX_HEAP_SIZE for backwards compatibility (but with the effect that it disables new size calculation). I tried to make it slightly more posixly correct, but as usual no guarantees given that I have no posix shell to test it on.

I'm not really happy about the shell acrobatics and my confidence that there is not some left-over issue is not high. Should we just not worry about MAX_HEAP_SIZE compatibility and remove all that compatibility cruft? Plenty of acrobatics left still, but it would remove the more hideous parts.",,,,,,,,,,,,,,,,,,,,21/Jan/11 08:24;scode;2023-v2.txt;https://issues.apache.org/jira/secure/attachment/12468957/2023-v2.txt,21/Jan/11 00:57;scode;2023.txt;https://issues.apache.org/jira/secure/attachment/12468935/2023.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,56:02.3,,,no_permission,,,,,,,,,,,,20404,,,Mon Jan 24 20:31:50 UTC 2011,,,,,,0|i0g8un:,92864,urandom,urandom,,,,,,,,,"21/Jan/11 03:56;jbellis;bq. automatic newsize calculation in the face of a manually specified MAX_HEAP_SIZE was problematic. Either one has to duplicate JVM parsing of MAX_HEAP_SIZE or ask the user to set MAX_HEAP_SIZE_IN_MB (or similar) instead.

I would rather say ""if you manually specify MAX_HEAP_SIZE, you must also specify HEAP_NEWSIZE.""","21/Jan/11 08:24;scode;v2 is significantly simpler and cleaner (IMO). Tested on MacOS and Linux.

FWIW, the reason I didn't do ""must set both"" initially is that I felt the new size was a lot more obscure/advanced than merely setting heap size. v2 tries to mitigate this by documenting what to do if unsure/don't care.

","24/Jan/11 19:40;urandom;Please avoid making unrelated changes to whitespace/styling as it makes review more difficult.  Otherwise, it looks good.

Committed.","24/Jan/11 19:59;hudson;Integrated in Cassandra-0.7 #199 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/199/])
    fix young gen sizing logic

Patch by Peter Shuller (w/ minor changes); review by eevans for CASSANDRA-2023
","24/Jan/11 20:31;hudson;Integrated in Cassandra #687 (See [https://hudson.apache.org/hudson/job/Cassandra/687/])
    fix young gen sizing logic

Patch by Peter Shuller (w/ minor changes); review by eevans for CASSANDRA-2023
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't delete whole row from Hadoop MapReduce,CASSANDRA-2014,12496104,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,patrik.modesto,patrik.modesto,patrik.modesto,1/20/2011 8:55,3/12/2019 14:16,3/13/2019 22:24,2/17/2011 15:09,0.7.3,,,,0,,,,,,"ColumnFamilyRecordWriter.java doesn't support Mutation with Deletion without slice_predicat and super_column to delete whole row. The other way I tried is to specify SlicePredicate with empty start and finish and I got:

{code}
java.io.IOException: InvalidRequestException(why:Deletion does not yet support SliceRange predicates.)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter$RangeClient.run(ColumnFamilyRecordWriter.java:355)
{code}

I tryied to patch the ColumnFamilyRecordWriter.java like this:
{code}
--- a/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java
+++ b/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java
@@ -166,10 +166,17 @@ implements org.apache.hadoop.mapred.RecordWriter<ByteBuffer,List<org.apache.cass
             // deletion
             Deletion deletion = new Deletion(amut.deletion.timestamp);
             mutation.setDeletion(deletion);
+
             org.apache.cassandra.avro.SlicePredicate apred = amut.deletion.predicate;
-            if (amut.deletion.super_column != null)
+            if (apred == null && amut.deletion.super_column == null)
+            {
+                // epmty; delete whole row
+            }
+            else if (amut.deletion.super_column != null)
+            {
                 // super column
                 deletion.setSuper_column(copy(amut.deletion.super_column));
+            }
             else if (apred.column_names != null)
             {
                 // column names
{code}

but that didn't work as well.",Debian Linux 2.6.32 amd64,,,,,,,,,,,,,,,,,,,25/Jan/11 07:49;patrik.modesto;2014-mr-delete-whole-row.patch;https://issues.apache.org/jira/secure/attachment/12469256/2014-mr-delete-whole-row.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,09:07.9,,,no_permission,,,,,,,,,,,,20399,,,Thu Feb 17 15:34:31 UTC 2011,,,,,,0|i0g8sf:,92854,jbellis,jbellis,,,,,,,,,"25/Jan/11 07:49;patrik.modesto;The patch actually worked, I just did misinterpret the data.","17/Feb/11 08:51;patrik.modesto;Hi, can the fix be merged please?","17/Feb/11 15:09;jbellis;committed, thanks!","17/Feb/11 15:34;hudson;Integrated in Cassandra-0.7 #284 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/284/])
    Handle whole-row deletions in CFOutputFormat
patch by Patrik Modesto; reviewed by jbellis for CASSANDRA-2014
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java doesn't actually read its data,CASSANDRA-1915,12494256,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,brandon.williams,brandon.williams,12/29/2010 16:58,3/12/2019 14:16,3/13/2019 22:24,12/29/2010 19:38,0.7.1,,,,0,,,,,,"stress.java doesn't actually read back the keys it inserts, but also reports no errors.  This is evident on larger (1M) runs where the read request rate is equal to what the bloom filter can do.  Stress.py also cannot find the rows that stress.java inserts.",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,29/Dec/10 19:27;xedin;CASSANDRA-1915.patch;https://issues.apache.org/jira/secure/attachment/12467131/CASSANDRA-1915.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:56.6,,,no_permission,,,,,,,,,,,,20368,,,Wed Dec 29 20:21:56 UTC 2010,,,,,,0|i0g873:,92758,brandon.williams,brandon.williams,,,,,,,,,"29/Dec/10 19:38;brandon.williams;Committed, thanks!","29/Dec/10 20:21;hudson;Integrated in Cassandra-0.7 #133 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/133/])
    Fix for stress.java using wrong key names and not detecting empty keys.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-1915
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ArrayIndexOutOfBoundsException while executing repair on a freshly added node (0.7.0),CASSANDRA-1959,12495158,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,tbritz,tbritz,1/10/2011 19:03,3/12/2019 14:16,3/13/2019 22:24,1/11/2011 23:41,0.6.10,0.7.1,,,0,,,,,,"Hi,

I added a node to the cluster (20 nodes in total) and ran repair on it after a while.

The repair still runs, but there are errors in the log file (see below).

Some of the data in the clsuter has been filled with rc-4. The cluster runs on version 0.7.0 (the release linked on the main cassandra web site).

Any ideas what might cause this?
(PS. 0.7.0 turns up as unreleased version in Affects Version/s:) 


INFO [CompactionExecutor:1] 2011-01-10 19:55:20,684 SSTableReader.java (line 158) Opening /hd2/cassandra_md5/data/table_x/table_x-e-4
 INFO [CompactionExecutor:1] 2011-01-10 19:55:20,775 SSTableReader.java (line 158) Opening /hd2/cassandra_md5/data/table_x/table_x_meta-e-14
ERROR [RequestResponseStage:3] 2011-01-10 19:55:20,856 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.fastRemove(ArrayList.java:441)
        at java.util.ArrayList.remove(ArrayList.java:424)
        at com.google.common.collect.AbstractMultimap.remove(AbstractMultimap.java:219)
        at com.google.common.collect.ArrayListMultimap.remove(ArrayListMultimap.java:60)
        at org.apache.cassandra.net.MessagingService.responseReceivedFrom(MessagingService.java:436)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:40)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [RequestResponseStage:3] 2011-01-10 19:55:20,856 AbstractCassandraDaemon.java (line 91) Fatal exception in thread Thread[RequestResponseStage:3,5,main]
java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.fastRemove(ArrayList.java:441)
        at java.util.ArrayList.remove(ArrayList.java:424)
        at com.google.common.collect.AbstractMultimap.remove(AbstractMultimap.java:219)
        at com.google.common.collect.ArrayListMultimap.remove(ArrayListMultimap.java:60)
        at org.apache.cassandra.net.MessagingService.responseReceivedFrom(MessagingService.java:436)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:40)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,11/Jan/11 17:24;jbellis;1959-v2.txt;https://issues.apache.org/jira/secure/attachment/12468027/1959-v2.txt,11/Jan/11 21:03;messi;post-1959-v2-cleanup.patch.txt;https://issues.apache.org/jira/secure/attachment/12468057/post-1959-v2-cleanup.patch.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,21:54.8,,,no_permission,,,,,,,,,,,,20382,,,Wed Jan 12 00:38:08 UTC 2011,,,,,,0|i0g8g7:,92799,messi,messi,,,,,,,,,11/Jan/11 08:21;messi;Looks like MessagingService.targets needs to be synchronized.,"11/Jan/11 16:46;nickmbailey;Looks like this was caused by CASSANDRA-1905. Or maybe just exposed by it.

I think this may be particularly nasty due to the fact that droped responses will lead to inflated timings reported to the DynamicEndpointSnitch.  I'm not sure but I think this may be contributing to flapping in another case where I've seen this error.","11/Jan/11 17:24;jbellis;Right, this is from 1905 and there's definitely a concurrency bug here.

I don't want to use the synchronized multimap option though since *every* message hits this so we need better concurrency.  v2 is a patch that replaces multimap with a manually constructed Map of Sets.",11/Jan/11 21:03;messi;I saw that you added putTarget() as instance method although targets itself is a static field. This patch cleans up these inconsistencies and makes MessagingService a true singleton.,"12/Jan/11 00:37;hudson;Integrated in Cassandra-0.7 #152 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/152/])
    convert MessagingService into a true singleton
patch by Folke Behrens; reviewed by jbellis for CASSANDRA-1959
fix race condition in MessagingService.targets
patch by jbellis; reviewed by Folke Behrens for CASSANDRA-1959
","12/Jan/11 00:38;hudson;Integrated in Cassandra-0.6 #49 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/49/])
    backport CASSANDRA-1959 from 0.7
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal error processing insert java.lang.AssertionError  at org.apache.cassandra.service.StorageProxy.sendMessages(StorageProxy.java:219),CASSANDRA-1931,12494562,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,kmueller,kmueller,1/4/2011 5:29,3/12/2019 14:16,3/13/2019 22:24,1/4/2011 21:34,0.7.1,,,,0,,,,,,"ERROR [pool-1-thread-137] 2011-01-03 18:22:21,751 Cassandra.java (line 2960) Internal error processing insert
java.lang.AssertionError
        at org.apache.cassandra.service.StorageProxy.sendMessages(StorageProxy.java:219)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:174)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:412)
        at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:349)
        at org.apache.cassandra.thrift.Cassandra$Processor$insert.process(Cassandra.java:2952)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)",Linux Fedora 12 x86_64,,14400,14400,,0%,14400,14400,,,,,,,,CASSANDRA-1986,,,,04/Jan/11 16:49;tjake;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-1931-change-sendMessages-to-handle-many-mess.txt;https://issues.apache.org/jira/secure/attachment/12467437/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-1931-change-sendMessages-to-handle-many-mess.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,33:10.2,,,no_permission,,,,,,,,,,,,20374,,,Tue Jan 04 21:54:50 UTC 2011,,,,,,0|i0g8af:,92773,jbellis,jbellis,,,,,,,,,04/Jan/11 05:33;jbellis;Looks like this was introduced by CASSANDRA-1530.,"04/Jan/11 16:53;tjake;sendMessages was incorrectly assuming one message would be sent to a DC at a time, which isn't the case for batch mutations.

This patch groups the enpoints to a message so many messages can be sent.",04/Jan/11 21:34;jbellis;committed,"04/Jan/11 21:54;hudson;Integrated in Cassandra-0.7 #144 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/144/])
    fix batch mutations post-#1530
patch by tjake; reviewed by jbellis for CASSANDRA-1931
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"On the CLI, creating or updating a keyspace to use the NetworkTopologyStrategy breaks ""show keyspaces;""",CASSANDRA-2049,12496687,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,jeromatron,jeromatron,1/25/2011 15:23,3/12/2019 14:16,3/13/2019 22:24,1/27/2011 15:21,0.7.1,,,,0,,,,,,"To reproduce:
- Start fresh.
- Run ""show keyspaces;""
- Run ""create keyspace Keyspace1 with placement_strategy='org.apache.cassandra.locator.NetworkTopologyStrategy';""
- Run ""show keyspaces;""

Note how before it showed the system keyspace.  After it shows just:
Keyspace: Keyspace1:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
null

If you have multiple keyspaces, it will hide those as well.  Also, if you create the keyspace and then update it with NetworkTopologyStrategy, the same thing will happen.",,;26/Jan/11 22:02;xedin;1200,1200,0,1200,100%,1200,0,1200,,,,,,,,,,,26/Jan/11 22:03;xedin;CASSANDRA-2049.patch;https://issues.apache.org/jira/secure/attachment/12469486/CASSANDRA-2049.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:50.9,,,no_permission,,,,,,,,,,,,20413,,,Thu Jan 27 15:44:42 UTC 2011,,,,,,0|i0g90f:,92890,jbellis,jbellis,,,,,,,,,27/Jan/11 15:21;jbellis;committed,"27/Jan/11 15:44;hudson;Integrated in Cassandra-0.7 #217 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/217/])
    fix CLI ""show keyspaces"" with null options on NTS
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2049
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI should loop on describe_schema until agreement or fatel exit with stacktrace/message if no agreement after X seconds,CASSANDRA-2044,12496600,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,mdennis,mdennis,1/24/2011 20:36,3/12/2019 14:16,3/13/2019 22:24,1/25/2011 19:57,0.7.1,,,,0,,,,,,"see CASSANDRA-2026 for brief background.

It's easy to enter statements into the CLI before the schema has settled, often causing problems where it is no longer possible to get the nodes in agreement about the schema without removing the system directory.

The alleviate the most common problems with this, the CLI should issue the modification statement and loop on describe_schema until all nodes agree or until X seconds has passed.  If the timeout has been exceeded, the CLI should exit with an error and inform the user that the schema has not settled and further migrations are ill-advised until it does.

number_of_nodes/2+1 seconds seems like a decent wait time for schema migrations to start with.

Bonus points for making the value configurable.",,;25/Jan/11 20:14;xedin;7200,14400,0,7200,50%,14400,0,7200,,,,,,,,,,,25/Jan/11 01:01;xedin;CASSANDRA-2044.patch;https://issues.apache.org/jira/secure/attachment/12469224/CASSANDRA-2044.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,01:54.2,,,no_permission,,,,,,,,,,,,20411,,,Tue Jan 25 20:14:33 UTC 2011,,,,,,0|i0g8zb:,92885,jbellis,jbellis,,,,,,,,,"25/Jan/11 01:01;xedin;I have added option ""--schema-mwt"" to set wait time for schema migration in seconds from command line, if it was not set system will use node_count/2 + 1 secs or 5 secs if couldn't connect to JMX.

CLI checks node agreement on every keyspace create/drop and if nodes were not able to agree in given wait time system will print error message ""The schema has not settled in %d seconds and further migrations are ill-advised until it does."" and exit with -1 code.","25/Jan/11 19:57;jbellis;committed with a couple changes:

- default wait changed to flat 10s; schema propagation does not use gossip and is not expected to be proportional to number of nodes in cluster
- added validation to keyspace update and cf create/update/drop","25/Jan/11 20:14;hudson;Integrated in Cassandra-0.7 #209 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/209/])
    CLI attemptsto block for new schemato propagate
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2044
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix misuses of ByteBufferUtil.string(),CASSANDRA-1999,12495809,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,1/17/2011 17:48,3/12/2019 14:16,3/13/2019 22:24,1/17/2011 18:42,0.7.1,0.8 beta 1,,,0,,,,,,"ByteBufferUtil.string() takes a start offset and a length. It is however used as if taking
a start and end offset.",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,17/Jan/11 18:13;slebresne;0001-Fix-misuse-of-ByteBufferUtil.string.patch;https://issues.apache.org/jira/secure/attachment/12468582/0001-Fix-misuse-of-ByteBufferUtil.string.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,42:34.3,,,no_permission,,,,,,,,,,,,20395,,,Mon Jan 17 19:48:04 UTC 2011,,,,,,0|i0g8p3:,92839,jbellis,jbellis,,,,,,,,,17/Jan/11 17:49;slebresne;Attached patch against 0.7,17/Jan/11 18:13;slebresne;Fixed a tiny mistake. Should be good now.,"17/Jan/11 18:42;jbellis;committed.  (looks like these were introduced in CASSANDRA-1714, fwiw, so i'm going to tag affects-version to 0.7.1)","17/Jan/11 19:48;hudson;Integrated in Cassandra-0.7 #168 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/168/])
    fix ByteBuffer regressions from #1714
patch by slebresne; reviewed by jbellis for CASSANDRA-1999
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
saved row cache doesn't save the cache,CASSANDRA-2102,12497520,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mdennis,mdennis,mdennis,2/2/2011 22:42,3/12/2019 14:16,3/13/2019 22:24,2/10/2011 17:09,0.7.1,,,,0,,,,,,"saving row caches works by periodically iterating of the keySet() on the caches and writing the keys for the cached contents to disk.  The cache keys are DecoratedKeys.  DecoratedKeys contain a Token token and a ByteBuffer key.  The underlying buffer on the key gets reused so the contents change.  This means that all the cache entries have distinct tokens but only a handful of distinct key values.  This means that when the cache is loaded you only end up loading a handful of keys instead of the ones actually in your cache.

",,,240,240,,0%,240,240,,,,,,,,,,,,03/Feb/11 18:41;mdennis;2102-cassandra-0.7-v2.txt;https://issues.apache.org/jira/secure/attachment/12470173/2102-cassandra-0.7-v2.txt,02/Feb/11 22:51;mdennis;2102-cassandra-0.7.txt;https://issues.apache.org/jira/secure/attachment/12470082/2102-cassandra-0.7.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,09:35.2,,,no_permission,,,,,,,,,,,,20444,,,Thu Feb 10 22:11:36 UTC 2011,,,,,,0|i0g9br:,92941,jbellis,jbellis,,,,,,,,,"02/Feb/11 22:51;mdennis;attached patch switches to saving tokens to disk instead of keys, removes DecoratedKey.key from the entries in caches (fixing the bug and reducing memory consumption) and versions (kind of) the saved cache files.","02/Feb/11 23:09;jbellis;bq. The underlying buffer on the key gets reused

Where does this reuse happen?  RowMutation.deepCopy should be taking care of it on the local side, and sent-over-the-network buffers are not reused.",03/Feb/11 16:45;jbellis;duplicate of CASSANDRA-2076,"03/Feb/11 18:39;mdennis;not really a duplicate.

This deals with writing the cache correctly.

CASSANDRA-2076 should better handle in general when a cache file is corrupt.","03/Feb/11 18:41;mdennis;per IRC discussion v2 patch just copies the key when inserting into the cache.

Since CASSANDRA-1034 looks likely in the somewhat near future, it's pointless to make the changes in the original patch.",03/Feb/11 20:16;jbellis;Let's clone in storageproxy since it's not necessary for keys read over MessagingService,"09/Feb/11 00:01;mdennis;I really think safety is the way to go here (e.g. clone right before we put it into the cache).  Not to mention cloning early would be generating memcopies for things we don't need to if they never actually end up in the cache (e.g. things already in the cache, rejected by bloom filters).","10/Feb/11 16:56;jbellis;I think you're right, if your cache is doing its job (high hit rate) then copy-before-insert will be less copies than copy-for-local-operation.","10/Feb/11 17:09;jbellis;committed.  (even without TFFT enabled, it's still a good idea for the same reason as CASSANDRA-1801","10/Feb/11 22:11;hudson;Integrated in Cassandra-0.7 #276 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/276/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support deletes in counters,CASSANDRA-2101,12497514,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,kelvin,kelvin,kelvin,2/2/2011 22:05,3/12/2019 14:16,3/13/2019 22:24,2/5/2011 5:13,0.8 beta 1,,,,0,,,,,,Obey timestampOfLastDelete during reconciliation.,,,,,,,,,,,,,,,,,,CASSANDRA-1072,CASSANDRA-1936,03/Feb/11 20:55;kelvin;0001-CASSANDRA-2101-obey-tsOfLastDelete-remove-irrelevant.patch;https://issues.apache.org/jira/secure/attachment/12470183/0001-CASSANDRA-2101-obey-tsOfLastDelete-remove-irrelevant.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,08:02.9,,,no_permission,,,,,,,,,,,,20443,,,Sat Feb 05 06:42:05 UTC 2011,,,,,,0|i0g9bj:,92940,slebresne,slebresne,,,,,,,,,02/Feb/11 22:06;kelvin;timestampOfLastDelete was added and 90% implemented.  Missed this last logic.,02/Feb/11 22:07;kelvin;add logic to reconcile; test logic.,02/Feb/11 22:22;kelvin;modified patch w/o dependencies on expiring counter column code.,"03/Feb/11 13:08;slebresne;Before commenting on the patch itself, I want to use this ticket to recall that counter deletes are intrinsically broken. It has been said already but I'll use this comment to explain in more depth how so and keep a trace of this for the record.

First, I'll use the following notation:
{noformat}
  c(x, 3)@[4, 2] - for a counter column of name x, value 3, timestamp 4 and timestampOfLastDelete 2 (I'll use -1 as the min timestampOfLastDelete).
{noformat}
and
{noformat}
  d(x)@[5] - for a tombstone of name x and timestamp 5
{noformat}

And now suppose that the following inserts are done (in that order):
{noformat}
   c(x, 1)@[1, -1]
   d(x)@[2]
   c(x, 1)@[3, -1]
{noformat}

If these inserts are resolved in that order, everything is fine:
{noformat}
   c(x, 1)@[1, -1]
 + d(x)@[2]
=> d(x)@[2]
 + c(x, 1)@[3, -1]
=> c(x, 1)@[3, 2]
{noformat}

However, some reordering don't work. Namely, if you merge the two counts together, before you merge one of the count with the delete:
{noformat}
   c(x, 1)@[1, -1]
 + c(x, 1)@[3, -1]
=> c(x, 2)@[3, -1]
 + d(x)@[2]
=> c(x, 2)@[3, 2]
{noformat}

The problem is, the resolve operation is not commutative when you consider counter columns and tombstones. But Cassandra rely heavily on resolve being commutative (as a side note, I never understood the reason of the CommutativeType terminology in the code. It suggest that regular columns are not commutative, while they are as far as resolve is concerned. Resolve is not idempotent on counters however).

Not only is there no guarantee on which order the insert will be received by each node, but even if they are in the right order, there is no guarantee that (minor) compaction won't screw up this.

Hence I think that there is not much guarantee we can give on deletes. The only one I can think of is that when on issue a delete, you must wait to issue any following update that the delete have reach all the nodes and all of them have been fully compacted.

That being said, we can keep counter deletes. It's at least useful for cases where you know that you won't reuse a counter ever and want to get rid of the disk space. But I would add a very strong warning to its documentation.

Lastly, the deletion of full counter rows or super columns suffers the same problem for the same reason.
","03/Feb/11 13:08;slebresne;+1 on the patch in the spirit of ""let's make delete work as often as we can"".

However, I realized that the first 'if' of CounterColumn.reconcile() is dead-code since a CounterColumn is never markedForDelete(). It would be nice to remove it while submitting this
","03/Feb/11 20:55;kelvin;Good point, Sylvain!

Attached a new patch w/ the irrelevant code removed.",04/Feb/11 12:10;slebresne;So +1 on the new patch,05/Feb/11 05:13;jbellis;committed,"05/Feb/11 06:42;hudson;Integrated in Cassandra #709 (See [https://hudson.apache.org/hudson/job/Cassandra/709/])
    add delete support for counters
patch by Kelvin Kakugawa; reviewed by slebresne for CASSANDRA-2101
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read repair causes tremendous GC pressure,CASSANDRA-2069,12496983,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,1/27/2011 21:14,3/12/2019 14:16,3/13/2019 22:24,2/15/2011 17:15,0.7.3,,,,0,,,,,,"To reproduce: start a three node cluster, insert 1M rows with stress.java and rf=2.  Take one down, delete its data, then bring it back up and issue 1M reads against it.  After the run is done you will see at least 1 STW long enough to mark the node as dead, often 4 or 5.",,,57600,57600,,0%,57600,57600,,,,,,,,,,,,14/Feb/11 22:44;jbellis;2069-v10.txt;https://issues.apache.org/jira/secure/attachment/12471030/2069-v10.txt,04/Feb/11 06:04;jbellis;2069-v2.txt;https://issues.apache.org/jira/secure/attachment/12470216/2069-v2.txt,04/Feb/11 15:38;jbellis;2069-v3.txt;https://issues.apache.org/jira/secure/attachment/12470239/2069-v3.txt,04/Feb/11 19:37;jbellis;2069-v4.txt;https://issues.apache.org/jira/secure/attachment/12470271/2069-v4.txt,04/Feb/11 21:22;jbellis;2069-v5.txt;https://issues.apache.org/jira/secure/attachment/12470280/2069-v5.txt,07/Feb/11 17:09;jbellis;2069-v6.txt;https://issues.apache.org/jira/secure/attachment/12470472/2069-v6.txt,07/Feb/11 23:48;jbellis;2069-v7.txt;https://issues.apache.org/jira/secure/attachment/12470525/2069-v7.txt,08/Feb/11 05:02;jbellis;2069-v8.txt;https://issues.apache.org/jira/secure/attachment/12470544/2069-v8.txt,08/Feb/11 19:11;jbellis;2069-v9.txt;https://issues.apache.org/jira/secure/attachment/12470615/2069-v9.txt,04/Feb/11 02:14;jbellis;2069.txt;https://issues.apache.org/jira/secure/attachment/12470212/2069.txt,,,,10,,,,,,,,,,,,,,,,,,,08:11.5,,,no_permission,,,,,,,,,,,,20425,,,Tue Feb 15 21:43:50 UTC 2011,,,,,,0|i0g94v:,92910,brandon.williams,brandon.williams,,,,,,,,,27/Jan/11 22:08;jbellis;is this also true for 0.6?,"27/Jan/11 22:31;brandon.williams;No, only 0.7",28/Jan/11 16:16;jbellis;In the log snippets I saw the heap was legitimately nearly-full during the repair process.  What does MAT show is using all the memory?,03/Feb/11 22:35;brandon.williams;MAT indicates there are millions of pending futuretasks in StorageProxy.repairExecutor.,"04/Feb/11 00:05;brandon.williams;Bisecting this indicates two problems.  First, the tremendous GC pressure doesn't exhibit until we increase the newgen size.  If that is ignored and we only observe whether stress.java moves forward (it has the 'keep trying' rather than 'keep going' behavior) then the culprit is r1053245.

Both cases show that RR requests can grow unbounded, which is probably ok, but if it's the same request over and over we still queue it as many times as it's requested.","04/Feb/11 00:44;mdennis;I've observed similar issues with large a large newgen, largish memtables and writing at RF>1 at CL.Q/CL.ALL though it's not as immediate as it is with RR.

We should look into a way at bounding the number of outstanding requests (RR or ""send to replica""), or more aggressively timing out the older requests once we hit some configurable threshold of outstanding requests.
","04/Feb/11 02:14;jbellis;Attached patch does two things:

- allows the repair executor to use threads = # of cores
- stops sending repair requests when the executor queue gets too large.  (""dropped"" requests will be logged by MessagingService along with the other overload-scenario dropping.)",04/Feb/11 02:36;jbellis;Working on another approach that should let us throw away repair handlers in the expected case that everyone responds quickly.  This will be kinder to new gen gc since we won't have nearly as many survivors.,"04/Feb/11 06:04;jbellis;v2 adds a READ_REPAIR stage and	does resolve of digests that were not checked for the client result on that stage as soon as response() collects all the replies.	If there is a mismatch and we do a re-read of full	resultset, we also check those results on the RR stage based on response() (in AsyncRepairRunner, now in ReadCallback.)

I preserved the	feature	from v1	of not doing repairs if	the RR stage is	full.

Most of the code changes are about getting the right information into ReadCallback (e.g. endpoints) and some ceremony to make static typing happy (IReadCallback).

A side benefit is that StorageProxy.fetchRows is significantly cleaner (no more commandEndpoints or  repairs collections).
","04/Feb/11 15:38;jbellis;bq. I preserved the feature from v1 of not doing repairs if the RR stage is full.

Removed this for v3. It's complexity we don't need to solve a non-problem, when we're not hanging on to each callback until RPC_TIMEOUT (as demonstrated by 0.6).",04/Feb/11 19:37;jbellis;v4 fixes build,04/Feb/11 21:22;jbellis;v5 fixes a confusion of endpoints and handler.endpoints,"04/Feb/11 23:20;brandon.williams;No GC pressure now, however RR does not appear to actually occur.",05/Feb/11 01:20;jbellis;what does debug log show?,"05/Feb/11 01:27;brandon.williams;Nothing interesting, afaict.  Just a lot of this:
{noformat}
DEBUG 23:19:42,094 get_slice
DEBUG 23:19:42,094 get_slice
DEBUG 23:19:42,240 ReadCallback blocking for 1 responses
DEBUG 23:19:42,240 ReadCallback blocking for 1 responses
DEBUG 23:19:42,093 ReadCallback blocking for 1 responses
DEBUG 23:19:42,092 reading data for SliceFromReadCommand(table='Keyspace1', key='30343939323034', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,091 Read: 21 ms.
DEBUG 23:19:42,091 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30353032333935', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,091 Read: 17 ms.
DEBUG 23:19:42,241 Read: 159 ms.
DEBUG 23:19:42,091 Read: 12 ms.
DEBUG 23:19:42,091 reading data for SliceFromReadCommand(table='Keyspace1', key='30333636313035', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,091 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30343831383238', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,091 get_slice
DEBUG 23:19:42,242 Read: 161 ms.
DEBUG 23:19:42,242 ReadCallback blocking for 1 responses
DEBUG 23:19:42,091 get_slice
DEBUG 23:19:42,090 Read: 17 ms.
DEBUG 23:19:42,243 ReadCallback blocking for 1 responses
DEBUG 23:19:42,090 Read: 19 ms.
DEBUG 23:19:42,090 reading data for SliceFromReadCommand(table='Keyspace1', key='30353131363631', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,089 ReadCallback blocking for 1 responses
DEBUG 23:19:42,243 reading data for SliceFromReadCommand(table='Keyspace1', key='30353532343833', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,244 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30353532343833', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,244 Read: 1 ms.
DEBUG 23:19:42,243 reading data for SliceFromReadCommand(table='Keyspace1', key='30363137363033', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,242 reading data for SliceFromReadCommand(table='Keyspace1', key='30363234303332', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,242 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30333636313035', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,241 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30343939323034', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,245 Read: 3 ms.
{noformat}",07/Feb/11 17:09;jbellis;v6 adds the new repair callbacks to DES latency tracking.,07/Feb/11 23:48;jbellis;v7 rebases post-1530.,"08/Feb/11 05:02;jbellis;bq. RR does not appear to actually occur

No digest reads are being sent but I don't know why.  v8 adds better debug logging around ReadCallback.endpoints creation which governs that.",08/Feb/11 19:11;jbellis;v9 fixes ReadCallback construction and adds more asserts around AsyncRepairRunner.,10/Feb/11 20:35;jbellis;v10 fixes a race where a quick repair would remove the data needed for the response to client.  it also splits repair and digest-processing resolvers into different classes.,14/Feb/11 22:44;jbellis;rebased v10,"14/Feb/11 23:45;brandon.williams;+1, RR works, GC pressure is gone.",15/Feb/11 17:15;jbellis;committed,"15/Feb/11 21:43;hudson;Integrated in Cassandra-0.7 #280 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/280/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
refactor o.a.c.utils.UUIDGen to allow creating type 1 UUIDs for a given time,CASSANDRA-2067,12496972,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,1/27/2011 19:16,3/12/2019 14:16,3/13/2019 22:24,1/29/2011 17:27,0.8 beta 1,,,,0,,,,,,"CASSANDRA-2027 creates the need to generate type 1 UUIDs using arbitrary date/times.  IMO, this would be a good opportunity to replace o.a.c.utils.UUIDGen with the class that Gary Dusbabek wrote for Flewton (https://github.com/flewton/flewton/blob/master/src/com/rackspace/flewton/util/UUIDGen.java), which is better/more comprehensive.  We can even eliminate the dependency on JUG.

Patches to follow.",,,0,0,,0%,0,0,,,,,,,,,,,,27/Jan/11 19:27;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2067-o.a.c.utils.UUIDGen-adapted-from-flewto.txt;https://issues.apache.org/jira/secure/attachment/12469581/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2067-o.a.c.utils.UUIDGen-adapted-from-flewto.txt,27/Jan/11 19:27;urandom;ASF.LICENSE.NOT.GRANTED--v1-0002-eliminate-usage-of-JUG-for-UUIDs.txt;https://issues.apache.org/jira/secure/attachment/12469582/ASF.LICENSE.NOT.GRANTED--v1-0002-eliminate-usage-of-JUG-for-UUIDs.txt,27/Jan/11 19:27;urandom;ASF.LICENSE.NOT.GRANTED--v1-0003-remove-JUG-jar-and-references.txt;https://issues.apache.org/jira/secure/attachment/12469583/ASF.LICENSE.NOT.GRANTED--v1-0003-remove-JUG-jar-and-references.txt,29/Jan/11 05:05;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2067-o.a.c.utils.UUIDGen-adapted-from-flewto.txt;https://issues.apache.org/jira/secure/attachment/12469732/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2067-o.a.c.utils.UUIDGen-adapted-from-flewto.txt,29/Jan/11 05:05;urandom;ASF.LICENSE.NOT.GRANTED--v2-0002-eliminate-usage-of-JUG-for-UUIDs.txt;https://issues.apache.org/jira/secure/attachment/12469733/ASF.LICENSE.NOT.GRANTED--v2-0002-eliminate-usage-of-JUG-for-UUIDs.txt,29/Jan/11 05:05;urandom;ASF.LICENSE.NOT.GRANTED--v2-0003-remove-JUG-jar-and-license-files.txt;https://issues.apache.org/jira/secure/attachment/12469734/ASF.LICENSE.NOT.GRANTED--v2-0003-remove-JUG-jar-and-license-files.txt,,,,,,,,6,,,,,,,,,,,,,,,,,,,04:02.8,,,no_permission,,,,,,,,,,,,20424,,,Mon Jan 31 22:20:48 UTC 2011,,,,,,0|i0g94f:,92908,jbellis,jbellis,,,,,,,,,27/Jan/11 19:23;urandom;The patches have followed.,"28/Jan/11 16:04;jbellis;- message digest instantiation is expensive and synchronized, so we made a threadlocal for that in FBUtilities.  Should probably use that.
- I don't understand the purpose of clockOffsetTicker.  it's only used during the constructor, but constructor is only used once so clockOffsetTicker has no effect.
- i'd be happier if we used an AtomicLong instead of synchronization to protect lastNanos.  (look at java Random class for an example of using AtomicLong for a similar purpose)","29/Jan/11 05:26;urandom;bq. message digest instantiation is expensive and synchronized, so we made a threadlocal for that in FBUtilities. Should probably use that.

done

bq. I don't understand the purpose of clockOffsetTicker. it's only used during the constructor, but constructor is only used once so clockOffsetTicker has no effect.

it was a throw-back from an earlier iteration. removed.

bq. i'd be happier if we used an AtomicLong instead of synchronization to protect lastNanos. (look at java Random class for an example of using AtomicLong for a similar purpose)

I don't think this will work.  Each UUID is supposed to have a higher value than the previous (even with identical timestamps), and this would allow racing threads to get out of order.  For what it's worth, JUG's UUIDGenerator was synchronized in the same way here.

----

Also included in this most recent patchset is caching to makeNode() so that addresses only need to be hashed once.","29/Jan/11 16:46;jbellis;nits:

- nodecache can be final
- makeNode is only called from instance methods and should be non-static (dropping references to static ""instance"" field)

otherwise, +1",29/Jan/11 17:27;urandom;committed (free of nits).,"29/Jan/11 18:22;hudson;Integrated in Cassandra #700 (See [https://hudson.apache.org/hudson/job/Cassandra/700/])
    remove JUG jar and license files

Patch by eevans for CASSANDRA-2067
eliminate usage of JUG for UUIDs

Patch by eevans for CASSANDRA-2067
CASSANDRA-2067 o.a.c.utils.UUIDGen adapted from flewton

Patch by eevans for CASSANDRA-2067
",31/Jan/11 22:20;messi;You could also use the Preferences system to store a random permanent node ID.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2 (more) Misuses of ByteBuffer relative gets,CASSANDRA-2066,12496957,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,1/27/2011 16:28,3/12/2019 14:16,3/13/2019 22:24,1/27/2011 16:37,0.7.1,,,,0,,,,,,In RandomPartitioner and SerDeUtils,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,27/Jan/11 16:31;slebresne;0001-Fix-2-misuses-of-ByteBuffer-relative-gets.patch;https://issues.apache.org/jira/secure/attachment/12469560/0001-Fix-2-misuses-of-ByteBuffer-relative-gets.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,37:07.9,,,no_permission,,,,,,,,,,,,20423,,,Thu Jan 27 16:54:33 UTC 2011,,,,,,0|i0g947:,92907,jbellis,jbellis,,,,,,,,,"27/Jan/11 16:31;slebresne;Patch attached agains 0.7 branch.
I include a slight optimisation of ByteBufferUtil.getArray() too, that
will avoid a copy at least in some current use of 
RandomPartitionner.factory().fromByteArray() (and possibly other places)",27/Jan/11 16:37;jbellis;committed,"27/Jan/11 16:54;hudson;Integrated in Cassandra-0.7 #220 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/220/])
    fix possibleByteBuffer race conditions
patch by slebresne; reviewed by jbellis for CASSANDRA-2066
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
digest latencies are not included in snitch calculations,CASSANDRA-2085,12497283,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,2/1/2011 4:19,3/12/2019 14:16,3/13/2019 22:24,2/1/2011 12:38,0.6.12,,,,0,,,,,,"ResponseVerbHandler calls

        MessagingService.instance.maybeAddLatency(cb, message.getFrom(), age);

but maybeAddLatency needs to include DigestResponseHandler (it was ported from 0.7 where that no longer exists)",,,,,,,,,,,,,,,,,,,,01/Feb/11 04:21;jbellis;2085.txt;https://issues.apache.org/jira/secure/attachment/12469888/2085.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,19:11.1,,,no_permission,,,,,,,,,,,,20435,,,Tue Feb 01 14:43:52 UTC 2011,,,,,,0|i0g97z:,92924,stuhood,stuhood,,,,,,,,,"01/Feb/11 05:19;stuhood;+1

Tested manually with RF=2 on 3 nodes with ONE and RF=3 on 3 nodes with QUORUM. I'm going to go find and vote for any tickets that mention testing nodes in degraded states.","01/Feb/11 10:00;stuhood;Couldn't find any, but created CASSANDRA-2089",01/Feb/11 12:38;jbellis;committed,"01/Feb/11 14:43;hudson;Integrated in Cassandra-0.6 #55 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/55/])
    include digest responses in dynamic snitch latencies
patch by jbellis; reviewed by stuhood for CASSANDRA-2085
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SST counter repair,CASSANDRA-2095,12497410,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,kelvin,kelvin,kelvin,2/2/2011 1:24,3/12/2019 14:16,3/13/2019 22:24,2/2/2011 14:47,0.8 beta 1,,,,0,,,,,,"When creating an SST for AES of a commutative/counter CF, do not ""clean"" non-commutative/counter columns.  i.e. deleted columns.",,,,,,,,,,,,,,,,,,,,02/Feb/11 01:25;kelvin;0001-CASSANDRA-2095-do-not-clean-nodes-of-deleted-columns.patch;https://issues.apache.org/jira/secure/attachment/12470008/0001-CASSANDRA-2095-do-not-clean-nodes-of-deleted-columns.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,03:09.6,,,no_permission,,,,,,,,,,,,20439,,,Wed Feb 02 15:18:43 UTC 2011,,,,,,0|i0g9a7:,92934,slebresne,slebresne,,,,,,,,,02/Feb/11 01:25;kelvin;do not clean nodes of deleted columns,02/Feb/11 09:03;slebresne;+1,02/Feb/11 14:47;jbellis;committed,"02/Feb/11 15:18;hudson;Integrated in Cassandra #704 (See [https://hudson.apache.org/hudson/job/Cassandra/704/])
    When creating an SST for AES of a commutative/counter CF, do not clean non-commutative/counter columns. i.e. deleted columns
patch by Kelvin Kakugawa; reviewed by slebresne for CASSANDRA-2095
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load spikes due to MessagingService-generated garbage collection,CASSANDRA-2058,12496753,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,ketralnis,ketralnis,1/26/2011 1:20,3/12/2019 14:16,3/13/2019 22:24,2/15/2011 19:48,0.6.11,0.7.1,,,0,,,,,,"(Filing as a placeholder bug as I gather information.)

At ~10p 24 Jan, I upgraded our 20-node cluster from 0.6.8->0.6.10, turned on the DES, and moved some CFs from one KS into another (drain whole cluster, take it down, move files, change schema, put it back up). Since then, I've had four storms whereby a node's load will shoot to 700+ (400% CPU on a 4-cpu machine) and become totally unresponsive. After a moment or two like that, its neighbour dies too, and the failure cascades around the ring. Unfortunately because of the high load I'm not able to get into the machine to pull a thread dump to see wtf it's doing as it happens.

I've also had an issue where a single node spikes up to high load, but recovers. This may or may not be the same issue from which the nodes don't recover as above, but both are new behaviour","OpenJDK 64-Bit Server VM (build 1.6.0_0-b12, mixed mode)
Ubuntu 8.10
Linux pmc01 2.6.27-22-xen #1 SMP Fri Feb 20 23:58:13 UTC 2009 x86_64 GNU/Linux",,1440,1440,,0%,1440,1440,,,,,,,,,,,,27/Jan/11 18:30;brandon.williams;2058-0.7-v2.txt;https://issues.apache.org/jira/secure/attachment/12469574/2058-0.7-v2.txt,27/Jan/11 18:40;jbellis;2058-0.7-v3.txt;https://issues.apache.org/jira/secure/attachment/12469575/2058-0.7-v3.txt,27/Jan/11 04:06;jbellis;2058-0.7.txt;https://issues.apache.org/jira/secure/attachment/12469512/2058-0.7.txt,26/Jan/11 21:49;jbellis;2058.txt;https://issues.apache.org/jira/secure/attachment/12469482/2058.txt,26/Jan/11 01:29;ketralnis;cassandra.pmc01.log.bz2;https://issues.apache.org/jira/secure/attachment/12469368/cassandra.pmc01.log.bz2,26/Jan/11 03:38;ketralnis;cassandra.pmc14.log.bz2;https://issues.apache.org/jira/secure/attachment/12469380/cassandra.pmc14.log.bz2,26/Jan/11 03:30;ketralnis;graph a.png;https://issues.apache.org/jira/secure/attachment/12469378/graph+a.png,26/Jan/11 03:30;ketralnis;graph b.png;https://issues.apache.org/jira/secure/attachment/12469379/graph+b.png,,,,,,8,,,,,,,,,,,,,,,,,,,48:44.0,,,no_permission,,,,,,,,,,,,20419,,,Tue Feb 15 19:48:56 UTC 2011,,,,,,0|i0g92f:,92899,brandon.williams,brandon.williams,,,,,,,,,26/Jan/11 01:29;ketralnis;This is one of the affected nodes' logs from 7a..6p (uncompresses to ~33mb). Note that around 4p I added a job to pull a jstack every 120s. On this node around 5:46p I saw the version of the load spike where the node recovers (at around 17:53).,26/Jan/11 01:30;ketralnis;It occurs to me that my timestamps may be in a different time zone than the logs themselves,"26/Jan/11 01:35;ketralnis;Also since then I've had notably worse performance, reading is maybe 30% slower than before.

My next step will be to hope that the jstacks in that log are the same as the ones causing the largest outages and to disable to dynamic snitch (as much as i'd like to get 100% reproduction, I'd also rather not take my site down) to see if that resolves the problem. If it doesn't, then I'll turn it back on and revert to 0.6.8 to see if that does it",26/Jan/11 01:48;jbellis;I believe this is the same as CASSANDRA-2054 but will leave both open for now.,"26/Jan/11 03:30;ketralnis;Just had this happen again, attaching load/CPU graphs. Will have logs shortly.

I was in the middle of pushing out the change to turn off the DES. This is pmc14. As of when this happened, the nodes {pmc01 pmc04 pmc07 pmc10 pmc13 pmc16} had it turned off but the others have not been restarted","26/Jan/11 05:10;jbellis;bq. If it doesn't, then I'll turn it back on and revert to 0.6.8 to see if that does it

You were running 0.6.8 + DS before?  Or is ""it"" not DynamicSnitch?","26/Jan/11 06:48;ketralnis;bq. You were running 0.6.8 + DS before? Or is ""it"" not DynamicSnitch?

I was running 0.6.8 with no DES. Then I upgraded to 0.6.10 and turned it on. I had the aforementioned problems.

Now I'm running 0.6.10 with the DES turned off. (As of this writing, I'm still seeing the momentary spikes but thus far no sustained ones.)

If I continue to have the momentary or sustained spikes (I'll probably know by the morning), then I'll revert to 0.6.8, and turn *on* the DES.

If after that I continue to have problems I'll revert back to 0.6.8 with no DES, which is at least a configuration in which I didn't have any of these problems",26/Jan/11 14:23;jbellis;DES in 0.6.8 is a no-op unless you're doing quorum reads.,26/Jan/11 19:26;ketralnis;I am in fact still having both the momentary and the sustained failures and am rolling back to 0.6.8 with no DES (since you describe it as a no-op anyway),"26/Jan/11 21:22;ketralnis;I've rolled back to 0.6.8 with the DES disabled and not only has the load problem stopped, performance has also gone back up to previous levels","26/Jan/11 21:49;jbellis;Brandon's testing has narrowed the culprit down to CASSANDRA-1959.  As discussed on CASSANDRA-2054, the main problem there is with the NonBlockingHashMap introduced to track timed out latencies.

This patch reverts that and takes a different approach, of tracking the latency in the callback map.  This means that we need a unique messageId for each target we send a message to.  The Right Way to do this would be to have Message objects only contain the data to send, not the From address and not the messageId.  Refactoring Message is outside our scope here though, so instead we create a new Message for each target.

This does let us clean up the callback map in ResponseVerbHandler instead of in each Callback.  (That is what is going on in the changes to QRH, WRH, and AR.)","27/Jan/11 03:49;tjake;This looks good overall, nothing major I can see.

The only niggles are:
 
1. ExpiringMap; we could do the same with MapMaker and may be more bulletproof. see EvictionListener http://guava-libraries.googlecode.com/svn/trunk/javadoc/com/google/common/collect/MapMaker.html

2. I also wonder what impact (if any) there will be for generating a message per endpoint rather than re-using the same one as was perviously done.

But as-is it's still +1","27/Jan/11 03:57;jbellis;Thanks, Jake.

1. agreed, I'd like to upgrade at some point, but changing stuff i don't have to scares me at this point in 0.6.

2. we definitely saw a small speedup when i made that optimization the first time, but I'd rather have a working dynamic snitch.  (we can optimize later in 0.7 -- see The Right Way above.)  combined w/ the improved tcp performance in 0.6.10 we should still be ahead of 0.6.8 aka the last version that didn't have MessagingService bugs.",27/Jan/11 04:06;jbellis;port to 0.7 attached.,"27/Jan/11 16:51;brandon.williams;0.6 version looks good, RR, HH, and DES work, no more CPU spikes under heavy load.",27/Jan/11 16:56;jbellis;committed 0.6 version,"27/Jan/11 17:13;hudson;Integrated in Cassandra-0.6 #52 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/52/])
    reduce garbage generated by MessagingServiceto prevent loadspikes
patch by jbellis; reviewed by brandonwilliams and tjake for CASSANDRA-2058
","27/Jan/11 18:30;brandon.williams;0.7 v2 fixes the DES by incorporating the approach from CASSANDRA-2004 and having it register with MS directly and removing ILP.  However, it does not receive timings for the local node.",27/Jan/11 18:40;jbellis;v3 adds latency tracking to LocalReadRunnable,27/Jan/11 22:25;brandon.williams;+1,27/Jan/11 22:44;jbellis;committed to 0.7 and trunk,"27/Jan/11 23:48;mmalone;Jake/Jonathan,

FWIW, I re-implemented ExpiringMap with MapMaker using an eviction listener (but mostly maintaining the ExpiringMap API) a little while back while investigating some messaging service issues we were seeing. The patch is against 0.6.8, but here's the code if you wanna try it out: https://gist.github.com/a2f645c69ca8f44ccff3

It could definitely be simplified more by someone willing to make more widespread code changes. Actually, I think using MapMaker directly and getting rid of ExpiringMap would probably be best. *shrug*","28/Jan/11 00:01;jbellis;bq. I think using MapMaker directly and getting rid of ExpiringMap would probably be best

Agreed, opened CASSANDRA-2070 for that",31/Jan/11 21:14;ketralnis;I have upgraded to 0.6.11 and am definitely still seeing this problem (although I'm no longer seeing the 30% performance hit while the nodes are up),31/Jan/11 21:51;jbellis;Please tell me you're at least seeing this less often than with .10 :),"01/Feb/11 00:07;ketralnis;It's hard to say. I lost 5 nodes in about an hour, but I don't know how many I lost last time","01/Feb/11 15:58;pquerna;FYI, since upgrading to .10 we are also seeing this problem :(  Tried getting a jstack, but didn't work, tpstats etc all timed out.","01/Feb/11 17:51;brandon.williams;Paul, I would expect to see it on .10 (I can repro there) but that is what this ticket was supposed to address.  Can you repro with .11?",01/Feb/11 19:22;urandom;They're seeing it on r1064246 (one rev newer than 0.6.11).,"04/Feb/11 09:27;tbritz;I'm also seeing something similar on yesterday's svn version (the one with the Consistency level fix).

It only occurs if I enable JNA.

Nodes will experience enormous high kernel load (htop, red bar). Ssh sessions on these servers will lag extermely. Nodes won't take 100% cpu though, but the cluster is unusable.

(Just to note: it's a completely different pattern to the 100% cpu spike which occured before, and I can't reproduce it wihout JNA enabled)
","04/Feb/11 18:02;ketralnis;I don't have JNA on these hosts, so at least in my case it's not JNA-related.",04/Feb/11 20:20;brandon.williams;Could those who are seeing this issue please post the JVM flags they're using?,"04/Feb/11 21:08;ketralnis;JVM_OPTS="" \
        -ea \
        -Xms6656m \
        -Xmx6656m \
        -XX:+UseParNewGC \
        -XX:+UseConcMarkSweepGC \
        -XX:+CMSParallelRemarkEnabled \
        -XX:SurvivorRatio=8 \
        -XX:MaxTenuringThreshold=1 \
        -XX:CMSInitiatingOccupancyFraction=75 \
        -XX:+UseCMSInitiatingOccupancyOnly \
        -XX:+HeapDumpOnOutOfMemoryError \
        -XX:+UseThreadPriorities \
        -XX:ThreadPriorityPolicy=42 \
        -Dcassandra.compaction.priority=1 \
        -Dcom.sun.management.jmxremote.port=8080 \
        -Dcom.sun.management.jmxremote.ssl=false \
        -Dcom.sun.management.jmxremote.authenticate=false""


/usr/bin/java -ea -Xms6656m -Xmx6656m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Dcassandra.compaction.priority=1 -Dcom.sun.management.jmxremote.port=8080 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dstorage-config=bin/../conf -Dcassandra-foreground=yes -cp bin/../conf:bin/../build/classes:bin/../lib/antlr-3.1.3.jar:bin/../lib/clhm-production.jar:bin/../lib/commons-cli-1.1.jar:bin/../lib/commons-codec-1.2.jar:bin/../lib/commons-collections-3.2.1.jar:bin/../lib/commons-lang-2.4.jar:bin/../lib/google-collections-1.0.jar:bin/../lib/hadoop-core-0.20.1.jar:bin/../lib/high-scale-lib.jar:bin/../lib/jackson-core-asl-1.4.0.jar:bin/../lib/jackson-mapper-asl-1.4.0.jar:bin/../lib/jline-0.9.94.jar:bin/../lib/json-simple-1.1.jar:bin/../lib/libthrift-r917130.jar:bin/../lib/log4j-1.2.14.jar:bin/../lib/slf4j-api-1.5.8.jar:bin/../lib/slf4j-log4j12-1.5.8.jar org.apache.cassandra.thrift.CassandraDaemon


java version ""1.6.0_0""
IcedTea6 1.3.1 (6b12-0ubuntu6.7) Runtime Environment (build 1.6.0_0-b12)
OpenJDK 64-Bit Server VM (build 1.6.0_0-b12, mixed mode)


Linux pmc01 2.6.27-22-xen #1 SMP Fri Feb 20 23:58:13 UTC 2009 x86_64 GNU/Linux","15/Feb/11 19:48;jbellis;closing this so it's clear that the excessive object creation problem introduced in CASSANDRA-1905 is fixed in 0.6.11 / 0.7.1.

opened CASSANDRA-2170 for other load spikes.",,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableDeletingReference only deletes data files,CASSANDRA-2059,12496836,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,1/26/2011 15:28,3/12/2019 14:16,3/13/2019 22:24,1/28/2011 0:09,0.7.1,,,,0,,,,,,"Ching-Cheng Chen reports on the mailing list:
	

In SSTableDeletingReference, it try this operation

components.remove(Component.DATA);

before

STable.delete(desc, components);

However, the components was reference to the components object which was created inside SSTable by

this.components = Collections.unmodifiableSet(dataComponents);

As you can see, you can't try the remove operation on that components object.",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,26/Jan/11 15:48;jbellis;2059.txt;https://issues.apache.org/jira/secure/attachment/12469428/2059.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,29:21.8,,,no_permission,,,,,,,,,,,,20420,,,Fri Jan 28 00:46:26 UTC 2011,,,,,,0|i0g92n:,92900,stuhood,stuhood,,,,,,,,,26/Jan/11 15:48;jbellis;Patch to use Sets.difference instead of mutating components,"27/Jan/11 04:29;stuhood;+1

Do we have a separate issue to fix the lack-of-logged-exception problem? Opened CASSANDRA-2061... might be a dupe.",28/Jan/11 00:09;jbellis;committed,"28/Jan/11 00:46;hudson;Integrated in Cassandra-0.7 #224 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/224/])
    fix deletionof sstable non-data components
patch by jbellis; reviewed by stuhood for CASSANDRA-2059
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException during lazy row compaction of supercolumns,CASSANDRA-2104,12497548,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,dln,dln,2/3/2011 10:08,3/12/2019 14:16,3/13/2019 22:24,2/24/2011 19:09,0.7.3,,,,0,,,,,,"I ran into an exception when lazily compacting wide rows of TimeUUID columns.
It seems to trigger when a row is larger than {{in_memory_compaction_limit_in_mb}}.

Traceback:
{noformat}
 INFO [CompactionExecutor:1] 2011-02-03 10:59:59,262 CompactionIterator.java (line 135) Compacting large row XXXXXXXXXXXXX (76999384 bytes) incrementally
 ERROR [CompactionExecutor:1] 2011-02-03 10:59:59,266 AbstractCassandraDaemon.java (line 114) Fatal exception in thread T
 hread[CompactionExecutor:1,1,main]
 java.lang.IndexOutOfBoundsException
         at java.nio.Buffer.checkIndex(Buffer.java:514)
         at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)
         at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(TimeUUIDType.java:56)
         at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:45)
         at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:29)
         at java.util.concurrent.ConcurrentSkipListMap$ComparableUsingComparator.compareTo(ConcurrentSkipListMap.java:606
 )
         at java.util.concurrent.ConcurrentSkipListMap.findPredecessor(ConcurrentSkipListMap.java:685)
         at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:864)
         at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
         at org.apache.cassandra.db.SuperColumn.addColumn(SuperColumn.java:170)
         at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:195)
         at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:221)
         at org.apache.cassandra.io.LazilyCompactedRow$LazyColumnIterator.reduce(LazilyCompactedRow.java:204)
         at org.apache.cassandra.io.LazilyCompactedRow$LazyColumnIterator.reduce(LazilyCompactedRow.java:185)
         at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:62)
         at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
         at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
         at com.google.common.collect.Iterators$7.computeNext(Iterators.java:604)
         at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
         at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
         at org.apache.cassandra.db.ColumnIndexer.serializeInternal(ColumnIndexer.java:76)
         at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:50)
         at org.apache.cassandra.io.LazilyCompactedRow.<init>(LazilyCompactedRow.java:88)
         at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:137)
         at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
         at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
         at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
         at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
         at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
         at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
         at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
         at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:426)
         at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:122)
         at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:92)
         at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
         at java.util.concurrent.FutureTask.run(FutureTask.java:138)
         at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
         at java.lang.Thread.run(Thread.java:662)

{noformat}",,,28800,28800,,0%,28800,28800,,,,,,,,,,,,24/Feb/11 16:32;slebresne;0001-Use-the-right-comparator-when-deserializing-superCol.patch;https://issues.apache.org/jira/secure/attachment/12471850/0001-Use-the-right-comparator-when-deserializing-superCol.patch,24/Feb/11 18:16;slebresne;Unit-test.patch;https://issues.apache.org/jira/secure/attachment/12471857/Unit-test.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,41:22.7,,,no_permission,,,,,,,,,,,,20445,,,Thu Mar 10 10:04:36 UTC 2011,,,,,,0|i0g9c7:,92943,jbellis,jbellis,,,,,,,,,"03/Feb/11 16:41;jbellis;I believe you mentioned on the ML that you have data that compacts fine if you increase the in_memory limit, but not with lazy mode?

If so, can you get me a copy of that sstable to test with?  I can set up a place for you to upload it privately if you can't make it public.","04/Feb/11 11:22;dln;Unfortunately, I'm not at liberty to share that data, sorry. But I'm working on seeing if I can reproduce it with non-sensitive data...

These are timeuuid cols, wide rows (100k or so cols, with a few multi-mb columns but mostly <1k sizes). No validators.

The instance in question had small memtable thresholds (32MB), a pretty low heap (6GB) but nothing out of the ordinary.",16/Feb/11 02:51;jbellis;Any luck reproducing?,"23/Feb/11 14:30;dln;Nope, never managed to trigger it again.
Closing this issue for now.","23/Feb/11 14:56;jborgstrom;Hi Jonathan,

I now have a python script that seems to be able to reproduce this after a few runs (it does not seem to always happen)

http://jonas.borgstrom.se/cassandra/CASSANDRA-2104.txt

I've tested this on an empty single node 0.7.2 cluster and default cassandra.yaml, except for in_memory_compaction_limit_in_mb=32 hoping that would make it easier to reproduce.

A complete copy of the data directory (and config) after running the script a couple of times can be downloaded here:
http://jonas.borgstrom.se/cassandra/CASSANDRA-2104.tar.gz

And a copy of system.log from booting cassandra using this data directory:
http://jonas.borgstrom.se/cassandra/CASSANDRA-2104-system.log

Let me know if you need any more details.","23/Feb/11 15:14;jbellis;Jonas, are you also seeing the error only on rows larger than in_memory_compaction_limit then?","23/Feb/11 16:14;jborgstrom;Jonathan, yes that seems to be the case. For me key 3139 (19) seems to be the one triggering this. So increasing in_memory_compaction_limit enough to cover that key seems to do the trick. Even if some other keys are compacted incrementally.","24/Feb/11 11:42;jborgstrom;I've done some more testing now and I'm ONLY able to reproduce this when using both super columns and the TimeUUIDType column comparator.

After looking at the code TimeUUIDType seems to be the only marshaller that would actually notice (throw an exception) if its compare() method was called with a partial value (1-15 bytes in the case of UUIDs).

So to me it looks like the incremental compactor sometimes sends corrupted/partial data to the marshaller, at least for super column families. This corrupted/partial data is silently ignored unless the TimeUUIDType marshaller is used.","24/Feb/11 13:46;slebresne;I have a hard time reproducing (using your script). Do you use TimeUUIDType for the column or the super column names ? (I've tried both though).

I'll continue trying. However, if you happen to have some SSTables that directly triggers it and that is smaller that the ones you added above, that would be awesome (I'm struggling making enough room on my laptop for the ones you attached above :)).

Anyway, thanks for taking time on this, I'll continue to try.","24/Feb/11 14:47;jborgstrom;Hi Sylvain,

The last time I reproduce it I used ""create column family bar with column_type=Super and comparator=TimeUUIDType and subcomparator=UTF8Type;"" and had to run the test case twice.

Anyway here's a new data directory with only a subset of the sstables from the last dump. I get an exception in the log within seconds after issuing a ""compact"" command.

http://jonas.borgstrom.se/cassandra/CASSANDRA-2104-v2.tar.gz (153kB compressed and 127MB uncompressed)","24/Feb/11 16:36;slebresne;Thanks a lot Jonas, those new sstables were most useful.

Turns out the problem was that we were using the wrong comparator when deserializing, so we were comparing the column using TimeUUIDType (instead of UTF8Type).

Sadly, it could be that we were generating wrongly sorted on-disk superColumns. However any compaction with the attached patch should fix that anyway.",24/Feb/11 18:16;slebresne;Attaching a unit test that exposes the bug (and confirm the fix),24/Feb/11 19:09;jbellis;committed,"25/Feb/11 15:15;hudson;Integrated in Cassandra-0.7 #321 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/321/])
    ","10/Mar/11 00:47;akaris;Hi,

I'm also getting an IndexOutOfBounds exception when compacting.

Here's the detailed error I get on screen when running ""nodetool -h 10.3.133.10 compact"":

Error occured while compacting keyspace test
java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException
    at java.util.concurrent.FutureTask$Sync.innerGet(Unknown Source)
    at java.util.concurrent.FutureTask.get(Unknown Source)
    at org.apache.cassandra.db.CompactionManager.performMajor(CompactionManager.java:186)
    at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1678)
    at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1248)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
    at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
    at com.sun.jmx.mbeanserver.PerInterface.invoke(Unknown Source)
    at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Unknown Source)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown Source)
    at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
    at sun.rmi.transport.Transport$1.run(Unknown Source)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.rmi.transport.Transport.serviceCall(Unknown Source)
    at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IndexOutOfBoundsException
    at java.nio.Buffer.checkIndex(Unknown Source)
    at java.nio.HeapByteBuffer.getInt(Unknown Source)
    at org.apache.cassandra.db.DeletedColumn.getLocalDeletionTime(DeletedColumn.java:57)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedStandard(ColumnFamilyStore.java:822)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedColumnsOnly(ColumnFamilyStore.java:809)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeleted(ColumnFamilyStore.java:800)
    at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:94)
    at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:139)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
    at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
    at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:427)
    at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:217)
    at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
    at java.util.concurrent.FutureTask.run(Unknown Source)
    ... 3 more

And here's the error I'm getting in my log file:

ERROR [CompactionExecutor:1] 2011-03-09 19:16:52,299 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.IndexOutOfBoundsException
    at java.nio.Buffer.checkIndex(Unknown Source)
    at java.nio.HeapByteBuffer.getInt(Unknown Source)
    at org.apache.cassandra.db.DeletedColumn.getLocalDeletionTime(DeletedColumn.java:57)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedStandard(ColumnFamilyStore.java:822)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedColumnsOnly(ColumnFamilyStore.java:809)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeleted(ColumnFamilyStore.java:800)
    at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:94)
    at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:139)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
    at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
    at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:427)
    at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:217)
    at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
    at java.util.concurrent.FutureTask.run(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)

I run Cassandra 0.7.2. We have 8 machines in the cluster, the error happens only on one machine. I'm not sure it's the same issue than this ticket but it's the only reference I found about compacting and IndexOutOfBounds. We're not inserting any SuperColumn in that database.

Thanks for the help.","10/Mar/11 01:09;akaris;We upgraded to 0.7.3 and we still have the same error, so I guess it's a different problem :(","10/Mar/11 10:04;slebresne;Can you open a new ticket with the description of the problem then. If you're at liberty to attach an bad sstable to reproduce the problem, that would clearly help.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupted Commit logs,CASSANDRA-2128,12497895,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,pquerna,pquerna,2/7/2011 19:41,3/12/2019 14:16,3/13/2019 22:24,2/9/2011 17:07,0.6.12,0.7.2,,,0,,,,,,"Two of our nodes had a hard failure.

They both came up with a corrupted commit log.

On startup we get this:
{quote}
011-02-07_19:34:03.95124 INFO - Finished reading /var/lib/cassandra/commitlog/CommitLog-1297099954252.log
2011-02-07_19:34:03.95400 ERROR - Exception encountered during startup.
2011-02-07_19:34:03.95403 java.io.EOFException
2011-02-07_19:34:03.95403 	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
2011-02-07_19:34:03.95404 	at java.io.DataInputStream.readUTF(DataInputStream.java:572)
2011-02-07_19:34:03.95405 	at java.io.DataInputStream.readUTF(DataInputStream.java:547)
2011-02-07_19:34:03.95406 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:363)
2011-02-07_19:34:03.95407 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:318)
2011-02-07_19:34:03.95408 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:240)
2011-02-07_19:34:03.95409 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
2011-02-07_19:34:03.95409 	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:115)
2011-02-07_19:34:03.95410 	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
2011-02-07_19:34:03.95422 Exception encountered during startup.
2011-02-07_19:34:03.95436 java.io.EOFException
2011-02-07_19:34:03.95447 	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
2011-02-07_19:34:03.95458 	at java.io.DataInputStream.readUTF(DataInputStream.java:572)
2011-02-07_19:34:03.95468 	at java.io.DataInputStream.readUTF(DataInputStream.java:547)
2011-02-07_19:34:03.95478 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:363)
2011-02-07_19:34:03.95489 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:318)
2011-02-07_19:34:03.95499 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:240)
2011-02-07_19:34:03.95510 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
2011-02-07_19:34:03.95521 	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:115)
2011-02-07_19:34:03.95531 	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
{quote}

On node A, the commit log in question is 100mb.

On node B, the commit log in question is 60mb.

An ideal resolution would be if EOF is hit early, log something, but don't stop the startup.  Instead process everything that we have done so far, and keep going.
","cassandra-0.6 @ r1064246 (0.6.11)
Ubuntu 9.10
Rackspace Cloud
",,,,,,,,,,,,,,,,,,,09/Feb/11 16:36;jbellis;2128.txt;https://issues.apache.org/jira/secure/attachment/12470686/2128.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,43:06.7,,,no_permission,,,,,,,,,,,,20456,,,Wed Feb 09 18:54:54 UTC 2011,,,,,,0|i0g9hb:,92966,gdusbabek,gdusbabek,,,,,,,,,07/Feb/11 19:43;gdusbabek;0.7 handles this gracefully by using checksums when the RowMutations are read from the CL. 0.6 could handle this better with a try{}catch{} where the RowMutation is deserialized.,"07/Feb/11 19:49;gdusbabek;Actually, this is kind of bizarre. 0.6 does use a checksum.  The failure is happening after the RM has been read (in full) while we are attempting to deserialize it.","07/Feb/11 19:58;jbellis;0.6 uses checksums too, for this area.  (The one place it doesn't is the Header, which I posted a patch for in CASSANDRA-1815, but that doesn't seem to actually matter in practice.)

Here's the code in question:

{code}
                    long claimedCRC32;
                    byte[] bytes;
                    try
                    {
                        bytes = new byte[(int) reader.readLong()]; // readlong can throw EOFException too
                        reader.readFully(bytes);
                        claimedCRC32 = reader.readLong();
                    }
                    catch (EOFException e)
                    {
                        // last CL entry didn't get completely written.  that's ok.
                        break;
                    }

                    ByteArrayInputStream bufIn = new ByteArrayInputStream(bytes);
                    Checksum checksum = new CRC32();
                    checksum.update(bytes, 0, bytes.length);
                    if (claimedCRC32 != checksum.getValue())
                    {
                        // this part of the log must not have been fsynced.  probably the rest is bad too,
                        // but just in case there is no harm in trying them.
                        continue;
                    }

                    /* deserialize the commit log entry */
                    final RowMutation rm = RowMutation.serializer().deserialize(new DataInputStream(bufIn));
{code}

In other words, first we read the mutation into a buffer and checksum it.  If it passes, we try to deserialize that into a mutation.

It's expected that read-into-buffer can throw an EOF (which we expect and catch) but once it passes checksum it's not expected that mutation deserialize should fail.

(Yes, checksums aren't perfect, especially not 32bit ones, but for the checksum to generate a false positive on the last entry in the commitlog, on two different machines, is not a coincidence I'm buying.)

At first glance, the most likely explanation is a bug in RowMutation serializer.  But it's also possible that I'm wrong and it really is erroring out due to some unflushed data somehow.  If you turn on debug logging, it will include the offset in the commitlog of the mutation being replayed -- if it's very very close to the end of the file, then that would increase that likelihood.

bq. An ideal resolution would be if EOF is hit early, log something, but don't stop the startup. Instead process everything that we have done so far, and keep going.

Disagreed.  This is a serious enough bug that we should require human intervention before ignoring it and starting up anyway.",09/Feb/11 15:07;jbellis;CASSANDRA-2055 reports the exact same stacktrace: RM.deserialize EOFing on the very first field it tries to read.  Suspicious!,"09/Feb/11 16:20;jbellis;Paul sent me one of the CommitLog files exhibiting this problem.  It is 99614720 bytes long.

I added one more log entry when replaying it:

{code}
                    logger.debug(""checksum of "" + bytes.length + "" successful: "" + claimedCRC32);
                    /* deserialize the commit log entry */
{code}

The last entries in the log before dying are

{noformat}
DEBUG [main] 2011-02-09 09:39:15,577 CommitLog.java (line 213) Reading mutation at 97750254
DEBUG [main] 2011-02-09 09:39:15,577 CommitLog.java (line 240) checksum of 1209 successful: 2281213824
DEBUG [main] 2011-02-09 09:39:15,578 CommitLog.java (line 244) replaying mutation for MonitorApp...
DEBUG [main] 2011-02-09 09:39:15,578 CommitLog.java (line 213) Reading mutation at 97751479
DEBUG [main] 2011-02-09 09:39:15,580 CommitLog.java (line 240) checksum of 0 successful: 0
 INFO [main] 2011-02-09 09:39:15,604 CommitLog.java (line 253) Finished reading /var/lib/cassandra/commitlog/CommitLog-1297099954252.log
ERROR [main] 2011-02-09 09:39:15,652 CassandraDaemon.java (line 242) Exception encountered during startup.
java.io.EOFException
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
{noformat}

This is a good 2MB before the end of the file.  So I looked in a hex editor to see what was there, starting at offset 97751479.  There's about 8KB of pure 00 bytes, followed by nonsense that occasionally looks like a RowMutation (some appearances of column names cnt, avg, dev that are present in the early, intact part of the commitlog) but mostly does not: there are no more appearances of the keyspace name, MonitorApp, which is the first part of any real RowMutation in the 0.6 serialization format, and in one place there is even the string ""java.util.BitSet"" visible, which should only appear in the commitlog segment header at the start of the file (that we rewrite at each flush).

To me it looks like ""EC2 can write a bunch of nonsense in your commitlog during a hard failure,"" and the fix is to make the checksum process more robust against nonsense that happens to conform to the first couple bytes we read for a RowMutation.",09/Feb/11 16:36;jbellis;Patch adds a check to the length of the row mutation read to make sure it is sane.  (Sane lengths will make sure the CRC can do its job.),"09/Feb/11 16:39;jbellis;0.7 already has a weaker fix for this (checking that serializedSize > 0, which isn't quite as strong as >= 10, but probably adequate in practice, and adding a separate checksum for the size itself.)","09/Feb/11 17:00;gdusbabek;+1, except don't modify the log4j config!","09/Feb/11 17:07;jbellis;committed, without the log4j changes","09/Feb/11 18:54;hudson;Integrated in Cassandra-0.6 #58 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/58/])
    update commitlog replay to catch bogus RowMutation lengths
patch by jbellis; reviewed by gdusbabek for CASSANDRA-2128
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Illegal file mode when saving caches,CASSANDRA-2131,12497914,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,amorton,amorton,amorton,2/7/2011 22:01,3/12/2019 14:16,3/13/2019 22:24,2/7/2011 23:29,0.7.1,,,,0,,,,,,"The following error is logged when trying to save caches


DEBUG [CompactionExecutor:1] 2011-02-08 07:30:03,647 CacheWriter.java (line 45) Saving /var/lib/cassandra/saved_caches/Keyspace1-ascii-KeyCache
ERROR [CompactionExecutor:1] 2011-02-08 07:30:03,725 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: Illegal mode ""w"" must be one of ""r"", ""rw"", ""rws"", or ""rwd""
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.IllegalArgumentException: Illegal mode ""w"" must be one of ""r"", ""rw"", ""rws"", or ""rwd""
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:197)
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:116)
	at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:48)
	at org.apache.cassandra.db.CompactionManager$9.runMayThrow(CompactionManager.java:746)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 6 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,29:36.2,,,no_permission,,,,,,,,,,,,20458,,,Mon Feb 07 23:29:36 UTC 2011,,,,,,0|i0g9hz:,92969,jbellis,jbellis,,,,,,,,,07/Feb/11 23:29;jbellis;fixed in r1068220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in ThreadPoolExecutor,CASSANDRA-2134,12497954,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,patrik.modesto,patrik.modesto,2/8/2011 6:57,3/12/2019 14:16,3/13/2019 22:24,2/9/2011 14:23,0.7.1,,,,0,,,,,,"On my two-node test setup I get repeatedly following error:

The 10.0.18.129 server log:
{noformat} 
 INFO 14:10:37,707 Node /10.0.18.99 has restarted, now UP again
 INFO 14:10:37,708 Checking remote schema before delivering hints
 INFO 14:10:37,708 Sleeping 45506ms to stagger hint delivery
 INFO 14:10:37,709 Node /10.0.18.99 state jump to normal
 INFO 14:11:23,215 Started hinted handoff for endpoint /10.0.18.99
ERROR 14:11:23,884 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
ERROR 14:11:23,885 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
{noformat} 

The 10.0.18.99 server log:
{noformat} 
 INFO 14:10:37,691 Binding thrift service to /0.0.0.0:9160
 INFO 14:10:37,693 Using TFastFramedTransport with a max frame size of
15728640 bytes.
 INFO 14:10:37,695 Listening for thrift clients...
 INFO 14:10:38,337 GC for ParNew: 954 ms, 658827608 reclaimed leaving
966732432 used; max is 4265607168
 INFO 14:11:27,142 Started hinted handoff for endpoint /10.0.18.129
ERROR 14:11:27,370 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
ERROR 14:11:27,371 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
{noformat}

It happen durring batch_mutate test or after restart, when there are commitlogs to replay. Using current 0.7.1 from cassandra-0.7 branch.",Linux 2.6.32-bpo.4edois1-openvz-amd64 #1 SMP x86_64 GNU/Linux,,60,60,,0%,60,60,,,,,,,,,,,,08/Feb/11 14:57;slebresne;0001-Fix-BBUtil.string-offset-related-to-position-instead.patch;https://issues.apache.org/jira/secure/attachment/12470594/0001-Fix-BBUtil.string-offset-related-to-position-instead.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,57:44.3,,,no_permission,,,,,,,,,,,,20460,,,Wed Feb 09 14:23:18 UTC 2011,,,,,,0|i0g9in:,92972,jbellis,jbellis,,,,,,,,,08/Feb/11 14:57;slebresne;Attached patch should fix this.,"08/Feb/11 15:29;hudson;Integrated in Cassandra-0.7 #258 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/258/])
    fix ByteBufferUtil.string position
patch by slebresne; reviewed by jbellis for CASSANDRA-2134
","08/Feb/11 15:54;patrik.modesto;I can still get exception in ThreadPoolExecutor.

1st server:

{noformat}
ERROR 16:50:34,349 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR 16:50:34,351 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{noformat}

2nd server:

{noformat}
ERROR 16:50:37,532 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR 16:50:37,547 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{noformat}","08/Feb/11 16:52;jbellis;If you haven't removed the corrupt HintsColumnFamily files, you need to do so.",09/Feb/11 12:12;patrik.modesto;Deleting the old HintColumnFamily files helped. Thanks.,09/Feb/11 14:23;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot parse generation after restart,CASSANDRA-1989,12495657,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,stuhood,stuhood,1/15/2011 5:43,3/12/2019 14:16,3/13/2019 22:24,1/15/2011 15:20,0.8 beta 1,,,,0,,,,,,"Looks like CASSANDRA-1714 broke some parsing of the generation on some restarts: haven't tracked it down yet, but this is likely to be obvious for someone who worked on that issue.
{code}java.lang.UnsupportedOperationException
        at java.nio.ByteBuffer.array(ByteBuffer.java:940)
        at org.apache.cassandra.utils.FBUtilities.byteBufferToInt(FBUtilities.java:212)
        at org.apache.cassandra.db.SystemTable.incrementAndGetGeneration(SystemTable.java:286)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:356)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:184)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:54)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:240)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:133){code}
",,;15/Jan/11 15:33;xedin;7200,7200,0,7200,100%,7200,0,7200,,CASSANDRA-1964,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,09:25.0,,,no_permission,,,,,,,,,,,,20390,,,Sat Jan 15 16:24:17 UTC 2011,,,,,,0|i0g8mv:,92829,jbellis,jbellis,,,,,,,,,"15/Jan/11 15:09;xedin;I have added patch (CASSANDRA-1714-additions-to-trunk.patch) to CASSANDRA-1714 which will fix this for trunk, cassandra-0.7 branch does not have this issue.",15/Jan/11 15:20;jbellis;committed patch attached to CASSANDRA-1714,"15/Jan/11 16:24;hudson;Integrated in Cassandra #672 (See [https://hudson.apache.org/hudson/job/Cassandra/672/])
    fix use of direct buffers from CASSANDRA-1714
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-1989
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra.bat does fail when CASSANDRA_HOME contains a whitespace,CASSANDRA-2237,12499597,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,norman,norman,norman,2/24/2011 14:24,3/12/2019 14:16,3/13/2019 22:24,2/24/2011 19:56,0.7.3,,Packaging,,0,,,,,,"If you try to start cassandra from a directory with whitespaces you will see a stacktrace similar to this:

Starting Cassandra Server
Exception in thread ""main"" java.lang.NoClassDefFoundError: and
Caused by: java.lang.ClassNotFoundException: and
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:303)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:316)
Could not find the main class: and.  Program will exit.
",windows,,,,,,,,,,,,,,,,,,,24/Feb/11 14:26;norman;CASSANDRA-2237.diff;https://issues.apache.org/jira/secure/attachment/12471837/CASSANDRA-2237.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,56:40.8,,,no_permission,,,,,,,,,,,,20520,,,Fri Feb 25 15:15:13 UTC 2011,,,,,,0|i0ga67:,93078,jbellis,jbellis,,,,,,,,,24/Feb/11 14:26;norman;Here is the batch which fix this,24/Feb/11 14:29;norman;the patch must get applied in the bin/ folder.,"24/Feb/11 19:56;jbellis;committed, thanks!","25/Feb/11 15:15;hudson;Integrated in Cassandra-0.7 #321 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/321/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Startup fails due to cassandra trying to delete nonexisting file,CASSANDRA-2206,12499221,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,tbritz,tbritz,2/21/2011 11:57,3/12/2019 14:16,3/13/2019 22:24,2/24/2011 15:14,0.7.3,,,,0,,,,,,"Hi,

On one of our nodes, startup fails due to cassandra trying to delete a nonexistant ""Data"" file (see below).

Why that file is missing is another mistery... The log file entries don't show any ERROR messages before cassandra restarted (for reasons I don't know) and this error occured.

Directory listing:

total 109M
-rw-r--r-- 1 root root  51M 2011-02-21 05:25 table_task-f-1666-Data.db
-rw-r--r-- 1 root root 243K 2011-02-21 05:25 table_task-f-1666-Filter.db
-rw-r--r-- 1 root root 6.1M 2011-02-21 05:25 table_task-f-1666-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 05:25 table_task-f-1666-Statistics.db
-rw-r--r-- 1 root root 9.8M 2011-02-21 11:36 table_task-f-1703-Data.db
-rw-r--r-- 1 root root  57K 2011-02-21 11:36 table_task-f-1703-Filter.db
-rw-r--r-- 1 root root 1.3M 2011-02-21 11:36 table_task-f-1703-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:36 table_task-f-1703-Statistics.db
-rw-r--r-- 1 root root 292K 2011-02-21 11:42 table_task-f-1704-Data.db
-rw-r--r-- 1 root root 1.7K 2011-02-21 11:42 table_task-f-1704-Filter.db
-rw-r--r-- 1 root root  42K 2011-02-21 11:42 table_task-f-1704-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:42 table_task-f-1704-Statistics.db
-rw-r--r-- 1 root root 364K 2011-02-21 11:52 table_task-f-1705-Data.db
-rw-r--r-- 1 root root 2.0K 2011-02-21 11:52 table_task-f-1705-Filter.db
-rw-r--r-- 1 root root  50K 2011-02-21 11:52 table_task-f-1705-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:52 table_task-f-1705-Statistics.db
-rw-r--r-- 1 root root 535K 2011-02-21 12:10 table_task-f-1706-Data.db
-rw-r--r-- 1 root root 2.8K 2011-02-21 12:10 table_task-f-1706-Filter.db
-rw-r--r-- 1 root root  70K 2011-02-21 12:10 table_task-f-1706-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 12:10 table_task-f-1706-Statistics.db
-rw-r--r-- 1 root root  11M 2011-02-21 12:11 table_task-f-1707-Data.db
-rw-r--r-- 1 root root  18M 2011-02-21 09:47 table_task_meta-f-417-Data.db
-rw-r--r-- 1 root root 271K 2011-02-21 09:47 table_task_meta-f-417-Filter.db
-rw-r--r-- 1 root root 6.7M 2011-02-21 09:47 table_task_meta-f-417-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 09:47 table_task_meta-f-417-Statistics.db
-rw-r--r-- 1 root root 1.2M 2011-02-21 10:47 table_task_meta-f-418-Data.db
-rw-r--r-- 1 root root  18K 2011-02-21 10:47 table_task_meta-f-418-Filter.db
-rw-r--r-- 1 root root 460K 2011-02-21 10:47 table_task_meta-f-418-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 10:47 table_task_meta-f-418-Statistics.db
-rw-r--r-- 1 root root 791K 2011-02-21 11:47 table_task_meta-f-419-Data.db
-rw-r--r-- 1 root root  13K 2011-02-21 11:47 table_task_meta-f-419-Filter.db
-rw-r--r-- 1 root root 311K 2011-02-21 11:47 table_task_meta-f-419-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:47 table_task_meta-f-419-Statistics.db
-rw-r--r-- 1 root root  57K 2011-02-21 12:11 table_task-tmp-f-1707-Filter.db
-rw-r--r-- 1 root root 1.4M 2011-02-21 12:11 table_task-tmp-f-1707-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 12:11 table_task-tmp-f-1707-Statistics.db


Cassandra log:

/software/cassandra/bin/cassandra
rm: cannot remove `/software/cassandra/lib/jna.jar': No such file or directory
root@intr1n3:/cassandra/data/table_task#  INFO 12:47:29,020 Logging initialized
 INFO 12:47:29,030 Heap size: 2614493184/2614493184
 INFO 12:47:29,031 JNA not found. Native methods will be disabled.
 INFO 12:47:29,038 Loading settings from file:/software/cassandra/conf/cassandra.yaml
 INFO 12:47:29,320 DiskAccessMode is standard, indexAccessMode is mmap
 INFO 12:47:29,332 Creating new commitlog segment /hd1/cassandra_md5/commitlog/CommitLog-1298288849332.log
 INFO 12:47:29,422 Opening /cassandra/data/system/Schema-f-244
 INFO 12:47:29,434 Opening /cassandra/data/system/Migrations-f-244
 INFO 12:47:29,437 Opening /cassandra/data/system/LocationInfo-f-137
 INFO 12:47:29,440 Opening /cassandra/data/system/HintsColumnFamily-f-352
 INFO 12:47:29,441 Opening /cassandra/data/system/HintsColumnFamily-f-353
 INFO 12:47:29,465 Loading schema version 54bc134e-2229-11e0-9159-fdf0b6b4b562
 WARN 12:47:29,623 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
ERROR 12:47:29,638 Exception encountered during startup.
java.io.IOError: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:145)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:468)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:153)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
Caused by: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:51)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:41)
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:133)
        ... 4 more
Exception encountered during startup.
java.io.IOError: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:145)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:468)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:153)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
Caused by: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:51)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:41)
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:133)
        ... 4 more
",linux,,3600,3600,,0%,3600,3600,,,,,,,,,,,,23/Feb/11 23:44;jbellis;2206-v2.txt;https://issues.apache.org/jira/secure/attachment/12471786/2206-v2.txt,21/Feb/11 14:52;jbellis;2206.txt;https://issues.apache.org/jira/secure/attachment/12471565/2206.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,37:08.3,,,no_permission,,,,,,,,,,,,20503,,,Thu Feb 24 15:39:39 UTC 2011,,,,,,0|i0g9yv:,93045,gdusbabek,gdusbabek,,,,,,,,,"21/Feb/11 14:37;jbellis;You can manually fix this by removing the ""tmp"" from all the tmp-f-1707 files.",21/Feb/11 14:51;jbellis;Patch to fix this for new files by making sure we rename -Data last.,"23/Feb/11 07:57;mdennis;renaming -data files last doesn't actually fix the problem.

You can test this by running

{noformat}
cd /var/lib/cassandra/data/system
touch Schema-tmp-f-763-Data.db  Schema-f-763-Filter.db
{noformat}

before starting cassandra.

This would simulate a crash between renaming all the the components besides -data and renaming -data (though really a crash any time between when the first component is renamed and before -data is renamed will exhibit the issue).
","23/Feb/11 23:44;jbellis;v2 also fixes a bug where tmp and non-tmp Descriptors were considered equal, which is what confused the file scanning for scrub in your example",24/Feb/11 15:12;gdusbabek;+1,24/Feb/11 15:14;jbellis;committed,"24/Feb/11 15:39;hudson;Integrated in Cassandra-0.7 #316 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/316/])
    improve detection and cleanup ofpartially-written sstables
patch by jbellis; reviewed by gdusbabek for CASSANDRA-2206
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot get range slice of super columns in reversed order,CASSANDRA-2212,12499291,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,muga_nishizawa,muga_nishizawa,2/22/2011 2:36,3/12/2019 14:16,3/13/2019 22:24,2/23/2011 17:55,0.6.13,0.7.3,,,1,,,,,,"I cannot get range slice of super columns in reversed order.  These data are stored in Cassandra in advance.  On the other hand, range slice of these data in normal order can be acquired.

You can reproduce the bug by executing attached programs.  
- 1. Start Cassandra daemon on localhost (number of thrift port is 9160)
- 2. Create keyspace and column family, according to ""create_table.cli"", 
- 3. Execute ""cassandra_sample_insert.py"", storing pairs of row keys and super columns
- 4. Execute ""cassandra_sample_rangeslice.py"" and get range slice of stored super columns
""cassandra_sample_insert.py"" and ""cassandra_sample_rangeslice.py"" require pycassa.  

You will need to execute 4.""cassandra_sample_rangeslice.py"" with following options so that you get range slice of super columns in reversed order.  

 % python cassandra_sample_rangeslice.py -r 00082 00083

On the other hand, to get range slice in normal order, you will need to use following options.  

 % python cassandra_sample_rangeslice.py -f 00082 00083

00082 and 00083 are the specified key range.  Range slice can be acquired in normal order but, I cannot get it in reversed order.  

I assume that there may be a bug within the code for acquiring the index block of specified range.  In fact, 00083 is included in gap between lastName of index block and firstName of next index block.   ","Fedore 11, Intel Core i5",,21600,21600,,0%,21600,21600,,,,,,,,,,,,23/Feb/11 14:53;slebresne;0001-Fix-IndexHelp.indexFor-for-reverse-query.patch;https://issues.apache.org/jira/secure/attachment/12471739/0001-Fix-IndexHelp.indexFor-for-reverse-query.patch,18/Mar/11 14:17;slebresne;2212_0.6.patch;https://issues.apache.org/jira/secure/attachment/12473997/2212_0.6.patch,22/Feb/11 02:39;muga_nishizawa;cassandra_sample_insert.py;https://issues.apache.org/jira/secure/attachment/12471596/cassandra_sample_insert.py,22/Feb/11 02:40;muga_nishizawa;cassandra_sample_rangeslice.py;https://issues.apache.org/jira/secure/attachment/12471597/cassandra_sample_rangeslice.py,22/Feb/11 02:39;muga_nishizawa;create_table.cli;https://issues.apache.org/jira/secure/attachment/12471595/create_table.cli,,,,,,,,,5,,,,,,,,,,,,,,,,,,,11:53.3,,,no_permission,,,,,,,,,,,,20508,,,Fri Mar 18 18:11:34 UTC 2011,,,,,,0|i0ga07:,93051,jbellis,jbellis,,,,,,,,,"22/Feb/11 05:11;thobbs;I am able to reproduce this with revision r1072164 (from Feb. 18, a couple days after 0.7.2) on a single node.  Other column ranges work as expected for reversed slices.","23/Feb/11 14:53;slebresne;Thanks a lot Muga for the test script, and you are right, this a bug in getting the index block during reverse queries.

Patch attached with unit tests for IndexHelper.","23/Feb/11 17:15;jbellis;Sylvain, can you summarize the bug and how this patch fixes it?","23/Feb/11 17:32;slebresne;Sure.

The problem is that we were not picking the right index slot for reverse query. Let's take the example from the unit test, and say your index look like this:
  [0..5][10..15][20..25]

And say you look for the slice [13..17]. When doing forward slice, we we doing a binary search comparing 13 (the start of the query) to the lastName part of the index slot, which is fine. You'll end up with the ""first"" slot, going from left to right, that may contain the start.

When doing a reverse query, we were doing the same thing, only using as a start column the end of the query, aka 17 in my example. However, comparing 17 with the lastName of each index slot, you end up selecting the last slot, which is wrong (the slice exit early since 17 is not in the range).

What you want to do is pick the ""first"" slot, but now going from right to left, that may contain start. So you want to find the slot where firstName > start and take the slot just before.

I hope I'm clear. Anyway, that's what the patch does. ","23/Feb/11 17:55;jbellis;committed, with your explanation added as a comment to indexFor :)","23/Feb/11 18:54;hudson;Integrated in Cassandra-0.7 #311 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/311/])
    fix reversed slice queries on large rows
patch by slebresne; reviewed by jbellis for CASSANDRA-2212
",18/Mar/11 08:13;paulrbrown;I think that I have a situation where this occurs against 0.6.8 as well.  Is this fix suitable for backporting onto 0.6?,18/Mar/11 14:17;slebresne;Attaching patch against 0.6,18/Mar/11 18:11;jbellis;committed for 0.6.13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOFException during name query,CASSANDRA-2165,12498670,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,slebresne,slebresne,2/15/2011 18:11,3/12/2019 14:16,3/13/2019 22:24,2/15/2011 20:42,0.7.2,,,,0,EOF,,,,,"As reported by Jonas Borgstrom on the mailing list:

{quote}
While testing the new 0.7.1 release I got the following exception:

ERROR [ReadStage:11] 2011-02-15 16:39:18,105
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.io.IOError: java.io.EOFException
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:75)
       at
org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:59)
       at
org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
       at
org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1274)
       at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1166)
       at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
       at org.apache.cassandra.db.Table.getRow(Table.java:384)
       at
org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:60)
       at
org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:473)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
       at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
       at java.io.DataInputStream.readInt(DataInputStream.java:392)
       at
org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:48)
       at
org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:30)
       at
org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:108)
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:106)
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:71)
       ... 12 more

{quote}",,,3600,3600,,0%,3600,3600,,,,,,,,CASSANDRA-2234,,,,15/Feb/11 18:13;slebresne;0001-Fix-bad-signed-conversion-from-byte-to-int.patch;https://issues.apache.org/jira/secure/attachment/12471093/0001-Fix-bad-signed-conversion-from-byte-to-int.patch,15/Feb/11 19:35;jbellis;2165-1.txt;https://issues.apache.org/jira/secure/attachment/12471101/2165-1.txt,15/Feb/11 19:38;jbellis;2165-2.txt;https://issues.apache.org/jira/secure/attachment/12471102/2165-2.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,35:52.3,,,no_permission,,,,,,,,,,,,20476,,,Fri Feb 18 20:03:01 UTC 2011,,,,,,0|i0g9pr:,93004,slebresne,slebresne,,,,,,,,,"15/Feb/11 18:13;slebresne;The problem is when creating an inputStream from a ByteBuffer. We directly return the result of BB.get(), but this can be negative which breaks the method contract.

The fix is the return of the get(), the addition of available() is more of an improvement.","15/Feb/11 19:35;jbellis;patch 1 provides a long-test that exposes the bug.  this requires turning off row caching in the test keyspaces to keep them from covering up the error.  for good measure, key caching is also tured off except in KeyCacheSpace / KeyCacheTest.",15/Feb/11 19:38;jbellis;patch 2 fixes the bug,15/Feb/11 20:42;jbellis;committed,"15/Feb/11 21:43;hudson;Integrated in Cassandra-0.7 #280 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/280/])
    fix column bloom filter deserialization
patch by jbellis and slebresne for CASSANDRA-2165
","18/Feb/11 20:03;jbellis;For those not following the mailing list: this is a read-time error, upgrading to 0.7.2 fixes the problem with no data loss.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
saved caches written with BufferedRandomAccessFile cannot be read by ObjectInputStream,CASSANDRA-2174,12498718,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mdennis,mdennis,mdennis,2/16/2011 4:35,3/12/2019 14:16,3/13/2019 22:24,2/16/2011 21:38,0.7.3,,,,0,,,,,,The CacheWriter is currently writing with BufferedRandomAccessFile which is incompatible with ObjectInputStream resulting in stack traces about corrupted stream headers when loading a saved cache.,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,16/Feb/11 20:21;mdennis;2174-cassandra-0.7.txt;https://issues.apache.org/jira/secure/attachment/12471220/2174-cassandra-0.7.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,05:28.7,,,no_permission,,,,,,,,,,,,20480,,,Wed Feb 16 22:52:17 UTC 2011,,,,,,0|i0g9rr:,93013,jbellis,jbellis,,,,,,,,,16/Feb/11 04:38;mdennis;patch is on top of CASSANDRA-2172,"16/Feb/11 06:05;jbellis;We don't use ObjectInputStream for cache reads anymore, and we need to use BRAF to get the non-buffer-cache-clobbering effect on cache writes.

We don't care about 0.7.0 reading 0.7.1/2 caches, since we made the choice to allow 0.7.1 to write new-format data files (CASSANDRA-1555) which is a much bigger incompatibility.

If 0.7.1/2 can't read 0.7.0, then we goofed, but changing it back (breaking 0.7.2 -> 0.7.3) seems like too late.","16/Feb/11 06:17;jbellis;Oops, I applied the patch and _then_ looked for uses of OIS.  Those operations are not commutative. :)","16/Feb/11 06:19;jbellis;So...  the important fix is the CFS change, and the CacheWriter change is gratuitous?","16/Feb/11 20:21;mdennis;I neglected to see the skipCache flag and was intending to make the read/write code symmetric.

attached patch contains only CFS change from OIS to DIS. ",16/Feb/11 21:38;jbellis;committed,"16/Feb/11 22:52;hudson;Integrated in Cassandra-0.7 #282 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/282/])
    read saved caches with DataInputStream
patch by mdennis; reviewed by jbellis for CASSANDRA-2174
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadCallback AssertionError: resolver.getMessageCount() <= endpoints.size(),CASSANDRA-2282,12500665,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,thobbs,thobbs,3/7/2011 19:34,3/12/2019 14:16,3/13/2019 22:24,3/9/2011 14:58,0.7.4,,,,0,,,,,,"In a three node cluster with RF=2, when trying to page through all rows with get_range_slices() at CL.ONE, I get timeouts on the client side.  Looking at the Cassandra logs, all of the nodes show the following AssertionError repeatedly:

{noformat}
ERROR [RequestResponseStage:2] 2011-03-07 19:10:27,527 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.AssertionError
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR [RequestResponseStage:2] 2011-03-07 19:10:27,529 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[RequestResponseStage:2,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

The nodes are all running 0.7.3.  The cluster was at size 3 before any data was inserted, and everything else appears perfectly healthy.",,,,,,,,,,,,,,,,,,,,08/Mar/11 18:02;jbellis;2282-v2.txt;https://issues.apache.org/jira/secure/attachment/12473017/2282-v2.txt,08/Mar/11 16:34;jbellis;2282.txt;https://issues.apache.org/jira/secure/attachment/12472984/2282.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,26:35.9,,,no_permission,,,,,,,,,,,,20538,,,Fri Apr 22 05:59:19 UTC 2011,,,,,,0|i0gag7:,93123,thobbs,thobbs,,,,,,,,,"08/Mar/11 04:26;skamio;In our case, the same AssertionError occurs on multi node cluster with replication factor = 3 (0.7.3 release version).
Feeding data into cassandra looks ok (consistency level = QUORUM). Though, UnavailableException was received via hector 0.7.0-28 several times. It warns about number of replica (see the following stack trace). It might relate to this problem. But not sure. 

When querying data, AssertionError occurs in cassandra and client gets timedout exception.
Our client queries in several query types in different column families. This timeout occurs quite often in secondary index query.
The error is only logged on the host which client connects via thrift (according to timestamp in log).

Another experience is when I tried to retrieve data via CLI.
Query like ""list Standard1 limit 10"" returns results normally. But the cassandra logs the AssertionError on the host. Other node doesn't have the log.
(When query returns ""null"" (I guess there is not enough replica), this exception is very likely to be logged.)

-------------------
* Unavailable exception stack trace received via hector 0.7.0-28 when feeding data into cassandra:

me.prettyprint.hector.api.exceptions.HUnavailableException: : May not be enough replicas present to handle consistency level.
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:52)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$1.execute(KeyspaceServiceImpl.java:95)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$1.execute(KeyspaceServiceImpl.java:88)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:221)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:129)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.batchMutate(KeyspaceServiceImpl.java:100)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.batchMutate(KeyspaceServiceImpl.java:106)
        at me.prettyprint.cassandra.model.MutatorImpl$2.doInKeyspace(MutatorImpl.java:203)
        at me.prettyprint.cassandra.model.MutatorImpl$2.doInKeyspace(MutatorImpl.java:200)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
        at me.prettyprint.cassandra.model.MutatorImpl.execute(MutatorImpl.java:200)
        at jp.co.rakuten.gsp.cassandra_connector.feeder.CassandraFeeder.batchInsert(CassandraFeeder.java:506)
        at jp.co.rakuten.gsp.purchase_history.cassandra_connector.PHCassandraFeeder.consume(PHCassandraFeeder.java:240)
        at jp.co.rakuten.gsp.cassandra_connector.feeder.CassandraFeeder.process(CassandraFeeder.java:330)
        at jp.co.rakuten.gsp.cassandra_connector.feeder.Feeder.run(Feeder.java:164)
        at java.lang.Thread.run(Thread.java:662)
Caused by: UnavailableException()
        at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:16485)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:916)
        at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:890)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$1.execute(KeyspaceServiceImpl.java:93)
        ... 16 more

","08/Mar/11 14:04;terjem;From ReadCallback.java


        this.endpoints = repair || resolver instanceof RowRepairResolver
                       ? endpoints
                       : endpoints.subList(0, Math.min(endpoints.size(), blockfor)); // min so as to not throw exception until assureSufficient is called

This will cut the list of endpoints to whatever is the consistency requirement (in the case where repair is false, which for instance happens all the time for a rangescan).

Later:
assert resolver.getMessageCount() <= endpoints.size()

which will cause an assert if all nodes answers on a range request (or if you have a random readrepair).

I am actually not 100% sure what the fix is right now as I have not had time to scan the rest of the code and need to leave office for today, but that is the problem anyway :)
To be honest, the logic here may look a bit broken.

The assert happening here may good either. The error condition is not returned to the client, so it will hang around waiting for a timeout to occur.
Maybe the code should throw some exception before the assert?

Something like
assert resolver.getMessageCount() <= endpoints.size() :  ""Got "" + resolver.getMessageCount() + "" replies but only know "" + endpoints.size() + "" endpoints"";
may also be good?","08/Mar/11 14:06;terjem;Just to make this very clear.
The list of endpoints is cut to the consistency requirement so if:
- The request requires gossip with more than one node
- all nodes answers
- you do not use consistencylevel ALL,
then the assert will trigger.",08/Mar/11 16:34;jbellis;The bug is that range queries were not restricting the set of endpoints it queries to handler.endpoints the way single-row queries do. Fix attached with a couple extra comments.,"08/Mar/11 17:57;skamio;Hi Jonathan,

I tried your patch. But still the assertion happens in our cluster. Query is IndexedSlicesQuery with QUORUM.


ERROR [RequestResponseStage:23] 2011-03-09 02:47:23,343 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[RequestResponseStage:23,5,main]
java.lang.AssertionError: Got 3 replies but requests were only sent to 2 endpoints
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:129)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [RequestResponseStage:10] 2011-03-09 02:48:48,463 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[RequestResponseStage:10,5,main]
java.lang.AssertionError: Got 3 replies but requests were only sent to 2 endpoints
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:129)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
",08/Mar/11 18:02;jbellis;v2 adds same fix for indexed queries,"08/Mar/11 21:40;jbellis;Workaround: remove this line from cassandra-env.sh

{noformat}
JVM_OPTS=""$JVM_OPTS -ea""
{noformat}",08/Mar/11 21:50;thobbs;v2 patch fixes the issue for me.,"09/Mar/11 03:18;skamio;v2 patch works for me too. Server side errors are gone.

My client still gets TimedOut exception. But maybe this is a different problem.","09/Mar/11 14:55;jbellis;bq. My client still gets TimedOut exception. But maybe this is a different problem.

real timeouts can still happen. :)

i'd suggest 1) checking for dropped message warnings in the log; if there aren't any, 2) enabling debug logging to see where the commands are going and not coming back from in time.",09/Mar/11 14:58;jbellis;committed,"09/Mar/11 15:08;skamio;I'm trying to find out when the timeout happens. So, I put logging to CassandraServer.java and  ReadCallback.java just before timeout exception.
I saw Timeout occurs even when 2 responses are received for query with quorum. 2 responses should be enough for quorum. Is it correct behavior?

------
 INFO [pool-1-thread-44] 2011-03-09 23:34:47,685 ReadCallback.java (line 125) Operation timed out - received only 2 responses from /xx.xx.xx.67, /xx.xx.xx.68, command class = org.apache.cassandra.db.SliceFromReadCommand
 INFO [pool-1-thread-44] 2011-03-09 23:34:47,686 CassandraServer.java (line 110) --- timed out: Operation timed out - received only 2 responses from /xx.xx.xx.67, /xx.xx.xx.68,  .
-----
","09/Mar/11 15:16;hudson;Integrated in Cassandra-0.7 #366 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/366/])
    update CHANGES for CASSANDRA-2282 that got committed already (in r1079812 I think)
",09/Mar/11 15:21;jbellis;And what does the Blockfor/repair is %s/%s; setting up requests to %s debug line show?,"09/Mar/11 15:39;skamio;Timeout setting in cassandra.yaml is 15sec. So, the following must be the line. IP address of cassandra node which has the log is xx.xx.xx.70.

----
DEBUG [pool-1-thread-121] 2011-03-09 23:34:32,726 ReadCallback.java (line 88) Blockfor/repair is 2/true; setting up requests to /xx.xx.xx.66,/xx.xx.xx.67,/xx.xx.xx.68
----
","09/Mar/11 17:04;jbellis;Sounds like you're getting replies back from digest request but not data.  In StorageProxy:

{code}
            // The data-request message is sent to dataPoint, the node that will actually get
            // the data for us. The other replicas are only sent a digest query.
{code}

Note that debug logs show which node gets the data request.","22/Apr/11 05:59;stuhood;> Sounds like you're getting replies back from digest request but not data.
Created CASSANDRA-2540",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming Old Format Data Fails in 0.7.3 after upgrade from 0.6.8,CASSANDRA-2283,12500677,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,eonnen,eonnen,3/7/2011 21:00,3/12/2019 14:16,3/13/2019 22:24,4/18/2011 21:58,0.7.5,,,,0,,,,,,"After successfully upgrading a 0.6.8 ring to 0.7.3, we needed to bootstrap in a new node relatively quickly. When starting the new node with an assigned token in auto bootstrap mode, we see the following exceptions on the new node:

INFO [main] 2011-03-07 10:37:32,671 StorageService.java (line 505) Joining: sleeping 30000 ms for pending range setup
 INFO [main] 2011-03-07 10:38:02,679 StorageService.java (line 505) Bootstrapping
 INFO [HintedHandoff:1] 2011-03-07 10:38:02,899 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.211.14.200
 INFO [HintedHandoff:1] 2011-03-07 10:38:02,900 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.211.14.200
 INFO [CompactionExecutor:1] 2011-03-07 10:38:04,924 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuff-f-1
 INFO [CompactionExecutor:1] 2011-03-07 10:38:05,390 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuff-f-2
 INFO [CompactionExecutor:1] 2011-03-07 10:38:05,768 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-1
 INFO [CompactionExecutor:1] 2011-03-07 10:38:06,389 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-2
 INFO [CompactionExecutor:1] 2011-03-07 10:38:06,581 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-3
ERROR [CompactionExecutor:1] 2011-03-07 10:38:07,056 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [CompactionExecutor:1] 2011-03-07 10:38:08,480 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-5
 INFO [CompactionExecutor:1] 2011-03-07 10:38:08,582 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid_reg_idx-f-1
ERROR [CompactionExecutor:1] 2011-03-07 10:38:08,635 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [CompactionExecutor:1] 2011-03-07 10:38:08,666 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [CompactionExecutor:1] 2011-03-07 10:38:08,855 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid_reg_idx-f-4

Two attempts to bootstrap in the new node both exhibited this behavior. On the node owning the tokens being migrated, stream activity is visible but doesn't update any progress I think due to the issues on the receiving host.







Lastly, just case it's relevant, we had an EC2 node die underneath us during the upgrade so not all nodes were drained. This didn't affect the upgrade but I wanted to note it her to be thorough.","0.7.3 upgraded from 0.6.8, Linux Java HotSpot(TM) 64-Bit Server VM (build 17.1-b03, mixed mode)",,,,,,,,,,,,,,,,,,,28/Mar/11 14:36;jbellis;2283.txt;https://issues.apache.org/jira/secure/attachment/12474779/2283.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,52:01.9,,,no_permission,,,,,,,,,,,,20539,,,Mon Apr 18 22:21:41 UTC 2011,,,,,,0|i0gagf:,93124,stuhood,stuhood,,,,,,,,,09/Mar/11 14:52;jbellis;CASSANDRA-2296 is going to cause streaming trouble too. In other words: scrub won't fix this unless run w/ a build that has 2296 applied.,"09/Mar/11 17:10;jbellis;bq. scrub won't fix this

that is, for sstables containing expired tombstones","12/Mar/11 20:31;stuhood;While not ideal, this is actually supposed to throw an exception in SSTableWriter.createBuilder, but that is dependent on a correctly versioned Descriptor being created on the destination side. I expect that streaming is dropping the source version when it creates the destination descriptor.","24/Mar/11 20:54;eonnen;Ok, I can confirm that after upgrading to 0.7.4 where 2296 was applied, and after performing a scrub, we were able to bootstrap in new nodes again.",25/Mar/11 19:03;jbellis;IMO the right thing to do here is to deserialize enough of the data sent during stream to (a) rewrite it to latest format and (b) write bloom filter and row index -- currently this is done in a second pass post-stream.,"26/Mar/11 09:51;stuhood;> (a) rewrite it to latest format and (b) write bloom filter and row index
I was hoping that we could get away with just doing (b) in order to avoid having to re-write the data file, but it certainly simplifies things to re-write in the current format.

EDIT: CASSANDRA-2336 took a step toward allowing rebuilding and index writing to be version specific, in order to implement (b). I'm most of the way through an implementation of CASSANDRA-2319 on top of it, but I don't see a clear answer for a/b/a+b.","28/Mar/11 14:34;jbellis;You're right, making it one-pass isn't feasible without writing the streamed row out to a temporary file first, since we don't have a way to rebuild the row-level bloom filter + block index.  In other words, not really any more one-pass than the existing approach.",28/Mar/11 14:36;jbellis;Patch to preserve version across streams. Also removes obsolete component field from PendingFile (we only stream Component.DATA).,12/Apr/11 22:13;stuhood;Looks good: only comment is that BootstrapTest should probably purposely use an old version and check that it is preserved.,"18/Apr/11 21:58;jbellis;added version check to BootstrapTest.

committed w/ just the version changes -- left PendingFile alone. (CASSANDRA-2438 suggests we might want to stream fully-formed sstables for bulk load.)","18/Apr/11 22:21;hudson;Integrated in Cassandra-0.7 #442 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/442/])
    preserve version when streaming data from old sstables
patch by jbellis; reviewed by Stu Hood for CASSANDRA-2283
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant javadoc fails on windows,CASSANDRA-2248,12499705,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,norman,norman,norman,2/25/2011 12:47,3/12/2019 14:16,3/13/2019 22:24,2/25/2011 15:06,0.7.3,,Packaging,,0,,,,,,"When try to run ""ant javadoc"" (or any task that include javadoc) on windows it fails with the error:

Javadoc failed: java.io.IOException: Cannot run program ""c:\Program Files\Java\jdk1.6.0_17\bin\javadoc.exe"": CreateProcess error=87, The parameter is incorrect","windows 7, ant 1.8.2",,,,,,,,,,,,,,,,,,,25/Feb/11 12:49;norman;CASSANDRA-2248.diff;https://issues.apache.org/jira/secure/attachment/12471927/CASSANDRA-2248.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,06:10.0,,,no_permission,,,,,,,,,,,,20524,,,Fri Feb 25 15:37:46 UTC 2011,,,,,,0|i0ga8n:,93089,jbellis,jbellis,,,,,,,,,25/Feb/11 12:49;norman;This fix the build on windows.,25/Feb/11 13:01;norman;Related to this: https://issues.apache.org/bugzilla/show_bug.cgi?id=41958,"25/Feb/11 15:06;jbellis;committed, thanks!","25/Feb/11 15:37;hudson;Integrated in Cassandra-0.7 #322 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/322/])
    fix ant javadoc on Windows
patch by Norman Maurer; reviewed by jbellis for CASSANDRA-2248
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool scrub hangs or throws an exception,CASSANDRA-2240,12499601,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,kunda,kunda,2/24/2011 15:32,3/12/2019 14:16,3/13/2019 22:24,3/1/2011 16:10,0.7.3,,Tool/nodetool,,0,,,,,,"trying to run nodetool scrub hung or (only happened one time) threw the following exception:

ERROR [CompactionExecutor:1] 2011-02-28 10:26:26,620 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
        at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:538)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
",using build #314 from hudson,,57600,57600,,0%,57600,57600,,,,,,,,,,,,01/Mar/11 02:49;jbellis;2240-v2.txt;https://issues.apache.org/jira/secure/attachment/12472270/2240-v2.txt,01/Mar/11 04:45;jbellis;2240-v3.txt;https://issues.apache.org/jira/secure/attachment/12472278/2240-v3.txt,01/Mar/11 13:22;slebresne;2240-v5.patch;https://issues.apache.org/jira/secure/attachment/12472305/2240-v5.patch,01/Mar/11 14:46;jbellis;2240-v6.txt;https://issues.apache.org/jira/secure/attachment/12472313/2240-v6.txt,01/Mar/11 05:30;jbellis;2240.txt;https://issues.apache.org/jira/secure/attachment/12472279/2240.txt,28/Feb/11 23:16;jbellis;2240.txt;https://issues.apache.org/jira/secure/attachment/12472250/2240.txt,27/Feb/11 10:44;kunda;exception2.txt;https://issues.apache.org/jira/secure/attachment/12472102/exception2.txt,27/Feb/11 10:34;kunda;jstack1.txt;https://issues.apache.org/jira/secure/attachment/12472100/jstack1.txt,27/Feb/11 10:34;kunda;signatureBuckets-f-104.tar.gz;https://issues.apache.org/jira/secure/attachment/12472101/signatureBuckets-f-104.tar.gz,28/Feb/11 08:07;kunda;system.log.2.gz;https://issues.apache.org/jira/secure/attachment/12472152/system.log.2.gz,28/Feb/11 08:01;kunda;system.log.gz;https://issues.apache.org/jira/secure/attachment/12472151/system.log.gz,24/Feb/11 15:41;kunda;test-0.6.x-tables.tar.gz;https://issues.apache.org/jira/secure/attachment/12471844/test-0.6.x-tables.tar.gz,27/Feb/11 10:44;kunda;userChannelFilter-f-210.tar.gz;https://issues.apache.org/jira/secure/attachment/12472103/userChannelFilter-f-210.tar.gz,13,,,,,,,,,,,,,,,,,,,37:07.3,,,no_permission,,,,,,,,,,,,20521,,,Tue Mar 01 20:15:08 UTC 2011,,,,,,0|i0ga6v:,93081,slebresne,slebresne,,,,,,,,,"24/Feb/11 15:37;jbellis;what is going on in the compactionmanager when it's ""hung?""  (use jstack)",24/Feb/11 15:41;kunda;attached the tables that can be used to reproduce the hang,"24/Feb/11 16:03;jbellis;This is not a valid sstable.  It claims (from its lack of version string) that it contains encoded row keys, meaning <token>:<key>, but it actually does not.  scrub can't help you with that.","24/Feb/11 16:07;jbellis;... on closer inspection it does have colon-delimited keys, but scrub doesn't see them.  ","24/Feb/11 16:08;kunda;Here's the trace I got (narrowed down):

""CompactionExecutor:1"" prio=10 tid=0x000000001eb67800 nid=0x7fdb runnable [0x0000000040dc4000]
   java.lang.Thread.State: RUNNABLE
        at java.io.DataInputStream.readFully(DataInputStream.java:195)
        at java.io.DataInputStream.readLong(DataInputStream.java:416)
        at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:51)
        at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:30)
        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:108)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:86)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:549)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

   Locked ownable synchronizers:
        - <0x00002aaabd5f71c8> (a java.util.concurrent.ThreadPoolExecutor$Worker)
        - <0x00002aaabd6cd008> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)","24/Feb/11 19:37;jbellis;What do you see when you apply the patch for CASSANDRA-2241?

What create column family statement should I give the cli, to create this userActionUtilsKey CF?","24/Feb/11 19:46;jbellis;also, can you attach the system.log from when you started cassandra?  we're trying to figure out why it's using the new-version BloomFilterSerializer, when it should be using LegacyBloomFilterSerializer.","24/Feb/11 19:50;kunda;I didn't have time to apply the patch yet - but I will next week.
Regarding the example CF - as [~slebresne] commented in CASSANDRA-2217, it was indeed created in Cassandra 0.6.5 -
so the cli cannot be used to create it ;)

However, here is KS xml element that defined it in storage-conf.xml:
{code:xml} 
    <Keyspace Name=""generalUtils"">
      <ColumnFamily Name=""userActionUtilsKey"" CompareWith=""UTF8Type"" />
      <ColumnFamily Name=""facebookShowIds"" CompareWith=""UTF8Type"" />

      <ReplicaPlacementStrategy>org.apache.cassandra.locator.RackUnawareStrategy</ReplicaPlacementStrategy>
      <ReplicationFactor>1</ReplicationFactor>
      <EndPointSnitch>org.apache.cassandra.locator.EndPointSnitch</EndPointSnitch>
    </Keyspace>
{code}","24/Feb/11 19:50;jbellis;When I create a keyspace and CF w/ default settings and scrub it, I get

{noformat}
 INFO 13:49:17,594 Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/userActionUtilsKey-9-Data.db')
 INFO 13:49:17,856 Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/userActionUtilsKey-9-Data.db') complete
{noformat}","24/Feb/11 20:14;kunda;I had to go back 26 x 20MiB logs consisting mostly of lines such as:
{code} INFO [CompactionExecutor:1] 2011-02-24 11:18:10,262 SSTableIdentityIterator.java (line 90) Invalid bloom filter in SSTableReader(path='/vm1/cassandraDB/data/Keyspace2/ruleGroup-f-243-Data.db'); will rebuild it {code}
which were preceded by a bunch of NegativeArraySizeExceptions, which were after the initialization log:
{code}
INFO [main] 2011-02-24 11:08:43,595 AbstractCassandraDaemon.java (line 77) Logging initialized
 INFO [main] 2011-02-24 11:08:43,605 AbstractCassandraDaemon.java (line 97) Heap size: 8330936320/8331984896
 INFO [main] 2011-02-24 11:08:43,606 CLibrary.java (line 61) JNA not found. Native methods will be disabled.
 INFO [main] 2011-02-24 11:08:43,613 DatabaseDescriptor.java (line 121) Loading settings from file:/usr/local/apache-cassandra-2011-02-24_02-21-51/conf/cassandra.yaml
 INFO [main] 2011-02-24 11:08:43,821 DatabaseDescriptor.java (line 181) DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO [main] 2011-02-24 11:08:43,893 SSTable.java (line 147) Deleted /vm1/cassandraDB/data/system/LocationInfo-f-210
 INFO [main] 2011-02-24 11:08:43,894 SSTable.java (line 147) Deleted /vm1/cassandraDB/data/system/LocationInfo-f-212
 INFO [main] 2011-02-24 11:08:43,894 SSTable.java (line 147) Deleted /vm1/cassandraDB/data/system/LocationInfo-f-211
 INFO [main] 2011-02-24 11:08:43,894 SSTable.java (line 147) Deleted /vm1/cassandraDB/data/system/LocationInfo-f-209
 INFO [main] 2011-02-24 11:08:43,919 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/IndexInfo-f-5
 INFO [main] 2011-02-24 11:08:43,937 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/Schema-f-89
 INFO [main] 2011-02-24 11:08:43,945 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/Migrations-f-85
 INFO [main] 2011-02-24 11:08:43,947 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/Migrations-f-87
 INFO [main] 2011-02-24 11:08:43,948 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/Migrations-f-86
 INFO [main] 2011-02-24 11:08:43,951 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/LocationInfo-f-213
 INFO [main] 2011-02-24 11:08:43,953 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/LocationInfo-f-214
 INFO [main] 2011-02-24 11:08:43,982 DatabaseDescriptor.java (line 461) Loading schema version b6fdb590-3e9e-11e0-8d0e-34b74a661156
 WARN [main] 2011-02-24 11:08:44,128 DatabaseDescriptor.java (line 493) Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
 INFO [main] 2011-02-24 11:08:44,188 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommenders-f-176
 INFO [main] 2011-02-24 11:08:44,191 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommenders-f-177
 INFO [main] 2011-02-24 11:08:44,192 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommenders-f-178
 INFO [main] 2011-02-24 11:08:44,197 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/businessRule-f-200
 INFO [main] 2011-02-24 11:08:44,204 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommendTo-f-92
 INFO [main] 2011-02-24 11:08:44,205 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommendTo-f-93
 INFO [main] 2011-02-24 11:08:44,209 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/followers-f-175
 INFO [main] 2011-02-24 11:08:44,213 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/names-f-165
 INFO [main] 2011-02-24 11:08:44,218 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/ruleGroup-f-244
 INFO [main] 2011-02-24 11:08:44,218 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/ruleGroup-f-243
 INFO [main] 2011-02-24 11:08:44,225 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommendFrom-f-109
 INFO [main] 2011-02-24 11:08:44,233 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/channels-f-2866
 INFO [main] 2011-02-24 11:08:44,262 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/channels-f-2867
 INFO [main] 2011-02-24 11:08:44,286 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommendAgain-f-1425
 INFO [main] 2011-02-24 11:08:44,293 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userProfile-f-816
 INFO [main] 2011-02-24 11:08:44,294 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userProfile-f-815
 INFO [main] 2011-02-24 11:08:44,301 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6153
 INFO [main] 2011-02-24 11:08:44,302 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6039
 INFO [main] 2011-02-24 11:08:44,443 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6130
 INFO [main] 2011-02-24 11:08:44,498 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6040
 INFO [main] 2011-02-24 11:08:44,591 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6143
 INFO [main] 2011-02-24 11:08:44,608 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6036
 INFO [main] 2011-02-24 11:08:44,676 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6035
 INFO [main] 2011-02-24 11:08:44,852 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6038
 INFO [main] 2011-02-24 11:08:44,953 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6134
 INFO [main] 2011-02-24 11:08:44,960 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6114
 INFO [main] 2011-02-24 11:08:44,976 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6096
 INFO [main] 2011-02-24 11:08:45,040 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6147
 INFO [main] 2011-02-24 11:08:45,048 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6152
 INFO [main] 2011-02-24 11:08:45,062 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-5990
 INFO [main] 2011-02-24 11:08:45,087 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-6063
 INFO [main] 2011-02-24 11:08:45,109 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-6109
 INFO [main] 2011-02-24 11:08:45,110 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-5991
 INFO [main] 2011-02-24 11:08:45,136 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-6108
 INFO [main] 2011-02-24 11:08:45,154 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userChannelFilter-f-207
 INFO [main] 2011-02-24 11:08:45,154 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userChannelFilter-f-208
 INFO [main] 2011-02-24 11:08:45,162 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/popularCloudIds-f-203
 INFO [main] 2011-02-24 11:08:45,166 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userActions-f-1160
 INFO [main] 2011-02-24 11:08:45,177 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/headends-f-5793
 INFO [main] 2011-02-24 11:08:45,195 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/headends-f-5801
 INFO [main] 2011-02-24 11:08:45,196 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/headends-f-5761
 INFO [main] 2011-02-24 11:08:45,202 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/headends-f-5800
 INFO [main] 2011-02-24 11:08:45,213 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userActionLikes-f-10
 INFO [main] 2011-02-24 11:08:45,218 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userAttributes-f-2517
 INFO [main] 2011-02-24 11:08:45,219 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userAttributes-f-2533
 INFO [main] 2011-02-24 11:08:45,221 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userAttributes-f-2532
 INFO [main] 2011-02-24 11:08:45,230 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-102
 INFO [main] 2011-02-24 11:08:45,607 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-103
 INFO [main] 2011-02-24 11:08:45,799 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-88
 INFO [main] 2011-02-24 11:08:46,319 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-93
 INFO [main] 2011-02-24 11:08:46,846 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-63
 INFO [main] 2011-02-24 11:08:49,879 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-83
 INFO [main] 2011-02-24 11:08:51,340 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/userSignatures-22
 INFO [main] 2011-02-24 11:08:51,351 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/userSignatures-23
 INFO [main] 2011-02-24 11:08:51,355 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/userSignatures-17
 INFO [main] 2011-02-24 11:08:51,385 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/userSignatures-24
 INFO [main] 2011-02-24 11:08:51,390 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureFunctions-5
 INFO [main] 2011-02-24 11:08:51,394 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/generalUtils/userActionUtilsKey-9
 INFO [main] 2011-02-24 11:08:51,396 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/generalUtils/facebookShowIds-f-37
 INFO [main] 2011-02-24 11:08:51,398 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureBuckets-6
 INFO [main] 2011-02-24 11:08:51,527 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureBuckets-5
 INFO [main] 2011-02-24 11:08:51,903 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/userSignatures-1
 INFO [main] 2011-02-24 11:08:52,006 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/userSignatures-e-3
 INFO [main] 2011-02-24 11:08:52,009 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/userSignatures-2
 INFO [main] 2011-02-24 11:08:52,013 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureFunctions-2
 INFO [main] 2011-02-24 11:08:52,013 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureFunctions-1
 INFO [main] 2011-02-24 11:08:52,014 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureFunctions-e-3
 INFO [main] 2011-02-24 11:08:52,019 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/facebookActions-f-186
 INFO [main] 2011-02-24 11:08:52,019 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/facebookActions-f-185
 INFO [main] 2011-02-24 11:08:52,022 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/watchingUsers-f-477
 INFO [main] 2011-02-24 11:08:52,038 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/showState-e-35
 INFO [main] 2011-02-24 11:08:52,039 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/showState-e-37
 INFO [main] 2011-02-24 11:08:52,039 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/showState-e-36
 INFO [main] 2011-02-24 11:08:52,042 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/watchingUserHistory-f-462
 INFO [main] 2011-02-24 11:08:52,042 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/watchingUserHistory-f-461
 INFO [main] 2011-02-24 11:08:52,043 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/watchingUserHistory-f-463
 INFO [main] 2011-02-24 11:08:52,045 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/chats/chatUsers-f-129
 INFO [main] 2011-02-24 11:08:52,046 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/chats/chatHistory-f-129
 INFO [main] 2011-02-24 11:08:52,055 CommitLogSegment.java (line 50) Creating new commitlog segment /vm1/cassandraDB/commitlog/CommitLog-1298538532055.log
 INFO [main] 2011-02-24 11:08:52,061 CommitLog.java (line 155) Replaying /vm1/cassandraDB/commitlog/CommitLog-1298537386811.log
 INFO [main] 2011-02-24 11:08:52,073 CommitLog.java (line 311) Finished reading /vm1/cassandraDB/commitlog/CommitLog-1298537386811.log
 INFO [main] 2011-02-24 11:08:52,074 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for recommendAgain at CommitLogContext(file='/vm1/cassandraDB/commitlog/CommitLog-1298538532055.log', position=0)
 INFO [main] 2011-02-24 11:08:52,076 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-recommendAgain@205025794(636 bytes, 7 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,077 Memtable.java (line 157) Writing Memtable-recommendAgain@205025794(636 bytes, 7 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,113 Memtable.java (line 164) Completed flushing /vm1/cassandraDB/data/Keyspace2/recommendAgain-f-1426-Data.db (859 bytes)
 INFO [main] 2011-02-24 11:08:52,118 CommitLog.java (line 163) Log replay complete
 INFO [main] 2011-02-24 11:08:52,131 StorageService.java (line 354) Cassandra version: 2011-02-24_02-21-51
 INFO [main] 2011-02-24 11:08:52,131 StorageService.java (line 355) Thrift API version: 19.4.0
 INFO [main] 2011-02-24 11:08:52,132 StorageService.java (line 368) Loading persisted ring state
 INFO [main] 2011-02-24 11:08:52,132 StorageService.java (line 404) Starting up server gossip
 INFO [main] 2011-02-24 11:08:52,137 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/vm1/cassandraDB/commitlog/CommitLog-1298538532055.log', position=148)
 INFO [main] 2011-02-24 11:08:52,137 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-LocationInfo@1068086436(29 bytes, 1 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,138 Memtable.java (line 157) Writing Memtable-LocationInfo@1068086436(29 bytes, 1 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,178 Memtable.java (line 164) Completed flushing /vm1/cassandraDB/data/system/LocationInfo-f-215-Data.db (80 bytes)
 INFO [main] 2011-02-24 11:08:52,198 StorageService.java (line 468) Using saved token 51653040247566871911249877869558549493
 INFO [main] 2011-02-24 11:08:52,199 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/vm1/cassandraDB/commitlog/CommitLog-1298538532055.log', position=444)
 INFO [main] 2011-02-24 11:08:52,199 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-LocationInfo@832074392(53 bytes, 2 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,199 Memtable.java (line 157) Writing Memtable-LocationInfo@832074392(53 bytes, 2 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,230 Memtable.java (line 164) Completed flushing /vm1/cassandraDB/data/system/LocationInfo-f-216-Data.db (163 bytes)
 INFO [CompactionExecutor:1] 2011-02-24 11:08:52,231 CompactionManager.java (line 395) Compacting [SSTableReader(path='/vm1/cassandraDB/data/system/LocationInfo-f-213-Data.db'),SSTableReader(path='/vm1/cassandraDB/data/system/LocationInfo-f-214-Data.db'),SSTableReader(path='/vm1/cassandraDB/data/system/LocationInfo-f-215-Data.db'),SSTableReader(path='/vm1/cassandraDB/data/system/LocationInfo-f-216-Data.db')]
 INFO [main] 2011-02-24 11:08:52,233 Mx4jTool.java (line 72) Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO [CompactionExecutor:1] 2011-02-24 11:08:52,292 CompactionManager.java (line 482) Compacted to /vm1/cassandraDB/data/system/LocationInfo-tmp-f-217-Data.db.  851 to 445 (~52% of original) bytes for 3 keys.  Time: 61ms.
 INFO [main] 2011-02-24 11:08:52,296 CassandraDaemon.java (line 112) Binding thrift service to /0.0.0.0:9160
 INFO [main] 2011-02-24 11:08:52,298 CassandraDaemon.java (line 126) Using TFastFramedTransport with a max frame size of 15728640 bytes.
 INFO [Thread-3] 2011-02-24 11:08:52,300 CassandraDaemon.java (line 153) Listening for thrift clients...
{code}","24/Feb/11 20:17;kunda;The previously mentioned tons of ""Invalid bloom filter in SSTableReader"" logs ended with a single stacktrace:
{code}
ERROR [CompactionExecutor:1] 2011-02-24 11:36:19,268 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
	at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
	at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
	at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:541)
	at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
	at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{code}
followed by what appears to be standard flushing/switching logs.","24/Feb/11 20:20;kunda;I now restarted the server again, and two things happened:
1) performing a scrub on the example CF and a bunch of other CFs succeeded without hanging
2) performing a scrub on a different CF resulting in the following stack trace (on different retries):
{code}
ERROR [CompactionExecutor:1] 2011-02-24 22:02:30,329 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: java.io.EOFException
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:113)
	at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:549)
	at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
	at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:776)
	at org.apache.cassandra.io.sstable.IndexHelper.skipBloomFilter(IndexHelper.java:47)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:104)
	... 8 more
{code}
3) and on another CF:
{code}
ERROR [CompactionExecutor:1] 2011-02-24 22:28:28,002 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
        at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:541)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}","24/Feb/11 20:55;jbellis;I really can't help with just a stacktrace and none of the logs leading up to it.

Remember that scrub snapshots before it does its thing, so it's easy to restore the pre-scrubbed versions and try again.",25/Feb/11 16:23;jbellis;Couldn't reproduce by scrubbing sstables produced by 0.6 stress.py,"27/Feb/11 10:28;kunda;After performing more tests, I realized why the problem could not be reproduced - the scrub process hang on a different CF, and afterwards any scrub operation would hang, until the server is restarted.
I was able to narrow down the problem to a specific sstable - I will soon post it along the stack trace.",27/Feb/11 10:34;kunda;Here is the sstable that hangs the scrub process - ran on 0.7 build #325,27/Feb/11 10:44;kunda;And here is another sstable that doesn't hang but throws an exception,"27/Feb/11 11:23;kunda;Update: restarted server and rerun scrub on the attached signatureBuckets CF, this time did not hung but gave the following error:
 INFO [CompactionExecutor:1] 2011-02-27 13:21:27,928 CompactionManager.java (line 511) Scrubbing SSTableReader(path='/vm1/cassandraDB/data/userSimilarity/signatureBuckets-f-104-Data.db')
 INFO [CompactionExecutor:1] 2011-02-27 13:21:28,430 SSTableIdentityIterator.java (line 90) Invalid bloom filter in SSTableReader(path='/vm1/cassandraDB/data/userSimilarity/signatureBuckets-f-104-Data.db'); will rebuild it
 INFO [CompactionExecutor:1] 2011-02-27 13:21:28,430 SSTableIdentityIterator.java (line 99) Invalid row summary in SSTableReader(path='/vm1/cassandraDB/data/userSimilarity/signatureBuckets-f-104-Data.db'); will rebuild it
ERROR [CompactionExecutor:1] 2011-02-27 13:21:28,434 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: java.io.EOFException: attempted to skip 758940931 bytes but only skipped 1400349
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:113)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:548)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException: attempted to skip 758940931 bytes but only skipped 1400349
        at org.apache.cassandra.io.sstable.IndexHelper.skipBloomFilter(IndexHelper.java:51)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:104)
        ... 8 more

","27/Feb/11 12:05;kunda;Repeating the process now results in a different failure on a different and very big (>500MB) sstable of the same signatureBuckets CF:

ERROR [CompactionExecutor:1] 2011-02-27 13:26:01,307 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
        at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:538)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

I think it has something to do with [~jbellis]'s comment about missing colon delimiter -
if scrub cannot solve this, is there any way to fix this problem?
","27/Feb/11 21:53;wajam;We are running into the same issue using #325 from Hudson. Exception everytime.

Everything started when we upgraded from 0.6.8 to 0.7.2. One of the node (17 nodes total) started to be slow, we realized one CF wasn't compacting and throw exception (Error in ThreadPoolExecutor java.io.IOError: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0).

So I gave the new scrub function a try and but looks like it doesnt work. We really need to have this fixed!

Thank you!",27/Feb/11 22:45;jbellis;can someone post a log file from a failed scrub attempt?  ideally with log level debug but I'll take info if that's what you have.,"27/Feb/11 23:14;wajam;I tried to activate debug log but since this is a live node, it seems there is WAY too much going on. Here is the INFO log:

ERROR 23:10:35,105 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:246)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:262)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:223)
        at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(ConcurrentSkipListMap.java:1521)
        at java.util.concurrent.ConcurrentSkipListMap.<init>(ConcurrentSkipListMap.java:1471)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:366)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:314)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:129)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:172)
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:78)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:139)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:448)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:123)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:93)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:68)
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:242)
        ... 25 more
 INFO 23:10:35,106 Scrubbing SSTableReader(path='/var/lib/cassandra/data/Wajam/Comment-f-710-Data.db')
ERROR 23:10:37,489 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:540)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

And the exception that is output by nodetool scrub:

Error occured while scrubbing keyspace <keyspacename>
java.util.concurrent.ExecutionException: java.io.EOFException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.CompactionManager.performScrub(CompactionManager.java:203)
        at org.apache.cassandra.db.ColumnFamilyStore.scrub(ColumnFamilyStore.java:963)
        at org.apache.cassandra.service.StorageService.scrub(StorageService.java:1247)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:540)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        ... 3 more

","28/Feb/11 08:01;kunda;Attached a system log leading up to a ""java.io.EOFException: attempted to skip X bytes but only skipped Y"" error during a scrub of a very big (probably uncompactable) sstable","28/Feb/11 08:07;kunda;another system log, this time leading to a vanilla java.io.EOFException","28/Feb/11 21:25;jbellis;Looked at userChannelFilter-f-210.tar.gz.  Data file does not match index even a little.  Scrub can't help there.
","28/Feb/11 21:41;wajam;I have a 71MB SS table that scrub fail to fix so maybe you would be interested in having it ? There is private data in there so I could upload it somewhere so only you can download it. Let me know how we can work together.

Thank you!","28/Feb/11 23:16;jbellis;Patch to make scrub less crashy.  It still can't magically fix severely corrupted files like the exhibits here, though.","28/Feb/11 23:19;jbellis;Sebastien, go ahead and try scrub after rebuilding w/ the newest patch.

But if your example is like the others, you're seeing something other than the CASSANDRA-2211/CASSANDRA-2216 corruption that scrub is intended to deal with.

So far the only suggestion I have to track that down is to start over and run with snapshot_before_compaction turned on in cassandra.yaml, so when a corrupt sstable is generated we will know where it came from.","01/Mar/11 00:57;wajam;Jonathan,

I'm pretty positive I'm running into CASSANDRA-2216.

Applied the patch and still having the same exception, maybe this is related to CASSANDRA-2256 ?

ERROR 00:51:55,214 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:541)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

","01/Mar/11 02:49;jbellis;2211/2216 would leave valid row keys written, which is where yours is corrupt.  That is why I think this is something else.

Here is a v2 though that will catch the error and try the next row from the index.","01/Mar/11 03:58;wajam;
Now it seems to hang for a while on a 1.4GB sstable. Eventually I get this exception...

ERROR 03:56:29,413 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:569)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
","01/Mar/11 04:18;wajam;Looking at the data directory, it looks like it was working as excepted before the exception happened. The tmp SStable is 1.0GB... Looks like we are getting close!",01/Mar/11 04:45;jbellis;v3 adds support for the index file ending before the data file,"01/Mar/11 04:57;wajam;I will try v3 soon and let you know. While looking at the patch, I found a typo:

logger.warn(""No valid rows found while scrubbing "" + sstable + ""; it is marked for deltion now. If you want to attempt manual recovery, you can find a copy in the pre-scrub snapshot"");

Should be ""deletion"" instead of ""deltion""... no big deal :)","01/Mar/11 05:02;wajam;Now I'm getting this:

ERROR 04:58:53,338 Error reading index file.  Scrub does not (yet) know how to recover from corrupt index files; you can try rebuilding it offline.  See http://www.mail-archive.com/user@cassandra.apache.org/msg03325.html
ERROR 04:58:53,338 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.io.EOFException
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:565)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:559)
        ... 7 more

And

root@WajamCassandra12:/usr/share/cassandra# nodetool -h 127.0.0.1 scrub Wajam Wajam
Error occured while scrubbing keyspace Wajam
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.EOFException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.CompactionManager.performScrub(CompactionManager.java:204)
        at org.apache.cassandra.db.ColumnFamilyStore.scrub(ColumnFamilyStore.java:963)
        at org.apache.cassandra.service.StorageService.scrub(StorageService.java:1247)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: java.io.EOFException
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:565)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        ... 3 more
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:559)
        ... 7 more
",01/Mar/11 05:30;jbellis;v4 attached.  (might be my last for the night; getting late here.  but I'll check back first thing in the morning),"01/Mar/11 05:51;wajam;Scrub is running with v4 right now, we will see what happen, no exception so far. There is these things being displayed about fifty millions time in the log tho:) :

 INFO 05:33:29,521 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:29,548 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:29,859 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:29,882 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:31,223 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:31,249 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:31,644 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:31,678 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,102 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,129 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,393 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,425 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,713 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,734 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,004 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,031 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,305 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,331 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,599 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,623 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,897 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,917 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:34,330 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:34,375 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:35,504 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:35,525 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:35,933 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it

Either way, as long as it fix my ss table I will be happy :) I will let you know in the morning if there is any more issue

Thank you!
","01/Mar/11 13:22;slebresne;Stepping in while Jonathan takes some well deserved rest and attaching v5 (based on last version attached). This makes the following changes:

* In doScrub(), move the first indexFile.readLong() out of the assert.
* Fix computation of dataStartFromIndex (was missing th 4 or 8 bytes for the data size).
* IndexHelper.defreezeBloomFilter don't leave the file pointer after the bloomFilter for new BF since it reads directly from the file (instead of reading the bytes at once and deserializing from that). Correct this.
* Log if a row has been correctly read the first time but index start and size are different from data start and size (since index should then be manually rebuilt).
* Do a retry if dataStart == dataStartFromIndex but dataSize != dataSizeFromIndex (in case the row size only has been corrupted).

Lastly, a minor remarks: the patch removes a flush in BF.serialize(). Maybe this belongs to another ticket ?
","01/Mar/11 13:42;wajam;Thank you Sylvain

Unfortunatly, i'm unable to apply v5 patch for some reason, is it a svn or git patch ?","01/Mar/11 13:57;slebresne;It's a git patch, git apply or 'patch -p1 -i 2240-v5.patch' should do the trick (it does here). It's based on current cassandra-0.7 branch. ",01/Mar/11 14:46;jbellis;v6 applies Sylvain's fixes to v4 (which mysteriously disappeared from jira),01/Mar/11 14:46;jbellis;I also backed out the read-directly-from-file change in IndexHelper.  Will create a new ticket for that.,"01/Mar/11 14:47;wajam;Yep, that worked. So it's currently running with your patch, and so far I get no output from scrub after running for like 5 mins, I usually get the ""Scrubbing SSTABLE"" in the first min so I'm not sure if that's normal. There is the ""tmp"" files in the data directory but looks like they are stuck at 0 byte.

Oh, looks like I will try v6 now then!

Jonathan, v4 is there, you named it 2240.txt :)","01/Mar/11 15:00;jbellis;bq. v4 is there, you named it 2240.txt

Oops -- that was the original patch, not actually v4.  Must have been tired.","01/Mar/11 15:28;slebresne;Alright, v6 looks good, +1. Though, we may want to wait to see if it works alright for Sebastien too.","01/Mar/11 15:36;wajam;I'm testing it as we speak!

Log are flooded with this:

 INFO 15:31:15,783 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-164-Data.db'); will rebuild it
 INFO 15:31:15,783 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-164-Data.db'); will rebuild it
 INFO 15:31:15,783 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-164-Data.db'); will rebuild it
 INFO 15:31:15,783 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-164-Data.db'); will rebuild it

I'm not sure why it's scrubbing ""WebsiteWajams"" CF as I asked for ""Wajam"" (nodetool -h 127.0.0.1 scrub Wajam Wajam) ... Either way, this sstable is messed up too so that's fine.

Hopefully it won't just do like v4 that ran all night and was still echoing invalid bloom filter for the same sstable in the log this morning, Looks like it was stuck in a infinite loop or something.

It looks like it's scrubbing another sstable now so that's already an improvment.

Will keep you guys updated!


","01/Mar/11 15:48;wajam; INFO 15:42:52,690 Scrub of SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-155-Data.db') complete: 3271113 in new sstable

""3271113 in new sstable""... I'm not sure it's 3271113 what... Looks like ""good rows"" according to the patch, might want to specify! :)

I think it's the first time I see this line tho so it's good news. Is there any way the ""invalid bloom filter, will rebuild it"" line can only appear once per sstable ? I'm guessing its generating useless IO writing about 1000 of those lines/second in the log!

This should be definitly included in 0.7.3 which hopefully will be released soon!

Good work guys, very appreciated :)","01/Mar/11 16:10;wajam;I did see a few exceptions every now and then. Not sure if you need to do anything about this. Sorry for all these posts, hopefully it help :):

224574- INFO 16:06:40,744 Retrying from row index; data is 245642090 bytes starting at 42
224657- WARN 16:06:40,744 Retry failed too.  Skipping to next row (retry's stacktrace follows)
224745-java.io.IOError: java.io.EOFException
224783- at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:117)
224884: at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:610)
224966- at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
225050- at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
225131- at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
225202- at java.util.concurrent.FutureTask.run(FutureTask.java:166)
225263- at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
225347- at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
225431- at java.lang.Thread.run(Thread.java:636)
225473-Caused by: java.io.EOFException
225505- at java.io.DataInputStream.readInt(DataInputStream.java:392)
225567- at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:47)","01/Mar/11 16:10;jbellis;committed w/ update to goodRows message and moving the BF/row header messages to debug level

thanks!","01/Mar/11 16:14;jbellis;bq. I did see a few exceptions every now and then

Did you see any ""Error reading index file"" lines before that?  That's the only way I can think that it would come up with such a strange row size.
","01/Mar/11 16:22;wajam;Yep I did see error reading index file before that.

Any chance to include this in 0.7.3 release ? I'm sure it would help a ton of people :)","01/Mar/11 16:30;jbellis;Yes, we'll get this into 0.7.3.","01/Mar/11 16:53;hudson;Integrated in Cassandra-0.7 #336 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/336/])
    make nodetool scrub more robust
patch by jbellis and slebresne; tested by Sébastien Giroux for CASSANDRA-2240
","01/Mar/11 20:15;kunda;Thank you so much, my sstables are finally clean!",,,,,,,
Gossiper Starvation,CASSANDRA-2253,12499862,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mikaels,mikaels,mikaels,2/27/2011 16:03,3/12/2019 14:16,3/13/2019 22:24,2/28/2011 23:53,0.7.3,,,,0,,,,,,"Gossiper periodic task will get into starvation in case large sstable files need to be deleted.
Indeed the SSTableDeletingReference uses the same scheduledTasks pool (from StorageService) as the Gossiper and other periodic tasks, but the gossiper tasks should run each second to assure correct cluster status (liveness of nodes). In case of large sstable files to be deleted (several GB) the delete operation can take more than 30 sec, thus making the whole cluster going into a wrong state where nodes are marked as not living while they are!
This will lead to unneeded additional load like hinted hand off, wrong cluster state, increase in latency.

One of the possible solution is to use a separate pool for periodic and non periodic tasks. 
I've implemented such change and it resolves the problem. 
I can provide a patch ","linux, windows",,7200,7200,,0%,7200,7200,,,,,,,,,,,,28/Feb/11 23:02;mikaels;CASSANDRA-0.7-2253.txt;https://issues.apache.org/jira/secure/attachment/12472246/CASSANDRA-0.7-2253.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,57:07.3,,,no_permission,,,,,,,,,,,,20526,,,Tue Mar 01 00:04:48 UTC 2011,,,,,,0|i0ga9r:,93094,jbellis,jbellis,,,,,,,,,"27/Feb/11 16:57;jbellis;bq. use a separate pool for periodic and non periodic tasks

that's reasonable; so might splitting Gossiper off to its own executor","27/Feb/11 18:05;mikaels;I also though having gossiper in its own executor, nevertheless it means that other periodic tasks may come to starvation because of the heavy non periodic tasks. Therefore i chose to use a separate pool for the heavy one instead.
",27/Feb/11 22:44;jbellis;Sounds good to me.  Were you going to submit a patch?,28/Feb/11 03:43;mikaels;yes,"28/Feb/11 23:01;mikaels;Add a new pool for non periodic heavyweight tasks, to eliminate the starvation of periodic short time execution task like Gossiper.
Additionally add debug statement for periodic and non periodic task ","28/Feb/11 23:02;mikaels;patch for bug 2253, Gossip starvation","28/Feb/11 23:53;jbellis;committed the executor change

omitted the debug log statements; the ""right"" way to do that is to create an executor subclass that logs that for us instead of relying on boilerplate code in the caller","01/Mar/11 00:04;hudson;Integrated in Cassandra-0.7 #333 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/333/])
    movefile deletions off of scheduledtasks executor
patch by Mikael Sitruk; reviewed by jbellis for CASSANDRA-2253
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A bug in BufferedRandomAccessFile,CASSANDRA-2213,12499317,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,leojay,leojay,leojay,2/22/2011 9:15,3/12/2019 14:16,3/13/2019 22:24,2/22/2011 18:04,0.7.3,,,,0,,,,,,"The first line of BufferedRandomAccessFile.readAtMost is
{code}if (length >= bufferEnd && hitEOF){code}

I think It should be "">"" instead of "">="",
Here is a test for this:{code}
    @Test
    public void testRead() throws IOException {
        File tmpFile = File.createTempFile(""readtest"", ""bin"");
        tmpFile.deleteOnExit();

        // Create the BRAF by filename instead of by file.
        BufferedRandomAccessFile rw = new BufferedRandomAccessFile(tmpFile.getPath(), ""rw"");
        rw.write(new byte[] {1});

        rw.seek(0);
        byte[] buffer = new byte[1];
        assert rw.read(buffer) == 1;
        assert buffer[0] == 1;
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,04:36.7,,,no_permission,,,,,,,,,,,,20509,,,Tue Feb 22 18:57:54 UTC 2011,,,,,,0|i0ga0f:,93052,jbellis,jbellis,,,,,,,,,"22/Feb/11 18:04;jbellis;committed, thanks!","22/Feb/11 18:57;hudson;Integrated in Cassandra-0.7 #304 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/304/])
    avoid EOFing on requests for the last bytes in a file
patch by Leo Jay; reviewed by jbellis for CASSANDRA-2213
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression caused by cache-avoiding code in BRAF,CASSANDRA-2218,12499383,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,2/22/2011 18:19,3/12/2019 14:16,3/13/2019 22:24,2/22/2011 18:41,0.7.3,,,,0,,,,,,"As reported by Ivan Georgiev on the mailing list, BRAF.reBuffer unnecessarily does extra read + fadvise when seeking to the end of the file.",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,22/Feb/11 18:21;jbellis;2218.txt;https://issues.apache.org/jira/secure/attachment/12471646/2218.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,31:06.3,,,no_permission,,,,,,,,,,,,19347,,,Tue Feb 22 18:57:55 UTC 2011,,,,,,0|i0ga1z:,93059,tjake,tjake,,,,,,,,,22/Feb/11 18:31;tjake;good find +1,22/Feb/11 18:41;jbellis;committed,"22/Feb/11 18:57;hudson;Integrated in Cassandra-0.7 #304 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/304/])
    fix BRAF performancewhen seeking toEOF
patch by Ivan Georgiev; reviewed by tjake for CASSANDRA-2218
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchElement exception on node which is streaming a repair,CASSANDRA-2316,12501231,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,alienth,alienth,3/12/2011 7:12,3/12/2019 14:16,3/13/2019 22:24,4/18/2011 23:04,0.7.5,,,,0,repair,,,,,"Running latest SVN snapshot of 0.7.

When I ran a repair on a node, that node's neighbor threw the following exception. Let me know what other info could be helpful.

{code}
 INFO 23:43:44,358 Streaming to /10.251.166.15
ERROR 23:50:21,321 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.util.NoSuchElementException
        at com.google.common.collect.AbstractIterator.next(AbstractIterator.java:146)
        at org.apache.cassandra.service.AntiEntropyService$Validator.add(AntiEntropyService.java:366)
        at org.apache.cassandra.db.CompactionManager.doValidationCompaction(CompactionManager.java:825)
        at org.apache.cassandra.db.CompactionManager.access$800(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$6.call(CompactionManager.java:358)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}",,,,,,,,,,,,,,,,CASSANDRA-2324,,,,18/Mar/11 19:55;jbellis;2316-assert.txt;https://issues.apache.org/jira/secure/attachment/12474026/2316-assert.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,45:06.1,,,no_permission,,,,,,,,,,,,20557,,,Tue Apr 19 14:04:20 UTC 2011,,,,,,0|i0ganr:,93157,stuhood,stuhood,,,,,,,,,"14/Mar/11 16:45;jbellis;The loop in validator.add apparently assumes that _some_ range will contain any given row. But

- ranges is supposed to be ""invalid"" ranges not all ranges
- comments in validator.add say it is called for each row in the CF
- so any rows that are not part of an ""invalid"" range will cause this exception

So either my superficial understanding of what ""invalid"" ranges are is broken, or the comments are wrong, or I'm surprised we're not hitting this a lot more frequently.",14/Mar/11 16:47;jbellis;Looks like this dates back to 0.6.,"17/Mar/11 19:36;stuhood;""Invalid"" ranges in the tree are ranges that need to be hashed. The idea was that the tree could be persisted between repair sessions, and ranges would be invalidated as writes arrived: then the validation compaction would only need to compact invalid ranges of the tree.

In the current implementation, the tree will only contain invalid ranges, since it is being created from scratch for every repair.",17/Mar/11 19:39;stuhood;I wonder if this is a keys-out-of-order problem?,"18/Mar/11 02:39;jbellis;Since we iterate over each key in the CF, order shouldn't actually matter should it?","18/Mar/11 08:04;stuhood;Order matters, because there will be up to 2^16 invalid ranges. If keys arrive out of order we will consume ranges that should have contained keys, possibly leading us to consume all invalid ranges.

Either way, an assert that keys are arriving in order would be handy here.",18/Mar/11 19:55;jbellis;proposed assert attached,18/Apr/11 22:12;stuhood;+1 For the assert.,"19/Apr/11 14:04;hudson;Integrated in Cassandra-0.7 #447 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/447/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommutativeRowIndexer always read full row in memory,CASSANDRA-2313,12501164,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,3/11/2011 16:37,3/12/2019 14:16,3/13/2019 22:24,4/7/2011 20:32,0.8 beta 1,,,,1,,,,,,"CommutativeRowIndexer use CFSerializer.deserializeColumns() that read the full row in memory. We should use PreCompactedRow/LazilyCompactedRow instead to avoid this on huge row.

As an added benefit, using PreCompactedRow will avoid a current seek back to write the row size.",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,05/Apr/11 18:01;slebresne;0001-Introduce-CompactionController-to-handler-compaction-v2.patch;https://issues.apache.org/jira/secure/attachment/12475515/0001-Introduce-CompactionController-to-handler-compaction-v2.patch,16/Mar/11 10:48;slebresne;0001-Introduce-CompactionController-to-handler-compaction.patch;https://issues.apache.org/jira/secure/attachment/12473776/0001-Introduce-CompactionController-to-handler-compaction.patch,05/Apr/11 18:01;slebresne;0002-Make-CommutativeRowIndexer-uses-AbstractCompactionRo-v2.patch;https://issues.apache.org/jira/secure/attachment/12475516/0002-Make-CommutativeRowIndexer-uses-AbstractCompactionRo-v2.patch,16/Mar/11 10:48;slebresne;0002-Make-CommutativeRowIndexer-uses-AbstractCompactionRo.patch;https://issues.apache.org/jira/secure/attachment/12473777/0002-Make-CommutativeRowIndexer-uses-AbstractCompactionRo.patch,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,06:34.1,,,no_permission,,,,,,,,,,,,20555,,,Thu Apr 07 21:50:06 UTC 2011,,,,,,0|i0gan3:,93154,stuhood,stuhood,,,,,,,,,"16/Mar/11 10:48;slebresne;Attaching patch against trunk.

This turns out to be slightly harder than expected because (PreCompacted|LazilyCompacted)Row and SSTableIdentityIterator were relying on a SSTableReader, but while rebuilding the index, we don't have one yet (and faking one would probably be fragile).

Instead, a first patch introduces CompactionController, that is used to manage the compaction options. This remove parts of the dependency mentioned above.  I also think that it cleans code and slightly optimize it in that it avoid recreating a HashSet of the sstables for each given row.

The second patch modify CommutativeRowIndexer to use (PreCompacted|LazilyCompacted)Row, which actually greatly simply the code there.
",05/Apr/11 18:01;slebresne;Attaching rebased patch set.,"07/Apr/11 06:06;stuhood;+1
This looks good, thanks Sylvain.",07/Apr/11 20:32;slebresne;Committed as r1089993,"07/Apr/11 21:50;hudson;Integrated in Cassandra #835 (See [https://hudson.apache.org/hudson/job/Cassandra/835/])
    Use {Lazy|Pre}CompactedRow for CommutativeRowIndexer
patch by slebresne; reviewed by stuhood for CASSANDRA-2313
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM on repair with many inconsistent ranges,CASSANDRA-2301,12500936,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,j.casares,j.casares,j.casares,3/9/2011 19:46,3/12/2019 14:16,3/13/2019 22:24,3/11/2011 2:57,0.7.4,,,,0,,,,,,"Repair can OOM when lots of ranges are inconsistent, causing many sstables to be streamed.

I replicated the error by making 1264 3MB sstables on one node, added a second node, changed the replication factor to 2, and ran a repair.

Looking at the heap dump of the original failure, there were 2.4GB of FutureTasks, each taking up 8MB of space. I tracked down the BufferedRandomAccessFile and made sure that it was cleared every time it was closed inside of src/java/org/apache/cassandra/io/sstable/SSTableWriter.java.

Attached is the patch I used which stopped the error when I was trying to replicate it.",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,09/Mar/11 20:09;jbellis;2301-v2.txt;https://issues.apache.org/jira/secure/attachment/12473193/2301-v2.txt,09/Mar/11 20:14;jbellis;2301-v3.txt;https://issues.apache.org/jira/secure/attachment/12473194/2301-v3.txt,09/Mar/11 19:47;j.casares;2301.diff;https://issues.apache.org/jira/secure/attachment/12473187/2301.diff,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,09:12.8,,,no_permission,,,,,,,,,,,,20548,,,Fri Mar 11 03:47:16 UTC 2011,,,,,,0|i0gakf:,93142,jbellis,jbellis,,,,,,,,,"09/Mar/11 20:09;jbellis;I'm actually not sure how this could help -- dfile.close() already un-references the buffer, so there's no need to set dfile itself to be null.  Also, the file pointer changes as we read through the file, so changing bytescomplete to init-once is broken. (This is what nodetool compactionstats uses.)

My guess is that the second time around you just got lucky and index build was able to keep up w/ streaming enough to avoid OOMing.

But there is a bug here with the file/buffer handling -- we should lazy-init this once we're ready to build the index, rather than when we enqueue the task.  Patch attached w/ this approach.",09/Mar/11 20:14;jbellis;v2 overcomplicated the problem.  v3 attached w/o extra Builder fields.,10/Mar/11 21:23;mdennis;+1 on v3,"10/Mar/11 23:54;j.casares;+1 on v3.

Ran the patched code twice and my cluster of 2 didn't OOM.",11/Mar/11 02:57;jbellis;committed,"11/Mar/11 03:47;hudson;Integrated in Cassandra-0.7 #373 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/373/])
    reduce memory use during streaming of multiple sstables
patch by jbellis; reviewed by mdennis and tested by Joaquin Casares for CASSANDRA-2301
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Scrub resulting in ""bloom filter claims to be longer than entire row size"" error",CASSANDRA-2296,12500831,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,alienth,alienth,3/9/2011 1:40,3/12/2019 14:16,3/13/2019 22:24,3/9/2011 14:37,0.7.4,,Legacy/Tools,,0,,,,,,"Doing a scrub on a node which I upgraded from 0.7.1 (was previously 0.6.8) to 0.7.3. Getting this error multiple times:
{code}
 WARN [CompactionExecutor:1] 2011-03-08 18:33:52,513 CompactionManager.java (line 625) Row is unreadable; skipping to next
 WARN [CompactionExecutor:1] 2011-03-08 18:33:52,514 CompactionManager.java (line 599) Non-fatal error reading row (stacktrace follows)
java.io.IOError: java.io.EOFException: bloom filter claims to be longer than entire row size
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:117)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:590)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException: bloom filter claims to be longer than entire row size
        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:113)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:87)
        ... 8 more
 WARN [CompactionExecutor:1] 2011-03-08 18:33:52,515 CompactionManager.java (line 625) Row is unreadable; skipping to next
 INFO [CompactionExecutor:1] 2011-03-08 18:33:53,777 CompactionManager.java (line 637) Scrub of SSTableReader(path='/cassandra/data/reddit/Hide-f-671-Data.db') complete: 254709 rows in new sstable
 WARN [CompactionExecutor:1] 2011-03-08 18:33:53,777 CompactionManager.java (line 639) Unable to recover 1630 that were skipped.  You can attempt manual recovery from the pre-scrub snapshot.  You can also run nodetool repair to transfer the data from a healthy replica, if any
{code}",,,7200,7200,,0%,7200,7200,,,,,,,,,,,,09/Mar/11 03:33;jbellis;2296.txt;https://issues.apache.org/jira/secure/attachment/12473092/2296.txt,09/Mar/11 01:58;alienth;sstable_part1.tar.bz2;https://issues.apache.org/jira/secure/attachment/12473088/sstable_part1.tar.bz2,09/Mar/11 01:58;alienth;sstable_part2.tar.bz2;https://issues.apache.org/jira/secure/attachment/12473089/sstable_part2.tar.bz2,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,40:01.7,,,no_permission,,,,,,,,,,,,20546,,,Thu Mar 10 19:08:11 UTC 2011,,,,,,0|i0gajb:,93137,slebresne,slebresne,,,,,,,,,"09/Mar/11 02:40;jbellis;With debug logging turned on it looks like this:

{noformat}
[lots of rows around 119 bytes long]
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 559) row 337a306f615f666c38756a is 119 bytes
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 588) Index doublecheck: row 337a306f615f666c38756a is 119 bytes
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 550) Reading row at 44385
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 559) row 34306536785f666f666b65 is 0 bytes
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,252 CompactionManager.java (line 588) Index doublecheck: row 34306536785f666f666b65 is 0 bytes
 WARN [CompactionExecutor:1] 2011-03-08 20:34:12,253 CompactionManager.java (line 606) Non-fatal error reading row (stacktrace follows)
java.io.IOError: java.io.EOFException: bloom filter claims to be 734305 bytes, longer than entire row size 0
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:125)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:597)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:57)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:196)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
Caused by: java.io.EOFException: bloom filter claims to be 734305 bytes, longer than entire row size 0
        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:113)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:95)
        ... 8 more
 WARN [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 632) Row is unreadable; skipping to next
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 550) Reading row at 44406
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 559) row 34616465655f66707a6178 is 119 bytes
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 588) Index doublecheck: row 34616465655f66707a6178 is 119 bytes
[lots more rows around 119 bytes]
{noformat}

In other words: there's an row that's empty except for the key, which is causing the problem because we're not supposed to write rows like that.  I checked with a hex editor and that's what it looks like.

The good news is that scrub is correctly skipping it and recovering everything else fine.

The bad news is we have (or possibly, had) a bug that was causing those empty rows to be written.",09/Mar/11 02:59;jbellis;added asserts to catch zero-length rows in r1079650,"09/Mar/11 03:07;alienth;Got the following error while restarting *after* I ran the scrub on that same node:

{code}
ERROR [CompactionExecutor:1] 2011-03-08 19:54:48,023 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: java.io.EOFException
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:117)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:67)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:179)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:144)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:136)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:39)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:449)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:124)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:94)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}","09/Mar/11 03:12;jbellis;The first scrub ended with

{noformat}
 INFO [CompactionExecutor:1] 2011-03-08 20:45:39,174 CompactionManager.java (line 644) Scrub of SST\
ableReader(path='/var/lib/cassandra/data/KS1/Hide-f-671-Data.db') complete: 254709 rows in new ssta\
ble
 WARN [CompactionExecutor:1] 2011-03-08 20:45:39,174 CompactionManager.java (line 646) Unable to re\
cover 1630 rows that were skipped.  You can attempt manual recovery from the pre-scrub snapshot.  Y\
ou can also run nodetool repair to transfer the data from a healthy replica, if any
{noformat}

Scrubbing the scrubbed version again, ended with

{noformat}
 INFO 21:11:01,349 Scrub of SSTableReader(path='/var/lib/cassandra/data/KS1/Hide-f-672-Data.db') complete: 253308 rows in new sstable
 WARN 21:11:01,349 Unable to recover 1401 rows that were skipped.  You can attempt manual recovery from the pre-scrub snapshot.  You can also run nodetool repair to transfer the data from a healthy replica, if any
{noformat}

Scrub is eating rows.","09/Mar/11 03:21;jbellis;Scrub writes a zero-length row when tombstones expire and there is nothing left, instead of writing no row at all.  So, as the clock rolls forwards and more tombstones expire, you will usually get a few more zero-length rows written, that will be cleaned out by the next scrub.","09/Mar/11 03:22;hudson;Integrated in Cassandra-0.7 #362 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/362/])
    add asserts to make sure we don't write zero-length rows; see CASSANDRA-2296
",09/Mar/11 03:33;jbellis;fix attached.  now skips tombstoned rows properly w/o leaving stubs in the new sstable.,"09/Mar/11 09:48;slebresne;In the retry part, the goodRows++ after the if should be removed to avoid counting rows twice.

Other than this, +1",09/Mar/11 14:37;jbellis;committed w/ ++ fix,"09/Mar/11 15:02;hudson;Integrated in Cassandra-0.7 #365 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/365/])
    avoid writing empty rows when scrubbing tombstoned rows
patch by jbellis; reviewed by slebresne for CASSANDRA-2296
",09/Mar/11 16:58;jbellis;also added test in r1079882,"10/Mar/11 00:32;alienth;I applied the patch and retried. Getting a new exception. Thousands of this:

{code}
 WARN [CompactionExecutor:1] 2011-03-09 17:29:59,752 CompactionManager.java (line 641) Row at 517805025 is unreadable; skipping to next
 INFO [CompactionExecutor:1] 2011-03-09 17:29:59,752 SSTableWriter.java (line 108) Last written key : DecoratedKey(125686934811414729670440675125192621396, 627975726c2833626333626339353363353762313133373331336461303233396438303534312c66692e676f73757065726d6f64656c2e636f6d2f70726f66696c65732f2f6170706c65747265713d3132373333393332313937363529)
 INFO [CompactionExecutor:1] 2011-03-09 17:29:59,752 SSTableWriter.java (line 109) Current key : DecoratedKey(11081980355438931816706032048128862258, 30303063623061323633313463653465376663333561303531326333653737363333663065646134)
 INFO [CompactionExecutor:1] 2011-03-09 17:29:59,752 SSTableWriter.java (line 110) Writing into file /var/lib/cassandra/data/reddit/permacache-tmp-f-168615-Data.db
 WARN [CompactionExecutor:1] 2011-03-09 17:29:59,752 CompactionManager.java (line 607) Non-fatal error reading row (stacktrace follows)
java.io.IOException: Keys must be written in ascending order.
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:111)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:128)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:598)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}

Keys are getting written back improperly?",10/Mar/11 01:36;jbellis;Looks like a different problem. What is the context at debug level?,"10/Mar/11 01:51;alienth;I'll check the debug output on it tomorrow. I should note that I ran a scrub on this same set of data yesterday on 0.7.3. I got two errors regarding another CF, but nothing for the CF which is now complaining.","10/Mar/11 18:52;alienth;Here is the debug output. Going to get a comparison on the unpatched 0.7.3 to see if there is any difference.

{code}
DEBUG 11:50:52,510 Reading row at 504216964
DEBUG 11:50:52,510 row 636f6d6d656e74735f706172656e74735f3233383135363235 is 66 bytes
DEBUG 11:50:52,510 Index doublecheck: row 636f6d6d656e74735f706172656e74735f3233383135363235 is 66 bytes
 INFO 11:50:52,511 Last written key : DecoratedKey(125686934811414729670440675125192621396, 627975726c2833626333626339353363353762313133373331336461303233396438303534312c66692e676f73757065726d6f64656c2e636f6d2f70726f66696c65732f2f6170706c65747265713d3132373333393332313937363529)
 INFO 11:50:52,511 Current key : DecoratedKey(11047858886149374835950241979723972473, 636f6d6d656e74735f706172656e74735f3233383135363235)
 INFO 11:50:52,511 Writing into file /var/lib/cassandra/data/reddit/permacache-tmp-f-168492-Data.db
 WARN 11:50:52,511 Non-fatal error reading row (stacktrace follows)
java.io.IOException: Keys must be written in ascending order.
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:111)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:128)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:598)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}

",10/Mar/11 19:08;alienth;Disregard. Getting the same thing on unpatched 0.7.3. I'll create a separate bug report.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsupportedOperationException: Overflow in bytesPastMark(..),CASSANDRA-2297,12500876,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,muga_nishizawa,muga_nishizawa,3/9/2011 11:41,3/12/2019 14:16,3/13/2019 22:24,3/11/2011 18:32,0.7.4,,,,0,,,,,,"I hit the following exception on a row that was more than 60GB.  
The row has column families of super column type.

This problem is discussed by the following thread.  
http://www.mail-archive.com/dev@cassandra.apache.org/msg01881.html

{code}
ERROR [HintedHandoff:1] 2011-02-26 18:49:35,708 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.UnsupportedOperationException: Overflow: 2147484294
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.UnsupportedOperationException: Overflow: 2147484294
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.bytesPastMark(BufferedRandomAccessFile.java:477)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.getNextBlock(IndexedSliceReader.java:179)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:120)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:1)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:118)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:142)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1290)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1167)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:138)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$1(HintedHandOffManager.java:262)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{code}","Java 1.6.0_23, CentOS 5.5 (64bit)",,3600,3600,,0%,3600,3600,,,,,,,,,,,,10/Mar/11 17:07;jbellis;2297.txt;https://issues.apache.org/jira/secure/attachment/12473291/2297.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,07:49.6,,,no_permission,,,,,,,,,,,,20547,,,Fri Mar 11 23:07:02 UTC 2011,,,,,,0|i0gajj:,93138,slebresne,slebresne,,,,,,,,,"10/Mar/11 17:07;jbellis;fix bytesPastMark to return long.

Note that if your row index is > 2GB then you should probably increase column_index_size_in_kb to 256 or higher.  I've created CASSANDRA-2308 to expose row index size so this can be tuned more accurately.","11/Mar/11 18:16;slebresne;It breaks BufferedRandomAccessFileTest.
But +1 on the patch otherwise.",11/Mar/11 18:32;jbellis;committed w/ removal of obsolete BRAFTest code,"11/Mar/11 23:07;hudson;Integrated in Cassandra-0.7 #375 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/375/])
    fix HH delivery when column index is larger than 2GB
patch by jbellis; reviewed by slebresne for CASSANDRA-2297
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unbounded key range only ever scans first node in ring,CASSANDRA-1722,12479555,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,jbellis,jbellis,11/10/2010 3:20,3/12/2019 14:16,3/13/2019 22:24,11/10/2010 14:31,0.6.8,0.7.0 rc 1,,,0,,,,,,"{code:Java}
        List<AbstractBounds> ranges = getRestrictedRanges(new Bounds(leftToken, p.getMinimumToken()));
{code}

when called with empty start key this means we have a Bounds(minToken, minToken), which hits the getRR special case

{code:Java}
        // special case for bounds containing exactly 1 token
        if (queryRange instanceof Bounds && queryRange.left.equals(queryRange.right))
        {
            if (logger.isDebugEnabled())
                logger.debug(""restricted single token match for query "" + queryRange);
            return Collections.singletonList(queryRange);
        }
{code}

Looks like this broke as a side effect of CASSANDRA-1442.  Prior to that a bounds from [T, minToken] was considered ""up to infinity"" by getRR so would span multiple nodes.",,,,,,,,,,,,,,,,,,,,10/Nov/10 04:58;stuhood;0001-Add-guard-to-the-Bounds-special-case-to-treat-min-rang.txt;https://issues.apache.org/jira/secure/attachment/12459221/0001-Add-guard-to-the-Bounds-special-case-to-treat-min-rang.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:24.6,,,no_permission,,,,,,,,,,,,20272,,,Thu Nov 11 22:18:19 UTC 2010,,,,,,0|i0g6zj:,92562,jbellis,jbellis,,,,,,,,,"10/Nov/10 04:58;stuhood;Guard the ""exact one token"" special case from the minimum token: a minimum token range should wrap.","10/Nov/10 14:31;jbellis;committed, thanks!",11/Nov/10 22:15;stuhood;This actually affects get_range_slices as well: querying a key range for start_key='' and end_key='' results in an empty bounds. jbellis backported it to the 0.6 branch.,11/Nov/10 22:18;jbellis;Also committed to 0.6.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
word_count/pig loadfunc don't match the ColumnFamilyInputFormat ByteBuffer signature,CASSANDRA-1725,12479620,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,jeromatron,jeromatron,11/10/2010 18:32,3/12/2019 14:16,3/13/2019 22:24,11/10/2010 21:09,0.7.0 rc 1,,,,0,,,,,,"In recent commits, ColumnFamilyInputFormat's signature has changed to use ByteBuffers.  This signature needs to match the word_count example and the pig load func.  There are a few options:
1) The ColumnFamilyInputFormat signature itself didn't need to change - it could translate from byte[] to ByteBuffer internally.
2) Change the word_count and pig load func to use ByteBuffer
3) Just use the AvroColumnFamilyInputFormat.

I think option 1 would be best for now since that's less of a change.",,,,,,,,,,,,,,,,,,,,10/Nov/10 19:34;tjake;1725_v1.txt;https://issues.apache.org/jira/secure/attachment/12459270/1725_v1.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,34:55.2,,,no_permission,,,,,,,,,,,,20274,,,Wed Nov 10 21:09:13 UTC 2010,,,,,,0|i0g707:,92565,jeromatron,jeromatron,,,,,,,,,"10/Nov/10 18:36;jeromatron;Also, would be nice to have the pig load func as part of core so that we aren't bitten by this.",10/Nov/10 19:34;tjake;Tested both pig and word_count now work,"10/Nov/10 20:10;jeromatron;tested both - work fine.

+1

hopefully this won't change again in the future - so that people's cassandra+hadoop code doesn't have to change much.",10/Nov/10 21:09;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read repair does not always work correctly,CASSANDRA-1316,12470095,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,7/24/2010 23:50,3/12/2019 14:15,3/13/2019 22:24,7/27/2010 16:47,0.6.4,,,,0,,,,,,"Read repair does not always work.  At the least, we allow violation of the CL.ALL contract.  To reproduce, create a three node cluster with RF=3, and json2sstable one of the attached json files on each node.  This creates a row whose key is 'test' with 9 columns, but only 3 columns are on each machine.  If you get_count this row in quick succession at CL.ALL, sometimes you will receive a count of 6, sometimes 9.  After the ReadRepairManager has sent the repairs, you will always get 9, which is the desired behavior.

I have another data set obtained in the wild which never fully repairs for some reason, but it's a bit large to attach (600ish columns per machine.)  I'm still trying to figure out why RR isn't working on this set, but I always get different results when reading at any CL including ALL, no matter how long I wait or how many reads I do.",,,,,,,,,,,,,,,,,,,,26/Jul/10 15:24;brandon.williams;001_correct_responsecount_in_RRR.txt;https://issues.apache.org/jira/secure/attachment/12450474/001_correct_responsecount_in_RRR.txt,27/Jul/10 02:41;jbellis;1316-RRM.txt;https://issues.apache.org/jira/secure/attachment/12450552/1316-RRM.txt,26/Jul/10 16:08;jbellis;RRR-v2.txt;https://issues.apache.org/jira/secure/attachment/12450479/RRR-v2.txt,25/Jul/10 20:15;brandon.williams;cassandra-1.json;https://issues.apache.org/jira/secure/attachment/12450423/cassandra-1.json,25/Jul/10 20:15;brandon.williams;cassandra-2.json;https://issues.apache.org/jira/secure/attachment/12450424/cassandra-2.json,25/Jul/10 20:15;brandon.williams;cassandra-3.json;https://issues.apache.org/jira/secure/attachment/12450425/cassandra-3.json,,,,,,,,6,,,,,,,,,,,,,,,,,,,08:40.0,,,no_permission,,,,,,,,,,,,20077,,,Sat Dec 11 07:35:22 UTC 2010,,,,,,0|i0g4bb:,92129,,,,,,,,,,,24/Jul/10 23:53;brandon.williams;I tried to bisect this issue and went as far back as 0.4.2 without finding a successful version.  I will see if the data that never repairs can be scrubbed so I can attach it to this ticket for debugging.,"25/Jul/10 00:02;brandon.williams;It looks like the key difference between the real data and the toy data that is attached is that the real data has the key in multiple sstables.  If left this way, RR never fully works, but if I force a compaction then it succeeds.","25/Jul/10 20:18;brandon.williams;Updated json files illustrate one possible scenario:  nodes 1 and 2 have a column, and node 3 has the column tombstoned with the same timestamp.  It looks like tombstones aren't taking precedence.","26/Jul/10 15:24;brandon.williams;Patch to solve one problem: use derterminBlockFor to set the correct response count passed to RRR, so CL.ALL works.","26/Jul/10 16:08;jbellis;v2 updates QRH arguments to use responseCount as well, even though it's ignored",26/Jul/10 22:11;brandon.williams;Committed RRRv2.,27/Jul/10 02:41;jbellis;patch that simplifies debugging by removing ReadRepairManager which is mostly 100-odd lines of obfuscation around MessagingService.sendOneWay.  (backport from CASSANDRA-1077 which was applied to trunk two months ago),"27/Jul/10 16:47;jbellis;Brandon's first patch fixing reads at CL.ALL turns out to be the only bug.  The rest is obscure-but-valid behavior when expired tombstones haven't been replicated across the cluster (i.e., the tombstones exist on some nodes, but not all).  Let me give an example:

say node A has columns x and y, where x is an expired tombstone with timestamp T1, and node B has live column x, at time T2 where T2 < T1.

if you read at ALL you will see x from B and y from A.  you will _not_ see x from A -- since it is expired, it is no longer relevant off-node.  thus, the ALL read will send a repair of column x to A, since it was ""missing.""

But next time you read from A the tombstone will supress the newly-written copy of x-from-B still, because its timestamp is higher.  So the replicas won't converge.

This is not a bug, because the design explicitly allows that behavior when tombstones expire before being propagated to all nodes; see http://wiki.apache.org/cassandra/DistributedDeletes.  The best way to avoid this of course is to run repair frequently enough to ensure that tombstones are propagated within GCGraceSeconds of being written.

But if you do find yourself in this situation, you have two options to get things to converge again:

1) the simplest option is to simply perform a major compaction on each node, which will eliminate all expired tombstones.

2) but if you want to propagate as many of the tombstones as possible first, increase your GCGraceSeconds setting everywhere (requires rolling restart), and perform a full repair as described in http://wiki.apache.org/cassandra/Operations.  After the repair is complete you can put GCGraceSeconds back to what it was.
","11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
can't write with consistency level of one after some nodes fail,CASSANDRA-524,12439463,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,edmond,edmond,10/29/2009 22:00,3/12/2019 14:15,3/13/2019 22:24,10/30/2009 18:27,0.5,,,,0,,,,,,"Start a 3 node cluster with a replication factor of 2.  Then take down two nodes.

If I write with a consistency level of ONE on any key, I get an InvalidRequestException:

ERROR [pool-1-thread-45] 2009-10-29 21:27:10,120 StorageProxy.java
(line 183) error writing key 1
InvalidRequestException(why:Cannot block for less than one replica)
       at org.apache.cassandra.service.QuorumResponseHandler.<init>(QuorumResponseHandler.java:52)
       at org.apache.cassandra.locator.AbstractReplicationStrategy.getResponseHandler(AbstractReplicationStrategy.java:64)
       at org.apache.cassandra.service.StorageService.getResponseHandler(StorageService.java:869)
       at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:162)
       at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:473)
       at org.apache.cassandra.service.CassandraServer.insert(CassandraServer.java:424)
       at org.apache.cassandra.service.Cassandra$Processor$insert.process(Cassandra.java:819)
       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:624)
       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)

Oddly, a write with a consistency level of QUORUM succeeds for certain
keys (but fails with others) even though I only have one live node.",trunk@r830776,,,,,,,,,,,,,,,,,,,30/Oct/09 03:13;jbellis;524.patch;https://issues.apache.org/jira/secure/attachment/12423654/524.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:48.8,,,no_permission,,,,,,,,,,,,19737,,,Sat Oct 31 12:34:32 UTC 2009,,,,,,0|i0fzgn:,91343,,,,,,,,,,,30/Oct/09 17:43;edmond;Patched and verified the fix.,30/Oct/09 18:27;jbellis;committed,"31/Oct/09 12:34;hudson;Integrated in Cassandra #244 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/244/])
    fix hinted handoff map computation
patch by jbellis; tested by Edmond Lau for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache capacity settings done via nodetool get reset on memtable flushes,CASSANDRA-1079,12464310,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,kingryan,kingryan,5/12/2010 0:18,3/12/2019 14:15,3/13/2019 22:24,5/17/2010 22:33,0.6.2,,,,0,,,,,,"In an experiment we set cache capacities via nodetool. The config file had the KeyCache for this CF at 1000000, we set the RowCache to 10000000 via nodetool.

The next time we flushed a memtable for that CF, the cache capacity settings got reverted to what is in the conf file. We repeated the experiment with the same results.",,,,,,,,,,,,,,,,,,,,17/May/10 15:40;jbellis;1079.txt;https://issues.apache.org/jira/secure/attachment/12444694/1079.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,33:05.0,,,no_permission,,,,,,,,,,,,19985,,,Mon May 17 22:33:09 UTC 2010,,,,,,0|i0g2vb:,91895,,,,,,,,,,,12/May/10 00:33;jbellis;this is working as designed.  the config file is The Source Of Truth for settings it contains.,"12/May/10 00:43;kingryan;If we're not going to let the jmx-based setting live longer than one memtable, we should probably remove the ability to set it that way. The current behavior is too surprising.","12/May/10 00:48;jbellis;It's not surprising if you remember that the conf is The Source Of Truth. :)

I'm big on ""there should only be one way to do it"" but restarting to test cache size effects is too painful.","12/May/10 02:58;jmhodges;??I'm big on ""there should only be one way to do it"" but restarting to test cache size effects is too painful.??

I'm confused. Did you mean ""isn't""?","12/May/10 03:35;stuhood;I think the disconnect is that Ryan and Jeff would like to be able to tune cache sizes on a cluster that is receiving writes, which is difficult when it is reset per-memtable.","12/May/10 03:58;jbellis;Oh.  Sometimes I need to read things twice -- yes, it's a bug if it's not persisting between flushes.  (I'm saying that it's not supposed to persist b/t restarts, unless you change the conf file too.)","12/May/10 06:12;kingryan;Yes, ""not persisting  between restarts"" is expected, it currently gets reset when at every flush.","12/May/10 17:05;kingryan;It appears the problem is that SSTableTracker's replace method calls updateCacheSizes, which reads the cache settings from DatabaseDescriptor (which doesn't get updated by the jmx command).",17/May/10 15:40;jbellis;patch to not recalculate cache capacity if it's been manually modified,17/May/10 17:04;kingryan;Patch looks good to me.,17/May/10 22:33;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SlicePredicate does not always round-trip correctly,CASSANDRA-1049,12463696,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/4/2010 19:29,3/12/2019 14:15,3/13/2019 22:24,5/24/2010 19:45,0.6.2,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,24/May/10 18:59;jbellis;1049-0.6.txt;https://issues.apache.org/jira/secure/attachment/12445367/1049-0.6.txt,04/May/10 19:48;jbellis;1049-test.txt;https://issues.apache.org/jira/secure/attachment/12443620/1049-test.txt,24/May/10 18:11;jbellis;1049.txt;https://issues.apache.org/jira/secure/attachment/12445359/1049.txt,13/May/10 13:50;scottfines;SliceRangeSerializationTest.java;https://issues.apache.org/jira/secure/attachment/12444399/SliceRangeSerializationTest.java,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,48:49.2,,,no_permission,,,,,,,,,,,,19974,,,Mon May 24 19:45:54 UTC 2010,,,,,,0|i0g2ov:,91866,,,,,,,,,,,04/May/10 19:48;jbellis;converted Mark Schnitzius's example from the ML to a failing unit test,"04/May/10 21:37;jbellis;Jeremy reports that he can reproduce using Thrift 0.3 rc, too.","13/May/10 13:48;scottfines;I can confirm that this is also an issue with SliceRange itself, not just with SlicePredicate. I am attaching my UnitTest to this report as well(Though perhaps it should be set as a linked Issue instead).

I'm a bit new to Cassandra, so please let me know if there is something amiss with this unittest.","17/May/10 16:05;jbellis;The bug is, thrift json serialization is broken.

patch to switch to using binary serializer, converted to a hex string.  inefficient but this is not a performance-sensitive method.",24/May/10 16:22;urandom;Can you rebase please?,24/May/10 18:11;jbellis;rebased,24/May/10 18:38;jbellis;rebased to 0.6,24/May/10 18:59;jbellis;0.6 patch w/ unit test,24/May/10 19:30;urandom;+1,24/May/10 19:45;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraServiceDataCleaner.prepare() fails with IOException.,CASSANDRA-1979,12495504,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,zznate,cng1066,cng1066,1/13/2011 16:09,3/12/2019 14:14,3/13/2019 22:24,1/17/2011 17:07,0.7.1,,,,0,,,,,,"CassandraServiceDataCleaner.prepare() fails with an IOException if run in isolation.  It seems that initializing the DataDescriptor creates a new CommitLog file, and then the cleaner tries to delete this file and fails.

16:06:07.204 [main] INFO  o.a.c.config.DatabaseDescriptor - Loading settings from file:/C:/workspace/sandbox/target/classes/cassandra.yaml
16:06:07.282 [main] DEBUG o.a.c.config.DatabaseDescriptor - Syncing log with a period of 10000
16:06:07.282 [main] INFO  o.a.c.config.DatabaseDescriptor - DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
16:06:07.797 [main] DEBUG o.a.c.config.DatabaseDescriptor - setting auto_bootstrap to false
16:06:07.797 [main] INFO  o.a.c.db.commitlog.CommitLogSegment - Creating new commitlog segment target/var/lib/cassandra/commitlog\CommitLog-1294934767797.log
16:06:07.813 [main] DEBUG o.apache.cassandra.io.util.FileUtils - Deleting CommitLog-1294934767797.log
Exception in thread ""main"" java.io.IOException: Failed to delete C:\workspace\sandbox\target\var\lib\cassandra\commitlog\CommitLog-1294934767797.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:201)
	at org.apache.cassandra.contrib.utils.service.CassandraServiceDataCleaner.cleanDir(CassandraServiceDataCleaner.java:99)
	at org.apache.cassandra.contrib.utils.service.CassandraServiceDataCleaner.cleanupDataDirectories(CassandraServiceDataCleaner.java:53)
	at org.apache.cassandra.contrib.utils.service.CassandraServiceDataCleaner.prepare(CassandraServiceDataCleaner.java:44)
	at cng.sandbox.App.main(App.java:15)

This also seems to leave a bunch of threads running in the background, so the process has to be manually killed.

This was tested with the javautils in the 0.7.0 branch.",Windows XP,,,,,,,,,,,,,,,,,,,17/Jan/11 16:56;zznate;1979.txt;https://issues.apache.org/jira/secure/attachment/12468574/1979.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,55:39.6,,,no_permission,,,,,,,,,,,,20389,,,Mon Jan 17 19:48:03 UTC 2011,,,,,,0|i0g8kn:,92819,,,,,,,,,,,"14/Jan/11 22:55;zznate;This code path had not been tested in a while. Test case had the following issues:
- not using bytebuffer
- storage-config param from 0.6
- non-framed transport

Patch brings everything up to date with current trunk.",14/Jan/11 23:01;zznate;I'm not sure this belongs in contrib anymore as it will just get out of date again. I would prefer we remove it in favor of doing something smarter in refactoring out o.a.c.SchemaLoader and friends in the test/unit tree to a module of some sort and including that on the test classpath of the main build file. (In conjunction with CASSANDRA-1848 perhaps?),15/Jan/11 19:34;jbellis;patch doesn't apply to 0.7 for me w/ p0 or p1 option,17/Jan/11 16:56;zznate;Oops - diff'ed from wrong level. New patched diff'ed from top level.,"17/Jan/11 17:07;jbellis;committed

ultimately I think we should move this into Hector or a github project (CASSANDRA-1805) although it's possible we could make changes to core to make this easier.",17/Jan/11 17:22;zznate;Per my comments on CASSANDRA-1805 I'm fine appropriating this. We have an issue open to break out some other testing utils into a non-hector-core dependent 'test-utils' module anyhoo. ,"17/Jan/11 19:48;hudson;Integrated in Cassandra-0.7 #168 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/168/])
    fixes for contrib/javautils
patch by Nate McCall for CASSANDRA-1979
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Bootstrap breaks data stored (missing rows, extra rows, column values modified)",CASSANDRA-1992,12495685,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,matkor,matkor,1/15/2011 21:47,3/12/2019 14:14,3/13/2019 22:24,1/19/2011 19:32,0.7.1,,,,0,,,,,,"Scenario:
Two fresh (empty /data /commitog /saved_caches dirs) cassandra installs.
Start first one.
Run data inserting program [1],  run again in verify mode - all data intact.
Bootstrap 2nd node.
Run verification again, now it fails.

Issue is very strange to me as cassandra works perfectly for me when cluster nodes stay the same for days now but any bootstrap ( 1 -> 2 nodes, 2 -> 3 nodes, 2->3 nodes RF=2) breaks data.

I am running cassandra with 1GB heap size, 32bit userland on 64bit kernels, not sure what else could matter there.
Any hints ?
Thanks in advance, regards.

[1] simple program generating data and later verifying data.
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/test.py

[2] Logs from 1st node:
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/system-3.4.log

[3] Logs from 2nd (bootstraping node)
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/system-3.8.log

","Linux 2.6.36-1 #1 SMP Tue Nov 9 09:56:02 CET 2010 x86_64 Intel(R)_Core(TM)2_Quad_CPU____Q8300__@_2.50GHz PLD Linux
glibc-2.12-4.i686
java-sun-1.6.0.22-1.i686
",;19/Jan/11 08:34;brandon.williams;3600,28800,0,3600,12%,28800,0,3600,,,,,,,,,,,19/Jan/11 08:21;brandon.williams;1992.txt;https://issues.apache.org/jira/secure/attachment/12468740/1992.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:57.9,,,no_permission,,,,,,,,,,,,20391,,,Tue Feb 01 13:41:15 UTC 2011,,,,,,0|i0g8nj:,92832,jbellis,jbellis,,,,,,,,,"15/Jan/11 22:27;matkor;Not sure if it is important but:

After loading data ring looks like:
Address         Status State   Load            Owns    Token
192.168.3.4     Up     Normal  59.02 KB        100.00% 0

After bootstraping:
192.168.3.4     Up     Normal  199.28 KB       56.45%  0
192.168.3.8     Up     Normal  115.03 KB       43.55%  74091174110465149971373554442555361956

Load gets tripled on 1st node.","16/Jan/11 03:21;stuhood;Are both nodes reporting the same ring (via nodetool ring) after the bootstrap? The last entry in 3.8.log indicates that it thinks 3.4 is dead, but this might just be because you stopped the nodes before collecting the logs.

Also, what exactly is the error you get from your script?","16/Jan/11 10:51;matkor;Stu,
Yes both nodes show same via nodetool ring: 
192.168.3.4 Up Normal 199.28 KB 56.45% 0
192.168.3.8 Up Normal 115.03 KB 43.55% 74091174110465149971373554442555361956

You are right, I stopped cluster by stopping 3.4 first. (Anyway it would be good to store information of gracefull shutdown of node in system.log, IMHO) 

Error is:
[matkor@laptop-hp ~/src/caswife]$ python test.py
column_family: 'CF0'
row_key: 'row=0 '
row_key: 'row=1 \x00'
row_key: 'row=2 \x00\x01'
row_key: 'row=3 \x00\x01\x02'
row_key: 'row=4 \x00\x01\x02\x03'
row_key: 'row=5 \x00\x01\x02\x03\x04'
row_key: 'row=6 \x00\x01\x02\x03\x04\x05'
Traceback (most recent call last):
  File ""test.py"", line 36, in <module>
    loaded_cols_dict = current_cf.get(row_key)
  File ""/usr/share/python2.7/site-packages/pycassa/columnfamily.py"", line 362, in new_f
  File ""/usr/share/python2.7/site-packages/pycassa/columnfamily.py"", line 429, in get
pycassa.cassandra.ttypes.NotFoundException: NotFoundException()

So program found 7 rows but failed to find 8th as from my understanding of pycassa.","16/Jan/11 11:31;matkor;One more thing:
I repeated setup today, also reaching missing row(s) but 2nd node got different token 61078635599166706937511052402724559481
Following
http://wiki.apache.org/cassandra/Operations#Load_balancing ,
having 1st node token set to 0 and using RandomPartitioner, I would expect  2nd node toke to be set always in middle of token space to
850705917302346158658436518579420528
?

","16/Jan/11 12:23;matkor;Another thing: 
By luck, I discovered that restarting one or both nodes makes data to be served again intact.
To avoid picking token 2nd node  randomness I set it explicitly to 85070591730234615865843651857942052864
.
So scenario is now:
Clean pycassa installs with tokens set to 0 and 85070591730234615865843651857942052864.

Starting first node [1], uploading data, verifing ok, ring:
192.168.3.4     Up     Normal  59.02 KB        100.00% 0

Bootstraping 2nd node, waiting to finish streaming, data verification bad:
column_family: 'CF0'
row_key: 'row=0 '
Traceback (most recent call last):
  File ""test.py"", line 36, in <module>
    loaded_cols_dict = current_cf.get(row_key)
  File ""/usr/share/python2.7/site-packages/pycassa/columnfamily.py"", line 362, in new_f
  File ""/usr/share/python2.7/site-packages/pycassa/columnfamily.py"", line 429, in get
pycassa.cassandra.ttypes.NotFoundException: NotFoundException()
Final ring (same on both nodes):
192.168.3.4     Up     Normal  199.28 KB       50.00%  0
192.168.3.8     Up     Normal  135.7 KB        50.00%  85070591730234615865843651857942052864

Restaring 192.168.3.4, data _same_ _error_ as above, ring changes to:
192.168.3.4     Up     Normal  201.51 KB       50.00%  0
192.168.3.8     Up     Normal  135.7 KB        50.00%  85070591730234615865843651857942052864

Restarting 192.168.3.8, data _verified_ _ok_ , ring changes on (same on both nodes) to:
192.168.3.4     Up     Normal  201.51 KB       50.00%  0
192.168.3.8     Up     Normal  145.8 KB        50.00%  85070591730234615865843651857942052864

[1] Logs from 1st 192.168.3.4 node:
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/logs_with_restart/system-3.4.log

[2] Logs from 2nd 192.168.3.8 node:
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/logs_with_restart/system-3.8.log

HIH, regards","16/Jan/11 18:25;brandon.williams;Since the issue appears to be a missing row, can you reproduce with contrib/py_stress?","16/Jan/11 20:44;matkor;Brandon, yes and no ;).
Unable with original contrib/py_stress as it uses only one CF to do all tests. Most of my issues looks like missing row in 2nd CF or broken data in 2nd CF (like contents of 1st CF injected into 2nd CF).
I slightly modified contrib/py_stress so it created 3 standard CFs and 3 super CFs [1] and allows to select one wants to operate via --column_family_idx= switch and I can reproduce:

Starting 1st node.

$ python stress.py --nodes 192.168.3.8 --operation insert  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=1       
Created keyspaces.  Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00823852300644,0
$ python stress.py --nodes 192.168.3.8 --operation insert  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=2 
Keyspace already exists.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00132475852966,0
$ python stress.py --nodes 192.168.3.8 --operation insert  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=3
Keyspace already exists.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00138550519943,0

Verification of data in each CF:
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=3
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00282711744308,0
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=2
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00149053096771,0
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=1
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00125009775162,0

Bootstrap 2nd node and now failure:
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=1
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00376108169556,0
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=2
Key 074 not found
Key 061 not found
Key 047 not found
( cut 40 more Key 0xx not found)
Key 047 not found
Key 042 not found
Key 058 not found
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Key 033 not found
100,20,20,0.00241538286209,0

Similar failure for 3rd CF.

[1]: Modified stress.py from 0.7.0 with --column_family_idx= added.
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/stress.py
","16/Jan/11 20:51;matkor;And again, restarting first node, cuts number of missing row by more or less half, restarting 2nd node cures all missing rows.","17/Jan/11 09:33;ivol;I have the exact same problem with an existing installation and was preparing to create an issue for it, but found this issue just before creating it. I'll describe the issue I have, maybe that provides some relevant information.

I ran into this issue with Cassandra 0.7 trying to add just one node to an existing one-node cluster. The existing node contains already some data when the second node is added to the cluster. This is what I did:

Setup
I have two nodes both running on Linux; a server called 'veers' on 172.16.2.203 and a 'r2d2' on 172.16.2.206. I use Cassandra 0.7 and only change the following settings in the cassandra.yaml and log4j-server.properties (I use the default values for all other entries):

In cassandra.yaml:

initial_token: 0
data_file_directories: /vol/users/ivol/cassandra_work/data
commitlog_directory: /vol/users/ivol/cassandra_work/commitlog
saved_caches_directory: /vol/users/ivol/cassandra_work/saved_caches
seeds: 172.16.2.203
listen_address: 172.16.2.203
rpc_address: 172.16.2.203

In log4j-server.properties:

log4j.appender.R.File=/vol/users/ivol/cassandra_work/system.log


Now I start the first node and connect it using cassandra-cli. I add the following keyspace, column families and rows:

create keyspace Default;
use Default;

create column family Role;
set Role['user_1']['name'] = 'User 1';
set Role['user_2']['name'] = 'User 2';
set Role['user_3']['name'] = 'User 3';

create column family Gadget;
set Gadget['gadget_1']['name'] = 'Gadget 1';
set Gadget['gadget_2']['name'] = 'Gadget 2';
set Gadget['gadget_3']['name'] = 'Gadget 3';

After this 'list Role' and 'list Gadget' return the proper rows.

Now I append a second node to the cluster, with this configuration:

In cassandra.yaml:

initial_token:
auto_bootstrap: true
data_file_directories: /vol/users/ivol/cassandra_work/data
commitlog_directory: /vol/users/ivol/cassandra_work/commitlog
saved_caches_directory: /vol/users/ivol/cassandra_work/saved_caches
seeds: 172.16.2.203
listen_address: 172.16.2.206
rpc_address: 172.16.2.206

In log4j-server.properties:

log4j.appender.R.File=/vol/users/ivol/cassandra_work/system.log


Now I start the second node. Bootstrapping takes some time, about 2 minutes in total but finishes without any warnings or errors:

...
INFO [main] 2011-01-17 09:58:09,170 StorageService.java (line 399) Joining: getting load information
INFO [main] 2011-01-17 09:58:09,171 StorageLoadBalancer.java (line 366) Sleeping 90000 ms to wait for load information...
INFO [GossipStage:1] 2011-01-17 09:58:10,447 Gossiper.java (line 577) Node /172.16.2.203 is now part of the cluster
INFO [HintedHandoff:1] 2011-01-17 09:58:11,451 HintedHandOffManager.java (line 192) Started hinted handoff for endpoint /172.16.2.203
INFO [GossipStage:1] 2011-01-17 09:58:11,451 Gossiper.java (line 569) InetAddress /172.16.2.203 is now UP
INFO [HintedHandoff:1] 2011-01-17 09:58:11,453 HintedHandOffManager.java (line 248) Finished hinted handoff of 0 rows to endpoint /172.16.2.203
INFO [main] 2011-01-17 09:59:39,189 StorageService.java (line 399) Joining: getting bootstrap token
INFO [main] 2011-01-17 09:59:39,203 BootStrapper.java (line 148) New token will be 110533280274756817580689726417060138498 to assume load from /172.16.2.203
INFO [main] 2011-01-17 09:59:39,265 StorageService.java (line 399) Joining: sleeping 30000 ms for pending range setup
INFO [main] 2011-01-17 10:00:09,272 StorageService.java (line 399) Bootstrapping
INFO [main] 2011-01-17 10:00:09,663 CassandraDaemon.java (line 77) Binding thrift service to /172.16.2.206:9160
INFO [main] 2011-01-17 10:00:09,666 CassandraDaemon.java (line 91) Using TFramedTransport with a max frame size of 15728640 bytes.
INFO [main] 2011-01-17 10:00:09,671 CassandraDaemon.java (line 119) Listening for thrift clients...

Although everything seemed to worked just fine, when node 2 is completely finished bootstrapping the rows in the 'Role' and 'Gadget' Column Families are messed up;

list Role;

-------------------
RowKey: user_3
=> (column=6e616d65, value=557365722033, timestamp=1295254678545000)

1 Row Returned.


list Gadget;

-------------------
RowKey: user_2
=> (column=6e616d65, value=557365722032, timestamp=1295254678514000)
-------------------
RowKey: gadget_2
=> (column=6e616d65, value=4761646765742032, timestamp=1295254678805000)
-------------------
RowKey: gadget_3
=> (column=6e616d65, value=4761646765742033, timestamp=1295254679429000)
-------------------
RowKey: gadget_1
=> (column=6e616d65, value=4761646765742031, timestamp=1295254678771000)
-------------------
RowKey: user_1
=> (column=6e616d65, value=557365722031, timestamp=1295254678449000)

5 Rows Returned.

So 2 rows have been moved from CF 'Role' to 'Gadget', just by adding a node to the cluster. The actual result differs each time I try, but always some rows have been moved to some other CF. The problem seems the same as the one described by Mateusz.

I also found out that restarting the nodes seems to 'fix' the issue. Also changing the replication factor from 1 to 2 most of the times 'resolves' the issue.","17/Jan/11 15:57;jbellis;if restarting nodes fixes it, it sounds like the streamed data is not getting wired in correctly to the sstabletracker","19/Jan/11 08:21;brandon.williams;There were two bugs here in StreamInSession.  First, it was adding all streamed sstables to the last CFS it saw.  Secondly, secondary index generation was being performed against all sstables seen.  This patch switches from a scalar CFS and a separate list of all sstables to a hash of lists where the CFS is the key and the value is the sstables that belong to it.","19/Jan/11 14:29;jbellis;I think we need to get rid of StreamInSession.table and StreamHeader.table too, then.  (But leave it on the wire protocol as an empty string, for compatibility w/ 0.7.0)",19/Jan/11 19:24;nickmbailey;This would be a good test to have in the distributed test set up.,"19/Jan/11 19:32;jbellis;Brandon said on IRC: ""maybe the table is there for some kind of reason, and the sessions are separated by keyspace.""

This is correct, so I'm going to leave the table field even though it's currently not referenced.  So it's not dead weight. I added an assert that each sstable we're adding at the end of the session belongs to the table that the sender said it was for.

Also added an assert to CFS.addSSTable to verify that the SSTR being added does belong to the CF it is being added to.

Committed w/ those two additions.","19/Jan/11 19:52;hudson;Integrated in Cassandra-0.7 #177 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/177/])
    fix streaming of multiple CFs during bootstrap
patch by brandonwilliams; reviewed by jbellis for CASSANDRA-1992
",01/Feb/11 13:41;jdamick;is there is any way to repair the problem without deleting all of my data? (shutting down and bringing back up did not solve the problem),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system.test_thrift_server.TestMutations.test_batch_mutate_standard_columns appears to be non deterministic,CASSANDRA-944,12460991,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,mdennis,mdennis,4/2/2010 3:02,3/12/2019 14:14,3/13/2019 22:24,4/6/2010 21:33,0.7 beta 1,,Legacy/Tools,,0,,,,,,"system.test_thrift_server.TestMutations.test_batch_mutate_standard_columns appears to be non deterministic.  The first time I ran the thrift tests after a clean checkout it failed.  However, it did not fail the ~10 times after that.

{code}
mdennis@mdennis:~/c/cassandra$ nosetests test/system/test_thrift_server.py 
...........E....................................
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_batch_mutate_standard_columns
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.6/nose/case.py"", line 183, in runTest
    self.test(*self.arg)
  File ""/home/mdennis/c/cassandra/test/system/test_thrift_server.py"", line 318, in test_batch_mutate_standard_columns
    _assert_column('Keyspace1', column_family, key, 'c1', 'value1')
  File ""/home/mdennis/c/cassandra/test/system/test_thrift_server.py"", line 43, in _assert_column
    raise Exception('expected %s:%s:%s:%s:%s, but was not present' % (keyspace, column_family, key, column, value) )
Exception: expected Keyspace1:Standard1:key_27:c1:value1, but was not present

----------------------------------------------------------------------
Ran 48 tests in 184.700s

FAILED (errors=1)
{code}","xubuntu 9.10
Linux mdennis 2.6.31-20-generic #58-Ubuntu SMP Fri Mar 12 05:23:09 UTC 2010 i686 GNU/Linux
",,,,,,,,,,,,,,,,,,,06/Apr/10 16:19;brandon.williams;944.patch;https://issues.apache.org/jira/secure/attachment/12440920/944.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,22:02.1,,,no_permission,,,,,,,,,,,,19932,,,Wed Apr 07 12:41:54 UTC 2010,,,,,,0|i0g21j:,91761,,,,,,,,,,,"02/Apr/10 11:22;gdusbabek;I bet you're running a mac.

If you increase the time.sleep(0.1) to a larger value, the problem will go away.  The problem is that the operation being tested is happening at ConsistencyLevel.ZERO and you are feeling the 'eventual' part of 'eventual consistency.'

I wonder if it would be a good idea to make the sleep value a constant and have it set to 0.15 if MacOS is detected.","02/Apr/10 17:24;mdennis;Sorry, not on a mac, on Linux (see environment field for details )

I was thinking more like lowering the sleep, but putting it in a loop with a much higher max wait time (like an entire second).

Assuming the test is trying to verify that eventually the correct data shows up (and is not trying to verify the data shows up in under 0.1 seconds), then I believe something similar to the following would address the issue.

wait = 1s
start = now
verified = false
while not (verified = try_to_verify) and now < start+wait:
  sleep(0.05)

if not verified:
  fail_test


thoughts?
","02/Apr/10 18:39;gdusbabek;That would work.  Maybe something like a assert_within(func, wait) function.  If func doesn't return true within wait, then the test fails.",06/Apr/10 16:19;brandon.williams;Patch to add a loop as described and use it in conjunction with CL.ZERO,06/Apr/10 21:33;jbellis;committed,"07/Apr/10 12:41;hudson;Integrated in Cassandra #400 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/400/])
    fix heisenbug in system tests, especially common on OS X.  patch by Brandon Williams; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect neighbor calculation in repair,CASSANDRA-924,12460506,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,hujn,hujn,3/29/2010 6:00,3/12/2019 14:14,3/13/2019 22:24,4/5/2010 21:04,0.6.1,,,,0,,,,,,"With Replicationfactor=2, if a server is brought down and its data directory wiped out, it doesn't restore its data replica after restart and nodeprobe repair.
Steps to reproduce:
1) Bring up a cluster with three servers cs1,2,3, with their initial token set to 'foo3', 'foo6', and 'foo9', respectively. ReplicationFactor is set to 2 on all 3.
2) Insert 9 columns with keys from 'foo1' to 'foo9', and flush. Now I have foo1,2,3,7,8,9 on cs1, foo1,2,3,4,5,6, on cs2, and foo4,5,6,7,8,9
on cs3. So far so good
3) Bring down cs3 and wipe out its data directory
4) Bring up cs3
5) run nodeprobe repair Keyspace1 on cs3, the flush
At this point I expect to see cs3 getting its data back. But there's nothing in its data directory. I also tried getting all columns with
ConsistencyLevel::ALL to see if that'll do a read pair. But still cs3's data directory is empty.
",CentOS 5.2,,,,,,,,,,,,,,,,,,,02/Apr/10 05:08;stuhood;0001-Calculate-neighbors-using-getRangeToEndpointMap.patch;https://issues.apache.org/jira/secure/attachment/12440577/0001-Calculate-neighbors-using-getRangeToEndpointMap.patch,02/Apr/10 05:08;stuhood;0002-Always-trigger-streaming-repairs.patch;https://issues.apache.org/jira/secure/attachment/12440578/0002-Always-trigger-streaming-repairs.patch,02/Apr/10 05:08;stuhood;0003-Unit-test-for-AEService-neighbor-calculation.patch;https://issues.apache.org/jira/secure/attachment/12440579/0003-Unit-test-for-AEService-neighbor-calculation.patch,30/Mar/10 00:39;hujn;cs1.log;https://issues.apache.org/jira/secure/attachment/12440155/cs1.log,29/Mar/10 21:33;hujn;cs1.log;https://issues.apache.org/jira/secure/attachment/12440113/cs1.log,30/Mar/10 00:39;hujn;cs2.log;https://issues.apache.org/jira/secure/attachment/12440156/cs2.log,29/Mar/10 21:33;hujn;cs2.log;https://issues.apache.org/jira/secure/attachment/12440114/cs2.log,30/Mar/10 00:39;hujn;cs3.log;https://issues.apache.org/jira/secure/attachment/12440157/cs3.log,29/Mar/10 21:33;hujn;cs3.log;https://issues.apache.org/jira/secure/attachment/12440115/cs3.log,29/Mar/10 16:59;hujn;error.log;https://issues.apache.org/jira/secure/attachment/12440082/error.log,29/Mar/10 16:09;hujn;error.log;https://issues.apache.org/jira/secure/attachment/12440075/error.log,29/Mar/10 06:02;hujn;error.log;https://issues.apache.org/jira/secure/attachment/12440040/error.log,29/Mar/10 06:02;hujn;storage-conf.xml;https://issues.apache.org/jira/secure/attachment/12440039/storage-conf.xml,13,,,,,,,,,,,,,,,,,,,32:32.3,,,no_permission,,,,,,,,,,,,19922,,,Mon Apr 05 21:04:38 UTC 2010,,,,,,0|i0g1x3:,91741,,,,,,,,,,,29/Mar/10 06:02;hujn;conf file used in the test and error log,"29/Mar/10 14:32;stuhood;Do you have the logs from one of the other machines? AntiEntropyService on the failed box appears to have given the correct result, but both of the other boxes should have had tree mismatches.",29/Mar/10 16:09;hujn;log file from cs1,"29/Mar/10 16:35;rschildmeijer;I could be wrong but isn't this exception very simliar to the ""upgrade to jdk build 18""-problem?",29/Mar/10 16:59;hujn;log file from cs2,"29/Mar/10 18:47;stuhood;This one is pretty serious... apparently AntiEntropyService is always calculating who to initiate repairs with using getNaturalEndpoints(mytoken), so when RF is less than the number of nodes, it is likely that trees are only sent in one direction (clockwise around the ring). Additionally, we need to do the inverse of getNaturalEndpoints, and determine which other nodes we are holding replicas for.","29/Mar/10 19:44;stuhood;Uses getRangeToAddressMap to calculate all endpoints that node A is storing replicas for, in addition to endpoints storing replicas for node A.","29/Mar/10 20:06;stuhood;Backport of the 924.patch for 0.5.

Jianing: would you mind testing the appropriate patch for your version?","29/Mar/10 21:33;hujn;still doesn't work for me. i've attached logs from all servers.
(i added the ""get neighbors"" log just to be sure i was running the patched code)","29/Mar/10 22:51;stuhood;Soo, we resolved one issue, and exposed another. Because less than 5% of possible keys (in the UTF-8 space) were out of sync between the nodes, AntiEntropyService decided not to do a repair.

I'll update the patches with a solution this evening. Thanks for your patience!","29/Mar/10 23:39;jbellis;is the problem that it's guessing for % of ""utf8 space"" instead of ""keys actually on the servers?""","29/Mar/10 23:53;stuhood;Yea, it's a naive comparison based only on the trees. But fixing the stubbed out 'range read-repair' option is much easier.","30/Mar/10 00:06;stuhood;Patches updated to remove stubbed out 'range read-repair' option.

Jianing: can you give your test one more try?",30/Mar/10 00:39;hujn;still no luck. log files attached.,"30/Mar/10 01:34;stuhood;I think you might have used the wrong version of the patch (I see log entries that I removed in the most recent version). I just deleted the older version, and I've tested that this version works on a 3 node cluster with RF=2.

Very sorry to use you like a guinea pig like that, but thank you very much for trying out each version.","30/Mar/10 01:53;hujn;Sorry my bad. I was indeed using the wrong patch in my last test (wget renamed the file). The latest patch works!

Thank you so much for bearing with me and getting it fixed so fast.",30/Mar/10 02:04;jbellis;is this something we can add a unit test for?,"02/Apr/10 05:08;stuhood;Patchset for 0.6 and trunk, including a test for neighbor calculation.","05/Apr/10 19:44;jbellis;why does the test patch add

endpoints.add(FBUtilities.getLocalAddress());

to forceTableRepair?","05/Apr/10 19:49;stuhood;> why does the test patch add endpoints.add to forceTableRepair?
Rather than implementing the neighbor calculation in two places, I wanted to reuse the logic in AES. In addition to the neighbors, forceTableRepair sends a tree request to localhost, so we add it back to the neighbor list.",05/Apr/10 21:04;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In a cluster, get_range_slices() does not return all the keys it should",CASSANDRA-1198,12467113,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,cgist,cgist,6/16/2010 18:26,3/12/2019 14:14,3/13/2019 22:24,6/25/2010 22:12,0.6.3,,,,0,,,,,,"Row iteration with get_range_slices() does not return all keys. This behaviour only occurs with more than one node and depends on how the nodes are located on the ring.

To reproduce, insert some records into a cluster with more than one node. A subsequent row iteration will return fewer records than were inserted. This has been observed when 1) inserting into a single node, bootstrapping a second node then using get_range_slices() and 2) inserting into a cluster of several nodes then using get_range_slices().

This appears to be similar to https://issues.apache.org/jira/browse/CASSANDRA-781","Linux 2.6.18-128.1.10.el5.xs5.5.0.51xen
Java build 1.6.0_17-b04
Cassandra 0.7 2010-06-15 including patch for https://issues.apache.org/jira/browse/CASSANDRA-1130",,,,,,,,,,,,,,,,,,,20/Jun/10 05:51;jbellis;1198.txt;https://issues.apache.org/jira/secure/attachment/12447546/1198.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,39:37.4,,,no_permission,,,,,,,,,,,,20031,,,Fri Jun 25 22:12:51 UTC 2010,,,,,,0|i0g3lj:,92013,,,,,,,,,,,"16/Jun/10 18:39;jbellis;Christopher reported on IRC that he cannot reproduce in 0.6, so this does appear to be a 0.7-only bug.","20/Jun/10 03:58;cgist;It looks like this issue is a regression of https://issues.apache.org/jira/browse/CASSANDRA-1042 so it should in fact affect 0.6.3, though I haven't confirmed.

When doing a row iteration starting from token 0, since the ranges are no longer sorted by ring order after #1042 but by wrap order, the iteration could start with the range to the left of 0. This results in either iterating through fewer than all keys, or iterating through duplicate keys. I have seen the following different behaviour, varying based on the ring tokens, key count and distribution, and range count:
1) too few keys because range slice command had lower count than keys in (z,0] range and subsequent range slice was restricted to only the (z,0] range
2) duplicate keys because range slice command had higher count than keys in (z,0] range and keys in range (z,0] were repeated
3) too few keys because ranges were handled out of token order, eg. (z,0] before (y,z] so no keys in (y,z] were returned

Also it would appear that token ranges passed into get_range_slices() are in fact start inclusive, contrary to the wiki. Is this correct?","20/Jun/10 05:51;jbellis;Excellent work.  Yes, the sort that was removed was in fact necessary (which is stated in the docstring to getRestrictedRanges, but then the sort was done elsewhere so the confusion was understandable).

This patch adds back the sort (this time in gRR) and moves the endpoints-handling to getRangeSlice (where doing just-in-time liveness checking has the added benefit of improving availability for large queries) to make gRR less muddled.

(Token-based range queries are turned into Bounds objects, which are start-inclusive, as opposed to Range objects, which are not.)","20/Jun/10 11:52;jeromatron;Oh crumb!

We had fixed the problem manifest in 1045 but overlooked that part of it. Nice catch.

I'll go ahead and try that patch out on the word count example to make sure it works tomorrow.

Thanks Christopher and Jonathan.","21/Jun/10 15:57;jeromatron;Hmmm, I tried latest vanilla 0.6 branch with this patch and the duplicates have come back in the word count example. ",25/Jun/10 22:09;urandom;+1,25/Jun/10 22:12;jbellis;committed.  (CASSANDRA-1042 is re-opened for fixing the original bug.),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""java.net.ConnectException: Connection timed out"" in MESSAGE-STREAMING-POOL:1",CASSANDRA-1019,12462907,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,btoddb,btoddb,4/23/2010 21:12,3/12/2019 14:14,3/13/2019 22:24,5/26/2010 19:00,0.6.3,0.7 beta 1,,,0,,,,,,"after doing a nodetool repair on a node in my cluster, i see the following exception on 4 out of the 7 nodes.  replication factor is 3.  no compactions happening.  no client traffic to the cluster.  nodetool streams (on one of the nodes not repaired) shows the following which is not ever increasing:

Mode: Normal
Streaming to: /192.168.132.117
   /data/cassandra-data/data/UdsProfiles/stream/UdsProfiles-43-Data.db 0/523088443
Not receiving any streams.


in addition those same four nodes all show AE-SERVICE-STAGE with pending
work, and been showing this for several hours now. each node in the
cluster has less than 2gb, so it should be finished by now.

here is the exception:

2010-04-23 10:08:43,416 ERROR [MESSAGE-STREAMING-POOL:1]
[DebuggableThreadPoolExecutor.java:101] Error in ThreadPoolExecutor
java.lang.RuntimeException: java.net.ConnectException: Connection timed out
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.ConnectException: Connection timed out
at sun.nio.ch.Net.connect(Native Method)
at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:507)
at org.apache.cassandra.net.FileStreamTask.runMayThrow(FileStreamTask.java:60)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
... 3 more
2010-04-23 10:08:43,417 ERROR [MESSAGE-STREAMING-POOL:1]
[CassandraDaemon.java:78] Fatal exception in thread
Thread[MESSAGE-STREAMING-POOL:1,5,main]
java.lang.RuntimeException: java.net.ConnectException: Connection timed out
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.ConnectException: Connection timed out
at sun.nio.ch.Net.connect(Native Method)
at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:507)
at org.apache.cassandra.net.FileStreamTask.runMayThrow(FileStreamTask.java:60)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
... 3 more

",,,,,,,,,,,,,,,,,,,,24/May/10 20:50;stuhood;1019-for-0.6-0001-Add-exponentially-backed-off-retry-to-FileStreamTask.patch;https://issues.apache.org/jira/secure/attachment/12445382/1019-for-0.6-0001-Add-exponentially-backed-off-retry-to-FileStreamTask.patch,24/May/10 20:50;stuhood;1019-for-trunk-0001-Rename-CompletedFileStatus-to-FileStatus-to-indicate.patch;https://issues.apache.org/jira/secure/attachment/12445383/1019-for-trunk-0001-Rename-CompletedFileStatus-to-FileStatus-to-indicate.patch,24/May/10 20:50;stuhood;1019-for-trunk-0002-Rename-StreamCompletionHandler-to-FileStatusHandler-.patch;https://issues.apache.org/jira/secure/attachment/12445384/1019-for-trunk-0002-Rename-StreamCompletionHandler-to-FileStatusHandler-.patch,24/May/10 20:50;stuhood;1019-for-trunk-0003-Rename-StreamCompletionAction-to-Action-and-change-d.patch;https://issues.apache.org/jira/secure/attachment/12445385/1019-for-trunk-0003-Rename-StreamCompletionAction-to-Action-and-change-d.patch,24/May/10 20:52;stuhood;1019-for-trunk-0004-Add-exponentially-backed-off-retry-to-FileStreamTask.patch;https://issues.apache.org/jira/secure/attachment/12445386/1019-for-trunk-0004-Add-exponentially-backed-off-retry-to-FileStreamTask.patch,,,,,,,,,5,,,,,,,,,,,,,,,,,,,19:02.4,,,no_permission,,,,,,,,,,,,19958,,,Thu May 27 12:47:28 UTC 2010,,,,,,0|i0g2i7:,91836,,,,,,,,,,,"25/Apr/10 05:19;jbellis;Stu, could you add some ""retry N (3?  10?) times before aborting the stream"" logic?",25/Apr/10 21:34;stuhood;I should be able to tackle this Tuesday or Wednesday.,"26/Apr/10 16:15;btoddb;some more info that may assist.  we have just purchased new machines for our test cluster and we are having lots of trouble with the NICs going down.  this causes an extremely long timeout situation and could have been the catalyst for this problem.

this situation does cause the cluster to behave very poorly because the connection takes several minutes to timeout.  this type of situation makes me want the ability to manually take a node out of the cluster and prevent nodes from gossiping to it.  is this something that has been talked about?",24/May/10 20:50;stuhood;Adds retries with exponential backoff to the connect step in FileStreamTask.,"24/May/10 20:52;stuhood;...and a set that does the same thing for trunk. 1019-for-0.6-0001 should be the same as 1019-for-trunk-0004.

The rest of the set performs some cleanups in the Streaming package to give things more appropriate names and document them a little better.",26/May/10 15:41;gdusbabek;+1 on the 0.6 change.  Re trunk: would it make sense to rename FileStatus.STREAM -> FileStatus.RESTREAM?,"26/May/10 16:12;stuhood;> Re trunk: would it make sense to rename FileStatus.STREAM -> FileStatus.RESTREAM?
I debated doing that, but since the FileStatus object exists for the lifetime of the transfer it needs to have an initial Action/status that indicates it is streaming.","27/May/10 12:47;hudson;Integrated in Cassandra #447 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/447/])
    rename StreamCompletionAction to Action. patch by stuhood, reviwed by gdusbabek. CASSANDRA-1019
rename StreamCompletionHandler to FileStatusHandler. patch by stuhood, reviwed by gdusbabek. CASSANDRA-1019
rename CompletedFileStatus to FileStatus. patch by stuhood, reviwed by gdusbabek. CASSANDRA-1019
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_range_slice returns multiple copies of each row for ConsistencyLevel > ONE,CASSANDRA-884,12458977,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,omerhj,omerhj,omerhj,3/12/2010 20:50,3/12/2019 14:14,3/13/2019 22:24,3/19/2010 20:36,0.6,,,,0,,,,,,"I've noticed that both 0.5.1 and 0.6b2 return multiple identical copies of the data stored in my keyspace whenever I make a call to get_range_slice or get_range_slices using
ConsistencyLevel.QUORUM and ReplicationFactor is greater than one.

So with ReplicationFactor set to 2 for my application's KeySpace I get double the number of KeySlices that I expect to get. When using ConsistencyLevel.ONE I get only one KeySlice for each row.

I've seen this happen with Cassandra 0.5.1 and with 0.6 beta 2. The behavior on 0.6 beta 2 is exhibited with both get_range_slice and get_range_slices.

The attached Java program demonstrates the issue for 0.6 beta 2. The program writes a series of single-column rows into the Standard1 table, and then uses get_range_slice to receive a list of all row. The returned number of rows is consistently twice the number of rows written to the database. I wipe out the database completely before running the test.
",4-cluster Gentoo Linux 2.6.18 with a ReplicationFactor of 2,,,,,,,,,,,,,,,,,,,16/Mar/10 20:28;omerhj;0001-RangeSliceResponseResolver.patch;https://issues.apache.org/jira/secure/attachment/12438957/0001-RangeSliceResponseResolver.patch,16/Mar/10 20:57;jbellis;884-v2.txt;https://issues.apache.org/jira/secure/attachment/12438962/884-v2.txt,12/Mar/10 20:53;omerhj;TestApp2.java;https://issues.apache.org/jira/secure/attachment/12438639/TestApp2.java,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,57:48.8,,,no_permission,,,,,,,,,,,,19902,,,Mon May 31 23:29:19 UTC 2010,,,,,,0|i0g1o7:,91701,,,,,,,,,,,12/Mar/10 20:53;omerhj;The attached Java program demonstrates that multiple copies of rows are returned by get_range_slice. At least it does here :-),"16/Mar/10 20:28;omerhj;I think I have identified the source of my problem. I'm still new to the Cassandra source code so I could have this completely wrong. The patch is for the 0.6 version of org.apache.cassandra.service.RangeSliceResponseResolver.

Removing redundant copies of returned rows appears to happen in the RangeSliceResponseResolver class. This class uses an anonymous innner class that extends ReducingIterator to weed out the duplicates.

ReducingIterator provides a computeNext() method that compares successive items to see if they are duplicates. It does this by comparing the current and previous ('last') items to its isEqual() method.

RangeSliceResponseResolver does not override that isEqual method. That causes Pair<Row, InetAddress> objects to be compared with each other.  The ReducingIterator.isEqual method always returns false, because (1)  Row doesn't specify an equals() method and (2) even if it did, the InetAddresses of rows retrieved from different Cassandra instances would still be different. This causes each row to be seen as unique.

The attached patch repairs this by providing the ReducingIterator derivative in RangeSliceResponseResolver with an isEqual() method that compares the key and cf members of the Row objects. It ignores the InetAddress component of the Pair.

Alternatively I could have added an equals() method to Row which would have simplified the isEqual() method in RangeSliceResponseResolver.java a bit.

","16/Mar/10 20:57;jbellis;Your analysis is spot on, good work there.

I think though that equals should just look at the key: we want to collect (in the ""reduced"" list) all versions of the rows associated with that key, so we can repair any differences.  If we only collect identical versions together then the repair is a no-op.

Attached is a diff that makes this change and adds a missing versionSources.clear() call.",16/Mar/10 20:58;jbellis;Can you test v2?,"18/Mar/10 16:05;omerhj;Your v2 patch works for me. The attached TestApp2 now gives back the expected result. Also, all JUnit test cases for my Cassandra-using application now pass. Thanks!
","18/Mar/10 16:25;jbellis;committed, thanks!","19/Mar/10 18:35;omerhj;We've started using the wrapper API I've written for Cassandra for light development here and quite unexpectedly I've started getting NullPointerExceptions whenever a get_range_slices request is performed against a particular CF. The CF in question has a replication factor of 2. get_range_slices still works fine against the Standard1 CF that's exercised by the TestApp2 file included in this ticket. In my storage-conf.xml Standard1 is set up with a replication factor of 3.

Here is the stack trace I'm seeing over and over again, on each of my 4 servers:

ERROR [pool-1-thread-64] 2010-03-19 12:23:30,119 Cassandra.java (line 1440) Internal error processing get_range_slices
java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceResponseResolver$2.isEqual(RangeSliceResponseResolver.java:81)
        at org.apache.cassandra.service.RangeSliceResponseResolver$2.isEqual(RangeSliceResponseResolver.java:74)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:135)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:130)
        at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:101)
        at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:41)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:86)
        at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:592)
        at org.apache.cassandra.thrift.CassandraServer.getRangeSlicesInternal(CassandraServer.java:587)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:559)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.process(Cassandra.java:1432)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1115)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

Additional notes:
I'm using revision 924837 of the Cassandra 0.6 branch which includes the patch above. For some reason the line numbers in RangeSliceResponseResolver seem to be off by one in the stack trace, but I'm positive that it's the patched version being used with the isEqual() method that tests only the keys.

I guess this might be fixed by adding a few null checks to that isEqual() method, but perhaps that would just be hiding a problem somewhere else in the code.
","19/Mar/10 18:56;jbellis;Can you add this to isEqual to see which part is null?  You're right, that ""shouldn't happen,"" let's not just band-aid w/ null checks.

                assert o1 != null && o2 != null : ""null pair"";
                assert o1.left != null && o2.left != null : ""null row"";
","19/Mar/10 20:26;omerhj;During my last build an apache-casssandra-0.6-b2.jar somehow made its way into the lib directories, overriding the newer 0.6-b3.jar that I should have been using. I removed the older apache-casssandra-0.6-b2.jar file that contained my older patch, restarted my instances and the problem disappeared. So it looks likely that this was operator error on my part. 




","19/Mar/10 20:36;jbellis;k, marking closed again","31/May/10 21:07;ajslater;Please re-open or duplicate.

Testing with 0.6-trunk today:

Reading with CL > ONE returns multiple copies of the same column per key consistent with the replicas queried before return. i.e,  for RC=3, a QUORUM read yields 2 copies and an ALL read returns 3.
This is with pycassa get_range() which is using get_range_slice()

I see the same behavior with 0.6.1 and 0.6.2 debs

If my experience is not unique, anyone using get_range_slice is now deluged with duplicate data.","31/May/10 22:16;jbellis;AJ, can you create a new issue with some sample code to reproduce what you are seeing?",31/May/10 23:29;ajslater;CASSANDRA-1145 Opened,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair leaving FDs unclosed,CASSANDRA-1752,12480189,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,jbellis,jbellis,11/17/2010 9:15,3/12/2019 14:14,3/13/2019 22:24,12/1/2010 22:42,0.6.9,,,,0,,,,,,"""We noticed that after a `nodetool repair` was ran, several of our nodes reported high disk usage; -- even one node hit 100% disk usage. After a restart of that node, disk usage drop instantly by 80 gigabytes -- well that was confusing, but we quickly formed the theory that Cassandra must of been holding open references to deleted file descriptors.

""Later, i found this node as an example, it is using about 8-10 gigabytes more than it should be -- 118 gigabytes reported by df, yet du reports only 106 gigabytes in the cassandra directory (nothing else on the mahcine). As you can see from the lsof listing, it is holding open FDs to files that no longer exist on the filesystem, and there are no open streams or as far as I can tell other reasons for the deleted sstable to be open.

""This seems to be related to running a repair, as we haven't seen it in any other situations before.""

A quick check of FileStreamTask shows that the obvious base is covered:
{code}
        finally
        {
            try
            {
                raf.close();
            }
            catch (IOException e)
            {
                throw new AssertionError(e);
            }
        }
{code}

So it seems that either the transfer loop is never finishing to get to that finally block (in which case why isn't it showing up in outbound streams?) or something else is the problem.",,,,,,,,,,,,,,,,,,,,01/Dec/10 04:23;thobbs;1752-0.6-v2.txt;https://issues.apache.org/jira/secure/attachment/12465023/1752-0.6-v2.txt,01/Dec/10 19:34;thobbs;1752-0.6-v3.txt;https://issues.apache.org/jira/secure/attachment/12465061/1752-0.6-v3.txt,29/Nov/10 23:58;thobbs;1752-0.6.txt;https://issues.apache.org/jira/secure/attachment/12464921/1752-0.6.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,15:58.4,,,no_permission,,,,,,,,,,,,20291,,,Wed Dec 01 23:59:27 UTC 2010,,,,,,0|i0g767:,92592,mdennis,mdennis,,,,,,,,,29/Nov/10 19:15;thobbs;This appears to be a large part of the problem: http://bugs.sun.com/view_bug.do?bug_id=4724038,"29/Nov/10 19:44;jbellis;But the unmapping is supposed to take place at finalization time, which is also when we actually issue the unlink.","29/Nov/10 20:04;thobbs;Ah, when StreamOut.transferSSTables() blocks on waitForStreamCompletion(), the list of SSTableReaders is still in scope, so they aren't garbage collected.","30/Nov/10 00:47;thobbs;The temporary files that are streamed get deleted whenever the node receives a message saying that the file was streamed successfully.  There isn't a need for SSTableReaders at all in this case; only the names of the files produced by the anticompaction are needed for streaming.  The fix here is to to simply close the SSTableWriter without opening an SSTableReader after anticompaction and return a the list of filenames for use with streaming instead.  This way, if waitForStreamCompletion() hangs indefinitely, there are no SSTRs around to keep the FDs open.","30/Nov/10 04:37;mdennis;+1 

(but CompactionManager.java:355-358 are superfluous given the loop check after that statement and the added return at the end of the method)",30/Nov/10 18:51;jbellis;can't we leave the timing and logging code inside the Helper compaction method to reduce duplication in its callers?,30/Nov/10 18:52;jbellis;where do the temporary files get deleted post-stream with this patch?,01/Dec/10 01:53;thobbs;Files are deleted post-stream in StreamOutManager.finishAndStartNext().  I'll clean up the code a bit and post a new patch shortly.,01/Dec/10 04:23;thobbs;Cleaned up version of patch attached.,01/Dec/10 17:51;jbellis;committed,01/Dec/10 18:01;jbellis;reverted -- tests fail to build,"01/Dec/10 18:29;hudson;Integrated in Cassandra-0.6 #13 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/13/])
    avoid opening readers on anticompacted to-be-streamed temporary files
patch by thobbs; reviewed by mdennis and jbellis for CASSANDRA-1752
",01/Dec/10 19:34;thobbs;Unit tests are fixed in the v3 patch.,"01/Dec/10 22:42;jbellis;re-committed, thanks","01/Dec/10 23:59;hudson;Integrated in Cassandra-0.6 #14 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/14/])
    avoid opening readers on anticompacted to-be-streamed temporary files
patch by thobbs; reviewed by mdennis and jbellis for CASSANDRA-1752
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM on Commit log Replay,CASSANDRA-885,12458978,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,pquerna,pquerna,3/12/2010 20:50,3/12/2019 14:14,3/13/2019 22:24,3/19/2010 21:32,0.6,,,,0,,,,,,"Running 0.5.

We had a node reboot, and when it came back up, it was unable to replay the commit logs, it would OOM every time.

We upped the max heap to 6 gigs, but it didn't help.

I have a heap dump and have it opened in Eclipse MAT.

Anything specific I should pull out?

Class Name                                                                                   | Shallow Heap | Retained Heap | Percentage 
----------------------------------------------------------------------------------------------------------------------------------------
                                                                                             |              |               |            
org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor @ 0x7fad06454f48                |          112 | 2,604,583,312 |     84.35% 
|- java.util.concurrent.LinkedBlockingQueue @ 0x7fad06405b78                                 |           80 | 2,604,579,864 |     84.35% 
|- java.util.HashSet @ 0x7fad071b2110                                                        |           24 |         2,952 |      0.00% 
|- java.lang.String @ 0x7fad071b3588  org.apache.cassandra.concurrent:type=ROW-MUTATION-STAGE|           40 |           176 |      0.00% 
|- org.apache.cassandra.concurrent.NamedThreadFactory @ 0x7fad0714e998                       |           32 |            56 |      0.00% 
|- java.util.concurrent.locks.ReentrantLock$NonfairSync @ 0x7fad0722b570                     |           48 |            48 |      0.00% 
|- java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject @ 0x7fad071b3560    |           40 |            40 |      0.00% 
|- java.util.concurrent.atomic.AtomicInteger @ 0x7fad071b20e0                                |           24 |            24 |      0.00% 
|- java.util.concurrent.locks.ReentrantLock @ 0x7fad071b20f8                                 |           24 |            24 |      0.00% 
|- java.util.concurrent.ThreadPoolExecutor$CallerRunsPolicy @ 0x7fad071b2128                 |           16 |            16 |      0.00% 
'- Total: 9 entries                                                                          |              |               |            
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad063b3390                                   |          104 |    95,871,520 |      3.10% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad063b3460                                   |          104 |    82,056,536 |      2.66% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad063b3188                                   |          104 |    72,894,176 |      2.36% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad06331798                                   |          104 |    45,394,360 |      1.47% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad06331660                                   |          104 |    33,895,560 |      1.10% 
java.lang.Thread @ 0x7fad060e2320  main Native Stack, Thread                                 |          168 |    33,573,328 |      1.09% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad06287e50                                   |          104 |    33,528,680 |      1.09% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad063b3530                                   |          104 |    33,423,720 |      1.08% 
org.apache.cassandra.db.ColumnFamilyStore @ 0x7fad063b32c0                                   |          104 |    20,221,624 |      0.65% 
org.apache.cassandra.db.Memtable @ 0x7fad06410120                                            |           96 |     9,383,880 |      0.30% 
org.apache.cassandra.db.ColumnFamily @ 0x7fad33df3690                                        |           80 |     1,190,016 |      0.04% 
org.apache.cassandra.io.SSTableReader @ 0x7fad0653a4a0                                       |           72 |       587,008 |      0.02% 
org.apache.cassandra.io.SSTableReader @ 0x7fad06c41ef0                                       |           72 |       543,552 |      0.02% 
org.apache.cassandra.io.SSTableReader @ 0x7fad06456450                                       |           72 |       541,320 |      0.02% 
Total: 15 of 49,239 entries                                                                  |              |               |            
----------------------------------------------------------------------------------------------------------------------------------------
",,,,,,,,,,,,,,,,,,,,16/Mar/10 22:30;jbellis;885-v3.txt;https://issues.apache.org/jira/secure/attachment/12438982/885-v3.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,53:29.9,,,no_permission,,,,,,,,,,,,19903,,,Fri Mar 19 21:32:47 UTC 2010,,,,,,0|i0g1of:,91702,,,,,,,,,,,12/Mar/10 20:53;kingryan;Did you have a large number of hints on this node?,"12/Mar/10 21:05;pquerna;It should not of had hinted handoff, it was the only machine that was down at this time.",15/Mar/10 15:29;brandon.williams;Perhaps this is related to CASSANDRA-896?  I think Paul mentioned on irc that most of his heap was in LinkedBlockingQueue.,"15/Mar/10 15:32;jbellis;it should definitely do a full GC before OOMing, though.  the jvm is quite thorough about that (hence the gc storm behavior as you approach the limit)","16/Mar/10 22:12;jbellis;If I'm reading the MAT thing correctly, it's saying 85% of the ram is used by the ROW-MUTATION executor queue, meaning that commitlog recovery was reading mutations + shoving them in the queue faster than they could be consumed and applied.

Applying the stage limits from the first patch of CASSANDRA-685 should fix this, although the rest of 685 will have to wait for 0.7.",16/Mar/10 22:24;jbellis;attached.,16/Mar/10 22:27;jbellis;v2 fixes buggy assert,"16/Mar/10 22:30;jbellis;v3 sets minimum of 2 threads on read, write, and response threads","19/Mar/10 19:38;brandon.williams;+1, performance is still good.",19/Mar/10 21:32;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a few insert operations failed while bootstrapping,CASSANDRA-731,12446281,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jaakko,david.pan,david.pan,1/22/2010 2:42,3/12/2019 14:14,3/13/2019 22:24,2/10/2010 15:57,0.5,,,,0,,,,,,"I inserted 10000 key/value while bootstrapping and found 2 insert operations failed.

DEBUG [pool-1-thread-63] 2010-01-20 17:01:57,033 StorageProxy.java (line 225) insert writing key 15530 to 10981@/10.81.37.65
ERROR [pool-1-thread-46] 2010-01-20 17:01:57,033 Cassandra.java (line 1064) Internal error processing insert
java.lang.AssertionError
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:157)
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:76)
at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1178)
at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
at org.apache.cassandra.service.CassandraServer.insert(CassandraServer.java:417)
at org.apache.cassandra.service.Cassandra$Processor$insert.process(Cassandra.java:1056)
at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
at java.lang.Thread.run(Thread.java:619)
ERROR [pool-1-thread-44] 2010-01-20 17:01:57,033 Cassandra.java (line 1064) Internal error processing insert
java.lang.AssertionError
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:157)
at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:76)
at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1178)
at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
at org.apache.cassandra.service.CassandraServer.insert(CassandraServer.java:417)
at org.apache.cassandra.service.Cassandra$Processor$insert.process(Cassandra.java:1056)
at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
at java.lang.Thread.run(Thread.java:619)


I traced the code and found the following assertion failed :
/* org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(Collection<InetAddress>) */
assert map.size() == targets.size(); 

The following reasons caused this issue:
1) targets is a list , not a map, as a result there may be some duplicated IP.
2) The following codes are not atomic :
org.apache.cassandra.service.StorageService.handleStateNormal(InetAddress, String)
        tokenMetadata_.updateNormalToken(token, endPoint);
        calculatePendingRanges();

 That's to say the IP may be both in the naturalEndpoints and pendingRanges.

eg : 
targets is IPa, IPb, IPc, IPa; (size = 4)
then, the map will be IPa, IPb, IPc. (size = 3)
as a result, assert failed.







",,,86400,86400,,0%,86400,86400,,,,,,,,,,,,22/Jan/10 02:49;david.pan;731-throw_internal_exception_while_bootstrapping.patch;https://issues.apache.org/jira/secure/attachment/12431087/731-throw_internal_exception_while_bootstrapping.patch,22/Jan/10 02:56;david.pan;the new log after patch.txt;https://issues.apache.org/jira/secure/attachment/12431088/the+new+log+after+patch.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,41:18.3,,,no_permission,,,,,,,,,,,,19842,,,Wed Feb 10 15:57:03 UTC 2010,,,,,,0|i0g0q7:,91548,,,,,,,,,,,"22/Jan/10 02:49;david.pan;This is a simple way to deal with this problem, but it's not the final solution.
I think the best way is to make the modification of TokenMetadata atomic, but that needs a big change both in the TokenMetadata and StorageService.
Consider that the pendingRange is needed in writing, bootstrapping and leaving only, this simple modification looks like ok at current time.","22/Jan/10 02:56;david.pan;After patch, I add a 500ms sleep between "" tokenMetadata_.updateNormalToken(token, endPoint);"" and "" calculatePendingRanges();"" to make it easy to repeat the problem.
Through the log, you can see it :-)","28/Jan/10 01:41;jaakko;Yeah, you're right. I'll have a look at this.","09/Feb/10 09:31;jaakko;This assert might fail for the reason David said, but it might also fail because targets includes a node with pending ranges that has died (for instance did not complete bootstrap). 

It does not matter if a node is in pending ranges and normal token map simultaneously, as that time is very short and may only result in a write going to too many nodes once in a while. Gossip propagation is far from being instant, so writes go to wrong places routinely anyway when nodes move. It's a much smaller thing than locking whole metadata for the duration of calculations. 

I think removing the assert is enough. It should be removed in any case, since it is checking a condition that is no longer valid. I don't see any other problem in this part.",10/Feb/10 15:57;jbellis;+1 Jaakko's diagnosis.  Committed assertion removal to 0.5 and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in MappedFileDataInput.skipBytes when slicing a large number of keys,CASSANDRA-857,12458371,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,edmonds,edmonds,3/7/2010 4:08,3/12/2019 14:14,3/13/2019 22:24,3/20/2010 1:15,0.6,,,,0,,,,,,"i'm getting the following error when performing a range query that is supposed to return a large number of keys:

ERROR [ROW-READ-STAGE:9] 2010-03-07 03:54:49,672 CassandraDaemon.java (line 78) Fatal exception in thread Thread[ROW-READ-STAGE:9,5,main]
java.lang.AssertionError
	at org.apache.cassandra.io.util.MappedFileDataInput.skipBytes(MappedFileDataInput.java:104)
	at org.apache.cassandra.io.SSTableReader.getPosition(SSTableReader.java:382)
	at org.apache.cassandra.io.SSTableReader.getFileDataInput(SSTableReader.java:481)
	at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:54)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:851)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:771)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:740)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1040)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)","debian, amd64, sun java 1.6.0_12",,,,,,,,,,,,,,,,,,,19/Mar/10 21:08;jbellis;857.txt;https://issues.apache.org/jira/secure/attachment/12439328/857.txt,19/Mar/10 18:10;jbellis;857.txt;https://issues.apache.org/jira/secure/attachment/12439306/857.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,53:56.6,,,no_permission,,,,,,,,,,,,19892,,,Mon Mar 22 20:04:38 UTC 2010,,,,,,0|i0g1i7:,91674,,,,,,,,,,,"19/Mar/10 00:53;jbellis;There are two bugs w/ index positions crossing mmap 2GB segment boundaries.

Working on fix.

--- src/java/org/apache/cassandra/io/SSTableReader.java	(revision 925055)
+++ src/java/org/apache/cassandra/io/SSTableReader.java	(working copy)
@@ -363,6 +363,7 @@
             do
             {
                 DecoratedKey indexDecoratedKey;
+                // bug: EOFing may mean ""try next mmap segment,"" rather than ""we're done""
                 try
                 {
                     indexDecoratedKey = partitioner.convertFromDiskFormat(input.readUTF());
@@ -375,6 +376,7 @@
                 int v = indexDecoratedKey.compareTo(decoratedKey);
                 if (v == 0)
                 {
+                    // bug: ""get next info"" (to find data length) can cross mmap boundary & error out
                     PositionSize info;
                     if (!input.isEOF())
                     {
","19/Mar/10 18:10;jbellis;patch against 0.6 attached.  (will not apply cleanly to trunk, I will have to merge that separately.)","19/Mar/10 18:14;jbellis;patch moves indexPositions and spannedIndexDataPositions from SSTR into IndexSummary class, and adds spannedIndexPositions to map  indexPosition -> KeyPosition.  This lets us tell when we reach the last index entry we can safely read from a mmap segment, without having to read it.  (Relying on reading -> erroring out to tell us when this was the case is not as good, since we lose the ability to tell the difference b/t a corrupt file and a mmap buffer boundary.)",19/Mar/10 21:08;jbellis;rebased,19/Mar/10 22:14;stuhood;+1 Looks good.,20/Mar/10 01:15;jbellis;committed to 0.6; will merge to trunk,"22/Mar/10 20:04;jbellis;(this fix will be in 0.6 RC1, it is not in beta 3.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BufferUnderflowException occurs in RowMutationVerbHandler,CASSANDRA-1617,12477257,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,moores,moores,10/13/2010 17:01,3/12/2019 14:14,3/13/2019 22:24,10/20/2010 19:18,0.7 beta 3,,,,0,,,,,,"There might be a bug in hinted handoff?

I have a cluster of 8, replication factor of 3, doing reads/writes with QUORUM.
I have a single thread doing reads/writes of about 2kb across all nodes, running about 200hps.
When I shut down one node, within a few seconds I start seeing some very big recent write latencies, 4-5 seconds.
I looked at the system.log on the node with the adjacent token to the node that I shut down, and see a bad looking BufferUnderflowException:

INFO [WRITE-kv2-app02.dev.real.com/172.27.109.32] 2010-10-12 12:13:36,712 
OutboundTcpConnection.java (line 115) error writing to 
kv2-app02.dev.real.com/172.27.109.32
 INFO [WRITE-kv2-app02.dev.real.com/172.27.109.32] 2010-10-12 12:13:50,336 
OutboundTcpConnection.java (line 115) error writing to 
kv2-app02.dev.real.com/172.27.109.32
 INFO [Timer-0] 2010-10-12 12:14:22,792 Gossiper.java (line 196) InetAddress 
/172.27.109.32 is now dead.
ERROR [MUTATION_STAGE:1315] 2010-10-12 12:14:24,917 
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)
        at 
org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at 
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:1315] 2010-10-12 12:14:24,918 
AbstractCassandraDaemon.java (line 88) Fatal exception in thread 
Thread[MUTATION_STAGE:1315,5,main]
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)
        at 
org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at 
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:1605] 2010-10-12 12:14:28,919 
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)
        at 
org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at 
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
....
....

I restarted the previously stopped node, and the system recovers, but with a 
few more underlflow exceptions:

 INFO [GOSSIP_STAGE:1] 2010-10-12 12:15:44,537 Gossiper.java (line 594) Node 
/172.27.109.32 has restarted, now UP again
 INFO [HINTED-HANDOFF-POOL:1] 2010-10-12 12:15:44,537 HintedHandOffManager.java 
(line 196) Started hinted handoff for endpoint /172.27.109.32
 INFO [GOSSIP_STAGE:1] 2010-10-12 12:15:44,537 StorageService.java (line 643) 
Node /172.27.109.32 state jump to normal
 INFO [HINTED-HANDOFF-POOL:1] 2010-10-12 12:15:44,538 HintedHandOffManager.java 
(line 252) Finished hinted handoff of 0 rows to endpoint /172.27.109.32
 INFO [GOSSIP_STAGE:1] 2010-10-12 12:15:44,538 StorageService.java (line 650) 
Will not change my token ownership to /172.27.109.32
ERROR [MUTATION_STAGE:1635] 2010-10-12 12:15:45,083 
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
        at java.nio.ByteBuffer.get(ByteBuffer.java:675)
        at 
org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at 
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)","Centos 5.4, jdk 1.6.0_20-b02, 16 core xeon, 8 node cluster",,,,,,,,,,,,,,,,,,,20/Oct/10 18:55;brandon.williams;1617.txt;https://issues.apache.org/jira/secure/attachment/12457691/1617.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,42:50.3,,,no_permission,,,,,,,,,,,,20219,,,Thu Oct 21 14:55:50 UTC 2010,,,,,,0|i0g6bz:,92456,,,,,,,,,,,14/Oct/10 20:42;jbellis;did you upgrade this cluster from 0.6?,14/Oct/10 21:20;moores;Yea we stopped using 0.6 a while back.  All data was created from scratch with 0.7-beta2.,"15/Oct/10 20:25;franklovecchio;We have the nightly build as of yesterday of 0.7 beta 2, and now get the error below when trying to insert a record:  (CLI sytax is something like set CF ['something']['something1']['something2'] = 'value' )

ERROR [MutationStage:2278] 2010-10-15 20:09:39,523 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.nio.BufferUnderflowException
    at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:145)
    at java.nio.ByteBuffer.get(ByteBuffer.java:692)
    at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:636)
ERROR [MutationStage:2278] 2010-10-15 20:09:39,524 AbstractCassandraDaemon.java (line 88) Fatal exception in thread Thread[MutationStage:2278,5,main]
java.nio.BufferUnderflowException
    at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:145)
    at java.nio.ByteBuffer.get(ByteBuffer.java:692)
    at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:636)","18/Oct/10 23:35;brandon.williams;In CASSANDRA-1439, we changed the hint destinations from fixed-length encoding to variable-length encoding (by converting UTF-8, to make OPP happy.)  We neglected to frame the message, however.  Patch adds framing and fixes a debug message that is broken.",20/Oct/10 03:35;jbellis;use FBUtilities.read/writeShortByteArray?,20/Oct/10 16:49;brandon.williams;Updated.,"20/Oct/10 17:48;jbellis;writeshortbytearray applied to oldhint will create a new length ""frame,"" i think you just want to use out.writeFully(old) or something like that","20/Oct/10 18:55;brandon.williams;Good catch.  Updated to fix that, and name the oldHint variable more aptly so it's clearer why this is done.",20/Oct/10 19:13;jbellis;+1,20/Oct/10 19:18;brandon.williams;Committed.,"21/Oct/10 14:55;hudson;Integrated in Cassandra #572 (See [https://hudson.apache.org/hudson/job/Cassandra/572/])
    Add framing to hint destinations.  Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1617
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsupportedOperationException in system_update_column_family,CASSANDRA-1768,12480629,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,tjake,tjake,11/23/2010 2:14,3/12/2019 14:14,3/13/2019 22:24,11/23/2010 17:09,0.7.0 rc 2,,,,0,,,,,,"During testing I hit this section of code:

CFMetaData.java:662
{code}
 // remove the ones leaving.
       for (ByteBuffer indexName : toRemove)
           column_metadata.remove(indexName);
{code}

but column_metadata is defined as:

{code}
       this.column_metadata = Collections.unmodifiableMap(column_metadata);
{code}

So remove() will throw an exception.

{code}
java.lang.UnsupportedOperationException
        at java.util.Collections$UnmodifiableMap.remove(Collections.java:1288)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:662)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.<init>(UpdateColumnFamily.java:56)
        at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:863)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:3592)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}

This was introduced by CASSANDRA-1715",,,,,,,,,,,,,,,,,,,,23/Nov/10 15:44;gdusbabek;ASF.LICENSE.NOT.GRANTED--v0-0001-fix-add-remove-index-bugs-in-CFMetadata.txt;https://issues.apache.org/jira/secure/attachment/12460274/ASF.LICENSE.NOT.GRANTED--v0-0001-fix-add-remove-index-bugs-in-CFMetadata.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,25:51.6,,,no_permission,,,,,,,,,,,,20302,,,Sat Dec 11 07:35:20 UTC 2010,,,,,,0|i0g79r:,92608,,,,,,,,,,,"23/Nov/10 16:23;tjake;+1, there is no chance it could be called concurrently correct?",23/Nov/10 16:25;gdusbabek;correct. the mutation state is single-threaded.,"24/Nov/10 18:19;hudson;Integrated in Cassandra #606 (See [https://hudson.apache.org/hudson/job/Cassandra/606/])
    fix add/remove index bugs in CFMetadata. patch by gdusbabek, reviewed by tjake. CASSANDRA-1768
","11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable statistics causing intermittent CL test failures in trunk.,CASSANDRA-1430,12472490,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,gdusbabek,gdusbabek,8/25/2010 15:40,3/12/2019 14:14,3/13/2019 22:24,9/6/2010 22:51,0.7 beta 2,,,,0,,,,,,"    [junit] Testcase: testCleanup(org.apache.cassandra.db.CommitLogTest):	FAILED
    [junit] 2 != 1
    [junit] junit.framework.AssertionFailedError: 2 != 1
    [junit] 	at org.apache.cassandra.db.CommitLogTest.testCleanup(CommitLogTest.java:69)
    [junit] 
    [junit] 

I see this 1-2 times a day.",,,,,,,,,,,,,,,,,,,,04/Sep/10 01:08;brandon.williams;1430.txt;https://issues.apache.org/jira/secure/attachment/12453855/1430.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,42:20.5,,,no_permission,,,,,,,,,,,,20132,,,Sun Sep 12 19:39:07 UTC 2010,,,,,,0|i0g50f:,92242,jbellis,jbellis,,,,,,,,,25/Aug/10 20:40;gdusbabek;To prove that I'm not hallucinating: http://ci.apache.org/builders/cassandra-trunk/builds/437/steps/compile/logs/stdio,"27/Aug/10 20:42;jbellis;this is caused by sstable statistics being written after the flush (see ssTableWriter.closeAndOpenReader)

is it time to admit that storing sstable info, in sstables, wasn't a good idea?","27/Aug/10 20:44;jbellis;(this isn't just a test issue, i'm pretty sure it could bork drain, too)","27/Aug/10 22:20;gdusbabek;>is it time to admit that storing sstable info, in sstables, wasn't a good idea?
What if statistics operations ran on its own stage that could be halted at critical moments (flushing, for example).

","27/Aug/10 22:33;jbellis;it already feels like a rather ugly set of workarounds, and maybe it's time to cut our losses",27/Aug/10 22:43;gdusbabek;I can't disagree.  CASSANDRA-1382 should be on the list of things that get reverted when we figure out a better way of statistics.,27/Aug/10 22:57;jbellis;Let's just create an EstimatedHistogram serializer and write out one for each of the row sizes and column counts.  SSTable.components will have to be updated.,"03/Sep/10 23:04;brandon.williams;Patch to remove sstable stats from the system table, and make them their own sstable component.",04/Sep/10 00:51;jbellis;I'm getting build errors in a couple test classes,"04/Sep/10 01:08;brandon.williams;Oops, I'd forgotten about more tests being added due to CASSANDRA-1155's problems.  Patch updated to fix tests.",04/Sep/10 01:29;jbellis;+1,04/Sep/10 03:32;brandon.williams;Committed.,06/Sep/10 19:30;jbellis;this broke StreamingTransferTest (easy to miss in the ant output because it is a timeout rather than a failure),06/Sep/10 22:51;jbellis;fixed by CASSANDRA-1471,"12/Sep/10 19:39;hudson;Integrated in Cassandra #533 (See [https://hudson.apache.org/hudson/job/Cassandra/533/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted Handoff Exception,CASSANDRA-634,12443423,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jaakko,lenn0x,lenn0x,12/15/2009 17:36,3/12/2019 14:14,3/13/2019 22:24,12/23/2009 19:44,0.5,,,,0,,,,,,"Updated to the latest codebase from cassandra-0.5 branch. All nodes booted up fine and then I start noticing this error:

ERROR [HINTED-HANDOFF-POOL:1] 2009-12-14 22:05:34,191 CassandraDaemon.java (line 71) Fatal exception in thread Thread[HINTED-HANDOFF-POOL:1,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.gms.FailureDetector.isAlive(FailureDetector.java:146)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:106)
        at org.apache.cassandra.db.HintedHandOffManager.deliverAllHints(HintedHandOffManager.java:177)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:75)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:249)
        ... 3 more",,,,,,,,,,,,,,,,,,,,17/Dec/09 12:00;jaakko;634-1st-part-gossip-about-all-nodes.patch;https://issues.apache.org/jira/secure/attachment/12428285/634-1st-part-gossip-about-all-nodes.patch,21/Dec/09 16:47;jbellis;634-discard-obsolete.patch;https://issues.apache.org/jira/secure/attachment/12428637/634-discard-obsolete.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,34:29.4,,,no_permission,,,,,,,,,,,,19791,,,Wed Dec 23 19:44:13 UTC 2009,,,,,,0|i0g053:,91453,,,,,,,,,,,15/Dec/09 17:37;lenn0x;I'm wondering if this might be due to the fact that one of the node's in the cluster is currently down. This might offer a clue.,"16/Dec/09 14:34;jaakko;Very strange one.

Line 146 in FailureDetector is this:

EndPointState epState = Gossiper.instance().getEndPointStateForEndPoint(ep);

According to the stacktrace it does not even reach getEndPointStateForEndPoint, and even if it did, ep comes from InetAddress.getByAddress, so it cannot be null. Also, if the culprit was Gossiper constructor, that should also show in the trace.

This would mean that Gossiper.instance() returns null, but I don't know how that can happen. ""volatile"" in gossiper_ is actually not needed, but I don't know if having it there could cause such thing. ""volatile"" was added here pretty recently, so it just might be one possible explanation why this came up now.

BTW what version of java you're using?
","16/Dec/09 14:42;jbellis;No, 146 in the 0.5 branch is the next line:

        return epState.isAlive();
","16/Dec/09 15:03;jaakko;Oops, sorry. I need a bigger font :-(

(but still ""volatile"" is not needed there :-)

OK, could it be like this: There are nodes A and B in the cluster, with replication factor 2. Node B goes down and node C is introduced as a new node after this. Now A knows there are A, B and C in the cluster, but C only knows about A. Suppose at this time client sends a write request to A, which falls into A's range (and replica to B's range). B is offline, so instead a hinted write will go to C. Problem is, C will try to deliver this hint later to B, but its Gossiper has never heard of B, so endpointstate will be null.

If this is the cause, then a simple fix

if (epState == null)
    return false;

before line 146 should do the trick.
","16/Dec/09 15:07;jbellis;Are you sure we don't gossip state for down nodes to new cluster members?  If we don't, the token ranges will be wrong, which is a bigger problem.","16/Dec/09 15:24;jaakko;Gossip SYN only includes digests for live endpoints. 

You're right, this would indeed cause wrong ranges on new nodes that have not seen the dead node when it was still alive. Write would still go to the right node, but it would lack HINT flag.

Simply gossiping state about all nodes (dead or alive) would solve this problem, but have to have another look tomorrow morning if it would cause any side effects with failuredetector on the new node (might momentarily consider the dead node to be alive, although this would probably not be a problem at all).
","17/Dec/09 12:00;jaakko;The way node's alive/dead status gets determined currently is as follows:

1st time there is gossip about certain node:
- add new node to endpoint state and mark it alive (onAlive will be called)
- call onJoin

2nd and subsequent gossip about this node
- notify failuredetector whenever there is gossip about this endpoint -> failuredetector starts to monitor this node and set node's status dead if needed (it will not set it to alive)
- node is marked alive whenever there is gossip about it

The important things here are: (1) node is assumed to be alive when 1st info about it arrives and (2) failuredetector does not know anything about the node before 2nd gossip. That means we cannot simply start gossiping info about dead nodes, as their status would remain ""alive"" forever (that is, until the dead node comes online and activates failure detector)

Proposed fix (patch attached):

1st time gossip:
- add new node to endpoint state, but set its status as dead
- call onJoin (token metadata will be updated)

2nd and subsequent gossips:
- Unchanged. This 2nd gossip will trigger markAlive (and call onAlive) and activate failuredetector -> normal situation

In short: assume node to be dead unless otherwise proven by subsequent gossip. If the node is alive, it will be marked so within seconds. If it is dead, we have knowledge about its existence, but we consider it (correctly) to be dead.

There is a possibility of false ""alive"" interpretation, though: Cluster has nodes A, B and C. Suppose C has just gossiped to B and dies. At this time C's status in A is different (older) than in B. Now suppose at this instant node D enters the cluster and first gossips with A. In this case D will get the old gossip and only later the new one. This second newer gossip will cause C to be marked alive even though it is already dead. However, since second gossip will also activate failure detector, C will be correctly marked as dead in a few seconds, so this is probably OK (and anyway a very rare occurence).

Two open issues:
- Now that we're gossiping about dead nodes as well, gossip digest continues to grow without boundary when nodes come and go. This information will never disappear as it will be propagated to new nodes no matter how old and obsolete it is. To counter this, we need some mechanism to (1) either remove dead node from endpointstateinfo or (2) at some point stop to gossip about it, or both.

For (1): when we get removetoken command, it is probably safe to remove the endpoint immediately (STATE_LEFT is broadcasted by different endpoint, so info about token removal will remain in the gossiper). Another thing we could do is to keep track of nodes that have left. If nothing is heard about it for some time, we could assume that it is gone for good and remove it from gossiper after giving its STATE_LEFT enough time to spread.

For (2): We could gossip info only about nodes in either liveEndpoints or unreachableEndpoints (as opposed to endPointStateMap). Nodes are removed from unreachableEndpoints after three days of silence, so this would discard old information from the gossiper. Side effect would of course be that a node that is down more than three days but comes back later, might miss some of its data (new nodes that booted after the three day period would know nothing about this node).

(Attached patch should work as such, but does not take into account these last two issues)
","18/Dec/09 16:19;jbellis;committed to 0.5 and trunk.

Created CASSANDRA-644 for the gossip membership problem.",20/Dec/09 08:23;lenn0x;Does this patch address the NullException? I tried latest from branch 05 and am still seeing this exception.,"21/Dec/09 09:53;jaakko;It was supposed to solve it, but obviously it did not fully do so.

Problem in your case might be because hinted handoff data is persistent and gossiper data is not. Suppose there are nodes A and B. Suppose B goes down and A stores hinted data for it. Later A is restarted -> A still has hinted data for B, but after restart its gossiper knows nothing about B. It does not help even if we gossip about dead nodes, as nobody has ever heard of B. If B is gone forever, A can never get rid of hinted data.

Don't know what would be the best thing to do here. removetoken command could make efforts to redirect hints to new destination in case a hinted target is removed. However, if the endpoint has been lost from gossip/tokenmetadata, then there is nothing it can do as it does not know who the endpoint was. Another option would be to add manual command to redirect hinted data.

Other options?
","21/Dec/09 16:06;jbellis;IMO we should just drop the HH data with a warning (in case ops does intend to bring the dead node back).  In almost all cases, if a node is down that long it is going to be replaced entirely, and the replacement will bootstrap and not need the old HH data.

Since you should repair after bringing a dead node online anyway (b/c there is a window before the FD is aware that we should start doing HH), HH is just an optimization and this is OK.",21/Dec/09 16:47;jbellis;discard hints for nodes that are no longer part of the gossip network,"22/Dec/09 00:49;jaakko;If dropping HH data is OK, then this patch looks good to me.

One small thing: After a node starts, it will take some time before it knows about all cluster nodes. During this time if there is a request to deliver hints (not possible?), HH data will be discarded just because the node does not know about the other endpoint yet.
","22/Dec/09 02:25;jbellis;hint delivery is attempted when a node is signaled to be up, or every 1h (starting after a 1h delay), so this seems OK.","23/Dec/09 05:55;lenn0x;can you rebase this on 05? i am fixing it manually, strange it complained you just added a comment to INTERVAL_IN_MS.",23/Dec/09 19:25;lenn0x;+1 no more exceptions and I am seeing HH being disregarded for the node we removed now.,23/Dec/09 19:44;jbellis;committed to 0.5 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow KS definition with RF > # of nodes,CASSANDRA-1310,12469974,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,zznate,arya,arya,7/23/2010 1:15,3/12/2019 14:14,3/13/2019 22:24,7/29/2010 15:59,0.7 beta 1,,,,0,,,,,,"Cassandra 0.7 allows user to create Keyspaces with Replication Factor >  number of endpoints causing in java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1) exception in nodetool and Internal Errors on Thrift making the node useless.

Steps to Reproduce:

From a clean setup of Cassandra:
1. Start a single node out of cluster of 3. This means my configuration has the other two nodes in the seeds list, but have not restarted them yet;
2. Use Thrift API (I am using PHP) and create a Keyspace with replication factor 2;
3. The command executes with no exception or error;
4. Now try writing to it, you will get TException with Internal Error message;
5. Try nodetool ring and you will get Exception:

Exception in thread ""main"" java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1)
	at org.apache.cassandra.locator.RackUnawareStrategy.calculateNaturalEndpoints(RackUnawareStrategy.java:61)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:87)
	at org.apache.cassandra.service.StorageService.constructRangeToEndpointMap(StorageService.java:536)
	at org.apache.cassandra.service.StorageService.getRangeToAddressMap(StorageService.java:522)
	at org.apache.cassandra.service.StorageService.getRangeToEndpointMap(StorageService.java:496)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1449)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1284)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1382)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

Expected:

1. Either step 3 should not let you create the KS with RF 2 and 1 node in ring, or there should be a peaceful way for Cassandra to recover from IllegalStateException and replicate once other nodes become available.","CentOS 5.1
Trunc July 22nd",,,,,,,,,,,,,,,,,,,28/Jul/10 22:49;zznate;1310-v2.txt;https://issues.apache.org/jira/secure/attachment/12450765/1310-v2.txt,26/Jul/10 18:02;zznate;trunk-1310.txt;https://issues.apache.org/jira/secure/attachment/12450497/trunk-1310.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,34:30.0,,,no_permission,,,,,,,,,,,,20074,,,Fri Jul 30 13:37:48 UTC 2010,,,,,,0|i0g4a7:,92124,,,,,,,,,,,26/Jul/10 08:34;zznate;Patch validates on RF being less than the number of nodes and that the KsDef.name and CsDef.keyspace match when providing a KsDef to system_add_keyspace on CassandraServer,26/Jul/10 18:02;zznate;Removed thrift validation method as nothing else really needs to validate on KsDef ,"27/Jul/10 14:48;gdusbabek;I think we should allow the creation of keyspaces even when there aren't enough nodes to support the specified replication factor.  For example, it should be allowable for an operator to bring up a single node in a new cluster, define all the keyspaces and then start bringing up new nodes to populate the cluster.

Throwing IllegalStateException is a special case that should probably be handled in NodeTool (gently output the error string and die).",27/Jul/10 16:32;zznate;That makes sense to me. I would like to have a check on CsDef.keyspace = KsDef.name though. I hit that in a test case and it cause a really obtuse NPE. ,"28/Jul/10 21:14;gdusbabek;Nate: feel free to resubmit the patch for keyspace checks.  Also, having nodetool print just the error (not the whole stack trace) might be more user-friendly.",28/Jul/10 22:49;zznate;Just validates CsDef vs. KsDef have the same keyspace. Includes NodeCmd patch that prints an error msg instead of the stack trace.,29/Jul/10 15:59;gdusbabek;committed.,"30/Jul/10 13:37;hudson;Integrated in Cassandra #504 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/504/])
    print a more friendly error when printRing gets IllegalState. Patch by Nate McCall, reviewed by Gary Dusbabek. CASSANDRA-1310
check for ks/cf keyspace name agreement. Patch by Nate McCall, reviewed by Gary Dusbabek. CASSANDRA-1310
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Submit Flush is Failing with a RejectedExecutionException,CASSANDRA-471,12437324,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,10/5/2009 22:39,3/12/2019 14:14,3/13/2019 22:24,10/7/2009 14:16,0.5,,Legacy/Tools,,0,,,,,,"This is probably very specific to my BMT loading job; however, I have started running into this problem lately.  

$ bin/nodeprobe -host 127.0.0.1 flush_binary MyApp
Exception in thread ""main"" java.util.concurrent.RejectedExecutionException
        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1760)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:926)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlushBinary(ColumnFamilyStore.java:427)
        at org.apache.cassandra.service.StorageService.forceTableFlushBinary(StorageService.java:802)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1264)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1359)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,,,,,06/Oct/09 16:46;jbellis;471.patch;https://issues.apache.org/jira/secure/attachment/12421440/471.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,46:15.0,,,no_permission,,,,,,,,,,,,19706,,,Thu Oct 08 12:35:14 UTC 2009,,,,,,0|i0fz53:,91291,,,,,,,,,,,"06/Oct/09 16:46;jbellis;when enough compactions queued up, it would reject additional ones instead of waiting for a thread to free up.  this patch fixes that.",07/Oct/09 14:16;jbellis;Chris Were reports on -user that this patch fixes the bug for him.  Committed.,"08/Oct/09 12:35;hudson;Integrated in Cassandra #221 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/221/])
    use CallerRunsPolicy instead of rejecting runnables on multi-threaded executors w/ blocking queues
patch by jbellis; tested by Chris Were for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"in a cluster, get_range_slice() does not return all the keys it should",CASSANDRA-781,12455694,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,bjc,bjc,2/8/2010 22:32,3/12/2019 14:14,3/13/2019 22:24,2/23/2010 17:06,0.5,,,,0,,,,,,"get_range_slice() does not return the same set of keys as get_key_range() in 0.5.0 final.

I posted a program to reproduce the behavior:

http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg01474.html

Apparently, you must have more than one node to get the behavior. Also, it may depend on the locations of the nodes on the ring.. I.e., if you don't generate enough keys randomly, then by chance they could all fall on the same host and you might not see the behavior, although I was able to get it to happen using only 2 nodes and 10 keys.

Here are the other emails describing the issue:

http://www.mail-archive.com/cassandra-user@incubator.apache.org/msg02423.html
","Debian 5 lenny on EC2, Gentoo linux, Windows XP",,,,,,,,,,,,,,,,,,,22/Feb/10 20:38;jbellis;781-backport.txt;https://issues.apache.org/jira/secure/attachment/12436624/781-backport.txt,15/Feb/10 17:03;jbellis;781.txt;https://issues.apache.org/jira/secure/attachment/12435887/781.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,52:43.2,,,no_permission,,,,,,,,,,,,19861,,,Tue Feb 23 17:06:05 UTC 2010,,,,,,0|i0g11b:,91598,,,,,,,,,,,09/Feb/10 05:52;jbellis;Reproduced on trunk.  Can you try the attached patch there?  (We can backport to 0.5 afterwards.),"09/Feb/10 08:16;bjc;Before applying 781.txt to trunk I was getting an exception. The exception is now gone! Awesome. However, I still have the range scanning problem. Here are a few example runs of the test:

$ python test_bug.py get_range_slice
ebbde791748641be951802d64d48c62d not marked 0
9bfa7ad8abce48a9a45daccfa3772f29 not marked 0
$ python test_bug.py get_range_slice
c554dc532bfb462b950990b6824f11c1 not marked 0
e6c4a0100508451ea3a1d13088877dd9 not marked 0
$ python test_bug.py get_range_slice
$ python test_bug.py get_range_slice
$ python test_bug.py get_range_slice
27fc88c8e7ab489d96f4c749cc86aca1 not marked 0
$ python test_bug.py get_range_slice
b522cca4bd6f4282a507525598139f95 not marked 0
$ python test_bug.py get_range_slice
d045b3edabc949fea30242722a11587a not marked 0
$ 

Sigh, I just realized that under trunk my nodetool doesn't work. However, I got the tokens from the log:

INFO 08:10:50,338 Saved Token not found. Using 68054825649105441942293089893012253843
INFO 08:10:53,293 Saved Token not found. Using 44181284974408316254372647768836513112
","09/Feb/10 12:53;hbadenes;I am hitting a similar problem on 3-node cluster I run. I do receive 5 rows in the get_range_slice query (from """" to """"), instead of an empty result set as described here (there exist more than just those 5 returned).

I am running 0.5.0 and could be able to test a patch for that version.","09/Feb/10 13:30;slebresne;I add the same problem, range_slice on 2 nodes was missing results.
I updated to trunk and applied 781.txt. It fixes the missing results but
introduce a timeout exception (I don't believe it is the patch fault though).

The timeout happens with consistencyLevel.ONE (but I suspect it could happen
with QUORUM too). Looking a bit to the details, it seems that
RangeSliceResponseResolver wait for a response from every live natural
endpoints (in isDataPresent()). But in StorageProxy, the rangeSlice message is
only sent to 'responseCount' endpoints (which will be 1 for
consistencyLevel.ONE). Hence the timeout.

I'm not sure what would be the best way to deal with that though.","09/Feb/10 14:27;jbellis;Sylvain: absolutely right, patch 01 has a fix for this.

Jack: updated patch 02 with more logging at INFO, maybe that will help.  I can't reproduce even w/ your tokens and several runs of 100 keys.  I am using a slightly simpler test, though:

        import uuid
        ks = ""Keyspace1""
        cf = ""Super1""
        path = ColumnPath(cf, ""foo"", ""is"")
        value = ""cool""

        # insert, record keys in `keys` set
        keys = set()
        for i in xrange(100):
            key = uuid.uuid4().hex
            client.insert(ks, key, path, value, 0, ConsistencyLevel.ONE)
            keys.add(key)
    
        # remove keys found from set
        parent = ColumnParent(column_family=cf)
        slice_range = SliceRange(start=""key"", finish=""key"")
        predicate = SlicePredicate(slice_range=slice_range)        
        result = client.get_range_slice(ks, parent, predicate, """", """", 1000, ConsistencyLevel.ONE)
        for row in result:
            keys.discard(row.key)

        # if there are any left over, there is a bug
        assert not keys, list(sorted(keys))

... this will of course only work until you insert more than 1000 keys.  (maybe your original test has a similar limitation, i don't remember.)

(Are you still testing w/ RF of 2?  If so maybe patch 01 will help you too.)","09/Feb/10 21:08;bjc;Ok, I checked out a fresh copy of trunk and applied both new patches (0001 and 0002). There were some errors for 0002, but upon looking at the code it seems parts of the patch have already been committed to the SVN repo, so what I ended up with is the right thing.

$ patch -p1 <0001-fix-timeout-bug.txt 
patching file src/java/org/apache/cassandra/service/StorageProxy.java
$ patch -p1 <0002-fix-slices-over-non-trivial-wrapped-ranges.txt
patching file src/java/org/apache/cassandra/db/ColumnFamilyStore.java
Hunk #1 succeeded at 43 (offset 1 line).
Hunk #2 succeeded at 1075 (offset 2 lines).
patching file src/java/org/apache/cassandra/dht/AbstractBounds.java
Hunk #1 FAILED at 29.
1 out of 1 hunk FAILED -- saving rejects to file src/java/org/apache/cassandra/dht/AbstractBounds.java.rej
patching file src/java/org/apache/cassandra/dht/Bounds.java
Hunk #1 FAILED at 1.
Hunk #2 FAILED at 11.
2 out of 2 hunks FAILED -- saving rejects to file src/java/org/apache/cassandra/dht/Bounds.java.rej
patching file src/java/org/apache/cassandra/dht/Range.java
patching file src/java/org/apache/cassandra/service/StorageProxy.java
patching file src/java/org/apache/cassandra/service/StorageService.java
Reversed (or previously applied) patch detected!  Assume -R? [n] 
Apply anyway? [n] 
Skipping patch.
3 out of 3 hunks ignored -- saving rejects to file src/java/org/apache/cassandra/service/StorageService.java.rej
patching file test/unit/org/apache/cassandra/dht/BoundsTest.java
patching file test/unit/org/apache/cassandra/dht/RangeTest.java
$ 

Copied my storage-conf.xml into place, which has only two changes: Seeds defined and binding addresses changed to null strings. Thus, my RF is 1 now, not 2 as it was before.

I now use your simpler test. Here is the test with all the import statements:

import uuid

from thrift import Thrift
from thrift.transport import TTransport
from thrift.transport import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocolAccelerated
from cassandra import Cassandra
from cassandra.ttypes import *

num_keys = 10

socket = TSocket.TSocket(""10.212.87.165"", 9160)
transport = TTransport.TBufferedTransport(socket)
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
client = Cassandra.Client(protocol)

transport.open()

ks = ""Keyspace1""
cf = ""Super1""
path = ColumnPath(cf, ""foo"", ""is"")
value = ""cool""

# insert, record keys in `keys` set
keys = set()
for i in xrange(100):
    key = uuid.uuid4().hex
    client.insert(ks, key, path, value, 0, ConsistencyLevel.ONE)
    keys.add(key)

# remove keys found from set
parent = ColumnParent(column_family=cf)
slice_range = SliceRange(start=""key"", finish=""key"")
predicate = SlicePredicate(slice_range=slice_range)
result = client.get_range_slice(ks, parent, predicate, """", """", 1000, ConsistencyLevel.ONE)
for row in result:
    keys.discard(row.key)

# if there are any left over, there is a bug
assert not keys, list(sorted(keys))


I cleared out the data/commitlog dirs and launched both nodes. The tokens:

 INFO 21:02:08,768 Saved Token not found. Using 136351045523563703929320485474511375137
 INFO 21:02:09,509 Saved Token not found. Using 20118706661854036583649958139769313744

Now.. run the test!

$ python test_bug_simple.py
Traceback (most recent call last):
  File ""test_bug_simple.py"", line 36, in <module>
    result = client.get_range_slice(ks, parent, predicate, """", """", 1000, ConsistencyLevel.ONE) 
  File ""/usr/local/python//lib/python2.6/site-packages/cassandra/Cassandra.py"", line 486, in get_range_slice
  File ""/usr/local/python//lib/python2.6/site-packages/cassandra/Cassandra.py"", line 508, in recv_get_range_slice
thrift.Thrift.TApplicationException: Internal error processing get_range_slice


The log from the node I am querying:

 INFO 21:02:08,768 Saved Token not found. Using 13635104552356370392932048547451
1375137
 INFO 21:02:08,933 Starting up server gossip
 INFO 21:02:09,118 Cassandra starting up...
 INFO 21:02:10,686 Node /10.212.230.176 is now part of the cluster
 INFO 21:02:11,750 InetAddress /10.212.230.176 is now UP
 INFO 21:05:43,927 scanning node range (136351045523563703929320485474511375137,
20118706661854036583649958139769313744]
ERROR 21:05:43,927 Internal error processing get_range_slice
java.lang.AssertionError
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:16)
        at org.apache.cassandra.dht.Bounds.restrictTo(Bounds.java:34)
        at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.
java:559)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slice(Cassandra
Server.java:560)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slice.proce
ss(Cassandra.java:1189)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.jav
a:984)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadP
oolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:619)



","09/Feb/10 21:28;jbellis;Sorry, those patches that didn't apply were probably important. :)

I've rebased against r908163, can you revert and try again?","09/Feb/10 21:44;bjc;Makes sense! Can you try one more time? I still can't apply the patch properly.. Or, maybe I'm doing something wrong? Here is the transcript.

$ svn checkout -r908163 https://svn.apache.org/repos/asf/incubator/cassandra/trunk cassandra
...
A    cassandra/README.txt
 U   cassandra
Checked out revision 908163.
$ cd cassandra
$ wget https://issues.apache.org/jira/secure/attachment/12435345/0001-fix-timeout-bug.txt
--2010-02-09 21:40:51--  https://issues.apache.org/jira/secure/attachment/124353
45/0001-fix-timeout-bug.txt
Resolving issues.apache.org... 140.211.11.140
Connecting to issues.apache.org|140.211.11.140|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1873 (1.8K) [text/plain]
Saving to: `0001-fix-timeout-bug.txt'

100%[======================================>] 1,873       --.-K/s   in 0s      

2010-02-09 21:40:52 (47.0 MB/s) - `0001-fix-timeout-bug.txt' saved [1873/1873]

$ patch -p1 <0001-fix-timeout-bug.txt 
patching file src/java/org/apache/cassandra/service/StorageProxy.java
$ wget https://issues.apache.org/jira/secure/attachment/12435346/0002-fix-slices-over-non-trivial-wrapped-ranges.txt
--2010-02-09 21:41:13--  https://issues.apache.org/jira/secure/attachment/124353
46/0002-fix-slices-over-non-trivial-wrapped-ranges.txt
Resolving issues.apache.org... 140.211.11.140
Connecting to issues.apache.org|140.211.11.140|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 14776 (14K) [text/plain]
Saving to: `0002-fix-slices-over-non-trivial-wrapped-ranges.txt'

100%[======================================>] 14,776      74.8K/s   in 0.2s    

2010-02-09 21:41:14 (74.8 KB/s) - `0002-fix-slices-over-non-trivial-wrapped-ranges.txt' saved [14776/14776]

$ patch -p1 <0002-fix-slices-over-non-trivial-wrapped-ranges.txt 
patching file src/java/org/apache/cassandra/db/ColumnFamilyStore.java
patching file src/java/org/apache/cassandra/dht/AbstractBounds.java
Hunk #1 FAILED at 29.
1 out of 1 hunk FAILED -- saving rejects to file src/java/org/apache/cassandra/dht/AbstractBounds.java.rej
patching file src/java/org/apache/cassandra/dht/Bounds.java
Hunk #1 FAILED at 1.
Hunk #2 FAILED at 11.
2 out of 2 hunks FAILED -- saving rejects to file src/java/org/apache/cassandra/dht/Bounds.java.rej
patching file src/java/org/apache/cassandra/dht/Range.java
patching file src/java/org/apache/cassandra/service/StorageProxy.java
patching file src/java/org/apache/cassandra/service/StorageService.java
patching file test/unit/org/apache/cassandra/dht/BoundsTest.java
patching file test/unit/org/apache/cassandra/dht/RangeTest.java
$ 
","09/Feb/10 22:22;jbellis;weird, happens for me too.  lame!

attached Bounds and AbstractBounds as they should look, post-patch.  just do what you showed in the transcript, then overwrite the local copies with these.","09/Feb/10 23:24;bjc;Ok, got the patch applied properly and things look better! The simple test passes. Awesome!! :) However, the more complicated test uses a ""start"" offset after the first get_range_slice(), and that still causes an exception. From the log:

 INFO 23:15:52,425 scanning node range (20123910036548544936247138992367052936,67283373037552029587203789575295250400]
ERROR 23:15:52,425 Internal error processing get_range_slice
java.lang.AssertionError: [124451343962032323897724984972289130546,67283373037552029587203789575295250400]
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:26)
        at org.apache.cassandra.dht.Bounds.getRangeOrBounds(Bounds.java:74)
        at org.apache.cassandra.dht.Bounds.restrictTo(Bounds.java:59)
        at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:559)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slice(CassandraServer.java:560)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slice.process(Cassandra.java:1189)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:984)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

If you have trouble duplicating this one I can give you more information, but I did not have difficulty getting it to happen. It happened every time I tried. For example, you can get it to happen just by modifying one line of the simple test:

result = client.get_range_slice(ks, parent, predicate, ""b37e14bb37304e0096e2e77a8fc88a5b"", """", 1000, ConsistencyLevel.ONE)

Of course, if you don't scan starting from """" then the simple test doesn't make sense, because you might specifically exclude keys you are looking for by starting from the string I put in.

However, the more complicated test I posted earlier makes sense and exercises the start and end ranges of get_range_slice(). So..can we go back to the complicated test and specifically make that one work? It's also nice because you should be able to run it over and over, since it removes the keys at the end. With the simple test you have to manually flush the data and restart the servers each time.

Thanks so much for fixing this! I am getting more familiar with the java so soon I might be able to fix some bugs like this.","11/Feb/10 05:27;jbellis;committed patch 1, the timeout fix.

here is a new patch that should fix the remaining range issues.","11/Feb/10 06:53;bjc;Almost there I think. I found that the keys were not being returned in sorted order as they were before, so my trick of taking the last key in a limited range, and using that to start the next limited range did not work. So, I modified the test to sort the keys, then take the last one. When I did this I found another bug: when there are fewer keys in the specified range, duplicates are returned. Also, when I played around with the start and end for the range the server starting giving AssertionErrors:

ERROR 06:25:41,032 Internal error processing get_range_slice
java.lang.AssertionError: [125358492461525499902293558181143752059,1244525150549
22950280650433865080672503]
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:26)
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:18)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slice(Cassandra
Server.java:558)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slice.proce
ss(Cassandra.java:1189)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.jav
a:984)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadP
oolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:619)


The modified test (below) gets stuck in an infinite loop. If you run it in ipython, you can control-c to get back to the interpretor, then type ""result"" to look at it's contents. It is a sorted list of keys. Look at the last key.. Here is the transcript showing the last two keys from result:

 'ff8bfa30777f455695bf934ac7cfedac',
 'ffb701ea740646b9955f0e339f8e3ee2']

In [70]: result2 = client.get_range_slice(ks, cparent, p, start, """", seg, cl)

In [71]: len(result2)
Out[71]: 1000

In [72]: result3 = client.get_range_slice(ks, cparent, p, start, """", seg, cl)

In [73]: len(result3)
Out[73]: 1000

In [74]: start
Out[74]: 'ffb701ea740646b9955f0e339f8e3ee2'

In [75]: result4 = client.get_range_slice(ks, cparent, p, start, start, seg, cl)

In [76]: len(result4)
Out[76]: 1000

In [77]: result5 = client.get_range_slice(ks, cparent, p, start, start, seg, cl)

In [78]: len(result5)
Out[78]: 1

In [79]: 

That can't be right. Here is the latest test..

import sys
import time
import uuid

from thrift import Thrift
from thrift.transport import TTransport
from thrift.transport import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocolAccelerated

import sys
sys.path.insert(0,'/usr/local/cassandra/interface/thrift/gen-py')

from cassandra import Cassandra
from cassandra.ttypes import *

num_keys = 10000

socket = TSocket.TSocket(""10.212.87.165"", 9160)
transport = TTransport.TBufferedTransport(socket)
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
client = Cassandra.Client(protocol)

ks = ""Keyspace1""
cf = ""Super1""
cl = ConsistencyLevel.ONE

d = {}
    
transport.open()
    
if 1:
    ## insert keys using the raw thrift interface
    cpath = ColumnPath(cf, ""foo"", ""is"")
    value = ""cool""

    for i in xrange(num_keys):
        ts = time.time()
        key = uuid.uuid4().hex
        client.insert(ks, key, cpath, value, ts, cl)
        d[key] = 1

else:
    ## insert keys using pycassa!
    import pycassa

    client = pycassa.connect([""10.212.87.165:9160""])
    cf_test = pycassa.ColumnFamily(client, ks, cf, super=True)

    for i in xrange(num_keys):
        key = uuid.uuid4().hex
        cf_test.insert(key, { 'params' : { 'is' : 'cool' }})
        d[key] = 1


cparent = ColumnParent(column_family=cf)
slice_range = SliceRange(start=""key"", finish=""key"")
p = SlicePredicate(slice_range=slice_range)

done = False
seg = 1000
start = """"

## do a scan using either get_key_range() (deprecated) or get_range_slice()
## for every key returned that is in the dictionary, mark it as found
while not done:
    print ""start"", start
    result = client.get_range_slice(ks, cparent, p, start, """", seg, cl)

    def getkey(x):
        return x.key
    result = map(getkey, result)   
    result.sort()

    for r in result:
        if d.has_key(r): 
            d[r] = 0

    if len(result) < seg: done = True
    else: start = result[seg-1]

cpath = ColumnPath(column_family=cf, super_column='foo')

## get, remove all the keys
## print all the keys that were not marked 0
for k in d:
    result = client.get(ks, k, cpath, cl)
    #print result

    if d[k] == 1: 
        print k, ""not marked 0""
    #else:
    #    print k, ""was marked 0!""

    ts = time.time()
    client.remove(ks, k, cpath, ts, cl)




BTW, this time around my nodetool worked perfectly! When I first brought the two nodes up, they selected keys that were too close, and one node ended up with all the load. So I ran loadbalance, and it worked great! That was really awesome. The only thing I noticed was a single key that should have been found returned a NotFoundException. I'll keep an eye on this one, too. Best,

Jack","11/Feb/10 22:17;jbellis;new patch attached.

> ERROR 06:25:41,032 Internal error processing get_range_slice 

added InvalidRequestException when start > end, which fixes this.

> The modified test (below) gets stuck in an infinite loop

Your test is buggy. :)

range_slice (like key_range) is start-INCLUSIVE, so if you pass a key that exists as start, you will always get at least one result, the start one.

> the keys were not being returned in sorted order 

This is working fine for me.  Not sure what you were seeing.

> when there are fewer keys in the specified range, duplicates are returned

Sounds like another illustration of start-inclusiveness.

If you still see problems, can you narrow it down to a specific set of keys, rather than relying on randomness to maybe reproduce it once in a while?  That would help a lot.  Thanks!
","12/Feb/10 03:44;bjc;I don't think my test is buggy. I realize that the range is start inclusive, and it does pass a key that exists as start, but sets ""done = True"" if the range scan returns less keys than requested. Since it passes """" as the end/finish, this should return less keys than requested when you get to the end, provided you ask for more than one key (which I do).

I think the last remaining problem is with the sorting! I bet that is why using """" for my finish string doesn't work. Here's the problem I see now (transcript followed by test):

I put 10 random keys in, ask for them back. They aren't sorted, so I sort them and take the highest. I use that as start, and """" as finish. This should give me one key back, but instead I get 10. Could it be that my columnfamily definition is different than yours? Here's mine:

      <ColumnFamily ColumnType=""Super""
                    CompareWith=""UTF8Type""
                    CompareSubcolumnsWith=""UTF8Type""
                    Name=""Super1""
                    RowsCached=""1000""
                    KeysCachedFraction=""0""
                    Comment=""A column family with supercolumns, whose column and subcolumn names are UTF8 strings""/>


In [17]: run test_bug_simple2.py
result1 before sorting
af37b718213b4219897ea1564ebc8900
f196ad5537294840b2de0a636202dbd2
7578ba38b66d4708a38663717e020959
b5266af926a647c3a1a4d2f62dfe952c
d729d5181bac42a48ac3e49d9700047e
4d58b6fbea214d0c9c7a9f288feba2d8
41df7aee7d674a75a4943d89153f9bde
4e121f95459e4f67a6cd3c06b2d078e7
99b2b03675a8413f94e60e3d1bbded8c
66121c5c863f4c1f804a46b8c2136fe9
result1 after sorting
41df7aee7d674a75a4943d89153f9bde
4d58b6fbea214d0c9c7a9f288feba2d8
4e121f95459e4f67a6cd3c06b2d078e7
66121c5c863f4c1f804a46b8c2136fe9
7578ba38b66d4708a38663717e020959
99b2b03675a8413f94e60e3d1bbded8c
af37b718213b4219897ea1564ebc8900
b5266af926a647c3a1a4d2f62dfe952c
d729d5181bac42a48ac3e49d9700047e
f196ad5537294840b2de0a636202dbd2
start f196ad5537294840b2de0a636202dbd2
result2
f196ad5537294840b2de0a636202dbd2
7578ba38b66d4708a38663717e020959
b5266af926a647c3a1a4d2f62dfe952c
d729d5181bac42a48ac3e49d9700047e
4d58b6fbea214d0c9c7a9f288feba2d8
41df7aee7d674a75a4943d89153f9bde
4e121f95459e4f67a6cd3c06b2d078e7
99b2b03675a8413f94e60e3d1bbded8c
66121c5c863f4c1f804a46b8c2136fe9
b8b290a864464271ad30df1bbab2f2b7
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)

/k/jack/bridge/test_bug_simple2.py in <module>()
     54 for r in result2: print r.key
     55 
---> 56 assert len(result2) == 1
     57 
     58 

AssertionError: 
WARNING: Failure executing file: <test_bug_simple2.py>

In [18]: 





import uuid

from thrift import Thrift
from thrift.transport import TTransport
from thrift.transport import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocolAccelerated
 
import sys
sys.path.insert(0,'/usr/local/cassandra/interface/thrift/gen-py')
 
from cassandra import Cassandra
from cassandra.ttypes import *
 
socket = TSocket.TSocket(""10.212.87.165"", 9160)
transport = TTransport.TBufferedTransport(socket)
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
client = Cassandra.Client(protocol)
 
transport.open()
 
ks = ""Keyspace1""
cf = ""Super1""
path = ColumnPath(cf, ""foo"", ""is"")
value = ""cool""
    
for i in xrange(100):
    key = uuid.uuid4().hex
    client.insert(ks, key, path, value, 0, ConsistencyLevel.ONE)
 
parent = ColumnParent(column_family=cf)
slice_range = SliceRange(start=""key"", finish=""key"")
predicate = SlicePredicate(slice_range=slice_range)
 
result1 = client.get_range_slice(ks, parent, predicate, """", """", 10, ConsistencyLevel.ONE)
 
print ""result1 before sorting""
for r in result1: print r.key
 
def getkey(x): return x.key
 
print ""result1 after sorting""
result1 = map(getkey, result1)
result1.sort()
 
for r in result1: print r
 
start = result1[-1]
 
print ""start"", start
 
result2 = client.get_range_slice(ks, parent, predicate, start, """", 10, ConsistencyLevel.ONE)
 
print ""result2""
for r in result2: print r.key
 
assert len(result2) == 1


",12/Feb/10 03:48;jbellis;If you're using RP instead of OPP you will see that.,"12/Feb/10 04:18;bjc;Ahh!! Right you are, I was using RP instead of OPP. Ok, but now here is another problem: if I insert 10 keys and then ask for them back, it works. However, if I insert 10 more and do a range scan with start="""", I don't get the lowest key:

In [17]: run test_bug_simple3.py
insert aa3cf33059d64dac8aef4a250bc5ea9c
insert a7dbda2925eb4b439c89cd71d56b5113
insert f28e92d5e5554857940c9d3386bf4121
insert 2b8ec460e7d346cbaf3dcb00e1aaaf91
insert 7792c98f0c3948299c622c73b906df66
insert 37a8bfdb69b642ba8e96d33b060f789d
insert 38c18f5d3d2c46cbb4e44b603a8acdbd
insert bef8104ea9184abaa3f0788ef7b2e0db
insert 934fe04d30cc4a96b1f1a9e7930316b8
insert 1d3413e88af946349f148c4fafeb6bf7
result 1d3413e88af946349f148c4fafeb6bf7
result 2b8ec460e7d346cbaf3dcb00e1aaaf91
result 37a8bfdb69b642ba8e96d33b060f789d
result 38c18f5d3d2c46cbb4e44b603a8acdbd
result 7792c98f0c3948299c622c73b906df66
result a7dbda2925eb4b439c89cd71d56b5113
result aa3cf33059d64dac8aef4a250bc5ea9c
result bef8104ea9184abaa3f0788ef7b2e0db
result f28e92d5e5554857940c9d3386bf4121
start f28e92d5e5554857940c9d3386bf4121
result f28e92d5e5554857940c9d3386bf4121

In [18]: run test_bug_simple3.py
insert 4eb0300540ec4b4083fbaf33741fc4a5
insert 12b43ba967314b369faff7e59902d6c2
insert 5b4b729676bc4ea2816620c3b6dff080
insert cf2fda1b11d843f1ae7949dbbb7d179d
insert c9d0cf4a1e9a48caa143afd2b0268f70
insert 9a044cff59b940d5bfbeffd58b01ee8e
insert d2ee042f0b0b4f7ea86e6e2c0dfdcfdd
insert d239aee577684c27afea2fe7e3361bdf
insert 706b20976f974de49bda61d55b9c2a63
insert 36177455bc3b4469b7e6f51897c9f3ba
result a7dbda2925eb4b439c89cd71d56b5113
result aa3cf33059d64dac8aef4a250bc5ea9c
result bef8104ea9184abaa3f0788ef7b2e0db
result c9d0cf4a1e9a48caa143afd2b0268f70
result cf2fda1b11d843f1ae7949dbbb7d179d
start cf2fda1b11d843f1ae7949dbbb7d179d
result cf2fda1b11d843f1ae7949dbbb7d179d
result d239aee577684c27afea2fe7e3361bdf
result d2ee042f0b0b4f7ea86e6e2c0dfdcfdd
result f28e92d5e5554857940c9d3386bf4121

In [19]: 


See what I mean? In the first run I inserted ""1d3413e88af946349f148c4fafeb6bf7"" but the second range scan I get ""a7dbda2925eb4b439c89cd71d56b5113"" back first, even when I set start="""". Could this somehow be my fault too?

Test follows:


import uuid

from thrift import Thrift
from thrift.transport import TTransport
from thrift.transport import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocolAccelerated

import sys
sys.path.insert(0,'/usr/local/cassandra/interface/thrift/gen-py')

from cassandra import Cassandra
from cassandra.ttypes import *

socket = TSocket.TSocket(""10.212.87.165"", 9160)
transport = TTransport.TBufferedTransport(socket)
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
client = Cassandra.Client(protocol)

transport.open()

ks = ""Keyspace1""
cf = ""Super1""
path = ColumnPath(cf, ""foo"", ""is"")
value = ""cool""

for i in xrange(10):
    key = uuid.uuid4().hex
    print ""insert"", key
    client.insert(ks, key, path, value, 0, ConsistencyLevel.ONE)

parent = ColumnParent(column_family=cf)
slice_range = SliceRange(start=""key"", finish=""key"")
predicate = SlicePredicate(slice_range=slice_range)


result = client.get_range_slice(ks, parent, predicate, """", """", 5, ConsistencyLevel.ONE)
for row in result:
    print ""result"", row.key

start = result[-1].key

print ""start"", start

result = client.get_range_slice(ks, parent, predicate, start, """", 10, ConsistencyLevel.ONE)
for row in result:
    print ""result"", row.key

","13/Feb/10 04:28;jbellis;Ah, yes, definitely reintroduced a bug in picking the range to start scanning in.  Fix attached.","13/Feb/10 21:04;bjc;Some of the patches didn't apply, some problem as before? I checked out freshly just now. Can you post the individual files again? Thanks.. Or..maybe this is the problem: why does your most recently attached patch have a Wednesday time stamp on it? Here is what I get at the top:

commit 630c33353647f062134d66afa3b487d95abe03fe
Author: Jonathan Ellis <jonathan.ellis@rackspace.com>
Date:   Wed Feb 10 18:04:08 2010 -0600

    fix range queries

Shouldn't that be Friday? Here's the transcript:

$ svn checkout https://svn.apache.org/repos/asf/incubator/cassan
dra/trunk cassandra
A    cassandra/test
A    cassandra/test/unit
...
$ cd cassandra
$ wget https://issues.apache.org/jira/secure/attachment/12435762
/781.txt
--2010-02-13 20:57:49--  https://issues.apache.org/jira/secure/attachment/124357
62/781.txt
Resolving issues.apache.org... 140.211.11.140
Connecting to issues.apache.org|140.211.11.140|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 47140 (46K) [text/plain]
Saving to: `781.txt'

100%[======================================>] 47,140       120K/s   in 0.4s    

2010-02-13 20:57:50 (120 KB/s) - `781.txt' saved [47140/47140]

$ patch -p1 <781.txt 
patching file src/java/org/apache/cassandra/db/ColumnFamilyStore.java
patching file src/java/org/apache/cassandra/db/RangeSliceReply.java
patching file src/java/org/apache/cassandra/dht/AbstractBounds.java
Hunk #1 FAILED at 1.
Hunk #2 FAILED at 25.
2 out of 2 hunks FAILED -- saving rejects to file src/java/org/apache/cassandra/
dht/AbstractBounds.java.rej
patching file src/java/org/apache/cassandra/dht/Bounds.java
Hunk #1 FAILED at 1.
Hunk #2 FAILED at 8.
2 out of 2 hunks FAILED -- saving rejects to file src/java/org/apache/cassandra/
dht/Bounds.java.rej
patching file src/java/org/apache/cassandra/dht/Range.java
patching file src/java/org/apache/cassandra/service/RangeSliceResponseResolver.j
ava
patching file src/java/org/apache/cassandra/service/StorageProxy.java
patching file src/java/org/apache/cassandra/service/StorageService.java
patching file src/java/org/apache/cassandra/thrift/CassandraServer.java
patching file src/java/org/apache/cassandra/thrift/ThriftValidation.java
patching file test/system/test_server.py
patching file test/unit/org/apache/cassandra/dht/BoundsTest.java
patching file test/unit/org/apache/cassandra/dht/RangeIntersectionTest.java
patching file test/unit/org/apache/cassandra/dht/RangeTest.java
$

",13/Feb/10 21:43;jbellis;re-attached w/ line endings fixed.  ,"14/Feb/10 00:12;bjc;Thanks for fixing the patch, but I'm sorry...it still doesn't work right.. :( I found a couple of weird things. First, sometimes when I ask for 10, it gives more. Second, if I pass start="""" it still doesn't start at the beginning. 

However, this doesn't happen every time I start fresh. Maybe it's depedent on the tokens. Here are the tokens for the case I show below: 

 INFO 23:38:21,989 Saved Token not found. Using 0njMRYmU9KiXE80d 
 INFO 23:38:20,112 Saved Token not found. Using I8LW8J6h9UCuz0dC 

The first token belongs to the node I am attaching the client to.

This functionality seems surprisingly complicated. Maybe it would help to write down pseudo-code for the way it should work? I have to admit that I cannot piece it together by reading the comments in your patch.

Here is a transcript and test: 

First run, looks ok: 

ip-10-212-87-165$ python test_bug_simple3.py 
insert 10b470f49a7c46bd938d784ca4096b63 
insert 47f70aacb3e94e10acaf8e86edac7169 
insert 9a3b7d3b921345bebc4f2bedc1db7c01 
insert c1c9dab59abd4f4ca33ee79f71a179e9 
insert cff81b145faf4648ac8ae001973c6c75 
insert c752d33e5d344312908e5008e6cdae3e 
insert 6e9e32e8b89845bb935d993a9c8bcb13 
insert c286bf2711bc45c1ab033561112c2313 
insert 2dad487ddfa94c81b52c8b4d35d3cb5c 
insert 9c62c7dafdb94dfdbdf52b527bdd2b24 

result 10b470f49a7c46bd938d784ca4096b63 
result 2dad487ddfa94c81b52c8b4d35d3cb5c 
result 47f70aacb3e94e10acaf8e86edac7169 
result 6e9e32e8b89845bb935d993a9c8bcb13 
result 9a3b7d3b921345bebc4f2bedc1db7c01 
result 9c62c7dafdb94dfdbdf52b527bdd2b24 
result c1c9dab59abd4f4ca33ee79f71a179e9 
result c286bf2711bc45c1ab033561112c2313 
result c752d33e5d344312908e5008e6cdae3e 
result cff81b145faf4648ac8ae001973c6c75 
total_keys 10 

Second run, get 18 keys when I asked for 10: 

ip-10-212-87-165$ python test_bug_simple3.py 
insert f873d662dccf46c28080a01286e09ed8 
insert 903776c2f45740389aa52675bf47c7ec 
insert 0e80401a9052405a898d11e5ae874a13 
insert 398d51ba174b4c9db8c25ca6cd2c9454 
insert 50f1cd47dd284ee9b9573b4dfce39134 
insert 20fa43d2365b4dfab9b05a93992315d0 
insert e009d5b76e8840b784fe6b9b649ae1df 
insert 63497f9d63c74b99a681fa2fc52751ac 
insert 824bbcf997de48a99cad174e9e1f1eec 
insert 01c5a6506f4247068660c20338a03bb3 

result 01c5a6506f4247068660c20338a03bb3 
result 0e80401a9052405a898d11e5ae874a13 
result 10b470f49a7c46bd938d784ca4096b63 
result 20fa43d2365b4dfab9b05a93992315d0 
result 2dad487ddfa94c81b52c8b4d35d3cb5c 
result 398d51ba174b4c9db8c25ca6cd2c9454 
result 47f70aacb3e94e10acaf8e86edac7169 
result 50f1cd47dd284ee9b9573b4dfce39134 
result 63497f9d63c74b99a681fa2fc52751ac 
result 6e9e32e8b89845bb935d993a9c8bcb13 
result 824bbcf997de48a99cad174e9e1f1eec 
result 903776c2f45740389aa52675bf47c7ec 
result c1c9dab59abd4f4ca33ee79f71a179e9 
result c286bf2711bc45c1ab033561112c2313 
result c752d33e5d344312908e5008e6cdae3e 
result cff81b145faf4648ac8ae001973c6c75 
result e009d5b76e8840b784fe6b9b649ae1df 
result f873d662dccf46c28080a01286e09ed8 
total_keys 18 

Third run, start at ""ca.."" even though I pass start="""" and all the previous keys remain: 

ip-10-212-87-165$ python test_bug_simple3.py 
insert ca97d7efb63448f8a62d6f7f73044236 
insert 91363a713b714af88ac2191caeea5351 
insert 7b6756d0ab8e450b826b1abc7210d524 
insert e6c6765497af4078b93e1a1470bd3194 
insert e3457f26754c4e7cb7ef606f98e7bb78 
insert 99643eb237ea4ca8b50cac4bb4d58edd 
insert ec3e1f81359b4ae08cfed73899934a93 
insert ae2b990ceb044bf194a879059f823ecf 
insert 2c1494f0ad3d48d2bf4feb33f40cf38e 
insert 0cb1c2e906b64fee89f7729052e0810e 

result ae2b990ceb044bf194a879059f823ecf 
result c1c9dab59abd4f4ca33ee79f71a179e9 
result c286bf2711bc45c1ab033561112c2313 
result c752d33e5d344312908e5008e6cdae3e 
result ca97d7efb63448f8a62d6f7f73044236 
result cff81b145faf4648ac8ae001973c6c75 
result e009d5b76e8840b784fe6b9b649ae1df 
result e3457f26754c4e7cb7ef606f98e7bb78 
result e6c6765497af4078b93e1a1470bd3194 
result ec3e1f81359b4ae08cfed73899934a93 
total_keys 10 
ip-10-212-87-165$ 

Here is another run (different tokens):

 INFO 00:04:17,105 Saved Token not found. Using Iw1khrAgM5sd6WnX
 INFO 00:04:15,795 Saved Token not found. Using IgLbq912n2xEP99G

In this case I don't see the problem where I get back more keys than I asked for, but I don't get the 10 lowest keys in the second request. 10 are returned, but they are not ordered consistently with what I know is in the db.

First run, notice key ""893.."" is inserted:

ip-10-212-87-165$ python test_bug_simple3.py 
insert 7be5d87bc45843cfaffd36fd654aee53
insert 8ef4727d83474570aa2111bee3929a5f
insert 9a9a91b6b662430092db0209d63a5c9e
insert e45afe1f0e364012acd0dead5b75ea13
insert 10171c87634842aea4f16d46d611c435
insert 10b6f92ac6a447088a82c4ec13056f1e
insert c1acbde9ae454ea2819322975322206b
insert 89352cf117dd4cb9ab935cbb5f230ba0
insert 0b5d924f04174459969594d6293b9aca
insert e2471db4d8f445f2b0c36f3b2a5bb650

result 0b5d924f04174459969594d6293b9aca
result 10171c87634842aea4f16d46d611c435
result 10b6f92ac6a447088a82c4ec13056f1e
result 7be5d87bc45843cfaffd36fd654aee53
result 89352cf117dd4cb9ab935cbb5f230ba0
result 8ef4727d83474570aa2111bee3929a5f
result 9a9a91b6b662430092db0209d63a5c9e
result c1acbde9ae454ea2819322975322206b
result e2471db4d8f445f2b0c36f3b2a5bb650
result e45afe1f0e364012acd0dead5b75ea13
total_keys 10

Second run, notice the results start with ""0.."" but ""893.."" is not returned (though ""b2.."" is, and other higher keys):

ip-10-212-87-165$ python test_bug_simple3.py 
insert 85d282dfa03a466eb51d03f4eb5dacd5
insert a61e7757eaed4ef79fc7bf35f47843f7
insert 9b9b6e3f22994827b0dddcc16105ff7d
insert 880b1644636845d8b1c92faf1f6d8484
insert 5d1d7d7b26ec4540a89c027bccc17e06
insert 4df05d38950f44b29df604c165e1148f
insert 0c9f818aa28a47fb832b6a0929b94280
insert 7e7b829c136046a88b120a4a373d9a6b
insert b2d497713f9342de85bb31b5c0e69af6
insert 80e250253f1942aaab2e9b49880918c0

result 0b5d924f04174459969594d6293b9aca
result 0c9f818aa28a47fb832b6a0929b94280
result 10171c87634842aea4f16d46d611c435
result 10b6f92ac6a447088a82c4ec13056f1e
result 4df05d38950f44b29df604c165e1148f
result a61e7757eaed4ef79fc7bf35f47843f7
result b2d497713f9342de85bb31b5c0e69af6
result c1acbde9ae454ea2819322975322206b
result e2471db4d8f445f2b0c36f3b2a5bb650
result e45afe1f0e364012acd0dead5b75ea13
total_keys 10
ip-10-212-87-165$ 

I found another issue with ""nodetool ring"" which might be related to the patch: 

$ sudo bin/nodetool -h localhost ring 
Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException 
        at $Proxy0.getRangeToEndPointMap(Unknown Source) 
        at org.apache.cassandra.tools.NodeProbe.getRangeToEndPointMap(NodeProbe.java:151) 
        at org.apache.cassandra.tools.NodeCmd.printRing(NodeCmd.java:74) 
        at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:403) 
Caused by: java.rmi.UnmarshalException: error unmarshalling return; nested exception is: 
        java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: org.apache.cassandra.dht.OrderPreservingPartitioner 
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:173) 
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source) 
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.invoke(Unknown Source) 
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.invoke(RMIConnector.java:993) 
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:288) 
        ... 4 more 
Caused by: java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: org.apache.cassandra.dht.OrderPreservingPartitioner 
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1333) 
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871) 
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:351) 
        at java.util.HashMap.readObject(HashMap.java:1029) 
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
        at java.lang.reflect.Method.invoke(Method.java:597) 
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974) 
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849) 
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:351) 
        at sun.rmi.server.UnicastRef.unmarshalValue(UnicastRef.java:306) 
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:155) 
        ... 8 more 
Caused by: java.io.NotSerializableException: org.apache.cassandra.dht.OrderPreservingPartitioner 
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1156) 
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1509) 
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1474) 
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1392) 
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1150) 
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:326) 
        at java.util.HashMap.writeObject(HashMap.java:1000) 
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
        at java.lang.reflect.Method.invoke(Method.java:597) 
        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:945) 
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1461) 
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1392) 
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1150) 
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:326) 
        at sun.rmi.server.UnicastRef.marshalValue(UnicastRef.java:274) 
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:315) 
        at sun.rmi.transport.Transport$1.run(Transport.java:159) 
        at java.security.AccessController.doPrivileged(Native Method) 
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155) 
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535) 
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790) 
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
        at java.lang.Thread.run(Thread.java:619) 
$ 


Here is the test I ran above: 

import uuid 

from thrift import Thrift 
from thrift.transport import TTransport 
from thrift.transport import TSocket 
from thrift.protocol.TBinaryProtocol import TBinaryProtocolAccelerated 

import sys 
sys.path.insert(0,'/usr/local/cassandra/interface/thrift/gen-py') 

from cassandra import Cassandra 
from cassandra.ttypes import * 

socket = TSocket.TSocket(""10.212.87.165"", 9160) 
transport = TTransport.TBufferedTransport(socket) 
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport) 
client = Cassandra.Client(protocol) 

transport.open() 

ks = ""Keyspace1"" 
cf = ""Super1"" 
path = ColumnPath(cf, ""foo"", ""is"") 
value = ""cool"" 

for i in xrange(10): 
    key = uuid.uuid4().hex 
    print ""insert"", key 
    client.insert(ks, key, path, value, 0, ConsistencyLevel.ONE) 

print 

parent = ColumnParent(column_family=cf) 
slice_range = SliceRange(start=""key"", finish=""key"") 
predicate = SlicePredicate(slice_range=slice_range) 

total_keys = 0 

result = client.get_range_slice(ks, parent, predicate, """", """", 10, ConsistencyLevel.ONE) 
for row in result: 
    total_keys += 1 
    print ""result"", row.key 

print ""total_keys"", total_keys 


","15/Feb/10 17:03;jbellis;Yes, it's more complicated than it looks. :)

Attached version fixes regression w/ result set size, and also start key when it falls into a wrapped node range.

There's also a ton of debug logging if you turn that on in log4j, btw.","15/Feb/10 23:18;bjc;Victory!! I think this patch works. :)

One last possible issue: if I remove keys, then do a get_range_slice(), they still show up. A get() on a removed key will return a ""not found"" exception. Should get_range_slice() be aware of the removal? I'm guessing this is an issue about not properly processing the ""tombstone"".

Due to the complexity of get_range_slice(), maybe it's not worth processing the tombstone?","16/Feb/10 01:38;jbellis;right, for get_range_slice we changed the contract from get_key_range -- it can't tell the difference between ""this row has other data, but not data in the columns you requested"" and ""this row has been deleted entirely"" w/o a relatively expensive query, so we decided to just return the slice as-is.",16/Feb/10 02:48;jbellis;fix committed to trunk.  backport to 0.5 pending.,"17/Feb/10 17:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ","22/Feb/10 12:46;herchu;Is this fix going to be backported to 0.5? (any ETA?)
Thanks!","22/Feb/10 13:08;jbellis;Yes, that's at the top of my list.  (I've been busy with PyCon.)","22/Feb/10 13:20;herchu;Great, thank you. The testcases for the current fix seem thorough, but if it helps I can do the testing with my own data as soon as a new patch is available.",22/Feb/10 20:38;jbellis;attached backport of fix to 0.5.,"23/Feb/10 14:04;herchu;+1 to 781-backport.txt, it solved the problem in my cluster. Thank you!","23/Feb/10 17:06;jbellis;committed to 0.5, thanks for testing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cfstats is broken,CASSANDRA-1540,12474972,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,9/23/2010 19:36,3/12/2019 14:14,3/13/2019 22:24,9/23/2010 21:23,0.7 beta 2,,,,0,,,,,,"There appears to be a problem reading the cache stats:

bin/nodetool -h cassandra-6 cfstats
Keyspace: org.apache.cassandra.db.Table@620a3d3b
        Read Count: 9
        Read Latency: 2.563222222222222 ms.
        Write Count: 11
        Write Latency: 0.1572727272727273 ms.
        Pending Tasks: 0
                Column Family: LocationInfo
                SSTable count: 1
                Space used (live): 4886
                Space used (total): 4886
                Memtable Columns Count: 6
                Memtable Data Size: 179
                Memtable Switch Count: 1
                Read Count: 4
                Read Latency: 5.369 ms.
                Write Count: 8
                Write Latency: 0.210 ms.
                Pending Tasks: 0
Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
        at $Proxy5.getCapacity(Unknown Source)
        at org.apache.cassandra.tools.NodeCmd.printColumnFamilyStats(NodeCmd.java:326)
        at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:439)
Caused by: javax.management.InstanceNotFoundException: org.apache.cassandra.db:type=Caches,keyspace=org.apache.cassandra.db.Table@620a3d3b,cache=LocationInfoKeyCache
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1118)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:679)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:672)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1284)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1382)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:619)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
        at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:273)
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:251)
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:160)
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:885)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:280)
        ... 3 more
",,,,,,,,,,,,,,,,,,,,23/Sep/10 20:34;brandon.williams;1540.txt;https://issues.apache.org/jira/secure/attachment/12455412/1540.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,17:39.1,,,no_permission,,,,,,,,,,,,20187,,,Fri Sep 24 12:47:04 UTC 2010,,,,,,0|i0g5uv:,92379,,,,,,,,,,,23/Sep/10 20:34;brandon.williams;Patch to register the mbean name correctly.,23/Sep/10 21:17;jbellis;+1,23/Sep/10 21:23;brandon.williams;Committed.,"24/Sep/10 12:47;hudson;Integrated in Cassandra #545 (See [https://hudson.apache.org/hudson/job/Cassandra/545/])
    Register CFS Mbean with the correct keyspace name.  Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1540
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupt SSTable,CASSANDRA-452,12436357,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,sammy.yu,sammy.yu,9/22/2009 22:06,3/12/2019 14:14,3/13/2019 22:24,11/10/2009 5:03,0.5,,,,0,,,,,,"We noticed on one of our node the number of SStables is growing.  The compaction thread is alive and running.  We can see that it is constantly trying to compact the same set of sstables.  However, it is failing because one of the sstable is corrupt:

ERROR [ROW-READ-STAGE:475] 2009-09-21 00:29:17,068 DebuggableThreadPoolExecutor.java (line 125) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.io.EOFException
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:110)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:44)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:383)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:361)
        at org.apache.cassandra.utils.FBUtilities.readByteArray(FBUtilities.java:390)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:64)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:349)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:309)
        at org.apache.cassandra.db.filter.SSTableNamesIterator.<init>(SSTableNamesIterator.java:102)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1467)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1401)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1420)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1401)
        at org.apache.cassandra.db.Table.getRow(Table.java:589)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:78)
        ... 4 more","Pre 0.4 based on r805615 on trunk w/ #370, #392, #394, #405, #406, #418",,,,,,,,,,,,,,,,,,,22/Sep/09 23:15;sammy.yu;FriendActions-17120.tar.gz;https://issues.apache.org/jira/secure/attachment/12420322/FriendActions-17120.tar.gz,22/Sep/09 22:54;sammy.yu;FriendActions-17122-Data.db;https://issues.apache.org/jira/secure/attachment/12420317/FriendActions-17122-Data.db,22/Sep/09 22:54;sammy.yu;FriendActions-17122-Filter.db;https://issues.apache.org/jira/secure/attachment/12420318/FriendActions-17122-Filter.db,22/Sep/09 22:54;sammy.yu;FriendActions-17122-Index.db;https://issues.apache.org/jira/secure/attachment/12420319/FriendActions-17122-Index.db,14/Oct/09 01:32;sammy.yu;FriendActions-22478.tar.gz;https://issues.apache.org/jira/secure/attachment/12422052/FriendActions-22478.tar.gz,23/Sep/09 04:01;jbellis;keys.txt;https://issues.apache.org/jira/secure/attachment/12420346/keys.txt,,,,,,,,6,,,,,,,,,,,,,,,,,,,12:31.4,,,no_permission,,,,,,,,,,,,19694,,,Tue Nov 10 04:21:33 UTC 2009,,,,,,0|i0fz0v:,91272,,,,,,,,,,,"22/Sep/09 22:54;sammy.yu;corrupt sstable
",22/Sep/09 23:15;sammy.yu;good sstable,23/Sep/09 02:12;jbellis;What is the definition for this CF?,"23/Sep/09 02:14;lenn0x;<ColumnFamily ColumnType=""Super"" CompareWith=""BytesType"" CompareSubcolumnsWith=""BytesType"" Name=""FriendActions""/>","23/Sep/09 03:55;jbellis;17120 is corrupt, all right.

The local deletion time is 0, which is nonsense (it's generated from System.CurrentTimeMillis) and the timestamp associated with that delete doesn't look like your other timestamps.  After that it's clearly reading nonsense and eventually EOFs while trying to read 19988495 columns.  Some debug output from my local code:

DEBUG - key is 148447622005950731053602871503233733033:itemdiggs15891086
...
DEBUG - reading name of length 7                                                                                     
DEBUG - deserializing SC ironeus deleted @0/21025451015143424; reading 19988495 columns                              
DEBUG - deserializing subcolumn 0                                                                                    
DEBUG - reading name of length 27237                                                                                 
DEBUG - deserializing rryjamesstoneJ�� ...

I can't see how the compaction code could cause this kind of corruption.  (Your logs should show: is this even the product of a compaction?  Or is it a direct result of a memtable or BMt?)

If it is a product of compaction, do you have a snapshot of that sstable any time prior to that compaction?  Can you reproduce the bug compacting those files?

I hate to blame hardware but there are a couple things that indicate this might actually be caused by that.  First, a localDeletionTime of zero is exactly 1 bit away from Integer.MIN_VALUE in 2's complement.  All the other localDT values are Integer.MIN_VALUE as would be expected if no deletes are done.  Second, no bytes are being skipped (which is often how you see some expected small number of columns get huge) -- the row sizes are correct and all the keys are readable where they should be.

I will attach a list of keys in this sstable so you can force read repair on them.  Tomorrow I will patch compaction to be able to recover from this error, and if Sammy or Chris can do CASSANDRA-426 then we will be able to reproduce any such future errors (assuming they are compaction related, rather than memtable/BMt).",23/Sep/09 04:01;jbellis;keys from corrupt sstable file,"23/Sep/09 18:09;sammy.yu;We determined last night that 17120 is a result of compaction.  I'm also working on CASSANDRA-453 which will help us validate offline the integrity of sstables.

","14/Oct/09 01:32;sammy.yu;Another corrupt sstable:
Digg/FriendActions-22478-Data.db: could not be fully read last key=106990576908512342565493723105254981184:itemdiggs15991619 at position 1039957","14/Oct/09 11:52;jbellis;Is this still your almost-0.4 code, or trunk?","14/Oct/09 12:26;sammy.yu;This is still almost 0.4.
","14/Oct/09 16:04;jbellis;I don't think I can do a whole lot until someone runs this with SnapshotBeforeCompaction (from CASSANDRA-426) turned on, and gives me a ""compacting this set of sstables produces this corrupt row"" case (or less likely, proves that a row was already corrupt when it was first flushed from a memtable).  0.4.1 and trunk both include this patch now",09/Nov/09 19:05;jbellis;haven't heard anything on this in a while -- does it look like the new compaction code in 0.5 fixes it then?,"10/Nov/09 04:21;lenn0x;This is safe to close for now, we haven't run into a corruption issue yet.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the read race condition in CFStore for counters ,CASSANDRA-2105,12497550,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,2/3/2011 10:51,3/12/2019 14:14,3/13/2019 22:24,3/28/2011 14:45,0.8 beta 1,,,,1,counters,,,,,"There is a (known) race condition during counter read. Indeed, for standard
column family there is a small time during which a memtable is both active and
pending flush and similarly a small time during which a 'memtable' is both
pending flush and an active sstable. For counters that would imply sometime
reconciling twice during a read the same counterColumn and thus over-counting.

Current code changes this slightly by trading the possibility to count twice a
given counterColumn by the possibility to miss a counterColumn. Thus it trades
over-counts for under-counts.

But this is no fix and there is no hope to offer clients any kind of guarantee
on reads unless we fix this.
",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,04/Feb/11 16:56;slebresne;2115_option1_withLock.patch;https://issues.apache.org/jira/secure/attachment/12470250/2115_option1_withLock.patch,04/Feb/11 16:56;slebresne;2115_option2_nolock.patch;https://issues.apache.org/jira/secure/attachment/12470251/2115_option2_nolock.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,36:05.7,,,no_permission,,,,,,,,,,,,20446,,,Mon Mar 28 14:45:59 UTC 2011,,,,,,0|i0g9cf:,92944,,,,,,,,,,,"03/Feb/11 10:51;slebresne;I plan to reuse the fix I had made for #1546 that uses a ReadWriteLock to fix
this (unless I find a better idea in the meantime). Unless proven otherwise I
don't think this will have a huge impact on counter read performance, but if
someone finds a better idea, I'm listening.","04/Feb/11 16:56;slebresne;Attached not 1 but 2 options for this patch. I'm not sure with which version to go so I'm asking for opinions.

Version 1 is the one extracted from #1546. It uses a ReadWriteLock to protect from the race condition.

Version 2 don't use a lock. So less chances of lock contention which is always good. Only problem is, it still suffers in theory of a race condition. But I think this race condition is borderline impossible.
Basically, given a memtable m being flushed, let's call s(m) the sstable initially produced by its flushing and let's denote by s'(m) any sstable resulting of the compaction of s(m). The race is if a read thread sees m when grabbing the references to the memtable being flushed and sees s'(m) (not s(m), that is the initial race condition and this is not impossible at all) when grabing the reference to the sstables.
If it's unclear, the code has a comment explaining this that may be more clear.

So not sure which version to go with. I may slightly lean towards Version 1 because I usually side with correction before anything else, but since this is in a critical path it feels slightly wasteful to use a lock for this given how remote the race condition of version 2 seems.
","08/Mar/11 14:52;slebresne;I've opened CASSANDRA-2284 that provides what I think is a better solution than the one I have attached previously to this problem (I've opened it separately because it's a more generic solution, not just a counter related fix).

","28/Mar/11 14:36;hudson;Integrated in Cassandra #810 (See [https://hudson.apache.org/hudson/job/Cassandra/810/])
    Atomically switch cfstore memtables and sstables
patch by slebresne; reviewed by jbellis for CASSANDRA-2284 (and CASSANDRA-2105)
",28/Mar/11 14:45;slebresne;Fixed by CASSANDRA-2284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Expected both token and generation columns""",CASSANDRA-1146,12465830,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,jbellis,jbellis,6/1/2010 6:09,3/12/2019 14:14,3/13/2019 22:24,6/7/2010 13:46,0.6.3,0.7 beta 1,,,0,,,,,,"From the mailing list:

{code}
ERROR 16:14:35,975 Exception encountered during startup.
java.lang.RuntimeException: Expected both token and generation columns; found ColumnFamily(LocationInfo [Generation:false:4@4,])
    at org.apache.cassandra.db.SystemTable.initMetadata(SystemTable.java:159)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:305)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:99)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:177)
Exception encountered during startup.
{code}

Separately, the same user wrote: ""I added a server to my cluster. It had some junk in the system/LocationInfo files from previous, unsuccessful attempts to add the server to the cluster. (They were unsuccessful because I hadn't opened the port on that computer.)""

Perhaps that is why it was able to create the Generation column but not the Token?
",,,,,,,,,,,,,,,CASSANDRA-521,,,,,03/Jun/10 16:48;gdusbabek;0001-detect-partitioner-changes-and-fail-fast.patch;https://issues.apache.org/jira/secure/attachment/12446264/0001-detect-partitioner-changes-and-fail-fast.patch,05/Jun/10 04:59;gdusbabek;v2-0001-detect-partitioner-changes-and-fail-fast.patch;https://issues.apache.org/jira/secure/attachment/12446399/v2-0001-detect-partitioner-changes-and-fail-fast.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,46:30.8,,,no_permission,,,,,,,,,,,,20007,,,Sun Jun 06 02:20:30 UTC 2010,,,,,,0|i0g39z:,91961,,,,,,,,,,,02/Jun/10 14:46;gdusbabek;This is the error you get these days when you go from RP to OPP and back to RP.,02/Jun/10 14:53;gdusbabek;Increasing priority to major since startup doesn't fail after changing partitioners and doing so corrupts your system table.,"02/Jun/10 17:24;jbellis;Isn't this the same as that one issue for making system table always OPP, then?

IIRC we decided to wait until we had sstable versioning done, so we could grandfather in old system tables appropriately.","02/Jun/10 17:25;jbellis;Ah, CASSANDRA-521.  I see you're ahead of me again.","02/Jun/10 17:40;gdusbabek;We decided to wait because we promised we wouldn't change the sstable disk format for 0.5

I guess we need to ask that question again for 0.7.

I still think a header is the right way to implement this.","03/Jun/10 16:51;gdusbabek;patch is for trunk, but putting this back to 0.6 shouldn't pose a stability problem or interfere with upgrades.

Biggest change is that partitioner name is stored in system table and ST.initMetadata() flushes system tables after writing.

btw, this patch doesn't interfere with the sstable format.","04/Jun/10 04:19;jbellis;why the change to flushing system tables?  it's in the commitlog, so it will get replayed if necessary.",04/Jun/10 04:24;gdusbabek;The flush is for the case where a new node halts before the initial system CFs can be written to sstables.,04/Jun/10 12:29;jbellis;so it's covered by the commitlog.  (which CD replays before ST ever gets involved on the next restart),"04/Jun/10 13:04;gdusbabek;Right, but you don't want to replay the commit log if someone has changed the partitioner, do you?  That would be Bad.  The point of this patch is to detect a partitioner mismatch at the earliest possible moment.  Flushing the CL after updating the system table means that we have to wait for one less restart for the the system sstable files to appear so that we can rely on the mismatch detection code to work.","04/Jun/10 13:44;jbellis;ah, right.  i get it now.

in that case isn't CFS.forceBlockingFlush on STATUS_CF adequate?",04/Jun/10 13:53;gdusbabek;Yes.  I'll make that change.,05/Jun/10 04:59;gdusbabek;New patch only flushes STATUS_CF.,06/Jun/10 02:20;jbellis;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup can create sstables whose contents do not match their advertised version,CASSANDRA-2211,12499284,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,2/21/2011 23:28,3/12/2019 14:14,3/13/2019 22:24,2/22/2011 14:53,0.7.3,,,,0,,,,,,"Since cleanup switched to per-sstable operation (CASSANDRA-1916), the main loop looks like this:

{code}
                    if (Range.isTokenInRanges(row.getKey().token, ranges))
                    {
                        writer = maybeCreateWriter(sstable, compactionFileLocation, expectedBloomFilterSize, writer);
                        writer.append(new EchoedRow(row));
                        totalkeysWritten++;
                    }
                    else
                    {
                        while (row.hasNext())
                        {
                            IColumn column = row.next();
                            if (indexedColumns.contains(column.name()))
                                Table.cleanupIndexEntry(cfs, row.getKey().key, column);
                        }
                    }
{code}

... that is, rows that haven't changed we copy to the new sstable without deserializing, with EchoedRow.  But, the new sstable is created with CURRENT_VERSION which may not be what the old data consisted of.

(This could cause symptoms similar to CASSANDRA-2195 but I do not think it is the cause of that bug; IIRC the cluster in question there was not upgraded from an older Cassandra.)",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,22/Feb/11 12:40;slebresne;0001-2211-v3.patch;https://issues.apache.org/jira/secure/attachment/12471610/0001-2211-v3.patch,22/Feb/11 05:43;jbellis;2211-v2.txt;https://issues.apache.org/jira/secure/attachment/12471599/2211-v2.txt,21/Feb/11 23:49;jbellis;2211.txt;https://issues.apache.org/jira/secure/attachment/12471584/2211.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,40:14.7,,,no_permission,,,,,,,,,,,,20507,,,Tue Feb 22 15:20:55 UTC 2011,,,,,,0|i0g9zz:,93050,slebresne,slebresne,,,,,,,,,21/Feb/11 23:44;jbellis;This bug was introduced in 0.7.0 but in practice was not a problem until we changed sstable formats again in 0.7.1 for CASSANDRA-1555.,"22/Feb/11 02:41;jbellis;A better fix would be to have it echo if the data is on the current version, otherwise rewrite.  This would (a) be a better fit with our policy of not having to keep code around to write old versions and (b) allow a better upgrade path to version N + 1 (that doesn't support the old version sstables) than major compaction. I'll see if I can do that tonight.",22/Feb/11 05:43;jbellis;v2 as described above.,"22/Feb/11 12:40;slebresne;+1 on the patch. I'm just attaching a v3 that simply use getDefaultGcBefore() throughout CompactionManager (to make things cleaner)

Sadly, this is not the only place where we echo data wrongfully, cf. CASSANDRA-2216",22/Feb/11 14:53;jbellis;committed,"22/Feb/11 15:20;hudson;Integrated in Cassandra-0.7 #303 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/303/])
    fix for cleanup writing old-format data into new-version sstable
patch by jbellis; reviewed by slebresne for CASSANDRA-2211
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Crash during startup: SSTable doesn't handle corrupt (empty) tmp files,CASSANDRA-1904,12494118,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,tcn,tcn,12/27/2010 11:01,3/12/2019 14:14,3/13/2019 22:24,12/30/2010 13:57,0.7.0,,,,0,,,,,,"Applies to 0.7rc3 as well, but not yet selectable in Jira.

cassandra stumbles upons empty Data files and crashes during startup rather than ignoring these files:

java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:233)
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:284)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:200)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:225)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:448)
	at org.apache.cassandra.db.ColumnFamilyStore.addIndex(ColumnFamilyStore.java:305)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:246)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:448)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:436)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)
Exception encountered during startup.
java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:233)
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:284)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:200)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:225)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:448)
	at org.apache.cassandra.db.ColumnFamilyStore.addIndex(ColumnFamilyStore.java:305)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:246)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:448)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:436)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)

The empty Data/Index tmp files were in my case created and left over when I attempted to create a secondary index at runtime which crashed the JVM due to OOM.

SSTable handles IOExceptions so it should be an easy fix: in SSTable.estimateRowsFromIndex() just check for ifile.length() ==ifile.getFilePointer()==keys==0 and throw an IOException.",,,,,,,,,,,,,,,,,,,,29/Dec/10 23:26;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-CFS.scrubDataDirectories-to-include-index-files.txt;https://issues.apache.org/jira/secure/attachment/12467142/ASF.LICENSE.NOT.GRANTED--v1-0001-CFS.scrubDataDirectories-to-include-index-files.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,25:38.4,,,no_permission,,,,,,,,,,,,20364,,,Thu Dec 30 13:57:42 UTC 2010,,,,,,0|i0g84n:,92747,,,,,,,,,,,"29/Dec/10 21:25;gdusbabek;I get a different error, which happens before the error in Timo's stack trace.  I suspect his files weren't really zero-length, but the BRAF just reported them that way.  Either way: CFS.scrubDataDirectories() should take secondary indexes into account.

My forced error:

 INFO [main] SSTableReader.java@154 14:43:44,146 Opening /Users/gary.dusbabek/cass-configs/trunk/data_1/data/Keyspace1/Indexed1.birthdate_idx-f-1
DEBUG [main] SSTableReader.java@164 14:43:44,146 Load statistics for /Users/gary.dusbabek/cass-configs/trunk/data_1/data/Keyspace1/Indexed1.birthdate_idx-f-1
ERROR [main] ColumnFamilyStore.java@234 14:43:44,147 Corrupt sstable /Users/gary.dusbabek/cass-configs/trunk/data_1/data/Keyspace1/Indexed1.birthdate_idx-f-1=[Filter.db, Index.db, Data.db, Statistics.db]; skipped
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:375)
	at org.apache.cassandra.utils.EstimatedHistogram$EstimatedHistogramSerializer.deserialize(EstimatedHistogram.java:179)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:166)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:225)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:488)
	at org.apache.cassandra.db.ColumnFamilyStore.addIndex(ColumnFamilyStore.java:345)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:246)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:488)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:476)
	at org.apache.cassandra.db.Table.initCf(Table.java:360)
	at org.apache.cassandra.db.Table.<init>(Table.java:290)
	at org.apache.cassandra.db.Table.open(Table.java:107)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:162)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:54)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:240)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:133)
DEBUG [main] SSTableTracker.java@179 14:43:44,148 key cache capacity for Indexed1.birthdate_idx is 200000
DEBUG [main] SSTableTracker.java@190 14:43:44,149 row cache capacity for Indexed1.birthdate_idx is 0","29/Dec/10 22:11;tcn;What is BRAF? Yes, they were zero-length, at least according to ls and eclipse' debugger and I even deleted the files and recreated equally named, empty files manually :) 

IMHO code like

  int foo = 0;

  while(whatever) { whatever }

  bar / foo;

is always broken. A simple additional if-throw-IOException won't hurt and will help to grasp things faster.","29/Dec/10 22:55;gdusbabek;I reproduced the original error by removing the statistics db in my database.  I still think the right approach is to make sure that CFS.scrubDataDirectories() includes index CFs.  

FWIW empty files will cause problems all over the place, not just at that particular spot.  The approach that has worked so far is to 'scrub' the directories of undesired files at startup rather than address all the places in the code where we've assumed the files in the directories are healthy and supposed to be there.","29/Dec/10 23:36;jbellis;bq. I still think the right approach is to make sure that CFS.scrubDataDirectories() includes index CFs. 

+1, recovering from random files being missing isn't something we want to try to handle now.",29/Dec/10 23:40;jbellis;also +1 to the patch,30/Dec/10 13:57;gdusbabek;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When restarting Cassandra I get this error: java.io.IOError: java.io.IOException: Failed to delete  ..data\system\LocationInfo-e-1-Data.db,CASSANDRA-1852,12493087,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,aadel,aadel,12/13/2010 16:06,3/12/2019 14:14,3/13/2019 22:24,12/13/2010 16:23,0.7.0 rc 2,,,,0,,,,,,"java.io.IOError: java.io.IOException: Failed to delete C:\Documents and Settings\Bureaublad\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:483)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:102)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at com.remainsoftware.gravity.cassandra.internal.GCassandra$1.run(GCassandra.java:21)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Failed to delete C:\Documents and Settings\Bureaublad\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:479)
	... 7 more
ERROR 16:54:52,984 Exception encountered during startup.
java.io.IOError: java.io.IOException: Failed to delete C:\Documents and Settings\Bureaublad\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:483)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:102)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at com.remainsoftware.gravity.cassandra.internal.GCassandra$1.run(GCassandra.java:21)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Failed to delete C:\Documents and Settings\Bureaublad\cassandra\data\system\LocationInfo-e-1-Data.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:479)
	... 7 more",Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,20344,,,Mon Dec 13 16:23:10 UTC 2010,,,,,,0|i0g7t3:,92695,,,,,,,,,,,13/Dec/10 16:23;aadel;I just downloaded  the latest beta version: 0.7.0 rc 2.  It seems to be fixed. Thanks folks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CompareWith=""LongType"" CF mis-applies tombstones",CASSANDRA-386,12433532,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,eweaver,eweaver,8/20/2009 4:00,3/12/2019 14:14,3/13/2019 22:24,8/24/2009 21:49,,,,,0,,,,,,"jbellis: what is ""mis applying?""
evn: later inserts have no effect
jbellis: so you do a remove with timestamp X, then timestamp X + 1 has no effect?
evn: yeah
jbellis: did you try a similar test w/ a ascii comparewith?
evn: well the identical test passes w/ TimeUUIDType

      <Keyspace Name=""MultiblogLong"">      
        <KeysCachedFraction>0.01</KeysCachedFraction>
        <ColumnFamily CompareWith=""LongType"" Name=""Blogs""/>
        <ColumnFamily CompareWith=""LongType"" Name=""Comments""/>
      </Keyspace>

$ ruby test/cassandra_test.rb -n test_get_first_long_column
insert at 1250740275826063
.
1 tests, 3 assertions, 0 failures, 0 errors

$ ruby test/cassandra_test.rb -n test_get_first_long_column
remove at 1250740278998607
insert at 1250740279011751
F
  1) Failure:
test_get_first_long_column(CassandraTest) [test/cassandra_test.rb:70]:
<{<Cassandra::Long#13703350 time: Tue Jul 14 00:20:16 -0400 1970, usecs: 0, jitter: 3626>=>
  ""I like this cat""}> expected but was
<{}>.
1 tests, 1 assertions, 1 failures, 0 errors",,,,,,,,,,,,,,,,,,,,21/Aug/09 14:07;jbellis;386-v2.patch;https://issues.apache.org/jira/secure/attachment/12417274/386-v2.patch,21/Aug/09 00:41;jbellis;386.patch;https://issues.apache.org/jira/secure/attachment/12417199/386.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,29:53.8,,,no_permission,,,,,,,,,,,,19665,,,Tue Aug 25 14:21:27 UTC 2009,,,,,,0|i0fym7:,91206,,,,,,,,,,,"20/Aug/09 04:04;eweaver;exact sequence is as so:

1 read key range - no results
2 delete all keys in key range - which is nothing
3 insert key
4 check that key is there
5 read key range - one resutl
6 delete all keys in key range - which is key from step 3
7 insert key again, with new timestamp
8 check that key is there ***THIS FAILS***

each operation gets its own timestamp based on the (then) current time.","20/Aug/09 16:29;jbellis;this test passes for me:

    def test_long_remove(self):
        path = ColumnPath('StandardLong1', column=_i64(1))
        client.insert('Keyspace1', 'key1', path, 'value1', 0, ConsistencyLevel.ONE)
        client.remove('Keyspace1', 'key1', path, 1, ConsistencyLevel.ONE)
        _expect_missing(lambda: client.get('Keyspace1', 'key1', path, ConsistencyLevel.ONE))
        L = client.get_key_range('Keyspace1', 'StandardLong1', '', '', 10)
        assert L == [], L
        # resurrect
        client.insert('Keyspace1', 'key1', path, 'value2', 2, ConsistencyLevel.ONE)
        c = client.get('Keyspace1', 'key1', path, ConsistencyLevel.ONE).column
        assert c.value == 'value2', c
        L = client.get_key_range('Keyspace1', 'StandardLong1', '', '', 10)
        assert L == ['key1'], L

","20/Aug/09 16:31;jbellis;also if i change the remove selectivity to the CF:

        client.remove('Keyspace1', 'key1', ColumnPath('StandardLong1'), 1, ConsistencyLevel.ONE)
",20/Aug/09 16:33;eweaver;will dig deeper,"20/Aug/09 22:29;eweaver;it only happens with a small count value (try 1)

 I think the tombstones are getting applied against the count","20/Aug/09 22:29;eweaver;  def test_long_remove_bug
    @blogs_long.insert(:Blogs, key, {@longs[0] => 'I like this cat'})
    @blogs_long.remove(:Blogs, key)
    assert_equal({}, @blogs_long.get(:Blogs, key, :count => 1))

    @blogs_long.insert(:Blogs, key, {@longs[0] => 'I like this cat'})
    assert_equal({@longs[0] => 'I like this cat'}, @blogs_long.get(:Blogs, key, :count => 1)) # FAILS HERE
  end

","20/Aug/09 22:32;eweaver;Note that I am using get_slice, not get (sorry, that was not clear at all before):

""send_get_slice(""
""MultiblogLong""
""test_long_remove_bug""
<CassandraThrift::ColumnParent column_family:""Blogs"">
<CassandraThrift::SlicePredicate slice_range:<CassandraThrift::SliceRange start:"""", finish:"""", reversed:false, count:1>>
1
"")""
","20/Aug/09 22:53;jbellis;still confused. this works both with reversed slice and not:

    def test_long_remove(self):
        path = ColumnPath('StandardLong1', column=_i64(1))
        column_parent = ColumnParent('StandardLong1')
        sp = SlicePredicate(slice_range=SliceRange('', '', False, 1))

        client.insert('Keyspace1', 'key1', path, 'value1', 0, ConsistencyLevel.ONE)
        client.remove('Keyspace1', 'key1', ColumnPath('StandardLong1'), 1, ConsistencyLevel.ONE)
        slice = client.get_slice('Keyspace1', 'key1', column_parent, sp, ConsistencyLevel.ONE)
        assert slice == [], slice
        # resurrect
        client.insert('Keyspace1', 'key1', path, 'value2', 2, ConsistencyLevel.ONE)
        slice = [result.column
                 for result in client.get_slice('Keyspace1', 'key1', column_parent, sp, ConsistencyLevel.ONE)]
        assert slice == [Column(_i64(1), 'value2', 2)], slice
","20/Aug/09 23:12;eweaver; it's intermittent for me... if you're resetting the data folder each time, try running the test a bunch of times without doing that.",20/Aug/09 23:19;eweaver;full dump of client messages: http://pastie.org/pastes/590542,"20/Aug/09 23:34;jbellis;ok, i can reproduce when using (a) multiple iterations and (b) a different column name on each one",21/Aug/09 00:41;jbellis;include column container's deletion status when determining whether to include a column in the live count,21/Aug/09 03:18;junrao;Can't apply patch to trunk. Could you rebase?,21/Aug/09 03:31;jbellis;patch applies fine for me on a different machine.  what error are you seeing?,21/Aug/09 03:37;jbellis;(verified that patch applies to fresh checkout here),"21/Aug/09 06:25;euphoria;Patch *applies* but that lone ""if"" line at SliceQueryFilter.java:113 doesn't build or make sense over here.","21/Aug/09 06:27;eweaver;Doesn't apply to git checkout:

100%[=============================================================================================================================>] 6,902       --.-K/s   in 0.05s   

2009-08-20 23:26:18 (128 KB/s) - `-' saved [6902/6902]

patching file src/java/org/apache/cassandra/db/Column.java
patching file src/java/org/apache/cassandra/db/IColumn.java
patching file src/java/org/apache/cassandra/db/IColumnContainer.java
patching file src/java/org/apache/cassandra/db/SuperColumn.java
patching file src/java/org/apache/cassandra/db/filter/SliceQueryFilter.java
patching file test/system/test_server.py
[trunk]: created 8319a20: ""Applied patch: ""http://issues.apache.org/jira/secure/attachment/12417199/386.patch""""
 6 files changed, 57 insertions(+), 5 deletions(-)
Building Cassandra
Buildfile: build.xml

build-subprojects:

init:
    [mkdir] Created dir: /Users/eweaver/cassandra/server/build/classes
    [mkdir] Created dir: /Users/eweaver/cassandra/server/build/test/classes
    [mkdir] Created dir: /Users/eweaver/cassandra/server/src/gen-java

check-gen-cli-grammar:

gen-cli-grammar:
     [echo] Building Grammar /Users/eweaver/cassandra/server/src/java/org/apache/cassandra/cli/Cli.g  ....

build-project:
     [echo] apache-cassandra-incubating: /Users/eweaver/cassandra/server/build.xml
    [javac] Compiling 257 source files to /Users/eweaver/cassandra/server/build/classes
    [javac] /Users/eweaver/cassandra/server/src/java/org/apache/cassandra/db/filter/SliceQueryFilter.java:113: '(' expected
    [javac]             if 
    [javac]               ^
    [javac] /Users/eweaver/cassandra/server/src/java/org/apache/cassandra/db/filter/SliceQueryFilter.java:114: ')' expected
    [javac]             logger.debug(""collecting "" + column.getString(comparator));
    [javac]                                                                       ^
    [javac] 2 errors

BUILD FAILED
/Users/eweaver/cassandra/server/build.xml:119: Compile failed; see the compiler error output for details.

","21/Aug/09 13:52;jbellis;ah, thanks michael.  sometimes i need someone to point me to the obvious :)",21/Aug/09 14:07;jbellis;attached version that actually builds :),21/Aug/09 20:42;eweaver;ship it!,"21/Aug/09 21:21;junrao;The last test in SliceQueryFilter.collectReducedColumns() doesn't seem quite right. It seems that the test should be the following:
            // but we need to add all non-gc-able columns to the result for read repair
            if ( ( !column.isMarkedForDelete()
                   && (!container.isMarkedForDelete() || column.mostRecentChangeAt() > container.getMarkedForDeleteAt())
                 )    // this is to get all included columns
                 || (column.isMarkedForDelete() && column.getLocalDeletionTime() > gcBefore) // this is to get all deleted, but not garbage-collected columns
              )

","24/Aug/09 15:01;jbellis;that will return redundant columns if the container has been deleted more recently than anything relevant in the column.

added comments to the patch version:

            // but we need to add all non-gc-able columns to the result for read repair:
            // the column itself must be not gc-able, (1)
            // and if its container is deleted, the column must be changed more recently than the container tombstone (2)
            // (since otherwise, the only thing repair cares about is the container tombstone)
            if ((!column.isMarkedForDelete() || column.getLocalDeletionTime() > gcBefore) // (1)
                && (!container.isMarkedForDelete() || column.mostRecentChangeAt() > container.getMarkedForDeleteAt())) // (2)
","24/Aug/09 21:49;jbellis;irc:
> junrao: jbellis: #386 looks fine to me

committed, w/ comments as given above.","25/Aug/09 14:21;hudson;Integrated in Cassandra #177 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/177/])
    need to include column container's deletion status when determining whether to include a column in the live count.
patch by jbellis; reviewed by Jun Rao for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException,CASSANDRA-853,12458265,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,riffraff,btoddb,btoddb,3/5/2010 17:11,3/12/2019 14:14,3/13/2019 22:24,3/5/2010 20:12,0.6,,,,0,,,,,,"i'm seeing a lot of these ... any idea?

2010-03-04 18:53:21,455 ERROR [MEMTABLE-POST-FLUSHER:1] [DebuggableThreadPoolExecutor.java:94] Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:357)
        at org.apache.cassandra.db.ColumnFamilyStore$2.runMayThrow(ColumnFamilyStore.java:392)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.util.concurrent.ExecutionException: java.util.ConcurrentModificationException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:349)
        ... 8 more
Caused by: java.util.ConcurrentModificationException
        at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:605)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:385)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:71)
        at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:343)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at org.apache.cassandra.db.commitlog.CommitLogExecutorService.process(CommitLogExecutorService.java:113)
        at org.apache.cassandra.db.commitlog.CommitLogExecutorService.access$200(CommitLogExecutorService.java:35)
        at org.apache.cassandra.db.commitlog.CommitLogExecutorService$1.runMayThrow(CommitLogExecutorService.java:67)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
",,,,,,,,,,,,,,,,,,,,05/Mar/10 17:30;riffraff;CASSANDRA-853.patch;https://issues.apache.org/jira/secure/attachment/12438025/CASSANDRA-853.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,29:59.9,,,no_permission,,,,,,,,,,,,19890,,,Fri Mar 05 20:12:50 UTC 2010,,,,,,0|i0g1hb:,91670,,,,,,,,,,,"05/Mar/10 17:29;riffraff;this seems just a mis-use of the foreach loop, attaching patch that should fix it but AFAICT there are no tests for discardCompletedSegments at all? 
",05/Mar/10 19:48;riffraff;avoids collection modification while looping,"05/Mar/10 20:12;jbellis;Committed to 0.6 and trunk, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
slice offset breaks read repair,CASSANDRA-286,12429945,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,7/9/2009 14:26,3/12/2019 14:14,3/13/2019 22:24,7/16/2009 0:39,0.4,,,,0,,,,,,"[code]
        int liveColumns = 0;
        int limit = offset + count;

        for (IColumn column : reducedColumns)
        {
            if (liveColumns >= limit)
                break;
            if (!finish.isEmpty()
                && ((isAscending && column.name().compareTo(finish) > 0))
                    || (!isAscending && column.name().compareTo(finish) < 0))
                break;
            if (!column.isMarkedForDelete())
                liveColumns++;

            if (liveColumns > offset)
                returnCF.addColumn(column);
        }
[code]

The problem is that for offset to return the correct ""live"" columns, it has to ignore tombstones it scans before the first live one post-offset.

This means that instead of being corrected within a few ms of a read, a node can continue returning deleted data indefinitely (until the next anti-entropy pass).

Coupled with offset's inherent inefficiency (see CASSANDRA-261) I think this means we should take it out and leave offset to be computed client-side (which, for datasets under which it was reasonable server-side, will still be reasonable).",,,,,,,,,,,,,,,,,,,,15/Jul/09 22:07;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-286-r-m-offset-from-slice-api-we-could-live.txt;https://issues.apache.org/jira/secure/attachment/12413602/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-286-r-m-offset-from-slice-api-we-could-live.txt,15/Jul/09 22:07;jbellis;ASF.LICENSE.NOT.GRANTED--0002-fix-not-including-tombstone-only-keys-in-keyRange.txt;https://issues.apache.org/jira/secure/attachment/12413603/ASF.LICENSE.NOT.GRANTED--0002-fix-not-including-tombstone-only-keys-in-keyRange.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,30:34.4,,,no_permission,,,,,,,,,,,,19621,,,Thu Jul 16 13:20:21 UTC 2009,,,,,,0|i0fy07:,91107,,,,,,,,,,,"09/Jul/09 15:30;junrao;This is a problem with any APIs relying on offset, instead of value. All columns before the offset affect the outcome. So, if there is any incorrect column (whether it's missing deletes or missing inserts) before the offset doesn't get fixed immediately, the outcome will be incorrect.

One potential fix is to include all columns before offset in the repair logic, but not in thrift return. This won't affect performance much since we already have to scan those columns. This may complicates the overall logic a bit though.
 ","09/Jul/09 16:04;jbellis;Yes, that's the brute force fix, but it means that in the case of mass deletes in a given CF we could very possibly OOM collecting all the tombstones for a large offset.

Again, my rule of thumb is: features that allow the user to do something that slow things down are ok; features that allow the user to crash the server, are not. :)","09/Jul/09 17:17;sandeep_tata;I agree ... offset makes it hard to understand the cost of a get_slice. While it is very convenient for pagination, dropping it from the API is probably the right choice.
",15/Jul/09 22:08;jbellis;patch 2 fixes the bug with tombstone handling in get_key_range that Evan noticed in CASSANDRA-139,15/Jul/09 22:55;eweaver;Twitter collective is in favor of killing offset.,"15/Jul/09 22:58;jbellis;does that mean you're +1 on the patch, or the idea? :)","15/Jul/09 23:02;eweaver;I meant idea. But now I also mean patch.

Fixes my bugs. Code looks fine; ship it!",16/Jul/09 00:39;jbellis;committed,"16/Jul/09 13:20;hudson;Integrated in Cassandra #139 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/139/])
    fix not including tombstone-only keys in keyRange.
patch by jbellis; reviewed by Evan Weaver for 
r/m offset from slice api; we could live with being inefficient but not with breaking read repair.
patch by jbellis; reviewed by Evan Weaver for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bugs in tombstone handling in remove code,CASSANDRA-33,12421770,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,4/1/2009 13:47,3/12/2019 14:14,3/13/2019 22:24,4/20/2009 16:39,0.3,,,,0,,,,,,"[copied from dev list]

Avinash pointed out two bugs in my remove code.  One is easy to fix,
the other is tougher.

The easy one is that my code removes tombstones (deletion markers) at
the ColumnFamilyStore level, so when CassandraServer does read repair
it will not know about the tombstones and they will not be replicated
correctly.  This can be fixed by simply moving the removeDeleted call
up to just before CassandraServer's final return-to-client.

The hard one is that tombstones are problematic on GC (that is, major
compaction of SSTables, to use the Bigtable paper terminology).

One failure scenario: Node A, B, and C replicate some data.  C goes
down.  The data is deleted.  A and B delete it and later GC it.  C
comes back up.  C now has the only copy of the data so on read repair
the stale data will be sent to A and B.

A solution: pick a number N such that we are confident that no node
will be down (and catch up on hinted handoffs) for longer than N days.
 (Default value: 10?)  Then, no node may GC tombstones before N days
have elapsed.  Also, after N days, tombstones will no longer be read
repaired.  (This prevents a node which has not yet GC'd from sending a
new tombstone copy to a node that has already GC'd.)

Implementation detail: we'll need to add a 32-bit ""time of tombstone""
to ColumnFamily and SuperColumn.  (For Column we can stick it in the
byte[] value, since we already have an unambiguous way to know if the
Column is in a deleted state.)  We only need 32 bits since the time
frame here is sufficiently granular that we don't need ms.  Also, we
will use the system clock for these values, not the client timestamp,
since we don't know what the source of the client timestamps is.

Admittedly this is suboptimal compared to being able to GC immediately
but it has the virtue of being (a) easily implemented, (b) with no
extra components such as a coordination protocol, and (c) better than
not GCing tombstones at all (the other easy way to ensure
correctness).",,,,,,,,,,,CASSANDRA-87,CASSANDRA-29,CASSANDRA-34,,,,,,,17/Apr/09 15:20;jbellis;0001-preserve-tombstones-until-a-GC-grace-period-has-elap.patch;https://issues.apache.org/jira/secure/attachment/12405773/0001-preserve-tombstones-until-a-GC-grace-period-has-elap.patch,17/Apr/09 15:20;jbellis;0002-omit-tombstones-from-column_t-and-supercolumn_t-retu.patch;https://issues.apache.org/jira/secure/attachment/12405774/0002-omit-tombstones-from-column_t-and-supercolumn_t-retu.patch,17/Apr/09 15:53;jbellis;0003-make-GC_GRACE_IN_SECONDS-customizable-in-storage.con.patch;https://issues.apache.org/jira/secure/attachment/12405779/0003-make-GC_GRACE_IN_SECONDS-customizable-in-storage.con.patch,17/Apr/09 19:10;jbellis;0004-and-5-v2.patch;https://issues.apache.org/jira/secure/attachment/12405798/0004-and-5-v2.patch,17/Apr/09 18:49;junrao;0004_expose_remove_bug.patch;https://issues.apache.org/jira/secure/attachment/12405793/0004_expose_remove_bug.patch,17/Apr/09 18:50;junrao;0005_fix_exposed_remove_bug.patch;https://issues.apache.org/jira/secure/attachment/12405795/0005_fix_exposed_remove_bug.patch,18/Apr/09 01:11;junrao;0006_fix_sequencefile_bug.patch;https://issues.apache.org/jira/secure/attachment/12405832/0006_fix_sequencefile_bug.patch,20/Apr/09 16:27;junrao;0007_fix_another_sequencefile_bug.patch;https://issues.apache.org/jira/secure/attachment/12405946/0007_fix_another_sequencefile_bug.patch,,,,,,8,,,,,,,,,,,,,,,,,,,49:18.5,,,no_permission,,,,,,,,,,,,19521,,,Mon Apr 20 16:39:34 UTC 2009,,,,,,0|i0fwh3:,90859,,,,,,,,,,,10/Apr/09 14:53;jbellis;Moving the tombstone removal up to the storageproxy level so read repair can see them first blocks on Avinash's multiget changes.,"17/Apr/09 15:24;jbellis;MultiGet doesn't seem to be coming any time soon.  Guess we'll just have to deal with conflict resolution when it does.

Re the patches provided, they follow the outline above except that it turns out we can just use a single removeDeleted to handle both tombstones (which still supress old data ""below"" them in the tree, e.g., a deleted supercolumn does not need to keep its subcolumn data around) and GC.  So CFS and compaction can still calls removeDeleted, and then CassandraServer just has to remove the tombstones themselves in thriftifyColumns and thriftifySuperColumns.

Patch to make GC_GRACE_IN_SECONDS configurable forthcoming.",17/Apr/09 15:53;jbellis;added configuration patch.,"17/Apr/09 18:49;junrao;Haven't looked at the patch in details, but found another related remove bug. Patch 0004 adds two test cases and one of them testRemoveColumn1() exposes the problem and will fail.
",17/Apr/09 18:50;junrao;Attach a patch that fixes the problem exposed in 0004 patch.,"17/Apr/09 18:52;junrao;Another issue is that if you look at testRemoveColumn1() and testRemoveColumn2(), retrieved.getColumn(""Column1"") returns null. I am wondering how that affects read repairs.

Also, you need to patch test/conf/storage-conf.xml for GC_GRACE_IN_SECONDS.
","17/Apr/09 18:59;jbellis;+1 for Jun's patches.

IMO test/conf is fine using the default value.",17/Apr/09 19:10;jbellis;fixed tests to make it more obvious what should be happening (in 4-and-5-v2).,"17/Apr/09 19:11;jbellis;we do need the ability to read-repair CF and SC tombstones.  I'll open a separate ticket for that.

the reason the behavior retrieved.getColumn(""Column1"") == null is correct is, if we've deleted the CF (more recently than the column!) then we don't care what data used to be there, it should just be gone.  what we need to RR is the CF tombstone.

of course if there is a column tombstone _more_ recent than the CF tombstone then it should be preserved.",17/Apr/09 19:19;urandom;+1 to 0001-0003 and 0004-and-5-v2.patch,17/Apr/09 20:09;jbellis;committed,"18/Apr/09 01:11;junrao;There is a serious bug in SequenceFille.java with the current fix. Attach a patch in 0006.

Unfortunately, none of the unit test captures the fact that many fields such as localDeletionTime, markedForDeleteAt, totalNumOfCols were read incorrectly. Existing test cases work just by luck.",18/Apr/09 01:11;junrao;reopens the issue because of the new bug found.,"18/Apr/09 01:33;jbellis;applied, thanks for catching that",18/Apr/09 01:42;jbellis;created CASSANDRA-89 to remind us to add a test covering these code paths,"20/Apr/09 16:27;junrao;Include patch for another bug in SequeceFile.java where the size of the row is not calculated correctly. At this moment, this bug is not exposed.since the rowkey and the size of row written to outBuf were simply read and discarded in SSTable.next().

We should probably open another issue to clean up the code such that the row key and row size are not written to outBuf.
",20/Apr/09 16:28;junrao;reopen the issue because of the new bug found.,"20/Apr/09 16:39;jbellis;committed.

> We should probably open another issue to clean up the code such that the row key and row size are not written to outBuf. 

it's not clear to me what the right cleanup is for this specific piece since the context is so messy. :(",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra silently loses data when a single row gets large (under ""heavy load"")",CASSANDRA-9,12419172,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,neophytos,neophytos,3/22/2009 11:35,3/12/2019 14:14,3/13/2019 22:24,4/15/2009 20:31,0.3,,,,0,,,,,,"When you insert a large number of columns in a single row, cassandra silently loses some or all of these inserts while flushing memtable to disk (potentialy leaving you with zero-sized data files). This happens when the memtable threshold is violated, i.e. when currentSize_ >= threshold_ (MemTableSizeInMB) OR  currentObjectCount_ >= thresholdCount_ (MemTableObjectCountInMillions). This was a problem with the old code in code.google and the code with the jdk7 dependencies also. No OutOfMemory errors are thrown, there is nothing relevant in the logs. It is not clear why this happens under heavy load (when no throttle is used) as it works fine when when you pace requests. I have confirmed this with another member of the community.


In storage-conf.xml:

   <HashingStrategy>RANDOM</HashingStrategy>
   <MemtableSizeInMB>32</MemtableSizeInMB>
   <MemtableObjectCountInMillions>1</MemtableObjectCountInMillions>
   <Tables>
      <Table Name=""MyTable"">
          <ColumnFamily ColumnType=""Super"" ColumnSort=""Name"" Name=""MySuper""></ColumnFamily>
      </Table>
  </Tables>

You can also test it with different values for thresholdCount_ In db/Memtable.java, say:
    private int thresholdCount_ = 512*1024;


Here is a small program that will help you reproduce this (hopefully):

    private static void doWrite() throws Throwable
    {
        int numRequests=0;
        int numRequestsPerSecond = 3;
        Table table = Table.open(""MyTable"");
        Random random = new Random();
        byte[] bytes = new byte[8];
        String key = ""MyKey"";
        int totalUsed = 0;
        int total = 0;
        for (int i = 0; i < 1500; ++i) {
            RowMutation rm = new RowMutation(""MyTable"", key);
            random.nextBytes(bytes);
            int[] used = new int[500*1024];
            for (int z=0; z<500*1024;z++) {
                used[z]=0;
            }
            int n = random.nextInt(16*1024);
            for ( int k = 0; k < n; ++k ) {
                int j = random.nextInt(500*1024);
                if ( used[j] == 0 ) {
                    used[j] = 1;
                    ++totalUsed;
                    //int w = random.nextInt(4);
                    int w = 0;
                    rm.add(""MySuper:SuperColumn-"" + j + "":Column-"" + i, bytes, w);
                }
            }
            rm.apply();
            total += n;
            System.out.println(""n=""+n+ "" total=""+ total+"" totalUsed=""+totalUsed);
            //Thread.sleep(1000*numRequests/numRequestsPerSecond);
            numRequests++;
        }
        System.out.println(""Write done"");
    }


PS. Please note that (a) I'm no java guru and (b) I have tried this initially with a C++ thrift client. The outcome is always the same: zero-sized data files under heavy load --- it works fine when you pace requests.","code in trunk, linux-2.6.27-gentoo-r1/, java version ""1.7.0-nio2"", 4GB, Intel Core 2 Duo",,,,,,,,,,CASSANDRA-76,CASSANDRA-59,,,,,,,,09/Apr/09 15:29;jbellis;0001-better-fix-for-9-v2.patch;https://issues.apache.org/jira/secure/attachment/12405077/0001-better-fix-for-9-v2.patch,06/Apr/09 21:45;jbellis;0001-better-fix-for-9.patch;https://issues.apache.org/jira/secure/attachment/12404773/0001-better-fix-for-9.patch,23/Mar/09 20:58;jbellis;executor.patch;https://issues.apache.org/jira/secure/attachment/12403456/executor.patch,23/Mar/09 23:36;neophytos;shutdown-before-flush-against-trunk.patch;https://issues.apache.org/jira/secure/attachment/12403467/shutdown-before-flush-against-trunk.patch,24/Mar/09 14:53;jbellis;shutdown-before-flush-v2.patch;https://issues.apache.org/jira/secure/attachment/12403525/shutdown-before-flush-v2.patch,24/Mar/09 16:03;jbellis;shutdown-before-flush-v3-trunk.patch;https://issues.apache.org/jira/secure/attachment/12403528/shutdown-before-flush-v3-trunk.patch,23/Mar/09 22:16;jbellis;shutdown-before-flush.patch;https://issues.apache.org/jira/secure/attachment/12403465/shutdown-before-flush.patch,,,,,,,7,,,,,,,,,,,,,,,,,,,44:50.7,,,no_permission,,,,,,,,,,,,19513,,,Wed Apr 15 20:31:11 UTC 2009,,,,,,0|i0fwc7:,90837,,,,,,,,,,,"22/Mar/09 15:44;junrao;Neo,

Is the problem specific to super CF or does it show up in regular CF too? Also, have you tried flushing smaller amount of data to disk. In cassandra, if you insert a row with key ""FlushKey"", it forces a flush on the CF referenced in the insertion.

Jun","22/Mar/09 16:25;neophytos;Hi Jun, I've done most of the tests using super CFs. Having said that I just tried it with a name-sorted regular CF and the outcome seems to be the same (zero-sized data files when no throttle is used). Please don't take my word for regular CFs and try it out. One of the reasons it took me so long to report this in public was that I was not sure if (a) it was a problem specific to the hardware I'm using or (b) misuse of Cassandra's constructs. 

For the case of super CFs, I did extensive testing with the code.google codebase and I did try lowering the thresholds (i.e. threshold_ and thresholdCount_ in Memtable.java). The result of that was that it would write some of the files fine while others were zero-sized. I've tried it by forcing flushes (FlushKey), no.","22/Mar/09 16:27;neophytos;Last line should read: I've not tried it by forcing flushes (FlushKey), no. ","23/Mar/09 20:58;jbellis;This is your old friend the ConcurrentModificationException, Neophytos.  Only the ThreadPoolExecutor is eating the exception.  Took me hours to figure out where the hell the exception was disappearing to...  Here's a patch that exposes it.  Not sure what the fix for the CME is yet but at least it's out in the open and reproducible. :)","23/Mar/09 21:53;jbellis;Okay, I see the problem.

Doing an insert (-> CFS.apply -> MT.put) checks for threshold violation; if it's okay, it schedules a Putter with the mutation on the Memtable.apartments thread pool.

If it is NOT okay, it schedules a flush -- _on another thread pool_ (MemtableManager's flusher).

So, what happens is, you have a bunch of Putter objects, each with a reference to the old Memtable, in the first threadpool, when the flush starts in the 2nd.  These Putters cause the CME when they get to resolve() while the flush is computing the column index.  (This is why it is easier to make this happen on large rows: index computation takes longer).

I think the easiest fix will be to make the apartments threadpool (executorservice) non-static, and just have one per memtable; then flush could wait for the service to finish before doing its thing.  Memory and thread churn will be nominal since memtable flush is so rare, relatively speaking.  Creating a new thread after flush is no big deal.

I'll get on the patch, just wanted to post an update in the meantime so nobody else needs to bang his head on this.","23/Mar/09 22:16;jbellis;Here is the patch, following my proposed fix above.  Works like a champ.",23/Mar/09 23:26;neophytos;Confirmed. Thank you Jonathan.,23/Mar/09 23:36;neophytos;Just a quick diff against code in trunk for your convenience. Please verify with Jonathan's patch before commit.,"24/Mar/09 14:53;jbellis;Here is a cleaner solution that adds the flush to the ExecutorService terminated() method which seems cleaner than having the flush itself (running in the Manager service) reach back to the Memtable service and block while waiting for shutdown.  (In a busy system we don't want to block the Manager service.)

Note that this also handles waiting for gets() to finish before flushing -- any logic purely in put() will not be able to do that, because get never checks threshold or acquires a lock.",24/Mar/09 15:50;avinash.lakshman@gmail.com;I am going to look at this once I get into work. I will apply/fix this today.,24/Mar/09 16:04;jbellis;v3 applies cleanly against trunk.,24/Mar/09 21:58;avinash.lakshman@gmail.com;The problem was identified by Jonathan Ellis. I have a fix checked in that requires a change only in the Memtable class. Neophytos has verified that my change actually works. But the credit goes to Jonathan for identifying the problem which was the harder part of this whole exercise. I am deeming this case as closed.,"24/Mar/09 22:12;jbellis;the problem with r758044 is it does not address GETs -- you can still have Getter ops on the the service added after the Flusher, so they will execute during / after the flush.  that is why I split the apartments_ into a per-memtable instance var and run the flush on terminate.  it's the cleanest way to be correct w/ gets w/o introducing explicit locks.",24/Mar/09 22:32;avinash.lakshman@gmail.com;Hmm. Should that matter. Gets do not modify the collection. I was under the impression that CME occurs when one thread tries to modify a collection when another is iterating over it. I will continue to look. Of course the whole apartment concept was introduced to eliminate locks.,"24/Mar/09 23:18;jbellis;Gets won't cause the CME, flush will. :)  calling columnFamily.clear(); as it goes can cause a CME as Get looks through it.  Of course even if it did not you would get back invalid results operating on a half-cleared-out Memtable.


In general it is just difficult to reason about concurrency when an object is being accessed from multiple threads at a time, even if it were okay today to ""cheat"" a bit, it will probably bite us down the road.",24/Mar/09 23:49;avinash.lakshman@gmail.com;Not sure if there can ever be a Getter on the queue after a Flusher has been enqueued. Once you are in the state where a Flusher() has been enqueued there can be no Getter() for the same Memtable. Anyways I will look into it tonight again.,"25/Mar/09 00:21;jbellis;> Not sure if there can ever be a Getter on the queue after a Flusher has been enqueued. Once you are in the state where a Flusher() has been enqueued there can be no Getter() for the same Memtable.

That is how easy it is to be fooled in these things -- that is what we want, but we are not enforcing that.

In particular note that the line

    		cf = apartments_.get(cfName_).submit(call).get();

is not atomic.

GET thread can execute apartments_.get(cfName_)

then PUT thread gets CPU (or executes concurrently on another core), switches memtable, and queues Flusher.

Thread A gets the CPU back and calls submit.  Getter is now on queue after Flusher.",25/Mar/09 00:26;avinash.lakshman@gmail.com;Ahh. I see. For some reason I was seeing from inside the apartment. This is no good. I will fix it tonight. ,"25/Mar/09 00:36;jbellis;Sorry, that's the right result but the wrong explanation.  (Son was howling at me -- very distracting.)

It is the getter creation / submit that is problematic, not the apartment get/submit.

that is,

{code}
thread A                                                              thread B
new Getter(key, cfName, filter);
                                                                             new Flusher(cLogCtx);
                                                                             submit(flusher);
submit(getter);
{code}","25/Mar/09 00:43;sandeep_tata;I think today, because of the way the code stands, you won't enqueue Getters on the table after you enqueue a flusher. But, I don't see how simply adding a flusher to the apartment's DebuggableThreadPool (instead of running the flusher in a separate thread) guarantees that there are no concurrent Putters/Getters still in the threadpool. Am I'm missing something? 

I agree that running flush in terminated() by overriding the method is the cleanest approach. You don't have to rely on the fact that the rest of the code (today) is such that you won't end up queuing a Getter after a Flusher (I'm guessing this is what Jonathan meant by ""cheat"" a bit today). This guarantee is precisely the reason ThreadPoolExecutor provides this hook.



","25/Mar/09 01:05;sandeep_tata;Ah, there's a whole bunch of worker threads talking to CFStore (and therefore memtable) -- I see how we can end up with Getters after adding a Flusher",25/Mar/09 02:44;avinash.lakshman@gmail.com;It is an issue that is actually a non-issue. In the worst case the Getter will return NULL since it read an empty memtable (maybe memtable got cleared). But that is fine because now the disk read will happen from buffer cache. It is not incorrect. No harm will be done.,"25/Mar/09 03:04;jbellis;Worst case, the flush clear() happens while the getter is iterating columns, and you get CME.",25/Mar/09 03:32;avinash.lakshman@gmail.com;Get rid of clear() :). It is a useless call anyway.,25/Mar/09 03:54;avinash.lakshman@gmail.com;Actually I take that back. There is no CME unless iterators are involved. But nevertheless the safest thing would be to not do the clear(). And I think everything will be good.,25/Mar/09 04:07;jbellis;I thought the point of clear was to free up memory as the flush progresses.  Isn't that worth a dozen more lines of code?,"25/Mar/09 11:15;jbellis;Ah, I see, we were talking about different clear(). Yes, the one from the end that you removed is always irrelevant (and not going to cause a CME).

It is the columnFamily.clear() in the middle that is still there that both frees up memory (\yes, of course by ""free up memory"" I mean ""make it available to be GC'd"") and can cause CME on the iterations that GET does.","26/Mar/09 20:50;junrao;Looking at the latest code. Both flush and put on a CF are submitted to the same ExecutorPool for that CF. Since the ExecutorPool has 1 thread in it, this means that the flushing of an old memtable will not run concurrently with the updates on a new memtable in the same CF. This limits concurrency.
 ",26/Mar/09 20:55;avinash.lakshman@gmail.com;The put thread does not run the flush. You submit to the put thread. It submits it to the manager service. Maybe I am missing something here?,"26/Mar/09 20:56;jbellis;Wrong.  The Flusher that goes on the Memtable executor is just a stub that kicks off the real flush in the MemtableManager's executor.

So when you have a Getter queued after that flusher, which can happen as I described above, the getter can get a CME while it is iterating through columns at the same time as the real flush calls cf.clear().","26/Mar/09 20:59;jbellis;(I was writing my comment at the same time as Avinash, so my ""Wrong"" was referring to Jun's assertion that ""the flushing of an old memtable will not run concurrently with the updates on a new memtable"".)",26/Mar/09 21:15;junrao;OK. I see it now. The real work of Flush is done in a separate thread. Sorry for the false alarm.,06/Apr/09 21:45;jbellis;Fixes potential CME with GETs.,09/Apr/09 15:04;jbellis;(Todd pointed out that having a per-Memtable executor is also more efficient by not needing to hash CF names to look up the executor.),09/Apr/09 15:29;jbellis;rebased to HEAD,"09/Apr/09 18:02;johanoskarsson;From my limited understanding of that code the latest patch gets a +1, looks clean. But I'd recommend that someone with a bit more experience look at it.","10/Apr/09 15:09;avinash.lakshman@gmail.com;I am just confused about one thing here. Why is there a chance of a CME on a get? I mean as far as I know a CME occurs when one thread is iterating (using an iterator) and another tries to modify the collection. That is not something that can happen here on a get, I think. So if that is the case there is no need for this change. Hash function cf name lookup is a non issue here.","10/Apr/09 15:16;jbellis;Right.  Get iterates over the columns, depending on the filter used.  Flush still clears out each CF as it is flushed:

                ssTable.append(partitioner.decorateKey(key), buffer);
                bf.add(key);
                columnFamily.clear();

This is behavior I want to keep since the overhead can be relatively high when column values are small.  And the new code is simpler to reason about since you only ever have one thread accessing things rather than executing gets during the flush.  (If we took the clear() out, we would be ok for now, but what if someone changes flush in six months?  One thread at a time is safer, especially vs _almost_ always one thread at a time which becomes easy to forget the exceptions.)",10/Apr/09 15:50;jbellis;Forgot to mention one more benefit to executor-per-memtable: this lets us easily call forceFlush in tests and then wait for the flush to finish before proceeding to do tests on the flushed sstable.  (That is why #59 blocks on this.),10/Apr/09 16:03;avinash.lakshman@gmail.com;Why would you want to wait to do tests? In the real world that is not what happens. You should be able to do reads even before the flush is complete. It should be seamless. Even a new memtable is served out the old one is maintained till the flush is complete. So this should really matter. If you just want to test the writes into SSTable then write into it and then test. I think this should not be a reason for the proposed change. May I missing something here.,"10/Apr/09 16:06;jbellis;It's a side benefit for the change, not a motivation.

Certainly testing a flush and making sure the resulting sstable has the same data that the memtable did is a good test to have.","14/Apr/09 00:33;tlipcon;Here's a review against the newest patch:

First, some style nits in Memtable.java:
  - runningExecutorServices member variable should have a trailing _ for style consistency
  - same with executor_

Regarding the actual contents of the patch, I sort of dislike subclassing the executor to do work on terminate, but it's the cleanest solution I can think of, so +1","14/Apr/09 05:03;jbellis;I will make the style changes; thanks for the review, Todd.

Any further discussion needed before commit?",15/Apr/09 20:31;jbellis;committed,,,,,,,,,,,,,,,,,
"hudson test failure: ""Forked Java VM exited abnormally.""",CASSANDRA-1834,12492681,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,urandom,urandom,12/7/2010 23:04,3/12/2019 14:13,3/13/2019 22:24,12/11/2010 16:12,0.7.0 rc 3,,,,0,,,,,,"https://hudson.apache.org/hudson/view/A-F/view/Cassandra/job/Cassandra-0.7/56/

{noformat}
    [junit] Testsuite: org.apache.cassandra.service.EmbeddedCassandraServiceTest
    [junit] Testsuite: org.apache.cassandra.service.EmbeddedCassandraServiceTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.service.EmbeddedCassandraServiceTest:BeforeFirstTest:	Caused an ERROR
    [junit] Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
    [junit] junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.service.EmbeddedCassandraServiceTest FAILED (crashed)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,59:18.2,,,no_permission,,,,,,,,,,,,20334,,,Sat Dec 11 16:12:28 UTC 2010,,,,,,0|i0g7p3:,92677,,,,,,,,,,,"07/Dec/10 23:40;urandom;a bisect suggests it was introduced here: https://svn.apache.org/viewvc?view=revision&revision=1041951

""reads at ConsistencyLevel > 1 throwUnavailableException immediately if insufficient live nodes exist
patch by jbellis and tjake for CASSANDRA-1803""","08/Dec/10 00:59;jbellis;usually when i've seen ""vm exited abnormally"" there is something in the log or on stdout, can we get the subprocess stdout from hudson?","08/Dec/10 01:36;urandom;Is this not it? https://hudson.apache.org/hudson/view/A-F/view/Cassandra/job/Cassandra-0.7/56/console

That's everything that I see when it fails on me locally.","08/Dec/10 01:43;jbellis;that's the console of the junit jvm, not the console of the jvm it forked to run the test","08/Dec/10 04:27;urandom;{noformat}
org.apache.cassandra.config.ConfigurationException: Found system table files, but they couldn't be loaded. Did you change the partitioner?
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:236)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:105)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:183)
        at org.apache.cassandra.service.EmbeddedCassandraService.init(EmbeddedCassandraService.java:72)
        at org.apache.cassandra.service.EmbeddedCassandraServiceTest.setup(EmbeddedCassandraServiceTest.java:78)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
        at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:785
{noformat}",08/Dec/10 05:54;jbellis;I bet having ECST extend CleanupHelper will clear that up,"08/Dec/10 06:56;tjake;When this happens to me I normally do remove the build/test dir to resolve (some kind of corruption)
","11/Dec/10 16:12;jbellis;bq. I bet having ECST extend CleanupHelper will clear that up 

done.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hinted handoff null pointer exception,CASSANDRA-585,12441733,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,11/26/2009 2:37,3/12/2019 14:13,3/13/2019 22:24,11/26/2009 14:58,,,,,0,,,,,,"During the course of running the cluster I have now run into this error

2009-11-26_02:28:08.99076 ERROR - Error in executor futuretask
2009-11-26_02:28:08.99076 java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.NullPointerException
2009-11-26_02:28:08.99076       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2009-11-26_02:28:08.99076       at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2009-11-26_02:28:08.99076       at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:112)
2009-11-26_02:28:08.99076       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
2009-11-26_02:28:08.99076       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-26_02:28:08.99076       at java.lang.Thread.run(Thread.java:636)
2009-11-26_02:28:08.99076 Caused by: java.lang.RuntimeException: java.lang.NullPointerException
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager$3.run(HintedHandOffManager.java:281)
2009-11-26_02:28:08.99076       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
2009-11-26_02:28:08.99076       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2009-11-26_02:28:08.99076       at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2009-11-26_02:28:08.99076       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-26_02:28:08.99076       ... 2 more
2009-11-26_02:28:08.99076 Caused by: java.lang.NullPointerException
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.RowMutation.add(RowMutation.java:119)
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:115)
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:219)
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:75)
2009-11-26_02:28:08.99076       at org.apache.cassandra.db.HintedHandOffManager$3.run(HintedHandOffManager.java:277)
2009-11-26_02:28:08.99076       ... 6 more

","r884373
3 node cluster",,,,,,,,,,,,,,,,,,,26/Nov/09 03:48;jbellis;585.patch;https://issues.apache.org/jira/secure/attachment/12426181/585.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,48:19.6,,,no_permission,,,,,,,,,,,,19766,,,Thu Nov 26 14:58:39 UTC 2009,,,,,,0|i0fzu7:,91404,,,,,,,,,,,26/Nov/09 03:48;jbellis;this should fix it,"26/Nov/09 05:47;dispalt;seems to work well, no exceptions yet...",26/Nov/09 14:58;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read Repair throws UnknownHostException,CASSANDRA-529,12439984,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sammy.yu,sammy.yu,sammy.yu,11/5/2009 23:16,3/12/2019 14:13,3/13/2019 22:24,11/9/2009 17:26,0.5,,,,0,,,,,,"Read repair appears to be failing due to the facet that InetAddress.toString() is used into the repair call's key which will be passing ""hostname/ipaddress"" into InetAddress.getByName()

INFO [CACHETABLE-TIMER-4] 2009-11-05 14:41:38,786 Cachetable.java (line 107) Exception was generated at : 11/05/2009 14:41:38 on thread CACHETABLE-TIMER-4
java.net.UnknownHostException: /10.2.131.90
java.lang.RuntimeException: java.net.UnknownHostException: /10.2.131.90
        at org.apache.cassandra.service.ReadRepairManager$ReadRepairPerformer.callMe(ReadRepairManager.java:75)
        at org.apache.cassandra.service.ReadRepairManager$ReadRepairPerformer.callMe(ReadRepairManager.java:58)
        at org.apache.cassandra.utils.Cachetable$CacheMonitor.run(Cachetable.java:102)
        at java.util.TimerThread.mainLoop(Timer.java:512)
        at java.util.TimerThread.run(Timer.java:462)
Caused by: java.net.UnknownHostException: /10.2.131.90
        at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
        at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:849)
        at java.net.InetAddress.getAddressFromNameService(InetAddress.java:1200)
        at java.net.InetAddress.getAllByName0(InetAddress.java:1153)
        at java.net.InetAddress.getAllByName(InetAddress.java:1083)
        at java.net.InetAddress.getAllByName(InetAddress.java:1019)
        at java.net.InetAddress.getByName(InetAddress.java:969)
        at org.apache.cassandra.service.ReadRepairManager$ReadRepairPerformer.callMe(ReadRepairManager.java:71)
        ... 4 more",,,,,,,,,,,,,,,,,,,,06/Nov/09 00:21;sammy.yu;0001--CASSANDRA-529.patch;https://issues.apache.org/jira/secure/attachment/12424170/0001--CASSANDRA-529.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,24:55.2,,,no_permission,,,,,,,,,,,,19739,,,Tue Nov 10 12:33:51 UTC 2009,,,,,,0|i0fzhr:,91348,,,,,,,,,,,"06/Nov/09 00:21;sammy.yu;Use getHostAddress instead of toString()
",06/Nov/09 00:24;lenn0x;+1,"06/Nov/09 00:25;jbellis;+1

goffinet to commit?",09/Nov/09 17:26;lenn0x;committed,"10/Nov/09 12:33;hudson;Integrated in Cassandra #254 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/254/])
    Use getHostAddress instead of toString(), Read Repair was throwing an Exception. Patch by sammyyu, reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception after about 2000 inserts,CASSANDRA-151,12424834,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,nk11,nk11,5/7/2009 17:13,3/12/2019 14:13,3/13/2019 22:24,5/11/2009 14:54,,,,,0,,,,,,"Wtih this client code and default configuration:

int max = 100000;
Random random = new Random();
for (int a = 0; a < max; a  ) {
     System.out.println(a);
     client.insert(""Table1"", ""k1:""   random.nextInt(Integer.MAX_VALUE), ""Super1:x"", new byte[] { (byte) 1 }, 0);
}

I get after about 2000 inserts

DEBUG [pool-1-thread-1] 2009-05-07 20:04:30,942 StorageProxy.java (line 120) insert writing key k1:1355213513 to [127.0.0.1:7000]
ERROR [ROW-MUTATION-STAGE:4] 2009-05-07 20:04:30,942 RowMutationVerbHandler.java (line 99) Error in row mutation
java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.db.SuperColumnSerializer.fillSuperColumn(SuperColumn.java:368)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:349)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:314)
	at org.apache.cassandra.db.ColumnFamily$ColumnFamilySerializer.deserialize(ColumnFamily.java:515)
	at org.apache.cassandra.db.ColumnFamily$ColumnFamilySerializer.deserialize(ColumnFamily.java:455)
	at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:374)
	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:384)
	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:337)
	at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:69)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:46)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
",,,,,,,,,,,,,,,,,,,,07/May/09 20:06;jbellis;151.patch;https://issues.apache.org/jira/secure/attachment/12407571/151.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:21.5,,,no_permission,,,,,,,,,,,,19570,,,Wed May 13 09:26:39 UTC 2009,,,,,,0|i0fx6n:,90974,,,,,,,,,,,"07/May/09 17:17;nk11;correction, I get it after much less, but at around 2000 inserts it freezes","07/May/09 17:27;jbellis;you need to specify three values for the column ""path"" in a supercolumn insert -- ColumnFamily:SuperColumn:subcolumn.  you are only specifying two (Super1:x).

I will add a check for this on the server.","07/May/09 18:25;nk11;right... that was the exception, my fault.
the freeze is still there after the fix unfortunately. no exception in the logs this time","07/May/09 19:07;jbellis;the freeze is probably memory pressure like I said in IRC.  reduce your memtable settings to flush more often, or increase -Xmx in bin/cassandra.in. 

but first you should upgrade, i noticed your client code was out of date.","07/May/09 19:30;nk11;you were right, lowering the MemtableSizeInMB did it.",07/May/09 20:06;jbellis;more robust parameter checking for get_column,"07/May/09 22:52;junrao;Patch looks good to me.
",11/May/09 14:54;jbellis;committed,"13/May/09 09:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
storage-conf.xml reformatting,CASSANDRA-373,12433363,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,8/18/2009 18:34,3/12/2019 14:13,3/13/2019 22:24,8/20/2009 19:48,0.4,,,,0,,,,,,"Our sample config (conf/storage-conf.xml) is the canonical source of configuration documentation. As such readability should be a priority, and it should serve as the best possible basis for customization.

I propose the following:

1. Wherever possible, lines should wrap at 75 chars. The file will be edited post-installation by operational personnel, who are often confined to standard 80 character terminals.

2. Indention of 2 spaces to make maximum use of horizontal real estate.

3. More distinctive multi-line comments.

4. Path locations that conform to the FHS.

Patch to follow...",,,,,,,,,,,,,,,,,,,,18/Aug/09 18:42;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt;https://issues.apache.org/jira/secure/attachment/12416901/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt,18/Aug/09 18:47;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt;https://issues.apache.org/jira/secure/attachment/12416902/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt,19/Aug/09 21:56;urandom;ASF.LICENSE.NOT.GRANTED--v3-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt;https://issues.apache.org/jira/secure/attachment/12417060/ASF.LICENSE.NOT.GRANTED--v3-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt,20/Aug/09 03:07;urandom;ASF.LICENSE.NOT.GRANTED--v4-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt;https://issues.apache.org/jira/secure/attachment/12417093/ASF.LICENSE.NOT.GRANTED--v4-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,57:58.6,,,no_permission,,,,,,,,,,,,19658,,,Fri Aug 21 12:34:39 UTC 2009,,,,,,0|i0fyjj:,91194,,,,,,,,,,,"18/Aug/09 18:48;urandom;whoops, v1 had an error, see v2 instead.","18/Aug/09 19:57;euphoria;In general, looks good.  While reviewing, noticed an existing internal conflict between microseconds and milliseconds for CommitLogSyncBatchWindowInMS that should be corrected.","19/Aug/09 20:54;dehora;{quote}
1. Wherever possible, lines should wrap at 75 chars.
{quote}

is this not best done by reducing the comment noise? 

Also verbose comments tend to be annoying in practice operationally - hey get in the way of the the actual elements  you need to set/read when you need to see/read them on a console - tomcat is an example offender and a system i'm tired of stripping comments from.

{quote}
4. Path locations that conform to the FHS.
{quote}

Not for us to mandate surely.


 * The tilde's at the beginning of the comment lines impair reading.  

* I don't like putting advice or good rules of thumb in the config - would we do this kind of thing in the thrift? 

 * CommitLogSyncDelay : ""in millis"" but other elements have the InFoo quantity pattern. Inconsistent.

 * MemtableObjectCountInMillions: asking people to type in floats is error prone, why not MemtableObjectCount?

* CommitLogSync: doesn't document its legal values

* Thrift*: should be wrapped in an holding element. Because, and I'm guessing here, Thrift will not remain the only, or even the default, transport/api.

* InitialToken: doesn't document what happens when empty

* Seed, ListenAddress, ThriftAddress: don't document whether they take Ipv6 addresses

","19/Aug/09 20:59;jbellis;> 1. Wherever possible, lines should wrap at 75 chars.

Who seriously uses a vt100 rather than remoting in these days?  You can always use a larger terminal and/or an editor that handles wrapped lines sanely.
","19/Aug/09 21:06;jbellis;> verbose comments tend to be annoying in practice operationally

That's the lesser of evils compared to out-of-date documentation of the options.","19/Aug/09 21:14;dehora;{quote}
That's the lesser of evils compared to out-of-date documentation of the options. 
{quote}

Agreed!","19/Aug/09 21:31;urandom;>Who seriously uses a vt100 rather than remoting in these days? You can always use a larger terminal and/or an editor that handles wrapped lines sanely.

You have to wrap somewhere; the point of 80 chars is that it is the minimum you could expect to see in use.

This is pretty much best practice among projects with hand edited configuration files. A quick survey of the files under /etc on any *nix box should confirm this.","19/Aug/09 21:50;euphoria;There are a few errors in the CommitLog area that Eric and I discussed on IRC that will need to be fixed (including the legal values Bill mentions).

The path locations are just examples -- we have to choose something to default to, and the best thing to default to would be a common standard.  Users are still, of course, able to specify whatever writable paths they desire.",19/Aug/09 21:57;urandom;v3-0001-CASSANDRA-373-re-format-factor-conf-storage-conf.xml.txt brings the file up-to-date with the commitlog options in trunk.,"19/Aug/09 22:02;urandom;>  The tilde's at the beginning of the comment lines impair reading.

Ok. The intention was to make it more readable by marking the left-most margin of the comment block. I did this with a tilde only because that was already in use for the copyright header, and I thought that looked ok. Suggestions welcome.

>  * CommitLogSyncDelay : ""in millis"" but other elements have the InFoo quantity pattern. Inconsistent.
> 
>  * MemtableObjectCountInMillions: asking people to type in floats is error prone, why not MemtableObjectCount?
> 
> * CommitLogSync: doesn't document its legal values
> 
> * Thrift*: should be wrapped in an holding element. Because, and I'm guessing here, Thrift will not remain the only, or even the default, transport/api.
> 
> * InitialToken: doesn't document what happens when empty
> 
> * Seed, ListenAddress, ThriftAddress: don't document whether they take Ipv6 addresses 

I had intended the scope of this ticket to be formatting/styling. Could you open a separate ticket for these?","19/Aug/09 23:39;jbellis;Can we at least wrap things symmetrically?

      <ColumnFamily ColumnType=""Super"" 
                                    CompareWith=""UTF8Type""
                                    CompareSubcolumnsWith=""UTF8Type""
                                    Name=""Super1""/>

instead of

+      <ColumnFamily ColumnType=""Super"" CompareWith=""UTF8Type""
+              CompareSubcolumnsWith=""UTF8Type"" Name=""Super1""/>
","20/Aug/09 03:09;urandom;> Can we at least wrap things symmetrically?

Sure. See v4 patch.
",20/Aug/09 03:19;jbellis;+1,20/Aug/09 03:26;euphoria;Still says 'microseconds' but +1 if microseconds is changed to milliseconds for the commit log batch delay,20/Aug/09 19:48;urandom;committed.,21/Aug/09 00:07;dehora;Please revert this and remove the verbose comments and the leading tildes. All I'm going to do with them is strip them out.,"21/Aug/09 00:25;jbellis;> remove the verbose comments

not going to happen.  see above re lesser of evils.

> and the leading tildes

if you have a better suggestion, you are welcome to submit a patch -- Eric said above that he was open to suggestions to improve this but none were forthcoming.

> All I'm going to do with them is strip them out. 

That's fine, but because you don't need them doesn't mean they are not useful for others.","21/Aug/09 12:34;hudson;Integrated in Cassandra #174 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/174/])
     re{format,factor} conf/storage-conf.xml
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"get_slice ignores the ""start"" parameter",CASSANDRA-81,12422833,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,4/14/2009 22:54,3/12/2019 14:13,3/13/2019 22:24,4/22/2009 22:54,0.3,,,,0,,,,,,"get_slice(string tablename, string key, string columnFamily_column, i32 start, i32 count) is expected is return all columns starting at offset ""start"" subject to a maximum of ""count"" columns. The current code does not do this.
Example interaction:

./Cassandra-remote insert 'Table1' 'key' 'DATA:c1' 'val1' 1
None
./Cassandra-remote insert 'Table1' 'key' 'DATA:c2' 'val2' 1
None
./Cassandra-remote insert 'Table1' 'key' 'DATA:c3' 'val3' 1
None

./Cassandra-remote get_slice 'Table1' 'key' 'DATA'  0 2

[ {'columnName': 'c1', 'value': 'val1', 'timestamp': 1},
  {'columnName': 'c2', 'value': 'val2', 'timestamp': 1}]

./Cassandra-remote get_slice 'Table1' 'key' 'DATA'  1 2

[ {'columnName': 'c1', 'value': 'val1', 'timestamp': 1},
  {'columnName': 'c2', 'value': 'val2', 'timestamp': 1}]]  <<---- Same as prev! ""start"" ignored

./Cassandra-remote  get_slice 'Table1' 'key' 'DATA'  0 1
[{'columnName': 'c1', 'value': 'val1', 'timestamp': 1}]

./Cassandra-remote get_slice 'Table1' 'key' 'DATA'  2 1
[{'columnName': 'c1', 'value': 'val1', 'timestamp': 1}]    <<---- Same as prev! ""start"" ignored




",all,,,,,,,,,,,,,,,,,,,16/Apr/09 18:46;jbellis;81-v5.patch;https://issues.apache.org/jira/secure/attachment/12405681/81-v5.patch,20/Apr/09 23:57;sandeep_tata;CASSANDRA-81-v6.patch;https://issues.apache.org/jira/secure/attachment/12405969/CASSANDRA-81-v6.patch,22/Apr/09 22:47;sandeep_tata;CASSANDRA-81-v7.patch;https://issues.apache.org/jira/secure/attachment/12406176/CASSANDRA-81-v7.patch,15/Apr/09 03:17;sandeep_tata;fix_for_get_slice.patch;https://issues.apache.org/jira/secure/attachment/12405487/fix_for_get_slice.patch,15/Apr/09 18:21;sandeep_tata;get_slice_fix_and_unit_tests_v2.patch;https://issues.apache.org/jira/secure/attachment/12405553/get_slice_fix_and_unit_tests_v2.patch,15/Apr/09 23:19;sandeep_tata;get_slice_fix_and_unit_tests_v3.patch;https://issues.apache.org/jira/secure/attachment/12405595/get_slice_fix_and_unit_tests_v3.patch,16/Apr/09 16:49;sandeep_tata;get_slice_fix_and_unit_tests_v4.patch;https://issues.apache.org/jira/secure/attachment/12405665/get_slice_fix_and_unit_tests_v4.patch,15/Apr/09 03:08;sandeep_tata;unit_tests_for_get_slice.patch;https://issues.apache.org/jira/secure/attachment/12405485/unit_tests_for_get_slice.patch,,,,,,8,,,,,,,,,,,,,,,,,,,15:40.7,,,no_permission,,,,,,,,,,,,19538,,,Wed Apr 22 22:54:18 UTC 2009,,,,,,0|i0fwrb:,90905,,,,,,,,,,,"15/Apr/09 03:04;sandeep_tata;There's an another, more subtle problem with this bug. Even with start=0, the columns returned are not guaranteed to be in order.

Suppose that you:

write col2
write col3

flush -- the memtable is now empty, the ssTable contains col2 and col3
write col1
write col4
write col5

get_slice(table, colfam, 0, 3):

You should get col1, col2, col3 --> not col1, col4, col5 (from current memtable alone)
","15/Apr/09 03:08;sandeep_tata;Added unit tests that produce errors for get_slice when
a) called with nonzero start
b) when data needs to be fetched from ssTables and Memtable","15/Apr/09 03:17;sandeep_tata;Basic idea:

Instead of passing in a CountFilter, the getRow call resolves the entire row, drops the first ""offset"" cols, picks the next ""count"" cols and returns. For the given semantics, you can't get around resolving the full row.

",15/Apr/09 03:41;sandeep_tata;The current patch only tests this with columns -- we'll need some tests for super columns next.,"15/Apr/09 16:15;jbellis;functionality patch looks ok.  (but watch bracing -- }else should be two lines to be consistent.  I can fix that up on apply though in this case.)

some comments on the tests --

1. does testGetRowSingleColumn add anything, coverage-wise?  If not, I don't want to maintain it. :)

2. don't catch test exceptions, let them get raised.  that's a lot more informative than assertTrue(False).

3. table.clearSnapshot() is a no-op here, so leave that out.  (if you inherit from ServerTest that will do a slightly better job of cleanup but the code relies too much on static structures for us to do it ""right.""  so don't worry too much about that.)
","15/Apr/09 18:21;sandeep_tata;Thanks for the review. Attached a revised version.

>1. does testGetRowSingleColumn add anything, coverage-wise? If not, I don't want to maintain it. :) 
It isn't much, but It does test one of the simpler getRow calls in Table. 

2. don't catch test exceptions, let them get raised. that's a lot more informative than assertTrue(False). 
Fixed.

>3. table.clearSnapshot() is a no-op here, so leave that out. (if you inherit from ServerTest that will do a slightly better job of cleanup but the code relies too much on static structures for us to do it ""right."" so don't worry too much about that.) 
Removed. I like the idea of inheriting ServerTest -- I modified TableTest to do that. Everything still passes :-)","15/Apr/09 20:46;jbellis;oh, one more thing -- flush is asynchronous, so you'll want to call waitForFlush like the CFS tests.","15/Apr/09 23:19;sandeep_tata;Changes with v3:

1. Added supercolumn tests
2. Moved the filtering code into CountFilter and called it from Table
3. Added call to waitForFlush ","16/Apr/09 02:22;jbellis;hmm, patch does not apply cleanly against trunk for me.  which is weird since CountFilter hasn't been touched in a while.  Can you try regenerating after svn up just to be sure?",16/Apr/09 16:49;sandeep_tata;Regenerated v4 after an svn up -- there were some changes to ColumnFamilyStoreTest that I hadn't updated to.,"16/Apr/09 18:46;jbellis;I cleaned up a couple things in CountFilter.

1. use this() for constructor overloading instead of pasting code

2. values.length == 1 is _not_ the same as !isSuper().  You could be slicing supercolumns of a super CF, or you could be slicing normal columns of a standard CF.  (This is an easy mistake to make because the API is designed poorly.  But fixing that is a job for 0.4 I think.)

testGetRowSuperColumnOffsetCount errors out b/c of cleanup problems.  When I applied it after CASSANDRA-85 (which improves cleanup considerably), the test fails.  (Possibly my fault for sloppy conflict resolution.)

I suggest we apply 85 first to at least get to a reproducible state in the test.","16/Apr/09 20:13;sandeep_tata;Makes sense. I'll finish reviewing 85, and then redo this patch.","20/Apr/09 23:57;sandeep_tata;Remade patch after #85.


> 2. values.length == 1 is _not_ the same as !isSuper(). You could be slicing supercolumns of a super CF, or you could be slicing normal columns of a standard CF. (This is an easy mistake to make because the API is designed poorly. But fixing that is a job for 0.4 I think.) 

Good catch. Fixed. 

I don't like that getColumnCount returns 1+number of subcolumns for supercolumns. That is, cf.getAllColumns.size() is not equal to cf.getColumnCount. If at some point we decide to change this, we'll have to fix these unit tests.","22/Apr/09 22:32;jbellis;                if (count == Integer.MAX_VALUE && start == 0) //Don't need to filter

this test needs to be modified a bit since the defaults are -1, -1.  I scanned the rest of the code and I think the other tests on count and start are ok, can you doublecheck that?

                    IFilter filter = new CountFilter(count, start);
                    filteredCf = filter.filter(cf, columnFamily);

style nit: can we inline the filter creation?
","22/Apr/09 22:47;sandeep_tata;1. fixed test.
2. inlined filter :-)",22/Apr/09 22:54;jbellis;applied,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range scan over two nodes returns wrong data,CASSANDRA-348,12432373,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,markr,markr,8/6/2009 10:35,3/12/2019 14:13,3/13/2019 22:24,8/12/2009 21:27,0.4,,,,0,,,,,,"I've got two nodes with tokens 00000000 and 88888888. I add 16 rows in which are spread over them, then do a key range scan.

I can scan part of the range successfully, but if I try to scan the entire range of keys (0-f) then I get unexpected results

./Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 0 31 1000
['00', '01', '10', '11', '20', '21', '30', '31']

./Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 3 81 1000
['30', '31', '40', '41', '50', '51', '60', '61', '70', '71', '80', '81']

 ./Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 7 b1 1000
['70', '71', '80', '81', '90', '91', 'a0', 'a1', 'b0', 'b1']

./Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 a g 1000
['a0', 'a1', 'b0', 'b1', 'c0', 'c1', 'd0', 'd1', 'e0', 'e1', 'f0', 'f1']

All of which returned as I expected.

But when I range scan the whole lot (0-g) then I get:

./Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 0 g 1000
[ '00',
  '90',
  '91',
  'a0',
  'a1',
  'b0',
  'b1',
  'c0',
  'c1',
  'd0',
  'd1',
  'e0',
  'e1',
  'f0',
  'f1']

Where have 01-81 gone?

I'll attach the data loading script.",,,,,,,,,,,,,,,,,,,,07/Aug/09 20:28;markr;348-2-fixup-2.patch;https://issues.apache.org/jira/secure/attachment/12415886/348-2-fixup-2.patch,07/Aug/09 20:16;markr;348-2-fixup.patch;https://issues.apache.org/jira/secure/attachment/12415885/348-2-fixup.patch,07/Aug/09 16:52;jbellis;348-2.patch;https://issues.apache.org/jira/secure/attachment/12415866/348-2.patch,12/Aug/09 20:59;jbellis;348-3-v2.patch;https://issues.apache.org/jira/secure/attachment/12416357/348-3-v2.patch,12/Aug/09 17:51;jbellis;348-3.patch;https://issues.apache.org/jira/secure/attachment/12416343/348-3.patch,06/Aug/09 13:26;jbellis;348.diff;https://issues.apache.org/jira/secure/attachment/12415744/348.diff,07/Aug/09 09:03;markr;LoadAndScan.py;https://issues.apache.org/jira/secure/attachment/12415832/LoadAndScan.py,06/Aug/09 10:37;markr;setup.cas;https://issues.apache.org/jira/secure/attachment/12415723/setup.cas,,,,,,8,,,,,,,,,,,,,,,,,,,26:24.4,,,no_permission,,,,,,,,,,,,19645,,,Thu Aug 13 13:13:49 UTC 2009,,,,,,0|i0fydz:,91169,,,,,,,,,,,"06/Aug/09 10:37;markr;This is a cassandra-cli script used to load the test data which gets the results above.

","06/Aug/09 10:39;markr;Relevant config:

node1:

    <Partitioner>org.apache.cassandra.dht.OrderPreservingPartitioner</Partitioner>
    <InitialToken>00000000</InitialToken>

node2:

    <Partitioner>org.apache.cassandra.dht.OrderPreservingPartitioner</Partitioner>
    <InitialToken>88888888</InitialToken>

Most of the rest is as shipped.",06/Aug/09 13:26;jbellis;this patch fixes a minor bug (probably not the cause of your problems) and adds debug logging.  can you try with this patch and post the debug statements involving RangeCommand and RangeReply?,"06/Aug/09 13:45;markr;I've applied the patch and the bug is still there, here is the debug output:

NODE 1 debug output:

DEBUG - get_key_range
DEBUG - reading RangeCommand(table='Keyspace1', columnFamily=Standard1, startWith='0', stopAt='g', maxResults=100) from 58@127.0.0.1:7000
DEBUG - Sending RangeReply(keys=[00, 90, 91, a0, a1, b0, b1, c0, c1, d0, d1, e0, e1, f0, f1], completed=false) to 58@127.0.0.1:7000
DEBUG - Processing response on an async result from 58@127.0.0.1:7000
DEBUG - reading RangeCommand(table='Keyspace1', columnFamily=Standard1, startWith='f1', stopAt='g', maxResults=85) from 59@127.0.0.2:7000
DEBUG - Processing response on an async result from 59@127.0.0.2:7000

NODE 2 debug output:

DEBUG - Sending RangeReply(keys=[], completed=false) to 59@127.0.0.1:7000

bin/nodeprobe -host localhost ring
DEBUG - Loading settings from bin/../conf/storage-conf.xml
Token(00000000)                                 1 127.0.0.2      |<--|
Token(88888888)                                 1 127.0.0.1      |-->|
","06/Aug/09 14:09;jbellis;remember these are string keys, not really numeric.  '0' is not part of the ['00000000' , '88888888' ) range.  (neither is the key '00' of course.)

i bet you get all the keys if you query for '', 'g' instead of '0', 'g'.","06/Aug/09 15:00;markr;I am aware that the keys are strings :)

Keys should presumably not HAVE to be within the range of two tokens in the ring - keys outside the range will be stored anyway?

I tried the above, same result:

cassandra/Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 '' g 100
[ '00',
  '90',
  '91',
  'a0',
  'a1',
  'b0',
  'b1',
  'c0',
  'c1',
  'd0',
  'd1',
  'e0',
  'e1',
  'f0',
  'f1']
","06/Aug/09 15:07;markr;Even more weird:

 cassandra/Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 00000000 g 100
[ '90',
  '91',
  'a0',
  'a1',
  'b0',
  'b1',
  'c0',
  'c1',
  'd0',
  'd1',
  'e0',
  'e1',
  'f0',
  'f1']

Now it misses everything from 0 to 81","06/Aug/09 15:10;markr;cassandra/Cassandra-remote -h localhost:9160 get_key_range Keyspace1 Standard1 '' g 100
[ '00',
  '90',
..
  'f1']

Debug logs:

NODE 1:

DEBUG - get_key_range
DEBUG - reading RangeCommand(table='Keyspace1', columnFamily=Standard1, startWith='', stopAt='g', maxResults=100) from 2208@127.0.0.1:7000
DEBUG - Sending RangeReply(keys=[00, 90, 91, a0, a1, b0, b1, c0, c1, d0, d1, e0, e1, f0, f1], completed=false) to 2208@127.0.0.1:7000
DEBUG - Processing response on an async result from 2208@127.0.0.1:7000
DEBUG - reading RangeCommand(table='Keyspace1', columnFamily=Standard1, startWith='f1', stopAt='g', maxResults=85) from 2209@127.0.0.2:7000
DEBUG - Processing response on an async result from 2209@127.0.0.2:7000

NODE 2:
DEBUG - Sending RangeReply(keys=[], completed=false) to 2209@127.0.0.1:7000
","06/Aug/09 15:34;jbellis;looks like a bug in the node selection code.

i'll commit the bugfix+logging patch for now.",06/Aug/09 15:41;junrao;The node selection code seems to be designed only for RackUnaware.,"06/Aug/09 16:34;jbellis;Node selection code is working as designed but it is not quite what getKeyRange expects.

The node selection is ""pick the node whose token is nearest to the decorated key, _always rounding up_.""  so what you end up with here is 3 range sections:

["""", 000000000] node A (the one with token 00000000)
(00000000, 88888888] node B (the one with token 88888888)
(88888888, infinity) node A again

so, key 00 goes on node A, but 01-88 go on node B.  then 09-ff go on node A again.

we could hack around this in getKeyRange but it seems like the Right Fix is to make it so A has ['', 88888888) and B has [88888888, inf), no?

what do you think, Jun?  is there any inherent advantage to ""round up"" instead of ""round down"" that I have forgotten?

[yeah, we're ignoring RackAware for now]","06/Aug/09 17:24;junrao;Interesting. It seems the problem is that you started with a key that's in the middle btw 2 adjacent tokens and you need to go back to the very first node to complete the full scan. The current code seems to stop as soon as you hit the first node again. It seems that this will happen whether you roundup or rounddown. So, maybe we should let the first node be scanned twice, one at the beginning and another at the end.
","06/Aug/09 19:27;jbellis;You are right, we're going to have this problem either way we round the keys to tokens.  Take this example, I was wrong about how the tokens would work, it would be

[00000000, 88888888) A
[88888888, inf) and ['', 00000000) B

so either way starting from '' you're going to have to re-scan part of the same range when you wrap.","07/Aug/09 09:03;markr;I have attached a python script LoadAndScan.py which uses the thrift interface to load a bunch of test data then do lots of range scans to check the results are right.

This can be made into an automated system test, you are free to use it.","07/Aug/09 09:04;markr;The LoadAndScan.py script succeeds when there is a single node, and various cases fail when there are more nodes with tokens which overlap the range 0000-ffff 

I have tried it with 1,2 and 4 nodes, the more nodes the more failure cases.","07/Aug/09 12:49;hudson;Integrated in Cassandra #160 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/160/])
    fix range query buglet; add debug logging
patch by jbellis; tested by Mark Robson for 
","07/Aug/09 16:52;jbellis;This patch fixes the main problem.  There are two things going on in this patch:

 - we switch from trying to get the next endpoint by increasing offset to asking tokenMetadata for ""the next one.""  this will always be correct where the offset approach will not (usually you want offset to just be 1, but sometimes you have to keep increasing it if no results are found but the range is still not finished)
 - we merge results differently when the endpoint responsible for where the ring wraps is involved, since that endpoint can hold keys from both the beginning and end of the range.","07/Aug/09 19:37;markr;With 348-2.patch I get

DEBUG - get_key_range
DEBUG - reading RangeCommand(table='Keyspace1', columnFamily=Standard1, startWith='0', stopAt='1', maxResults=1000) from 593@127.0.0.1:7000
DEBUG - Sending RangeReply(keys=[0000], completed=false) to 593@127.0.0.1:7000
DEBUG - Processing response on an async result from 593@127.0.0.1:7000
DEBUG - reading RangeCommand(table='Keyspace1', columnFamily=Standard1, startWith='0', stopAt='1', maxResults=999) from 594@127.0.0.2:7000
DEBUG - Processing response on an async result from 594@127.0.0.2:7000
ERROR - Internal error processing get_key_range
java.lang.UnsupportedOperationException
        at java.util.Collections$UnmodifiableCollection.addAll(Collections.java:1044)
        at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:673)
        at org.apache.cassandra.service.CassandraServer.get_key_range(CassandraServer.java:557)
        at org.apache.cassandra.service.Cassandra$Processor$get_key_range.process(Cassandra.java:1095)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:758)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

When attempting to do a range scan which crosses over nodes.

Also get the warning:

    [javac] Note: /home/mark/cassandra/cassandra-trunk/src/java/org/apache/cassandra/tools/KeyChecker.java uses or overrides a deprecated API.

At compile-time.
",07/Aug/09 20:16;markr;This patch goes on top of 348-2.patch and fixes the exception I described.,07/Aug/09 20:28;markr;This patch 348-2-fixup-2.patch supersedes the previous one and fixes another case where it was trying to modify a readonly list.,"07/Aug/09 20:31;markr;With the combined efforts of your patch and my patch, range scans are a lot *closer* to working correctly. My system test program runs successfully with two nodes 0 and 8, but still fails a few test cases when there are four nodes 0,4,8,c

However, the remaining test cases which are failing are ones where a range covers at least three nodes.These are unlikely to happen to anyone in production unless their nodes are very close together or their keys very sparse and they're doing massive range scans.

But it would be nice if we covered all cases.","07/Aug/09 20:48;jbellis;Committed -2 with a simpler fix for the readonly list.

If you can find a way to reproduce the remaining bug in a 2-node setup that will make it easier to debug.","07/Aug/09 21:28;markr;Technically this is fixed as I can't reproduce it in a two-node setup any more. On the other hand, some range scans still return missing results in a three-node setup.

So either, close this and open a new one for the three-node case, or continue to work on a solution.

I fired up three nodes with tokens 0,4,8 then used the attached LoadAndScan.py.

This gives 3 errors out of 120 get_key_range commands. When run on two nodes (0,8) it passes, as on a single node.",07/Aug/09 21:47;jbellis;can you post the debug logs from a 3-node failure as before?,"08/Aug/09 12:35;hudson;Integrated in Cassandra #161 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/161/])
    - switch from trying to get the next endpoint by increasing offset to asking tokenMetadata for ""the next
one."" this will always be correct where the offset approach will not (usually you want offset to just be 1,
but sometimes you have to keep increasing it if no results are found but the range is still not finished)
 - merge results differently when the endpoint responsible for where the ring wraps is involved, since
that endpoint can hold keys from both the beginning and end of the range.

patch by jbellis; tested by Mark Robson for 
","12/Aug/09 17:51;jbellis;with the -3 patch, LoadAndScan.py passes all tests on 3 nodes for me.","12/Aug/09 20:54;markr;With the -3 patch, LoadAndScan.py now passes with 3 and 4 nodes if ReplicationFactor=1.

Unfortunately, setting ReplicationFactor=2 now breaks it.
",12/Aug/09 20:59;jbellis;fixes replication > 1 bugs,"12/Aug/09 21:27;jbellis;IRC: > Looks better

committed.","13/Aug/09 06:12;markr;Jonathan, 

The latest patch passes every range scan I have thrown at it, including with replication > 1

So it all looks good to me.

I will hopefully incorporate my tests into the suite soon.

Mark","13/Aug/09 13:13;hudson;Integrated in Cassandra #166 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/166/])
    give up on trying to optimize startWith -- it's basically impossible when replication factor > 1 b/c of the range wrap point.
patch by jbellis; tested by Mark Robson for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction can't find files,CASSANDRA-606,12442556,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,12/5/2009 23:34,3/12/2019 14:13,3/13/2019 22:24,12/6/2009 1:46,0.5,,,,0,,,,,,"We have been seeing issues with compaction running very often. We ran into this case when I found out in one of our CFs we were inserting a high volume of columns. Our threshold for memtable size flushes is 64MB but the MemtableObjectCountInMillions is 0.1 (we know this is low and will increase). On average we are writing so much data that compaction kicks off reguarly. And when we start trying to lookup data, we get lots of errors during compaction and get_slice (since sometimes files don't get cleaned up). In every event that this has occurred, a -Filter file was left behind. Never Data or Index.

ERROR [COMPACTION-POOL:1] 2009-12-05 15:04:47,412 DebuggableThreadPoolExecutor.java (line 120) Error in executor futuretask
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: /mnt/var/cassandra/data/Digg/UserActivity-1243-Data.db (No such file or directory)
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:112)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.FileNotFoundException: /mnt/var/cassandra/data/Digg/UserActivity-1243-Data.db (No such file or directory)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
        at org.apache.cassandra.io.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
        at org.apache.cassandra.io.SSTableScanner.<init>(SSTableScanner.java:47)
        at org.apache.cassandra.io.SSTableReader.getScanner(SSTableReader.java:386)
        at org.apache.cassandra.io.CompactionIterator.getCollatingIterator(CompactionIterator.java:65)
        at org.apache.cassandra.io.CompactionIterator.<init>(CompactionIterator.java:48)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:902)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:861)
        at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:663)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:180)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:177)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
 INFO [main] 2009-12-05 15:08:54,384 SSTable.java (line 156) Deleted /mnt/var/cassandra/data/Digg/UserActivity-1130-Data.db

",,,,,,,,,,,,,,,,,,,,06/Dec/09 01:29;lenn0x;0001-ColumnFamily.onStart-was-not-checking-files-related-.patch;https://issues.apache.org/jira/secure/attachment/12427082/0001-ColumnFamily.onStart-was-not-checking-files-related-.patch,06/Dec/09 01:39;lenn0x;0001-v2-ColumnFamily.onStart-was-not-checking-files-related-.patch;https://issues.apache.org/jira/secure/attachment/12427083/0001-v2-ColumnFamily.onStart-was-not-checking-files-related-.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,31:13.9,,,no_permission,,,,,,,,,,,,19777,,,Sun Dec 06 12:35:08 UTC 2009,,,,,,0|i0fzyv:,91425,,,,,,,,,,,06/Dec/09 00:20;lenn0x;Looks like ColumnFamilyStore.onStart() is not restricting itself to it's own CF files only. After CF A onstart finishes CF B onstart could remove orphans.,06/Dec/09 01:31;jbellis;the filename.contains(columnFamily_)) and cfName.equals(columnFamily_) checks are redundant too w/ this,06/Dec/09 01:44;jbellis;+1 v2,"06/Dec/09 12:35;hudson;Integrated in Cassandra #279 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/279/])
    ColumnFamily.onStart() was not checking files related to only it's column family. It was possible to remove files from other CFs thinking they were orphans. Skip files not apart of the CF. patch by goffinet; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BufferUnderflowExceptions,CASSANDRA-1513,12474434,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,wr0ngway,wr0ngway,9/17/2010 19:48,3/12/2019 14:13,3/13/2019 22:24,12/28/2010 15:49,,,,,0,,,,,,"Seeing a number of these in my log when running a trunk build from 9/11/2010
No idea how to duplicate it, hopefully you can make sense of it from the stack trace

ERROR [MUTATION_STAGE:19] 2010-09-14 02:24:50,704 DebuggableThreadPoolExecutor.
java (line 102) Error in ThreadPoolExecutorjava.nio.BufferUnderflowException
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:145)
        at java.nio.ByteBuffer.get(ByteBuffer.java:692)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:62)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

","Ubuntu 10.04.1, 1.6.0_18-b18",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,49:45.3,,,no_permission,,,,,,,,,,,,20175,,,Tue Dec 28 15:49:45 UTC 2010,,,,,,0|i0g5iv:,92325,,,,,,,,,,,"17/Sep/10 20:09;wr0ngway;Actually, I can get these pretty regularly (in my cluster anyway) - quite a few happen on some nodes when I restart a specific node
",28/Dec/10 15:49;jbellis;believe this was one of our thrift 0.5 upgrade bugs.  should be fixed in latest 0.7 rc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ByteBuffer bug in ExpiringColumn.updateDigest() ,CASSANDRA-1679,12478619,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,tjake,tjake,10/28/2010 20:40,3/12/2019 14:13,3/13/2019 22:24,10/28/2010 21:31,0.7.0 rc 1,,,,0,,,,,,"The MessageDigest calls in ExpringColumn change the position of the bytebuffer.

",,,,,,,,,,,,,,,,,,,,28/Oct/10 20:41;tjake;1679_v1.txt;https://issues.apache.org/jira/secure/attachment/12458274/1679_v1.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,31:22.4,,,no_permission,,,,,,,,,,,,20255,,,Thu Oct 28 21:31:22 UTC 2010,,,,,,0|i0g6pz:,92519,,,,,,,,,,,28/Oct/10 21:31;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sstable2json generates invalid json for ""paged"" rows",CASSANDRA-2188,12498989,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,skamio,skamio,2/18/2011 2:43,3/12/2019 14:13,3/13/2019 22:24,2/22/2011 17:38,0.7.3,0.8 beta 1,Legacy/Tools,,0,,,,,,"I have a json file created with sstable2json for a column family of super column type. But json2sstable failed to create sstable from the file. It's because file format is wrong. 

 WARN 11:41:55,141 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
org.codehaus.jackson.JsonParseException: Unexpected character ('""' (code 34)): was expecting comma to separate OBJECT entries
 at [Source: dump.json; line: 2, column: 739439661]
        at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:929)
        at org.codehaus.jackson.impl.JsonParserBase._reportError(JsonParserBase.java:632)
        at org.codehaus.jackson.impl.JsonParserBase._reportUnexpectedChar(JsonParserBase.java:565)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:128)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:93)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:65)
        at org.codehaus.jackson.map.deser.MapDeserializer._readAndBind(MapDeserializer.java:197)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:145)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:23)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:1261)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:517)
        at org.codehaus.jackson.JsonParser.readValueAs(JsonParser.java:897)
        at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:208)
        at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:197)
        at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:421)
ERROR: Unexpected character ('""' (code 34)): was expecting comma to separate OBJECT entries
 at [Source: dump.json; line: 2, column: 739439661]

When I looked at the file, I found that a comma is missing between super columns. The part of data is like this: 

[""756e697473"",
 ""32"",
 1297926692097000, false]]}""32303036303830373135303030302f313030303030303030302d32303036313030322d303030303030303639382d612f30"": {
""deletedAt"": -9223372036854775808,
 ""subColumns"": [[""5f64656c"",
 """",
 1297926692097000,
 false],

You'll see no comma between } and "". 

",linux,,7200,7200,,0%,7200,7200,,,,,,,,,,,,22/Feb/11 12:14;xedin;CASSANDRA-2188.patch;https://issues.apache.org/jira/secure/attachment/12471607/CASSANDRA-2188.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,03:11.4,,,no_permission,,,,,,,,,,,,20490,,,Tue Feb 22 18:57:54 UTC 2011,,,,,,0|i0g9uv:,93027,jbellis,jbellis,,,,,,,,,"18/Feb/11 10:03;xedin;This is a problem when sstable2json then, can you please tell me the version of the cassandra from which you were running sstable2json? can you regenerate json file using the lastest version of the cassandra and check if it is corrent using for example http://www.jsonlint.com/? Because I can't reproduce a problem which broken JSON for super column families on my side and need a bit more details on this...","22/Feb/11 04:47;muga_nishizawa;Hi Pavel, 

I was able to reproduce the problem above with my sstable file.  I explained the detail of how to generate problematic sstable on CASSANDRA-2212.  Please check it.  
",22/Feb/11 05:01;jbellis;I'm not sure what CASSANDRA-2212 has to do with json. Do you mean we should close this ticket?,22/Feb/11 12:14;xedin;Fixed problem in SSTable2JSON which was causing this error - no delimiter was set after each of the row paged part.,22/Feb/11 17:38;jbellis;committed,"22/Feb/11 18:57;hudson;Integrated in Cassandra-0.7 #304 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/304/])
    fix sstable2json large-row pagination
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2188
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pending range collision between nodes,CASSANDRA-603,12442507,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jaakko,lenn0x,lenn0x,12/4/2009 20:45,3/12/2019 14:13,3/13/2019 22:24,12/16/2009 18:13,0.5,,,,0,,,,,,"We bootstrapped 5 nodes on the east coast from an existing cluster (5) on west. We waited at least 60 seconds before starting up each node so it would start bootstrapping. We started seeing these types of errors:

 INFO [GMFD:1] 2009-12-04 01:45:42,065 Gossiper.java (line 568) Node /X.X.X.140 has now joined.
ERROR [GMFD:1] 2009-12-04 01:46:14,371 DebuggableThreadPoolExecutor.java (line 127) Error in ThreadPoolExecutor
java.lang.RuntimeException: pending range collision between /X.X.X.139 and /X.X.X.140
        at org.apache.cassandra.locator.TokenMetadata.addPendingRange(TokenMetadata.java:242)
        at org.apache.cassandra.service.StorageService.updateBootstrapRanges(StorageService.java:481)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:402)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:692)
        at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:657)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:610)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(Gossiper.java:978)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [GMFD:1] 2009-12-04 01:46:14,378 CassandraDaemon.java (line 71) Fatal exception in thread Thread[GMFD:1,5,main]   
java.lang.RuntimeException: pending range collision between /X.X.X.139 and /X.X.X.140
java.lang.RuntimeException: pending range collision between /X.X.X.139 and /X.X.X.140
        at org.apache.cassandra.locator.TokenMetadata.addPendingRange(TokenMetadata.java:242)
        at org.apache.cassandra.service.StorageService.updateBootstrapRanges(StorageService.java:481)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:402)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:692)
        at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:657)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:610)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(Gossiper.java:978)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619) ",,,,,,,,,,,,,,,,,,,,16/Dec/09 07:44;jaakko;603.patch;https://issues.apache.org/jira/secure/attachment/12428139/603.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,47:38.6,,,no_permission,,,,,,,,,,,,19774,,,Thu Dec 17 22:03:03 UTC 2009,,,,,,0|i0fzy7:,91422,,,,,,,,,,,"07/Dec/09 00:47;jaakko;Pending range collision check is currently too trigger-happy and should be relaxed a bit. At the moment pending range collision happens if any of the node's ranges clash, although it should happen only if the primary range node is booting to clashes.

However, if your nodes already had finished bootstrapping, then this is another issue completely. Haven't seen such case myself, but I'll have a look.
","07/Dec/09 13:02;jaakko;Could you please check if the nodes in question (.139 and .140) had already completed bootstrap (log entry ""Bootstrap completed..."" on INFO level)?
","07/Dec/09 17:29;lenn0x;No they did not. The messages started showing up a few minutes after I believe, anticompaction was still running.","08/Dec/09 13:16;jaakko;For now it is best to wait until one bootstrap has finished before starting another one. As mentioned pending ranges clash is relatively easy to get (happens if any of replica ranges are the same), so before this one is fixed, you might easily see this even ""without reason"".
","08/Dec/09 13:28;jaakko;As for the fix, there are two (at least) two options I think:

(1) Add a list of pending primary ranges (or tokens) to token metadata. Currently primary and replica pending ranges are all in one list, so there is no way to check afterwards if primary ranges collide.

(2) Ditch pending ranges completely and convert it to pending tokens. Problem with pending ranges is that it is static structure (determined at the time of bootstrap/leaving) and does not react to token changes during the operation. This introduces a number of difficult-to-prove-that-it-works-correctly and difficult-to-handle-correctly corner cases regarding node movement as proved by various mail and JIRA discussions recently. If we had a list of pending tokens instead, it would adapt to any changes that happen during the move operation. There are currently issues in pending range handling (not cleaned up correctly in all cases, thread/atomicy issues, leaving coordination, etc) that would mostly go away if we swiched to pending tokens instead, I think. Might be that I'm overlooking something obvious here, but to me it seems like dynamically adapting pending token list would be more suitable for this.
","08/Dec/09 17:26;lenn0x;Jaakko,

Just curious, for all the node movement coordination operations, would Zookeeper make any of this a bit easier to manage?","10/Dec/09 13:23;jaakko;It certainly is worth considering. For now I think we can do without, but with automatic load balancing coordination issues will become more complex.

I'm going to have a look at this pending range issue tomorrow.
","11/Dec/09 11:32;jaakko;Patch attached. Modifications:

Keep track of booting and leaving tokens and calculate pending ranges again every time there is status change. This will keep them up to date. To ensure that pending ranges cover node's final range, following reasoning is used in calculation:

(1) When in doubt, it is better to write too much to a node than too little. That is, if there are multiple nodes moving, calculate the biggest ranges a node could have. Cleaning up unneeded data afterwards is better than missing writes during movement.

(2) When a node leaves, ranges for other nodes can only grow (a node might get additional ranges, but it will not lose any of its current ranges as a result of a leave). Therefore we will first remove _all_ leaving tokens for the sake of calculation and then check what ranges would go where if all nodes are to leave. This way we get the biggest possible ranges with regard current leave operations, covering all subsets of possible final range values.

(3) When a node bootstraps, ranges of other nodes can only get smaller. Without doing complex calculations to see if multiple bootstraps overlap, we simply base calculations on the same token ring used before (reflecting situation after all leave operations have completed). Bootstrapping nodes will be added and removed one by one to that metadata and checked what their ranges would be. This will give us the biggest possible ranges the node could have. It might be that other bootstraps make our actual final ranges smaller, but it does not matter as we can clean up the data afterwards.

Bootstrap Token collision (old pending range collision) is thrown now only if bootstrap tokens are identical.

Calculating pending ranges is rather heavy operation, but since it is done only once when a node changes state in the cluster, it should be manageable.

This patch would make #572 cleaner to do, since we now know which way a node is going and can update pending ranges according to any changes.

Edit: this also removes nodeprobe cancelpendingranges. That would be pointless now. If there is a node/token that has not finished move operation, nodeprobe removetoken will do the trick.","14/Dec/09 22:32;jbellis;1, 2, and 3 are all how the existing code works in my mind (which may not be how it works in reality :).  What does the extra tracking of bootstrap tokens & leaving endpoints buy us?  As you said, pending ranges shouldn't overlap in the first place unless there is a token collision.","15/Dec/09 05:55;jaakko;Unfortunately it doesn't quite work that way :)

First the case of leaving nodes:

Problem with current implementation is that pending ranges is calculated only once at the time of leaving. Suppose there is a ring of nodes A, B, C, D and E with replication factor 2. Ring status is this:

(primary, replica)
E-A, D-E
A-B, E-A
B-C, A-B
C-D, B-C
D-E, C-D

Suppose C prepares to leave. After hearing STATE_LEAVING from C, ring status will be:

(primary, replica, pending)
E-A, D-E
A-B, E-A
B-C, A-B
C-D, B-C, A-B
D-E, C-D, B-C

Now suppose also B leaves. After receiving STATE_LEAVING, ring status with current implementation will be:
E-A, D-E
A-B, E-A
B-C, A-B, E-A
C-D, B-C, A-B
D-E, C-D, B-C

This is clearly wrong, as (1) E-A is being streamed to C, even though it is leaving and (2) D is not getting this range, even if it is supposed to.

In order to do this right, we will need to know at all times what nodes are leaving and calculate ranges accordingly. An anonymous pending ranges list is not enough, as that does not tell which node is leaving and/or if the ranges are there because of bootstrap or leave operation.


As for bootstrapping and pending range collision:

Suppose that there is a ring of nodes A, C and E, with replication factor 3. Node D bootstraps between C and E, so its pending ranges will be E-A, A-C and C-D. Now suppose node B bootstraps between A and C at the same time. Its pending ranges would be C-E, E-A and A-B. Now both nodes have pending range E-A in their list, which will cause pending range collision even though we're only talking about replica range, not even primary range. The same thing happens for any nodes that boot simultaneously between same two nodes. For this we cannot simply make pending ranges a multimap, since that would make us unable to notice the real problem of two nodes trying to boot using the same token. In order to do this properly, we need to know what tokens are booting at any time.
","15/Dec/09 21:14;jbellis;Thanks for the explanation.  Committed, w/ parts of the above as comments to TokenMetadata.

One minor quibble is, I'd really prefer to avoid having this circular TM <-> ReplicationStrategy dependency cycle.  (Which is part of the reason the code was structured the way it was: RS would pass TM the info it needed to do its thing in a concurrency-safe fashion, w/o needing to reach into RS itself which makes auditing for thread-safety much harder).  So if you think of a way to refactor that, even better. :)","15/Dec/09 21:15;jbellis;Oops, I take back the ""committed"" part -- I'm getting test failures.  I think they just need to be updated to use the new method signatures.","16/Dec/09 07:44;jaakko;New version:
- Moved calculatePendingRanges to StorageService
- fixed test errors",16/Dec/09 18:13;jbellis;committed to 0.5 and trunk,"17/Dec/09 22:03;hudson;Integrated in Cassandra #291 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/291/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure to flush commit log,CASSANDRA-694,12445429,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,ryandaum,ryandaum,1/13/2010 19:11,3/12/2019 14:13,3/13/2019 22:24,1/14/2010 20:09,0.5,,,,0,,,,,,"The following exception occurs consistently on at least node (note did not occur on other same-configured nodes) during startup:

INFO - Replaying /var/lib/cassandra/commitlog/CommitLog-1262855754427.log, /var/lib/cassandra/commitlog/CommitLog-1262832689989.log, /var/lib/cassandra/commitlog/CommitLog-1262885833186.log, /var/lib/cassandra/commitlog/CommitLog-1262900845019.log, /var/lib/cassandra/commitlog/CommitLog-1262913267844.log, /var/lib/cassandra/commitlog/CommitLog-1262927898170.log, /var/lib/cassandra/commitlog/CommitLog-1262961421039.log, /var/lib/cassandra/commitlog/CommitLog-1262977175175.log, /var/lib/cassandra/commitlog/CommitLog-1262989588783.log, /var/lib/cassandra/commitlog/CommitLog-1263000573676.log, /var/lib/cassandra/commitlog/CommitLog-1263013691393.log, /var/lib/cassandra/commitlog/CommitLog-1263044706108.log, /var/lib/cassandra/commitlog/CommitLog-1263060004191.log, /var/lib/cassandra/commitlog/CommitLog-1263071446342.log, /var/lib/cassandra/commitlog/CommitLog-1263082950154.log, /var/lib/cassandra/commitlog/CommitLog-1263095400814.log, /var/lib/cassandra/commitlog/CommitLog-1263118331046.log, /var/lib/cassandra/commitlog/CommitLog-1263143402963.log, /var/lib/cassandra/commitlog/CommitLog-1263155294308.log, /var/lib/cassandra/commitlog/CommitLog-1263166154352.log, /var/lib/cassandra/commitlog/CommitLog-1263178359247.log, /var/lib/cassandra/commitlog/CommitLog-1263202112017.log, /var/lib/cassandra/commitlog/CommitLog-1263230932274.log, /var/lib/cassandra/commitlog/CommitLog-1263250726505.log, /var/lib/cassandra/commitlog/CommitLog-1263264159438.log, /var/lib/cassandra/commitlog/CommitLog-1263289964249.log, /var/lib/cassandra/commitlog/CommitLog-1263317974387.log, /var/lib/cassandra/commitlog/CommitLog-1263331989090.log, /var/lib/cassandra/commitlog/CommitLog-1263344147667.log, /var/lib/cassandra/commitlog/CommitLog-1263359751527.log, /var/lib/cassandra/commitlog/CommitLog-1263395707008.log, /var/lib/cassandra/commitlog/CommitLog-1263397833524.log, /var/lib/cassandra/commitlog/CommitLog-1263398736183.log, /var/lib/cassandra/commitlog/CommitLog-1263399753707.log, /var/lib/cassandra/commitlog/CommitLog-1263401667504.log, /var/lib/cassandra/commitlog/CommitLog-1263404640782.log, /var/lib/cassandra/commitlog/CommitLog-1263405827234.log, /var/lib/cassandra/commitlog/CommitLog-1263406901115.log
INFO - LocationInfo has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(LocationInfo)@25934689
INFO - HintsColumnFamily has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(HintsColumnFamily)@4766820
INFO - AdXRequestStatistics has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(AdXRequestStatistics)@21521158
INFO - TokenGoogleIDCF has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(TokenGoogleIDCF)@22889075
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.AssertionError: Blocking serialized executor is not yet implemented
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:84)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:1045)
        at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:395)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:448)
        at org.apache.cassandra.db.Table.flush(Table.java:464)
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:397)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:65)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:90)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:135)
        ... 5 more

And the same exception occurs intermittently on other node (running) nodes during 'nodeprobe flush':

root@domU-12-31-38-00-26-31:~# nodeprobe -host localhost -port 8080 flush Logger
Exception in thread ""main"" java.lang.AssertionError: Blocking serialized executor is not yet implemented
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:84)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:1045)
        at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:395)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:448)
        at org.apache.cassandra.service.StorageService.forceTableFlush(StorageService.java:984)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1264)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1359)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)","Linux  2.6.21.7-2.fc8xen #1 SMP Fri Feb 15 12:39:36 EST 2008 i686 GNU/Linux, ec2 small instance",,,,,,,,,,,,,,,,,,,13/Jan/10 20:37;jbellis;694-0.5.txt;https://issues.apache.org/jira/secure/attachment/12430168/694-0.5.txt,13/Jan/10 21:09;jbellis;694-trunk.txt;https://issues.apache.org/jira/secure/attachment/12430173/694-trunk.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,37:20.4,,,no_permission,,,,,,,,,,,,19823,,,Thu Jan 14 20:09:04 UTC 2010,,,,,,0|i0g0i7:,91512,,,,,,,,,,,13/Jan/10 19:19;ryandaum;Note that the error occurs with build off trunk (svn rev 898899) as well.,13/Jan/10 20:37;jbellis;assumption that all single-threaded executors have an unbounded queue is no longer valid.  this patch provides a policy for dealing with single thread executors w/ a full queue.,13/Jan/10 21:09;jbellis;version for trunk w/ more comments & a unit test.  will backport when i commit to 0.5.,"13/Jan/10 21:23;jbellis;from irc:

rdaum> that patch works
",14/Jan/10 20:09;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add LICENSE and NOTICE files to jar,CASSANDRA-309,12431042,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,euphoria,urandom,urandom,7/21/2009 16:34,3/12/2019 14:13,3/13/2019 22:24,7/21/2009 23:03,,,Legacy/Tools,,0,,,,,,A copy of the LICENSE and NOTICE files should be included in the jar to make it distributable stand-alone. See http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg00561.html,,,,,,,,,,,CASSANDRA-301,,,,,,,,,21/Jul/09 19:04;euphoria;309_v1.diff;https://issues.apache.org/jira/secure/attachment/12414129/309_v1.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,04:54.0,,,no_permission,,,,,,,,,,,,19629,,,Tue Jul 21 23:03:14 UTC 2009,,,,,,0|i0fy5b:,91130,,,,,,,,,,,21/Jul/09 19:04;euphoria;This patch against trunk should do it.  Also applies with fuzz to 0.3 branch.,21/Jul/09 23:03;jbellis;committed to 0.3 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable gets removed if index file is missing,CASSANDRA-343,12432217,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,8/4/2009 23:31,3/12/2019 14:13,3/13/2019 22:24,9/1/2009 23:38,0.4,,,,0,,,,,,"If an SSTable is found but is missing an index, the data file is marked corrupt and deleted. We should fix this, and have index re-generation.",,,,,,,,,,,,,,,,,,,,01/Sep/09 22:38;lenn0x;0001-CASSANDRA-343.-Do-not-delete-an-SSTable-if-it-s-corr.patch;https://issues.apache.org/jira/secure/attachment/12418310/0001-CASSANDRA-343.-Do-not-delete-an-SSTable-if-it-s-corr.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,45:20.9,,,no_permission,,,,,,,,,,,,19644,,,Tue Sep 01 23:38:58 UTC 2009,,,,,,0|i0fycv:,91164,,,,,,,,,,,"04/Aug/09 23:45;jbellis;How are you getting in that state?  The index and bloom filter files are renamed first.  If you're manually deleting index files it's not Cassandra's job to guess what you want it to do.  Otherwise we have a bug.

(In general, if we are in a ""can't happen"" state then we should leave things the way they are and let a human intervene.  In this case I would fail the startup and let the operator decide whether he wants to delete the sstable or build an index.)","04/Aug/09 23:49;lenn0x;I deleted the index file. But in any case, if the index is missing, I think we should try to at least regenerate if possible. There could be a chance of corruption. Also it does delete the data file, which I am thinking is a _bad_ idea no matter what state its in.",04/Aug/09 23:51;lenn0x;I am for letting the operator re-generate the index and fail startup. So that means we need to bail and not delete. And how would we provide the option to re-generate?,"05/Aug/09 00:10;jbellis;> how would we provide the option to re-generate? 

I suppose we could inaugurate a contrib/ section if you want to write such a tool.  Definitely shouldn't be part of the core tho.","05/Aug/09 00:14;jbellis;Actually we should probably just post it here, and if anyone else ever needs it, _then_ put it in contrib/ :)",01/Sep/09 22:38;lenn0x;Do not delete table if corrupt.,01/Sep/09 23:38;jbellis;committed w/ minor tweaking,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
debian init script could be more consistent w/ bin/cassandra,CASSANDRA-1407,12471941,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,8/18/2010 20:17,3/12/2019 14:13,3/13/2019 22:24,8/18/2010 21:11,0.7 beta 2,,Packaging,,0,,,,,,"With the introduction of conf/cassandra-env.sh it should be possible to eliminate the separately maintained /etc/default/cassandra.  The init script should also use a JAVA_HOME derived from the java binary in PATH (if it exists), both because this is how things work in Debian (_the_ Java is the one chosen by alternatives), and because this is how bin/cassandra works as well.

Patches to follow.",,,,,,,,,,,,,,,,,,,,18/Aug/10 20:20;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-1407-refactor-assigment-of-JAVA_HOME-in-init.txt;https://issues.apache.org/jira/secure/attachment/12452452/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-1407-refactor-assigment-of-JAVA_HOME-in-init.txt,18/Aug/10 20:20;urandom;ASF.LICENSE.NOT.GRANTED--v2-0002-CASSANDRA-1407-use-conf-cassandra-env.sh-in-place-of-d.txt;https://issues.apache.org/jira/secure/attachment/12452453/ASF.LICENSE.NOT.GRANTED--v2-0002-CASSANDRA-1407-use-conf-cassandra-env.sh-in-place-of-d.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,58:27.5,,,no_permission,,,,,,,,,,,,20122,,,Sat Aug 21 11:14:37 UTC 2010,,,,,,0|i0g4vb:,92219,,,,,,,,,,,18/Aug/10 20:58;brandon.williams;+1,18/Aug/10 21:11;urandom;committed.,"21/Aug/10 11:14;hudson;Integrated in Cassandra #518 (See [https://hudson.apache.org/hudson/job/Cassandra/518/])
    use conf/cassandra-env.sh in place of defaults

Patch by eevans; reviewed by Brandon Williams for CASSANDRA-1407
refactor assigment of JAVA_HOME in init script

Path by eevans; review by Brandon Williams for CASSANDRA-1407
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush creates empty SSTables if nothing exists in that CF,CASSANDRA-532,12440045,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,11/6/2009 17:45,3/12/2019 14:13,3/13/2019 22:24,11/10/2009 5:20,0.5,,,,0,,,,,,"When calling flush() through nodeprobe, we see SSTables being created that are empty for CFs:

 INFO [COMPACTION-POOL:1] 2009-11-05 22:58:09,515 ColumnFamilyStore.java (line 850) Compacting [org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-9-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-10-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-7-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-11-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-8-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-5-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-6-Data.db')]
ERROR [COMPACTION-POOL:1] 2009-11-05 22:58:09,516 DebuggableThreadPoolExecutor.java (line 120) Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:112)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.SSTableReader.getApproximateKeyCount(SSTableReader.java:102)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:866)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:830)
        at org.apache.cassandra.db.ColumnFamilyStore.doMajorCompactionInternal(ColumnFamilyStore.java:673)
        at org.apache.cassandra.db.ColumnFamilyStore.doMajorCompaction(ColumnFamilyStore.java:645)
        at org.apache.cassandra.db.CompactionManager$OnDemandCompactor.run(CompactionManager.java:123)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
",,,,,,,,,,,,,,,,,,,,10/Nov/09 04:50;lenn0x;0001-forceFlushBinary-was-calling-memtable-not-binaryMemt.patch;https://issues.apache.org/jira/secure/attachment/12424436/0001-forceFlushBinary-was-calling-memtable-not-binaryMemt.patch,06/Nov/09 17:51;jbellis;532.patch;https://issues.apache.org/jira/secure/attachment/12424211/532.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,57:00.1,,,no_permission,,,,,,,,,,,,19740,,,Tue Nov 10 05:20:43 UTC 2009,,,,,,0|i0fzif:,91351,,,,,,,,,,,06/Nov/09 17:52;lenn0x;+1 looks good,06/Nov/09 17:57;jbellis;committed,"07/Nov/09 12:33;hudson;Integrated in Cassandra #251 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/251/])
    avoid flushing empty binarymemtable
patch by jbellis; reviewed by goffinet for 
",10/Nov/09 04:50;lenn0x;forceFlushBinary() was calling memtable not binaryMemtable_,"10/Nov/09 05:04;jbellis;+1

(apply to 0.4 first, then merge to trunk pls)",10/Nov/09 05:20;lenn0x;commited,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_slice needs to allow returning all columns,CASSANDRA-262,12429005,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,6/26/2009 22:53,3/12/2019 14:13,3/13/2019 22:24,7/4/2009 5:45,0.4,,,,0,,,,,,"Right now get_slice requires you to enter a 'large' value, -1 used to indicate 'all columns'. We should allow this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,43:23.8,,,no_permission,,,,,,,,,,,,19612,,,Sun Nov 08 00:57:10 UTC 2009,,,,,,0|i0fxv3:,91084,,,,,,,,,,,"01/Jul/09 15:43;jbellis;How about if we add a configuration parameter for ""maximum columns to slice at once,"" that defaults to something high enough (1000?) that you really ought to page if you need more than that anyway, and low enough that it's in no danger of crashing your server from OOM?",01/Jul/09 15:51;lenn0x;+1. I am okay with this.,"01/Jul/09 16:01;jbellis;Eric, since you're already in the slice code for CASSANDRA-263, do you think you could take a stab at this too?","01/Jul/09 22:35;jbellis;Actually, as with many of my ""brilliant"" ideas, this one looks worse given a little more time. :)

I don't see what purpose this would serve over the client just sending 1000 for the count, other than obfuscation.","04/Jul/09 01:10;eweaver;For what it's worth, I'm having the Ruby client currently default to limit = 100...I couldn't really care less what the server thinks the limit is. I expect other clients would be similar.
","04/Jul/09 05:45;jbellis;yes, that's the right idea.

I set the default for all the count parameters to 100 in cassandra.thrift.  I don't know of any languages for which thrift actually generates working defaults but it's thrift's bug now. :)","04/Jul/09 12:34;hudson;Integrated in Cassandra #128 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/128/])
    default all count args to 100.  patch by jbellis for 
","08/Nov/09 00:57;kristopolous;FYI, The last link is now dead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper thread deadlock,CASSANDRA-778,12455667,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,2/8/2010 18:49,3/12/2019 14:13,3/13/2019 22:24,2/8/2010 19:39,0.6,,,,0,,,,,,"Found this while attempting to bootstrap a node with more than a trivial amount of data:

Found one Java-level deadlock:
=============================
""GMFD:1"":
  waiting to lock monitor 0x0000000100861d60 (object 0x00000001066a7ed8, a org.apache.cassandra.service.StorageService),
  which is held by ""main""
""main"":
  waiting to lock monitor 0x0000000100860710 (object 0x0000000106c7c968, a org.apache.cassandra.gms.Gossiper),
  which is held by ""GMFD:1""

Java stack information for the threads listed above:
===================================================
""GMFD:1"":
	at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:226)
	- waiting to lock <0x00000001066a7ed8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:634)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:502)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:445)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:812)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:607)
	at org.apache.cassandra.gms.Gossiper.handleNewJoin(Gossiper.java:582)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:649)
	- locked <0x0000000106c7c968> (a org.apache.cassandra.gms.Gossiper)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAck2VerbHandler.doVerb(Gossiper.java:1061)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
""main"":
	at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:861)
	- waiting to lock <0x0000000106c7c968> (a org.apache.cassandra.gms.Gossiper)
	at org.apache.cassandra.service.StorageService.startBootstrap(StorageService.java:347)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:318)
	- locked <0x00000001066a7ed8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:99)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:174)

Found 1 deadlock.

main acquires SS lock and doesn't release it before attempting to acquire the Gossiper lock.  Meanwhile, the gossip stage acquires the Gossiper lock and then attempts to acquire the SS lock.

Solution is to have finer-grained locking on the resource in SS (map of replication strategies), or to move the collection to a different class (DD maybe?).  This was introduced in CASSANDRA-620.",,,,,,,,,,,,,,,,,,,,08/Feb/10 19:18;gdusbabek;0001-fix-deadlock.patch;https://issues.apache.org/jira/secure/attachment/12435196/0001-fix-deadlock.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,56:20.1,,,no_permission,,,,,,,,,,,,19859,,,Wed Feb 17 17:54:43 UTC 2010,,,,,,0|i0g10n:,91595,,,,,,,,,,,08/Feb/10 18:50;gdusbabek;introduce a fine-grained lock around SS.replicationStrategies.,08/Feb/10 18:56;jbellis;Can we get rid of the manual locking entirely instead?  ISTM that either (a) instantiating all RS in the SS constructor or (b) using NBHM and the ConcurrentMap apis (putIfAbsent etc) would fix this.,08/Feb/10 19:18;gdusbabek;does away with manual locking.,08/Feb/10 19:23;jbellis;+1,"08/Feb/10 19:39;gdusbabek;r907771. 

Modified slightly so that it throws a RTE if a bogus table is given (to comply with unit tests).","17/Feb/10 17:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_key_range timeout and exception,CASSANDRA-153,12424856,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,nk11,nk11,5/7/2009 20:20,3/12/2019 14:13,3/13/2019 22:24,5/11/2009 23:57,,,,,0,,,,,,"My test code:

		int max = 5000;		
		for (int a = 0; a < max; a++) {
			System.out.println(a);
			client.insert(""Table1"", ""k1:"" + a, ""Super1:x:x"", new byte[] { (byte) 1 }, 0, false);
		}		

		client.get_key_range(""Table1"", ""k1:0"", ""k1:1000"", 1000);

Produces in the logs:

ERROR [ROW-READ-STAGE:9] 2009-05-07 23:04:56,609 CassandraDaemon.java (line 61) Fatal exception in thread Thread[ROW-READ-STAGE:9,5,main]
java.lang.RuntimeException: corrupt sstable
	at org.apache.cassandra.db.FileStruct.seekTo(FileStruct.java:107)
	at org.apache.cassandra.db.Table.getKeyRange(Table.java:905)
	at org.apache.cassandra.service.RangeVerbHandler.doVerb(RangeVerbHandler.java:23)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:46)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readUnsignedShort(Unknown Source)
	at java.io.DataInputStream.readUTF(Unknown Source)
	at java.io.RandomAccessFile.readUTF(Unknown Source)
	at org.apache.cassandra.io.SequenceFile$AbstractReader.getPositionFromBlockIndex(SequenceFile.java:562)
	at org.apache.cassandra.db.FileStruct.seekTo(FileStruct.java:86)
	... 6 more
ERROR [pool-1-thread-2] 2009-05-07 23:05:01,593 Cassandra.java (line 1187) Internal error processing get_key_range
java.lang.RuntimeException: error reading keyrange RangeCommand(table='Table1', startWith='k1:0', stopAt='k1:1000', maxResults=1000)
	at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:682)
	at org.apache.cassandra.service.CassandraServer.get_key_range(CassandraServer.java:511)
	at org.apache.cassandra.service.Cassandra$Processor$get_key_range.process(Cassandra.java:1183)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:805)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.TimeoutException: Operation timed out.
	at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:95)
	at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:677)
	... 7 more

",,,,,,,,,,,,,,,,,,,,08/May/09 16:18;jbellis;153-02.patch;https://issues.apache.org/jira/secure/attachment/12407631/153-02.patch,07/May/09 22:04;jbellis;153.patch;https://issues.apache.org/jira/secure/attachment/12407580/153.patch,11/May/09 14:28;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-153-unit-test-to-expose-bug-system-test-is-r.txt;https://issues.apache.org/jira/secure/attachment/12407781/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-153-unit-test-to-expose-bug-system-test-is-r.txt,11/May/09 14:28;jbellis;ASF.LICENSE.NOT.GRANTED--0002-cannonicalize-all-accesses-to-indexMeatdataMap.txt;https://issues.apache.org/jira/secure/attachment/12407782/ASF.LICENSE.NOT.GRANTED--0002-cannonicalize-all-accesses-to-indexMeatdataMap.txt,11/May/09 14:28;jbellis;ASF.LICENSE.NOT.GRANTED--0003-check-for-at-end-of-data-in-iterator-init.txt;https://issues.apache.org/jira/secure/attachment/12407783/ASF.LICENSE.NOT.GRANTED--0003-check-for-at-end-of-data-in-iterator-init.txt,,,,,,,,,5,,,,,,,,,,,,,,,,,,,04:43.6,,,no_permission,,,,,,,,,,,,19571,,,Wed May 13 09:26:39 UTC 2009,,,,,,0|i0fx73:,90976,,,,,,,,,,,"07/May/09 20:21;nk11;1 node

Default config except:

 <MemtableSizeInMB>1</MemtableSizeInMB>
 <MemtableObjectCountInMillions>0.001</MemtableObjectCountInMillions>
","07/May/09 22:04;jbellis;try it with this patch.  if that doesn't work, rm -r /var/cassandra/* to reset things and try again; it's possible that you have corrupt data from a bug that is fixed now.","07/May/09 22:30;nk11;patched, rebuilded, removed var/cassandra, still failing with the same exception","08/May/09 06:32;nk11;If it helps:

The exception is thrown in SequenceFile.java at line 562 in getPositionFromBlockIndex() method.

     String blockIndexKey = file_.readUTF();

The key is ""k1:0"" and blockIndexPosition=102016	

If I do a file_.read() there I got -1 so the end of the file is reached.","08/May/09 16:18;jbellis;committed first patch.

i can reproduce the exception now using a different test after one more patch to fix another bug first (attached).",08/May/09 16:18;jbellis;fix for CME during getKeyRange,08/May/09 17:22;nk11;it works,08/May/09 21:02;nk11;I spoke to soon. For 20000 keys it reproduces again...,09/May/09 01:41;jbellis;committed second patch.  still working on reading-past-EOF bug.  I think it's compaction-related.,"11/May/09 14:31;jbellis;fixed two more bugs.  the first is what you were running into; SequenceFile and FileStruct both assume when seeking that the block index exists at the end of the Coordinate computed by SSTable.  But we weren't always consistent in specifying the index filename and instead of raising an error Coordinate would just return something bogus.  these are both fixed in 02.

03 fixes a bug in FS iteration when no keys in the range specified exist in a given SSTable.","11/May/09 20:53;junrao;The fix in patch 0003 seems redundant. The same code is already called in advance(). The correct fix seems to be getting rid of  line
           saved = key;
in FileStructIterator().
","11/May/09 21:18;jbellis;no, because then we will always skip the current key, which in the Range context is always going to be a key we are interested in (since seekTo skips all the keys we do not want, stopping when it comes to the first interesting one).

saved = key

in the constructor is what allows next() to return that current key.

but, if seekTo actually skipped _everything_ (and current key is block index) then we don't want to include that.  that's what 0003 fixes.

(I checked in case my intuition was wrong and the test_range_collation system test fails with your alternative, so they are indeed not equivalent.)","11/May/09 22:39;junrao;Ok, I understood this now. You are right and the patch looks fine. 

What confused me is that the iterator over FileStruct relies on a seekTo call first, which is not how a typical iterator works.

I got a CME execption on testCompactions. I guess that's related to another issue you just opened.
",11/May/09 23:57;jbellis;committed,"13/May/09 09:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Message Serializer slows down/stops responding,CASSANDRA-487,12438043,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sammy.yu,sammy.yu,sammy.yu,10/14/2009 4:54,3/12/2019 14:13,3/13/2019 22:24,10/16/2009 20:36,0.4,,,,0,,,,,,"We ran into an issue with where the MESSAGE-SERIALIZER-POOL piles up with tasks.
$ /usr/sbin/nodeprobe -host localhost tpstats
FILEUTILS-DELETE-POOL, pending tasks=0
MESSAGING-SERVICE-POOL, pending tasks=0
MESSAGE-SERIALIZER-POOL, pending tasks=10785714
RESPONSE-STAGE, pending tasks=0
BOOT-STRAPPER, pending tasks=0
ROW-READ-STAGE, pending tasks=0
MESSAGE-DESERIALIZER-POOL, pending tasks=0
GMFD, pending tasks=0
LB-TARGET, pending tasks=0
CONSISTENCY-MANAGER, pending tasks=0
ROW-MUTATION-STAGE, pending tasks=0
MESSAGE-STREAMING-POOL, pending tasks=0
LOAD-BALANCER-STAGE, pending tasks=0
MEMTABLE-FLUSHER-POOL, pending tasks=0

In the log, this seems to have happened when we stopped 2 of the other nodes in our cluster.  This node will  time out on any thrift requests.  Looking through the logs we found the following two exceptions:
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:349)
        at java.util.Collections.sort(Collections.java:120)
        at org.apache.cassandra.net.TcpConnectionManager.getLeastLoaded(TcpConnectionManager.java:108)
        at org.apache.cassandra.net.TcpConnectionManager.getConnection(TcpConnectionManager.java:71)
        at org.apache.cassandra.net.MessagingService.getConnection(MessagingService.java:306)
        at org.apache.cassandra.net.MessageSerializationTask.run(MessageSerializationTask.java:66)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

java.util.NoSuchElementException
        at java.util.AbstractList$Itr.next(AbstractList.java:350)
        at java.util.Collections.sort(Collections.java:120)
        at org.apache.cassandra.net.TcpConnectionManager.getLeastLoaded(TcpConnectionManager.java:108)
        at org.apache.cassandra.net.TcpConnectionManager.getConnection(TcpConnectionManager.java:71)
        at org.apache.cassandra.net.MessagingService.getConnection(MessagingService.java:306)
        at org.apache.cassandra.net.MessageSerializationTask.run(MessageSerializationTask.java:66)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

This appears to have happened on all 4 MESSAGE-SERIALIZER-POOL threads 
I will attach the complete log.
",,,,,,,,,,,,,,,,,,,,14/Oct/09 13:21;sammy.yu;0001-Added-locks-around-remove-operation-so-that-Concurre.patch;https://issues.apache.org/jira/secure/attachment/12422101/0001-Added-locks-around-remove-operation-so-that-Concurre.patch,14/Oct/09 16:51;jbellis;487-lock-all-connection-ops.patch;https://issues.apache.org/jira/secure/attachment/12422115/487-lock-all-connection-ops.patch,14/Oct/09 04:56;sammy.yu;system-487.log.gz;https://issues.apache.org/jira/secure/attachment/12422062/system-487.log.gz,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,30:39.3,,,no_permission,,,,,,,,,,,,19716,,,Fri Oct 16 20:36:46 UTC 2009,,,,,,0|i0fz8f:,91306,,,,,,,,,,,"14/Oct/09 05:21;sammy.yu;I suspect this is because the TcpConnectionManager.removeConnection does not have a lock wrapped around it.
","14/Oct/09 13:21;sammy.yu;This looks like it exists in 0.5 as well.  Attached patch applies for 0.5.  I've wrapped the removeConnections method by using locks.  We may forgo this by using CopyOnWriteArrayList, but it may be too expensive.
",14/Oct/09 16:30;junrao;The patch looks good to me. Have your tried it in your deployment?,"14/Oct/09 16:38;sammy.yu;I've done some light testing, but we'll stress it out some more today.","14/Oct/09 16:51;jbellis;the problem is that the original code tries to ""cheat"" and rely on Vector's built-in synchronization for one-line ops.  but as this exception shows, even that has problems since operations like Collections.sort aren't synchronized (even though superfically it looks like a one-liner).

here is a more general patch that turns the Vector into an ArrayList and always does explicit locking of the collection, to remove the temptation to cheat like that.","14/Oct/09 17:04;jbellis;One other race in addToPool (not in my patch):

            if (contains(connection))
            {
                return;
            }

needs to be inside the lock_.",14/Oct/09 17:45;jbellis;Created CASSANDRA-488 for more deep fixes to the TcpConnManager area.,"15/Oct/09 00:27;sammy.yu;Tested jbellis' patch in production-like environment with normal operating state and restarted multiple nodes.
",16/Oct/09 20:36;jbellis;committed to 0.4 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
occasional CME in getKeyRange,CASSANDRA-161,12425118,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/11/2009 16:30,3/12/2019 14:13,3/13/2019 22:24,5/12/2009 19:11,0.3,,,,0,,,,,,"   [testng] java.util.ConcurrentModificationException
   [testng]     at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
   [testng]     at java.util.HashMap$KeyIterator.next(HashMap.java:828)
   [testng]     at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)
   [testng]     at org.apache.cassandra.db.Table.getKeyRange(Table.java:903)
   [testng]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testCompactions(ColumnFamilyStoreTest.java:426)
   [testng] ... Removed 22 stack frames
",,,,,,,,,,,,,,,,,,,,12/May/09 16:39;jbellis;0003-test-to-more-reliably-reproduce-CME-during-range.-f.patch;https://issues.apache.org/jira/secure/attachment/12407890/0003-test-to-more-reliably-reproduce-CME-during-range.-f.patch,12/May/09 15:49;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-161-fix-race-condition-between-when-memtable.txt;https://issues.apache.org/jira/secure/attachment/12407883/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-161-fix-race-condition-between-when-memtable.txt,12/May/09 15:49;jbellis;ASF.LICENSE.NOT.GRANTED--0002-fix-race-condition-in-compaction-it-was-possible-fo.txt;https://issues.apache.org/jira/secure/attachment/12407884/ASF.LICENSE.NOT.GRANTED--0002-fix-race-condition-in-compaction-it-was-possible-fo.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,01:14.0,,,no_permission,,,,,,,,,,,,19576,,,Wed May 13 09:26:39 UTC 2009,,,,,,0|i0fx8v:,90984,,,,,,,,,,,"12/May/09 15:50;jbellis;02
    fix race condition in compaction -- it was possible for a read thread to ""snapshot"" ssTables_, then have
    the compactor thread delete those (after merging them into a new file) before the read thread checked   
    them.  Since the read thread's ""snapshot"" doesn't include the new merged sstable, it incorrectly tells  
    the caller that the key does not exist.                                                                 

01
    fix race condition between when memtable is replaced as the active one and when it's added to
    the set of historical (pending flush) memtables                                                            

these patches also fix the heisenbugs in NameSort and TimeSort tests.",12/May/09 15:51;jbellis;(as part of 02 I switched to using a TreeSet which also fixes CASSANDRA-159.),12/May/09 16:39;jbellis;test to more reliably reproduce CME during range.  fix by locking out compact during range query,12/May/09 19:01;urandom;+1,12/May/09 19:11;jbellis;committed,"13/May/09 09:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    test to more reliably reproduce CME during range.  fix by locking out compact during range query.
patch by jbellis; reviewed by Eric Evans for 
fix race condition in compaction -- it was possible for a read thread to ""snapshot"" ssTables_, then have
the compactor thread delete those (after merging them into a new file) before the read thread checked
them.  Since the read thread's ""snapshot"" doesn't include the new merged sstable, it incorrectly tells
the caller that the key does not exist.
patch by jbellis; reviewed by Eric Evans for 
fix race condition between when memtable is replaced as the active one and when it's added to
the set of historical (pending flush) memtables.
patch by jbellis; reviewed by Eric Evans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
column values are only being validated in insert(),CASSANDRA-2259,12500108,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mishravivek,urandom,urandom,3/1/2011 22:06,3/12/2019 14:13,3/13/2019 22:24,3/8/2011 17:31,0.7.4,,Legacy/CQL,,0,,,,,,insert() is the only code path that currently results in validate() being called for column values; it is possible to write invalid column values using batch_mutate(),,,7200,7200,,0%,7200,7200,,,,,,,,,,,,07/Mar/11 16:42;jbellis;2259-v2.txt;https://issues.apache.org/jira/secure/attachment/12472833/2259-v2.txt,07/Mar/11 08:17;mishravivek;CASSANDRA-2259_v1.0;https://issues.apache.org/jira/secure/attachment/12472806/CASSANDRA-2259_v1.0,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,19:47.5,,,no_permission,,,,,,,,,,,,20532,,,Tue Mar 08 17:28:46 UTC 2011,,,,,,0|i0gab3:,93100,jbellis,jbellis,,,,,,,,,"07/Mar/11 08:19;mishravivek;@Eric:

Attached code change.
Changes are:
1) introduced a new method validateColumnValue().
2) On call to ThriftValidation.validateMutation() is modified to invoke validateColumnValue().

Patch attached for review.

Let me know if this fine then i can pick up this issue.","07/Mar/11 16:42;jbellis;v2 attached:

- adds test that fails pre-fix
- uses existing validateColumn instead of creating a new identical method
- moves validateColumns call out of validateColumn, and renames to validateColumnNames
- renames validateColumn to validateColumnData
- adds comments to main methods in TV","07/Mar/11 18:47;mishravivek;Jonathan,
Possible to add a svn patch file?",07/Mar/11 18:54;jbellis;That's what I attached.,"07/Mar/11 19:06;mishravivek;+1 for change.Modifying and reusing method is good thing.


Only thing is if we only column Family in validateColumnData() method. Then we can avoid : 

validateColumnData(keyspace, new ColumnParent(cfName), cosc.column); (creating ColumnParent instance). 
in validateColumnOrSuperColumn() method.","08/Mar/11 16:48;jbellis;committed w/ your improvement, thanks!","08/Mar/11 17:06;hudson;Integrated in Cassandra-0.7 #357 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/357/])
    validate column values in batches aswell assingle-Column inserts
patch by Vivek Mishra and jbellis for CASSANDRA-2259
",08/Mar/11 17:28;mishravivek;Should i mark it as resolve?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cfstats shows deleted cfs,CASSANDRA-1334,12470463,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,arya,arya,7/29/2010 19:00,3/12/2019 14:13,3/13/2019 22:24,8/3/2010 20:14,0.7 beta 1,,,,0,,,,,,"cfstats shows deleted CFs inside the keyspace after the CF was deleted from the keyspace using thrift service call system_drop_column_family

Steps to Reproduce:
1. Setup a 3 node cluster with clean slate from trunc;
2. create a keyspace with rf=2 and a standard cf using service call system_add_keyspace
3. create another cf with system_add_column_family
4. batch_mutate some rows into the new column family you created in step 3
5. call describe_keyspace to get a list of cfs inside your KS
6. iterate through the result and call system_drop_column_family for each
7. look at cfstats result. it is still showing the very first cf we create in step 2 in the list","CentOS 5.2
trunk",,,,,,,,,,,,,,,,,,,03/Aug/10 19:53;gdusbabek;0001-unregiter-CF-mbeans-when-a-CF-is-dropped.patch;https://issues.apache.org/jira/secure/attachment/12451149/0001-unregiter-CF-mbeans-when-a-CF-is-dropped.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,56:13.5,,,no_permission,,,,,,,,,,,,20087,,,Wed Aug 04 13:25:31 UTC 2010,,,,,,0|i0g4fb:,92147,,,,,,,,,,,"03/Aug/10 19:56;jbellis;should we add a CFS.getMBeanName to remove the potential for that code getting out of sync w/ the register?

+1 otherwise",03/Aug/10 20:02;gdusbabek;yeah. that makes good sense.  I'll put it in when I commit.,"04/Aug/10 13:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    unregister CF mbean when a CF is dropped. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1334
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blank listen_address/rpc_address no longer binds based on hostname,CASSANDRA-1394,12471650,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,brandon.williams,brandon.williams,8/15/2010 20:11,3/12/2019 14:13,3/13/2019 22:24,8/16/2010 16:49,0.7 beta 2,,,,0,,,,,,"With the switch to yamlbeans, if I make listen_address or rpc_address blank, they bind to localhost.  Instead, they should bind to whatever hostname and /etc/hosts point at.  This makes it much easier to use a unified config across a cluster.  Also, yamlbeans doesn't respect the 'null' keyword, which should be the same as a blank value, but is instead treated as a literal string.  This makes programmatic generation of the config file difficult.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,49:15.2,,,no_permission,,,,,,,,,,,,20115,,,Tue Aug 17 12:59:50 UTC 2010,,,,,,0|i0g4sf:,92206,,,,,,,,,,,16/Aug/10 16:49;jbellis;reverted the yamlbeans switch.,"17/Aug/10 12:59;hudson;Integrated in Cassandra #516 (See [https://hudson.apache.org/hudson/job/Cassandra/516/])
    revert switch to yamlbeans.  patch by jbellis; reviewed by Jon Hermes for CASSANDRA-1394
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle mmapping index files greater than 2GB,CASSANDRA-669,12444719,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,1/5/2010 15:57,3/12/2019 14:13,3/13/2019 22:24,1/7/2010 2:58,0.6,,,,0,,,,,,"""Who would ever have an index file larger than 2GB?"" I thought.  Turns out it's not that hard with narrow rows... :)",,,,,,,,,,,,,,,,,,,,06/Jan/10 21:43;jbellis;ASF.LICENSE.NOT.GRANTED--0001-store-data-information-for-any-index-entries-spanning-.txt;https://issues.apache.org/jira/secure/attachment/12429581/ASF.LICENSE.NOT.GRANTED--0001-store-data-information-for-any-index-entries-spanning-.txt,06/Jan/10 21:43;jbellis;ASF.LICENSE.NOT.GRANTED--0002-add-support-for-multiple-mmapped-index-segments-and-ad.txt;https://issues.apache.org/jira/secure/attachment/12429582/ASF.LICENSE.NOT.GRANTED--0002-add-support-for-multiple-mmapped-index-segments-and-ad.txt,06/Jan/10 21:43;jbellis;ASF.LICENSE.NOT.GRANTED--0003-instead-of-providing-a-RandomAccessFile-like-interface.txt;https://issues.apache.org/jira/secure/attachment/12429583/ASF.LICENSE.NOT.GRANTED--0003-instead-of-providing-a-RandomAccessFile-like-interface.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,21:04.0,,,no_permission,,,,,,,,,,,,19811,,,Fri Jan 08 12:38:35 UTC 2010,,,,,,0|i0g0cv:,91488,,,,,,,,,,,"05/Jan/10 21:14;jbellis;02
    add support for multiple mmapped index segments, and add mmap_index_only option

01
    store data information for any index entries spanning a mmap segment boundary when reading the index (with a BufferedRAF)
","05/Jan/10 23:21;brandon.williams;Received the following traceback after testing (during compaction):

ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Negative position
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:53)
        at org.apache.cassandra.db.CompactionManager$CompactionExecutor.afterExecute(CompactionManager.java:591)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.IllegalArgumentException: Negative position
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:739)
        at org.apache.cassandra.io.SSTableReader.mmap(SSTableReader.java:273)
        at org.apache.cassandra.io.SSTableReader.<init>(SSTableReader.java:234)
        at org.apache.cassandra.io.SSTableWriter.closeAndOpenReader(SSTableWriter.java:157)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:307)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:102)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:83)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
","06/Jan/10 03:54;jbellis;heh, that's actually a 2nd bug.  patch 03 attached to fix.","06/Jan/10 05:02;jbellis;fix the -- in the xml in 02, and change BUFFER_SIZE to long in 03 as well to force the promotion of the position parameter.",06/Jan/10 15:35;brandon.williams;+1,"06/Jan/10 16:07;brandon.williams;Oops, nevermind my +1, receiving the following traceback when trying to get_slice now:

Caused by: java.lang.AssertionError
        at org.apache.cassandra.io.util.MappedFileDataInput.length(MappedFileDataInput.java:36)
        at org.apache.cassandra.io.util.MappedFileDataInput.read(MappedFileDataInput.java:52)
        at java.io.InputStream.read(InputStream.java:171)
        at org.apache.cassandra.io.util.MappedFileDataInput.readUnsignedShort(MappedFileDataInput.java:358)
        at java.io.DataInputStream.readUTF(DataInputStream.java:589)
        at org.apache.cassandra.io.util.MappedFileDataInput.readUTF(MappedFileDataInput.java:381)
        at org.apache.cassandra.io.SSTableReader.getPosition(SSTableReader.java:419)
        at org.apache.cassandra.io.SSTableReader.getFileDataInput(SSTableReader.java:537)
        at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:54)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamilyInternal(ColumnFamilyStore.java:859)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:817)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:786)
        at org.apache.cassandra.db.Table.getRow(Table.java:405)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:59)
        at org.apache.cassandra.service.StorageProxy$weakReadLocalCallable.call(StorageProxy.java:692)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        ... 3 more
",06/Jan/10 17:24;jbellis;patch 04 to fix.,"06/Jan/10 21:44;jbellis;... latest patches fix the fd leak and index reading bugs, and rebased bugfixes into 01 and 02.","07/Jan/10 02:50;brandon.williams;+1, for real this time.  40M narrow keys used to create a 2.2GB index when fully compacted, all is well.",07/Jan/10 02:58;jbellis;committed,"07/Jan/10 13:08;hudson;Integrated in Cassandra #316 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/316/])
    instead of providing a RandomAccessFile-like interface in FileDataInput implementing seek and trying to keep people from shooting themselves in the foot by forgetting that it may only represent a 2GB segment of a larger file, provide an InputStream-like interface emphasizing mark/reset
patch by jbellis; tested by Brandon Williams for 
add support for multiple mmapped index segments, and add mmap_index_only option
patch by jbellis; tested by Brandon Williams for 
store data information for any index entries spanning a mmap segment boundary when reading the index (with a BufferedRAF) so we don't have to deal with that at read time.
patch by jbellis; tested by Brandon Williams for 
","08/Jan/10 12:38;hudson;Integrated in Cassandra #317 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/317/])
    fix bad rebase causing regression in the  patchset (use index path to open index file).  patch by jbellis
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
refactor system tests to accommodate avro,CASSANDRA-812,12456849,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,2/19/2010 22:15,3/12/2019 14:13,3/13/2019 22:24,2/19/2010 22:29,0.7 beta 1,,,,0,,,,,,The patches that follow refactor the existing functional tests in order to better accommodate functional tests for avro.,,,,,,,,,,,,,,,,,,,,19/Feb/10 22:16;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-812-stubbed-out-functional-tests-from-avro.txt;https://issues.apache.org/jira/secure/attachment/12436389/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-812-stubbed-out-functional-tests-from-avro.txt,19/Feb/10 22:16;urandom;ASF.LICENSE.NOT.GRANTED--v1-0002-renamed-thrift-tests-for-consistency.txt;https://issues.apache.org/jira/secure/attachment/12436390/ASF.LICENSE.NOT.GRANTED--v1-0002-renamed-thrift-tests-for-consistency.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,24:03.9,,,no_permission,,,,,,,,,,,,19876,,,Sat Feb 20 12:38:43 UTC 2010,,,,,,0|i0g187:,91629,,,,,,,,,,,"19/Feb/10 22:19;urandom;Running these requires the Python module for avro, (works-for-me against trunk as of today).",19/Feb/10 22:24;jbellis;+1,19/Feb/10 22:29;urandom;committed.,"20/Feb/10 12:38;hudson;Integrated in Cassandra #361 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/361/])
    renamed thrift tests for consistency

Patch by eevans for 
 stubbed out functional tests from avro

Patch by eevans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming error on bootstrap,CASSANDRA-546,12440508,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,rays,rays,11/12/2009 16:17,3/12/2019 14:13,3/13/2019 22:24,11/13/2009 2:32,0.5,,,,0,,,,,,"Received the following error while bootstrapping a new node:

DEBUG - Adding stream context /usr/local/cassandra-trunk/data/data/Mahalo/VideosInSection-tmp-1-Data.db:4096052 for /10.1.10.198 ...
DEBUG - Sending a stream initiate done message ...
WARN - Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.2.4.114:7000 remote=/2xx.2xx.194.2xx:55515]
WARN - Exception was generated at : 11/12/2009 15:30:38 on thread MESSAGING-SERVICE-POOL:3
Streaming context has not been set.
java.lang.IllegalStateException: Streaming context has not been set.
	at org.apache.cassandra.net.io.StreamContextManager.getStreamContext(StreamContextManager.java:264)
	at org.apache.cassandra.net.io.ContentStreamState.<init>(ContentStreamState.java:47)
	at org.apache.cassandra.net.io.ProtocolHeaderState.morphState(ProtocolHeaderState.java:66)
	at org.apache.cassandra.net.io.StartState.doRead(StartState.java:48)
	at org.apache.cassandra.net.io.ProtocolHeaderState.read(ProtocolHeaderState.java:39)
	at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:96)
	at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:427)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)

full log ---> http://pastie.org/695593","FreeBSD 7.2-RELEASE amd64 
Diablo Java HotSpot(TM) 64-Bit Server VM (build 10.0-b23, mixed mode)",,,,,,,,,,,,,,,,,,,12/Nov/09 22:48;jbellis;546.patch;https://issues.apache.org/jira/secure/attachment/12424784/546.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:21.0,,,no_permission,,,,,,,,,,,,19748,,,Fri Nov 13 12:35:04 UTC 2009,,,,,,0|i0fzlj:,91365,,,,,,,,,,,"12/Nov/09 18:58;jbellis;this was fixed by 538, but a new one was introduced: http://pastie.org/695859","12/Nov/09 21:15;rays;applied patch and get the below error, which is (from what I can tell) the same as the first error.

WARN - Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.2.4.114:7000 remote=/7x.6x.2xx.1xx:63447]
WARN - Exception was generated at : 11/12/2009 20:36:42 on thread MESSAGING-SERVICE-POOL:3
Streaming context has not been set.
java.lang.IllegalStateException: Streaming context has not been set.
	at org.apache.cassandra.net.io.StreamContextManager.getStreamContext(StreamContextManager.java:264)
	at org.apache.cassandra.net.io.ContentStreamState.<init>(ContentStreamState.java:47)
	at org.apache.cassandra.net.io.ProtocolHeaderState.morphState(ProtocolHeaderState.java:66)
	at org.apache.cassandra.net.io.StartState.doRead(StartState.java:48)
	at org.apache.cassandra.net.io.ProtocolHeaderState.read(ProtocolHeaderState.java:39)
	at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:96)
	at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:427)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)

svn diff output: http://pastie.org/696251",12/Nov/09 21:35;jbellis;difficult to disentangle this from CASSANDRA-541.  moving fixes there.,"12/Nov/09 22:28;jbellis;ok, this is separate from the generic bootstrap/move bugs

the problem is, Ray has two interfaces and mostly Cassandra uses the internal one, but bootstrap is trying to stream over the external one which confuses things.

with the debug info from CASSANDRA-541 this is clear:

DEBUG - Sending BootstrapMetadataMessage to /10.1.10.198 for (68939025851256836916907001051563673941,85173388956504742541769293679392562704]
ERROR - java.lang.IllegalStateException: Streaming context has not been set for /74.6x.2xx.1xx
",12/Nov/09 22:48;jbellis;attempt to force correct local address in bind,"13/Nov/09 02:32;jbellis;from irc:

raysl: looks like it's working

committed","13/Nov/09 12:35;hudson;Integrated in Cassandra #257 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/257/])
    force bind to correct address
patch by jbellis; reviewed by Ray Slakinski for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift swallows certain classes of network errors,CASSANDRA-72,12422577,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,4/10/2009 15:21,3/12/2019 14:13,3/13/2019 22:24,4/15/2009 16:36,0.3,,,,0,,,,,,"Thrift logs network errors via java.util.logging, but we use log4j instead.  I've submitted a patch to make thrift use log4j too.  When that goes in we should upgrade.",,,,,,,,,,,,,,,,,,,,14/Apr/09 21:45;jbellis;72.patch;https://issues.apache.org/jira/secure/attachment/12405467/72.patch,14/Apr/09 21:49;jbellis;libthrift.jar;https://issues.apache.org/jira/secure/attachment/12405468/libthrift.jar,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,22:25.0,,,no_permission,,,,,,,,,,,,19534,,,Wed Apr 15 16:36:10 UTC 2009,,,,,,0|i0fwpb:,90896,,,,,,,,,,,"14/Apr/09 21:45;jbellis;upgrade to thrift svn r763981.  (for Java, this should be identical to thrift 0.1.)  the only changes are regenerating service/ thrift code and s|com.facebook.thrift/org.apache/thrift|.",15/Apr/09 16:22;urandom;This all looks good to me. +1,15/Apr/09 16:36;jbellis;applied,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE on system_update_cf when adding an index to a column without existing metadata,CASSANDRA-1764,12480549,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,thobbs,thobbs,11/22/2010 1:34,3/12/2019 14:13,3/13/2019 22:24,11/23/2010 19:14,0.7.0 rc 2,,,,0,,,,,,"When trying to create a secondary index using system_update_column_family(), if you try to add an index on a column that does not already have an existing entry in the CfDef's column_metadata, a NullPointerException is thrown.

Looks like the logic in o.a.c.config.CFMetaData.apply() is faulty.  Specifically, creating a toUpdate Set (similar to the toAdd and toDelete) sets and using that for the loop ~ line 663 would fix this.",Cassandra 0.7 branch,,,,,,,,,,,,,,,,,,,23/Nov/10 18:49;thobbs;1764-test-v2.txt;https://issues.apache.org/jira/secure/attachment/12460292/1764-test-v2.txt,22/Nov/10 01:38;thobbs;1764-test.txt;https://issues.apache.org/jira/secure/attachment/12460152/1764-test.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,04:17.7,,,no_permission,,,,,,,,,,,,20300,,,Sat Dec 11 07:35:19 UTC 2010,,,,,,0|i0g78v:,92604,,,,,,,,,,,23/Nov/10 17:04;gdusbabek;+1.  test passes after applying the fix for CASSANDRA-1768.,23/Nov/10 17:13;gdusbabek;reopening to track committing the test.,23/Nov/10 19:14;gdusbabek;committed.,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Review uses of FileStruct to make sure they are using decorated or raw keys correctly,CASSANDRA-67,12422430,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,4/9/2009 1:41,3/12/2019 14:13,3/13/2019 22:24,4/9/2009 21:41,,,,,0,,,,,,"Jun Rao commented in #58,

The problem is that FileStruct.key_ is referenced directly in 4 places. At least 2 of those places assume key_ to be the real key, instead of decorated key. These 2 places are in
ColumnFamilyStore.doFileAntiCompaction() (key_ is assigned to lastkey, which is used in isKeyInRanges)
ColumnFamilyStore.doFileCompaction()

In the above places, key_ has to be undeocrated first. Also, we need to make key_ private in FileStruct and use getKey() for referencing.
",,,,,,,,,,,,,,,,,,,,09/Apr/09 02:06;jbellis;67-v2.patch;https://issues.apache.org/jira/secure/attachment/12405031/67-v2.patch,09/Apr/09 18:22;jbellis;67-v3.patch;https://issues.apache.org/jira/secure/attachment/12405088/67-v3.patch,09/Apr/09 18:36;jbellis;67-v4.patch;https://issues.apache.org/jira/secure/attachment/12405090/67-v4.patch,09/Apr/09 21:23;jbellis;67-v5.patch;https://issues.apache.org/jira/secure/attachment/12405104/67-v5.patch,09/Apr/09 02:00;jbellis;67.patch;https://issues.apache.org/jira/secure/attachment/12405030/67.patch,,,,,,,,,5,,,,,,,,,,,,,,,,,,,25:22.7,,,no_permission,,,,,,,,,,,,19532,,,Thu Apr 09 21:41:03 UTC 2009,,,,,,0|i0fwo7:,90891,,,,,,,,,,,09/Apr/09 02:00;jbellis;Clean up FileStruct and make it iterable.  This improves the API and will also be necessary for range queries.,"09/Apr/09 02:06;jbellis;Undecorate FS.key when calling isKeyInRanges per Jun's findings.

Note that the rest of anticompaction (and compaction) assume they are dealing with decorated keys, so that (and FS.key) are left alone.","09/Apr/09 17:25;junrao;Reviewed this patch. Here are the comments.

1. FileStruct.getNextKey() should throw IOException (instead of RuntimeException) and let callers deal with it.

2. FileStruct.SeekTo() is not used.

3. FileStruct.iterator() gives user the impression that one can open up multiple independent iterators, but it is not.

4. In the new SSTable format, the block indexes are stored at the end of the file. If you encounter a blockindex key, you can be sure that you will never see a real key afterward.
So, need to change what FileStruct.getNextKey() does when incurring blockindex key.
","09/Apr/09 18:22;jbellis;1. Agreed.

2. It's going to be used by the range patch, but okay, to be consistent I will take it out of this one. :)

3. Added comment warning that iterators are not independent.

4. Good catch!  I was just going off the old advance() method and didn't notice that.  So when I get to the blockindex key I will treat it as if it were EOF.

v3 patch attached.","09/Apr/09 18:26;junrao;3. Wouldn't it be better if FileStruct implements iterator interface directly, if there should be only 1 iterator expected on FileStruct?","09/Apr/09 18:36;jbellis;Yes.  v4 attached.

Also, to avoid confusion with the next() method from Iterator, renamed getNextKey() back to advance().",09/Apr/09 19:07;junrao;FileStructIterator seems unnecessary. It seems it's better to fold what's in FileStructIterator to FileStruct itself.,09/Apr/09 19:08;jbellis;Disagree.  It's cleaner to not have it in the main namespace.,"09/Apr/09 21:23;jbellis;version w/o iteration code, per IRC comments.","09/Apr/09 21:37;junrao;comments on v5:
1. remove the following unreferenced pacakges in FileStruct
java.util.Iterator
sun.reflect.generics.reflectiveObjects.NotImplementedException

2. occasional triggers the following failure in test. Not always reproducible. Wonder if it's another timing issue here.
   [testng] FAILED: testGetCompactionBuckets
   [testng] java.lang.AssertionError
   [testng]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testGetCompactionBuckets(ColumnFamilyStoreTest.java:289)
   [testng] ... Removed 22 stack frames
   [testng]

Other than the above, the patch looks fine.
","09/Apr/09 21:41;jbellis;buckets test exception is unrelated.  there is a separate ticket open for that.

committed w/ unused imports removed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
possible NPE in StorageProxy?,CASSANDRA-631,12443176,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,riffraff,riffraff,riffraff,12/12/2009 12:27,3/12/2019 14:13,3/13/2019 22:24,12/12/2009 14:50,0.5,0.6,,,0,,,,,,"insert() in StorageProxy contains a logging statement that refers to a possibly un-initialized variable
{{{
logger.debug(""insert writing key "" + rm.key() + "" to "" + unhintedMessage.getMessageId() + ""@"" + hintedTarget + "" for "" + target);
}}}

this could happen if getHintedEndpointMap(rm.key(), naturalEndpoints) returns only elements for which target.equals(hintedTarget) returns false, which seems possible to me. 

Looking at the code I get the feeling the reference should probably be to 'hintedMessage', instead of ""unhintedMessage"", if not so an 
assert statement could be appropriate",all,,600,600,,0%,600,600,,,,,,,,,,,,12/Dec/09 12:41;riffraff;CASSANDRA-631-big.patch;https://issues.apache.org/jira/secure/attachment/12427820/CASSANDRA-631-big.patch,12/Dec/09 12:41;riffraff;CASSANDRA-631-tiny.patch;https://issues.apache.org/jira/secure/attachment/12427819/CASSANDRA-631-tiny.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,50:17.1,,,no_permission,,,,,,,,,,,,19790,,,Thu Dec 17 22:03:03 UTC 2009,,,,,,0|i0g04f:,91450,,,,,,,,,,,"12/Dec/09 12:41;riffraff;changes the hunhinted ->hinted as per summary. 

The -tiny patch only does that change .

The -big patch also removes other compile warnings from the file (generics, synthetic accessors) and avoids using useless allocations (Collections.max(Arrays.AsList(new ary[a,b])) seems unnecessary when there is only the need to compare two objects and commons-lang already provides a good enough method)","12/Dec/09 12:43;riffraff;forgot to say: with both patches I have a tet failure 

    [junit] Testcase: testImportSuperCf(org.apache.cassandra.tools.SSTableImportTest):	Caused an ERROR
    [junit] Invalid localDeleteTime read: -2099059532
    [junit] java.io.IOException: Invalid localDeleteTime read: -2099059532
    [junit] 	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:368)
    [junit] 	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:1)
    [junit] 	at org.apache.cassandra.db.filter.SSTableNamesIterator.<init>(SSTableNamesIterator.java:103)
    [junit] 	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
    [junit] 	at org.apache.cassandra.tools.SSTableImportTest.testImportSuperCf(SSTableImportTest.java:63)

at r889834 I also see it without the patches though :)","12/Dec/09 14:50;jbellis;applied -tiny to 0.5 and trunk, good catch.

-big removes private from a bunch of variables for no reason, so left that part out, but applied the generics fixes to trunk (although fixing generics only to add in an ObjectUtils call requiring a cast seems a little schizophrenic to me; saving a 2-element array allocation in get_range_slice is like picking up a teaspoon of sand from the seashore :)","12/Dec/09 18:30;riffraff;Sorry, I wrote it in the patch comment but I failed to express myself clearly: in my setup I have basically all warnings turned on, and for access of private fields from inner classes there are some performance-related ones (the need for compiler-generated accessor methods) which are not present in case of package visibility. 

It's a silly reason but I just use all possible warnings (+pmd, +findbugs) in my code, and this happen to be in the set, I'll just turn this off if it seems unnecessary. Shall I update the CodeStyle wiki page to point this out?

Same for generics, mostly a warning removal thingy. 
Regarding ObjectUtils, we save an array, a list and an iterator object instantiation, so it's 3 spoons! (that should be probably optimized away with escape analysis anyway :)

Thanks for applying the patch. ","12/Dec/09 20:39;jbellis;Interesting, I didn't know about that.

But if it's actually in a performance sensitive place, wouldn't the JIT inlining will take care of it?","13/Dec/09 09:47;riffraff;you can check it with JAD if you disable the inner class analisys, the code 
{{{
public class C {
  private static int var = 10;
  public static class Inner {
    public int meth(){
      return var;
    }
  }
}
}}}

gives back from the C.class 
{{{
public class C
{

    public C()
    {
    }

    static int access$000()
    {
        return var;
    }

    private static int var = 10;

}

}}}

and for C$Inner.class
{{{
public class C$Inner
{

    public C$Inner()
    {
    }

    public int meth()
    {
        return C.access$000();
    }
}
}}}

I _believe_ this is because inner classes had to be fitted over an existing class format in java 1.1, but I'm not sure. 


I agree that these method are most probably going to be optimized away, as I said I just changed the code because they are in the default warnings set  and I can simply change my warnings setup (and eventually update the CodeStyle wiki so the next person who comes across them will know it).","17/Dec/09 22:03;hudson;Integrated in Cassandra #291 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/291/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyOutputFormat drops mutations when batches fill up.,CASSANDRA-2255,12499973,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jeromatron,eldondev,eldondev,2/28/2011 21:06,3/12/2019 14:13,3/13/2019 22:24,3/1/2011 2:52,0.7.3,,,,1,,,,,,"queue.poll() takes a mutation,
but then the batch is already full,
so the while loop exits, ant the mutation we just got is dropped.",,,,,,,,,,,,,,,,,,,,28/Feb/11 21:22;eldondev;0001_Stop_dropping_mutations.txt;https://issues.apache.org/jira/secure/attachment/12472233/0001_Stop_dropping_mutations.txt,01/Mar/11 00:35;jeromatron;2255-patch-2.txt;https://issues.apache.org/jira/secure/attachment/12472260/2255-patch-2.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,32:22.8,,,no_permission,,,,,,,,,,,,20528,,,Tue Mar 01 04:15:26 UTC 2011,,,,,,0|i0gaa7:,93096,jbellis,jbellis,,,,,,,,,28/Feb/11 21:07;eldondev;This should help.,28/Feb/11 21:22;eldondev;Should apply cleanly to 0.7.0 tag,"28/Feb/11 21:32;brandon.williams;Committed, thanks!","28/Feb/11 21:56;hudson;Integrated in Cassandra-0.7 #332 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/332/])
    CFRW no longer loses mutations.
Patch by Eldon Stegall, reviewed by Stu Hood and brandonwilliams for
CASSANDRA-2255
",28/Feb/11 22:55;jbellis;Doesn't this mean batch sizes can be arbitrarily large?  Not only will this cause latency spikes on the server but you could OOM building the monster batch.,28/Feb/11 22:56;jbellis;also: the mutation = null; line is dead code.,28/Feb/11 23:42;jbellis;reverted,01/Mar/11 00:34;jeromatron;another go at the patch,"01/Mar/11 02:52;jbellis;lgtm, committed (we'll try to get it in to 0.7.3 re-spin)","01/Mar/11 04:15;hudson;Integrated in Cassandra-0.7 #334 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/334/])
    fix Hadoop ColumnFamilyOutputFormat droppingof mutations
patch by Eldon Stegall and Jeremy Hanna; reviewed by jbellis for CASSANDRA-2255
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Major compaction still leaves large set of files,CASSANDRA-473,12437407,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,10/6/2009 15:46,3/12/2019 14:13,3/13/2019 22:24,10/7/2009 19:59,0.4,,,,0,,,,,,"We did a major compaction on roughly 1000-2000 files. The disk drive had a capacity of 1.6TB. The disk usage with Cassandra was 1.1TB. I saw this error, maybe this is why compaction did not finish? Attaching system.log",,,,,,,,,,,,,,,,,,,,06/Oct/09 17:46;jbellis;473.patch;https://issues.apache.org/jira/secure/attachment/12421447/473.patch,06/Oct/09 15:47;lenn0x;system.log;https://issues.apache.org/jira/secure/attachment/12421434/system.log,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,46:21.7,,,no_permission,,,,,,,,,,,,19708,,,Thu Oct 08 12:35:14 UTC 2009,,,,,,0|i0fz5j:,91293,,,,,,,,,,,"06/Oct/09 17:46;jbellis;okay, so here's what is going on.  there is a feature involved, and a bug :)

the feature is that you will end up with multiple files if you try to major compact but don't have enough room.  cassandra can't r/m the old files until the new ones are finished (in the worst case), so it will cut down the compaction set to something it knows will fit in the remaining space.

the bug is that using subList in doCompaction is causing the java.util.Collections$UnmodifiableCollection.remove error, since like arrays.asList, the list objects returned by subList don't support remove().","07/Oct/09 19:47;lenn0x;This patch worked. It removed the old files, 200+ down to 32. (technically 8 data files)

+1",07/Oct/09 19:59;jbellis;committed to 0.4 and trunk,"08/Oct/09 12:35;hudson;Integrated in Cassandra #221 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/221/])
    create new collection when reducing the number of sstables compacted; the lists returned by subList are unmodifiable
patch by jbellis; reviewed by goffinet for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException when updating column family metadata,CASSANDRA-1736,12479813,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,bterm,bterm,11/12/2010 17:31,3/12/2019 14:13,3/13/2019 22:24,11/23/2010 15:26,0.7.0 rc 1,,,,0,,,,,,"From cli
> update column family Tweet with column_metadata=[{column_name:state, validation_class:UTF8Type}]
> set Tweet [x][state] = TX
> get Tweet where state = TX
No index columns present
> update column family Tweet with column_metadata=[{column_name:state, index_type:0, validation_class:UTF8Type}]
null
> list Tweet
java.net.SocketException: Broken pipe


ERROR [MigrationStage:1] 2010-11-12 09:12:28,618 AbstractCassandraDaemon.java (line 90) Fatal exception in thread Thread[Migra$
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$KeyIterator.next(HashMap.java:828)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1495)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.beforeApplyModels(UpdateColumnFamily.java:76)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:109)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:672)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [pool-1-thread-5] 2010-11-12 09:12:28,636 CustomTThreadPoolServer.java (line 175) Thrift error occurred during processin$
org.apache.thrift.protocol.TProtocolException: Required field 'why' was not present! Struct: InvalidRequestException(why:null)
        at org.apache.cassandra.thrift.InvalidRequestException.validate(InvalidRequestException.java:340)
        at org.apache.cassandra.thrift.InvalidRequestException.write(InvalidRequestException.java:309)
        at org.apache.cassandra.thrift.Cassandra$system_update_column_family_result.write(Cassandra.java:26764)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:3605)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,,,,,18/Nov/10 02:41;jbellis;1736-v2.txt;https://issues.apache.org/jira/secure/attachment/12459872/1736-v2.txt,14/Nov/10 19:33;jbellis;1736.txt;https://issues.apache.org/jira/secure/attachment/12459568/1736.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,06:27.6,,,no_permission,,,,,,,,,,,,20282,,,Sat Nov 20 03:12:00 UTC 2010,,,,,,0|i0g72n:,92576,gdusbabek,gdusbabek,,,,,,,,,"13/Nov/10 03:06;jbellis;This is a regression caused by treating the compaction marker as a component (CASSANDRA-1471, CASSANDRA-1544).  The CME comes when compaction modifies the components set while snapshot is iterating over them; since snapshot begins by flushing, this is actually fairly likely to happen (especially with non-JNA snapshots, i.e., slow ones).

IMO the best fix to this would be to stop treating the compaction marker as a component and make the components set Unmodifiable to close off that avenue of bugs in the future.","13/Nov/10 03:15;jbellis;One reason I don't think compaction marker belongs in components is that as this bug highlights, we could end up with the marker as part of a snapshot.  Which would cause a more subtle bug.  Here is the order of events:

{code}
My CFS has sstables A B C.
Flush introduces sstable D.
We begin compacting A B C D.
We begin iterating A B C D for snapshot.
During the iteration, compaction finishes producing sstable E.  A B C D are marked compacted.
snapshot finishes, with (say) C D marked compacted.
{code}

Now, the sstable tracker guarantees that we see a consistent view of the sstables -- we will either exactly one of  {A B C D} or {E}.  But by mixing the compaction marker in as a component we now have a snapshot that implies that A and B were live but C and D were compacted, and if we take that snapshot as-is and promote it to live data, when we restart Cassandra will purge C and D since they were marked compacted.

We could band-aid this in a number of ways but I think the less fragile approach is to treat the compaction marker as something separate from components.",14/Nov/10 19:33;jbellis;patch along the above lines,"15/Nov/10 18:39;stuhood;This patch adds the isCompacted method, but it isn't called anywhere, which raises the question: who might call that method in the future, or care that an SSTable is compacted?

Also, SSTable.delete() will fail if the SSTable is not marked compacted, for instance if the second branch of the 'if' in CFStore is triggered.","15/Nov/10 19:49;jbellis;bq. This patch adds the isCompacted method, but it isn't called anywhere

I will remove it.

bq. SSTable.delete() will fail if the SSTable is not marked compacted

If you're referring to 

{code}
            FileUtils.delete(desc.filenameFor(Component.COMPACTED_MARKER));
{code}

I changed that to not use deleteWithConfirm for exactly that reason.",18/Nov/10 02:37;jbellis;v2 removes boolean and replaces with assert that we're not instantiating a compacted SSTableReader,19/Nov/10 17:19;gdusbabek;+1,"19/Nov/10 20:05;hudson;Integrated in Cassandra-0.7 #19 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/19/])
    fix race between snapshot andcompaction
patch by jbellis; reviewed by gdusbabek for CASSANDRA-1736
","20/Nov/10 03:12;hudson;Integrated in Cassandra-0.7 #22 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/22/])
    avoid attempting to delete compacted sstables twice on restart
patch by jbellis to fix regression introduced by CASSANDRA-1736
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race conditions when reinitialisating nodes (OOM + Nullpointer),CASSANDRA-2228,12499491,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,tbritz,tbritz,2/23/2011 15:50,3/12/2019 14:13,3/13/2019 22:24,3/3/2011 22:59,0.7.4,,,,0,,,,,,"I had a corrupt system table which wouldn't compact anymore and I deleted the files and restarted cassandra and let it take the same token/ip address.

I experienced the same errors when I'm adding a newly installed node under the same token/ip address before calling repair.

1)
After a few seconds/minutes, I get a OOM error:


 INFO [FlushWriter:1] 2011-02-23 16:40:28,958 Memtable.java (line 164) Completed flushing /cassandra/data/system/Schema-f-15-Data.db (8037 bytes)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,965 Migration.java (line 133) Applying migration 3e30e76b-1e3f-11e0-8369-5a9c1faed4ae Add keyspace: table_userentriesrep factor:3rep strategy:SimpleStrategy{org.apache.cassandra.config.CFMetaData@58925d9[cfId=1024,tableName=table_userentries,cfName=table_userentries,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType@b44dff0,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=200000.0,readRepairChance=0.0,gcGraceSeconds=86400,defaultValidator=org.apache.cassandra.db.marshal.BytesType@b44dff0,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=3600,memtableFlushAfterMins=60,memtableThroughputInMb=64,memtableOperationsInMillions=10.0,column_metadata={}], org.apache.cassandra.config.CFMetaData@11ab7246[cfId=1025,tableName=table_userentries,cfName=table_userentries_meta,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType@b44dff0,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=200000.0,readRepairChance=0.0,gcGraceSeconds=86400,defaultValidator=org.apache.cassandra.db.marshal.BytesType@b44dff0,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=3600,memtableFlushAfterMins=60,memtableThroughputInMb=64,memtableOperationsInMillions=10.0,column_metadata={}]}
 INFO [MigrationStage:1] 2011-02-23 16:40:28,965 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for Migrations at CommitLogContext(file='/cassandra/commitlog/CommitLog-1298475572022.log', position=226075)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,966 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-Migrations@2121008793(12529 bytes, 1 operations)
 INFO [FlushWriter:1] 2011-02-23 16:40:28,966 Memtable.java (line 157) Writing Memtable-Migrations@2121008793(12529 bytes, 1 operations)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,966 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for Schema at CommitLogContext(file='/cassandra/commitlog/CommitLog-1298475572022.log', position=226075)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,967 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-Schema@139610466(8370 bytes, 15 operations)
 INFO [ScheduledTasks:1] 2011-02-23 16:40:28,972 StatusLogger.java (line 89) table_sourcedetection.table_sourcedetection                 0,0                 0/0            0/200000
ERROR [FlushWriter:1] 2011-02-23 16:41:01,240 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[FlushWriter:1,5,main]
java.lang.OutOfMemoryError: Java heap space
        at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:39)
        at java.nio.ByteBuffer.allocate(ByteBuffer.java:312)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:126)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:75)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:158)
        at org.apache.cassandra.db.Memtable.access$000(Memtable.java:51)
        at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:176)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)





2) If I restart then, I'm getting an Nullpointer exception. The OOM error will only appear once.

ERROR [main] 2011-02-23 16:42:32,782 AbstractCassandraDaemon.java (line 333) Exception encountered during startup.
java.lang.NullPointerException
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:768)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:925)
        at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:105)
        at org.apache.cassandra.service.MigrationManager.applyMigrations(MigrationManager.java:161)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:185)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)


Killing and restarting the node multiple times will eventually ""fix"" these errors.


Steps to reproduce. Remove complete data directory and restart node with same token/ip.

",,,,,,,,,,,,,,,,,,,,03/Mar/11 21:32;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-initialize-localEndpoint_.txt;https://issues.apache.org/jira/secure/attachment/12472612/ASF.LICENSE.NOT.GRANTED--v1-0001-initialize-localEndpoint_.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:19.4,,,no_permission,,,,,,,,,,,,20516,,,Thu Mar 03 23:32:31 UTC 2011,,,,,,0|i0ga47:,93069,,,,,,,,,,,"23/Feb/11 15:56;tbritz;The normal heap usage of the node is about half of the allocated heap:

Gossip active    : true
Load             : 30.99 GB
Generation No    : 1298476238
Uptime (seconds) : 308
Heap Memory (MB) : 1544.41 / 2493.38
","23/Feb/11 16:58;jbellis;The OOM is because flushing allocates a buffer the size of in_memory_compaction_limit.  Sounds like you need to lower that.

The NPE looks like a bug.",02/Mar/11 20:58;gdusbabek;What is listen_address in cassandra.yaml?  I'm having a hard time conceiving of a way for FBUtilities.getLocalAddress() to return null.,"02/Mar/11 20:59;thibaut.britz@trendiction.com;Hi,

I'm on vacation until March 14th.

For urgent matters, please contact:
Christophe Folschette (christophe.folschette@trendiction.com)
+352 20 33 35 32
","03/Mar/11 16:09;jbellis;It looks like we're doing Migration stuff before Gossiper.start is called.

One possible solution: initialize Gossiper.localEndpoint in constructor instead of in start.","03/Mar/11 17:14;gdusbabek;CHM throws NPE when the key is null.  To me, then, the bug is that FBU.getLocalAddress() is somehow returning null.  

I checked yesterday: even if local_address is unspecified, it ends up returning localhost.","03/Mar/11 17:18;jbellis;My point was that we can call put before initializing localEndpoint_ to FBU.gLA, so it's naturally going to be null.",03/Mar/11 20:46;gdusbabek;aha. I was looking in the trunk code (no chance of null).  I see the problem in the 0.7 branch.,03/Mar/11 22:03;jbellis;+1,03/Mar/11 22:59;gdusbabek;committed,"03/Mar/11 23:32;hudson;Integrated in Cassandra-0.7 #344 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/344/])
    initialize localendpoint in gossiper earlier. patch by gdusbabek, reviewed by jbellis. CASSANDRA-2228
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support starting an avro enabled node (experimental),CASSANDRA-811,12456836,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,2/19/2010 21:10,3/12/2019 14:13,3/13/2019 22:24,2/19/2010 21:33,0.7 beta 1,,,,0,,,,,,"The start script should support a  ""-a"" argument to make it possible to start an Avro enabled node. If/when Avro becomes a suitable replacement, the -a option can be dropped in favor of a -t option (as Avro becomes the deafult), which in turn can be dropped after Thrift is removed entirely.

The ThriftAddress and ThriftPort directives should also renamed to RPCAddress and RPCPort so that they can serve both RPC mechanism without creating confusion.

Patches to follow.",,,,,,,,,,,,,,,,,,,,19/Feb/10 21:11;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-811-use-less-specific-descriptors-for-Thrift.txt;https://issues.apache.org/jira/secure/attachment/12436372/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-811-use-less-specific-descriptors-for-Thrift.txt,19/Feb/10 21:11;urandom;ASF.LICENSE.NOT.GRANTED--v1-0002-start-avro-daemon-using-a-arg-to-startup-script.txt;https://issues.apache.org/jira/secure/attachment/12436373/ASF.LICENSE.NOT.GRANTED--v1-0002-start-avro-daemon-using-a-arg-to-startup-script.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,28:08.7,,,no_permission,,,,,,,,,,,,19875,,,Sat Feb 20 12:38:43 UTC 2010,,,,,,0|i0g17z:,91628,,,,,,,,,,,19/Feb/10 21:28;gdusbabek;+1 tested changes. correct daemon is run.,19/Feb/10 21:33;urandom;committed.,"20/Feb/10 12:38;hudson;Integrated in Cassandra #361 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/361/])
    release note mentioning renamed directives

Patch by eevans for 
start avro daemon using -a arg to startup script

Patch by eevans; reviewed by gdusbabek for 
use less specific descriptors for Thrift{Address,Port}

Patch by eevans; reviewed by gdusbabek for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exceptions after cleanup,CASSANDRA-1922,12494314,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,12/30/2010 17:29,3/12/2019 14:13,3/13/2019 22:24,12/30/2010 18:20,0.7.0,,,,0,,,,,,"It looks like CASSANDRA-1916 may have introduced a regression.  After running a cleanup, I get the following exception when trying to read:

{noformat}

ERROR 17:25:23,574 Fatal exception in thread Thread[ReadStage:99,5,main]
java.lang.AssertionError: skipping negative bytes is illegal: -1393754107
        at org.apache.cassandra.io.util.MappedFileDataInput.skipBytes(MappedFileDataInput.java:96)
        at org.apache.cassandra.io.sstable.IndexHelper.skipBloomFilter(IndexHelper.java:50)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:56)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:91)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:67)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:68)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1215)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1107)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1077)
        at org.apache.cassandra.db.Table.getRow(Table.java:384)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:68)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,,30/Dec/10 18:00;jbellis;1922.txt;https://issues.apache.org/jira/secure/attachment/12467174/1922.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,00:02.1,,,no_permission,,,,,,,,,,,,20371,,,Thu Dec 30 19:37:30 UTC 2010,,,,,,0|i0g88f:,92764,,,,,,,,,,,"30/Dec/10 18:00;jbellis;Patch includes test to reproduce the problem and a (one-line) fix in CompactionManager.  Also includes comments on AbstractCompactionIterator as to what each method is supposed to include, to make it easier to avoid this mistake in the future.",30/Dec/10 18:20;brandon.williams;Committed with unused import removed.,"30/Dec/10 19:37;hudson;Integrated in Cassandra-0.7 #137 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/137/])
    Fix CompactionManager regression from CASSANDRA-1916 and add a better
test and more docs to prevent in the future.
Patch by jbellis, reviewed by brandonwilliams for CASSANDRA-1922
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair doesn't do anything when a CF isn't specified,CASSANDRA-1535,12474874,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,9/22/2010 21:44,3/12/2019 14:13,3/13/2019 22:24,9/23/2010 4:14,0.7 beta 2,,,,0,,,,,,"Invoking repair on a node does not work.  With RF > 1 all I get is:

INFO 15:04:59,987 Waiting for repair requests to: []

And nothing happens.",,,,,,,,,,,,,,,,,,,,23/Sep/10 01:25;brandon.williams;1535.txt;https://issues.apache.org/jira/secure/attachment/12455338/1535.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,18:16.9,,,no_permission,,,,,,,,,,,,20183,,,Thu Sep 23 12:47:41 UTC 2010,,,,,,0|i0g5tr:,92374,,,,,,,,,,,23/Sep/10 01:25;brandon.williams;The simplest thing to do is repair all CFs when only a KS is specified.,"23/Sep/10 02:18;stuhood;+1
Thanks Brandon!",23/Sep/10 04:14;brandon.williams;Committed.,"23/Sep/10 12:47;hudson;Integrated in Cassandra #544 (See [https://hudson.apache.org/hudson/job/Cassandra/544/])
    Repair should repair all CFs when no CFs are specified.  Patch by brandonwilliams reviewed by Stu Hood for CASSANDRA-1535
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
index created from cli does not show up in keyspace metadata,CASSANDRA-1613,12477206,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,10/13/2010 3:44,3/12/2019 14:13,3/13/2019 22:24,10/14/2010 18:05,0.7 beta 3,,Legacy/Tools,,0,,,,,,"create column family Category with comparator=UTF8Type and column_metadata=[{column_name:level, validation_class:IntegerType, index_type:0, index_name:CategoryLevelIdx}]

succeeds, but after this
'show keyspaces' does not reveal the presence of the index. Only the column family is shown.",,,,,,,,,,,,,,,,,,,,14/Oct/10 10:41;xedin;CASSANDRA-1613.patch;https://issues.apache.org/jira/secure/attachment/12457146/CASSANDRA-1613.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,41:32.0,,,no_permission,,,,,,,,,,,,20218,,,Fri Oct 15 13:23:18 UTC 2010,,,,,,0|i0g6b3:,92452,jbellis,jbellis,,,,,,,,,"14/Oct/10 10:41;xedin;new format example:

{code}
    Column Family Name: Category {
      Column Family Type: Standard
      Column Sorted By: org.apache.cassandra.db.marshal.UTF8Type
      Column Metadata {
        Column Name: level {
          Validation Class: org.apache.cassandra.db.marshal.IntegerType
          Index Name: CategoryLevelIdx
          Index Type: KEYS
        }
      }
   }
{code}","14/Oct/10 18:03;jbellis;committed, w/ minor changes to formatting to prefer colons + indentation to {}.  python influence. :)","14/Oct/10 18:23;xedin;ok, nice :)","15/Oct/10 13:23;hudson;Integrated in Cassandra #566 (See [https://hudson.apache.org/hudson/job/Cassandra/566/])
    include CF metadata in cli 'show keyspaces'.  patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-1613
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when streaming sstables to other nodes, make sure they don't get compacted before they are streamed",CASSANDRA-538,12440374,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,11/11/2009 2:02,3/12/2019 14:13,3/13/2019 22:24,11/12/2009 23:37,0.5,,,,0,,,,,,"at one point (pre-ASF codebase?) this was handled with a bootstrap data dir, but anticompaction no longer puts files there.

at this point it's probably easier to just make sure streaming keeps a reference to the sstable objects, which will keep them from being deleted.",,,,,,,,,,,,,,,,,,,,11/Nov/09 18:07;jbellis;538.patch;https://issues.apache.org/jira/secure/attachment/12424627/538.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,15:47.7,,,no_permission,,,,,,,,,,,,19745,,,Fri Nov 13 12:35:04 UTC 2009,,,,,,0|i0fzjr:,91357,,,,,,,,,,,"11/Nov/09 18:08;jbellis;done as described.

also adds ""not null iterator"" to anticompaction similar to the one used in regular compaction.","12/Nov/09 02:15;junrao;In CFS.doFileAntiCompaction, why do you need to wrap a FilterIterator with a notNullPredicate over the CompactionIterator? The CompactionIterator never returns null, right?","12/Nov/09 02:21;jbellis;it returns null if either the row had a tombstone that no longer needs to be kept, or if there was a recoverable error (recoverable meaning we can skip to the next row)",12/Nov/09 16:15;junrao;The patch looks good then.,12/Nov/09 23:37;jbellis;committed,"13/Nov/09 12:35;hudson;Integrated in Cassandra #257 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/257/])
    r/m unused bootstrap directory and ensure streaming files live to be streamed
patch by jbellis; reviewed by Jun Rao for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bootstrapping nodes do not reject range slice requests,CASSANDRA-1739,12479839,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tilgovi,tilgovi,tilgovi,11/12/2010 23:00,3/12/2019 14:13,3/13/2019 22:24,11/14/2010 12:33,0.6.9,,,,0,,,,,,ReadVerbHandler rejects the request if the node is bootstrapping. I think RangeSliceVerbHandler should probably do the same.,,,,,,,,,,,,,,,,,,,,12/Nov/10 23:01;tilgovi;0001-reject-range-req-if-bootstrapping-CASSANDRA-1739.patch;https://issues.apache.org/jira/secure/attachment/12459494/0001-reject-range-req-if-bootstrapping-CASSANDRA-1739.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:12.1,,,no_permission,,,,,,,,,,,,20284,,,Sun Nov 14 12:33:13 UTC 2010,,,,,,0|i0g73b:,92579,jbellis,jbellis,,,,,,,,,"13/Nov/10 01:58;jbellis;Are you seeing other nodes route range scans to the bootstrapping node, or is this just covering your bases?","13/Nov/10 02:36;tilgovi;Covering bases.
I'm pretty new to the codebase. What avoids this happening normally? I guess the node is not considered a ""live endpoint"" while bootstrapping?

Similar code is in the ReadVerbHandler and StorageProxy's readLocal(). I wasn't sure what was redundant and what wasn't.","14/Nov/10 12:33;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bytebuffers of column data written locally prevent GC of original thrift mutation byte[],CASSANDRA-1801,12491821,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,mdennis,mdennis,12/1/2010 22:03,3/12/2019 14:13,3/13/2019 22:24,12/3/2010 19:25,0.7.0 rc 2,,,,0,,,,,,It appears C* isn't releasing buffers that were used during the writes of batch mutates.  See attached screenshot of heap dump.,,,,,,,,,,,,,,,,,,,,02/Dec/10 16:27;tjake;1801_v1.txt;https://issues.apache.org/jira/secure/attachment/12465143/1801_v1.txt,03/Dec/10 19:09;tjake;1801_v2.txt;https://issues.apache.org/jira/secure/attachment/12465262/1801_v2.txt,01/Dec/10 22:03;mdennis;screenie.png;https://issues.apache.org/jira/secure/attachment/12465079/screenie.png,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,27:13.9,,,no_permission,,,,,,,,,,,,20318,,,Sat Dec 11 07:35:09 UTC 2010,,,,,,0|i0g7hr:,92644,jbellis,jbellis,,,,,,,,,"02/Dec/10 16:27;tjake;The thrift transport is now zero-copy for local writes. This means a larger underlying byte[] is held onto for many writes. 

This patch does a deep copy of the row mutation BB before storing in local memtable to avoid this.

","02/Dec/10 21:46;gdusbabek;Would it be more prudent to ensure that copies of the BBs that are needed are made in Table.apply, rather than copying everything wholesale up front?",02/Dec/10 22:15;jbellis;We'd like to avoid doing a second copy for BB that we just read (copied) off IncomingTcpConnection.,03/Dec/10 19:09;tjake;v2 rebased for 0.7,03/Dec/10 19:25;jbellis;committed,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
split commitlog into header + mutations files,CASSANDRA-1179,12466697,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mdennis,jbellis,jbellis,6/10/2010 23:02,3/12/2019 14:13,3/13/2019 22:24,6/19/2010 16:09,0.7 beta 1,,,,0,,,,,,"As mentioned in CASSANDRA-1119, it seems possible that a commitlog header could be corrupted by a power loss during update of the header, post-flush.  We could try to make it more robust (by writing the size of the commitlogheader first, and skipping to the end if we encounter corruption) but it seems to me that the most foolproof method would be to split the log into two files: the header, which we'll overwrite, and the data, which is truly append only.  If If the header is corrupt on reply, we just reply the data from the beginning; the header allows us to avoid replaying data redundantly, but it's strictly an optimization and not required for correctness.",,,,,,,,,,,,,,,,,,,,15/Jun/10 21:15;jbellis;1179-v2.txt;https://issues.apache.org/jira/secure/attachment/12447172/1179-v2.txt,15/Jun/10 22:55;mdennis;trunk-1179-v3.txt;https://issues.apache.org/jira/secure/attachment/12447181/trunk-1179-v3.txt,16/Jun/10 19:19;mdennis;trunk-1179-v4.txt;https://issues.apache.org/jira/secure/attachment/12447266/trunk-1179-v4.txt,15/Jun/10 19:17;mdennis;trunk-1179.txt;https://issues.apache.org/jira/secure/attachment/12447163/trunk-1179.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,55:14.4,,,no_permission,,,,,,,,,,,,20023,,,Sun Jun 20 12:47:27 UTC 2010,,,,,,0|i0g3hb:,91994,,,,,,,,,,,"15/Jun/10 21:06;jbellis;made some minor changes, primarily using BRAF in writeCommitLogHeader (you don't get buffering w/ raw FileOutputStream, and BRAF is simpler than doing the FOS/BufferedOutputStream/FileChannel dance).  also added RecoveryManager3Test to test the .header missing entirely.

todo: still needs to delete the .headers after a successful replay as well as the .log.

more severe: after running ""bin/cassandra -f"" and C-c-ing several times in a row, I get

ERROR 16:02:14,537 Exception encountered during startup.
java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.read(BufferedRandomAccessFile.java:332)
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:381)
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:361)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:213)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:221)

(This looks like BRAF is throwing AIOOBE when it should really be EOFException)
",15/Jun/10 21:15;jbellis;(fixed trying to be to clever w/ metadata fsync -- we actually do need to include that the first time we write the file),"15/Jun/10 22:55;mdennis;trunk-1179-v3.txt deletes header files after successful replay, handle short entries and garbage size writes",16/Jun/10 00:04;mdennis;need to add a test for the corrupted / partially flushed segment,"16/Jun/10 01:54;jbellis;I'd rather fix BRAF to generate correct EOFExceptions in case other code runs into this.  (And by removing the EOFException check, we introduce a new bug that if the size int is incomplete, we die again.)","16/Jun/10 02:01;jbellis;(actually BRAF.read should be returning -1, so that RAF.readFully throws EOFException)","16/Jun/10 19:19;mdennis;{quote}
made some minor changes, primarily using BRAF in writeCommitLogHeader (you don't get buffering w/ raw FileOutputStream, and BRAF is simpler than doing the FOS/BufferedOutputStream/FileChannel dance).
{quote}

FOS doesn't sync on flush/close and as headers are ""optional"" now there is no reason to waste the IO.  Just to be sure I was remembering this correctly, I just now tested it.  It provides 80+% improvement over BRAF, even more on a heavily loaded system.  This was clearly a failure on my part to document it at as such.  The header is so small (56 bytes I think) the OS will cache it just fine and not using buffered output will avoid both the memcopies and GC from the buffers.

{quote}
todo: still needs to delete the .headers after a successful replay as well as the .log.
{quote}

thank you, I hadn't realized there were two places the logs were getting removed.  Done.

{quote}
I'd rather fix BRAF to generate correct EOFExceptions in case other code runs into this. (And by removing the EOFException check, we introduce a new bug that if the size int is incomplete, we die again.)

(actually BRAF.read should be returning -1, so that RAF.readFully throws EOFException) 
{quote}

It was not at EOF, the buffer the data was supposed to be written into was zero length.  There was data in the file, but no where to write it in the buffer (because the size read was 0, new byte[size] resulted in a zero length array was was then supposed to be filled by BRAF.readFully).

I've added tests to catch this problem (as well as other related ones) and also changed BRAF to throw a more reasonable exception (but not EOF).  I believe BRAF.readFully will already throw EOF if it is at the end of the file.

The size of the log entry is now CRCed on it's own.  Whlie testing with random garbage at the end of a commit log, I had written a really large int to the size field which resulted in recover() trying to allocate a massive byte[] and getting OOM.

{quote}
by removing the EOFException check, we introduce a new bug that if the size int is incomplete, we die again.
{quote}

good catch.  I have no idea WTF I was thinking, there was even a comment that warned about it that got removed when the try/catch was removed.  I was probably trying to test something and removed it so it'd spew but forgot to put it back.
","19/Jun/10 16:09;jbellis;committed, minus the BRAF change","20/Jun/10 12:47;hudson;Integrated in Cassandra #471 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/471/])
    split commitlog header into separate file and add size checksum to mutations.  patch by mdennis and jbellis for CASSANDRA-1179
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossip conviction threshold too low,CASSANDRA-610,12442691,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,12/8/2009 2:15,3/12/2019 14:13,3/13/2019 22:24,12/8/2009 21:28,0.5,,,,0,,,,,,"The current gossip conviction threshold is a bit too low, and can cause hosts to 'flap' under heavy load.  I suspect that the original author of the failure detector implementation originally used both suspect and convict, but at some point decided that there was no action to take when a host was suspected, or that it was not worth doing.  They appear to have short-circuited the code to convict on suspect to eliminate this, however this caused the suspicion threshold (5) to be used for convicting hosts instead of the conviction threshold (8).","debian lenny amd 64 OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",,,,,,,,,,,,,,,,,,,08/Dec/09 02:17;brandon.williams;610.patch;https://issues.apache.org/jira/secure/attachment/12427279/610.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,30:01.3,,,no_permission,,,,,,,,,,,,19779,,,Wed Dec 09 12:42:40 UTC 2009,,,,,,0|i0fzzr:,91429,,,,,,,,,,,08/Dec/09 02:16;brandon.williams;Patch to remove suspect() and replace it with convict() restoring the threshold of 8.,08/Dec/09 02:30;jbellis;LGTM.  Waiting for Dan to test.,08/Dec/09 21:28;jbellis;committed,"09/Dec/09 12:42;hudson;Integrated in Cassandra #282 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/282/])
    increase failure conviction threshold.  patch by Brandon Williams; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reads (get_column) miss data or return stale values if a memtable is being flushed,CASSANDRA-98,12423628,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,4/23/2009 22:08,3/12/2019 14:13,3/13/2019 22:24,4/24/2009 0:52,0.3,,,,0,,,,,,"Reads can return missing values (null/exception) or find stale copies of a column if the read happens during an SSTable flush.

The get_column can go in, and not find the data in the current memtable. When it looks in the ""historical"" memtable, if that CF has already been flushed, then  it gets cleared from the historical memtable. As a result, the read looks for the column in older SSTables and finds a stale value (if it exists) or returns with null.

It can be tricky to reproduce this problem, but the reason is pretty easy to see.

While subsequent reads might return the correct value (from disk), this behavior makes it very difficult for apps that expect to ""read your writes"", at least in the absence of failures.",all,,,,,,,,,,,,,,,,,,,23/Apr/09 22:12;sandeep_tata;CASSANDRA-98.patch;https://issues.apache.org/jira/secure/attachment/12406291/CASSANDRA-98.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,52:55.4,,,no_permission,,,,,,,,,,,,19549,,,Fri Apr 24 00:52:55 UTC 2009,,,,,,0|i0fwv3:,90922,,,,,,,,,,,"23/Apr/09 22:12;sandeep_tata;Simple solution: Don't call columnFamily.clear() until the entire ""historical"" memtable has been flushed. This way, you're not stuck in a state where you can't find the data in the memtables and the SSTable is not ready yet.

The impact of this change is that memory cannot get freed up partially while a flush is going on. This is an insignificant penalty.",24/Apr/09 00:52;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFS readStats_ and diskReadStats_ are missing,CASSANDRA-359,12432725,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sammy.yu,sammy.yu,sammy.yu,8/10/2009 22:55,3/12/2019 14:13,3/13/2019 22:24,8/11/2009 18:27,0.4,,Legacy/Tools,,0,,,,,,,,,,,,,,,,,,,,,,,,,,11/Aug/09 01:37;sammy.yu;0001-CASSANDRA-359-add-back-readStat-for-CFS.patch;https://issues.apache.org/jira/secure/attachment/12416151/0001-CASSANDRA-359-add-back-readStat-for-CFS.patch,11/Aug/09 17:30;sammy.yu;0002-CASSANDRA-359-add-back-readStat-for-CFS-2nd-patch.patch;https://issues.apache.org/jira/secure/attachment/12416221/0002-CASSANDRA-359-add-back-readStat-for-CFS-2nd-patch.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,44:57.6,,,no_permission,,,,,,,,,,,,19651,,,Wed Aug 12 13:08:26 UTC 2009,,,,,,0|i0fygf:,91180,,,,,,,,,,,11/Aug/09 16:44;jbellis;shouldn't we also get rid of getReadDiskHits from the mbean + implementation per our irc discussion?,"11/Aug/09 17:30;sammy.yu;Removed diskReadStats.
",11/Aug/09 18:27;jbellis;committed,"12/Aug/09 13:08;hudson;Integrated in Cassandra #165 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/165/])
    add back read latency stats for CFS.getColumnFamily.  patch by Sammy Yu; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove deleted endpoints from hinted CF,CASSANDRA-560,12441021,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jaakko,jaakko,jaakko,11/18/2009 7:00,3/12/2019 14:13,3/13/2019 22:24,11/18/2009 23:33,0.5,,,,0,,,,,,Endpoints need to be removed from hinted CF after hints have been delivered,,,,,,,,,,,,,,,,,,,,18/Nov/09 07:02;jaakko;560.patch;https://issues.apache.org/jira/secure/attachment/12425319/560.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,33:18.6,,,no_permission,,,,,,,,,,,,19756,,,Thu Nov 19 12:34:50 UTC 2009,,,,,,0|i0fzon:,91379,,,,,,,,,,,"18/Nov/09 07:02;jaakko;Seems this is enough.
",18/Nov/09 23:33;jbellis;committed,"19/Nov/09 12:34;hudson;Integrated in Cassandra #263 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/263/])
    add removeDeleted to deliverHintsToEndpoint.  patch by Jaakko Laine; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read operation with ConsistencyLevel.ALL throws exception,CASSANDRA-1152,12466039,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,yukim,yukim,6/3/2010 2:18,3/12/2019 14:13,3/13/2019 22:24,6/3/2010 14:20,0.6.3,0.7 beta 1,,,0,,,,,,"Read operations which use thrift.CassandraServer#readColumnFamily should allow consistency_level == ALL.
Current implementation just throws InvalidRequestException when consistency level is ALL.
Same thing applies to avro implementation.",,,,,,,,,,,,,,,,,,,,03/Jun/10 03:36;dylanegan;read_column_family_on_all;https://issues.apache.org/jira/secure/attachment/12446220/read_column_family_on_all,03/Jun/10 03:36;dylanegan;read_column_family_on_all_inline_with_thrift;https://issues.apache.org/jira/secure/attachment/12446219/read_column_family_on_all_inline_with_thrift,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,36:05.3,,,no_permission,,,,,,,,,,,,20011,,,Thu Jun 03 17:27:20 UTC 2010,,,,,,0|i0g3bb:,91967,,,,,,,,,,,"03/Jun/10 03:36;dylanegan;Created two patches to alleviate the ALL problem, but one of the patches brings the Avro interface inline with thrift in not supporting CL.ANY on read operations too.

Im not really following the Avro updates, but I assume the interface should be the same.",03/Jun/10 14:20;gdusbabek;Committed.  Thanks for the patch.,"03/Jun/10 17:13;dylanegan;Hi Gary,

Just to help me understand, why would the Avro interface be different to the Thrift one in terms of what is supported at the read level.

Cheers,

Dylan.",03/Jun/10 17:21;gdusbabek;My bad.  I intended to remove the CL.ANY block from avro and keep the CL.ALL block out.  CL.ANY isn't in our avro generation file yet.,03/Jun/10 17:27;gdusbabek;Recommitted.  Thanks Dylan!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiget looks up keys locally serially,CASSANDRA-555,12440677,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,11/13/2009 23:39,3/12/2019 14:13,3/13/2019 22:24,11/14/2009 21:06,0.5,,,,0,,,,,,"When you multi-get a set of keys, when those commands are put into localCommands to be processed, they run through a for loop serially, instead of being put onto StorageService.readStage_.",,,,,,,,,,,,,,,,,,,,14/Nov/09 03:53;lenn0x;0001-CASSANDRA-555.-Added-a-multi-get-callable-to-put-loc.patch;https://issues.apache.org/jira/secure/attachment/12424933/0001-CASSANDRA-555.-Added-a-multi-get-callable-to-put-loc.patch,14/Nov/09 13:48;jbellis;555-v2.patch;https://issues.apache.org/jira/secure/attachment/12424958/555-v2.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,48:22.8,,,no_permission,,,,,,,,,,,,19754,,,Sun Nov 15 12:34:05 UTC 2009,,,,,,0|i0fznj:,91374,,,,,,,,,,,"14/Nov/09 13:48;jbellis;LGTM

here it is reformatted and w/ unnecessary null checks removed",14/Nov/09 21:06;lenn0x;commited,"15/Nov/09 12:34;hudson;Integrated in Cassandra #259 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/259/])
    Fixed multi-get to put localCommands onto the StorageService.readStage when a node gets commands to process, instead of running them serially. 
patch by goffinet, minor cleanup and review by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BufferedRandomAccessFile.read doesn't always do full reads,CASSANDRA-565,12441227,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,11/19/2009 21:17,3/12/2019 14:13,3/13/2019 22:24,11/19/2009 22:26,0.5,,,,0,,,,,,"BufferedRandomAccessFile.read may read fewer bytes than required, even when EOF is not reached. This breaks commit log recovery, which assumes that when a read returns less than required, the EOF is reached.",,,,,,,,,,,,,,,,,,,,19/Nov/09 21:19;junrao;issue565.patchev1;https://issues.apache.org/jira/secure/attachment/12425527/issue565.patchev1,19/Nov/09 22:06;junrao;issue565.patchev2;https://issues.apache.org/jira/secure/attachment/12425532/issue565.patchev2,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,27:10.1,,,no_permission,,,,,,,,,,,,19757,,,Fri Nov 20 12:35:25 UTC 2009,,,,,,0|i0fzpr:,91384,,,,,,,,,,,19/Nov/09 21:19;junrao;Attach a patch.,19/Nov/09 21:27;jbellis;Can you add a test illustrating the bug?,"19/Nov/09 21:30;jbellis;since the contract for RandomAccessFile that BRAF is patterned on is that read() may return less bytes even if it is not EOF, IMO we should probably make the read-with-retry loop a separate method (readFully?)",19/Nov/09 21:49;jbellis;(now that I see that this is documented behavior I withdraw the request for a test),19/Nov/09 22:06;junrao;Patch v2. Use the existing readFully method in CommitLog instead.,19/Nov/09 22:12;jbellis;+1,19/Nov/09 22:26;junrao;committed to trunk,"20/Nov/09 12:35;hudson;Integrated in Cassandra #264 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/264/])
    BufferedRandomAccessFile.read doesn't always do full reads; patched by junrao, reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
handle old gossip properly,CASSANDRA-572,12441433,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jaakko,jaakko,jaakko,11/23/2009 9:20,3/12/2019 14:13,3/13/2019 22:24,12/21/2009 17:19,0.5,,,,0,,,,,,"(1) If a node has been moving in the ring, further bootstraps by other nodes will cause errors as they are handling STATE_LEAVING gossip without having such member in token metadata.

(2) When a node bootstraps, it handles all ep states in the order they happen to arrive. If the first one to arrive has moved in the past (that is, it has STATE_LEAVING in its ep state), getNaturalEndpoint will throw ArrayIndexOutOfBounds exception as sortedTokens.size() == 0.
",,,,,,,,,,,,,,,,,,,,21/Dec/09 09:31;jaakko;572-01-fix-index-out-of-bounds.patch;https://issues.apache.org/jira/secure/attachment/12428610/572-01-fix-index-out-of-bounds.patch,21/Dec/09 09:31;jaakko;572-02-use-one-move-state.patch;https://issues.apache.org/jira/secure/attachment/12428611/572-02-use-one-move-state.patch,21/Dec/09 09:31;jaakko;572-03-unit-tests.patch;https://issues.apache.org/jira/secure/attachment/12428612/572-03-unit-tests.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,19:42.9,,,no_permission,,,,,,,,,,,,19759,,,Mon Dec 21 17:19:19 UTC 2009,,,,,,0|i0fzrb:,91391,,,,,,,,,,,"23/Nov/09 09:23;jaakko;(1) Modified StorageMetadata.onChange to handle STATE_LEAVING and STATE_LEFT only if isMember is true

(2) We might fix this by simply making sure we're calling getNaturalEndpoints for members only, but imho it is better to fix at the source.
","23/Nov/09 16:19;jbellis;See CASSANDRA-559 for more discussion.

Jaakko, can you check on the mailing list to see if anyone actually needs this, before we commit to making our lives (a little bit) harder for 0.5 -> 0.5.1.","23/Nov/09 16:20;jbellis;Sorry, I was thrown off by the title, assuming old==0.4.  But that is actually not what this is about.","23/Nov/09 16:35;jbellis;This feels like we're solving the wrong problem.  A bootstrapping node should not be added to the list of nodes clients can connect to (however you want to manage that) until after it is done bootstrapping.  I'd rather add

if (StorageService.instance().isBootstrapMode())
    throw new UnavailableException()

to the appropriate StorageProxy methods.","24/Nov/09 00:50;jaakko;I probably described the problem a bit vaguely, another try:

Suppose all nodes in the cluster are running normally and none of them have moved. Their EP state includes STATE_BOOTSTRAPPING (if they were bootstrapped to the ring) and STATE_NORMAL in this order. Suppose there is nodeA, which gets loadbalanced. It goes through leaving, left and bootstrapping back to normal. After this its EP state includes (in this order!) LEAVING, LEFT, BOOTSTRAPPING, NORMAL. The important thing is that EP state can have only one of each state, and they will be handled by other nodes in the order added originally. This is fine for nodes that already were in the ring, as they have seen the _old_ NORMAL state. However, if we ever want to bootstrap another node to the ring, it will cause errors, as they will start to handle states from LEAVING. They have no knowledge of this node's state before they handle NORMAL, so we must handle LEAVING and LEFT properly. That is, we must do nothing if we do not have knowledge of the node.

So this is not related to the new node serving requests, only to handle state gossip from other nodes properly. My term old gossip was obviously badly chosen, perhaps old state information would be more appropriate.
","30/Nov/09 20:06;jbellis;(Waiting on this while I mull over the rack aware bootstrap mailing list thread, since this is irrelevant if we have a problem there and need to add a ""coordinator"" node to fix it.)","02/Dec/09 08:26;jaakko;I don't think this is related to move coordination, but a separate issue. This will happen for every new node that enters the cluster (bootstrapping or not) after a node has already moved. Problem is, handling STATE_LEAVING and STATE_LEFT will cause errors for a node that is not in token metadata, and a node will not end up being there before STATE_NORMAL has been seen for it. Since states are handled in the order they were added, STATE_LEAVING and STATE_LEFT will be handled before BOOTSTRAPPING and NORMAL. LEAVING and LEFT in this case have no meaning as they refer to the old already-gone token, but since application state can only grow, these states will remain part of gossip and new nodes must handle them properly.

Edit: This can wait, as there is no point to add now. Let's check the situation after gossiping analysis is done","02/Dec/09 18:23;jbellis;ok, I understand now.  for some reason I had a really hard time wrapping my head around this.

trying to make sure we ignore old applicationstate correctly in all cases seems like fixing the symptom instead of the real cause.

wouldn't it be simpler to have a single STATE aplicationstate object whose value would be a json (or whatever) tuple of [NORMAL|LEAVING|LEFT|BOOTSTRAPPING],ARG+?  that way only the most recent one would be present in the gossiper.","03/Dec/09 02:45;jaakko;Funny thing, I was just thinking about the same thing during breakfast. Have to eat more often :)

The problem with this is that handling state changes will become somewhat more complex as we must be prepared to handle transitions between any two states in any order. Current gossip model leaves a trace of what the node has node, and even in the face of network partitions we can ""play back"" the transitions when they eventually arrive. That is, if a node moves, we will still see LEAVING, LEFT, BOOTSTRAPPING and NORMAL and construct token metadata according to that. If we only have one value to represent node's current state, we might go from, say NORMAL to NORMAL, or even LEFT to LEAVING without seeing any of the intermediate steps. Of course this can be done, but needs extra care. Don't know how much, though. Might very well be that in the end this would be better than the current way.

But even this would not remove the need to handle old application state correctly. If a node enters the ring when another node is just LEAVING or LEFT, that state will be the first one to be seen, and it must be ignored since there is nothing that can be done if NORMAL has not been seen. I think the real cause is there in any case, so we can't avoid fixing the symptoms that arrive with it.

I'll try this out now that I'm working on the gossiping part anyway so we'll have some more insight on what it would look like.
","03/Dec/09 08:40;jaakko;OK, here's a patch that uses same state name (NODE_STATE) to gossip all movement information. Format is (BOOTSTRAPPING|NORMAL|LEAVING|LEFT)|token.

The main things caused by this modification to the state machine were:
(1) When a node is bootstrapping, we should clear pending ranges for this endpoint, as well as remove it from token metadata. These checks are not strictly necessary (I think), but are there to help transition from LEAVING -> BOOTSTRAPPING in case we missed LEFT due to network partition.
(2) For handleStateLeaving and handleStateLeft remove pending ranges for this endpoint before doing anything else. If we missed NORMAL, there might be obsolete pending ranges from BOOTSTRAP. Distant possibility, but possibility nonetheless.

Following additional check is not directly related to gossip format change and could happen even using the current model. This is a very unlikely event, but in a large (say, 200+ nodes) multi-DC cluster with lots of node movement, this could very well happen even with relatively short DC-to-DC network outage:
(1) Added a check to handleStateLeaving and handleStateLeft for the case that a node has made NORMAL -> LEAVING -> LEFT -> BOOTSTRAP -> NORMAL -> LEAVING [->LEFT] movement cycle without us seeing the intermediate stages. In this case we have information for the old token and now the node is leaving _new_ token. We cannot simply assert this, as it is possible this happens.

Now of course this already touches the subject what conditions we must take care of and what should be left to operators to handle. Some of them (like removing all references to the endpoint before continuing to handle bootstrapping) are questionable and might relax safety precautions, but if we do not do that, a modest 30s network outage might cause us not to see STATE_LEFT and we'd end up having strange pending ranges.

I don't expect this patch to be included as it is, but let's see what people think of this gossip change and then discuss what checks should be made :)
","07/Dec/09 20:34;jbellis;The fundamental question is, is it ever dangerous to overwrite intermediate states, such that a node who has been down or partitioned does something broken when it gets the latest [partial] information?

> if we do not do that, a modest 30s network outage might cause us not to see STATE_LEFT

Trying to figure out what you're referring to here...  We can't miss a STATE_LEFT from a node that is leaving permanently since that will remain its last state and will be gossiped forever.  And if we miss a STATE_LEFT from a node that is moving, a down node that comes back up later will get the new token location and ""snap"" it to the right spot immediately.

I assume that as usual I am slow on the uptake here. :)","10/Dec/09 03:22;jaakko;From the small experiment (patch) it would seem that it is not too dangerous, just needs a bit of extra care. From #617 it would seem that reducing total amount of data in the gossiper would be beneficial, so current move gossip model needs some attention. There are basically two options:

(1) use one state for all moves, and use tuple/triple in application state string to identify what the node is doing (as suggested by jbellis earlier)
(2) return to the ""old"" gossip model: one state to broadcast the token and separate small states to express what the token means. The reason for moving out of this model was to make all needed information part of one gossip message and not rely on them arriving at the same time. With small addition to StorageService, we might keep arriving token ""somewhere"" and only add it to token metadata when the actual mode gossip arrives. This would take care of the race condition and would also reduce the amount of data to be gossiped to bare minimum.

> I assume that as usual I am slow on the uptake here. :) 

I think the problem is in my ability to clearly express an idea :)
","10/Dec/09 04:01;jbellis;I think I would prefer option (1), since from experience I can attest that trying to write a 100% correct ""state machine"" for (2) is quite difficult.",10/Dec/09 13:28;jaakko;Run into one tricky issue with STATE_LEAVING. I'll check that tomorrow with #603 as they are connected.,"18/Dec/09 16:30;jbellis;What was the tricky issue?  With CASSANDRA-603 and CASSANDRA-639 committed, this needs rebasing at least.","19/Dec/09 00:13;jaakko;Sorry about the delay. I did this part again yesterday (somewhat different now after #603), but had insufficient time to test it properly. The patch would seem OK, so as soon as I have tested it a bit more, I'll submit it.

The problem earlier was how to handle state jump to 'leaving'. Now that pending ranges is calculated every time and we know what direction a node was going last time, this is not a problem anymore. You can expect a patch soon.
","21/Dec/09 09:31;jaakko;01:
Fix ArrayIndex exception in case the first gossip a new node sees is leaving or left.

02:
Use one state for all movement related gossip
Fix also some bugs related to pending ranges on the leaving node itself

03:
A bunch of unit tests for node movement",21/Dec/09 17:19;jbellis;committed to 0.5 and trunk (only change was turning isLeaving writeLock calls to readLock),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make sure application states are delivered in correct order,CASSANDRA-548,12440591,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jaakko,jaakko,jaakko,11/13/2009 7:44,3/12/2019 14:13,3/13/2019 22:24,11/16/2009 14:33,0.5,,,,0,,,,,,Application states must be delivered to subscribers in the same logical order they were added.,,,,,,,,,,,,,,,,,,,,16/Nov/09 13:44;jaakko;548-deliver-ap-states-in-correct-order.patch;https://issues.apache.org/jira/secure/attachment/12425064/548-deliver-ap-states-in-correct-order.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,09:03.8,,,no_permission,,,,,,,,,,,,19749,,,Tue Nov 17 12:34:46 UTC 2009,,,,,,0|i0fzlz:,91367,,,,,,,,,,,13/Nov/09 07:53;jaakko;Added method for getting application state names from ep state in the order defined by application state's version,"13/Nov/09 21:09;jbellis;looks good overall; some improvements:

implement Comparable<AppState>, to avoid having to cast so much

sorting with ArrayList and Collections.sort is going to be much faster than TreeSet and LinkedList

prefer inlining used-only-once variables like List<String> stateNames","16/Nov/09 13:44;jaakko;New version. The list of application states still gets copied twice, but since we're returing list of 'keys' ordered by 'value' criteria, there does not seem to be any data structure that would support that directly. We could of course return the list of map entries after it has been ordered, but that would make it slightly awkward to use. Another option would be to keep application states always in order.
","16/Nov/09 14:33;jbellis;committed, after making getSortedApplicationStates return List<Map.Entry<...>> so going back to the Map after that is unnecessary.","17/Nov/09 12:34;hudson;Integrated in Cassandra #261 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/261/])
    add getSortedApplicationStates to preserve state ordering when re-gossiping to other nodes.  patch by Jaakko Laine; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix junit related build issues,CASSANDRA-164,12425240,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,johanoskarsson,johanoskarsson,johanoskarsson,5/12/2009 16:51,3/12/2019 14:13,3/13/2019 22:24,5/12/2009 18:51,0.3,,,,0,,,,,,Since the junit switch no xml report files are generated and the build doesn't fail properly if a test fails,,,,,,,,,,,,,,,,,,,,12/May/09 16:51;johanoskarsson;CASSANDRA-164.patch;https://issues.apache.org/jira/secure/attachment/12407896/CASSANDRA-164.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,51:06.7,,,no_permission,,,,,,,,,,,,19579,,,Wed May 13 14:59:32 UTC 2009,,,,,,0|i0fx9j:,90987,,,,,,,,,,,"12/May/09 18:51;jbellis;applied, with the change to retain
      <formatter type=""brief"" usefile=""false""/>

instead of printsummary.  (the brief formatter includes the stacktrace for failures which is nice to have right there.)","13/May/09 09:16;johanoskarsson;The problem is that then we won't get the junit xml files, which means hudson can't pick them up and tell us what went wrong. I'll see if we can get both. 

As a side note I wish you wouldn't commit patches with additional, non reviewed changes. Personally I think it's better to comment on the issue and let the developer make the changes or argue a case against them.","13/May/09 09:18;johanoskarsson;Guess I spoke to soon, the xml one is there too. My other comment still stands though :)","13/May/09 09:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    fix junit-related build issues.  patch by johano; reviewed by jbellis for 
","13/May/09 14:50;jbellis;> I wish you wouldn't commit patches with additional, non reviewed changes

Refrain from judgement for a moment and let's look at how the workflow compares.

Without taking shortcuts:

If my change is good:

1) original patch uploaded
2) my patch uploaded
3) my patch approved
4) commit

If my change is not good:

1) original patch uploaded
2) my patch uploaded
3) new patch from reviewer
4) commit

(obviously repeat 2-3 as necessary, let's keep things simple though).

Now, with shortcuts:

If my change is good:

1) original patch uploaded
2) commit patch w/ my changes

If my change is not good:

1) original patch uploaded
2) commit patch w/ my changes
3) new patch from reviewer against my commit
4) commit new patch

So in the case where my change is not good, we have basically the same steps and our only loss is an extra svn commit at step 2.

But when my change _is_ good we cut the steps where someone is waiting on someone else in half.

That seems like the risk is worth it to me.  (And my track record is very good here; I can't remember ever making a change that the author/reviewer disagreed with.  Part of that is if I make a nontrivial change I do in fact upload patches first.)","13/May/09 14:59;johanoskarsson;I know we are not going to agree on this, but wanted to throw it out there anyway.

I didn't mean that you as a committer should upload a new, revised patch, that would be quite a workload. Instead what I was suggesting was that you suggest the changes needed in the ticket, wait for the developer to either submit a new patch or discuss your suggested changes. 

It's not that I don't trust your code changes. The main reason is that imho the latest patch in jira should be the one applied to svn for a few reasons, transparency being one and from my experience it is very helpful for power users to be able to download the patch and apply it to older versions of the software. We do this all the time at Last.fm with our Hadoop installation.

I'm not going to push this further, just wanted it said.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid Java identified by JDK7 in StorageProxy.java,CASSANDRA-366,12433113,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,euphoria,euphoria,euphoria,8/14/2009 19:23,3/12/2019 14:13,3/13/2019 22:24,8/14/2009 20:19,0.4,,,,0,,,,,,"Per http://bugs.sun.com/view_bug.do?bug_id=6182950 the two dispatchMessage() methods in StorageProxy.java should not compile.  JDK6 allows both to coexist, against the spec.  JDK7 fixes this, resulting in a compilation error on StorageProxy.java",,,,,,,,,,,,,,,,,,,,14/Aug/09 19:24;euphoria;366_v1.diff;https://issues.apache.org/jira/secure/attachment/12416596/366_v1.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,19:37.1,,,no_permission,,,,,,,,,,,,19653,,,Fri Aug 14 20:19:37 UTC 2009,,,,,,0|i0fyhz:,91187,,,,,,,,,,,14/Aug/09 19:24;euphoria;Attached patch renames one of the methods. All tests pass on both JDK6 and JDK7 following the patch.,14/Aug/09 20:19;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
extend authorization to column families,CASSANDRA-1554,12475364,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,9/28/2010 23:46,3/12/2019 14:13,3/13/2019 22:24,10/6/2010 15:20,0.7 beta 3,,,,0,,,,,,"Authorization is now based on a hierarchy of resources, but the hierarchy only extends as far as keyspaces.  At the very least, it should be possible to implement an authority that can distinguish between the creation, modification and deletion of column families, and reading and writing the data contained in them.",,,,,,,,,,,,,,,,,,,,05/Oct/10 14:33;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1554-refactor-SimpleAuthority-for-CF-resourc.txt;https://issues.apache.org/jira/secure/attachment/12456386/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1554-refactor-SimpleAuthority-for-CF-resourc.txt,05/Oct/10 14:33;urandom;ASF.LICENSE.NOT.GRANTED--v1-0002-refactor-ClientState-and-RPC-for-CF-authorizations.txt;https://issues.apache.org/jira/secure/attachment/12456387/ASF.LICENSE.NOT.GRANTED--v1-0002-refactor-ClientState-and-RPC-for-CF-authorizations.txt,05/Oct/10 14:33;urandom;ASF.LICENSE.NOT.GRANTED--v1-0003-CF-access-test-for-SimpleAuthority.txt;https://issues.apache.org/jira/secure/attachment/12456388/ASF.LICENSE.NOT.GRANTED--v1-0003-CF-access-test-for-SimpleAuthority.txt,06/Oct/10 05:36;stuhood;v1-0004-minimize-object-creation.txt;https://issues.apache.org/jira/secure/attachment/12456478/v1-0004-minimize-object-creation.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,30:06.9,,,no_permission,,,,,,,,,,,,20197,,,Tue Oct 12 14:03:56 UTC 2010,,,,,,0|i0g5xz:,92393,,,,,,,,,,,"29/Sep/10 17:30;stuhood;Targetting to 0.7.0: CF creation can be dangerous, and RAX needs to be able to lock it down.",05/Oct/10 14:36;urandom;See attached.,"06/Oct/10 05:36;stuhood;* I think we need to minimize object creation in ClientState.hasColumnFamilyAccess: this was the intention of the member ArrayList<Object> in ClientState before, but I think it's more important now that it's called for every CF touch. Attaching an 0004 which we can consider in a different issue if you want.

Thanks for working on this Eric... much appreciated.",06/Oct/10 15:20;urandom;committed.,"12/Oct/10 14:03;hudson;Integrated in Cassandra #563 (See [https://hudson.apache.org/hudson/job/Cassandra/563/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping is not threadsafe,CASSANDRA-779,12455685,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,2/8/2010 21:21,3/12/2019 14:13,3/13/2019 22:24,2/8/2010 21:55,0.6,,,,0,,,,,,"The bootstrapper thread (called from the main thread which has acquired the lock for SS via SS.init) currently makes a few calls into SS that require its lock.

Those methods need to be thread-safe, but do not need the same lock required by SS.init.",,,,,,,,,,,,,,,,,,,,08/Feb/10 21:25;gdusbabek;0001-fix-bootstrapping-deadlock.patch;https://issues.apache.org/jira/secure/attachment/12435205/0001-fix-bootstrapping-deadlock.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,34:07.5,,,no_permission,,,,,,,,,,,,19860,,,Wed Feb 17 17:54:43 UTC 2010,,,,,,0|i0g10v:,91596,,,,,,,,,,,"08/Feb/10 21:27;gdusbabek;This fixes the problem, but I don't see the point of having Bootstrapper launch a separate thread to make the file requests while SS.init waits for bootstrapping to finish.  Its work could mostly be replaced with a method in SS that calls into the static methods currently in Bootstrapper.","08/Feb/10 21:34;jbellis;+1 on the patch.  I'm not sure what the point of having the separate BS thread is, either.  Maybe it made more sense a dozen refactors ago. :)",08/Feb/10 21:55;gdusbabek;r907816,"17/Feb/10 17:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DSS rack-awareness doesn't really work,CASSANDRA-1103,12464814,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/18/2010 17:15,3/12/2019 14:13,3/13/2019 22:24,5/18/2010 21:56,0.7 beta 1,,,,0,,,,,,CASSANDRA-952 fixed most of the DSS issues but the attempted placement of machines on different racks w/in each DC is poor (comparing each node only to the rack of the 1st replica in that DC rather than all).,,,,,,,,,,,,,,,,,,,,18/May/10 17:19;jbellis;ASF.LICENSE.NOT.GRANTED--0001-clean-up-DSS-inline-getNaturalEndpointsInternal-remove.txt;https://issues.apache.org/jira/secure/attachment/12444813/ASF.LICENSE.NOT.GRANTED--0001-clean-up-DSS-inline-getNaturalEndpointsInternal-remove.txt,18/May/10 17:19;jbellis;ASF.LICENSE.NOT.GRANTED--0002-simplify-out-doneDataCenterIter-variable.txt;https://issues.apache.org/jira/secure/attachment/12444814/ASF.LICENSE.NOT.GRANTED--0002-simplify-out-doneDataCenterIter-variable.txt,18/May/10 17:19;jbellis;ASF.LICENSE.NOT.GRANTED--0003-fix-DSS-to-generate-unique-racks-when-possibleb.txt;https://issues.apache.org/jira/secure/attachment/12444815/ASF.LICENSE.NOT.GRANTED--0003-fix-DSS-to-generate-unique-racks-when-possibleb.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,06:19.7,,,no_permission,,,,,,,,,,,,19994,,,Thu May 20 12:42:39 UTC 2010,,,,,,0|i0g30n:,91919,,,,,,,,,,,"18/May/10 17:19;jbellis;03
    fix DSS to generate unique racks when possibleb

02
    simplify out doneDataCenterIter variable

01
    clean up DSS: inline getNaturalEndpointsInternal, remove reload-on-every-call, r/m redundant dcE
","18/May/10 20:06;jeromatron;Looks good, makes it more readable with the refactor.

Two comments:

1) DSS: in loadEndpoints, for dcTokens, could a Collection be used that is naturally sorted so it doesn't have to iterate again through to sort them?
2) should the comment on line 156 really be before line 160 instead where the loop is","18/May/10 21:56;jbellis;committed w/ suggested comment change

cleaning up loadEndpoints is a fine goal for another ticket :)","20/May/10 12:42;hudson;Integrated in Cassandra #441 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/441/])
    add back localEndpoints variable to track ""endpoints in this DC"" separately from ""total endpoints selected so far.""  fixes regression introduced in CASSANDRA-1103.  patch by jbellis
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_slice_from forces iterating all columns and leaks file handlers with exception ,CASSANDRA-201,12426517,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,5/27/2009 21:27,3/12/2019 14:13,3/13/2019 22:24,5/27/2009 21:41,0.4,,,,0,,,,,,"There are 2 bugs in the get_slice_from code in CFS.java. 
1. The following 2 lines forces all columns to be iterated in each iterator, which is inefficient.
            List<IColumn> L = new ArrayList();
            CollectionUtils.addAll(L, collated);
2. If any exception occurs, the opened file handlers are not closed.
",,,,,,,,,,,,,,,,,,,,27/May/09 21:29;junrao;issue201.patchv1;https://issues.apache.org/jira/secure/attachment/12409212/issue201.patchv1,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,41:14.5,,,no_permission,,,,,,,,,,,,19592,,,Thu May 28 12:34:53 UTC 2009,,,,,,0|i0fxhj:,91023,,,,,,,,,,,27/May/09 21:29;junrao;Attach a fix.,"27/May/09 21:41;jbellis;committed, with extra catch block in case close() throws (so we still release the lock in that case).","28/May/09 12:34;hudson;Integrated in Cassandra #90 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/90/])
    remove unnecessary List creation in getSliceFrom and move close() cleanup for to the finally block.  patch by Jun Rao; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename 'table' -> 'keyspace' in public APIs,CASSANDRA-1287,12469480,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,7/16/2010 19:11,3/12/2019 14:13,3/13/2019 22:24,7/23/2010 18:34,0.7 beta 1,,,,0,,,,,,"thrift.CfDef uses the name 'table' rather than 'keyspace'. We need to make sure that all of our public APIs use consistent naming, despite the fact that our private APIs won't change until 0.7 is branched.",,,,,,,,,,,,,,,,,,,,22/Jul/10 22:27;stuhood;0001-Rename-CfDef.-table-keyspace.patch;https://issues.apache.org/jira/secure/attachment/12450228/0001-Rename-CfDef.-table-keyspace.patch,22/Jul/10 23:48;jhermes;TRUNK-1287.txt;https://issues.apache.org/jira/secure/attachment/12450239/TRUNK-1287.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,42:10.6,,,no_permission,,,,,,,,,,,,20059,,,Tue Jul 27 13:41:08 UTC 2010,,,,,,0|i0g453:,92101,,,,,,,,,,,21/Jul/10 19:42;jbellis;all this needs is s/table/keyspace/ in interface/cassandra.thrift,22/Jul/10 22:27;stuhood;Renames CfDef.{table => keyspace},"22/Jul/10 23:48;jhermes;renamed table to keyspace in interface/cassandra.thrift.
renamed table to keyspace in thrift/CassandraServer.java.
committing updated thrift/gen-java.
updated Thrift API version to ""8.5.0"". (CASSANDRA-1276 has ""8.4.0"")",23/Jul/10 18:34;jbellis;committed,"27/Jul/10 13:41;hudson;Integrated in Cassandra #501 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/501/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testGetCompactionBuckets sometimes fail,CASSANDRA-57,12422138,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,johanoskarsson,johanoskarsson,4/6/2009 17:05,3/12/2019 14:13,3/13/2019 22:24,4/10/2009 19:09,0.3,,,,0,,,,,,"testGetCompactionBuckets fails randomly on the exact same build of Cassandra. Not sure how to reproduce it.

The only log output I have:
   [testng] FAILED: testGetCompactionBuckets
   [testng] java.lang.AssertionError
   [testng]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testGetCompactionBuckets(Unknown Source)
   [testng] ... Removed 22 stack frames
",,,,,,,,,,,,,,,,,,,,10/Apr/09 18:24;jbellis;57.patch;https://issues.apache.org/jira/secure/attachment/12405178/57.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,22:12.1,,,no_permission,,,,,,,,,,,,19529,,,Fri Apr 10 19:09:15 UTC 2009,,,,,,0|i0fwlz:,90881,,,,,,,,,,,10/Apr/09 18:22;jbellis;something is broken in NonBlockingHashMap re removing and re-adding the same collection mid-iteration; it ends up with multiple references to that collection.  (I tried using iter.remove too; same problem.)  switch back to ConcurrentHashMap here; bloated memory overhead is a non-issue for this use case.,10/Apr/09 19:06;urandom;Patch looks good to me. +1,10/Apr/09 19:09;jbellis;applied,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commitlog segments don't get deleted,CASSANDRA-459,12436668,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,9/26/2009 18:48,3/12/2019 14:13,3/13/2019 22:24,9/29/2009 3:49,0.4,0.5,,,0,,,,,,"System table is not created with a periodic flush, so any update there (such as storing token info) can prevent commitlog segments from being deleted.",,,,,,,,,,,,,,,,,,,,28/Sep/09 20:19;jbellis;ASF.LICENSE.NOT.GRANTED--0001-logging.txt;https://issues.apache.org/jira/secure/attachment/12420727/ASF.LICENSE.NOT.GRANTED--0001-logging.txt,28/Sep/09 20:19;jbellis;ASF.LICENSE.NOT.GRANTED--0002-CASSANDRA-459-flush-locationinfo-every-minute-and-hint.txt;https://issues.apache.org/jira/secure/attachment/12420728/ASF.LICENSE.NOT.GRANTED--0002-CASSANDRA-459-flush-locationinfo-every-minute-and-hint.txt,28/Sep/09 20:19;jbellis;ASF.LICENSE.NOT.GRANTED--0003-cleanup-don-t-preserve-dirty-bits-from-older-replay-s.txt;https://issues.apache.org/jira/secure/attachment/12420729/ASF.LICENSE.NOT.GRANTED--0003-cleanup-don-t-preserve-dirty-bits-from-older-replay-s.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,24:47.5,,,no_permission,,,,,,,,,,,,19698,,,Tue Sep 29 03:49:21 UTC 2009,,,,,,0|i0fz2f:,91279,,,,,,,,,,,"26/Sep/09 18:49;jbellis;01 fixes the bug
02 adds a bunch of debug logging to commitlog purging so you can see which CFs are blocking the discard
",26/Sep/09 21:24;teodor;I started a test to check that for whole night. ,"26/Sep/09 22:20;teodor;I'm very sorry, but patch could not be applied cleanly for cassandra-0.4:
% cat /spool/home/teodor/tmp/0001-CASSANDRA-459-flush-system-table-every-five-minutes.txt /spool/home/teodor/tmp/0002-logging.txt  | patch -p1 -C
....
Patching file src/java/org/apache/cassandra/db/CommitLog.java using Plan A...
Hunk #1 succeeded at 97 (offset 1 line).
Hunk #2 succeeded at 463 (offset -1 lines).
Hunk #3 succeeded at 514 (offset 1 line).
Hunk #4 failed at 532.
1 out of 4 hunks failed--saving rejects to src/java/org/apache/cassandra/db/CommitLog.java.rej
....

% cat src/java/org/apache/cassandra/db/CommitLog.java.rej
***************
*** 516,521 ****
                  }
                  else
                  {
                      BufferedRandomAccessFile logWriter = CommitLog.createWriter(oldFile);
                      writeCommitLogHeader(logWriter, oldCommitLogHeader.toByteArray());
                      logWriter.close();
--- 532,539 ----
                  }
                  else
                  {
+                     if (logger_.isDebugEnabled())
+                         logger_.debug(""Not safe to delete commit log "" + oldFile + ""; dirty is "" + oldCommitLogHeader.dirtyString());
                      BufferedRandomAccessFile logWriter = CommitLog.createWriter(oldFile);
                      writeCommitLogHeader(logWriter, oldCommitLogHeader.toByteArray());
                      logWriter.close();

It seems to me  that is not a stopper  to test. 
","26/Sep/09 22:34;jbellis;Yes, 02 doesn't need to apply to 0.4","27/Sep/09 05:59;teodor;Patch doesn't change anything :(   I use the same test script as for CASSANDRA-458.
Immediately after test start:
% ls -l /spool/cassandra/commitlog /spool/cassandra/data/Keyspace1
/spool/cassandra/commitlog:
total 784
-rw-r--r--  1 teodor  wheel  786432 Sep 27 02:42 CommitLog-1254004919295.log

/spool/cassandra/data/Keyspace1:
total 0

After night
% ls -l /spool/cassandra/commitlog /spool/cassandra/data/Keyspace1
/spool/cassandra/commitlog:
total 3856720
-rw-r--r--  1 teodor  wheel  134217971 Sep 27 09:52 CommitLog-1254004919295.log
-rw-r--r--  1 teodor  wheel  134217839 Sep 27 09:52 CommitLog-1254005826443.log
-rw-r--r--  1 teodor  wheel  134218376 Sep 27 09:52 CommitLog-1254006732843.log
-rw-r--r--  1 teodor  wheel  134217734 Sep 27 09:52 CommitLog-1254007676088.log
-rw-r--r--  1 teodor  wheel  134218324 Sep 27 09:52 CommitLog-1254008589671.log
-rw-r--r--  1 teodor  wheel  134218501 Sep 27 09:52 CommitLog-1254009457777.log
-rw-r--r--  1 teodor  wheel  134217841 Sep 27 09:52 CommitLog-1254010329142.log
-rw-r--r--  1 teodor  wheel  134218312 Sep 27 09:52 CommitLog-1254011183824.log
-rw-r--r--  1 teodor  wheel  134217795 Sep 27 09:52 CommitLog-1254012066380.log
-rw-r--r--  1 teodor  wheel  134218031 Sep 27 09:52 CommitLog-1254012920007.log
-rw-r--r--  1 teodor  wheel  134217764 Sep 27 09:52 CommitLog-1254013812856.log
-rw-r--r--  1 teodor  wheel  134217888 Sep 27 09:52 CommitLog-1254014728895.log
-rw-r--r--  1 teodor  wheel  134217842 Sep 27 09:52 CommitLog-1254015574082.log
-rw-r--r--  1 teodor  wheel  134217861 Sep 27 09:52 CommitLog-1254016456859.log
-rw-r--r--  1 teodor  wheel  134218257 Sep 27 09:52 CommitLog-1254017313996.log
-rw-r--r--  1 teodor  wheel  134217874 Sep 27 09:52 CommitLog-1254018190196.log
-rw-r--r--  1 teodor  wheel  134218228 Sep 27 09:52 CommitLog-1254019039153.log
-rw-r--r--  1 teodor  wheel  134217768 Sep 27 09:52 CommitLog-1254019901588.log
-rw-r--r--  1 teodor  wheel  134218072 Sep 27 09:52 CommitLog-1254020850481.log
-rw-r--r--  1 teodor  wheel  134218463 Sep 27 09:52 CommitLog-1254021705210.log
-rw-r--r--  1 teodor  wheel  134217787 Sep 27 09:52 CommitLog-1254022584617.log
-rw-r--r--  1 teodor  wheel  134217817 Sep 27 09:52 CommitLog-1254023445022.log
-rw-r--r--  1 teodor  wheel  134218548 Sep 27 09:52 CommitLog-1254024325493.log
-rw-r--r--  1 teodor  wheel  134217870 Sep 27 09:52 CommitLog-1254025178129.log
-rw-r--r--  1 teodor  wheel  134217763 Sep 27 09:52 CommitLog-1254026031303.log
-rw-r--r--  1 teodor  wheel  134217782 Sep 27 09:52 CommitLog-1254026990171.log
-rw-r--r--  1 teodor  wheel  134217781 Sep 27 09:52 CommitLog-1254027844609.log
-rw-r--r--  1 teodor  wheel  134217742 Sep 27 09:52 CommitLog-1254028740042.log
-rw-r--r--  1 teodor  wheel  134217810 Sep 27 09:52 CommitLog-1254029593664.log
-rw-r--r--  1 teodor  wheel   54067200 Sep 27 09:53 CommitLog-1254030447553.log

/spool/cassandra/data/Keyspace1:
total 785468
-rw-r--r--  1 teodor  wheel  215470950 Sep 27 08:48 Standard1-372-Data.db
-rw-r--r--  1 teodor  wheel    3833605 Sep 27 08:48 Standard1-372-Filter.db
-rw-r--r--  1 teodor  wheel   88353365 Sep 27 08:48 Standard1-372-Index.db
-rw-r--r--  1 teodor  wheel  204226037 Sep 27 09:18 Standard1-408-Data.db
-rw-r--r--  1 teodor  wheel     812005 Sep 27 09:18 Standard1-408-Filter.db
-rw-r--r--  1 teodor  wheel   32812386 Sep 27 09:18 Standard1-408-Index.db
-rw-r--r--  1 teodor  wheel   10467605 Sep 27 09:51 Standard1-444-Data.db
-rw-r--r--  1 teodor  wheel      30349 Sep 27 09:51 Standard1-444-Filter.db
-rw-r--r--  1 teodor  wheel    1442137 Sep 27 09:51 Standard1-444-Index.db
-rw-r--r--  1 teodor  wheel  201238111 Sep 27 09:53 Standard1-445-Data.db
-rw-r--r--  1 teodor  wheel     795205 Sep 27 09:53 Standard1-445-Filter.db
-rw-r--r--  1 teodor  wheel   32237630 Sep 27 09:53 Standard1-445-Index.db
-rw-r--r--  1 teodor  wheel   10458480 Sep 27 09:52 Standard1-446-Data.db
-rw-r--r--  1 teodor  wheel      30261 Sep 27 09:52 Standard1-446-Filter.db
-rw-r--r--  1 teodor  wheel    1438054 Sep 27 09:52 Standard1-446-Index.db

The single problem in logs is:
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 09:41:59,422 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 03:41:59,303 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 04:41:59,234 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 05:41:59,355 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 06:41:59,386 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 07:41:59,392 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 08:48:58,819 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)

Log fragment of one of that warnings:
 INFO [MINOR-COMPACTION-POOL:1] 2009-09-27 09:41:53,827 ColumnFamilyStore.java (line 1155) Compacted to /spool/cassandra/data/Keyspace1/Standard1-434-Data.db.  0/67462071 bytes for 111274/107527 keys read/written.  Time: 31884ms.
 INFO [PERIODIC-FLUSHER-POOL:1] 2009-09-27 09:41:59,033 ColumnFamilyStore.java (line 367) Standard1 has reached its threshold; switching in a fresh Memtable
 INFO [PERIODIC-FLUSHER-POOL:1] 2009-09-27 09:41:59,033 ColumnFamilyStore.java (line 1178) Enqueuing flush of Memtable(Standard1)@8993320
 INFO [MEMTABLE-FLUSHER-POOL:1] 2009-09-27 09:41:59,033 Memtable.java (line 186) Flushing Memtable(Standard1)@8993320
 INFO [MINOR-COMPACTION-POOL:1] 2009-09-27 09:41:59,422 ColumnFamilyStore.java (line 1013) Compacting []
 WARN [MINOR-COMPACTION-POOL:1] 2009-09-27 09:41:59,422 ColumnFamilyStore.java (line 1034) Nothing to compact (all files empty or corrupt)
 INFO [MEMTABLE-FLUSHER-POOL:1] 2009-09-27 09:42:06,148 Memtable.java (line 220) Completed flushing Memtable(Standard1)@8993320

",27/Sep/09 13:04;jbellis;Are you on the latest 0.4 branch before applying the patch?  It needs both r819004 and this one to work.,"27/Sep/09 15:59;teodor;Yes, sure.
Another run (r819316 + 0001 patch), just for rechek:
% ls -l /spool/cassandra/commitlog /spool/cassandra/data/Keyspace1 /spool/cassandra/data/system
/spool/cassandra/commitlog:
total 44144
-rw-r--r--  1 teodor  wheel  45154304 Sep 27 18:32 CommitLog-1254061669607.log

/spool/cassandra/data/Keyspace1:
total 35130
-rw-r--r--  1 teodor  wheel  10475393 Sep 27 18:29 Standard1-1-Data.db
-rw-r--r--  1 teodor  wheel     30365 Sep 27 18:29 Standard1-1-Filter.db
-rw-r--r--  1 teodor  wheel   1443066 Sep 27 18:29 Standard1-1-Index.db
-rw-r--r--  1 teodor  wheel  10472280 Sep 27 18:30 Standard1-2-Data.db
-rw-r--r--  1 teodor  wheel     30189 Sep 27 18:30 Standard1-2-Filter.db
-rw-r--r--  1 teodor  wheel   1434966 Sep 27 18:30 Standard1-2-Index.db
-rw-r--r--  1 teodor  wheel  10455997 Sep 27 18:32 Standard1-3-Data.db
-rw-r--r--  1 teodor  wheel     30293 Sep 27 18:32 Standard1-3-Filter.db
-rw-r--r--  1 teodor  wheel   1439401 Sep 27 18:32 Standard1-3-Index.db

/spool/cassandra/data/system:
total 6
-rw-r--r--  1 teodor  wheel  255 Sep 27 18:32 LocationInfo-1-Data.db
-rw-r--r--  1 teodor  wheel   85 Sep 27 18:32 LocationInfo-1-Filter.db
-rw-r--r--  1 teodor  wheel   50 Sep 27 18:32 LocationInfo-1-Index.db

After  hour and a half:
% ls -l /spool/cassandra/commitlog /spool/cassandra/data/Keyspace1 /spool/cassandra/data/system
/spool/cassandra/commitlog:
total 813840
-rw-r--r--  1 teodor  wheel  134217982 Sep 27 19:57 CommitLog-1254061669607.log
-rw-r--r--  1 teodor  wheel  134217875 Sep 27 19:57 CommitLog-1254062560798.log
-rw-r--r--  1 teodor  wheel  134217761 Sep 27 19:57 CommitLog-1254063429524.log
-rw-r--r--  1 teodor  wheel  134217809 Sep 27 19:57 CommitLog-1254064318004.log
-rw-r--r--  1 teodor  wheel  134217785 Sep 27 19:57 CommitLog-1254065174349.log
-rw-r--r--  1 teodor  wheel  134217849 Sep 27 19:57 CommitLog-1254066038411.log
-rw-r--r--  1 teodor  wheel   27459584 Sep 27 19:58 CommitLog-1254066927001.log

/spool/cassandra/data/Keyspace1:
total 612880
-rw-r--r--  1 teodor  wheel  203500680 Sep 27 19:07 Standard1-38-Data.db
-rw-r--r--  1 teodor  wheel     806965 Sep 27 19:07 Standard1-38-Filter.db
-rw-r--r--  1 teodor  wheel   32636516 Sep 27 19:07 Standard1-38-Index.db
-rw-r--r--  1 teodor  wheel  202494151 Sep 27 19:42 Standard1-75-Data.db
-rw-r--r--  1 teodor  wheel     801685 Sep 27 19:42 Standard1-75-Filter.db
-rw-r--r--  1 teodor  wheel   32441650 Sep 27 19:42 Standard1-75-Index.db
-rw-r--r--  1 teodor  wheel   67331570 Sep 27 19:49 Standard1-83-Data.db
-rw-r--r--  1 teodor  wheel     209845 Sep 27 19:49 Standard1-83-Filter.db
-rw-r--r--  1 teodor  wheel    9598920 Sep 27 19:49 Standard1-83-Index.db
-rw-r--r--  1 teodor  wheel   67375520 Sep 27 19:57 Standard1-92-Data.db
-rw-r--r--  1 teodor  wheel     209605 Sep 27 19:57 Standard1-92-Filter.db
-rw-r--r--  1 teodor  wheel    9579322 Sep 27 19:57 Standard1-92-Index.db

/spool/cassandra/data/system:
total 6
-rw-r--r--  1 teodor  wheel  255 Sep 27 18:32 LocationInfo-1-Data.db
-rw-r--r--  1 teodor  wheel   85 Sep 27 18:32 LocationInfo-1-Filter.db
-rw-r--r--  1 teodor  wheel   50 Sep 27 18:32 LocationInfo-1-Index.db
",27/Sep/09 16:03;jbellis;Could you try trunk + 0001 ?,"27/Sep/09 17:28;teodor;Sure, results:

% ls -l /spool/cassandra/commitlog /spool/cassandra/data/Keyspace1 /spool/cassandra/data/system
/spool/cassandra/commitlog:
total 89344
-rw-r--r--  1 teodor  wheel  91422720 Sep 27 20:36 CommitLog-1254068803560.log

/spool/cassandra/data/Keyspace1:
total 78400
-rw-r--r--  1 teodor  wheel  70339417 Sep 27 20:36 Standard1-9-Data.db
-rw-r--r--  1 teodor  wheel    210085 Sep 27 20:36 Standard1-9-Filter.db
-rw-r--r--  1 teodor  wheel   9601776 Sep 27 20:36 Standard1-9-Index.db

/spool/cassandra/data/system:
total 6
-rw-r--r--  1 teodor  wheel  255 Sep 27 20:31 LocationInfo-1-Data.db
-rw-r--r--  1 teodor  wheel   85 Sep 27 20:31 LocationInfo-1-Filter.db
-rw-r--r--  1 teodor  wheel   50 Sep 27 20:31 LocationInfo-1-Index.db

After hour:
% ls -l /spool/cassandra/commitlog /spool/cassandra/data/Keyspace1 /spool/cassandra/data/system
/spool/cassandra/commitlog:
total 564560
-rw-r--r--  1 teodor  wheel  134218167 Sep 27 21:27 CommitLog-1254068803560.log
-rw-r--r--  1 teodor  wheel  134218321 Sep 27 21:27 CommitLog-1254069672194.log
-rw-r--r--  1 teodor  wheel  134218547 Sep 27 21:27 CommitLog-1254070532837.log
-rw-r--r--  1 teodor  wheel  134217848 Sep 27 21:27 CommitLog-1254071417379.log
-rw-r--r--  1 teodor  wheel   40793246 Sep 27 21:28 CommitLog-1254072254525.log

/spool/cassandra/data/Keyspace1:
total 487238
-rw-r--r--  1 teodor  wheel  245938744 Sep 27 21:05 Standard1-37-Data.db
-rw-r--r--  1 teodor  wheel     807205 Sep 27 21:05 Standard1-37-Filter.db
-rw-r--r--  1 teodor  wheel   32638599 Sep 27 21:05 Standard1-37-Index.db
-rw-r--r--  1 teodor  wheel   70395458 Sep 27 21:13 Standard1-46-Data.db
-rw-r--r--  1 teodor  wheel     210325 Sep 27 21:13 Standard1-46-Filter.db
-rw-r--r--  1 teodor  wheel    9612077 Sep 27 21:13 Standard1-46-Index.db
-rw-r--r--  1 teodor  wheel   41011329 Sep 27 21:18 Standard1-51-Data.db
-rw-r--r--  1 teodor  wheel     122245 Sep 27 21:18 Standard1-51-Filter.db
-rw-r--r--  1 teodor  wheel    5619283 Sep 27 21:18 Standard1-51-Index.db
-rw-r--r--  1 teodor  wheel   70296583 Sep 27 21:27 Standard1-60-Data.db
-rw-r--r--  1 teodor  wheel     210085 Sep 27 21:27 Standard1-60-Filter.db
-rw-r--r--  1 teodor  wheel    9592964 Sep 27 21:27 Standard1-60-Index.db
-rw-r--r--  1 teodor  wheel   10444198 Sep 27 21:27 Standard1-61-Data.db
-rw-r--r--  1 teodor  wheel      30253 Sep 27 21:27 Standard1-61-Filter.db
-rw-r--r--  1 teodor  wheel    1437846 Sep 27 21:27 Standard1-61-Index.db

/spool/cassandra/data/system:
total 6
-rw-r--r--  1 teodor  wheel  255 Sep 27 20:31 LocationInfo-1-Data.db
-rw-r--r--  1 teodor  wheel   85 Sep 27 20:31 LocationInfo-1-Filter.db
-rw-r--r--  1 teodor  wheel   50 Sep 27 20:31 LocationInfo-1-Index.db


","28/Sep/09 20:22;jbellis;New patches attached.

What was happening is that when a ColumnFamily got marked dirty in the commitlog, all subsequent commitlog segments would also have that CF marked dirty, even after it was flushed.  When flush happened it could clear out old segments, but if the CF never had any more updates (like the LocationInfo CF in the system keyspace) then log segments would pile up.

This makes log segments only treat a CF as dirty if it actually has data written to it for that CF.",28/Sep/09 22:24;junrao;Patch looks good to me. +1,29/Sep/09 03:49;jbellis;committed w/ one more bug fix (r819823) to 0.4 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
findSuitableEndPoint not returning closest endPoint,CASSANDRA-648,12443852,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,12/19/2009 22:25,3/12/2019 14:13,3/13/2019 22:24,12/21/2009 19:11,0.5,,,,0,,,,,,"After we switched everything over to Rackaware, I was noticing queries still taking 80ms+. We run in multiple datacenter's and I had a suspicion that it was just connecting to a node on east coast. It looks like findSuitableEndPoint sorts endpoints, but does not use the returned sorted endpoints when needing to return the final endpoint. ",,,,,,,,,,,,,,,,,,,,19/Dec/09 22:27;lenn0x;0001-Returned-the-sorted-endpoints-properly-for-CASSANDRA.patch;https://issues.apache.org/jira/secure/attachment/12428550/0001-Returned-the-sorted-endpoints-properly-for-CASSANDRA.patch,21/Dec/09 16:34;jbellis;648-2.patch;https://issues.apache.org/jira/secure/attachment/12428636/648-2.patch,19/Dec/09 23:16;jbellis;648.patch;https://issues.apache.org/jira/secure/attachment/12428551/648.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,36:53.3,,,no_permission,,,,,,,,,,,,19800,,,Mon Dec 21 19:11:32 UTC 2009,,,,,,0|i0g087:,91467,,,,,,,,,,,"19/Dec/09 22:36;stuhood;+1... but before you commit, can you add a javadoc for that method, so we can end the cycle?

Thanks!","19/Dec/09 23:16;jbellis;For efficiency, I'd prefer to make sortByProximity work in-place as its name suggests.

But we need the non-in-place for the Multimaps we have.

So here is a patch w/ two versions, the one leveraging the other of course.","21/Dec/09 16:34;jbellis;move isInSameRack and isInSameDC into AbstractEPS, making the basic IEPS interface sorting by proximity and the rest an implementation detail that only the corresponding kind of ReplicationStrategy should care about",21/Dec/09 18:01;lenn0x;+1. Can you add the javadoc that Stu asked about for sortByProximity?,21/Dec/09 19:11;jbellis;committed w/ javadoc to 0.5 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Row.columnFamilies_ is initialized to Hashtable in one place, HashMap in another",CASSANDRA-199,12426393,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/26/2009 20:50,3/12/2019 14:13,3/13/2019 22:24,6/18/2009 20:17,0.4,,,,0,,,,,,"which is right?

note that this behavior dates to the initial import into apache svn.",,,,,,,,,,,,,,,,,,,,18/Jun/09 18:03;jbellis;199.patch;https://issues.apache.org/jira/secure/attachment/12411117/199.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,18:16.0,,,no_permission,,,,,,,,,,,,19590,,,Fri Jun 19 12:34:46 UTC 2009,,,,,,0|i0fxh3:,91021,,,,,,,,,,,"14/Jun/09 14:18;eribeiro;I don't think so.

Hashtable is a legacy class and it's advised to use Collections.synchronizedMap(new HashMap()) instead. Furthermore, the only reason to use a synchronized Map is concurrency control (it's the case here?).

+1 to replace Hashtable for HashMap.","18/Jun/09 18:03;jbellis;I can't find any places Row is used concurrently in multiple threads, so HashMap seems like the Right choice.

(Patch also does other minor cleanup to Row-related code that I ran across while looking.)","18/Jun/09 20:11;sandeep_tata;+1

Per IRC, we should also clean up misleading comments in Table.load",18/Jun/09 20:17;jbellis;committed w/ comment fix,"19/Jun/09 12:34;hudson;Integrated in Cassandra #113 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/113/])
    minor cleanup of Row code.  patch by jbellis; reviewed by Sandeep Tata for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system CFs default to large memtable throughputs on large heaps,CASSANDRA-2148,12498177,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mdennis,mdennis,mdennis,2/9/2011 21:09,3/12/2019 14:13,3/13/2019 22:24,2/9/2011 23:45,0.7.1,,,,0,,,,,,"The default memtableThroughputInMB is calculated now based on the heap size.  Most people running with a large heap in production explicitly set it for their memtable(s).  However, the the CFs in the system keyspace still default to the calculated value which on a large heap can be quite large.  HintsColumnFamily is really the only problematic one though as the others are flushed afters changes to them.

we should:

1) set the throughput on the hints CF to a reasonable max and min value - min(256, max(32, normalDefault/2))
2) set the throughput on the other system CFs to some small constant value (just as a safety); 8M sounds good",,,7200,7200,,0%,7200,7200,,,,,,,,,,,,09/Feb/11 23:32;mdennis;2148-cassandra-0.7.txt;https://issues.apache.org/jira/secure/attachment/12470740/2148-cassandra-0.7.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,45:55.5,,,no_permission,,,,,,,,,,,,20465,,,Thu Feb 10 00:44:39 UTC 2011,,,,,,0|i0g9lz:,92987,,,,,,,,,,,09/Feb/11 21:14;mdennis;attached patch bounds Hints CF throughput between 32M and 256M and fixes the throughput of other system CFs to 8M,09/Feb/11 23:32;mdennis;rebased,09/Feb/11 23:45;brandon.williams;Committed.,"10/Feb/11 00:44;hudson;Integrated in Cassandra-0.7 #272 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/272/])
    bound hints CF memtable throughput between 32M and 256M
Patch by Matthew Dennis, reviewed by brandonwilliams for CASSANDRA-2148
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_columns_since broken when called remotely,CASSANDRA-93,12423411,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,4/21/2009 21:04,3/12/2019 14:13,3/13/2019 22:24,4/30/2009 21:53,,,,,0,,,,,,get_columns_since returns wrong result when called from a node that doesn't own the data.,,,,,,,,,,,,,,,,,,,,21/Apr/09 21:11;junrao;issue93.patch001;https://issues.apache.org/jira/secure/attachment/12406057/issue93.patch001,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,53:12.7,,,no_permission,,,,,,,,,,,,19547,,,Thu Apr 30 21:53:12 UTC 2009,,,,,,0|i0fwtz:,90917,,,,,,,,,,,"21/Apr/09 21:11;junrao;Attach a fix. The problem is that the remote path (in ReadVerbHandler) is not consistent with the local path. In particular, the path for get_columns_since is missing.",30/Apr/09 21:53;jbellis;committed a while ago,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assertion fails in CFS.forceAntiCompaction,CASSANDRA-454,12436428,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,junrao,junrao,9/23/2009 16:10,3/12/2019 14:13,3/13/2019 22:24,9/24/2009 15:32,0.5,,,,0,,,,,,"I got the following exception. CFS.forceAntiCompaction asserts ranges is not null. However, null is passed in explicitly from forceAntiCompaction.

Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.ColumnFamilyStore.forceAntiCompaction(ColumnFamilyStore.java:275)
        at org.apache.cassandra.db.HintedHandOffManager.deliverAllHints(HintedHandOffManager.java:190)",,,,,,,,,,,,,,,,,,,,23/Sep/09 16:28;jbellis;454.patch;https://issues.apache.org/jira/secure/attachment/12420380/454.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,28:14.1,,,no_permission,,,,,,,,,,,,19695,,,Fri Sep 25 12:35:12 UTC 2009,,,,,,0|i0fz1b:,91274,,,,,,,,,,,23/Sep/09 16:28;jbellis;remove unused skip argument from anticompaction. change HHM to perform a major compaction instead of anticompaction since that's really what we want there,24/Sep/09 14:40;junrao;+1,24/Sep/09 15:32;jbellis;committed,"25/Sep/09 12:35;hudson;Integrated in Cassandra #208 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/208/])
    remove unused skip argument from anticompaction. change HHM to perform a major compaction instead of anticompaction since that's really what we want there
patch by jbellis; reviewed by junrao for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
handle empty unbootstrap ranges,CASSANDRA-573,12441434,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jaakko,jaakko,jaakko,11/23/2009 9:27,3/12/2019 14:13,3/13/2019 22:24,11/24/2009 20:27,0.5,,,,0,,,,,,"If there are no ranges needing transfer during unbootstrap, unbootstrap fails to complete.
",,,,,,,,,,,,,,,,,,,,23/Nov/09 09:30;jaakko;573-handle-empty-unbootstrap-ranges.patch;https://issues.apache.org/jira/secure/attachment/12425818/573-handle-empty-unbootstrap-ranges.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,37:19.6,,,no_permission,,,,,,,,,,,,19760,,,Wed Nov 25 12:34:11 UTC 2009,,,,,,0|i0fzrj:,91392,,,,,,,,,,,"23/Nov/09 09:30;jaakko;Moved leaving ring code to a separate helper function and call it right away if rangesMM is empty.
","23/Nov/09 16:37;jbellis;How could you not have any ranges needing transfer during unbootstrap?

If it isn't part of the ring at all (i.e. never completed bootstrap in the first place), we should throw an error if unbootstrap is invoked.","24/Nov/09 01:34;jaakko;Ranges needing transfer lists only ranges that the new endpoints do not already have. If ReplicationFactor == number_of_nodes, then rangesMM will be empty, as all nodes already have all ranges.
",24/Nov/09 20:27;jbellis;committed,"25/Nov/09 12:34;hudson;Integrated in Cassandra #268 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/268/])
    Moved leaving ring code to a separate helper function and call it right away if rangesMM is empty.  patch by Jaakko Laine; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift CfDef incomplete; missing row/key_save_period_in_seconds,CASSANDRA-1606,12477075,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jhermes,jhermes,jhermes,10/11/2010 21:38,3/12/2019 14:13,3/13/2019 22:24,10/13/2010 17:41,0.7 beta 3,,,,0,,,,,,"Missed from CASSANDRA-1417.
",,,,,,,,,,,,,,,,,,,,13/Oct/10 17:23;jhermes;1606.txt;https://issues.apache.org/jira/secure/attachment/12457085/1606.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,41:45.1,,,no_permission,,,,,,,,,,,,20214,,,Thu Oct 14 12:49:47 UTC 2010,,,,,,0|i0g69j:,92445,jbellis,jbellis,,,,,,,,,"13/Oct/10 17:23;jhermes;Patch adds the vars into the thrift CfDef, and cleans up/finishes CFMetaData.",13/Oct/10 17:41;jbellis;committed,"14/Oct/10 12:49;hudson;Integrated in Cassandra #565 (See [https://hudson.apache.org/hudson/job/Cassandra/565/])
    add row/key cache save periods to CfDef.
patch by Jon Hermes; reviewed by jbellis for CASSANDRA-1606
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_columns_in fails when when routed to a node that isn't the home for the key,CASSANDRA-21,12421486,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,3/28/2009 21:28,3/12/2019 14:13,3/13/2019 22:24,3/30/2009 15:26,,,,,0,,,,,,"get_columns_in fails when the request cannot be satisfied locally.

What steps will reproduce the problem?
1. Insert multiple columns in some row R in a Cassandra cluster that 
contains more than 1 node.
2. Submit a get_columns_in query to a bunch of nodes. Using the python thrift 
interface, this is something  like: 
./Cassandra-remote -h node0:9160 get_columns_in 'Mailbox' 'rowid123' 
'HeaderList' ""['col1','col2']""
./Cassandra-remote -h node1:9160 get_columns_in 'Mailbox' 'rowid123' 
'HeaderList' ""['col1','col2']""
 

I've traced the error to a bug in how ReadMessage.java gets de-serialized. See attached unit-test to reproduce this.
I'm also attaching a patch that fixes this.",all,,,,,,,,,,,,,,,,,,,28/Mar/09 21:30;sandeep_tata;ReadMessageTest.java;https://issues.apache.org/jira/secure/attachment/12404065/ReadMessageTest.java,28/Mar/09 21:30;sandeep_tata;patch_readmessage.txt;https://issues.apache.org/jira/secure/attachment/12404066/patch_readmessage.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,22:36.7,,,no_permission,,,,,,,,,,,,19519,,,Mon Mar 30 15:26:07 UTC 2009,,,,,,0|i0fwen:,90848,,,,,,,,,,,"28/Mar/09 21:30;sandeep_tata;ReadMessageTest simple constructs a readmessage, serializes it, deserializes it and checks if this was the same as the original.

My patch fixes it for the case of get_columns_in breaking. Haven't tested other cases.",28/Mar/09 23:22;neophytos;Check get_slice_by_names ot get_slice_super_by_names. ,"30/Mar/09 14:32;neophytos;My previous comment was in relation to the actual name of the ""IN"" operator in Cassandra, i.e. get_slice_by_names instead of get_columns_in.  The bug is still present though. It's missing an else before ""if( sinceTimestamp > 0 )"" in ReadMessage.java. Otherwise, get_columns_in / get_slice_by_names would fail when the request is not satisfied locally.","30/Mar/09 15:26;jbellis;Applied with minor modifications (toString method added to ReadMessage), thanks.

For future reference, it saves time if you can use the Cassandra code style conventions (particularly braces on new lines).  Also Cassandra is using testng, not junit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing or incorrect SVN properties in http://svn.apache.org/repos/asf/incubator/cassandra/tags/cassandra-0.4.0-beta1/,CASSANDRA-378,12433147,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sebb@apache.org,sebb@apache.org,sebb@apache.org,8/15/2009 9:27,3/12/2019 14:13,3/13/2019 22:24,8/20/2009 16:52,0.4,,Legacy/Tools,,0,,,,,,Missing SVN properties - see attached file to follow,,,,,,,,,,,,,,,,,,,,15/Aug/09 11:57;sebb@apache.org;cassandra-0.4.0-beta1.sh;https://issues.apache.org/jira/secure/attachment/12416659/cassandra-0.4.0-beta1.sh,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:30.5,,,no_permission,,,,,,,,,,,,19660,,,Fri Aug 21 12:34:39 UTC 2009,,,,,,0|i0fyif:,91189,,,,,,,,,,,15/Aug/09 11:27;euphoria;This patch targets a different project.,"15/Aug/09 12:00;sebb@apache.org;Sorry, attached the wrong file by mistake.

The issue is for CASSANDRA, not WINK, so please revert it to the proper project",15/Aug/09 12:25;jbellis;Hmm.  I clicked on CASSANDRA-368 and got taken to WINK-139?  I think someone fixed the wrong problem. :),"15/Aug/09 12:28;jbellis;personally I am -1 on propsetting every file to eol-style native. I am unlikely to remember to propset every new file that gets created so this will get out of date much faster than our current ""just use unix line endings and use an editor that can handle that"" plan.","15/Aug/09 12:44;sebb@apache.org;You don't need to remember to add the propset; just ensure that your SVN configuration is correct:

http://www.apache.org/dev/version-control.html#https-svn-config",19/Aug/09 15:37;euphoria;I proposed the same in CASSANDRA-111 and am still +1 on it.,20/Aug/09 16:52;jbellis;committed,"21/Aug/09 12:34;hudson;Integrated in Cassandra #174 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/174/])
    set miscellaneous svn properties (mime-type, eol-style LF on .sh).  patch by Sebb; reviewed by Michael Greene for 
svn ps svn:eol-style native *. patch by Sebb; reviewed by Michael Greene for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scan results out of order,CASSANDRA-1332,12470387,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,drevell,drevell,7/28/2010 22:43,3/12/2019 14:13,3/13/2019 22:24,8/6/2010 0:50,0.7 beta 1,,,,0,,,,,,"After inserting 10 keys ('0', '1', ... '9') and running scan() with start_key='' and count=7, scan() returns the keys  ['7', '3', '6', '5', '0', '8', '2']. When I scan() again with start_key='2' and count=7, I get the keys  ['2', '1', '9', '4', '7']. Notice that key ""7"" appears in both result sets, and the relative order of keys ""7"" and ""2"" is inconsistent between the two scan results. 

I see the problem when running on a 4-node cluster. When I run on a 1-node cluster, the problem does not occur. So the attached system test always passes, since system tests use a 1-node cluster, so the test doesn't actually demonstrate the problem.

A standalone Python program that reproduces the problem is at: http://pastebin.com/FwitG4wf","CentOS 5, Java 1.6.0, Cassandra trunk as of 28 July 2010",,,,,,,,,,,,,,,,,,,28/Jul/10 22:54;drevell;scan_test.patch;https://issues.apache.org/jira/secure/attachment/12450766/scan_test.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,25:37.6,,,no_permission,,,,,,,,,,,,20085,,,Fri Aug 06 00:50:39 UTC 2010,,,,,,0|i0g4ev:,92145,,,,,,,,,,,28/Jul/10 22:54;drevell;System test that reproduces the problem on a 4-node cluster,"29/Jul/10 00:25;brandon.williams;FWIW, Dave also told me that increasing the CL doesn't help, so it's not a consistency issue.",29/Jul/10 01:43;jbellis;is this also present in 0.6.4?,29/Jul/10 01:52;drevell;It isn't present in exactly the same form in 0.6.4 because scan() is new in 0.7. Would it be worth testing 0.6.4 get_range_slices and looking for similar behavior?,29/Jul/10 17:29;drevell;Bisecting the SVN history shows that this worked in r966733 but became broken in r966742. Both of those revisions are dated July 22.,05/Aug/10 20:50;jbellis;I believe this is fixed by the changes to StorageProxy made in CASSANDRA-1156.  Can you re-test?,"05/Aug/10 20:50;jbellis;I note that scan wasn't supposed to really work at all across multiple nodes, prior to 1156. :)","05/Aug/10 23:07;drevell;jbellis: I can't retest, scan() no longer exists (in cassandra.thrift).",05/Aug/10 23:34;jbellis;get_range_slices is un-deprecated instead,"06/Aug/10 00:12;drevell;It seems fixed. After updating to r982821 and running the same test, it now passes (with scan switched to get_range_slices).","06/Aug/10 00:50;jbellis;Great, thanks for the help!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
write CL > ONE regression,CASSANDRA-1986,12495612,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,kelvin,kelvin,kelvin,1/14/2011 18:16,3/12/2019 14:13,3/13/2019 22:24,1/15/2011 2:02,0.7.1,,,,0,,,,,,write CL > ONE regression by the DC refactor.,,,,,,,,,,,CASSANDRA-1964,,,,,,,,,14/Jan/11 18:21;kelvin;CASSANDRA-1986-0001-fix-write-CL-ONE-regression.patch;https://issues.apache.org/jira/secure/attachment/12468387/CASSANDRA-1986-0001-fix-write-CL-ONE-regression.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,51:25.8,,,no_permission,,,,,,,,,,,,19351,,,Sat Jan 15 02:44:44 UTC 2011,,,,,,0|i0g8m7:,92826,tjake,tjake,,,,,,,,,"14/Jan/11 18:21;kelvin;The multi-DC (local / remote) write path sends local DC messages only to the target node (the lead replica), instead of to each of the following replicas.","14/Jan/11 18:21;kelvin;fix regression: send local DC messages to the follower nodes, as well.","14/Jan/11 22:51;tjake;ah good catch, +1","15/Jan/11 02:44;hudson;Integrated in Cassandra-0.7 #160 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/160/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cannot move a node,CASSANDRA-1670,12478510,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,mdennis,mdennis,10/27/2010 22:28,3/12/2019 14:13,3/13/2019 22:24,11/3/2010 13:12,0.7.0 rc 1,,,,0,,,,,,"two node cluster (node0, node1).  node0 is listed as the only seed on both nodes.  Listen addresses explicitly set to an IP on both nodes. No initial token, no autobootstrap (but see below).  Bring up the ring.  Everything is fine on both nodes.

decom node1.  verify decom completed correctly by reading the logs on both nodes.  rm all data/logs on node1.  bring node1 up again.

One of two things happen:

* node0 thinks it is in a ring by itself, node1 thinks both nodes are in the ring.
* both node0 and node1 think they are in rings by themselves

If you restart node0 after decom, it appears to work normally.

Similar issues seem to present if you kill node1 (either when autobootstrapping before it completes or after it is in the ring) and removetoken.

",RAX,,,,,,,,,,,,,,,,,,,01/Nov/10 15:59;gdusbabek;1670-0.6.txt;https://issues.apache.org/jira/secure/attachment/12458550/1670-0.6.txt,01/Nov/10 16:28;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-code-that-tidied-Gossiper.justRemovedEndpoints_-was-no.txt;https://issues.apache.org/jira/secure/attachment/12458552/ASF.LICENSE.NOT.GRANTED--v1-0001-code-that-tidied-Gossiper.justRemovedEndpoints_-was-no.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,38:33.7,,,no_permission,,,,,,,,,,,,20250,,,Wed Nov 03 13:07:03 UTC 2010,,,,,,0|i0g6nz:,92510,,,,,,,,,,,"29/Oct/10 21:38;mbulman;Because move is decommission+bootstrap, the same behavior occurs when moving node1 as well.",01/Nov/10 15:54;gdusbabek;Looks like this bug goes all the way back to 0.6.,"01/Nov/10 15:59;gdusbabek;The code that removes endpoints from Gossiper.justRemovedEndpoints after RING_DELAY was only getting called if the endpoint had a state attached to it.  Since state is removed for decommissioned nodes, the code was never getting called. ",01/Nov/10 16:06;jbellis;how does moving the removal out of the for loop fix the state-attached-to-it problem?,"01/Nov/10 16:38;gdusbabek;When a node is decommissioned, it gets added to justRemovedEndpoints_, but removed from endpointStateMap_.  The old code will only remove a node from justRemovedEndpoints_ if it currently exists in endpointStateMap_.  If the node stays in justRemovedEndpoints_ (which it will currently), it can never be recognized as part of the ring because of the check in Gossiper.handleNewJoin().","01/Nov/10 22:47;mbulman;Running from nodetool as well as ripcord decommission code (direct call to StorageService) gets:

Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.service.StorageService.getLocalToken(StorageService.java:1128)
        at org.apache.cassandra.service.StorageService.startLeaving(StorageService.java:1527)
        at org.apache.cassandra.service.StorageService.decommission(StorageService.java:1546)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)","02/Nov/10 14:11;gdusbabek;Mike, can you provide a more complete log?  That trace is unrelated to the patch and likely indicates a different problem.","02/Nov/10 15:24;mbulman;That's all I get.  The only other thing I can add is that the node being decommissioned logs ""DECOMMISSIONING"" when level is set to DEBUG, and that the exception comes back almost immediately.  fwiw, I'm running this on r1029870 of the .7 branch","02/Nov/10 15:34;gdusbabek;There should be more. What I'm looking for is some indication that the node is not in the middle of a bootstrap operation, which would trigger this exception.","02/Nov/10 16:34;mbulman;INFO 15:48:22,176 Joining: getting load information                                                                                                                                                                        
 INFO 15:48:22,177 Sleeping 90000 ms to wait for load information...
DEBUG 15:48:23,053 GC for ParNew: 12 ms, 15822112 reclaimed leaving 15102720 used; max is 1268449280                                                                                                                        
DEBUG 15:48:23,166 attempting to connect to /184.106.231.n0
 INFO 15:48:23,553 Node /184.106.231.n0 is now part of the cluster                                                                                                                                                         
DEBUG 15:48:23,554 Resetting pool for /184.106.231.n0
DEBUG 15:48:23,559 Node /184.106.231.n0 state normal, token 104110673354167092736227093944218730763                                                                                                                        
DEBUG 15:48:23,559 New node /184.106.231.n0 at token 104110673354167092736227093944218730763
DEBUG 15:48:23,559 clearing cached endpoints                                                                                                                                                                                
DEBUG 15:48:24,167 attempting to connect to /184.106.231.n0
DEBUG 15:48:24,169 Disseminating load info ...                                                                                                                                                                              
 INFO 15:48:24,554 InetAddress /184.106.231.n0 is now UP
 INFO 15:48:24,554 Started hinted handoff for endpoint /184.106.231.n0                                                                                                                                                  
 INFO 15:48:24,557 Finished hinted handoff of 0 rows to endpoint /184.106.231.n0
DEBUG 15:49:24,169 Disseminating load info ...                                                                                                                                                                              
DEBUG 15:49:39,106 GC for ParNew: 16 ms, 16111512 reclaimed leaving 85537648 used; max is 1268449280
DEBUG 15:49:52,177 ... got load info                                                                                              
 INFO 15:49:52,177 Joining: getting bootstrap token
DEBUG 15:49:52,183 attempting to connect to /184.106.231.n0                                                                                                                                                                
DEBUG 15:49:52,191 Processing response on a callback from 270@/184.106.231.n0
 INFO 15:49:52,192 New token will be 19040081623932476870383442086276677899 to assume load from /184.106.231.n0                                                                                                             
DEBUG 15:49:52,193 clearing cached endpoints
DEBUG 15:49:52,194 Will try to load mx4j now, if it's in the classpath                                                                                                                                                      
 INFO 15:49:52,194 Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO 15:49:52,220 Binding thrift service to /0.0.0.0:9160                                                                                                                                                                  
 INFO 15:49:52,222 Using TFramedTransport with a max frame size of 15728640 bytes.
 INFO 15:49:52,226 Listening for thrift clients...                                                                                                                                                                          
DEBUG 15:50:24,170 Disseminating load info ...
DEBUG 15:50:52,247 DECOMMISSIONING                                                                                                                                                                                          
DEBUG 15:51:24,170 Disseminating load info ...
DEBUG 15:52:24,171 Disseminating load info ...                                                                                                                                                                              


From n0:

root@ripcord:/usr/src/cassandra/branches/cassandra-0.7# bin/nodetool -h 184.106.231.n0  ring                                                                                                                           
Address         Status State   Load            Token
                                       104110673354167092736227093944218730763                                                                                                                                             
184.106.228.n1 Up     Normal  5.3 KB          19040081623932476870383442086276677899
184.106.231.n0 Up     Normal  10.27 KB        104110673354167092736227093944218730763                                                                                                                                     

root@ripcord:/usr/src/cassandra/branches/cassandra-0.7# bin/nodetool -h 184.106.228.n1 decommission 
     <TRACE FROM MY PREVIOUS COMMENT>

root@ripcord:/usr/src/cassandra/branches/cassandra-0.7# bin/nodetool -h 184.106.231.n0 ring
Address         Status State   Load            Token                                       
                                       104110673354167092736227093944218730763    
184.106.228.n1 Up     Normal  5.3 KB          19040081623932476870383442086276677899      
184.106.231.n0 Up     Normal  10.27 KB        104110673354167092736227093944218730763     
","02/Nov/10 22:25;mbulman;Ok got it working properly.  Patch fixes the issue described.

As a node, that patch doesn't work in 0.7 branch because justRemovedEndPoints_ (.6) is now justRemovedEndpoints (.7).  Not sure how you guys handle that, but the change is simple enough.","02/Nov/10 22:58;jbellis;bq. The old code will only remove a node from justRemovedEndpoints_ if it currently exists in endpointStateMap_

Isn't it ""remove from jRE if _any_ [other] node exists in eSM?""  Which means this is only a bug in 2-node clusters?

+1 if so, just trying to understand.","03/Nov/10 13:07;gdusbabek;bq. sn't it ""remove from jRE if any [other] node exists in eSM?"" Which means this is only a bug in 2-node clusters?
Yes. good observation.  I'll only bother committing this to 0.7/trunk then.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Too many splits for ColumnFamily with only a few rows,CASSANDRA-1050,12463697,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,johanoskarsson,joosto,joosto,5/4/2010 19:31,3/12/2019 14:13,3/13/2019 22:24,5/25/2010 7:41,0.7 beta 1,,,,0,hadoop,keyspace,split,,,"ColumnFamilyInputFormat creates splits for the entire Keyspace.  If one ColumnFamily has 100 Million rows and another has only 100 rows, the number of splits will be the 1,526 (assuming 64k rows per split) for either one, since it is based on the total number of unique keys across the whole keyspace, and not on the number of rows in the ColumnFamily.",,,,,,,,,,,,,,,,,,,,24/May/10 20:08;johanoskarsson;CASSANDRA-0.6-1050.patch;https://issues.apache.org/jira/secure/attachment/12445374/CASSANDRA-0.6-1050.patch,21/May/10 14:15;johanoskarsson;CASSANDRA-1050.patch;https://issues.apache.org/jira/secure/attachment/12445169/CASSANDRA-1050.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,15:58.7,,,no_permission,,,,,,,,,,,,19975,,,Tue May 25 13:22:28 UTC 2010,,,,,,0|i0g2p3:,91867,,,,,,,,,,,"21/May/10 14:15;johanoskarsson;This patch for trunk should help with the problem. Run ant gen-thrift-java as the generated code is not in the patch.
I ran the word count test and that worked, have not had time to try it in the scenario described in the ticket though.",21/May/10 14:32;jeromatron;I wonder if this would address the problem in CASSANDRA-1042.  I'll give it a try and see.,21/May/10 16:59;jeromatron;Johan - I applied this patch on my local trunk and ran the word count on it - I get perfect results on all but the /tmp/wordcount3 - that gets 1006 instead of 1000.  It looks like it resolves many of the issues that were happening with CASSANDRA-1042 though.,24/May/10 15:59;jbellis;+1,24/May/10 20:08;johanoskarsson;Slightly modified version for 0.6,24/May/10 22:45;jbellis;Belatedly realized that if we're changing the [thrift] method signature we should leave it alone in the stable 0.6 series.  let's just commit to trunk.  Sorry about the extra work.,25/May/10 07:41;johanoskarsson;Committed to trunk.,"25/May/10 12:32;jbellis;btw, if you build the thrift code with ""ant gen-thrift-java"" it will re-run rat for you to avoid blowing away the apache license headers in the generated code","25/May/10 12:56;hudson;Integrated in Cassandra #445 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/445/])
    Hadoop input format now uses the specified column family to figure out the number of splits instead of the whole keyspace. Patch by johan, review by jbellis. CASSANDRA-1050
","25/May/10 13:22;johanoskarsson;I usually do, but my default thrift installation is 0.2.0 (used in all my other projects) and this required a newer version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to direct code.google.com users here,CASSANDRA-11,12419393,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,jbellis,jbellis,3/25/2009 14:38,3/12/2019 14:13,3/13/2019 22:24,3/28/2009 1:39,,,,,0,,,,,,"We need links on the old code.google.com site pointing users to (a) the new source code (b) the new issue tracker (c) the new mailing list archives and sign up pages.

(Jeff, I think this is something you can do as a project owner there.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,18:59.7,,,no_permission,,,,,,,,,,,,19514,,,Sat Mar 28 01:18:59 UTC 2009,,,,,,0|i0fwcn:,90839,,,,,,,,,,,"28/Mar/09 01:18;hammer;Yo,

I've made some changes to the Google Code site.

Later,
Jeff",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"get_string_list_property(""tables"") does not include CompareSubcolumnsWith",CASSANDRA-326,12431766,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,eweaver,eweaver,eweaver,7/30/2009 5:11,3/12/2019 14:13,3/13/2019 22:24,7/30/2009 15:17,0.4,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,30/Jul/09 05:27;eweaver;CASSANDRA-326.diff;https://issues.apache.org/jira/secure/attachment/12414987/CASSANDRA-326.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,35:54.7,,,no_permission,,,,,,,,,,,,19636,,,Fri Jul 31 12:34:07 UTC 2009,,,,,,0|i0fy93:,91147,,,,,,,,,,,"30/Jul/09 05:35;euphoria;Patch only applies to trunk with fuzz.  Not sure about the null assignment change -- objects are null by default in Java, no need to change that.

Fixes the bug though.  +1 minus the null assignment.","30/Jul/09 05:37;eweaver; I'm not sure about the null either. It didn't want to build without it. Some variable scope issue, or me being dumb.","30/Jul/09 05:45;euphoria;Go figure, it does complain about ""variable subcolumnComparator might not have been initialized"" if you don't set that null.

Well, +1s all around then.  Ship it!",30/Jul/09 15:17;jbellis;committed,"31/Jul/09 12:34;hudson;Integrated in Cassandra #153 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/153/])
    add CompareSubcolumnsWith to describe_keyspace.  patch by Evan Weaver; reviewed by Michael Greene for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableImportTest is looking for test resources that are not available via the pom,CASSANDRA-636,12443602,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,zznate,zznate,zznate,12/17/2009 1:58,3/12/2019 14:13,3/13/2019 22:24,12/18/2009 19:56,0.5,,,,0,,,,,,"Trying to run 'mvn test' and a fresh checkout of the 5.0 branch results in a failure of org.apache.cassandra.tools.SSTableImportTest

The test resources are currently located in [basedir]/test/resources yet there is no <testResources> directive to include such in the pom. The following patch includes a fix to the test case for looking in the top-level of the classpath as well as a fix to the pom for including these resources.",,,,,,,,,,,,,,,,,,,,17/Dec/09 01:59;zznate;patch;https://issues.apache.org/jira/secure/attachment/12428250/patch,17/Dec/09 18:12;zznate;patch.txt;https://issues.apache.org/jira/secure/attachment/12428318/patch.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,45:34.6,,,no_permission,,,,,,,,,,,,19792,,,Fri Dec 18 19:56:27 UTC 2009,,,,,,0|i0g05j:,91455,,,,,,,,,,,17/Dec/09 01:59;zznate;patch for fixing test case and test resource inclusion in pom,"17/Dec/09 02:03;zznate;Sorry if that patch file is a little confusing - I just realized I yanked an unused import from src/java/org/apache/cassandra/cli/CliClient.java as well. Should I create a new issue for this and attach separately? Please advise, I'm a little new at this. 
edit: just created CASSANDRA-637 for the CliClient import.","17/Dec/09 15:45;urandom;Unfortunately this patch breaks the ant run tests, can it be reworked to make both happy?

",17/Dec/09 18:12;zznate;Indeed I did not check against the ant build - my apologies. This patch includes changes to build.xml which work similarly to those made against the pom.,18/Dec/09 19:56;urandom;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range queries do not yet span multiple nodes,CASSANDRA-212,12427015,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,phatduckk,jbellis,jbellis,6/3/2009 9:22,3/12/2019 14:13,3/13/2019 22:24,7/31/2009 18:28,0.4,,,,0,,,,,,"Need ability to continue a query on the next node in the ring, if necessary",,,,,,,,,,,,,,,,,,,,31/Jul/09 00:23;phatduckk;0001-All-patches-for-212.-All-comments-in-https-issues.patch;https://issues.apache.org/jira/secure/attachment/12415079/0001-All-patches-for-212.-All-comments-in-https-issues.patch,28/Jul/09 21:39;phatduckk;0001-Cassandra-212.patch;https://issues.apache.org/jira/secure/attachment/12414807/0001-Cassandra-212.patch,31/Jul/09 18:10;phatduckk;rebased-212-against-9164940f41972e3611d1ad38a903ca39562e6feb.patch;https://issues.apache.org/jira/secure/attachment/12415135/rebased-212-against-9164940f41972e3611d1ad38a903ca39562e6feb.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,49:11.1,,,no_permission,,,,,,,,,,,,19596,,,Sat Aug 01 12:34:01 UTC 2009,,,,,,0|i0fxjz:,91034,,,,,,,,,,,"30/Jun/09 20:02;jbellis;All this needs is a bool added to RangeReply that is true if it stopped b/c the next local key was not part of the range, or the asked-for limit was reached.

If a reply comes back with that bool false, then the coordinator node, the one talking to the client, sends the query to the next node in the ring down from the one it just got a reply from and appends those results to the list it's going to return to the client.

Repeat until the i'm-done-bool comes back true.
","14/Jul/09 14:49;markr;Sounds like a pretty serious bug in the general case, but might not happen very often in practice.",21/Jul/09 22:47;jbellis;(the relevant starting point is StorageProxy.getKeyRange),"21/Jul/09 23:24;junrao;This can be a bit tricky because the key ranges btw 2 consecutive nodes overlap (because of replication). When moving to the next node, you want to be careful not to pick up duplicated keys.

Also, the approach that Jonathan described forces nodes to be scanned sequentially. Sometimes, it is more efficient to scan multiple nodes in parallel, especially if maxResult is unspecified.","21/Jul/09 23:52;jbellis;> When moving to the next node, you want to be careful not to pick up duplicated keys. 

that is why you give the last value received from node #1 as the start_with parameter to node #2.  so at most there will be one duplicate.

> Sometimes, it is more efficient to scan multiple nodes in parallel

Unless you have trivially small amounts of data on each node, in which case it doesn't matter, any number that's going to fit in memory is going to be better served by sequential scanning since the odds are excellent that you won't have to cross to another node.","28/Jul/09 01:07;phatduckk;first cut at http://github.com/phatduckk/Cassandra/commit/b535b00f2917995f93f5838a98e08931e2b52680
I did a bunch of refactoring to get a lot of the get*Endpoint*() type methods to take an offset.

still need to work on the replicas.

anyways... wanted feedback on the approach. anyone wanna take a look and lemme know what you think?","28/Jul/09 02:57;jbellis;looks good, modulo the headers being in all the diffs (whitespace?)

> still need to work on the replicas

all you need to do is change

command = new RangeCommand(command.table, command.columnFamily, command.startWith, command.stopAt, command.maxResults - rangeKeys.size());

to pass in the last key from the previous node as the start.  right?  or are you talking about something else?","28/Jul/09 21:39;phatduckk;attempt at #212

unit tests and nosetests pass","29/Jul/09 03:10;jbellis;i'm a little confused by the git commit message.

which of those are ""mission accomplished"" and which are ""to-dos"" if any? :)","29/Jul/09 03:21;jbellis;we still need this part

> this needs a bool added to RangeReply that is true if it stopped b/c the next local key was not part of the range, or the asked-for limit was reached

otherwise we have to do extra queries for every range command that doesn't hit the max results or the end key.

also,

            while (endPoint != null)

                if (endPoint.toString().equals(firstEndpoint)) break

these seem redundant, shouldn't it be while (true) if (endpoint == null) { throw } ?
","29/Jul/09 22:55;phatduckk;created an object called RangeResult that encapsulates the keys and a boolean flag called isInRange.

i plumbed this thru RangeReply and to StorageProxy.getKeyRange

there's a squashed commit with all changes at:
http://github.com/phatduckk/Cassandra/commit/a5b2d264a1567a97b04e6d7874e0114a3ebd32d8

is the RangeResult stuff ok? It seemed to be the best way to flag whether a node had gone thru its entire range or not

(ignore ws diff - i'll prune those when i make a patch)","30/Jul/09 02:46;jbellis;Looks good to me overall.

RangeResult looks an awful lot like RangeReply w/ less methods.  Could we just use RangeReply and cut out the middleman?  I dunno.  Java feels so clunky here.

One way to return multiple values would be like this

        return new HashMap<String, Object>() {{
            put(""keys"", listOfStrings);
            put(""finished"", true);
        }};

Who says constructor blocks aren't useful? :)

Can we the bool in RangeReply it something besides isInRange?  In the range does not imply ""contains the entire range"" to me.  Rather the opposite relationship.  If you don't like ""finished"" maybe isComplete, rangeComplete, rangeCompletedLocally, ...","30/Jul/09 04:02;phatduckk;i thought about the HashMap thing but Sammy and I thought that was kinda hacky. I'll see about merging RangeResult with RangeReply.

I'll have some alternative(s) tomorrow. 

Also - i have no problem renaming the boolean. rangeCompletedLocally sounds most descriptive to me so ill go with that",31/Jul/09 00:15;phatduckk;single patch file to address the range queries,31/Jul/09 00:23;phatduckk;new patch with previous comments addressed,"31/Jul/09 15:09;jbellis;does not apply to trunk (because of CASSANDRA-111 maybe?), can you rebase?",31/Jul/09 18:10;phatduckk;rabased my last patch against trunk @ 9164940f41972e3611d1ad38a903ca39562e6feb,"31/Jul/09 18:28;jbellis;committed, thanks!","01/Aug/09 12:34;hudson;Integrated in Cassandra #154 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/154/])
    Make range queries continue on the next node when necessary.
Patch by Arin Sarkissian; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_slice needs offset + limit,CASSANDRA-261,12429004,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,lenn0x,lenn0x,6/26/2009 22:52,3/12/2019 14:13,3/13/2019 22:24,7/6/2009 18:25,0.4,,,,0,,,,,,Right now get_slice does not allow you to provide an offset.  This would help for pagination.,,,,,,,,,,,,,,,,,,,,29/Jun/09 17:03;junrao;issue261.patchv1;https://issues.apache.org/jira/secure/attachment/12412083/issue261.patchv1,06/Jul/09 17:30;junrao;issue261.patchv2;https://issues.apache.org/jira/secure/attachment/12412633/issue261.patchv2,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,11:29.0,,,no_permission,,,,,,,,,,,,19611,,,Wed Jul 08 12:47:08 UTC 2009,,,,,,0|i0fxuv:,91083,,,,,,,,,,,"26/Jun/09 23:11;jbellis;Me, explaining new get_slice: so now you would slice from """" with whatever your desired limit is, then slice from the last name that returns to page

Ian: Hm. That means that you have to chain from one call to the next, rather than being able to calculate chunksize*page.  e.g. you wouldn't be able to jump from page 1 to 3.

I'm convinced now -- could you submit a patch w/ your column offset code? :)","29/Jun/09 17:03;junrao;Attach a patch that adds the offset support.
","29/Jun/09 19:32;urandom;If I'm understanding this patch correctly, it will have to sequentially scan until past the offset before it can start returning results. Assuming you're using this for pagination, each call is going to require scanning through more and more results.

If this is the case, I'm not sure it's a good idea to hide these sorts of inefficiencies behind an API like this. ","29/Jun/09 20:26;junrao;Yes, the code has to scan and skip the portion of data between 0 and offset. So, the larger the offset, the worse the performance. It's hard to avoid this scanning overhead since offset is based on data merged from all sstables and it's difficult to push offset into individual sstables.

However, this is still better than the old get_slice that has to materialize the whole CF. A typical use case of this function is to support page scrolling. Typically, a user goes to earlier pages much more frequently. So this api could still be useful.  We should document this impact though.
","01/Jul/09 22:50;jbellis;I think the argument ""I'm a big boy, let me make the decision whether the tradeoff is worth it"" applies here better than in CASSANDRA-262.  (It's one thing to allow a client to bog down the server with something potentially slow.  It's another to let the client crash you.  RDBMSes draw a similar distinction, although you could argue that they have no choice. :)","01/Jul/09 23:56;junrao;Right. It seems that some applications need this functionality. Although this implementation doesn't give uniform performance, it is better than the old implementation or the application doing its own implementation.",03/Jul/09 20:37;jbellis;Could you update your patch to include updating the system tests?  (setup instructions at http://wiki.apache.org/cassandra/HowToContribute),"06/Jul/09 17:30;junrao;Patch v2.
* patch the system test
* rebase to head of trunk","06/Jul/09 18:25;jbellis;committed, thanks!","08/Jul/09 12:47;hudson;Integrated in Cassandra #131 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/131/])
    add offset support to get_slice.  patch by Jun Rao; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition with ConcurrentLinkedHashMap,CASSANDRA-405,12434478,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,9/1/2009 4:14,3/12/2019 14:13,3/13/2019 22:24,9/2/2009 15:04,0.4,,,,0,,,,,,We are seeing a race condition with ConcurrentLinkedHashMap using appendToTail. We could remove the ConcurrentLinkedHashMap for now until that's resolved.,,,,,,,,,,,,,,,,,,,,01/Sep/09 14:44;jbellis;405.patch;https://issues.apache.org/jira/secure/attachment/12418250/405.patch,01/Sep/09 04:15;lenn0x;stack.log.gz;https://issues.apache.org/jira/secure/attachment/12418216/stack.log.gz,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,53:14.6,,,no_permission,,,,,,,,,,,,19673,,,Tue Sep 15 20:11:54 UTC 2009,,,,,,0|i0fyqf:,91225,,,,,,,,,,,01/Sep/09 04:15;lenn0x;Our stack trace from the running node in question. ,01/Sep/09 04:53;jbellis;have you checked the other nodes for stuck threads like this?,"01/Sep/09 04:57;sammy.yu;We did multiple stack dumps they are all the same.  There is no progression. 

In this stack trace there are two sets of stack traces
pool-1-thread-{63,62,61,59,58,54,53,51,49,47} and ROW-READ-STAGE{8,7,5,4,3,2,1}:
""ROW-READ-STAGE:8"" prio=10 tid=0x00007f1b78b52000 nid=0x1945 runnable [0x0000000046532000]
   java.lang.Thread.State: RUNNABLE
	at com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap$Node.appendToTail(ConcurrentLinkedHashMap.java:536)
	at com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap.putIfAbsent(ConcurrentLinkedHashMap.java:281)
	at com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap.put(ConcurrentLinkedHashMap.java:256)
	at org.apache.cassandra.io.SSTableReader.getPosition(SSTableReader.java:241)
	at org.apache.cassandra.db.filter.SSTableNamesIterator. (SSTableNamesIterator.java:46)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1445)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1398)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.Table.getRow(Table.java:589)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:78)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:44)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

ROW-READ-STAGE6 is a little different:
""ROW-READ-STAGE:6"" prio=10 tid=0x00007f1b78b4e000 nid=0x1943 runnable [0x0000000046330000]
   java.lang.Thread.State: RUNNABLE
	at com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap$Node.appendToTail(ConcurrentLinkedHashMap.java:540)
	at com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap.putIfAbsent(ConcurrentLinkedHashMap.java:281)
	at com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap.put(ConcurrentLinkedHashMap.java:256)
	at org.apache.cassandra.io.SSTableReader.getPosition(SSTableReader.java:241)
	at org.apache.cassandra.db.filter.SSTableNamesIterator. (SSTableNamesIterator.java:46)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1445)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1398)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.Table.getRow(Table.java:589)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:78)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:44)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

All the other pool-1-threads are in the readLock waiting state due to the writeLock
""pool-1-thread-12425"" prio=10 tid=0x00007f1b7857e000 nid=0x3fe6 waiting on condition [0x00007f1892528000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  0x00007f1b8534e848> (a java.util.concurrent.locks.ReentrantReadWriteLock$FairSync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:877)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1197)
	at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:594)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1412)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1398)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.Table.getRow(Table.java:589)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
	at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:609)
	at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:320)
	at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)
	at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:173)
	at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:213)
	at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:551)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:539)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)","01/Sep/09 14:44;jbellis;remove LRU key cache from 0.4; the ConcurrentLinkedHashMap library is buggy (see http://code.google.com/p/concurrentlinkedhashmap/issues/detail?id=9)

I will revisit some kind of key cache early in 0.5.","02/Sep/09 02:45;lenn0x;+1 This patch looks good. Let's remove from 0.4, and work on a better implementation on 0.5","02/Sep/09 03:08;sammy.yu;Yes we have this running fine in production now.  Mentioned to jbellis other Concurrent LRU cache implementation:
http://svn.apache.org/viewvc/lucene/solr/trunk/src/common/org/apache/solr/common/util/ConcurrentLRUCache.java?view=log
http://svn.apache.org/viewvc/lucene/solr/trunk/src/java/org/apache/solr/search/FastLRUCache.java?view=log
that we could use in 0.5","02/Sep/09 03:41;ben.manes;When performing a postmortem on this issue, please review how the ConcurrentLinkedHashMap was added.  The project page stated:

> Note: The algorithm needs further testing and is not deemed production ready. It is functional under concurrent tests, but needs additional load testing to assert correctness.

That load testing, provided in the standard unit test runs, uncovered the issue and thus it was not promoted to a release status.  I haven't had time in the last few months to work on this project, but even the last check-in notes that its leaving debug code to help resolve it later.  The project states on the front page and FAQ that the goal is more educational than formal usage, hence I avoided known algorithms (which would be the correct approach if it was work-related).

The ConcurrentLRUCache uses a watermark approach which is valid, but suffers from stampeding and is an offline algorithm.  Its still an excellent approach and one of many possibilities described in the FAQ.  I am personally a fan of soft-reference based caching for global data, which is evicted in LRU order, because it allows the GC to manage what it does best (memory!) and promotes not overburdening the application server.

Please treat this as an issue where the blame is both 3p as I did not stress heavily enough not to use this in production and internal for not evaluating a 3p project enough to recognize that it warned about its production status.  I will update the project page to better communicate and provide a performant modification that is thread-safe for those that need a solution.  Please re-evaluate your own internal processes to determine why the bad call was made.

I am not trying to shift blame, but my pet peeve is when firefighting production and no one learns because then it just happens again.  Its very frustrating, even more so if I actually work there! ;-)

Cheers!
Ben","02/Sep/09 14:57;jbellis;Actually, I did read your svn logs enough to note that the bug referenced on the front page found by the ehcache people was fixed.  Too bad I missed the other, but it's not the end of the world.

In general, I suggest not getting all pissy with the people actually performing the ""additional load testing"" you claim is needed.  Early adopters are an important asset. :P

(Note that this bug is against a beta version of Cassandra, so at the lowest level the process worked: the bug was uncovered before the final release.  Although if the front page of CLRU weren't so out of date, or there had actually been an issue in the tracker for a bug known for months, the pain could have been avoided.)",02/Sep/09 15:04;jbellis;committed 405.patch to 0.4 and trunk,"02/Sep/09 17:00;ben.manes;No ""pissy""-ness, and I agree with you.  I've just been in too many firefighting sessions where the result is moving on and hitting the same problem 3 months later, and then 3 months later again.  People are usually so charged up fixing the issue that I find, perhaps incorrectly, that you have to be blunt to get their attention.  So no worries on my side. :-)

But yep, definately my fault for a good chunk of this.  Sorry for the inconvenience.

Cheers,
Ben","03/Sep/09 12:52;hudson;Integrated in Cassandra #186 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/186/])
    remove buggy concurrentlinkedhashmap library and lru cache.  too late in 0.4 to try to debug the library -- will revisit for 0.5.  patch by jbellis; reviewed by Chris Goffinet for 
",15/Sep/09 20:11;jbellis;CASSANDRA-423 is the cache-for-0.5 ticket.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicEndpointSnitch is defeated by the caching done in Strategy,CASSANDRA-1350,12470730,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,jbellis,jbellis,8/3/2010 14:56,3/12/2019 14:13,3/13/2019 22:24,8/12/2010 14:41,0.7 beta 2,,,,0,,,,,,"can we move the caching into AbstractEndpointSnitch instead?

also: AES.register appears to never be called.",,,,,,,,,,,,,,,,,,,,05/Aug/10 17:43;gdusbabek;0001-move-endpoint-cache-from-ARS-to-AES.patch;https://issues.apache.org/jira/secure/attachment/12451348/0001-move-endpoint-cache-from-ARS-to-AES.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,44:59.0,,,no_permission,,,,,,,,,,,,20096,,,Sat Aug 14 12:48:35 UTC 2010,,,,,,0|i0g4in:,92162,,,,,,,,,,,05/Aug/10 17:44;gdusbabek;I noticed that DynamicEndpointSnitch doesn't worry much about invalidating windows (and scores) from nodes that are decommissioned.  Is that on purpose?,07/Aug/10 14:08;jbellis;won't those be taken care of by the every-10-minutes global invalidate?,"09/Aug/10 14:57;gdusbabek;Their latencies are cleared, but the scores and windows are never removed.  It's a minor point, but updateScores() could be a little more efficient if it didn't have to consider decommissioned nodes.","11/Aug/10 17:21;brandon.williams;The latencies being cleared is the most important part.  At the next interval, if no more latency data is received for that node, updateScores() going to calculate a score for an empty deque, which means the score() function is just to going to do a size check and return 0, which is pretty cheap, so I'm not sure it's worth the extra complexity of notifying the snitch about decommissioned nodes.",12/Aug/10 14:07;jbellis;+1 Gary's patch,"14/Aug/10 12:48;hudson;Integrated in Cassandra #514 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/514/])
    move endpoint cache from ARS to AES. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1350
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop output SlicePredicate is slow and doesn't work as intended,CASSANDRA-1246,12468445,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,7/2/2010 15:30,3/12/2019 14:13,3/13/2019 22:24,7/7/2010 13:21,0.7 beta 1,,,,0,,,,,,"The output SlicePredicate is only used to attempt to check that no data exists in the range that we're going to be writing data.  This is 

(a) slow, since it performs get_range_slices across the entire key range, meaning we'll hit every node in the cluster if there is no data (which is supposed to be the normal case)
(b) wrong, since it appears to be intended to use keyList.size to allow data in column X to not interfere with an output to column Y, but that is not how get_range_slices works -- if you have data (or even a tombstone) in any column, you'll get the key back in your result list.  so what you would have to do is scan every key, and check the list of columns returned, which in the case of data actually existing in other columns will be prohibitively slow
",,,,,,,,,,,,,,,,,,,,02/Jul/10 15:34;jbellis;1246.txt;https://issues.apache.org/jira/secure/attachment/12448578/1246.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,48:27.8,,,no_permission,,,,,,,,,,,,20050,,,Wed Jul 07 13:48:29 UTC 2010,,,,,,0|i0g3vz:,92060,,,,,,,,,,,02/Jul/10 15:35;jbellis;patch to remove output SlicePredicate,"07/Jul/10 02:48;jeromatron;jonathan - looks fine except the docs.

ColumnFamilyOutputFormat class javadocs as well as its method checkOutputSpecs javadocs both still mention the output slice predicate.",07/Jul/10 13:21;jbellis;committed w/ cleaned up javadoc.  thanks!,"07/Jul/10 13:48;hudson;Integrated in Cassandra #488 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/488/])
    r/m Hadoop outputSlicePredicate.  patch by jbellis; reviewed by jhanna for CASSANDRA-1246
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyOutputFormat only writes the first column,CASSANDRA-1842,12492961,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jeromatron,brandon.williams,brandon.williams,12/10/2010 17:24,3/12/2019 14:13,3/13/2019 22:24,12/12/2010 4:03,0.7.0 rc 3,,,,0,,,,,,"In CASSANDRA-1774 we fixed a problem where only the last column was being written.  However, it appears that we only write the first one now.  This is easy to observe in the word count example:

{noformat}
RowKey: text2
=> (column=word1, value=1, timestamp=1291666461685000)
{noformat}

is what the output for text2 looks like, but there should be another column, word2.  If the word count is run without CFOF it works correctly.",,,,,,,,,,,,,,,,,,,,12/Dec/10 03:12;jeromatron;1842-2.txt;https://issues.apache.org/jira/secure/attachment/12466092/1842-2.txt,12/Dec/10 02:35;jbellis;1842.txt;https://issues.apache.org/jira/secure/attachment/12466091/1842.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,35:11.6,,,no_permission,,,,,,,,,,,,20340,,,Sun Dec 12 03:49:56 UTC 2010,,,,,,0|i0g7qv:,92685,,,,,,,,,,,12/Dec/10 02:35;jbellis;This is a bug where the WordCount example incorrectly wraps a byte array that is subsequently re-used w/o copying.  Fix attached.,12/Dec/10 03:12;jeromatron;Added some housekeeping to the patch like a semi-colon to the README and some naming in WordCount.java - also tested.,"12/Dec/10 03:49;hudson;Integrated in Cassandra-0.7 #71 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/71/])
    copy Text bytes because it gets re-used
patch by jbellis; tested by Jeremy Hanna for CASSANDRA-1842
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
subcolumn-reading code doesn't properly account for tombstoned columnfamily ,CASSANDRA-484,12437776,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/10/2009 1:44,3/12/2019 14:13,3/13/2019 22:24,10/12/2009 17:08,0.4,,,,0,,,,,,"mostly getColumnFamily returns null if there was no data for the query, but it can also return an empty CF if it is tombstoned.  (for 0.5, consider returning empty CFs for no data so we don't have this confusion.)",,,,,,,,,,,,,,,,,,,,10/Oct/09 01:45;jbellis;484-04.patch;https://issues.apache.org/jira/secure/attachment/12421793/484-04.patch,10/Oct/09 01:45;jbellis;TestCassandra.java;https://issues.apache.org/jira/secure/attachment/12421794/TestCassandra.java,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,19714,,,Mon Oct 12 17:08:02 UTC 2009,,,,,,0|i0fz7r:,91303,,,,,,,,,,,"10/Oct/09 01:45;jbellis;ramzi's test case, lightly modified",12/Oct/09 17:08;jbellis;committed w/ system test to 0.4 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in AntiEntropyService.getNeighbors,CASSANDRA-1028,12463120,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,jbellis,jbellis,4/27/2010 15:25,3/12/2019 14:13,3/13/2019 22:24,7/23/2010 18:03,0.7 beta 1,,,,0,,,,,,"Sometimes, but not always, I see this during a test run:

    [junit] Testsuite: org.apache.cassandra.service.AntiEntropyServiceTest
    [junit] Tests run: 10, Failures: 0, Errors: 0, Time elapsed: 3.189 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 10:19:09,743 Error in executor futuretask
    [junit] java.util.concurrent.ExecutionException: java.lang.NullPointerException
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    [junit] 	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    [junit] 	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:637)
    [junit] Caused by: java.lang.NullPointerException
    [junit] 	at java.util.AbstractCollection.addAll(AbstractCollection.java:303)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:151)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.rendezvous(AntiEntropyService.java:176)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:86)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$Validator.call(AntiEntropyService.java:487)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	... 2 more
    [junit] ------------- ---------------- ---------------

Ideally it would be nice if this could cause an actual test failure when it happens.  Not sure how feasible that is.",,,,,,,,,,,,,,,,,,,,15/May/10 18:32;stuhood;0001-Clear-AES-before-begininning-the-next-test.patch;https://issues.apache.org/jira/secure/attachment/12444580/0001-Clear-AES-before-begininning-the-next-test.patch,22/Jul/10 21:15;stuhood;0001-Die-bug-die.patch;https://issues.apache.org/jira/secure/attachment/12450217/0001-Die-bug-die.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,32:39.3,,,no_permission,,,,,,,,,,,,19964,,,Tue Jul 27 13:41:02 UTC 2010,,,,,,0|i0g2k7:,91845,,,,,,,,,,,"15/May/10 18:32;stuhood;Since nothing we do with the TokenMetadata is multithreaded, I'm fairly certain this is caused by background tasks left in the AES stage after tests. This patch clears the stage during teardown. 100ish runs of the test seem to confirm that this fixes the problem.","17/May/10 14:50;jbellis;committed, thanks","18/May/10 13:31;hudson;Integrated in Cassandra #439 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/439/])
    block for AES to clear before we teardown the token metadata for the next test.  patch by Stu Hood; reviewed by jbellis for CASSANDRA-1028
","06/Jul/10 04:01;jbellis;Getting this again / still in latest trunk:

    [junit] Exception in thread ""AE-SERVICE-STAGE:1"" java.lang.NullPointerException
    [junit] 	at java.util.AbstractCollection.addAll(AbstractCollection.java:303)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:164)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.rendezvous(AntiEntropyService.java:184)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:84)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.doVerb(AntiEntropyService.java:684)
    [junit] 	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:637)
",22/Jul/10 21:15;stuhood;Double flush AE_SERVICE_STAGE to ensure that tasks triggered by existing tasks are cleared.,23/Jul/10 18:03;jbellis;committed,"27/Jul/10 13:41;hudson;Integrated in Cassandra #501 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/501/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema only fully propagates from seeds,CASSANDRA-1824,12492268,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,brandon.williams,brandon.williams,12/6/2010 18:10,3/12/2019 14:13,3/13/2019 22:24,12/14/2010 22:44,0.7.0 rc 3,,,,0,,,,,,"If you have nodes X, Y, and Z, and Y already has some schema, but X and Z do not, and X is the seed node for the cluster, X will pick up the schema from Y, but it will never propagate to Z.  If X has the schema, it will propagate to both Y and Z.",,,,,,,,,,,,,,,,,,,,14/Dec/10 22:25;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0001-always-put-schema-state-in-local-gossip.txt;https://issues.apache.org/jira/secure/attachment/12466258/ASF.LICENSE.NOT.GRANTED--v2-0001-always-put-schema-state-in-local-gossip.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,17:54.3,,,no_permission,,,,,,,,,,,,20327,,,Wed Dec 15 16:15:06 UTC 2010,,,,,,0|i0g7mv:,92667,brandon.williams,brandon.williams,,,,,,,,,"09/Dec/10 17:17;gdusbabek;can't reproduce.  Tested on 3-node clusters in trunk, 0.7 and 0.7-rc1.","09/Dec/10 20:12;brandon.williams;It takes me a few tries, but the procedure is:

1) start a non-seed, load schema
2) start other non-seed
3) start seed

Usually I can reproduce in 10 tries or less.","14/Dec/10 22:39;gdusbabek;Here is how things broke:
1. Node 2 starts up and is fed a schema. it then includes SCHEMA in its gossip state as part of *applying the migration.*
2. Node 2 is restarted. It no longer includes SCHEMA in its gossip state.
3. Start any number of nodes. None of them will get schema.

The fix puts SCHEMA in the gossip state every time schema is announced.","14/Dec/10 22:44;brandon.williams;+1, committed.","14/Dec/10 23:33;hudson;Integrated in Cassandra-0.7 #81 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/81/])
    fix breaking tests from CASSANDRA-1824
","15/Dec/10 16:15;hudson;Integrated in Cassandra #631 (See [https://hudson.apache.org/hudson/job/Cassandra/631/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair should never reuse a tree,CASSANDRA-640,12443711,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,12/18/2009 1:14,3/12/2019 14:13,3/13/2019 22:24,12/19/2009 8:06,0.5,,,,0,,,,,,"AEService currently 'caches' MerkleTrees that have been generated by the local node, and can respond to a request for a tree with a cached version. This means that despite possibly having performed a repair, the two nodes will still be holding cached trees, and can respond to one another with the cached version, resulting in redundant repairs.

Rather than a cache, the map of trees is intended to be a rendezvous point for two nodes to exchange relatively up-to-date trees. A map still might work, but other synchronization objects should be explored.",,,,,,,,,,,,,,,,,,,,19/Dec/09 07:11;stuhood;0.5-branch-640-one-use-tree-and-repair-frequency.diff;https://issues.apache.org/jira/secure/attachment/12428529/0.5-branch-640-one-use-tree-and-repair-frequency.diff,18/Dec/09 08:50;stuhood;640-one-use-tree-and-repair-frequency.diff;https://issues.apache.org/jira/secure/attachment/12428411/640-one-use-tree-and-repair-frequency.diff,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,26:50.5,,,no_permission,,,,,,,,,,,,19795,,,Sat Dec 19 08:06:34 UTC 2009,,,,,,0|i0g06f:,91459,,,,,,,,,,,"18/Dec/09 08:50;stuhood;Here is a patch containing a fix for the issue mentioned above, and 1 other important issue:
 * To prevent redundant repairs, when a local tree is generated, it is compared to each neighbor tree exactly once before being GCd. We keep per-neighbor references to the tree in the 'trees' map, and they are removed as remote trees arrive (or as they time out).
 * Natural (automatic) repairs can happen if a major compaction is triggered automatically. This patch adds a maximum frequency at which these automatic repairs can happen (hardcoded to 1 hour in this patch). The issue was that even during simple stress.py tests, the repair process would kick off multiple times, and waste a lot of effort.

This patch also removes the 'final' keyword from Pair<T1,T2>, while adding it to hashCode and equals (which should give roughly the same performance benefits as calling the whole class final). When Pair was final, it was harder to create self-explanatory tuples.

Thanks!",18/Dec/09 18:26;junrao;The patch looks reasonable to me.,19/Dec/09 06:21;lenn0x;Can you rebase this to apply on cassandra-0.5 branch? It does not apply cleanly,19/Dec/09 07:11;stuhood;Attaching a version of the patch rebased for 0.5.,"19/Dec/09 08:06;lenn0x;Typo in commit message, but this has been commited to trunk and 05 branch. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fat clients are never removed,CASSANDRA-1730,12479741,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,11/11/2010 19:17,3/12/2019 14:13,3/13/2019 22:24,12/3/2010 21:05,0.6.9,,,,0,,,,,,"After a failed bootstrap, these lines repeat infinitely:

 INFO [Timer-0] 2010-11-11 01:58:32,708 Gossiper.java (line 406) FatClient /10.104.73.164 has been silent for 3600000ms, removing from gossip
 INFO [GMFD:1] 2010-11-11 01:59:03,685 Gossiper.java (line 591) Node /10.104.73.164 is now part of the cluster

Changing the IP on the node but using the same token causes a conflict, requiring either a full cluster restart or changing the token.  This is especially easy to run into in practice in a virtual environment such as ec2.",,,,,,,,,,,,,,,,,,,,03/Dec/10 20:18;brandon.williams;1730.txt;https://issues.apache.org/jira/secure/attachment/12465270/1730.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,50:53.8,,,no_permission,,,,,,,,,,,,20278,,,Sat Dec 11 07:35:12 UTC 2010,,,,,,0|i0g71b:,92570,,,,,,,,,,,23/Nov/10 19:50;jbellis;is this related to CASSANDRA-1518?,23/Nov/10 19:53;brandon.williams;I don't think so since this was encountered on 0.6,"03/Dec/10 20:18;brandon.williams;The problem here is that node A removes the fat client, but node B's timeout hasn't been exceeded yet, so it still has it.  RING_DELAY elapses, and B propagates the fat client back to A, and when B does remove it A will do the inverse.  Part of the problem here appears to be that the FD is adaptive and RING_DELAY is static, and since both occurrences of this I know of have been on ec2, I suspect somehow the difference between all nodes marking the client as dead has exceeded RING_DELAY.  Since by way of CASSANDRA-644 the 1h timeout for fatclients appears to have been chosen arbitrarily, I think we should reduce this to RING_DELAY / 2, giving justRemovedEndpoints plenty of leeway to prevent re-gossiping the fatclient to other nodes that have already removed it and evicted it from jRE.  Trivial patch attached. ",03/Dec/10 20:22;jbellis;+1,03/Dec/10 21:05;brandon.williams;Committed.,"03/Dec/10 22:01;hudson;Integrated in Cassandra-0.6 #16 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/16/])
    Reduce FatClient timeout to RING_DELAY / 2.  Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1730.
","07/Dec/10 20:30;hudson;Integrated in Cassandra #615 (See [https://hudson.apache.org/hudson/job/Cassandra/615/])
    ","11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping new node causes RowMutationVerbHandler Couldn't find cfId,CASSANDRA-1645,12478033,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,bterm,bterm,10/21/2010 21:26,3/12/2019 14:13,3/13/2019 22:24,10/28/2010 21:25,0.7.0 rc 1,,,,0,,,,,,"Existing 0.7.0-beta2 cluster adding 1 new node with no data data on it. Enable bootstrapping and start new node and received stream of Couldn't find cfId, added keyspaces via cli and errors stopped. Node did not bootstrap.


ERROR [MUTATION_STAGE:6] 2010-10-21 15:46:58,950 RowMutationVerbHandler.java (line 81) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1011
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:113)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:365)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:375)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:21] 2010-10-21 15:46:58,950 RowMutationVerbHandler.java (line 81) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1011
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:113)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:365)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:375)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:31] 2010-10-21 15:46:58,950 RowMutationVerbHandler.java (line 81) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1016
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:113)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:365)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:375)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MUTATION_STAGE:4] 2010-10-21 15:46:58,950 RowMutationVerbHandler.java (line 81) Error in row mutation",,,,,,,,,,,,,,,,,,,,22/Oct/10 21:31;stuhood;1645.diff;https://issues.apache.org/jira/secure/attachment/12457873/1645.diff,28/Oct/10 21:20;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0001-FBU.decodeToUtf8-duplicates-the-BB-so-other-threads-ca.txt;https://issues.apache.org/jira/secure/attachment/12458280/ASF.LICENSE.NOT.GRANTED--v2-0001-FBU.decodeToUtf8-duplicates-the-BB-so-other-threads-ca.txt,28/Oct/10 21:20;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0002-BytesToken.convertByteBuffer-duplicates-the-BB-so-othe.txt;https://issues.apache.org/jira/secure/attachment/12458281/ASF.LICENSE.NOT.GRANTED--v2-0002-BytesToken.convertByteBuffer-duplicates-the-BB-so-othe.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,35:35.8,,,no_permission,,,,,,,,,,,,20236,,,Fri Oct 29 15:41:25 UTC 2010,,,,,,0|i0g6i7:,92484,,,,,,,,,,,"21/Oct/10 21:35;jbellis;is this reproducible for you?  if so, can you try the latest nightly build?  (https://hudson.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/)","22/Oct/10 17:41;brandon.williams;Can't reproduce against trunk, this is was likely fixed sometime after beta2.","22/Oct/10 20:48;brandon.williams;Reproduced upgrading beta2 to trunk.  The underlying cause is that there is a problem deserializing the schema, but the bootstrap continues on anyway and then doesn't know about the CFs when it's done.

 INFO 20:43:00,007 Sleeping 90000 ms to wait for load information...
ERROR 20:43:00,291 Error in ThreadPoolExecutor
java.lang.ClassCastException: org.apache.avro.generic.GenericData$Record cannot be cast to org.apache.cassandra.avro.KsDef
        at org.apache.cassandra.db.migration.avro.AddKeyspace.put(AddKeyspace.java:24)
        at org.apache.avro.generic.GenericDatumReader.setField(GenericDatumReader.java:152)
        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:142)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:114)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:118)
        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:142)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:114)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:105)
        at org.apache.cassandra.io.SerDeUtils.deserializeWithSchema(SerDeUtils.java:98)
        at org.apache.cassandra.db.migration.Migration.deserialize(Migration.java:262)
        at org.apache.cassandra.db.DefinitionsUpdateResponseVerbHandler.doVerb(DefinitionsUpdateResponseVerbHandler.java:57)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR 20:43:00,293 Fatal exception in thread Thread[ReadStage:1,5,main]
java.lang.ClassCastException: org.apache.avro.generic.GenericData$Record cannot be cast to org.apache.cassandra.avro.KsDef
        at org.apache.cassandra.db.migration.avro.AddKeyspace.put(AddKeyspace.java:24)
        at org.apache.avro.generic.GenericDatumReader.setField(GenericDatumReader.java:152)
        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:142)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:114)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:118)
        at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:142)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:114)
        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:105)
        at org.apache.cassandra.io.SerDeUtils.deserializeWithSchema(SerDeUtils.java:98)
        at org.apache.cassandra.db.migration.Migration.deserialize(Migration.java:262)
        at org.apache.cassandra.db.DefinitionsUpdateResponseVerbHandler.doVerb(DefinitionsUpdateResponseVerbHandler.java:57)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)


","22/Oct/10 21:43;brandon.williams;+1, committed.","23/Oct/10 12:49;hudson;Integrated in Cassandra #574 (See [https://hudson.apache.org/hudson/job/Cassandra/574/])
    Return correct SpecificDatumReader for schema records.  Patch by Stu Hood, reviewed by brandonwilliams for CASSANDRA-1645.
","27/Oct/10 22:30;bterm;upgraded thrift to 0.5.0, upgraded cassandra to 0.7.0-rc1, removed all data and logs. used default configuration other than changing ip addresses. started cassandra and created keyspaces via cli and started sending data to cassandra. 


ERROR [MutationStage:19] 2010-10-27 17:21:46,739 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1018
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:368)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:378)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:336)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MutationStage:25] 2010-10-27 17:21:46,739 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1013
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:368)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:378)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:336)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MutationStage:27] 2010-10-27 17:21:46,739 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1018
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:368)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:378)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:336)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MutationStage:18] 2010-10-27 17:21:46,739 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1013
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:368)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:378)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:336)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MutationStage:14] 2010-10-27 17:21:46,739 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1018
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:368)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:378)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:336)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)","28/Oct/10 14:17;bterm;steps taken to reproduced it on latest trunk (r1028108):
Stop all running instances of cassandra. remove all files from data and log directories. build source from trunk or grab rc1. change cassandra.yaml seed list, thrift ip addresses and partitioner (using ordered). Start all nodes and watch logs to ensure they cluster. use cassandra-cli on node1 to configure keyspaces with a replication factor of 3, all column families are using comparator = bytestype. start sending data to the cluster (using N python clients). Errors start occurring for RowMutationVerbHandler.java Error in row mutation ...",28/Oct/10 21:13;jbellis;+1 for duplicate fix,"29/Oct/10 15:41;hudson;Integrated in Cassandra #580 (See [https://hudson.apache.org/hudson/job/Cassandra/580/])
    BytesToken.convertByteBuffer duplicates the BB so other threads can trust its position. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1645
FBU.decodeToUtf8 duplicates the BB so other threads can trust its position. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1645
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ivy.jar is included in the binary distribution,CASSANDRA-2046,12496610,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,stephenc,stephenc,1/24/2011 22:15,3/12/2019 14:13,3/13/2019 22:24,1/24/2011 22:21,0.7.1,,,,0,,,,,,"The build currently copys ivy.xml into the bin.tar.gz

according to Eric, this is a bug


-rw-r--r-- 0/0          910990 2011-01-06 16:46 apache-cassandra-0.7.0/lib/ivy-2.1.0.jar
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,21:12.0,,,no_permission,,,,,,,,,,,,20412,,,Mon Jan 24 22:34:10 UTC 2011,,,,,,0|i0g8zr:,92887,,,,,,,,,,,24/Jan/11 22:21;urandom;committed.,"24/Jan/11 22:34;hudson;Integrated in Cassandra-0.7 #202 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/202/])
    do not install ivy jar to artifacts

Patch by eevans for CASSANDRA-2046
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RejectedExecutionException in getKeyRange,CASSANDRA-165,12425259,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/12/2009 19:19,3/12/2019 14:13,3/13/2019 22:24,5/13/2009 22:37,0.3,,,,0,,,,,,"Haven't been able to repro in unit tests but fails in system tests about 50% of the time.

java.lang.RuntimeException: java.util.concurrent.RejectedExecutionException
	at org.apache.cassandra.service.RangeVerbHandler.doVerb(RangeVerbHandler.java:27)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:46)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.RejectedExecutionException
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1760)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:92)
	at org.apache.cassandra.db.Memtable.sortedKeyIterator(Memtable.java:412)
	at org.apache.cassandra.db.Table.getKeyRangeUnsafe(Table.java:912)
	at org.apache.cassandra.db.Table.getKeyRange(Table.java:883)

",,,,,,,,,,,,,,,,,,,,13/May/09 19:58;jbellis;165-2-v2.patch;https://issues.apache.org/jira/secure/attachment/12408036/165-2-v2.patch,13/May/09 17:38;jbellis;165-2.patch;https://issues.apache.org/jira/secure/attachment/12408017/165-2.patch,12/May/09 20:43;jbellis;CASSANDRA-165.patch;https://issues.apache.org/jira/secure/attachment/12407918/CASSANDRA-165.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,35:16.7,,,no_permission,,,,,,,,,,,,19580,,,Wed May 13 22:37:19 UTC 2009,,,,,,0|i0fx9r:,90988,,,,,,,,,,,"12/May/09 21:34;jbellis;[commented on 163 initially by mistake]

here is the problem

    void put(String key, ColumnFamily columnFamily, CommitLog.CommitLogContext cLogCtx) throws IOException
    {
        isDirty_ = true;
        executor_.submit(new Putter(key, columnFamily));
        if (isThresholdViolated())
        {
            enqueueFlush(cLogCtx);
        }
    }

(enqueueFlush is the one that swaps out this memtable for a new one in CFS and calls shutdown)

the problem is that we submit first and ask questions later, so we can clearly submit to this [old] memtable on one thread after another thread starts the shutdown.

the only option I see is going to back to the old double-checked-ish logic of checking threshold first, then recursing to resubmit if we switch memtables. (overriding TPE.execute/addIfUnderMaximumPoolSize is not an option since liberal use of private variables is made.)",12/May/09 22:35;urandom;+1,"12/May/09 22:48;urandom;Spoke too soon. After more local test runs, I got a RejectedExecutionException in the same place.","13/May/09 09:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    to avoid adding work to terminating executor (which will raise an exception) we need to check for threshold violated _first_, and move puts to the new memtable once flush has started.  patch by jbellis; reviewed by Eric Evans for 
","13/May/09 17:38;jbellis;    replace executor with locking.  the interaction between the executor service terminating and the CFS
    was inherently unsafe -- you would have to lock anyway to make it safe, the atomic reference wasn't
    enough, so at that point you might as well get rid of the executor.
","13/May/09 19:58;jbellis;fixes regression mentioned in IRC.  (need to use a writelock for the put call too, not just the memtable switch.  duh.)",13/May/09 22:01;urandom;+1,13/May/09 22:37;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnComparatorFactoryTest test fails,CASSANDRA-82,12422834,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,4/14/2009 22:57,3/12/2019 14:13,3/13/2019 22:24,4/15/2009 0:22,0.3,,,,0,,,,,,ColumnComparatorFactoryTest test fails in unit test. NullPointer exception.,,,,,,,,,,,,,,,,,,,,14/Apr/09 22:59;junrao;issue_82.patch;https://issues.apache.org/jira/secure/attachment/12405474/issue_82.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,22:22.6,,,no_permission,,,,,,,,,,,,19539,,,Wed Apr 15 00:22:22 UTC 2009,,,,,,0|i0fwrj:,90906,,,,,,,,,,,14/Apr/09 23:00;junrao;attached a patch.,15/Apr/09 00:22;jbellis;looks good.  applied.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Methods removed from FileUtils break CassandraServiceDataCleaner in contrib/javautils,CASSANDRA-1522,12474614,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,zznate,zznate,zznate,9/20/2010 17:14,3/12/2019 14:13,3/13/2019 22:24,9/20/2010 18:47,0.7 beta 2,,,,0,,,,,,CassandraServiceDataCleaner relied on methods in FileUtils that were removed in revision: 998464,,,,,,,,,,,,,,,,,,,,20/Sep/10 17:15;zznate;1522.patch;https://issues.apache.org/jira/secure/attachment/12455059/1522.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,47:29.9,,,no_permission,,,,,,,,,,,,20178,,,Mon Sep 20 18:47:29 UTC 2010,,,,,,0|i0g5kv:,92334,,,,,,,,,,,"20/Sep/10 17:15;zznate;re-adds removed methods, leaving other changes internal intact","20/Sep/10 18:47;brandon.williams;+1, Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avro get_range_slices should default to CL.ONE,CASSANDRA-1456,12473167,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jeromatron,jeromatron,jeromatron,9/2/2010 17:46,3/12/2019 14:13,3/13/2019 22:24,9/2/2010 17:52,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,02/Sep/10 17:47;jeromatron;0001-Adding-default-for-CL.ONE-for-get_range_slices.patch;https://issues.apache.org/jira/secure/attachment/12453705/0001-Adding-default-for-CL.ONE-for-get_range_slices.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,52:35.8,,,no_permission,,,,,,,,,,,,20149,,,Thu Sep 02 17:52:35 UTC 2010,,,,,,0|i0g567:,92268,,,,,,,,,,,"02/Sep/10 17:47;jeromatron;Added patch to make default in method itself, as defaults in IDL don't work like they do in thrift.",02/Sep/10 17:52;brandon.williams;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra leaks FDs,CASSANDRA-283,12429861,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,ieure,ieure,7/8/2009 21:34,3/12/2019 14:13,3/13/2019 22:24,7/13/2009 18:56,0.4,,,,0,,,,,,"Cassandra leaks file descriptors like crazy. I started getting these errors after a few hours of uptime:

java.lang.RuntimeException: java.io.FileNotFoundException: /var/cassandra/data/Digg-Items-2-Data.db (Too many open files)
	at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:84)
	at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:181)
	at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:859)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.FileNotFoundException: /var/cassandra/data/Digg-Items-2-Data.db (Too many open files)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
	at org.apache.cassandra.io.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:141)
	at org.apache.cassandra.io.SequenceFile$BufferReader.init(SequenceFile.java:811)
	at org.apache.cassandra.io.SequenceFile$Reader.<init>(SequenceFile.java:743)
	at org.apache.cassandra.io.SequenceFile$BufferReader.<init>(SequenceFile.java:805)
	at org.apache.cassandra.io.SequenceFile$ColumnGroupReader.<init>(SequenceFile.java:248)
	at org.apache.cassandra.io.SSTableReader.getColumnGroupReader(SSTableReader.java:346)
	at org.apache.cassandra.db.SSTableColumnIterator.<init>(ColumnIterator.java:61)
	at org.apache.cassandra.db.ColumnFamilyStore.getSliceFrom(ColumnFamilyStore.java:1589)
	at org.apache.cassandra.db.Table.getRow(Table.java:596)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:60)
	at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:600)
	at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:303)
	at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:80)

I have an open file limit of 1024. Examining the lsof output for Cassandra shows 975 FDs for the same file: /var/cassandra/data/Digg-Items-2-Data.db

Clearly, these FDs are leaking somewhere.","Debian Etch, J2SE 1.6.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,31:09.5,,,no_permission,,,,,,,,,,,,19619,,,Mon Jul 13 18:56:21 UTC 2009,,,,,,0|i0fxzj:,91104,,,,,,,,,,,10/Jul/09 15:31;junrao;There exists code in get_slice() that closes all files it opens at the end. Could you describe your scenario a bit more? Are you just issuing get_slice calls? How many requests were issued?,10/Jul/09 15:37;jbellis;Let's re-test after committing CASSANDRA-287 so there's only one code path to debug. :),"10/Jul/09 15:43;jbellis;Actually now that I think of it I did fix a FD leak in 287 already.  In the code

               iter = filter.getSSTableColumnIterator(sstable);
               if (iter.hasNext())
               {
                   returnCF.delete(iter.getColumnFamily());
                   iterators.add(iter);
               }
               else
               {
                   iter.close();
               }

the ""else"" clause didn't exist before, and since it wasn't added to the list of iterators it wouldn't get closed at the end.","11/Jul/09 15:01;jbellis;CASSANDRA-287 is committed.

Ian, do you still see leaking?","13/Jul/09 18:18;ieure;We're not seeing this right now, though I suspect the issue still exists somewhere.

We determined that the problem was caused because Cassandra corrupted it's data files in some way which caused it to leak FDs when calling get_slice. After removing the data files, we couldn't reproduce the issue anymore.

So there's the issue with the initial corruption, which we haven't reproduced, and the issue with the FDs, which we might be able to reproduce at some point if the corruption can be reproduced.
","13/Jul/09 18:56;jbellis;I'm going to close this then because CASSANDRA-287 fixes the most obvious FD leak.  Feel free to reopen if you have any more problems.

(Did you save a copy of the bad sstable files?)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.cassandra.config.CFMetaData defines equals but does not define hashCode,CASSANDRA-945,12460992,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mdennis,mdennis,mdennis,4/2/2010 3:07,3/12/2019 14:13,3/13/2019 22:24,4/6/2010 21:45,0.7 beta 1,,,,0,,,,,,"org.apache.cassandra.config.CFMetaData defines equals but does not define hashCode

On a related note, it should probably be using org.apache.commons.lang.builder.[EqualsBuilder | HashCodeBuider]",,,,,,,,,,,,,,,,,,,,03/Apr/10 02:07;mdennis;CASSANDRA-945.patch;https://issues.apache.org/jira/secure/attachment/12440669/CASSANDRA-945.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,33:59.8,,,no_permission,,,,,,,,,,,,19933,,,Wed Apr 07 12:41:55 UTC 2010,,,,,,0|i0g21r:,91762,,,,,,,,,,,03/Apr/10 02:07;mdennis;patch against trunk r930452,"06/Apr/10 20:33;stuhood;+1

Thanks Matt.","06/Apr/10 21:45;jbellis;committed, thanks!","07/Apr/10 12:41;hudson;Integrated in Cassandra #400 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/400/])
    add CFMetaData.hashCode.  patch by Matthew Dennis; reviewed by Stu Hood for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect IOException in SequenceFile when BF returns false positive,CASSANDRA-126,12424341,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,4/30/2009 20:42,3/12/2019 14:13,3/13/2019 22:24,5/1/2009 2:25,,,,,0,,,,,,"In SequenceFile.AbstractReader.seekTo(), an IOException is thrown when a key can't be found in an SSTable. This is incorrect. The BF associated with the SSTable could return a false positive. So a key may indeed not be present in the SSTable file. We need to handle this case accordingly.",,,,,,,,,,,,,,,,,,,,30/Apr/09 23:00;jbellis;126-v2.patch;https://issues.apache.org/jira/secure/attachment/12406972/126-v2.patch,30/Apr/09 20:44;junrao;issue126.patch_v1;https://issues.apache.org/jira/secure/attachment/12406957/issue126.patch_v1,30/Apr/09 21:59;junrao;issue126.patch_v1.test;https://issues.apache.org/jira/secure/attachment/12406962/issue126.patch_v1.test,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,57:18.8,,,no_permission,,,,,,,,,,,,19559,,,Fri May 01 13:09:51 UTC 2009,,,,,,0|i0fx1b:,90950,,,,,,,,,,,30/Apr/09 20:44;junrao;Attach a fix that deals with non-existing keys in SSTables explicitly.,"30/Apr/09 20:57;jbellis;If I understand what you are doing, shouldn't you take out the IOException catches that deal with the current way of doing things? e.g. in SSTable,

            catch( IOException ex )
            {
                logger_.info(""Bloom filter false positive"", ex);
            }

I'd also like to see a test case that exercises this code path.","30/Apr/09 21:07;jbellis;sorry, I was looking at the wrong git repo.  I don't see any exceptions anymore that you'd want to change, either.",30/Apr/09 21:59;junrao;Add a testcase that covers the code path when a BF gives a false positive.,"30/Apr/09 23:00;jbellis;thanks, looks good.

i'm not comfortable w/ introducing more code duplication, though.  what do you think of this version?",30/Apr/09 23:58;junrao;patch v2 looks good to me.,01/May/09 02:25;jbellis;committed,"01/May/09 13:09;hudson;Integrated in Cassandra #55 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/55/])
    avoid raising an exception in SequenceFile.next when a key does not exist (e.g. when bloom filter gives a false positive).  patch by Jun Rao; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load spikes,CASSANDRA-2170,12498681,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,jbellis,jbellis,2/15/2011 19:46,3/12/2019 14:13,3/13/2019 22:24,10/4/2011 17:28,,,,,0,,,,,,"as reported on CASSANDRA-2058, some users are still seeing load spikes on 0.6.11, even with fairly low-volume read workloads.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,40:52.5,,,no_permission,,,,,,,,,,,,18533,,,Wed Oct 12 17:46:46 UTC 2011,,,,,,0|i0g9qv:,93009,,,,,,,,,,,15/Feb/11 19:48;jbellis;AFAIK nobody has seen this on 0.7.1.,09/Mar/11 17:16;jbellis;I wonder if this could have been CASSANDRA-2175.,09/Mar/11 17:16;jbellis;(I.e. memory pressure caused by key cache preheating being too aggressive.),14/Mar/11 13:45;jbellis;No new reports of similar behavior on 0.6.11 or 0.6.12,"18/May/11 22:40;brandon.williams;I never saw anyone reliably report this on any platform except ec2, so I strongly the suspect the cause was what is covered in this link:

https://silverline.librato.com/blog/main/EC2_Users_Should_be_Cautious_When_Booting_Ubuntu_10_04_AMIs","10/Aug/11 05:08;alienth;Re-opening per request of driftx.

So, still seeing this problem ever since our upgrade from 0.6.7.

It is 100% consistent on 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.8.0, 0.8.1. I've tried Sun JRE and OpenJDK. Tried with JNA and without. Tried Ubuntu 08.04/10.04/10.10/11.04, as well as RHEL5.1. It *only* happens on coordinator nodes.

For the 0.8 ring, I created a brand new ring and added data from our app one CF at a time. As soon as I added a busy CF, the problem popped up again. The load on the boxes in the new ring is under 1 all the time, except for when the load spike occurs.",15/Aug/11 22:23;brandon.williams;From the comment [here|https://issues.apache.org/jira/browse/CASSANDRA-2845?focusedCommentId=13083455&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13083455] I would suggest try running without jsvc (if you are now and have JNA enabled),"15/Sep/11 19:34;alienth;Tried without jsvc, same result.

This problem is also consistent on 0.8.5 on Debian Squeeze. I replicated the issue on a coordinator which only owned 1 token.

The node I'm replicating it on now is at a load of 0 almost all of the time, except during the spikes.

The symptoms continue to appear identical to #2054","16/Sep/11 21:27;alienth;I did a couple more thread dumps on spiking nodes. One interesting thing I'm seeing is that there are a high number of NBHM threads in the runnable state during the load spikes. One the dumps I analyzed, there were often 200-300 of these threads in RUNNABLE.

Here is an example of the threads: https://gist.github.com/ef215227b85bdff5f033","17/Sep/11 20:13;scode;Wow, interesting. Are you sure it's 0.8.5 though? The stack trace is not matching what I see in the 0.8.5 tag (mismatched line number for MessagingService.addCallback()).

We've been seeing load spikes on 0.7, but havent reported it because it's such an old version. However we were never able to grab stacks because no JMX query would ever succeed during this condition.

The stack trace indicates it's stuck doing resize operations on the NBHM where each thread is trying to help the resizing operation along by performing potentially duplicate (for forward progress producing) work.

Do you have a list of all stacks? Do you find any thread (should be 0 or 1) that are executing in ExpiringMap.CacheMonitor.run() at the time of the load spikes?

I guess we're seeing some kind of fallen-and-cant-get-up senario having to do with the resize. Maybe dogpiling the resize is making it overall slow enough that it never gets unstuck without a temporary stop in incoming requests. Or some such. That's gut feely speculation without having actually looked at it carefully, so take it with a grain of salt :)
","27/Sep/11 23:29;alienth;The stack trace in that example was from a 0.8.1 node. The same problem has occurred on my 0.8.5 nodes.

I checked the stack trace that I had, and I couldn't find any threads executing in any ExpiringMap stuff.

We switched to HSHA about a week ago and it appears to have resolved the loads pikes. Is the stack trace I gave interesting enough to pursue further investigation into the issue, or should I just leave the answer as 'HSHA' ?","04/Oct/11 17:28;brandon.williams;bq. Is the stack trace I gave interesting enough to pursue further investigation into the issue, or should I just leave the answer as 'HSHA' ?

I'm content with calling this 'ec2 sucks when there are tons of threads.' (Jason told me he's opening 2000+ connections)","12/Oct/11 17:46;brandon.williams;FWIW, I was able to finally reproduce this by throwing a ton of connections at a cold cassandra instance.  Increasing the rpc_min_threads initially seems to help avoid locking the machine up briefly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CliClient contains an unused, unavailable import",CASSANDRA-637,12443605,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,zznate,zznate,zznate,12/17/2009 2:06,3/12/2019 14:13,3/13/2019 22:24,1/4/2010 19:34,0.5,,,,0,,,,,,"The following library is included in src/java/org/apache/cassandra/cli/CliClient.java yet is not used or included via the pom:

import org.hamcrest.core.IsSame
",,,,,,,,,,,,,,,,,,,,17/Dec/09 02:08;zznate;patch.txt;https://issues.apache.org/jira/secure/attachment/12428251/patch.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,34:13.4,,,no_permission,,,,,,,,,,,,19793,,,Mon Jan 04 19:34:13 UTC 2010,,,,,,0|i0g05r:,91456,,,,,,,,,,,"04/Jan/10 19:34;urandom;This import was already removed, by a patch you submitted to CASSANDRA-636. Thanks... again :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ant release target doesn't include all jars in binary tarball,CASSANDRA-850,12458223,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,johanoskarsson,johanoskarsson,3/5/2010 10:18,3/12/2019 14:13,3/13/2019 22:24,3/26/2010 17:53,0.6,,,,0,,,,,,"The ant release target doesn't create a complete tarball, the jars in build/lib/jars are not included.",,,,,,,,,,,,,,,,,,,,25/Mar/10 15:53;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-850-runtime-dependencies-formerly-handled-by.txt;https://issues.apache.org/jira/secure/attachment/12439786/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-850-runtime-dependencies-formerly-handled-by.txt,25/Mar/10 15:53;urandom;ASF.LICENSE.NOT.GRANTED--v2-0002-licensing-and-attribution-for-newly-added-jars.txt;https://issues.apache.org/jira/secure/attachment/12439787/ASF.LICENSE.NOT.GRANTED--v2-0002-licensing-and-attribution-for-newly-added-jars.txt,25/Mar/10 15:53;urandom;ASF.LICENSE.NOT.GRANTED--v2-0003-remove-default-runtime-ivy-config.txt;https://issues.apache.org/jira/secure/attachment/12439788/ASF.LICENSE.NOT.GRANTED--v2-0003-remove-default-runtime-ivy-config.txt,25/Mar/10 15:53;urandom;ASF.LICENSE.NOT.GRANTED--v2-0004-remove-build-lib-jars-from-runtime-search-paths.txt;https://issues.apache.org/jira/secure/attachment/12439789/ASF.LICENSE.NOT.GRANTED--v2-0004-remove-build-lib-jars-from-runtime-search-paths.txt,25/Mar/10 15:53;urandom;ASF.LICENSE.NOT.GRANTED--v2-0005-do-not-ship-build.xml-and-ivy.xml-in-binary-dist.txt;https://issues.apache.org/jira/secure/attachment/12439790/ASF.LICENSE.NOT.GRANTED--v2-0005-do-not-ship-build.xml-and-ivy.xml-in-binary-dist.txt,25/Mar/10 15:53;urandom;ASF.LICENSE.NOT.GRANTED--v2-0006-update-release-notes-and-readme-for-ivy-rollback.txt;https://issues.apache.org/jira/secure/attachment/12439791/ASF.LICENSE.NOT.GRANTED--v2-0006-update-release-notes-and-readme-for-ivy-rollback.txt,05/Mar/10 10:21;johanoskarsson;CASSANDRA-850.patch;https://issues.apache.org/jira/secure/attachment/12437991/CASSANDRA-850.patch,25/Mar/10 22:19;urandom;v2-0007-remove-gratuitous-copy-of-junit-from-lib.patch;https://issues.apache.org/jira/secure/attachment/12439832/v2-0007-remove-gratuitous-copy-of-junit-from-lib.patch,25/Mar/10 22:19;urandom;v2-0008-reference-lib-licenses-from-LICENSE.txt.patch;https://issues.apache.org/jira/secure/attachment/12439833/v2-0008-reference-lib-licenses-from-LICENSE.txt.patch,,,,,9,,,,,,,,,,,,,,,,,,,25:59.9,,,no_permission,,,,,,,,,,,,19889,,,Mon Mar 29 15:42:30 UTC 2010,,,,,,0|i0g1gn:,91667,,,,,,,,,,,05/Mar/10 10:21;johanoskarsson;This patch makes the release target include all the jars,"05/Mar/10 13:25;jbellis;My understanding is that this is deliberate, or we'd have to go back to maintaining NOTICE for all those.","05/Mar/10 13:52;johanoskarsson;Isn't the point of a binary release tarball that it should work with minimal effort out of the box? 
Surely the overhead of maintaining the notice file is a tiny bit of extra effort for a small group of developers and of a major benefit to a large group of users? I would agree that it might not be needed in the source version though.","05/Mar/10 17:09;urandom;> Isn't the point of a binary release tarball that it should work with minimal effort out of the box?

The added effort basically boils down to invoking `ant ivy-retrieve' no?

> Surely the overhead of maintaining the notice file is a tiny bit of extra effort for a small group of developers and of a major benefit to a large group of users?

I think ""tedious"" is a better adjective than ""tiny"". Also, despite the very best of intentions, it wasn't being kept up properly (which is a pretty common outcome of tedious manual tasks).

We traded:

* manual dependency management
* the requirement to document license and attribution
* license incompatibility issues (caused by redistribution)

For:

* requiring ant to be installed (which doesn't seem to be too onerous)
* requiring network connectivity
* invoking `ant ivy-retrieve'

I'm not opposed to returning to the past practice of embedding all of the jars, especially if someone is stepping up to do a better job of maintaining this, but I think the changeset needs to revert Ivy, and include the jars and the necessary changes to NOTICE and LICENSE .
","10/Mar/10 16:21;jbellis;Johan mentioned in IRC that http://ant.apache.org/ivy/history/latest-milestone/ivyfile/license.html could allow auto-generating LICENSE and NOTICE.

This is metadata from the maven repo right?  Not something we would maintain ourselves in ivy.xml?","10/Mar/10 16:28;jbellis;For the record, the official description of NOTICE is as follows (from http://www.apache.org/legal/src-headers.html#notice):

   0.  Every Apache distribution should include a NOTICE file in the top directory, along with the standard LICENSE file.
   1. The top of each NOTICE file should include the following text, suitably modified to reflect the product name and year(s) of distribution of the current and past versions of the product:

                Apache [PRODUCT_NAME]
                Copyright [yyyy] The Apache Software Foundation

                This product includes software developed at
                The Apache Software Foundation (http://www.apache.org/).
        

   2. The remainder of the NOTICE file is to be used for required third-party notices. The NOTICE file may also include copyright notices moved from source files submitted to the ASF.

The only official description of LICENSE I found is http://apache.org/dev/apply-license.html#new, which seems to mean that LICENSE should always be the ASL 2.

I'm not sure where the practice of ""include a copy of each dependency's license in LICENSE"" comes from; I couldn't find it documented anywhere.","10/Mar/10 17:01;urandom;> I'm not sure where the practice of ""include a copy of each dependency's license in LICENSE"" comes from; I couldn't find it documented anywhere.

There is obviously a hard requirement to include licensing information for everything with a license, but the requirement to put it all in LICENSE came from individuals on general@incubator and wasn't universally agreed upon.

Some folks thought it was enough to include it, others thought that it should at least be discoverable via LICENSE, and others still were adamant that they be entirely contained with LICENSE. We went with the latter primarily to avoid controversy (we needed release votes).","23/Mar/10 15:26;gdusbabek;Have we reached consensus on this?  If not, I want to offer my opinion.

We started using ivy for several reasons, but the big problem I thought it was solving was avoiding license management.  If we end up having to update LICENSE.txt/NOTICE.txt manually in order to ship a full binary, I prefer to go back to keeping the dependencies in svn.  Let's rid of ivy.",23/Mar/10 15:30;jbellis;+1 getting rid of ivy if we're manually updating license files.,"23/Mar/10 17:47;urandom;I suppose we could keep it around for managing build dependencies, stuff that we're unwilling or unable to check into subversion, (rat, cobertura, and maybe parts of antlr). I don't know if that's enough justification; just throwing out the option.","24/Mar/10 23:50;urandom;0001: This patch places a copy of everything that used to be downloaded by the ivy ""default"" conf (i.e. `ant ivy-retrieve'), into lib/ (it probably requires git to apply this patch).

0002: Adds license and attribution info for the new jars, and moves the existing third-party license info from LICENSE to lib/licenses.

0003: Cleans out everything from the ivy ""default"" conf that has since been moved to lib/, and merges the ""build"" and ""qa"" confs to simplify things. Also makes the necessary changes to build.xml so that Ivy is only used to manage build dependencies.

0004: Adjusts jar file search path in scripts and batch files.

0005: Fixes the release target to no longer include build.xml and ivy.xml in the binary dist.

0006: Updates the release notes and readme files.

It would be great to have another set of eyes on this, I can't hardly imagine *not* missing something in all of these changes.","25/Mar/10 03:49;jbellis;I think the commons-logging and httpclient ivy lines are just there for contrib/word_count (i was lazy).  We could totally just r/m those entirely (and any dependencies those had, in turn?) and let people who want to run the contrib example get them manually or w/ ivy.","25/Mar/10 15:56;urandom;commons-{logging,httpclient} have been removed from the v2 patches.","25/Mar/10 21:11;johanoskarsson;+1, looks good to me. 

Couple of thoughts:
* Is there a reason to keep the junit 3.8.1 jar in lib and also use ivy to fetch junit 4.6?
* Due to my lack of legal expertise I will assume the bits in the notice file are sufficient.
* I remember someone voiced the opinion earlier that licenses should be pulled into a single file in the root, can't remember which of license/notice it was. Personally I think that this approach with one license file per jar approach is much more user friendly and surely it must be as valid from a legal standpoint?","25/Mar/10 21:31;jbellis;1. all (?) our junit tests require junit 4, so that should be an easy call. :)
3. that was someone on incubator-general pissing on our release because it wasn't The Way He Liked, there was no legal reason that per-file was invalid brought up in that discussion","25/Mar/10 22:11;urandom;> Is there a reason to keep the junit 3.8.1 jar in lib and also use ivy to fetch junit 4.6? 

We were explicitly pulling in junit 4.6 to use for our unit tests, and implicitly pulling in junit 3.8.1 as a dependency for at least one of those other jars in lib/ (Ivy would resolve that to 4.6 only). So that explains why it is the way it is, though I do suspect that the jar(s) that declared a dependency on junit did so in error, (or we simply don't exercise those parts of the lib(s)). 

The unit tests, system tests and cassandra-cli (one of those aforementioned jars is jline, used by the cli) all seem to work after removing lib/junit-3.8.1.jar , so it's probably safe to leave it out.

> Due to my lack of legal expertise I will assume the bits in the notice file are sufficient. 

I'm not sure it would satisfy everyone on the incubator list, but it is correct with respect to the legal requirements, at least according to my understanding of the respective licenses (actually, it errs on the side of caution wherever I wasn't 100% sure).

> I remember someone voiced the opinion earlier that licenses should be pulled into a single file in the root

That was something else that came up on the incubator list, and TTBOMK there wasn't consensus for this either. I can find no such requirement in my reading of the actual licenses. I can however append a sentence or two to LICENSE that refers to lib/license for third-party libs, (I think that would satisfy that camp).","25/Mar/10 22:20;urandom;0007: removes junit 3.8.1 and the corresponding license file

0008: adds a reference to lib/licenses from LICENSE.txt","26/Mar/10 22:41;mike.javorski;Sorry.. I'm a bit late to the party (and did so because of the 6MB delta download git just pulled).  Wouldn't managing the ivy.xml and the NOTICES file in concert meet the ASF legal requirements? You could run ivy-retrieve as part of the dist process and eliminate the requirement for ant and ivy for the end user, and still get the benefits on the development side.

On a related note (and I realize this doesn't effect the official SVN users),  git users will get a nice jump in repo size every time there is a lib change with this route. Shouldn't be a regular occurrence, but if it's easily avoided, why not make it ""nicer""

I am happy to take a crack at a patch to this effect if there is interest.

[Edit:] looks like the first patch of this issue would have done that, so I guess no patch needed from me","29/Mar/10 15:42;urandom;Ivy dependency retrieval is a dynamic process, I really don't see how you could make any guarantees that an ivy.xml and NOTICE/LICENSE pair maintained in lock-step would remain in syn.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read repair of tombstones on columnfamilies and supercolumns,CASSANDRA-87,12423157,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,4/17/2009 19:14,3/12/2019 14:13,3/13/2019 22:24,4/24/2009 0:46,0.3,,,,0,,,,,,existing RR code only repairs column tombstones (since normally CF/SC do not have values associated w/ them directly).,,,,,,,,,,,,,,,,,,,,22/Apr/09 23:33;jbellis;0001-clean-up-Read-Repair-code-to-use-non-deprecated-APIs.patch;https://issues.apache.org/jira/secure/attachment/12406184/0001-clean-up-Read-Repair-code-to-use-non-deprecated-APIs.patch,22/Apr/09 23:33;jbellis;0002-use-resolve-moved-to-CF-from-CFS-rather-than-dupli.patch;https://issues.apache.org/jira/secure/attachment/12406186/0002-use-resolve-moved-to-CF-from-CFS-rather-than-dupli.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,26:34.6,,,no_permission,,,,,,,,,,,,19543,,,Fri Apr 24 00:46:52 UTC 2009,,,,,,0|i0fwsn:,90911,,,,,,,,,,,"22/Apr/09 23:34;jbellis;02
    use resolve (moved to CF from CFS) rather than duplicating code in repair to do the
    same thing.  this automatically gets us most of the way to handling tombstones in  
    read repair.  the rest is minor tweaks to CF.diff and SC.diff.  added tests.       

01
    clean up Read Repair code to use non-deprecated APIs
","23/Apr/09 22:26;junrao;Patch looks fine to me.

It seems to me that arg to Row.repair should be rowNew, instead of rowOld.","23/Apr/09 22:53;jbellis;the idea was to suggest a difference between repair, where the row passed in should be incorporated into ""this"", and diff, where the row passed in is a superset of ""this"".

suggestions?","23/Apr/09 22:56;junrao;how about targetRow?
","24/Apr/09 00:45;jbellis;going with rowOther and [row|cf]Composite, respectively",24/Apr/09 00:46;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snapshot should include index and bloomfile,CASSANDRA-353,12432458,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sammy.yu,sammy.yu,sammy.yu,8/7/2009 0:23,3/12/2019 14:13,3/13/2019 22:24,8/7/2009 17:08,0.4,,Legacy/Tools,,0,,,,,,,,,,,,,,,,,,,,,,,,,,07/Aug/09 02:40;sammy.yu;0001-Work-for-cassandra-353.patch;https://issues.apache.org/jira/secure/attachment/12415808/0001-Work-for-cassandra-353.patch,07/Aug/09 14:23;sammy.yu;0002-Work-for-cassandra-353.patch;https://issues.apache.org/jira/secure/attachment/12415852/0002-Work-for-cassandra-353.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,51:17.2,,,no_permission,,,,,,,,,,,,19647,,,Sat Aug 08 12:35:20 UTC 2009,,,,,,0|i0fyf3:,91174,,,,,,,,,,,"07/Aug/09 02:40;sammy.yu;snapshot now includes index and filter file.
",07/Aug/09 12:51;jbellis;that looks funny to me -- isn't that linking the same filename to 3 targets?,"07/Aug/09 14:23;sammy.yu;Jonathon, thanks for spotting this.   source is now fixed :)
",07/Aug/09 17:08;jbellis;committed,"08/Aug/09 12:35;hudson;Integrated in Cassandra #161 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/161/])
    include index and bloom filter in snapshots.  patch by Sammy Yu; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Easy to OOM on log replay since memtable limits are ignored,CASSANDRA-609,12442690,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,12/8/2009 2:09,3/12/2019 14:13,3/13/2019 22:24,12/9/2009 4:55,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,08/Dec/09 17:10;jbellis;609-2.patch;https://issues.apache.org/jira/secure/attachment/12427350/609-2.patch,08/Dec/09 02:32;jbellis;609.patch;https://issues.apache.org/jira/secure/attachment/12427281/609.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,35:37.6,,,no_permission,,,,,,,,,,,,19778,,,Wed Dec 09 12:42:41 UTC 2009,,,,,,0|i0fzzj:,91428,,,,,,,,,,,"08/Dec/09 02:32;jbellis;respect memtable thresholds when replaying commit log; use normal write path, but add a writeCommitLog flag so we don't re-write out another CL entry for the replay","08/Dec/09 16:28;jbellis;To clarify: under low load, you won't much exceed a single memtable's worth of inserts since when a memtable flushes it marks the commitlog header as ""start replay from here.""  But under high load, you can have a significant amount of inserts done while flushes are queued up in sort + write executors, and this is where you run into trouble on replay.","08/Dec/09 16:35;junrao;During recovery, we used to rely on CFS.switchMemtable to call onMemtableFlush(ctx) to discard commit log files. With this patch, onMemtableFlush(ctx) won't be called during recovery. When will the commit log files be deleted?
","08/Dec/09 17:10;jbellis;I was thinking that the final table.flush is a ""normal"" flush so that should take care of it.  But thinking about it more we do have a corner case of ""the last write in the log happened to exactly hit the threshold and triggered a no-discard flush, so the final flush saw a clean memtable and did nothing.""

Patch 2 (applies on top of first) addresses this.  (Most of the patch is converting CL to a true singleton so replay has access to the normal context operations w/o ugly hacks.)",09/Dec/09 01:13;junrao;My bad. I got confused about how log files are deleted during recovery. It turns out that all log files are deleted explicitly in RecoveryManager.doRecovery(). The deletion doesn't rely on the flushing logic. So even v1 of the patch is fine.,"09/Dec/09 01:36;jbellis;I didn't know that, either.  I think that's a bug -- if something goes wrong during recovery we definitely shouldn't be deleting data that hasn't been replayed.  I'd feel safer taking that out, what do you think?","09/Dec/09 02:15;junrao;Currently, CL.discardCompletedSegments() doesn't really discard log files properly during recovery. The problem is that clHeaders_ is never populated during recovery.

Deleting log files explicitly during recovery may not be that bad. If anything goes wrong during recovery, we will get either an IOException or a RuntimeException. In either case, log files won't be deleted.","09/Dec/09 04:47;jbellis;You're right, because it assumes that when CL is instantiated it's starting fresh.  I'll commit patch 1.","09/Dec/09 12:42;hudson;Integrated in Cassandra #282 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/282/])
    respect memtable thresholds when replaying commit log
patch by jbellis; reviewed by Jun Rao for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Findbugs: Static initializers,CASSANDRA-614,12442762,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,stuhood,stuhood,12/8/2009 19:04,3/12/2019 14:13,3/13/2019 22:24,12/14/2009 20:00,0.6,,,,0,,,,,,"We have a few singleton classes that are not doing their lazy initialization correctly (we really need a generic solution to this problem, sigh). In most cases, we are missing 'volatile' on the static field. See attached.

In order to find unused method parameters for CASSANDRA-608, I ran FindBugs against Cassandra, and found a few interesting issues we ought to explore (but not the unused method params, oi.)",,,,,,,,,,,,,,,,,,,,08/Dec/09 19:42;jbellis;614.patch;https://issues.apache.org/jira/secure/attachment/12427377/614.patch,08/Dec/09 19:04;stuhood;lazy-init.txt;https://issues.apache.org/jira/secure/attachment/12427368/lazy-init.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,42:13.3,,,no_permission,,,,,,,,,,,,19782,,,Thu Dec 17 22:03:02 UTC 2009,,,,,,0|i0g00n:,91433,,,,,,,,,,,08/Dec/09 19:42;jbellis;fixes attached,10/Dec/09 03:19;stuhood;Looks good to me. +1,14/Dec/09 20:00;jbellis;rebased & committed,"17/Dec/09 22:03;hudson;Integrated in Cassandra #291 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/291/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
race condition in compaction makes it possible to return null when data in fact exists,CASSANDRA-160,12425108,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/11/2009 15:33,3/12/2019 14:13,3/13/2019 22:24,5/13/2009 2:29,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,19575,,,Wed May 13 02:29:24 UTC 2009,,,,,,0|i0fx8n:,90983,,,,,,,,,,,13/May/09 02:29;jbellis;fixed by patch 02 for CASSANDRA-161,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix cobertura coverage report,CASSANDRA-167,12425323,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,johanoskarsson,johanoskarsson,johanoskarsson,5/13/2009 11:51,3/12/2019 14:13,3/13/2019 22:24,5/13/2009 20:53,0.3,,,,0,,,,,,"Since the move to Junit the cobertura reports have broken, we should fix it.",,,,,,,,,,,,,,,,,,,,13/May/09 11:51;johanoskarsson;CASSANDRA-167.patch;https://issues.apache.org/jira/secure/attachment/12407987/CASSANDRA-167.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,53:45.5,,,no_permission,,,,,,,,,,,,19581,,,Thu May 14 11:00:54 UTC 2009,,,,,,0|i0fxa7:,90990,,,,,,,,,,,13/May/09 11:51;johanoskarsson;Fixes the cobertura reporting,"13/May/09 20:53;jbellis;committed.

I notice that ""ant cobertura"" is about 4x slower than ""ant test,"" but i guess that can't be helped with forking on.","14/May/09 11:00;hudson;Integrated in Cassandra #75 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/75/])
    fix cobetura report.  patch by johano; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log replay bugs,CASSANDRA-264,12429099,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,6/29/2009 15:29,3/12/2019 14:13,3/13/2019 22:24,6/30/2009 4:06,0.4,,,,0,,,,,,#NAME?,,,,,,,,,,,,,,,,,,,,29/Jun/09 17:49;jbellis;0002-v2.patch;https://issues.apache.org/jira/secure/attachment/12412089/0002-v2.patch,29/Jun/09 16:54;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-264-don-t-rm-when-replay-fails.txt;https://issues.apache.org/jira/secure/attachment/12412081/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-264-don-t-rm-when-replay-fails.txt,29/Jun/09 16:54;jbellis;ASF.LICENSE.NOT.GRANTED--0002-add-test-catching-buggy-update-of-header-on-flush-ref.txt;https://issues.apache.org/jira/secure/attachment/12412082/ASF.LICENSE.NOT.GRANTED--0002-add-test-catching-buggy-update-of-header-on-flush-ref.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,36:06.0,,,no_permission,,,,,,,,,,,,19614,,,Tue Jun 30 13:00:43 UTC 2009,,,,,,0|i0fxvj:,91086,,,,,,,,,,,"29/Jun/09 16:56;jbellis;02 reads
  add test catching buggy update of header on flush; refactor so there is only one version of code doing those writes (the correct one)

the bug was in the writeCommitLogHeader called by discard; it didn't write the length first so readLong() on replay would get basically garbage",29/Jun/09 17:36;junrao;The patch looks fine. I don't see any new test cases added though and am wondering how this bug escaped the unit test before.,"29/Jun/09 17:49;jbellis;oops, forgot to add the new test to 0002.  here is the new version of that patch.

the old test doesn't catch it because it doesn't add enough data to cause a flush, which is where the commitlogheader corruption happened.",29/Jun/09 18:01;junrao;The new patch looks good.,30/Jun/09 04:06;jbellis;committed,"30/Jun/09 13:00;hudson;Integrated in Cassandra #124 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/124/])
    add test catching buggy update of header on flush; refactor so there is only one version of code doing those writes (the correct one).
patch by jbellis; reviewed by Jun Rao for 
don't remove commitlog files when replay fails; you lose the chance to fix a bug, as well as your data.
patch by jbellis; reviewed by Jun Rao for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testNameSort fails,CASSANDRA-59,12422158,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,johanoskarsson,johanoskarsson,4/6/2009 21:00,3/12/2019 14:13,3/13/2019 22:24,4/16/2009 20:42,0.3,,,,0,,,,,," [testng] FAILED: testNameSort
   [testng] java.lang.AssertionError
   [testng]     at org.apache.cassandra.db.ColumnFamilyStoreTest.validateNameSort(ColumnFamilyStoreTest.java:188)
   [testng]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testNameSort(ColumnFamilyStoreTest.java:72)
   [testng] ... Removed 22 stack frames
","java version ""1.6.0_07"", Ubuntu",,,,,,,,,,CASSANDRA-85,,,,,,,,,15/Apr/09 21:29;jbellis;59.patch;https://issues.apache.org/jira/secure/attachment/12405582/59.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,38:15.2,,,no_permission,,,,,,,,,,,,19530,,,Thu Apr 16 20:42:18 UTC 2009,,,,,,0|i0fwmf:,90883,,,,,,,,,,,"06/Apr/09 22:38;jbellis;The problem is that waitForFlush doesn't always do what it says.

I'm working on a more elegant solution but in the meantime adding Thread.sleep(1000) at the end of waitForFlush will work around this.",06/Apr/09 22:44;jbellis;committed the sleep workaround temporarily.,"15/Apr/09 21:30;jbellis;patch to make forceFlush block until the flush action is queued on MemtableManager.  That way calling forceFlush; waitForFlush will be guaranteed that the action waitFF puts on MtM will run after the flush completes, i.e., the wait will actually do what it's supposed to.
","15/Apr/09 21:33;jbellis;Johan, can you review?",16/Apr/09 20:30;urandom;Looks good to me. +1,16/Apr/09 20:42;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix the EndPointState time bookmarking in Gossiper ,CASSANDRA-13,12419509,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,zhu han,zhu han,3/26/2009 14:48,3/12/2019 14:13,3/13/2019 22:24,4/22/2009 18:41,0.3,,,,0,,,,,,"In Gossiper#doStatusCheck(line 485 to line 489)
The code is
                long l = now - epState.getUpdateTimestamp(); --> should be : ""long l = epState.getUpdateTimestamp();""
                long duration = now - l;
                if ( !epState.isAlive() && (duration > aVeryLongTime_) )
                {
                    evictFromMembership(endpoint);
                }
",,,60,60,,0%,60,60,,,,,,,,,,,,22/Apr/09 16:02;jbellis;13.patch;https://issues.apache.org/jira/secure/attachment/12406146/13.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,42:31.3,,,no_permission,,,,,,,,,,,,19516,,,Wed Apr 22 18:41:21 UTC 2009,,,,,,0|i0fwd3:,90841,,,,,,,,,,,"03/Apr/09 13:42;jbellis;Hi Zhu,

You should join us on #cassandra on freenode IRC.  If you don't have an irc client, this web-based one is ok: http://www.mibbit.com/?server=irc.freenode.net&channel=%23cassandra&nick=yournamehere",22/Apr/09 16:02;jbellis;fix duration calculation to avoid evicting dead endpoints instantly,22/Apr/09 18:41;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_column should enforce a column specified in input,CASSANDRA-92,12423384,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,4/21/2009 16:21,3/12/2019 14:13,3/13/2019 22:24,4/21/2009 22:15,,,,,0,,,,,,"Today, if one specified something like: get_column 'Table1' 'row1' 'Standard1', one gets a random single column. This is incorrect. Users of get_column should always provide a CF and at least a column name, not just the CF.
",,,,,,,,,,,,,,,,,,,,21/Apr/09 16:23;junrao;issue92.patch001;https://issues.apache.org/jira/secure/attachment/12406037/issue92.patch001,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,15:08.2,,,no_permission,,,,,,,,,,,,19546,,,Tue Apr 21 22:15:08 UTC 2009,,,,,,0|i0fwtr:,90916,,,,,,,,,,,"21/Apr/09 16:23;junrao;Attach a fix that block throws an exception if get_column doesn't specify a column.
",21/Apr/09 22:15;jbellis;applied,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_column call broken with recent ReadCommand change,CASSANDRA-90,12423321,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,4/21/2009 0:25,3/12/2019 14:13,3/13/2019 22:24,4/21/2009 2:59,,,,,0,,,,,,get_column returns null on an existing column. ,,,,,,,,,,,,,,,,,,,,21/Apr/09 00:30;junrao;issue90.patch001;https://issues.apache.org/jira/secure/attachment/12405974/issue90.patch001,21/Apr/09 00:31;junrao;issue90.patch002;https://issues.apache.org/jira/secure/attachment/12405975/issue90.patch002,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,59:28.1,,,no_permission,,,,,,,,,,,,19544,,,Tue Apr 21 02:59:28 UTC 2009,,,,,,0|i0fwtb:,90914,,,,,,,,,,,"21/Apr/09 00:30;junrao;Attach a testcase that identifies the problem. The problem is caused because when colnames is not provided in ReadCommand, colnames_ doesn't have the same reference as EMPTY_COLUMNS. This broken the test in ReadCommand.doRow().",21/Apr/09 00:31;junrao;Attach a fix.,"21/Apr/09 02:59;jbellis;+1

committed w/ minor modification of changing the test to !columnNames.isEmpty() instead of trying to preserve EMPTY_COLUMNS equality.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memtable sometimes not enqueued for flush,CASSANDRA-320,12431601,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,7/28/2009 16:21,3/12/2019 14:13,3/13/2019 22:24,7/29/2009 15:05,,,,,0,,,,,,"There appears to be a (rare) race condition with flushing memtables.   Occasionally, when Cassandra is in the process of flushing a table, another switch will occur.  The latter table will be added to memtablesPendingFlush in ColumnFamilyStore, but it will not be enqueued for flush and thus hang around forever.  Adding an else clause and a print statement to Memtable.java's enqueueFlush method reveals that the memtable is not being flushed because isFrozen_ is already true.",Debian lenny amd64 sun jdk 1.6,,,,,,,,,,,,,,,,,,,28/Jul/09 17:01;jbellis;320.patch;https://issues.apache.org/jira/secure/attachment/12414767/320.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,01:22.5,,,no_permission,,,,,,,,,,,,19635,,,Thu Jul 30 12:34:28 UTC 2009,,,,,,0|i0fy7r:,91141,,,,,,,,,,,28/Jul/09 17:01;jbellis;combine switchMemtable and enqueueFlush so that they always operate on the same object.  move forceFlush guts from MT to CFS since it encapsulates better there,"29/Jul/09 14:44;brandon.williams;+1

I'm no longer able to reproduce the issue with this patch after many millions of sustained inserts.","30/Jul/09 12:34;hudson;Integrated in Cassandra #152 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/152/])
    combine switchMemtable and enqueueFlush so that they always operate on the same object.  move forceFlush guts from MT to CFS since it encapsulates better there
patch by jbellis; reviewed by Brandon Williams for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLog.add doesn't really force to disk,CASSANDRA-182,12425601,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,sandeep_tata,sandeep_tata,5/15/2009 16:17,3/12/2019 14:13,3/13/2019 22:24,7/28/2009 2:45,0.4,,,,0,,,,,,CommitLog.add does't really force writes to disk. This could result in acked writes being lost.,,,,,,,,,,,,,,,,,,,,27/Jul/09 15:45;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-182-handle-incomplete-CL-entries-on-recover.txt;https://issues.apache.org/jira/secure/attachment/12414626/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-182-handle-incomplete-CL-entries-on-recover.txt,27/Jul/09 15:45;jbellis;ASF.LICENSE.NOT.GRANTED--0002-mv-AbstractWriter-to-its-own-top-level-class-and-remov.txt;https://issues.apache.org/jira/secure/attachment/12414627/ASF.LICENSE.NOT.GRANTED--0002-mv-AbstractWriter-to-its-own-top-level-class-and-remov.txt,27/Jul/09 15:45;jbellis;ASF.LICENSE.NOT.GRANTED--0003-naive-fsync-after-each-log-entry.txt;https://issues.apache.org/jira/secure/attachment/12414628/ASF.LICENSE.NOT.GRANTED--0003-naive-fsync-after-each-log-entry.txt,27/Jul/09 15:45;jbellis;ASF.LICENSE.NOT.GRANTED--0004-threadpoolexecutor.txt;https://issues.apache.org/jira/secure/attachment/12414629/ASF.LICENSE.NOT.GRANTED--0004-threadpoolexecutor.txt,27/Jul/09 15:45;jbellis;ASF.LICENSE.NOT.GRANTED--0005-custom-CommitLogExecutorService-that-can-fsync-per-mul.txt;https://issues.apache.org/jira/secure/attachment/12414630/ASF.LICENSE.NOT.GRANTED--0005-custom-CommitLogExecutorService-that-can-fsync-per-mul.txt,27/Jul/09 15:45;jbellis;ASF.LICENSE.NOT.GRANTED--0006-config-options.txt;https://issues.apache.org/jira/secure/attachment/12414631/ASF.LICENSE.NOT.GRANTED--0006-config-options.txt,27/Jul/09 15:45;jbellis;ASF.LICENSE.NOT.GRANTED--0007-arrayblockingqueue.txt;https://issues.apache.org/jira/secure/attachment/12414632/ASF.LICENSE.NOT.GRANTED--0007-arrayblockingqueue.txt,27/Jul/09 15:45;jbellis;ASF.LICENSE.NOT.GRANTED--0008-updates.txt;https://issues.apache.org/jira/secure/attachment/12414633/ASF.LICENSE.NOT.GRANTED--0008-updates.txt,15/May/09 16:41;sandeep_tata;CASSANDRA-182.patch;https://issues.apache.org/jira/secure/attachment/12408264/CASSANDRA-182.patch,,,,,9,,,,,,,,,,,,,,,,,,,22:30.7,,,no_permission,,,,,,,,,,,,19586,,,Tue Jul 28 13:29:27 UTC 2009,,,,,,0|i0fxdb:,91004,,,,,,,,,,,"15/May/09 16:41;sandeep_tata;1. Added a class in SequenceFile called SyncWriter
2. New configuration option CommitLogForceLogs if you want ""safe"" writes. Not turning this on leaves everything else the same as before. All this does is use SyncWriter for the logger.

For trunk and 0.4 we should revisit the options we want to allow on the logs, especially once we re-architect the logger to use batched forces.","15/May/09 17:22;jbellis;The more I look at the commitlog code the more I'm inclined to go with my initial reaction, that we shouldn't mess with this for 0.3... :-|

The FastSync option looks like A&P's attempt to deal with this.  It does call force() on close, for instance.  (Is that really all we need for durability?)  Having two options for syncing, one of which is commented to have issues and the other is virtually entirely untested, doesn't really seem like an improvement to me.

If we did go with ""let's add a third option"" then IMO we should be doing the force in CommitLog not the writer.  updateHeader for instance does a looping write w/ a single writer; i assume only one sync for the entire method is needed, not one sync per write.

For 0.3 I suggest emailing A&P off-list and see if they can clarify why fastsync has problems purging log files.  If it is something simple to fix great.  Otherwise let's note to ""use fastsync for durable writes, but this has the drawback of not purging log files.  fixing this is a priority for 0.4""","15/May/09 18:38;sandeep_tata;FastSync (inappropriately named, IMHO) option tells the CommitLog to use a FastConcurrentWriter. This certainly doesn't force to disk, and therefore is not an option for durable writes.

If we don't add a third option for 0.3 which forces to disk, we simply have *no* durable writes. Even on blocking writes, it is possible that the data has not hit disk in any of the replica's logs.

We have two options:
1. Release as is, and say ""we don't offer durable writes yet""
2. Add an option to force the logs and say ""this is currently a low-performing option and will be improved for 0.4""

We can do the forces in CommitLog (probably a cleaner approach in terms of eliminating unnecessary forces, but updateHeader writes only for CFs encountered for the first time).  The approach in CASSANDRA-182.patch seemed minimally invasive to me :-)

Building a well-tested high-performance logger will take significantly more time. (Just testing the logger for correctness is non-trivial.)","15/May/09 19:02;jbellis;FCW does force on close, leading me to believe that for whatever reason A&P felt that was all that was needed to call the option ""Sync.""

Of course I could be reading too much into this.  Always a possibility in ""code archaeology."" :)","21/May/09 19:54;jbellis;Assuming we don't hear back from Avinash & Prashant, I suggest that we include a ""known issues"" section in the README for 0.3 noting this problem.  I'm not comfortable with basing the CL on brand-new code.

For 0.4 let's take a closer look at FastSync and either build on that or rip it out and go with the approach in your patch (either in CL or the writer).  I'd like to see this coupled with a CL executor that force()s less than once per mutation if there is a constant stream of writes, similar to postgresql's commit_delay setting (http://www.postgresql.org/docs/8.3/static/runtime-config-wal.html).","22/May/09 14:53;sandeep_tata;I haven't heard back from A&P.
I'm fine with releasing as is and saying logging doesn't work correctly yet and that it'll be fixed soon.",22/May/09 14:54;sandeep_tata;Won't fix in 0.3,"21/Jul/09 22:39;jbellis;Are you working on this, Sandeep?  If not I will take it.","21/Jul/09 23:16;sandeep_tata;Nope, I'm not looking at this right now. ","22/Jul/09 01:15;jbellis;confirmed that naive fsync-per-write kills performance -- i'm seeing 1/10 the throughput.

will try the executor approach I mentioned above.","22/Jul/09 06:01;sandeep_tata;I did figure how to avoid using force(true) to flush the metadata in addition to the data (2 disk forces instead of one). If you preallocate the pages for the log file, you need to force(true) only when you have to grab more pages for the log. This would of course require some extra work in the logger code so the end of the log can be tracked without depending on file size/end of file.","22/Jul/09 14:49;jbellis;the executor approach is looking really good, but that's worth keeping in mind down the road.","25/Jul/09 05:35;jbellis;07
    switch to arrayblockingqueue; it's a little cleaner and performance seems unaffected

06
    config options.  increasing write threads to 32 (was 4) allows performance to be mostly decent again, especially if you crank the delay up to 10ms

05
    custom CommitLogExecutorService that can fsync per multiple CL additions

04
    threadpoolexecutor, just using an out-of-the-box executor as a first step

03
    naive fsync-after-each-log-entry.  performance _sucks._

02
    mv AbstractWriter to its own top-level class and remove redundant IFileWriter

01
    handle incomplete CL entries on recover (a fairly important bug fix)
","27/Jul/09 15:45;jbellis;removed old patches, attached rebased ones","27/Jul/09 22:41;junrao;The patch looks good to me. Thanks, Jonathan.
",28/Jul/09 02:45;jbellis;committed,"28/Jul/09 13:29;hudson;Integrated in Cassandra #151 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/151/])
    Use arrayblockingqueue in commitlog executor; this cleans up the code a bit (performance is unaffected since the writes and syncs are far more expensive than any queue ops)
patch by jbellis; reviewed by Jun Rao for 
add config options for commitlog syncing
patch by jbellis; reviewed by Jun Rao for 
custom CommitLogExecutorService that can fsync per multiple CL additions
patch by jbellis; reviewed by Jun Rao for 
move log ops to callables on a threadpoolexecutor instead of synchronizing.  this prepares the way to merge multiple add() calls into a single sync.
patch by jbellis; reviewed by Jun Rao for 
naive fsync-after-each-log-entry
patch by jbellis; reviewed by Jun Rao for 
mv AbstractWriter to its own top-level class and remove redundant IFileWriter
patch by jbellis; reviewed by Jun Rao for 
handle incomplete CL entries on recover
patch by jbellis; reviewed by Jun Rao for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SuperColumn.getSubColumn fails assertion when subcolumn is not present,CASSANDRA-91,12423322,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,4/21/2009 1:15,3/12/2019 14:13,3/13/2019 22:24,4/21/2009 3:07,0.3,,,,0,,,,,,"When a subColumn is not present, SuperColumn.getSubColumn fails an assertion instead of simply returning null. Returning null will let us write much cleaner unit tests for supercolumns. ",,,,,,,,,,,,,,,,,,,,21/Apr/09 01:16;sandeep_tata;CASSANDRA-91.patch;https://issues.apache.org/jira/secure/attachment/12405978/CASSANDRA-91.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,07:36.0,,,no_permission,,,,,,,,,,,,19545,,,Tue Apr 21 03:07:58 UTC 2009,,,,,,0|i0fwtj:,90915,,,,,,,,,,,21/Apr/09 01:16;sandeep_tata;This has to be the easiest patch to review :-) Who wants it?,"21/Apr/09 03:07;jbellis;SuperColumn patch did not apply (!?) but not a big deal.  Changed assert to 

        assert column == null || column instanceof Column;

also removed the assert on value(String key) that was kind of a no-op (value() would error out if the isinstance were false, and there was a separate throw for null checking).",21/Apr/09 03:07;jbellis;(applied),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compilation fails in eclipse,CASSANDRA-86,12423025,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,4/16/2009 16:45,3/12/2019 14:13,3/13/2019 22:24,4/16/2009 18:24,,,,,0,,,,,,"Although ant build works, CassandraServer,java fails to compile in eclipse. Complaining about missing com.facebook.thrift.TException.",Eclipse on Windows,,,,,,,,,,,,,,,,,,,16/Apr/09 17:32;jbellis;cassandra86-v2.patch;https://issues.apache.org/jira/secure/attachment/12405673/cassandra86-v2.patch,16/Apr/09 16:49;junrao;cassandra86.patch;https://issues.apache.org/jira/secure/attachment/12405666/cassandra86.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,32:55.4,,,no_permission,,,,,,,,,,,,19542,,,Thu Apr 16 17:46:10 UTC 2009,,,,,,0|i0fwsf:,90910,,,,,,,,,,,"16/Apr/09 16:49;junrao;Attach a patch. It seems the problem is when referring to FacebookBase, which is probably compiled with the old thrift package. 

Going forward, we should either include the source code in libfb303.jar or remove all dependencies on it.
","16/Apr/09 17:32;jbellis;Like I said in IRC, I prefer to get rid of it entirely.  Does the attached patch look OK to you?",16/Apr/09 17:46;junrao;V2 patch looks good to me. We should also remove lib/libfb303.jar.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hinted handoff will give up if there are not unique nodes to store each hint,CASSANDRA-496,12438320,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/16/2009 17:01,3/12/2019 14:13,3/13/2019 22:24,10/20/2009 17:49,0.5,,,,0,,,,,,"this is contrary to HH's design, which should be to allow writes as long as the number of nodes specified by ConsistencyLevel can receive the write (which may be zero)",,,,,,,,,,,,,,,,,,,,16/Oct/09 17:23;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-496-inline-getNextAvailable.txt;https://issues.apache.org/jira/secure/attachment/12422375/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-496-inline-getNextAvailable.txt,16/Oct/09 17:23;jbellis;ASF.LICENSE.NOT.GRANTED--0002-fix-HH-to-not-give-up-when-any-node-is-live.txt;https://issues.apache.org/jira/secure/attachment/12422376/ASF.LICENSE.NOT.GRANTED--0002-fix-HH-to-not-give-up-when-any-node-is-live.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,53:53.3,,,no_permission,,,,,,,,,,,,19721,,,Wed Oct 21 12:34:23 UTC 2009,,,,,,0|i0fzaf:,91315,,,,,,,,,,,16/Oct/09 19:53;urandom;+1,20/Oct/09 17:49;jbellis;committed,"21/Oct/09 12:34;hudson;Integrated in Cassandra #234 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/234/])
    fix HH to not give up when any node is live
patch by jbellis; reviewed by eevans for 
inline getNextAvailable
patch by jbellis; reviewed by eevans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misspelled ColumSort attribute results in deleted commit logs and .db files,CASSANDRA-8,12419164,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,hammer,hammer,3/21/2009 18:52,3/12/2019 14:13,3/13/2019 22:24,3/25/2009 3:56,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,21/Mar/09 19:54;armchairalligator;column_sort_fix.patch;https://issues.apache.org/jira/secure/attachment/12403375/column_sort_fix.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,54:15.3,,,no_permission,,,,,,,,,,,,19512,,,Tue Mar 24 15:47:29 UTC 2009,,,,,,0|i0fwbz:,90836,,,,,,,,,,,"21/Mar/09 18:56;hammer;From Alexander Staubo:

""""""
I was confused by the meaning of the ""ColumnSort"" attribute in the
config file, and typed something invalid. The result was the following
exception on compaction, when Cassandra starts up:

java.lang.NullPointerException
       at org.apache.cassandra.config.DatabaseDescriptor.getTypeInfo(DatabaseDescriptor.java:729)
       at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:62)
       at org.apache.cassandra.db.CompactSerializerInvocationHandler.invoke(CompactSerializerInvocationHandler.java:50)
       at $Proxy0.serialize(Unknown Source)
       at org.apache.cassandra.db.Memtable.flushForRandomPartitioner(Memtable.java:461)
       at org.apache.cassandra.db.Memtable.flush(Memtable.java:440)
       at org.apache.cassandra.db.Memtable.forceflush(Memtable.java:279)
       at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:409)
       at org.apache.cassandra.db.Table.flush(Table.java:857)
       at org.apache.cassandra.db.CommitLog.doRecovery(CommitLog.java:408)
       at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:317)
       at org.apache.cassandra.db.RecoveryManager.recoverEachTable(RecoveryManager.java:90)
       at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:77)
       at org.apache.cassandra.db.DBManager.<init>(DBManager.java:112)
       at org.apache.cassandra.db.DBManager.instance(DBManager.java:61)
       at org.apache.cassandra.service.StorageService.start(StorageService.java:465)
       at org.apache.cassandra.service.CassandraServer.start(CassandraServer.java:96)
       at org.apache.cassandra.service.CassandraServer.main(CassandraServer.java:1049)

...after which the commit logs were gone, and there were no .db files.
So my database went *poof*.

The attached patch emits a warning if the ColumnSort attribute is
invalid, and uses the appropriate default of ""Time"" in that case.

However, this fixes only one possible failure scenario for
compactions, but not the underlying problem that a failing compaction
hoses the database. In my opinion, failing compactions should never
result in the commit logs being deleted, regardless of the failure
cause.
""""""",21/Mar/09 19:54;armchairalligator;Attached patch to add warning on misconfiguration,24/Mar/09 15:47;avinash.lakshman@gmail.com;This works for now. However a more disciplined approach is required. For now this case is closed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Abort bootstrap if our IP is already in the token ring,CASSANDRA-663,12444552,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,1/4/2010 1:31,3/12/2019 14:13,3/13/2019 22:24,1/5/2010 17:16,0.5,,,,0,,,,,,Trying to bootstrap a node w/ the same IP as one that is Down but not removed from the ring should give an error.,,,,,,,,,,,,,,,,,,,,04/Jan/10 17:07;jbellis;663.txt;https://issues.apache.org/jira/secure/attachment/12429355/663.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,12:14.5,,,no_permission,,,,,,,,,,,,19809,,,Tue Jan 05 17:11:50 UTC 2010,,,,,,0|i0g0bj:,91482,,,,,,,,,,,04/Jan/10 17:12;urandom;+1,"04/Jan/10 23:32;jaakko;IMHO this check should be done before starting the bootstrap process (immediately after ""... got load info""). At that point we've already slept for RING_DELAY, so waiting for another RING_DELAY in startBootstrap is probably not useful. If we do the check at its current place, we've already started to gossip our bootsrap token, which will cause the other rightful IP owner to be removed from token metadata.
","04/Jan/10 23:46;jbellis;I'd like to have this as a sanity check on all move operations, not just the initial bootstrap.  What about putting the check at the beginning of SS.startBootstrap?","05/Jan/10 00:52;jaakko;As long as it is before gossip, it does not make so big difference.

However, I don't know what this check would achieve in a move operation. If the node is up and running in order to do a move operation, it must be in token metadata as well. Move does not change IP address, so this check for duplicate IP address is relevant only in first bootstrap.
","05/Jan/10 17:11;jbellis;you're right,  this makes the most sense after ""got load info.""  committed with that change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy.insertBlocking does not perform hinted handoff,CASSANDRA-383,12433523,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,8/20/2009 0:22,3/12/2019 14:13,3/13/2019 22:24,8/24/2009 16:49,0.4,,,,0,,,,,,"insertBlocking should use getNStorageEndPointMap (like insert) instead of just getNStorageEndPoint so that it can perform hinted handoff.

",all,,,,,,,,,,CASSANDRA-375,,,,,,,,,20/Aug/09 00:47;sandeep_tata;383-v1.patch;https://issues.apache.org/jira/secure/attachment/12417085/383-v1.patch,21/Aug/09 02:15;sandeep_tata;383-v2.patch;https://issues.apache.org/jira/secure/attachment/12417209/383-v2.patch,21/Aug/09 16:55;sandeep_tata;383-v3.patch;https://issues.apache.org/jira/secure/attachment/12417283/383-v3.patch,21/Aug/09 18:55;sandeep_tata;383-v4.patch;https://issues.apache.org/jira/secure/attachment/12417301/383-v4.patch,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,08:07.6,,,no_permission,,,,,,,,,,,,19662,,,Tue Aug 25 14:21:26 UTC 2009,,,,,,0|i0fylj:,91203,,,,,,,,,,,20/Aug/09 00:28;sandeep_tata;Need to fix this so both insert paths can echo writes to the bootstrapping nodes for CASSANDRA-375,20/Aug/09 00:47;sandeep_tata;insertBlocking now uses getNStorageEndPointMap,20/Aug/09 01:08;jbellis;eyeballing it looks ok.  will commit after i get to my machine that has Thrift installed tomorrow to run nosetests. :),20/Aug/09 16:20;jbellis;I get many nosetests failures with this patch (on a single machine ofc),"21/Aug/09 02:15;sandeep_tata;Oops, can't quite cast to EndPoint[] like I was doing.
v2 passes nosetests and ant test.","21/Aug/09 06:01;euphoria;I think this is what you'll want to do, getting rid of the temp counter.
{{{
EndPoint[] endPoints = (EndPoint[])endpointMap.keySet().toArray(new EndPoint[endpointMap.size()]);
}}}

It might be pedantic, but I also think we should be making the quorum calculation consistent, instead of
availability style
{{{
(DatabaseDescriptor.getReplicationFactor() / 2) + 1)
}}}
versus blocking style
{{{
(DatabaseDescriptor.getReplicationFactor() >> 1) + 1)
}}}

Apart from these, looks good.",21/Aug/09 16:55;sandeep_tata;Thanks -- v3 with Michaels' suggestions.,"21/Aug/09 17:02;jbellis;instead of
            if (endpointMap.size() < (DatabaseDescriptor.getReplicationFactor() / 2) + 1)
            {
                throw new UnavailableException();
            }
            int blockFor = determineBlockFor(consistency_level);

we should have

            int blockFor = determineBlockFor(consistency_level);
            if (endpointMap.size() < blockFor)
            {
                throw new UnavailableException();
            }
",21/Aug/09 17:03;jbellis;minor point: the (EndPoint[]) is redundant in the toArray call,"21/Aug/09 17:47;jbellis;rather belatedly I realized that we have a deeper problem here.

we don't want to count hint destinations as counting towards the W consistency level, or we are violating our contract that W + R > N = consistent (or unavailableexception).

we do want to write hints if for instance a quorum write is requested and we have 2 of 3 nodes live, we write the third to a hint.  but that can't count towards the quorum since that node won't be a potential read target.  so getNStorageEndPointMap is the wrong thing to do here.

i think we actually have to go below that abstraction: use getStorageEndPoints directly, count the live nodes to see if we have enough to satisfy consistency_level, write those out, and if that is less than N, then add hinted writes.","21/Aug/09 18:27;sandeep_tata;True, the blockFor semantics so far has simply been a W-durability guarantee.
If we want to guarantee W + R > N = consistent, HHO nodes can't count towards W.
","21/Aug/09 18:55;sandeep_tata;v4 fixes blockFor semantics to actually block for W live nodes and then do hinted handoff after that succeeds.

We don't want to do HHO in parallel to make sure we don't write any value in case the quorum write fails .. clients will see a failed write, but quorum reads might succeed after handoff.",24/Aug/09 16:48;jbellis;committed v4 w/ minor changes (renamed getLiveNodes -> getUnhintedNodes),"25/Aug/09 14:21;hudson;Integrated in Cassandra #177 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/177/])
    allow blocking write to create hints if not enough of the ""correct"" nodes are live, but enough are to fulfil the requested consistency level.  patch by Sandeep Tata; reviewed by jbellis and Michael Greene for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Token Ranges Do Not Wrap Correctly,CASSANDRA-495,12438245,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,10/15/2009 20:45,3/12/2019 14:13,3/13/2019 22:24,10/15/2009 22:21,0.5,,,,0,,,,,,"Token ranges for two equal tokens should be considered to wrap, but currently do not.",,,,,,,,,,,,,,,,,,,,15/Oct/09 21:48;stuhood;cassandra-495.diff;https://issues.apache.org/jira/secure/attachment/12422284/cassandra-495.diff,15/Oct/09 21:03;stuhood;cassandra-495.diff;https://issues.apache.org/jira/secure/attachment/12422278/cassandra-495.diff,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,21:38.0,,,no_permission,,,,,,,,,,,,19720,,,Fri Oct 16 12:34:26 UTC 2009,,,,,,0|i0fza7:,91314,,,,,,,,,,,"15/Oct/09 21:04;stuhood;That should do it. Also, toString didn't agree with the comments or implementation.","15/Oct/09 21:48;stuhood;This new version of the patch makes the left of the range exclusive, and the right inclusive, in order to agree with the rest of the codebase (as pointed out by junrao and jbellis).",15/Oct/09 22:21;jbellis;committed,"16/Oct/09 12:34;hudson;Integrated in Cassandra #229 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/229/])
    fix token to cover (left, right] like the rest of Cassandra (and Range.toString).  patch by Stu Hood; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make nosetest retrieve the stdout and stderr message when Cassandra fails to start,CASSANDRA-306,12430718,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sammy.yu,sammy.yu,sammy.yu,7/16/2009 22:28,3/12/2019 14:13,3/13/2019 22:24,7/17/2009 3:33,0.4,,Legacy/Tools,,0,,,,,,,,,,,,,,,,,,,,,,,,,,16/Jul/09 22:51;sammy.yu;0001-CASSANDRA-306.patch;https://issues.apache.org/jira/secure/attachment/12413753/0001-CASSANDRA-306.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,33:22.4,,,no_permission,,,,,,,,,,,,19626,,,Fri Jul 17 13:01:51 UTC 2009,,,,,,0|i0fy4n:,91127,,,,,,,,,,,16/Jul/09 22:51;sammy.yu;Made nosetest more robust in handling when Cassandra daemon doesn't start properly,"17/Jul/09 03:33;urandom;Committed, thanks Sammy!","17/Jul/09 13:01;hudson;Integrated in Cassandra #140 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/140/])
    improved error handling for system test server startup

Capture stderr and stdout when process is dead; only kill if it isn't
already dead.

Patch by Sammy Yu; reviewed by eevans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system tests fail straingely if an instance is running,CASSANDRA-282,12429827,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,euphoria,urandom,urandom,7/8/2009 17:10,3/12/2019 14:13,3/13/2019 22:24,7/9/2009 17:40,0.4,,Legacy/Tools,,0,,,,,,"The system tests blow up spectacularly if there is another instance of cassandra running (and it can be difficult to tell that this is why). Having the harness start its instance using alternate ports would prevent this from happening in some cases, but something should be done to test for instances left over from failed tests, and at least produce clearer error messages.",,,,,,,,,,,,,,,,,,,,08/Jul/09 22:49;euphoria;282-v1.diff;https://issues.apache.org/jira/secure/attachment/12412931/282-v1.diff,09/Jul/09 17:06;urandom;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-282-stop-drop-roll-if-uncleanly-shutdown.txt;https://issues.apache.org/jira/secure/attachment/12413036/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-282-stop-drop-roll-if-uncleanly-shutdown.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,49:00.4,,,no_permission,,,,,,,,,,,,19618,,,Fri Jul 10 12:35:56 UTC 2009,,,,,,0|i0fxzb:,91103,,,,,,,,,,,"08/Jul/09 22:49;euphoria;This patch shifts all the relevant ports by 10, so they are still predictable.  It also sets the timeout for a new client connection to 10 seconds, because 20 seconds gets really unreasonable if you are trying to debug things, and I can't imagine the regression test server taking longer than 10 seconds to come up.","09/Jul/09 17:09;urandom;+1 282-v1.diff

0001-CASSANDRA-282-stop-drop-roll-if-uncleanly-shutdown.txt is meant to be applied in addition to Michael's (282-v1.diff), and  causes the tests to fail with a (more )useful message if a previous run failed to cleanup.","09/Jul/09 17:32;euphoria;+1 on 0001-CASSANDRA-282-stop-drop-roll-if-uncleanly-shutdown.txt since it actually caught the error it was supposed to on my system, even though I didn't expect it.",09/Jul/09 17:40;urandom;committed.,"10/Jul/09 12:35;hudson;Integrated in Cassandra #133 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/133/])
    abort system tests if previous run shutdown uncleanly

Patch by eevans; reviewed by Michael Greene for 
move ports used in system tests; shorten timeout

Patch by Michael Greene; reviewed by eevans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_slice needs to support desc from last column,CASSANDRA-263,12429006,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,lenn0x,lenn0x,6/26/2009 22:55,3/12/2019 14:13,3/13/2019 22:24,7/31/2009 15:06,0.4,,,,0,,,,,,At the moment there's no way to ask for a slice starting with the last column and going desc,,,,,,,,,,,,,,,,,,,,30/Jul/09 23:24;junrao;issue263.patchv1;https://issues.apache.org/jira/secure/attachment/12415072/issue263.patchv1,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,31:53.4,,,no_permission,,,,,,,,,,,,19613,,,Sat Aug 01 12:34:00 UTC 2009,,,,,,0|i0fxvb:,91085,,,,,,,,,,,"26/Jun/09 23:31;junrao;Is the problem that you don't know what the last column is? Normally, an application should be able to insert a fake column that's always larger than any real columns. Do you think that's good enough?","27/Jun/09 00:22;jbellis;Yes.

I was thinking that we could have the ""CF"" format (w/ no "":"") stand for ""start w/ first if asc or last if desc.""  Reading the last from the column index should be reasonably efficient, right?",29/Jun/09 17:09;junrao;Using the column index to start with the last column group is possible and is efficient. I am just wondering if it is a good idea to give empty starting column different meanings depending on the ordering.,"29/Jun/09 17:18;jbellis;I'm open to alternative APIs, but that one makes sense to me and doesn't require adding extra parameters :)","30/Jul/09 23:24;junrao;Attach a fix. If startColumn is empty and isAscending is false, assume that we scan from the largest column in descending order.

Also fix a bug in SliceFilterQuery.filterSuperColumn() that didn't handle descending order properly.
","31/Jul/09 15:06;jbellis;committed, thanks!","01/Aug/09 12:34;hudson;Integrated in Cassandra #154 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/154/])
    allow start of [] to mean ""start with the largest value"" when ascending=false.  patch by Jun Rao; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
functional test errors on OSX,CASSANDRA-278,12429637,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,7/6/2009 19:58,3/12/2019 14:13,3/13/2019 22:24,7/8/2009 16:13,0.4,,Legacy/Tools,,0,,,,,,The -n argument to echo is (apparently )not portable. See http://gist.github.com/141615,,,,,,,,,,,,,,,,,,,,06/Jul/09 20:02;urandom;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-278-use-printf-instead-of-echo.txt;https://issues.apache.org/jira/secure/attachment/12412648/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-278-use-printf-instead-of-echo.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,38:37.6,,,no_permission,,,,,,,,,,,,19616,,,Thu Jul 09 12:35:34 UTC 2009,,,,,,0|i0fxyf:,91099,,,,,,,,,,,"08/Jul/09 04:38;euphoria;I seem to remember it being reported on IRC that this patch didn't work, but it's fixing the problem for me.  Before the patch, I get 22 failures on 10.5.7 and after the patch, all 22 pass OK.",08/Jul/09 16:13;urandom;Thanks for the feedback. Committed.,"09/Jul/09 12:35;hudson;Integrated in Cassandra #132 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/132/])
    replace non-portable use of echo -n with printf

Patch by Eric Evans; reviewed by Michael Greene for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
versioning isn't going to work when forwarding messages,CASSANDRA-2140,12498014,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,2/8/2011 18:06,3/12/2019 14:12,3/13/2019 22:24,2/9/2011 19:11,0.8 beta 1,,,,0,,,,,,SP.sendToHintedEndpoints needs to take care to create properly versioned messages that get forwarded to other nodes.,,,,,,,,,,,,,,,,,,,,09/Feb/11 18:41;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0001-cache-versioned-messages-in-StorageProxy.txt;https://issues.apache.org/jira/secure/attachment/12470699/ASF.LICENSE.NOT.GRANTED--v2-0001-cache-versioned-messages-in-StorageProxy.txt,09/Feb/11 18:41;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0002-fix-misspeeling.txt;https://issues.apache.org/jira/secure/attachment/12470700/ASF.LICENSE.NOT.GRANTED--v2-0002-fix-misspeeling.txt,09/Feb/11 18:41;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0003-MessageProducer-int-to-Integer.txt;https://issues.apache.org/jira/secure/attachment/12470701/ASF.LICENSE.NOT.GRANTED--v2-0003-MessageProducer-int-to-Integer.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,04:51.1,,,no_permission,,,,,,,,,,,,20462,,,Wed Feb 09 20:30:16 UTC 2011,,,,,,0|i0g9k7:,92979,jbellis,jbellis,,,,,,,,,"09/Feb/11 17:04;jbellis;+1 to the approach.  a couple possible improvements:

- I think all the calls to sendRR(prod.getMessage(Gossiper..), endpoint, handler) can be replaced with sendRR(prod, endpoint, handler)?
- Let's make CMP less wasteful by giving initialcapacity of 2 (common case: everyone is on same versio -- it gets multipled by load factor, so 2 gets rounded down to 1) and making getMessage take an Integer (so we auto[un]box zero times instead of 4)
",09/Feb/11 18:41;gdusbabek;v2 makes suggested improvements.,09/Feb/11 19:03;jbellis;+1,09/Feb/11 19:11;gdusbabek;committed.,"09/Feb/11 20:30;hudson;Integrated in Cassandra #719 (See [https://hudson.apache.org/hudson/job/Cassandra/719/])
    MessageProducer int to Integer. patch by gdusbabek, reviewed by jbellis. CASSANDRA-2140
fix misspeeling. patch by gdusbabek, reviewed by jbellis. CASSANDRA-2140
cache versioned messages in StorageProxy. patch by gdusbabek, reviewed by jbellis. CASSANDRA-2140
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encryption options are not validated correctly in DatabaseDescriptor,CASSANDRA-2152,12498269,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,zznate,zznate,zznate,2/10/2011 17:24,3/12/2019 14:12,3/13/2019 22:24,2/10/2011 18:43,0.8 beta 1,,,,0,,,,,,Missing configuration for encryption_options introduced via CASSANDRA-1567 result in an obtuse NPE from MessagingService,,,,,,,,,,,,,,,,,,,,10/Feb/11 17:36;zznate;2152.txt;https://issues.apache.org/jira/secure/attachment/12470786/2152.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,43:20.3,,,no_permission,,,,,,,,,,,,20467,,,Thu Feb 10 22:43:17 UTC 2011,,,,,,0|i0g9mv:,92991,,,,,,,,,,,10/Feb/11 17:36;zznate;checks for null before reference,10/Feb/11 18:43;gdusbabek;+1 committed.,"10/Feb/11 22:43;hudson;Integrated in Cassandra #721 (See [https://hudson.apache.org/hudson/job/Cassandra/721/])
    check for null encryption in MessagingService. patch by Nate McCall, reviewed by gdusbabek. CASSANDRA-2152
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't rely on flushkey_ special value to force flush,CASSANDRA-76,12422613,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,jbellis,jbellis,4/10/2009 22:56,3/12/2019 14:12,3/13/2019 22:24,5/5/2009 16:57,0.3,,,,0,,,,,,We can force flush programatically w/o needing this workaround.,,,,,,,,,,,,,,,,,,,,04/May/09 21:27;urandom;0001-trigger-flushes-directly-without-use-of-flushKey.txt;https://issues.apache.org/jira/secure/attachment/12407183/0001-trigger-flushes-directly-without-use-of-flushKey.txt,05/May/09 15:55;urandom;01-eliminate-special-flush-key.txt;https://issues.apache.org/jira/secure/attachment/12407256/01-eliminate-special-flush-key.txt,05/May/09 15:55;urandom;02-add-force-flush-to-mbean.txt;https://issues.apache.org/jira/secure/attachment/12407257/02-add-force-flush-to-mbean.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,27:30.0,,,no_permission,,,,,,,,,,,,19535,,,Tue May 05 16:57:30 UTC 2009,,,,,,0|i0fwq7:,90900,,,,,,,,,,,"04/May/09 21:27;urandom;I would like to wire a force flush method into the ColumnFamilyStore MBean, but before I do, I'd like to get some feedback on the attached patch.","05/May/09 15:58;urandom;01-eliminate-special-flush-key.txt (supercedes 0001-trigger-flushes-directly-without-use-of-flushKey.txt)

This changeset abstracts the logic for placing a flush on the queue into a new method, Memtable.enqueueFlush. enqueueFlush is called from both forceflush and put (when a threshold is exceeded), without the need to apply a special flushKey.

02-add-force-flush-to-mbean.txt 

This patch adds ColumnFamilyStore.forceflush to the mbean so that it can be activate through the management agent.","05/May/09 16:57;jbellis;applied, with minor modifications",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SystemTable is not persisted across reboots,CASSANDRA-362,12433009,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sammy.yu,sammy.yu,sammy.yu,8/13/2009 17:03,3/12/2019 14:12,3/13/2019 22:24,8/13/2009 22:16,0.4,,,,0,,,,,,"If you set InititalToken to """" and restart cassandra it should generate a initialtoken and store it in the SystemTable so that next time it is not regenerated.  However, this is not the case as a new inititaltoken is generated every time.
",,,,,,,,,,,,,,,,,,,,13/Aug/09 21:52;sammy.yu;0001-Based-on-jbellis-original-patch-3-and-what-I-had-lo.patch;https://issues.apache.org/jira/secure/attachment/12416499/0001-Based-on-jbellis-original-patch-3-and-what-I-had-lo.patch,13/Aug/09 21:56;sammy.yu;0002-Based-on-jbellis-original-patch-3-and-what-I-had-lo.patch;https://issues.apache.org/jira/secure/attachment/12416500/0002-Based-on-jbellis-original-patch-3-and-what-I-had-lo.patch,13/Aug/09 21:03;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-362-rename-underscores-away.txt;https://issues.apache.org/jira/secure/attachment/12416484/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-362-rename-underscores-away.txt,13/Aug/09 21:03;jbellis;ASF.LICENSE.NOT.GRANTED--0002-rename-getInitialToken-to-getToken-it-doesn-t-actuall.txt;https://issues.apache.org/jira/secure/attachment/12416485/ASF.LICENSE.NOT.GRANTED--0002-rename-getInitialToken-to-getToken-it-doesn-t-actuall.txt,13/Aug/09 21:03;jbellis;ASF.LICENSE.NOT.GRANTED--0003-fix-incompletely-configured-system-table-and-query-for.txt;https://issues.apache.org/jira/secure/attachment/12416486/ASF.LICENSE.NOT.GRANTED--0003-fix-incompletely-configured-system-table-and-query-for.txt,,,,,,,,,5,,,,,,,,,,,,,,,,,,,55:13.9,,,no_permission,,,,,,,,,,,,19652,,,Fri Aug 14 14:06:51 UTC 2009,,,,,,0|i0fyh3:,91183,,,,,,,,,,,13/Aug/09 20:55;markr;Sounds pretty serious - but easy to resolve.,"13/Aug/09 21:07;jbellis;01 and 02 are renaming and cleanup.

03 has fixes for 4 separate bugs that colluded to break this. :)

+            tableKeysCachedFractions_.put(""system"", 0.01);

this is required to flush now, so we need to generate it for the system table (keyspace).

-                    /* TODO: Remove this to not process Hints */

for some reason FB decided not to replay writes to the system tables.  the comment indicates that this was an optimization for hinted handoff, but if there isn't much hinted handoff data, then replaying isn't a Big Deal, and if there _is_ a lot then you probably don't want to throw it away.

this was biting us here since we moved the token/generation info into the system table instead of a special-cased file.

+        QueryFilter filter = new IdentityQueryFilter(LOCATION_KEY, new QueryPath(LOCATION_CF));

the old query only fetched generation.  this is the easiest way to fetch the whole CF.

+        return ColumnFamily.create(getTableName(), getColumnFamilyName());

old code was broken (passing null in several places).  not sure why; didn't investigate too closely since the new code is the more straightforward solution anyway.","13/Aug/09 21:08;jbellis;(note, if you want to restart rapidly you will still need to turn commitlogsync on for the write to actually hit the log and get replayed.)","13/Aug/09 21:09;jbellis;(note2: these patches are generated on top of 358, so if they don't apply by themselves, feel free to review that first :)","13/Aug/09 21:52;sammy.yu;Enabled system table so that it can be queried
Disabled hinted CF from being replayed (original behavior)
","13/Aug/09 21:56;sammy.yu;Removed hint hand off table check
Ignore My 0001-patch.  This patch is a replacement for jbellis original 03.patch.

This patch applies cleanly against trunk
","13/Aug/09 22:16;jbellis;committed sammy's 0002, minus the table.flush replacing commented-out code.  will open a new ticket for cleanup.","14/Aug/09 14:06;hudson;Integrated in Cassandra #167 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/167/])
    Fixes for saving Token in SystemTable.  patch by jbellis and Sammy Yu for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supercolumn deserialization bug,CASSANDRA-255,12428810,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,6/24/2009 21:18,3/12/2019 14:12,3/13/2019 22:24,6/25/2009 21:07,0.4,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,24/Jun/09 21:20;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-255-add-asserts.txt;https://issues.apache.org/jira/secure/attachment/12411709/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-255-add-asserts.txt,24/Jun/09 21:20;jbellis;ASF.LICENSE.NOT.GRANTED--0002-add-tests-for-supercolumnfamily-removal-fix-bugs.txt;https://issues.apache.org/jira/secure/attachment/12411710/ASF.LICENSE.NOT.GRANTED--0002-add-tests-for-supercolumnfamily-removal-fix-bugs.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,00:48.1,,,no_permission,,,,,,,,,,,,19609,,,Fri Jun 26 13:45:36 UTC 2009,,,,,,0|i0fxtj:,91077,,,,,,,,,,,"24/Jun/09 21:19;jbellis;    (both the assert removal and the ""if"" removal in 02 are bug fixes.)
",25/Jun/09 21:00;sandeep_tata;+1,25/Jun/09 21:07;jbellis;committed,"26/Jun/09 13:45;hudson;Integrated in Cassandra #120 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/120/])
    add tests for supercolumnfamily removal; fix bugs.
(both the assert removal and the ""if"" removal are bug fixes.)

patch by jbellis; reviewed by Sandeep Tata for 
add asserts.  patch by jbellis; reviewed by Sandeep Tata for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RemoveToken waits for dead nodes,CASSANDRA-1605,12477046,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,nickmbailey,nickmbailey,nickmbailey,10/11/2010 17:59,3/12/2019 14:12,3/13/2019 22:24,10/12/2010 15:10,0.7 beta 3,,Legacy/Tools,,0,,,,,,RemoveToken will wait for replication confirmation from nodes that are down.   It should only wait for live nodes.,,,,,,,,,,,,,,,,,,,,11/Oct/10 19:46;nickmbailey;0001-Use-failure-detector-when-detecting-new-nodes.patch;https://issues.apache.org/jira/secure/attachment/12456885/0001-Use-failure-detector-when-detecting-new-nodes.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,10:33.2,,,no_permission,,,,,,,,,,,,20213,,,Thu Oct 14 12:49:49 UTC 2010,,,,,,0|i0g69b:,92444,,,,,,,,,,,11/Oct/10 19:46;nickmbailey;Updated to use the failure detector when determining who to wait for.,12/Oct/10 15:10;jbellis;committed w/ inline of getNewEndpoints since neither the name nor the javadoc was an accurate description of what it did anymore,"14/Oct/10 12:49;hudson;Integrated in Cassandra #565 (See [https://hudson.apache.org/hudson/job/Cassandra/565/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"fix ""ant codecoverage""",CASSANDRA-2243,12499624,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stephenc,jbellis,jbellis,2/24/2011 19:05,3/12/2019 14:12,3/13/2019 22:24,2/27/2011 2:42,0.7.3,,,,0,build,,,,,,,,,,,,,,,,,,,,,,,,,27/Feb/11 01:41;stephenc;CASSANDRA-2243.patch;https://issues.apache.org/jira/secure/attachment/12472082/CASSANDRA-2243.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,43:28.5,,,no_permission,,,,,,,,,,,,20522,,,Sun Feb 27 03:06:26 UTC 2011,,,,,,0|i0ga7j:,93084,jbellis,jbellis,,,,,,,,,27/Feb/11 01:43;stephenc;attached patch fixes cobertura code coverage and leverages the maven local repo to avoid having to specify cobertura jar location via a system property and also avoid shipping the GPL artifact (i.e. cobertura.jar) in the Apache distribution,27/Feb/11 02:10;stephenc;hmmm the whole way cobertura instrumentation had been implemented is a little messy and with this fix I have concerns about GPL leaking as cobertura is always on the classpath even for non-instrumented tests. I think the testmacro needs to be tweaked,"27/Feb/11 02:23;stephenc;OK, here is my concern.

Prior to this fix, the codecoverage target would only work if you ran something like

ant codecoverage -Dcobertura.dir=/path/to/cobertura.jar

if you just ran 

ant codecoverage

then it would fail.

The reasoning behind the above is to ensure that cobertura.jar is never bundled in the -bin.tar.gz or -src.tar.gz that we ship (due to cobertura.jar having GPL code)

With this patch, we never have the risk of bundling cobertura.jar as it remains safely in ~/.m2/repository/net/sourceforge/cobertura/... and never comes near the build directory...

However there is a side-effect, namely now the ant target ""test"" will always run with cobertura.jar on the classpath.

This should be minimal risk, as you are not bundling the test classpath, but it is up to the Cassandra PMC to decide on that risk as the Cassandra PMC has to approve the releases","27/Feb/11 02:42;jbellis;committed, thanks!","27/Feb/11 03:06;hudson;Integrated in Cassandra-0.7 #325 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/325/])
    fix ""ant codecoverage""
patch by Stephen Connolly; tested by jbellis for CASSANDRA-2243
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig loadfunc fails with java.io.FileNotFoundException: ...:.../job.jar!/storage-conf.xml,CASSANDRA-1590,12476720,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,tv42,tv42,10/6/2010 23:05,3/12/2019 14:12,3/13/2019 22:24,11/12/2010 21:51,0.6.9,,,,0,,,,,,"Trying to run the example job from contrib/pig (after fixing it to start at all in the first place; details later) results in this:


2010-10-06 15:43:32,117 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - (Name: Store(hdfs://localhost/tmp/temp-1257182404/tmp1075428643:org.apache.pig.builtin.BinStorage) - 1-60 Operator Key: 1-60)
2010-10-06 15:43:32,164 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.CombinerOptimizer - Choosing to move algebraic foreach to combiner
2010-10-06 15:43:32,224 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 3
2010-10-06 15:43:32,224 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 3
2010-10-06 15:43:32,302 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3
2010-10-06 15:43:40,356 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job
2010-10-06 15:43:40,450 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.
2010-10-06 15:43:40,457 [Thread-12] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
2010-10-06 15:43:40,950 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2010-10-06 15:43:41,038 [Thread-12] INFO  org.apache.cassandra.config.DatabaseDescriptor - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
2010-10-06 15:43:41,211 [Thread-12] WARN  org.apache.cassandra.config.DatabaseDescriptor - KeysCachedFraction is deprecated: use KeysCached instead.
2010-10-06 15:43:41,232 [Thread-12] WARN  org.apache.cassandra.config.DatabaseDescriptor - KeysCachedFraction is deprecated: use KeysCached instead.
2010-10-06 15:43:42,305 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_201010061447_0008
2010-10-06 15:43:42,305 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - More information at: http://localhost:50030/jobdetails.jsp?jobid=job_201010061447_0008
2010-10-06 15:44:15,025 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 33% complete
2010-10-06 15:44:17,037 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2010-10-06 15:44:17,037 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map reduce job(s) failed!
2010-10-06 15:44:17,067 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Failed!
2010-10-06 15:44:17,199 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2997: Unable to recreate exception from backed error: Error: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)
Details at logfile: /home/tv/casspig/cassandra/contrib/pig/pig_1286405010154.log

Contents of that pig_*.log:


Backend error message
---------------------
Error: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:542)
	at org.apache.cassandra.hadoop.ConfigHelper.getThriftPort(ConfigHelper.java:188)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.<init>(ColumnFamilyRecordReader.java:118)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$RowIterator.<init>(ColumnFamilyRecordReader.java:104)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:93)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.initialize(PigRecordReader.java:133)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
Caused by: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:106)
	at java.io.FileInputStream.<init>(FileInputStream.java:66)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:70)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:161)
	at com.sun.org.apache.xerces.internal.impl.XMLEntityManager.setupCurrentEntity(XMLEntityManager.java:653)
	at com.sun.org.apache.xerces.internal.impl.XMLVersionDetector.determineDocVersion(XMLVersionDetector.java:186)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:772)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:119)
	at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:235)
	at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:284)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:208)
	at org.apache.cassandra.utils.XMLUtils.<init>(XMLUtils.java:43)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:167)
	... 9 more

Pig Stack Trace
---------------
ERROR 2997: Unable to recreate exception from backed error: Error: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias topnames
	at org.apache.pig.PigServer.openIterator(PigServer.java:607)
	at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:544)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:241)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:162)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:138)
	at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:75)
	at org.apache.pig.Main.main(Main.java:380)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2997: Unable to recreate exception from backed error: Error: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/attempt_201010061447_0008_m_000000_0/work/file:/var/lib/hadoop-0.20/cache/hadoop/mapred/local/taskTracker/jobcache/job_201010061447_0008/jars/job.jar!/storage-conf.xml (No such file or directory)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getErrorMessages(Launcher.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getStats(Launcher.java:175)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:270)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:308)
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1007)
	at org.apache.pig.PigServer.store(PigServer.java:697)
	at org.apache.pig.PigServer.openIterator(PigServer.java:590)
	... 6 more
================================================================================


I'm attaching a tarball with everything needed to reproduce this, see the run script there.",Ubuntu 10.04 with Hadoop from Cloudera CDH3b2,,,,,,,,,,,,,,,,,,,06/Oct/10 23:06;tv42;casspig.tgz;https://issues.apache.org/jira/secure/attachment/12456553/casspig.tgz,11/Nov/10 18:38;tv42;p1590.diff;https://issues.apache.org/jira/secure/attachment/12459368/p1590.diff,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,55:31.0,,,no_permission,,,,,,,,,,,,20210,,,Tue Nov 16 01:01:09 UTC 2010,,,,,,0|i0g65r:,92428,,,,,,,,,,,"06/Oct/10 23:06;tv42;A script with some auxiliary files to reproduce said problem from scratch. Assumes a working Hadoop installation, pseudo-distributed mode is fine.","13/Oct/10 22:55;jeromatron;It works if you manually build pig and put your storage-conf.xml in the jar, but there is something wrong with how it's trying to find the storage-conf.xml.  It shouldn't matter with 0.7 since it's only using hadoop vars or env vars (CASSANDRA-1322).  However, it needs to be addressed in 0.6.x so that it's more usable.","11/Nov/10 18:38;tv42;This seems to be the right fix; URIs/URLs are loaded from jars, just filenames are not.","12/Nov/10 21:51;brandon.williams;Committed, thanks!",14/Nov/10 12:38;jbellis;Did this make it into 0.6.8?,16/Nov/10 01:01;brandon.williams;Not quite.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
loadbalance operation never completes on a 3 node cluster,CASSANDRA-1221,12467688,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,6/23/2010 12:39,3/12/2019 14:12,3/13/2019 22:24,7/20/2010 17:12,0.6.4,,,,1,,,,,,"Arya Goudarzi reports:

Please confirm if this is an issue and should be reported or I am doing something wrong. I could not find anything relevant on JIRA:

Playing with 0.7 nightly (today's build), I setup a 3 node cluster this way:

 - Added one node;
 - Loaded default schema with RF 1 from YAML using JMX;
 - Loaded 2M keys using py_stress;
 - Bootstrapped a second node;
 - Cleaned up the first node;
 - Bootstrapped a third node;
 - Cleaned up the second node;

I got the following ring:

Address       Status     Load          Range                                      Ring
                                      154293670372423273273390365393543806425
10.50.26.132  Up         518.63 MB     69164917636305877859094619660693892452     |<--|
10.50.26.134  Up         234.8 MB      111685517405103688771527967027648896391    |   |
10.50.26.133  Up         235.26 MB     154293670372423273273390365393543806425    |-->|

Now I ran:

nodetool --host 10.50.26.132 loadbalance

It's been going for a while. I checked the streams

nodetool --host 10.50.26.134 streams
Mode: Normal
Not sending any streams.
Streaming from: /10.50.26.132
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-3-Data.db/[(0,22206096), (22206096,27271682)]
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-4-Data.db/[(0,15180462), (15180462,18656982)]
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-5-Data.db/[(0,353139829), (353139829,433883659)]
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-6-Data.db/[(0,366336059), (366336059,450095320)]

nodetool --host 10.50.26.132 streams
Mode: Leaving: streaming data to other nodes
Streaming to: /10.50.26.134
  /var/lib/cassandra/data/Keyspace1/Standard1-d-48-Data.db/[(0,366336059), (366336059,450095320)]
Not receiving any streams.

These have been going for the past 2 hours.

I see in the logs of the node with 134 IP address and I saw this:

INFO [GOSSIP_STAGE:1] 2010-06-22 16:30:54,679 StorageService.java (line 603) Will not change my token ownership to /10.50.26.132

So, to my understanding from wikis loadbalance supposed to decommission and re-bootstrap again by sending its tokens to other nodes and then bootstrap again. It's been stuck in streaming for the past 2 hours and the size of ring has not changed. The log in the first node says it has started streaming for the past hours:

INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,255 StreamOut.java (line 72) Beginning transfer process to /10.50.26.134 for ranges (154293670372423273273390365393543806425,69164917636305877859094619660693892452]
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,255 StreamOut.java (line 82) Flushing memtables for Keyspace1...
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,266 StreamOut.java (line 128) Stream context metadata [/var/lib/cassandra/data/Keyspace1/Standard1-d-48-Data.db/[(0,366336059), (366336059,450095320)]] 1 sstables.
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,267 StreamOut.java (line 135) Sending a stream initiate message to /10.50.26.134 ...
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,267 StreamOut.java (line 140) Waiting for transfer to /10.50.26.134 to complete
 INFO [FLUSH-TIMER] 2010-06-22 17:36:53,370 ColumnFamilyStore.java (line 359) LocationInfo has reached its threshold; switching in a fresh Memtable at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1277249454413.log', position=720)
 INFO [FLUSH-TIMER] 2010-06-22 17:36:53,370 ColumnFamilyStore.java (line 622) Enqueuing flush of Memtable(LocationInfo)@1637794189
 INFO [FLUSH-WRITER-POOL:1] 2010-06-22 17:36:53,370 Memtable.java (line 149) Writing Memtable(LocationInfo)@1637794189
 INFO [FLUSH-WRITER-POOL:1] 2010-06-22 17:36:53,528 Memtable.java (line 163) Completed flushing /var/lib/cassandra/data/system/LocationInfo-d-9-Data.db
 INFO [MEMTABLE-POST-FLUSHER:1] 2010-06-22 17:36:53,529 ColumnFamilyStore.java (line 374) Discarding 1000


Nothing more after this line.

Am I doing something wrong?",,,,,,,,,,,,,,,,,,,,20/Jul/10 15:25;gdusbabek;0.6-conviction-fix.diff;https://issues.apache.org/jira/secure/attachment/12449928/0.6-conviction-fix.diff,20/Jul/10 11:39;gdusbabek;0001-Gossiper-and-FD-never-called-MS.convict-to-shut-down.patch;https://issues.apache.org/jira/secure/attachment/12449920/0001-Gossiper-and-FD-never-called-MS.convict-to-shut-down.patch,15/Jul/10 00:24;arya;system1.log;https://issues.apache.org/jira/secure/attachment/12449507/system1.log,15/Jul/10 00:24;arya;system2.log;https://issues.apache.org/jira/secure/attachment/12449508/system2.log,15/Jul/10 00:24;arya;system3.log;https://issues.apache.org/jira/secure/attachment/12449509/system3.log,,,,,,,,,5,,,,,,,,,,,,,,,,,,,05:00.8,,,no_permission,,,,,,,,,,,,20038,,,Wed Jul 21 12:50:25 UTC 2010,,,,,,0|i0g3qn:,92036,,,,,,,,,,,"14/Jul/10 00:05;arya;Hi Gary,

I was able to reproduce this using today's nightly build. This time i used a smaller data set (500000 keys) and I got the following:

[agoudarzi@cas-test3 scripts]$ nodetool --host 10.50.26.132 ring   
Address         Status State   Load            Token                                       
                                       160348796167900510561059505917619274541    
10.50.26.134    Up     Normal  116.98 MB       32717880524093094169411234083126184860      
10.50.26.132    Up     Leaving 58.58 MB        75101027859180840627831025901565139619      
10.50.26.133    Up     Normal  117.09 MB       160348796167900510561059505917619274541    

[agoudarzi@cas-test3 scripts]$ nodetool --host 10.50.26.132 streams
Mode: Leaving: streaming data to other nodes
Streaming to: /10.50.26.133
   /var/lib/cassandra/data/Keyspace1/Standard1-d-17-Data.db/[(0,54080834)]
Not receiving any streams.
[agoudarzi@cas-test3 scripts]$ nodetool --host 10.50.26.133 streams
Mode: Normal
Not sending any streams.
Not receiving any streams.

From the logs of 10.50.26.132 it seams that it tried to tell 10.50.26.133 to claim its stream:

INFO [STREAM-STAGE:1] 2010-07-13 16:50:35,994 StreamOut.java (line 135) Sending a stream initiate message to /10.50.26.133 ...
INFO [STREAM-STAGE:1] 2010-07-13 16:50:35,994 StreamOut.java (line 140) Waiting for transfer to /10.50.26.133 to complete

But nothing in 133's log acknowledges the receipt of the request from 132 and as you see above it shows that it is getting no streams and this has been going for the past hour or so.

-Arya

","14/Jul/10 21:50;gdusbabek;Arya,  can you supply the nodetool commands you are using that constitute ""cleanup""?  I've tried a few times now and can't get the failure you describe.  In your latest test was .132 the second or third node booted?","14/Jul/10 22:55;arya;132 is node1
133 is node2
134 is node3

Give me some time and I'll regenerate all the commands for you in details. ","15/Jul/10 00:20;arya;Gary, I missed one thing (Step 9-13) where I took a node down and insert. I tested this without step 9-13 and loadbalance worked. Not sure why step 9-13 changes everything. Here are the full production steps (I am also attaching the logs from all 3 nodes if it helps):

This is the ring topology I discuss here:

Node1 10.50.26.132 (Hostname: cas-test1)
Node2 10.50.26.133 (Hostname: cas-test2)
Node3 10.50.26.134 (Hostname: cas-test3)

This run is using today's nightly built from a clean setup.

Step 1: Startup Node1

[agoudarzi@cas-test1 ~]$ sudo /etc/init.d/cassandra start

Step 2: LoadSchemadFromYAML

I go to JConsole and call the function from o.a.c.service StorageService MBeans

Step 3: I insert 500000 keys into Standard1 CF using py_stress

$ python stress.py --num-keys 500000 --threads 8 --nodes 10.50.26.132 --keep-going --operation insert
Keyspace already exists.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
62455,6245,6245,0.00128478354559,10
121893,5943,5943,0.00134767375398,21
184298,6240,6240,0.00128336335573,31
248124,6382,6382,0.00124898537112,42
297205,4908,4908,0.00163303957852,52
340338,4313,4313,0.00189026848124,63
380233,3989,3989,0.00203818801591,73
444452,6421,6421,0.00124198496903,84
500000,5554,5554,0.00114441244599,93

Step 3: Let's Take a Look at Ring

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
10.50.26.132    Up     Normal  206.05 MB       139380634429053457983268837561452509806     

Step 4: Bootstrap Node 2 into cluster

[agoudarzi@cas-test2 ~]$ sudo /etc/init.d/cassandra start

Step 5: Check the ring
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Joining 5.84 KB         54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  206.05 MB       139380634429053457983268837561452509806     

Step 6: Check the streams on Node 1

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 streams
Mode: Normal
Streaming to: /10.50.26.133
   /var/lib/cassandra/data/Keyspace1/Standard1-d-5-Data.db/[(0,34658183), (89260032,109057810)]
   /var/lib/cassandra/data/Keyspace1/Standard1-d-6-Data.db/[(0,8746823), (22272929,27264363)]
   /var/lib/cassandra/data/Keyspace1/Standard1-d-8-Data.db/[(0,8749389), (22336617,27264253)]
   /var/lib/cassandra/data/Keyspace1/Standard1-d-9-Data.db/[(0,8235190), (21174782,25822054)]
   /var/lib/cassandra/data/Keyspace1/Standard1-d-7-Data.db/[(0,8642472), (22333239,27264347)]
Not receiving any streams.

Step 7: Check the ring from Node1 and Node2 and make sure they agree

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  233.93 MB       139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  233.93 MB       139380634429053457983268837561452509806

Step 8: Cleanup Node1

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 cleanup

Step 9: Check the ring agreement again

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  117 MB          139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  117 MB          139380634429053457983268837561452509806     

Step 9: Let's kill Node 2

[agoudarzi@cas-test2 ~]$ sudo /etc/init.d/cassandra stop 

Step 10: Check the ring on Node 1

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Down   Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  117 MB          139380634429053457983268837561452509806

Step 11: Let's try to insert 500000 more keys expecting lots of unavailable exceptions ad the only replica for some keys is dead and py_stress does not use CLevel.ANY or ZERO

$ python stress.py --num-keys 500000 --threads 8 --nodes 10.50.26.132 --keep-going --operation insert

Keyspace already exists.
UnavailableException()
UnavailableException()
UnavailableException()
....
...
..
.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
500000,2922,2922,0.000816067281446,67

Step 12: Check the ring

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Down   Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  205.35 MB       139380634429053457983268837561452509806     

Node 1 got more data as expected

Step 13: Bring up Node 2 again

[agoudarzi@cas-test2 ~]$ sudo /etc/init.d/cassandra start

Step 14: Check the Ring

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  205.35 MB       139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.132    Up     Normal  205.35 MB       139380634429053457983268837561452509806

Step 15: Bootstrap Node 3

[agoudarzi@cas-test3 ~]$ sudo /etc/init.d/cassandra start

Step 11: Check Ring 

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Normal  234 MB          139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Normal  234 MB          139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.134 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Normal  234 MB          139380634429053457983268837561452509806     

Step 12: Cleanup Node 1 (132)

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 cleanup

Step 13: Check Ring

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Normal  58.89 MB        139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Normal  58.89 MB        139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.134 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Normal  58.89 MB        139380634429053457983268837561452509806     

Looks as expected. Node 1 (132) has the least load. Let's loadbalance it.

Step 14: Loadbalance Node 1

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 loadbalance &
[1] 27457

Step 15: Check Ring

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Leaving 58.89 MB        139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Leaving 58.89 MB        139380634429053457983268837561452509806     
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.134 ring
Address         Status State   Load            Token                                       
                                       139380634429053457983268837561452509806    
10.50.26.133    Up     Normal  116.94 MB       54081521187303805945240848606999860232      
10.50.26.134    Up     Normal  116.68 MB       96565427321648203609592911083606603165      
10.50.26.132    Up     Leaving 58.89 MB        139380634429053457983268837561452509806     

Step 16: Check the Streams

[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.132 streams
Mode: Leaving: streaming data to other nodes
Streaming to: /10.50.26.133
   /var/lib/cassandra/data/Keyspace1/Standard1-d-17-Data.db/[(0,54278564)]
Not receiving any streams.
[agoudarzi@cas-test1 ~]$ nodetool --host=10.50.26.133 streams
Mode: Normal
Not sending any streams.
Not receiving any streams.

PROBLEM:
Notice 132 says I am streaming to 133 but 133 says ""Not receiving any streams!""  


",15/Jul/10 00:24;arya;Cassandra System Logs for node 1-3,15/Jul/10 20:30;gdusbabek;Thanks Arya. I can reproduce this.  Now just to fix it.,"20/Jul/10 11:48;gdusbabek;Several problems on this ticket.
1. MessaingService implemented IFailureDetector and was in charge of shutting down TCP connections during a partition.  However, it was never added to the listeners in FD.  This meant that MS.convict() was never getting called.
2. Under the right conditions (I still don't understand this fully), java sockets still give every indication they are connected even though the host on the other end is down.  A single write will succeed, even though no bytes are sent.  Seriously.  In our case the single write was a StreamInitiateMessage that we then wait forever to be acked the the [dead] remote host.

My solution was to use Gossiper.convict, which calls SS.onDead, to call MS.convict().  I don't think it makes sense to have MS implement IFailureDetector since we treat Gossiper as authoritative with respect to node alive-ness.

I'll spend some time checking this morning, but I suspect we have the same situation in 0.6.","20/Jul/10 15:25;gdusbabek;patch for 0.6.  I couldn't get stress.py to work in my branch, but the same problem should be present.  All tests pass with this patch.","20/Jul/10 16:17;jbellis;+1

(but fix brace placement in onDead please)","21/Jul/10 12:50;hudson;Integrated in Cassandra #496 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/496/])
    failure detection wasn't closing sockets. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1221
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use batch_mutate in stress.py,CASSANDRA-1067,12464068,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,5/9/2010 0:21,3/12/2019 14:12,3/13/2019 22:24,5/10/2010 18:53,0.7 beta 1,,,,0,,,,,,"batch_insert was deprecated in trunk, which broke stress.py.",,,,,,,,,,,,,,,,,,,,09/May/10 00:21;stuhood;0001-Use-batch_mutate.patch;https://issues.apache.org/jira/secure/attachment/12444044/0001-Use-batch_mutate.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,49:48.7,,,no_permission,,,,,,,,,,,,19982,,,Mon May 10 18:53:15 UTC 2010,,,,,,0|i0g2sn:,91883,,,,,,,,,,,09/May/10 00:21;stuhood;Modifies stress.py to use batch_mutate.,10/May/10 18:49;brandon.williams;+1,10/May/10 18:53;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping might skip needed ranges.,CASSANDRA-902,12459419,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,3/17/2010 19:40,3/12/2019 14:12,3/13/2019 22:24,3/17/2010 22:23,0.6,,,,0,,,,,,"Bootstrapper.getRangeWithSources should return a multimap with as many keys as myRangeAddresses.  But with the way the two loops are structured, they are not guaranteed to ever examine all of myRanges.  To see why, consider a scenario where the inner-loop breaks on the first element in myRanges.  myRangeAddresses will only ever have one key in it.

Solution is to swap the order of the loops.",,,,,,,,,,,,,,,,,,,,17/Mar/10 19:43;gdusbabek;bootstrap-range-addr-calculation.txt;https://issues.apache.org/jira/secure/attachment/12439060/bootstrap-range-addr-calculation.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,29:54.3,,,no_permission,,,,,,,,,,,,19911,,,Wed Mar 17 20:29:54 UTC 2010,,,,,,0|i0g1s7:,91719,,,,,,,,,,,"17/Mar/10 19:43;gdusbabek;patched against trunk.  The same code exists in 5, so is likely a problem there too.","17/Mar/10 20:17;gdusbabek;This appears to only be a problem when the number of pending ranges for a node is greater than the number of ranges currently in place (which can't happen in 0.6).  If so, this only needs to be fixed in trunk and I can roll it in to CASSANDRA-826.",17/Mar/10 20:29;jbellis;+1 0.6 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SuperColumn tombstones are sometimes not propagated from the Memtable to the caller,CASSANDRA-101,12423651,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,4/24/2009 3:06,3/12/2019 14:12,3/13/2019 22:24,4/29/2009 2:02,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,24/Apr/09 20:05;jbellis;101.patch;https://issues.apache.org/jira/secure/attachment/12406392/101.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,55:12.0,,,no_permission,,,,,,,,,,,,19551,,,Wed Apr 29 02:02:24 UTC 2009,,,,,,0|i0fwvr:,90925,,,,,,,,,,,24/Apr/09 20:05;jbellis;Create supercolumn containers for single-column fetches w/ appropriate tombstone if applicable.,"28/Apr/09 21:55;urandom;Looks good, +1",29/Apr/09 02:02;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DebuggableScheduledThreadPoolExecutor only schedules a task once,CASSANDRA-455,12436431,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,junrao,junrao,9/23/2009 16:58,3/12/2019 14:12,3/13/2019 22:24,9/28/2009 18:44,0.4,0.5,,,0,,,,,,"DebuggableScheduledThreadPoolExecutor only schedules a task exactly once, instead of periodically. This affects scheduled flushers and periodic hints delivery.
",,,,,,,,,,,,,,,,,,,,28/Sep/09 15:12;jbellis;455.patch;https://issues.apache.org/jira/secure/attachment/12420702/455.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,10:56.4,,,no_permission,,,,,,,,,,,,19696,,,Mon Sep 28 18:44:34 UTC 2009,,,,,,0|i0fz1j:,91275,,,,,,,,,,,"23/Sep/09 17:00;junrao;The problem seems to be in DebuggableScheduledThreadPoolExecutor.afterExecute(). If I remove the line
 DebuggableThreadPoolExecutor.logFutureExceptions(r);
then all tasks are scheduled periodically as expected. Not sure if this is the right fix though.
","24/Sep/09 14:54;junrao;It seems that one needs to call DebuggableThreadPoolExecutor.logFutureExceptions(r) to catch any exception while running the scheduled task. However, once get() is called on the FutureTask, the task is no longer scheduled any more. Anyone knows how do address this?","24/Sep/09 15:10;jbellis;scheduleAtFixedRate ""Creates and executes a periodic action that becomes enabled first after the given initial delay, and subsequently with the given period; that is executions will commence after initialDelay then initialDelay+period, then initialDelay + 2 * period, and so on. If any execution of the task encounters an exception, subsequent executions are suppressed...""

so get() shouldn't be causing a cancel, but if an exception is found then we need to re-schedule it manually.  (If you cast the Runnable in afterExecute to ScheduledFutureTask you can get access to the scheduling info.)

if get is causing the cancel even w/o any exceptions being involved then I guess you'll need to source dive in ScheduledThreadPoolExecutor to see what is going on.","24/Sep/09 15:20;junrao;""if get is causing the cancel even w/o any exceptions being involved then I guess you'll need to source dive in ScheduledThreadPoolExecutor to see what is going on.""

That seems to be the case. Here is what happens. 

Without exception:
When DebuggableThreadPoolExecutor.logFutureExceptions(r) is called in afterExecute(), tasks are no longer executed afterwards. When DebuggableThreadPoolExecutor.logFutureExceptions(r) is removed, tasks are scheduled as expected.

With exception:
If DebuggableThreadPoolExecutor.logFutureExceptions(r) is removed, exception is not logged.","24/Sep/09 15:28;jbellis;then i guess we should always reschedule after checking for exceptions.

but i'd be more comfortable if i saw the code that is un-scheduling it, so i could be sure there are no conditions under which we could end up w/ 2 copies of the same task scheduled.","25/Sep/09 20:25;jbellis;Okay, here's what's going on.

ScheduledFutureTask overrides FT.run as follows:

        public void run() {
            if (isPeriodic())
                runPeriodic();
            else
                ScheduledFutureTask.super.run();
        }

we are scheduling periodic tasks, so the normal run() method, which just wrapps sync.innerRun(), is not called.  Instead we call runPeriodic, of which the important part here is

            boolean ok = ScheduledFutureTask.super.runAndReset();

not run()!  runAndReset()!  which says

    /**
     * Executes the computation without setting its result, and then
     * resets this Future to initial state, failing to do so if the
     * computation encounters an exception or is cancelled.  This is
     * designed for use with tasks that intrinsically execute more
     * than once.
     * @return true if successfully run and reset
     */
    protected boolean runAndReset() {
        return sync.innerRunAndReset();
    }

key point is ""... without setting its result.""  sure enough, sync.innerRAR sets the state back to 0 when it is done -- but get() will block until state is RAN or CANCELLED, i.e. until there is a result.  So the reason get() makes the task stop executing is it deadlocks the executor thread that it's running on.

as far as i can see there is no public way to get an exception out of a FutureTask.sync w/o calling get.  I guess we can get it out using reflection tho.

For 0.4 I will just revert the commit that added logFutureExceptions to DebuggableScheduledThreadPoolExecutor.",25/Sep/09 20:37;jbellis;ScheduledFutureTask is private.  @Q#%$,25/Sep/09 20:52;jbellis;performed the revert on the 0.4 branch.,"25/Sep/09 21:25;jbellis;I took a stab at using STPE.decorateTask to make a new ScheduledFuture that doesn't eat exceptions but it's going to be messy and fragile.

Ripping that crap out and using old-school Timers looks like a better option.","28/Sep/09 15:12;jbellis;    Replace DebuggableScheduledThreadPoolExecutor with non-Scheduled Executors and Timers.  This allows logging
    exceptions from repeated tasks, which is basically impossible with STPE.
",28/Sep/09 17:48;junrao;Looks good to me. Perhaps we should add some logging for the starting of each scheduled task.,"28/Sep/09 18:42;jbellis;both flushing and HHO log as soon as they start so I think adding logging at the timer level clutters things up more than clarifies.  (maybe if we decide we have too many Timer threads around and consolidate into a global service, though.)",28/Sep/09 18:44;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
antientropyservice patchset nearly doubles compaction time,CASSANDRA-629,12443145,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,jbellis,jbellis,12/11/2009 20:55,3/12/2019 14:12,3/13/2019 22:24,12/16/2009 18:00,0.5,,,,0,,,,,,"CompactionsTest goes from 2.9s to 5.7s for me, before/after the 193 patchset.  This is not an acceptable speed hit for when no repair work is actually being done, assuming it is representative of ""real"" compactions.",,,,,,,,,,,,,,,,,,,,13/Dec/09 08:02;stuhood;629-1-utility-changes.diff;https://issues.apache.org/jira/secure/attachment/12427846/629-1-utility-changes.diff,13/Dec/09 08:02;stuhood;629-2-use-xor-in-merkletree.diff;https://issues.apache.org/jira/secure/attachment/12427847/629-2-use-xor-in-merkletree.diff,13/Dec/09 09:59;stuhood;629-3-notify-neighbors.diff;https://issues.apache.org/jira/secure/attachment/12427849/629-3-notify-neighbors.diff,14/Dec/09 22:23;stuhood;629-4-comment-on-laziness.diff;https://issues.apache.org/jira/secure/attachment/12427962/629-4-comment-on-laziness.diff,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,02:28.0,,,no_permission,,,,,,,,,,,,19788,,,Thu Dec 17 22:03:04 UTC 2009,,,,,,0|i0g03z:,91448,,,,,,,,,,,"13/Dec/09 08:02;stuhood;After benchmarking and trying to optimize the current code as much as possible, I gave up on trying to generate the full range down to hash depth.

Jun suggested using XOR on 193, both for speed and simplicity, but I was worried about hash collisions. In order to make using XOR a little more secure, I bumped the digests we generate in AEService to SHA-256, so that we minimize the chance of collision as much as possible.

Time to validate 1 million rows dropped from 84 seconds to 1.9 seconds (including hashing).","13/Dec/09 09:59;stuhood;Also, I noticed that in trunk a node won't broadcast a tree to neighbors unless they requested it, which means that when a major compaction happens naturally, the tree is wasted. Another quick patch.",14/Dec/09 19:50;jbellis;does not appreciably reduce CompactionsTest runtime for me,"14/Dec/09 21:01;jbellis;from IRC:

stuhood: 193 added another testcase to CompactionsTest 

... and sure enough, removing testCompactionReadonly makes it fast again.  Sorry for the false alarm.

I'm still +1 on reducing validation time, though. :)","14/Dec/09 22:23;junrao;The last sentence of TODO above AES.validator.add() is confusing. Other than that, the patch looks good to me.

Also, don't forget to add a comment saying that hash of inner nodes in Merkle tree are calculated on demand.","14/Dec/09 22:23;stuhood;At Jun's request, add a comment on the lazy calculation of the hashes of inner nodes.","14/Dec/09 22:38;stuhood;> The last sentence of TODO above AES.validator.add() is confusing
Ah. That should read ""we do not bother*"".",15/Dec/09 22:33;junrao;Committed to trunk. Thanks Stu.,15/Dec/09 23:02;jbellis;IMO we should backport this to 0.5 too.  Any objections?,15/Dec/09 23:45;stuhood;I think that would be wise.,16/Dec/09 18:00;jbellis;also committed to 0.5,"17/Dec/09 22:03;hudson;Integrated in Cassandra #291 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/291/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bloom filters have much higher false-positive rate than expected,CASSANDRA-68,12422480,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,4/9/2009 15:13,3/12/2019 14:12,3/13/2019 22:24,4/17/2009 20:18,0.3,,,,0,,,,,,Gory details: http://spyced.blogspot.com/2009/01/all-you-ever-wanted-to-know-about.html,,,,,,,,,,,,,,,,,,,,09/Apr/09 21:54;jbellis;0001-r-m-unused-code-including-entire-CountingBloomFilte.patch;https://issues.apache.org/jira/secure/attachment/12405113/0001-r-m-unused-code-including-entire-CountingBloomFilte.patch,09/Apr/09 21:54;jbellis;0002-replace-JenkinsHash-w-MurmurHash.-its-hash-distrib.patch;https://issues.apache.org/jira/secure/attachment/12405114/0002-replace-JenkinsHash-w-MurmurHash.-its-hash-distrib.patch,09/Apr/09 21:55;jbellis;0003-rename-BloomFilter.fill-add.patch;https://issues.apache.org/jira/secure/attachment/12405115/0003-rename-BloomFilter.fill-add.patch,09/Apr/09 21:56;jbellis;0004-rewrite-bloom-filters-to-use-murmur-hash-and-combina.patch;https://issues.apache.org/jira/secure/attachment/12405116/0004-rewrite-bloom-filters-to-use-murmur-hash-and-combina.patch,11/Apr/09 12:38;jbellis;0004-v3.patch;https://issues.apache.org/jira/secure/attachment/12405230/0004-v3.patch,10/Apr/09 02:19;jbellis;0004a-tests.patch;https://issues.apache.org/jira/secure/attachment/12405132/0004a-tests.patch,10/Apr/09 02:19;jbellis;0004b-code.patch;https://issues.apache.org/jira/secure/attachment/12405133/0004b-code.patch,11/Apr/09 12:46;jbellis;0005-switch-back-to-old-hash-generation-code-to-demonstra.patch;https://issues.apache.org/jira/secure/attachment/12405232/0005-switch-back-to-old-hash-generation-code-to-demonstra.patch,10/Apr/09 23:13;sandeep_tata;fp_test_for_old_code.patch;https://issues.apache.org/jira/secure/attachment/12405205/fp_test_for_old_code.patch,10/Apr/09 23:37;sandeep_tata;fp_test_for_old_code_v2.patch;https://issues.apache.org/jira/secure/attachment/12405208/fp_test_for_old_code_v2.patch,11/Apr/09 12:49;jbellis;words.gz;https://issues.apache.org/jira/secure/attachment/12405233/words.gz,,,11,,,,,,,,,,,,,,,,,,,51:27.9,,,no_permission,,,,,,,,,,,,19533,,,Fri Apr 17 20:18:46 UTC 2009,,,,,,0|i0fwof:,90892,,,,,,,,,,,"09/Apr/09 21:54;jbellis;1. r/m unused code, including entire CountingBloomFilter","09/Apr/09 21:54;jbellis;2. replace JenkinsHash w/ MurmurHash.  its hash distribution is just as good, and it's faster",09/Apr/09 21:55;jbellis;3. rename BloomFilter.fill -> add,09/Apr/09 21:56;jbellis;4. rewrite bloom filters to use murmur hash and combinatorics to generate better hash values.  (again: see blog post if you want details of approaches that were tried and rejected.)  add test suite.,"10/Apr/09 01:51;sandeep_tata;Modify tests to catch FileNotFound for ""/usr/share/dict/words"" and not run those tests per conversation on irc.
","10/Apr/09 02:24;jbellis;Made words test optional.

Also split 0004 changes into tests and code, but I don't think that's going to be too useful.  If you want to test the old hash functions the easiest thing is probably to modify Filter.getHashBuckets to use the old hash functions instead.

But I remember that you will see from 50% to 200% more FP than you should.  Sorry I don't have the code anymore.  (Lost in a git rebase, apparently.)","10/Apr/09 06:35;sandeep_tata;+1

This is a large patch, but the code is structured nicely so it shouldn't be too hard for others to take a look.

I have 2 suggestions for changes:

1. Can you add short high-level comments describing the design of the Filter, BloomFilter, CountingBloomFilter classes so others who want to take a look at the patch get the idea quickly ?

2. Since this is a somewhat large patch, I'd wait a couple of days to check if people have questions:
a) the new bloomfilter is any faster/slower than the old one
b) they have some tests comparing false-positive rates of this vs old one.




",10/Apr/09 07:05;sandeep_tata;The regression to watch for is if the bloomfilters sizes are significantly different. I'm guessing not : Jonathan -- do you have any estimates?,"10/Apr/09 12:35;jbellis;I commented getHashBuckets which is the core of the change.  Those who are not familiar with bloom filters in general are referred to wikipedia. :)

Normal BloomFilter size is going to be the same as the old; both are based on a BitSet which is about as efficient as you can get.

CountingBloomFilter size is going to be half the size of the old since the old uses a full byte per bucket and this uses a half byte.  (If you reach a count of 15 your filter is way too small to be useful anyway; there is no reason to allow a count of 255.)","10/Apr/09 14:10;jbellis;To answer the question from IRC:

The new hash generator is increasingly faster than the old as more hashes are needed.  But the real win is in lower false positives, meaning the user has to do that much less of the far more expensive operations that it's trying to avoid by using a bloom filter in the first place.",10/Apr/09 19:25;avinash.lakshman@gmail.com;What's the problem here again? Not sure I follow? What is the fundamental reason for any change here?,"10/Apr/09 19:36;avinash.lakshman@gmail.com;The reason I ask is the following:

(1) The Bloom Calculations table is straight out of some paper. I cannot quite recall now.
(2) THe Counting Bloom Filter is left there because I think it could still be used for certain purposes. So I wouldn't want to get rid of it too soon.
(3) We use 8 bits/element and 5 hash function to get the false positive rate as indicated by the paper in its table which is captured in the Bloom Calculations.

So how are we concluding which hash is faster based on what scientific evidence? I am not quite sure I follow the rest of the comments w.r.t. the justification of this patch.","10/Apr/09 19:46;jbellis;Avinash, 

1. BC is not an issue.  The numbers there are fine.
2. Yes, patch 4 adds back a new CBF that uses half the memory (by using a half byte per bucket instead of a full byte).
3. the problem is the hash functions themselves.  to elaborate, the old code used a hardcoded list of hash functions which were NOT independent -- for 6 or less hash functions, these functions were only actually generating 2 distinct hash values, so the false positive rate was far far higher than would have been expected.  After extensive comparsions with Jenkins, Murmur, and SHA-based approaches, it now generates hashes using Murmur as a base and then combinatorics as described here: http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/esa06.pdf

We're concluding it's faster based on the two weeks of testing I did. :)  But the real win is getting to the theoretically predicted false positive rate instead of much higher.

",10/Apr/09 21:26;sandeep_tata;What we need now is some code so we can reproduce the false-positive rate comparison :-),"10/Apr/09 21:50;jbellis;Fine.  Here is a simple test that demonstrates the problem on the old code that you could have written in about a minute:

public class BloomFilterTest
{
    @Test
    public void testHashes() {
        Set<Integer> hashResults = new HashSet<Integer>();
        for (ISimpleHash hash : BloomFilter.hashLibrary_) {
            hashResults.add(hash.hash(""1""));
        }
        assert hashResults.size() == BloomFilter.hashLibrary_.size() : hashResults.size();
    }
}

This shows that the old hash functions only generate 5 unique values (from 11 functions).  A little inspection will show that only 2 of those are in the first six, as I mentioned above.

If that doesn't tell you ""oh yeah the false positive rate will be the roof"" then you need to read the wikipedia page on Bloom filters. :)",10/Apr/09 23:13;sandeep_tata;Patch to run false positive tests on the old code -- based completely on Jonathan's unit tests that test the new bloomfilters.,"10/Apr/09 23:37;sandeep_tata;If you want to compare the FP rates for before and after, first apply fp_test_for_old_code_v2.patch. The tests in BloomFilterTest print out the # false positives.

Roll back those patches and apply Jonathan's 1-4 to use the new bloomfilter. 

(v2 of patch uses 10000 elements to be directly comparable to Jonathan's patches, v1 changed this value to 100000)","11/Apr/09 12:46;jbellis;Attached 0004-v3, which cleans out printlns and adds the FP ratio to the assertionerror.

Attached 0005, which switches to the old hash generation code on top of the new Filter + tests framework.

Several small tests always fail because of hash collisions where there should not be any.  For our purposes these are noise except to confirm that the hash generation is broken.

False positives for testWords is the big red fail.  17% more FP than expected at MAX_FAILURE_RATE. of 0.1.  Increase MFP to force more hases to be used and this goes up:  22% at 0.01, and 66% at 0.001.  It would keep going up but we're maxing out the precalculated probabilities in BloomCalculations here.  (A FP of 0.001 is _not_ all that low for a bloom filter, btw, but for what we are using it for so far it is probably adequate.)

Note also that testManyRandom fails because the old code is limited to 11 hashes.  The new code can generate as many as needed.

","11/Apr/09 12:49;jbellis;here is the /usr/share/dict//words I'm using (gzipped).  it's the one from ubuntu 8.10 wamerican package.
","11/Apr/09 23:30;sandeep_tata;I ran some more tests, here's what I found:

Old code:
Test                                             MAX_FP Actual FP
FalsePositivesInt/Random     0.1            0.142
FalsePositivesInt/Random     0.01         (pass)
FalsePositivesInt/Random     0.001       (pass)
Words                                         0.1            0.15
Words                                         0.01          (pass)
Words                                         0.001        0.0013

New code:

FalsePositivesInt/Random     0.1           (pass)
FalsePositivesInt/Random     0.01         (pass)
FalsePositivesInt/Random     0.001       (pass)
Words                                         0.1            (pass)
Words                                         0.01          (pass)
Words                                         0.001        (pass)

The old bloomfilter certainly reports up to 50% more than expected false positive rates for some cases. The new bloomfilter is more predictable, it always passes.

On my machine, some quick-n-crude tests show that the new bloom-filter is about 4x slower. (I tested at FP rate = 0.01) . When you take into account the fact that the penalty for a false positive at least 3 orders of magnitude more expensive than the actual hash calculation (an FP usually means you'll end up hitting disk unnecessarily), it makes sense to use it even when you set the FP rate to 0.001. It is even more useful at higher rates.","13/Apr/09 15:15;jbellis;Prashant wrote on the mailing list:

""The results are a bit counter intuitive here I would have expected it to be faster with the same FP rate but   I am not sure why it is slower if you are just using a couple of hash functions and using double hashing...  I am sorry I haven't looked at the test code but have you tried it with large strings as keys ? e.g 128 byte keys , also with Longs.""

I replied:

""Murmur is a higher-quality hash and takes more operations to achieve its better key distribution.  But since the new implementation always uses two calls to Murmur no matter how many hashes are needed it is virtually constant time.  The random strings generated are 128 bytes.""",13/Apr/09 15:16;jbellis;Any further questions / comments / objections before we commit?,15/Apr/09 20:23;jbellis;One other point that is probably obvious: this change is incompatible with old serialized bloomfilters since the hash values for a given key necessarily change.,"17/Apr/09 20:18;jbellis;committed.  (33 breaks binary compatibility anyway, so might as well throw this one in too.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM during major compaction on many (hundreds) of sstables,CASSANDRA-436,12435316,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,9/10/2009 14:35,3/12/2019 14:12,3/13/2019 22:24,9/17/2009 21:10,0.5,,,,0,,,,,,"compaction deserializes rows during compaction before they are needed, one per sstable.  if we only deserialized on-demand the current algorithm would be fine on nearly arbitrarily large numbers of sstables.  (this is only important b/c it is useful to disable compactions during bulk load.)",,,,,,,,,,,,,,,,,,,,16/Sep/09 14:39;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-436-minor-fixes.txt;https://issues.apache.org/jira/secure/attachment/12419775/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-436-minor-fixes.txt,14/Sep/09 17:49;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-436-minor-fixes.txt;https://issues.apache.org/jira/secure/attachment/12419547/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-436-minor-fixes.txt,16/Sep/09 14:39;jbellis;ASF.LICENSE.NOT.GRANTED--0002-copy-FileStruct-to-SSTableScanner-and-remove-cruft.-M.txt;https://issues.apache.org/jira/secure/attachment/12419776/ASF.LICENSE.NOT.GRANTED--0002-copy-FileStruct-to-SSTableScanner-and-remove-cruft.-M.txt,14/Sep/09 17:49;jbellis;ASF.LICENSE.NOT.GRANTED--0002-copy-FileStruct-to-SSTableScanner-and-remove-cruft.-M.txt;https://issues.apache.org/jira/secure/attachment/12419548/ASF.LICENSE.NOT.GRANTED--0002-copy-FileStruct-to-SSTableScanner-and-remove-cruft.-M.txt,16/Sep/09 14:39;jbellis;ASF.LICENSE.NOT.GRANTED--0003-allow-ReducingIterator-to-reduce-from-one-type-to-a-di.txt;https://issues.apache.org/jira/secure/attachment/12419777/ASF.LICENSE.NOT.GRANTED--0003-allow-ReducingIterator-to-reduce-from-one-type-to-a-di.txt,14/Sep/09 17:49;jbellis;ASF.LICENSE.NOT.GRANTED--0003-allow-ReducingIterator-to-reduce-from-one-type-to-a-di.txt;https://issues.apache.org/jira/secure/attachment/12419549/ASF.LICENSE.NOT.GRANTED--0003-allow-ReducingIterator-to-reduce-from-one-type-to-a-di.txt,16/Sep/09 14:39;jbellis;ASF.LICENSE.NOT.GRANTED--0004-Replace-PriorityQueue-mess-with-a-CompactionIterator-t.txt;https://issues.apache.org/jira/secure/attachment/12419778/ASF.LICENSE.NOT.GRANTED--0004-Replace-PriorityQueue-mess-with-a-CompactionIterator-t.txt,14/Sep/09 17:49;jbellis;ASF.LICENSE.NOT.GRANTED--0004-Replace-PriorityQueue-mess-with-a-CompactionIterator-t.txt;https://issues.apache.org/jira/secure/attachment/12419550/ASF.LICENSE.NOT.GRANTED--0004-Replace-PriorityQueue-mess-with-a-CompactionIterator-t.txt,,,,,,8,,,,,,,,,,,,,,,,,,,33:42.7,,,no_permission,,,,,,,,,,,,19689,,,Fri Sep 18 13:03:42 UTC 2009,,,,,,0|i0fyx3:,91255,,,,,,,,,,,"14/Sep/09 17:50;jbellis;04
    Replace PriorityQueue mess with a CompactionIterator that efficiently yields compacted Rows from a set of
    sstables by feeding CollationIterator into a ReducingIterator transform.  (""Efficiently"" means we        
    never deserialize data until it is needed, so the number of sstables that can be compacted at once is    
    virtually unlimited, and if only one sstable contains a given key that row data will be copied over      
    without an intermediate de/serialize step.) This is a very natural fit                                   
    for the compaction algorithm and almost entirely gets rid of duplicated code between doFileCompaction and
    doAntiCompaction.

03
    allow ReducingIterator to reduce from one type to a different one

02
    copy FileStruct to SSTableScanner and remove cruft.  Migrate getKeyRange to new scanner class.

01
    minor cleanup
",16/Sep/09 03:33;lenn0x;I will be testing this tonight on our cluster. I'll need roughly 15-20 hours but should have some results tomorrow.,"16/Sep/09 07:40;lenn0x;This needs to be rebased, I can't apply the last patch",16/Sep/09 14:51;jbellis;rebased,"16/Sep/09 17:12;lenn0x;Tested on our cluster. Much better improvement! Before we were seeing 2-7GB of heap usage now its under 700MB.

+1",17/Sep/09 21:10;jbellis;committed,"18/Sep/09 13:03;hudson;Integrated in Cassandra #201 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/201/])
    Replace PriorityQueue mess with a CompactionIterator that efficiently yields compacted Rows from a set of sstables by feeding CollationIterator into a ReducingIterator transform.  (""Efficiently"" means we never deserialize data until it is needed, so the number of sstables that can be compacted at once is  virtually unlimited, and if only one sstable contains a given key that row data will be copied over without an intermediate de/serialize step.) This is a very natural fit for the compaction algorithm and almost entirely gets rid of duplicated code between doFileCompaction and doAntiCompaction.
patch by jbellis; reviewed by goffinet for 
allow ReducingIterator to reduce from one type to a different one
patch by jbellis; reviewed by goffinet for 
copy FileStruct to SSTableScanner and remove cruft.  Migrate getKeyRange to new scanner class.
patch by jbellis; reviewed by goffinet for 
minor fixes
patch by jbellis; reviewed by goffinet for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read repair on CL.ONE regression,CASSANDRA-1985,12495562,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,kelvin,kelvin,1/14/2011 0:32,3/12/2019 14:12,3/13/2019 22:24,1/15/2011 19:37,0.7.1,,,,0,,,,,,"read repair w/ CL.ONE had a regression.

The RepairCallback was dropped (in the background for CL.ONE), so ReadResponseResolver : resolve() was never called.",,,7200,7200,,0%,7200,7200,,,,,,,,CASSANDRA-982,,,,14/Jan/11 20:33;jbellis;1985-v2.txt;https://issues.apache.org/jira/secure/attachment/12468403/1985-v2.txt,14/Jan/11 00:45;kelvin;CASSANDRA-1985-0001-fix-CL.ONE-read-repair-regression.patch;https://issues.apache.org/jira/secure/attachment/12468326/CASSANDRA-1985-0001-fix-CL.ONE-read-repair-regression.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,10:22.0,,,no_permission,,,,,,,,,,,,19352,,,Sat Jan 15 19:53:24 UTC 2011,,,,,,0|i0g8lz:,92825,tjake,tjake,,,,,,,,,"14/Jan/11 00:33;kelvin;This regression occurred, here.",14/Jan/11 00:45;kelvin;ensure RR happens in the background.,"14/Jan/11 04:10;jbellis;It's possible that we missed something, but we we did test read repair post-982.  This is the part that does the resolve:

                if (repairs.contains(command))
                    repairExecutor.schedule(new RepairRunner(readCallback.resolver, command, endpoints), DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
","14/Jan/11 18:00;kelvin;Yes, you're right, that does schedule it, once.

The process for CL.ONE is:
1) schedule RR for data+digest and watch for a DigestMismatchException,
2) catch DME and call repair() to do a RR for data-only.

However, the handler for the second RR (that repair() returns) is never used.  So, even though it's collecting all the data repair messages, the RRR's resolve() never gets called.
","14/Jan/11 18:50;jbellis;The ""second RR"" (that is, the second read request, for performing repair when a mismatch was detected by the digest read) is this one:

{code}
                RepairCallback<Row> handler = repair(command, endpoints);
...
                repairResponseHandlers.add(handler);
...
            for (RepairCallback<Row> handler : repairResponseHandlers)
            {
                try
                {
                    Row row = handler.get();
{code}","14/Jan/11 19:44;kelvin;Yes, that's correct for read CL > ONE.  A quorum / all read goes through that path.  However, the CL.ONE case does not go through that code path.

The branch in the code is in fetchRows(...) when it checks for randomlyReadRepair(...).  If the targets > handler.blockfor, it does a background repair via RepairRunner in service.StorageProxy.  i.e. it won't go through the block of code you pasted, because a DigestMismatchException won't be thrown for CL.ONE.

Now, let's look at RepairRunner : runMayThrow.  It calls repair(command, endpoints), but the RepairCallback<row> that is returned by repair(...) is dropped on the floor.  So, resolve is never called on that RepairCallback's ReadResponseResolver.

The above error was found via my own set of distributed tests.",14/Jan/11 19:48;jbellis;I get it now: the callback from the repair() call in RepairRunner is the one that we don't resolve.,"14/Jan/11 20:33;jbellis;v2 keeps the resolve off the response stage, which we want to keep very low latency.",14/Jan/11 22:34;tjake;+1,15/Jan/11 19:37;jbellis;committed,"15/Jan/11 19:53;hudson;Integrated in Cassandra-0.7 #162 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/162/])
    fix read repair on CL.ONE regression
patch by jbellis; reviewed by tjake for CASSANDRA-1985
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get rid of internode directory,CASSANDRA-1357,12470811,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,8/4/2010 15:54,3/12/2019 14:12,3/13/2019 22:24,8/5/2010 17:47,0.7 beta 1,,,,0,,,,,,code should generate to src/gen-java.  the genavro file should be renamed and located in interface.,,,,,,,,,,,,,,,,,,,,05/Aug/10 14:08;gdusbabek;0001-remove-internode-directory-and-change-destination-fo.patch;https://issues.apache.org/jira/secure/attachment/12451332/0001-remove-internode-directory-and-change-destination-fo.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,15:46.4,,,no_permission,,,,,,,,,,,,20102,,,Sun Aug 08 13:52:17 UTC 2010,,,,,,0|i0g4k7:,92169,,,,,,,,,,,"04/Aug/10 16:15;jbellis;src/avro would also make sense for the genavro file, imo",04/Aug/10 16:52;gdusbabek;I didn't include the paranamer task on the avro generated to src/gen-java.  It's not clear that it is required.,"04/Aug/10 19:05;stuhood;> src/avro would also make sense for the genavro file, imo
Agreed: putting the private protocol definition in src/avro, leaving public protocols in interface/ and generating code for all protocols into src/gen-java would make sense.",05/Aug/10 14:07;gdusbabek;Rebased because of CASSANDRA-1356.,05/Aug/10 16:38;stuhood;+1 Thanks Gary!,"08/Aug/10 13:52;hudson;Integrated in Cassandra #510 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/510/])
    remove internode directory and change destination for generated avro code. patch by gdusbabek, reviewed by stuhood. CASSANDRA-1357
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
heisenbug in RoundRobinSchedulerTest,CASSANDRA-1279,12469264,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,rnirmal,jbellis,jbellis,7/14/2010 15:58,3/12/2019 14:12,3/13/2019 22:24,8/3/2010 17:27,0.7 beta 1,,,,0,,,,,,"Occasionally I see this error in the test suite:

    [junit] Testcase: testScheduling(org.apache.cassandra.scheduler.RoundRobinSchedulerTest):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.cassandra.scheduler.RoundRobinSchedulerTest.testScheduling(RoundRobinSchedulerTest.java:90)
    [junit] 
",,,,,,,,,,,,,,,,,,,,03/Aug/10 17:08;rnirmal;1279-v3.patch;https://issues.apache.org/jira/secure/attachment/12451135/1279-v3.patch,20/Jul/10 21:51;rnirmal;Cassandra-1279-v2.patch;https://issues.apache.org/jira/secure/attachment/12449982/Cassandra-1279-v2.patch,20/Jul/10 17:02;rnirmal;Cassandra-1279.patch;https://issues.apache.org/jira/secure/attachment/12449945/Cassandra-1279.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,02:27.6,,,no_permission,,,,,,,,,,,,20056,,,Wed Aug 04 13:25:29 UTC 2010,,,,,,0|i0g43b:,92093,,,,,,,,,,,"20/Jul/10 17:02;rnirmal;Fixed the problem, it's due to the unpredictable thread scheduling. The fixe loads the requests and then lets the scheduler proceed, that way the tests could be somewhat predictable and realize the expected behavior.","20/Jul/10 20:36;jbellis;can you add a comment as to what is going on?

also, adding a package-private method for the test to use would be cleaner than poking through reflection","20/Jul/10 21:51;rnirmal;The test is done with 15 simulated connections, 10 for K1(keyspace), 2 for K2 and 3 for k3. Requests came in the order of K1(1 thru 10), K2(11 thru 12), K3(13 thru15) and the test checked if K2 and K3 requests ran earlier then their request order. With the scheduler starting simultaneously, the requests were pretty much routed in order +/- the jvm thread scheduling order, hence the cause for the bug. 

Now the scheduler is paused still all the requests arrive and placed in their respective queues. When the scheduler is resumed, each pass retrieves one request from each keyspace queue, hence since K2 & K3 have only 2 and 3 requests, they get serviced faster than the order in which they arrived. This test just validates that the requests are RoundRobin and that's what we want to unit test.",20/Jul/10 22:03;jbellis;committed,"21/Jul/10 12:50;hudson;Integrated in Cassandra #496 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/496/])
    fix RoundRobinSchedulerTest heisenbug.  patch by Nirmal Ranganathan; reviewed by jbellis for CASSANDRA-1279
","03/Aug/10 15:14;jbellis;looks like this is only mostly fixed.  still getting

    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.cassandra.scheduler.RoundRobinSchedulerTest.testScheduling(RoundRobinSchedulerTest.java:93)
    ",03/Aug/10 15:24;rnirmal;Hmm.... looks like I'll have to change the way we test it. ,"03/Aug/10 17:08;rnirmal;The scheduler will only have 1 token at anytime and the run/release of each thread is synchronized, effectively running only one thread at a time. So hopefully no threading inconsistencies occur.","03/Aug/10 17:10;jbellis;Is this the point at which we say that ""this is different enough from 'live' code that it's not really a useful test anymore?""  Because I'm okay with that.","03/Aug/10 17:23;rnirmal;Yes I'd say that, because it doesn't reflect the actual concurrency that will take place. So if it's ok we could remove it.",03/Aug/10 17:27;jbellis;removed,"04/Aug/10 13:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    r/m RoundRobinSchedulerTest for CASSANDRA-1279; it doesn't appear possible to test concurrency usefully
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception starting up cluster with ByteOrderedPartitioner without schema set,CASSANDRA-1006,12462502,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,erickt,erickt,4/19/2010 22:35,3/12/2019 14:12,3/13/2019 22:24,4/20/2010 22:04,0.7 beta 1,,,,0,,,,,,"Testing out the new ByteOrderedPartitioner, I ran into this exception on the tip:

java.lang.NumberFormatException: For input string: ""To""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at org.apache.cassandra.utils.FBUtilities.hexToBytes(FBUtilities.java:361)
	at org.apache.cassandra.dht.AbstractByteOrderedPartitioner$1.fromString(AbstractByteOrderedPartitioner.java:133)
	at org.apache.cassandra.dht.BootStrapper$BootstrapTokenCallback.response(BootStrapper.java:246)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:36)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR 16:31:21,737 Fatal exception in thread Thread[RESPONSE-STAGE:1,5,main]

It works fine with the RandomPartitioner, however.",,,,,,,,,,,,,,,,,,,,20/Apr/10 02:35;stuhood;0001-Serialize-Tokens-using-TokenFactory.patch;https://issues.apache.org/jira/secure/attachment/12442256/0001-Serialize-Tokens-using-TokenFactory.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,35:45.9,,,no_permission,,,,,,,,,,,,19951,,,Tue Apr 20 22:04:29 UTC 2010,,,,,,0|i0g2fb:,91823,,,,,,,,,,,"19/Apr/10 22:48;erickt;I hacked up my code a bit, and it turns out the line ByteOrderedPartitioner is trying to parse is the string ""Token(bytes[4695b973940d36dce35be8d73832f848])"", which obviously is not a hex string.","20/Apr/10 00:00;erickt;Fyi, turns out this is happening whether or not I've set up the storage configuration.",20/Apr/10 02:35;stuhood;BootStrapper was serializing tokens using toString rather than TokenFactory.toString.,20/Apr/10 22:03;gdusbabek;+1,20/Apr/10 22:04;gdusbabek;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.net.SocketException: Invalid argument / java.net.NoRouteToHostException: Network is unreachable,CASSANDRA-628,12443144,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,12/11/2009 20:55,3/12/2019 14:12,3/13/2019 22:24,10/29/2010 21:40,0.7.0 rc 1,,,,1,,,,,,"This manifests as either a SocketException that occurs when starting a cassandra node, or a NoRouteToHostException which occurs when connecting with a client.

On Linux systems this is caused by IPV6_V6ONLY being set true. The docs (ipv6(7)) say that when set this causes sockets to be created IPv6 only, while the previous behavior also allowed sending and receiving packets using an  IPv4-mapped IPv6 address.

The quick fix is to either launch applications using the -Djava.net.preferIPv4Stack=true property, or on Linux systems set net.ipv6.bindv6only=0 (see sysctl(8)).

My limited understanding is that the previous behavior (IPV6_V6ONLY=0) was always considered a hack to be used until IPv6 was more mature/had gained traction and that a change in defaults was always inevitable, so in the long-term a Real Fix will be needed.

http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6342561
http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=560056
","Linux, FreeBSD, (possibly others)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,08:36.8,,,no_permission,,,,,,,,,,,,19787,,,Sat Oct 30 12:48:33 UTC 2010,,,,,,0|i0g03r:,91447,,,,,,,,,,,"22/May/10 17:08;jbellis;Should we apply the ""quick fix"" for now?","22/May/10 17:08;jbellis;that is, -Djava.net.preferIPv4Stack=true","23/May/10 02:58;urandom;We've at least one user who uses IPv6, and as far as I know, I'm the only one that has run into this particular problem; I think that setting -Djava.net.preferIPv4Stack=true would be optimizing for the wrong case. 

Also, this problem and the workarounds for it are pretty well known at this point, so it's possible that it isn't much of an issue because folks who run Java are already setting IPV6_V6ONLY=0. 

I mostly submitted this to document.","23/May/10 13:53;jbellis;Okay, resolving as not-a-problem then.","22/Oct/10 02:40;benjaminblack;Given the decreasing average skill level of users and the defaults on common distributions, this issue is becoming both more common and confusing more people.  Distributions are not going to be changing their defaults.  We should reconsider added the preferIPv4 setting to the default config, perhaps based on inspecting the setting of IPV6_V6ONLY.","22/Oct/10 07:16;scode;I agree. FWIW, this is an issue on FreeBSD too, so it's not limited to Linux (I'm not sure whether there is an equivalent to the sysctl work-around).

Googling for this issue I know lots of people seem to be running into this and asking about it on forums etc (not specifically to Cassandra). In the case of Cassandra, for many people it may be their first use of a production Java app. Assuming a non-programmer user, failing on this issue by default and having to figure out or ask about a stack trace is not a very good first-time experience.

While ideologically one might go for the ""it's the user's responsibility to sort out his networking"" argument, until IPv6 is more widely used it really seems like a completely sensible default. The level of expected clue expected if you're running in an IPv6 environment is currently high enough that having to nudge this system property is not an issue (especially if there's a faq entry/note about its use somewhere).
",29/Oct/10 21:40;urandom;committed.,"30/Oct/10 12:48;hudson;Integrated in Cassandra #581 (See [https://hudson.apache.org/hudson/job/Cassandra/581/])
    document CASSANDRA-628 change (prefer IPv4)

Patch by eevans
make -Djava.net.preferIPv4Stack=true default

Patch by eevans for CASSANDRA-628
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add updateForeignTokenUnsafe for BMT imports to work properly,CASSANDRA-523,12439443,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,10/29/2009 18:44,3/12/2019 14:12,3/13/2019 22:24,10/30/2009 19:46,0.5,,,,0,,,,,,"Since Binary Memtable is usually run in environments that use the Cassandra API without being a real node, we need a way to update the token metadata to be able to map keys to nodes.

",,,,,,,,,,,,,,,,,,,,30/Oct/09 19:30;lenn0x;0001-Added-updateForeignTokenUnsafe-for-BMT-v2.patch;https://issues.apache.org/jira/secure/attachment/12423711/0001-Added-updateForeignTokenUnsafe-for-BMT-v2.patch,29/Oct/09 18:47;lenn0x;0001-Added-updateForeignTokenUnsafe-for-BMT.patch;https://issues.apache.org/jira/secure/attachment/12423605/0001-Added-updateForeignTokenUnsafe-for-BMT.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,46:05.4,,,no_permission,,,,,,,,,,,,19736,,,Sat Oct 31 12:34:32 UTC 2009,,,,,,0|i0fzgf:,91342,,,,,,,,,,,30/Oct/09 19:30;lenn0x;Fixed contrib/bmt_example as well.,"30/Oct/09 19:46;jbellis;committed

(was on autopilot, forgot you commit your own patches now :)","31/Oct/09 12:34;hudson;Integrated in Cassandra #244 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/244/])
    add updateForeignTokenUnsafe for bulk loader.  patch by goffinet; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted HandOff doesn't work for SuperColumnFamilies,CASSANDRA-491,12438149,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,sammy.yu,sammy.yu,10/15/2009 1:47,3/12/2019 14:12,3/13/2019 22:24,10/20/2009 17:45,0.5,,,,0,,,,,,"I was combing through our logs and noticed the following error which seems to indicate that hinted handoffs doesn't work for Super Column families.

ERROR [HINTED-HANDOFF-POOL:25] 2009-10-14 00:11:58,723 DebuggableThreadPoolExecutor.java (line 127) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:250)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.db.SuperColumn.timestamp(SuperColumn.java:136)
        at org.apache.cassandra.db.HintedHandOffManager.deliverAllHints(HintedHandOffManager.java:180)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:52)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:246)
        ... 3 more
ERROR [HINTED-HANDOFF-POOL:25] 2009-10-14 00:11:58,723 CassandraDaemon.java (line 71) Fatal exception in thread Thread[HINTED-HANDOFF-POOL:25,5,main]
java.lang.RuntimeException: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:250)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.db.SuperColumn.timestamp(SuperColumn.java:136)
        at org.apache.cassandra.db.HintedHandOffManager.deliverAllHints(HintedHandOffManager.java:180)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:52)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:246)
        ... 3 more",,,,,,,,,,,,,,,,,,,,16/Oct/09 17:02;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-491-fix-HH-of-tombstones.txt;https://issues.apache.org/jira/secure/attachment/12422372/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-491-fix-HH-of-tombstones.txt,16/Oct/09 17:02;jbellis;ASF.LICENSE.NOT.GRANTED--0002-Avoid-using-non-existent-supercolumn-timestamp-in-dele.txt;https://issues.apache.org/jira/secure/attachment/12422373/ASF.LICENSE.NOT.GRANTED--0002-Avoid-using-non-existent-supercolumn-timestamp-in-dele.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,03:43.0,,,no_permission,,,,,,,,,,,,19718,,,Wed Oct 21 12:34:22 UTC 2009,,,,,,0|i0fz9b:,91310,,,,,,,,,,,"16/Oct/09 17:03;jbellis;02
    Avoid using non-existent supercolumn timestamp in delete op.  Better conceptual integrity to delete w/ correct system time anyway

01
    fix HH of tombstones

this should fix the problem w/o having to manually clear out old data.","20/Oct/09 15:27;junrao;So the only way to cleanup hinted data is to manually run cleanup compaction from nodeprode? We need to document that in the header of HHM.

Other than that, the patch looks good to me.","20/Oct/09 15:43;jbellis;patch includes this in HHM:

 * HHM never deletes the row from Application tables; there is no way to distinguish that
 * from hinted tombstones!  instead, rely on cleanup compactions to remove data
 * that doesn't belong on this node.

How would you suggest clarifying that?","20/Oct/09 17:33;junrao;Add this ""cleanup compactions have to be manually started through nodeprobe on each node"".","20/Oct/09 17:45;jbellis;done, and committed","21/Oct/09 12:34;hudson;Integrated in Cassandra #234 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/234/])
    Avoid using non-existent supercolumn timestamp in delete op.  Better conceptual integrity to delete w/ correct system time anyway.
patch by jbellis; reviewed by junrao for 
fix HH of tombstones.  patch by jbellis; reviewed by junrao for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make cassandra not allow itself to run out of memory during sustained inserts,CASSANDRA-157,12424977,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,daishi,daishi,5/8/2009 22:42,3/12/2019 14:12,3/13/2019 22:24,7/27/2009 22:41,,,,,0,,,,,,"Tv on IRC pointed out to me that the issue that I've been encountering
is probably point 2. in this roadmap:

    http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg00160.html

I was unable to find any existing issue for this topic, so I'm creating a new one.

Since this issue would block our use of Cassandra I'm happy to look into it,
but if this is a known issue perhaps there's already a plan for addressing it
that could be clarified?",,,,,,,,,,,,,,,,,,,,27/Jul/09 22:23;jbellis;157.patch;https://issues.apache.org/jira/secure/attachment/12414671/157.patch,12/May/09 17:15;daishi;Cassandra-157_Unregister_Memtable_MBean.diff;https://issues.apache.org/jira/secure/attachment/12407898/Cassandra-157_Unregister_Memtable_MBean.diff,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,44:37.2,,,no_permission,,,,,,,,,,,,19574,,,Tue Jul 28 13:29:27 UTC 2009,,,,,,0|i0fx7z:,90980,,,,,,,,,,,"09/May/09 00:44;jbellis;if you're running out of memory, start by decreasing these settings in conf/storage-conf.  svn up first because objectcount used to default to 0.1 (i.e. 100k objects).

    <!--
      The maximum amount of data to store in memory before flushing to
      disk. Note: There is one memtable per column family, and this threshold
      is based solely on the amount of data stored, not actual heap memory
      usage (there is some overhead in indexing the columns).
    -->
    <MemtableSizeInMB>32</MemtableSizeInMB>

    <!--
      The maximum number of columns in millions to store in memory
      before flushing to disk.  This is also a per-memtable setting.
      Use with MemtableSizeInMB to tune memory usage.
    -->
    <MemtableObjectCountInMillions>0.02</MemtableObjectCountInMillions>

","09/May/09 00:49;jbellis;Ah, I see you've had this discussion with urandom on irc.

Like he said, play with jconsole, it may be your limits are still not low enough or possibly there is a bug in its internal accounting.","11/May/09 17:32;urandom;I think we need a wiki page which details what we've learned about tuning thresholds (and yes, I suppose I am volunteerin :).","12/May/09 17:15;daishi;This patch removes the MBean reference to Memtable objects,
reducing the memory pressure under sustained inserts somewhat.
(This helps, but I think there are other remaining issues).",12/May/09 17:56;jbellis;great fix!  committed.,"13/May/09 09:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    unregister mbean on flush.  patch by daishi; reviewed by jbellis for 
",15/May/09 19:44;urandom;http://wiki.apache.org/cassandra/MemtableThresholds,04/Jun/09 22:41;jbellis;Is this still an issue?,"05/Jun/09 01:14;daishi;Sorry I haven't had a chance to follow up on this.

If I remember correctly the main unbounded memory growth
that caused OOMs did seem to go away, but that the exposed
properties didn't control the memory usage in a predictable way.

In particular I think the objCount property would schedule
a Memtable.flush(), but ColumnFamilyStore.memtablesPendingFlush
would be actually flushed according to a different schedule.
I haven't read enough of the code to know whether it would
be possible/make sense to allow user-control of latter schedule,
but that was the direction I was thinking in when I had to drop
this for a bit (maybe you can let me know if that makes no
sense or if there's a better way to think about things).
","05/Jun/09 01:18;jbellis;> ColumnFamilyStore.memtablesPendingFlush  would be actually flushed according to a different schedule

flushes are processed one at a time, first come first served, to avoid swamping the system with IO.

does that jive with what you saw?","05/Jun/09 01:42;daishi;> flushes are processed one at a time, first come first served, to avoid swamping the system with IO.
> does that jive with what you saw? 

I guess that depends on what ""first come first served"" means.
Basically what I observed was that I could control how
frequently Memtable.flush() happened, but that only
after N memtables had been ""queued up"" to flush
would they actually be flushed to disk (I presume -
my view is based only on observing memory usage).
I think N was 10-20, but I don't remember exactly right now.
",05/Jun/09 02:04;jbellis;That's strange.  Let me know if you can reproduce that behavior still with 0.3 or trunk.  It shouldn't be queueing up unless the flusher is busy with a different memtable.,"27/Jul/09 22:23;jbellis;Found the (a?) problem.

memtablesPendingFlush used NonBlockingHashSets to store the memtables-to-flush.  But NBHS uses a NBHMap under the hood, which when remove() is called, assigns a tombstone value to the key instead of actually removing it.  (See http://sourceforge.net/tracker/?func=detail&aid=2828100&group_id=194172&atid=948362.)
",27/Jul/09 22:34;brandon.williams;+1  The heap usage is much improved for me with this patch.,27/Jul/09 22:41;jbellis;committed,"28/Jul/09 13:29;hudson;Integrated in Cassandra #151 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/151/])
    use CSLS instead of NBHS for memtablesPendingFlush. See explanation here: http://sourceforge.net/tracker/?func=detail&aid=2828100&group_id=194172&atid=948362
patch by jbellis; reviewed by Brandon Williams for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mixed line endings in the codebase,CASSANDRA-111,12423791,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jmhodges,jmhodges,jmhodges,4/26/2009 1:15,3/12/2019 14:12,3/13/2019 22:24,7/30/2009 15:28,0.4,,,,0,,,,,,"There seems to be a lot of line endings in the codebase that end with DOS line endings. Some files are even mixes of DOS and Unix line endings. This patch removes all the DOS file endings and corrects them to Unix.

Also attached is the program that I used to remove them. 

Using Unix line endings in all places makes lots of things easier. At the very least, a consistent choice of line endings would. Has there been discussion of what should be the correct line ending to use?",,,,,,,,,,,,,,,,,,,,23/Jul/09 22:54;jmhodges;0001-unix-line-endings-everywhere-on-top-of-r797209.patch;https://issues.apache.org/jira/secure/attachment/12414390/0001-unix-line-endings-everywhere-on-top-of-r797209.patch,30/Jul/09 02:21;jmhodges;0001-unix-line-endings-on-799140.patch;https://issues.apache.org/jira/secure/attachment/12414976/0001-unix-line-endings-on-799140.patch,26/Apr/09 01:17;jmhodges;line_endings.patch;https://issues.apache.org/jira/secure/attachment/12406457/line_endings.patch,26/Apr/09 01:17;jmhodges;no.rb;https://issues.apache.org/jira/secure/attachment/12406456/no.rb,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,31:59.7,,,no_permission,,,,,,,,,,,,19553,,,Wed Aug 05 12:35:05 UTC 2009,,,,,,0|i0fwxz:,90935,,,,,,,,,,,26/Apr/09 01:17;jmhodges;This is the patch file to remove the DOS line endings from the code. no.rb is a ruby program that I ran to create this patch.,"26/Apr/09 01:26;jmhodges;Oh, and that program is so ugly because I wanted to be sure I was just replacing ""\r\n"" and not also adding newlines at the ends of files, etc. ","26/Apr/09 15:31;jbellis;The vast majority of the files use Windows line endings, which makes sense since the original authors develop on Windows.  It makes more sense to me to standardize on that, even though I develop on Linux.","02/Jul/09 20:02;euphoria;As long as we standardized on one or the other, as long as we set the svn:eol-style to native things should resolve themselves, right?  Windows users see Windows line endings and *X users see *X line endings.",02/Jul/09 21:59;jmhodges;I like the sound of this deep svn voodoo.,02/Jul/09 22:52;sandeep_tata;The mixed line-endings drove me nuts when developing on Windows. Cygwin made everything worse. None of the eol settings in git would do the right thing. Standardizing on one set of endings would be cause for great joy!,03/Jul/09 02:32;jbellis;I had no idea that existed.  Will it deal with the few files that have mixed line endings?,"03/Jul/09 02:58;euphoria;I don't think it would handle them properly, although I'm not sure.  You can set individual files to one or the other, or native, but I think you'd have to leave it without an svn:eol-style if you *wanted* to keep it mixed.  It'd be better to convert them first if there's no compelling reason for them to be mixed.

http://svnbook.red-bean.com/en/1.1/ch07s02.html#svn-ch-7-sect-2.3.5 is the relevant section of the subversion book.

The algorithm would be:
 - find all files we want to have proper eol's
 - convert them all to your native system's eol's
 - for each filename in files:
 - - svn propset svn:eol-style native filename
 - commit all files

The above is obviously most easily done with a script.

To keep things nice going forward after this is done, if each committer added the following section to their .subversion/config then new files would automatically have the correct eol style set:
[auto-props]
*.java = svn:eol-style=native
*.properties = svn:eol-style=native
*.py= svn:eol-style=native
*.sh = svn:eol-style=native;svn:executable
*.thrift = svn:eol-style=native
*.txt = svn:eol-style=native
*.xml = svn:eol-style=native","23/Jul/09 19:11;eweaver;I doubt the svn-magic will play well with Git.

Can we switch to Unix endings and be done with it?","23/Jul/09 19:24;euphoria;The svn-magic plays just fine with Git, git-svn knows about eol-style.  Even so, people not using git-svn and just using git directly would ignore the settings.

Re: Unix endings, yeah, I was suggesting moving to Unix endings.  Settings those styles allows editors and platforms without appropriate eol handling to treat them natively though.","23/Jul/09 22:49;jmhodges;Unix line ending conversion done from code in r797209.

Using this code:

{code}
find . -name ""*.java"" -print0 -o -name ""*.sh"" -print0 -o -name ""*.xml"" -print0 -o -name ""*.txt"" -print0 | xargs -0 dos2unix
{code}",23/Jul/09 22:51;jmhodges;I don't see anyone that prefers \r\n trying to make this consistent or making noise about wanting consistency. I say the Unix hackers win this one.,23/Jul/09 22:54;jmhodges;Corrected unix line endings patch. Last one was an aborted one that someone was renamed. Apologies.,"30/Jul/09 02:21;jmhodges;Since we're seeing more and more mixing of line endings (like in r798935), here is another patch moving cassandra to unix line endings. Patch is against r799140.","30/Jul/09 02:45;euphoria;with a trunk cassandra-crlf and a patched cassandra-crlf2 this returns no result:
diff -r -w cassandra-crlf cassandra-crlf2

+1 for applying this immediately.  There are no enormous patches in the queue and this doesn't mess with any Windows-specific files.

I still think we should be applying the svn properties and configuration I discuss above, but getting all the files into the correct format is the most important step.","30/Jul/09 15:28;jbellis;Committed.

 190 files changed, 30157 insertions(+), 30157 deletions(-)","31/Jul/09 12:34;hudson;Integrated in Cassandra #153 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/153/])
    run source files through dos2unix to standardize on \n line endings.  patch by Jeff Hodges; reviewed by Michael Greene for 
","05/Aug/09 12:35;hudson;Integrated in Cassandra #158 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/158/])
    fix .properties line endings. patch by jbellis; reviewed by Jeff Hodges for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exception when a node is joining,CASSANDRA-114,12424096,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,nk11,nk11,4/28/2009 21:58,3/12/2019 14:12,3/13/2019 22:24,4/30/2009 16:05,0.3,,,,0,,,,,,"When a new node is joining, StorageService is notified and the log shows this:

java.lang.NumberFormatException: For input string: ""T""
        at java.lang.NumberFormatException.forInputString(Unknown Source)
        at java.lang.Integer.parseInt(Unknown Source)
        at java.math.BigInteger.<init>(Unknown Source)
        at java.math.BigInteger.<init>(Unknown Source)
        at org.apache.cassandra.dht.RandomPartitioner$3.fromString(RandomPartitioner.java:97)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:646)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:770)
        at org.apache.cassandra.gms.Gossiper.handleNewJoin(Gossiper.java:644)

In my case, the EndPointState had a nodeIdState who's state_ was ""Token(165385238122940489124770581854348071118)"" and produced the exception.",,,,,,,,,,,,,,,,,,,,29/Apr/09 19:39;jbellis;114-v2.patch;https://issues.apache.org/jira/secure/attachment/12406821/114-v2.patch,29/Apr/09 02:45;jbellis;115.patch;https://issues.apache.org/jira/secure/attachment/12406735/115.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,45:02.3,,,no_permission,,,,,,,,,,,,19554,,,Fri May 01 13:09:51 UTC 2009,,,,,,0|i0fwyn:,90938,,,,,,,,,,,28/Apr/09 22:27;nk11;Perhaps in the StorageService.updateToken() the added ApplicationState should not be created using Token's toString(),29/Apr/09 02:45;jbellis;the attached patch should fix the problem.  can you test?,"29/Apr/09 19:30;nk11;It doesn't work yet.
In the StorageService.setup() there still is a Token.toString:
new ApplicationState(storageMetadata_.getStorageId().toString())
",29/Apr/09 19:39;jbellis;you're right.  updated patch attached.,30/Apr/09 16:04;nk11;the last one did it ,30/Apr/09 16:10;jbellis;committed patch.,"01/May/09 13:09;hudson;Integrated in Cassandra #55 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/55/])
    add string de/serialize code to TokenFactory for ApplicationState's benefit.
patch by jbellis; tested by nk11 and MarkR42 for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
weakreadremote has high response return (>100ms) ,CASSANDRA-219,12427238,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,6/5/2009 18:31,3/12/2019 14:12,3/13/2019 22:24,6/5/2009 18:52,0.3,,,,0,,,,,,"When a client makes a request to a node where data does not live, it tries to fetch from a remote node. We noticed that in some JDK's the select() has very strange bugs. Right now under:

src/java/org/apache/cassandra/net/SelectorManager.java

run() {
 ..
 select(100) should be changed to select(1);
}",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,43:04.1,,,no_permission,,,,,,,,,,,,19598,,,Fri Jun 05 18:43:04 UTC 2009,,,,,,0|i0fxlj:,91041,,,,,,,,,,,05/Jun/09 18:43;jbellis;committed to 0.3 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra cannot bootstrap when using DatacenterShardStrategy,CASSANDRA-1147,12465832,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mdennis,mdennis,mdennis,6/1/2010 7:03,3/12/2019 14:12,3/13/2019 22:24,6/14/2010 21:52,0.7 beta 1,,,,0,,,,,,"If C is configured to use DSS, the bootstrap process never completes.
",,,,,,,,,,,,,,,,,,,,14/Jun/10 21:51;jbellis;0002-1147.txt;https://issues.apache.org/jira/secure/attachment/12447071/0002-1147.txt,01/Jun/10 07:08;mdennis;CASSANDRA-1147.patch;https://issues.apache.org/jira/secure/attachment/12446001/CASSANDRA-1147.patch,01/Jun/10 20:29;mdennis;CASSANDRA-1147.patch2;https://issues.apache.org/jira/secure/attachment/12446061/CASSANDRA-1147.patch2,14/Jun/10 07:31;mdennis;CASSANDRA-1147.patch3;https://issues.apache.org/jira/secure/attachment/12447007/CASSANDRA-1147.patch3,14/Jun/10 17:40;mdennis;CASSANDRA-1147.patch4;https://issues.apache.org/jira/secure/attachment/12447043/CASSANDRA-1147.patch4,,,,,,,,,5,,,,,,,,,,,,,,,,,,,15:29.2,,,no_permission,,,,,,,,,,,,20008,,,Tue Jun 15 12:51:23 UTC 2010,,,,,,0|i0g3a7:,91962,,,,,,,,,,,01/Jun/10 07:08;mdennis;patch caches endpoints for getNaturalEndpoints (cleared on config or ring change) without requiring dcTokens/loadEndpoints and removes the need for CASSANDRA-1137,"01/Jun/10 13:15;jbellis;how does

         assert strategy.getReplicationFactor(""DC1"", table) == 3;

work when you're hardcoding it to 6 in the subclass?","01/Jun/10 14:35;mdennis;different methods.  the assert is for grf(String dc, String table) and corresponds to datacenters.properties the override is for grf(String table) and corresponds to replication_factor in cassandra.yaml","01/Jun/10 15:15;jbellis;could we just make DSS return sum(dc replication for table) as RF(table), for now?

(CASSANDRA-1066 should resolve this eventually by allowing us to move datacenter.properties into cassandra.yaml, and drop the per-table RF for strategies that it doesn't make sense for)",01/Jun/10 20:29;mdennis;patch2 against trunk r950215,"04/Jun/10 03:06;jbellis;caching natural endpoints seems like something we should put in AbstractReplicationStrategy for everyone.  what do you think?

(Given CASSANDRA-1014 I'm mildly excited about the opportunity to reduce the per-request garbage generated, by not rebuilding a fresh list every time.)

+        //is a new list required each time? (other strats do it)
+        //perhaps just an unmodifiable list?

yes, it's required, because it's going to be passed to sortByProximity.

+            total+=repFactor;

operator spacing please :)

+        System.out.println(""endpoints: "" + endpoints);

r/m this.",14/Jun/10 07:31;mdennis;CASSANDRA-1147.patch3 implements caching and tests for all replication strategies,"14/Jun/10 16:20;jbellis;please fix re-ordering imports incorrectly.  correct order is

  * java
  * org.apache.commons 
  * org.apache.slf4j
  * org.junit
  * everything else alphabetically

(http://wiki.apache.org/cassandra/CodeStyle)","14/Jun/10 17:40;mdennis;CASSANDRA-1147.patch4 follows coding convention for imports

(note: the patch produced by ""svn diff"" was not originally usable because of the line endings in RackAwareStrategyTest so dos2unix was run on the patch)","14/Jun/10 21:51;jbellis;Fix using rack as a key to datacenters map

Fix returning multiple copies of the same endpoint from DSS; refactored calculateNaturalEndpoints to return a Set to make it more clear that this is not allowed

simplified DSS.cNE and made it not put more replicas in a DC than are configured

Use NBHM instead of ConcurrentHM in ARS

Use map.clear instead of AtomicReference + creating a new map in ARS (clear is a very rare operation, better to make that a little slower in exchange for not having to do an extra, fenced dereference on the common ops)

Removed unused unregister method from IEPS

Removed interfaces with a single implementor (ARS)

Cleaned up formatting",14/Jun/10 21:52;jbellis;committed w/ above changes,"15/Jun/10 12:51;hudson;Integrated in Cassandra #466 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/466/])
    avoid allowing endpoints computed from old token map to persist after clearCachedEndpoints.  patch by mdennis and jbellis for CASSANDRA-1147
Fix bootstrap with DSS and add endpoint caching.
patch by mdennis and jbellis for CASSANDRA-1147
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE during read repair,CASSANDRA-478,12437607,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/8/2009 15:44,3/12/2019 14:12,3/13/2019 22:24,10/12/2009 19:57,0.4,,,,0,,,,,,"From Teodor Sigaev:

ERROR [RESPONSE-STAGE:1] 2009-10-08 17:05:25,864 DebuggableThreadPoolExecutor.java (line 110) Error in ThreadPoolExecutor
java.lang.NullPointerException
       at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:68)
       at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:55)
       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
       at java.lang.Thread.run(Thread.java:636)
ERROR [RESPONSE-STAGE:1] 2009-10-08 17:05:25,916 CassandraDaemon.java (line 71) Fatal exception in thread Thread[RESPONSE-STAGE:1,5,main]
java.lang.NullPointerException
       at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:68)
       at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:55)
       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)",,,,,,,,,,,,,,,,,,,,12/Oct/09 18:12;jbellis;478-asserts.patch;https://issues.apache.org/jira/secure/attachment/12421888/478-asserts.patch,12/Oct/09 18:55;jbellis;478-fix.patch;https://issues.apache.org/jira/secure/attachment/12421893/478-fix.patch,08/Oct/09 15:45;jbellis;478-logging.patch;https://issues.apache.org/jira/secure/attachment/12421630/478-logging.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,26:41.3,,,no_permission,,,,,,,,,,,,19711,,,Wed Oct 14 07:55:18 UTC 2009,,,,,,0|i0fz6f:,91297,,,,,,,,,,,08/Oct/09 15:45;jbellis;Patch to log the Row that encounters this error,"12/Oct/09 17:26;teodor;Got it again:
ERROR [RESPONSE-STAGE:2] 2009-10-12 21:17:44,256 DebuggableThreadPoolExecutor.java (line 110) Error in ThreadPoolExecutor
java.lang.RuntimeException: Error handling responses for Row(00000000000176799500 [)]
        at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:82)
        at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:55)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:70)
        ... 6 more
ERROR [RESPONSE-STAGE:2] 2009-10-12 21:17:44,256 CassandraDaemon.java (line 71) Fatal exception in thread Thread[RESPONSE-STAGE:2,5,main]
java.lang.RuntimeException: Error handling responses for Row(00000000000176799500 [)]
        at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:82)
        at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:55)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:70)
        ... 6 more
",12/Oct/09 17:51;jbellis;is it reproducible when you re-run the query on that key? (00000000000176799500),"12/Oct/09 18:10;teodor;No, it works ",12/Oct/09 18:12;jbellis;asserts to find where the null Message body is coming from,"12/Oct/09 18:16;teodor;Ok, will apply","12/Oct/09 18:55;jbellis;aha, it's a race condition :)

here's a fix.",12/Oct/09 18:58;jbellis;(there's no way for a null Message to be passed to response() -- see ResponseVerbHandler for details.  but it could see a null in the List if there is another thread modifying it concurrently.),12/Oct/09 19:02;urandom;Nice. :) +1,12/Oct/09 19:57;jbellis;committed to 0.4 and trunk,"13/Oct/09 12:35;hudson;Integrated in Cassandra #226 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/226/])
    move callback synchronization into ResponseVerbHandler where it's less likely to be a maintenance problem
patch by jbellis for 
r/m unused attachContext.
patch by jbellis for 
cleanup Message; add asserts
patch by jbellis for 
logging & cleanup in ConsistencyManager
patch by jbellis for 
",14/Oct/09 07:55;teodor;Thank you a lot!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""ant"" always compiles at least 36 files, even if no changes were made",CASSANDRA-1356,12470810,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,messi,jbellis,jbellis,8/4/2010 15:38,3/12/2019 14:12,3/13/2019 22:24,8/5/2010 13:45,0.7 beta 1,,Packaging,,0,,,,,,,,,,,,,,,,,,,,,,,,,,05/Aug/10 02:39;messi;correct-src-path.patch;https://issues.apache.org/jira/secure/attachment/12451297/correct-src-path.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,45:52.3,,,no_permission,,,,,,,,,,,,20101,,,Sun Aug 08 13:52:16 UTC 2010,,,,,,0|i0g4jz:,92168,,,,,,,,,,,05/Aug/10 13:45;gdusbabek;+1 committed.,"08/Aug/10 13:52;hudson;Integrated in Cassandra #510 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/510/])
    adjust src paths. patch by Folke Behrens, reviewed by Gary Dusbabek. CASSANDRA-1356
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli.bat batch file not passing arguments onto the main class,CASSANDRA-797,12456369,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,wolfeidau,wolfeidau,wolfeidau,2/15/2010 23:23,3/12/2019 14:12,3/13/2019 22:24,2/17/2010 0:35,0.6,,Legacy/Tools,,0,windows,,,,,"If you run the following command in windows the arguments will not get passed to the CliMain class.

E:\Working\apache-cassandra-incubating-trunk>bin\cassandra-cli.bat --host 127.0.0.1 --port 9160
",WIndows,,,,,,,,,,,,,,,,,,,17/Feb/10 00:13;wolfeidau;cassandra-cli-bat.patch;https://issues.apache.org/jira/secure/attachment/12436059/cassandra-cli-bat.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,57:16.5,,,no_permission,,,,,,,,,,,,19868,,,Wed Feb 17 17:54:43 UTC 2010,,,,,,0|i0g14v:,91614,,,,,,,,,,,15/Feb/10 23:27;wolfeidau;I have essentially rewritten the batch file using the daemon one as a template while also correcting the issue with arguments not being passed to the CLIMain class.,"16/Feb/10 00:29;wolfeidau;Updated patch which resolves the identified issue and cleans up the batch file a bit.

",16/Feb/10 01:31;wolfeidau;Patch to the cassandra-cli.bat in attached file which has been  tested on windows.,"16/Feb/10 18:57;urandom;This patch isn't applying for me, perhaps it needs to be rebased against current trunk?","17/Feb/10 00:13;wolfeidau;Re-created the patch the old fashion way rather than relying on Idea to generate it.

Tested this and it applies fine now.",17/Feb/10 00:35;urandom;committed; thanks,"17/Feb/10 17:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tombstone-only rows in sstables can be ignored,CASSANDRA-1063,12464004,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,stuhood,stuhood,5/7/2010 16:03,3/12/2019 14:12,3/13/2019 22:24,5/7/2010 20:09,0.6.2,,,,0,,,,,,"ColumnFamilyStoreTest has two tests that pass, that shouldn't.  These are obscuring bugs in tombstone handling.",,,,,,,,,,,,,,,,,,,,07/May/10 16:04;stuhood;0001-Remove-implementation-awareness-from-CFSTest.patch;https://issues.apache.org/jira/secure/attachment/12443976/0001-Remove-implementation-awareness-from-CFSTest.patch,07/May/10 18:24;jbellis;1063.txt;https://issues.apache.org/jira/secure/attachment/12443987/1063.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,24:41.1,,,no_permission,,,,,,,,,,,,19981,,,Fri May 07 20:09:07 UTC 2010,,,,,,0|i0g2rr:,91879,,,,,,,,,,,07/May/10 16:04;stuhood;Patch against 0.6.,"07/May/10 18:24;jbellis;There's actually a real bug getting masked here -- the correct answer is not null, but an empty, tombstoned CF.

In fact there are three bugs where we are incorrectly not taking into account row tombstones:

 - we can't just return null if the returnCF is empty, we need to check for tombstones via removeDeleted
 - we can't skip the delete call while building our iterator list, if the column iterator is empty
 - we can't skip deserializing the CF if all the columns in a names predicate are rejected by the bloom filter",07/May/10 18:43;stuhood;+1 for 1063.txt. Good finds!,07/May/10 20:09;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad EndpointSnitch config in trunk,CASSANDRA-1009,12462717,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,4/22/2010 0:55,3/12/2019 14:12,3/13/2019 22:24,4/22/2010 0:58,0.7 beta 1,,,,0,,,,,,A few config changes made it into trunk for 990/1000 that shouldn't have.,,,,,,,,,,,,,,,,,,,,22/Apr/10 00:56;stuhood;0001-Two-bad-config-changes-slipped-in-with-990.patch;https://issues.apache.org/jira/secure/attachment/12442498/0001-Two-bad-config-changes-slipped-in-with-990.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:53.7,,,no_permission,,,,,,,,,,,,19953,,,Thu Apr 22 00:58:53 UTC 2010,,,,,,0|i0g2fz:,91826,,,,,,,,,,,"22/Apr/10 00:56;stuhood;Fixes the default log level, and applies EndpointSnitch changes to the default config.",22/Apr/10 00:58;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
loadSchemaFromXml records no migrations,CASSANDRA-963,12461451,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,4/7/2010 20:01,3/12/2019 14:12,3/13/2019 22:24,4/8/2010 13:48,0.7 beta 1,,,,0,,,,,,This means that there is nothing to propagate to new nodes when schema is force loaded from xml.,,,,,,,,,,,CASSANDRA-962,,,,,,,,,08/Apr/10 00:15;gdusbabek;0001-SS.loadSchemaFromXML-should-record-migrations.patch;https://issues.apache.org/jira/secure/attachment/12441094/0001-SS.loadSchemaFromXML-should-record-migrations.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,43:46.2,,,no_permission,,,,,,,,,,,,19939,,,Fri Apr 09 13:43:11 UTC 2010,,,,,,0|i0g25r:,91780,,,,,,,,,,,08/Apr/10 13:43;jbellis;+1,"09/Apr/10 13:43;hudson;Integrated in Cassandra #402 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/402/])
    SS.loadSchemaFromXML should record migrations. Patch by Gary Dusbabek, reviewed by Jonathan Ellis. CASSANDRA-963
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable DEBUG file logging during unit tests,CASSANDRA-923,12460460,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,3/28/2010 1:48,3/12/2019 14:12,3/13/2019 22:24,3/28/2010 2:00,0.6,,,,0,,,,,,DEBUG logging to the build/test/logs directory was disabled at some point.,,,,,,,,,,,,,,,,,,,,28/Mar/10 01:48;stuhood;test-logging.patch;https://issues.apache.org/jira/secure/attachment/12439996/test-logging.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,00:39.4,,,no_permission,,,,,,,,,,,,19921,,,Sun Mar 28 02:00:39 UTC 2010,,,,,,0|i0g1wv:,91740,,,,,,,,,,,28/Mar/10 02:00;jbellis;committed to 0.6 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to close files in loadBloomFilter and loadIndexFile,CASSANDRA-533,12440111,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tim freeman,tim freeman,tim freeman,11/7/2009 22:29,3/12/2019 14:12,3/13/2019 22:24,11/7/2009 23:22,0.4,0.5,,,0,,,,,,"When starting Cassandra on a Windows system, I intermittently see errors like:

DEBUG - Expected bloom filter size : 2560
DEBUG - collecting Generation:false:4@9
DEBUG - collecting Token:false:16@0
INFO - Saved Token found: 25027551081353517716727338628156823602
ERROR - Error in ThreadPoolExecutor
java.util.concurrent.ExecutionException: java.io.IOException: Failed to delete LocationInfo-8-Index.db
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:8
6)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Failed to delete LocationInfo-8-Index.db
        at org.apache.cassandra.utils.FileUtils.deleteWithConfirm(FileUtils.java:55)
        at org.apache.cassandra.io.SSTableReader.delete(SSTableReader.java:269)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:1145)
        at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:686)
        at org.apache.cassandra.db.MinorCompactionManager$1.call(MinorCompactionManager.java:166)
        at org.apache.cassandra.db.MinorCompactionManager$1.call(MinorCompactionManager.java:163)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
DEBUG - Starting to listen on 127.0.0.1:7001
DEBUG - Binding thrift service to localhost:9160

The problem here is that the files are not closed immediately after they are read, and on Windows you can't delete an open file.  In general, failing to close a file causes more subtle problems on a Unix system.  You can run out of file descriptors if files are opened faster than the garbage collector closes them, and you can run out of disk space if the total data in the opened-but-deleted files is enough to fill the disk.

I see two places to fix, SSTableReader.loadIndexFile and SSTableReader.loadBloomFilter.  Here are revised definitions for those two methods:

    private void loadBloomFilter() throws IOException
    {
        DataInputStream stream = new DataInputStream(new FileInputStream(filterFilename()));
        try {
        	bf = BloomFilter.serializer().deserialize(stream);
        } finally {
        	stream.close();
        }
    }

    private void loadIndexFile() throws IOException
    {
        BufferedRandomAccessFile input = new BufferedRandomAccessFile(indexFilename(), ""r"");
        try {
        	indexPositions = new ArrayList<KeyPosition>();

        	int i = 0;
        	long indexSize = input.length();
        	while (true)
        	{
        		long indexPosition = input.getFilePointer();
        		if (indexPosition == indexSize)
        		{
        			break;
        		}
        		String decoratedKey = input.readUTF();
        		input.readLong();
        		if (i++ % INDEX_INTERVAL == 0)
        		{
        			indexPositions.add(new KeyPosition(decoratedKey, indexPosition));
        		}
        	}
        } finally {
        	input.close();
        }
    }   

I have not yet tested those changes, but they look desirable even if they don't fix the symptom I'm experiencing.  I did not search the code for other places that files are opened but not closed.
","Vista, Cassandra 0.4.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,22:44.6,,,no_permission,,,,,,,,,,,,19741,,,Sat Nov 07 23:22:44 UTC 2009,,,,,,0|i0fzin:,91352,,,,,,,,,,,07/Nov/09 22:47;tim freeman;The fix did get rid of the stacktrace,07/Nov/09 23:22;jbellis;committed to 0.4 branch; will merge to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowMutation applied twice in RowMutationVerbHandler,CASSANDRA-662,12444458,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,12/31/2009 22:20,3/12/2019 14:12,3/13/2019 22:24,12/31/2009 22:48,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,31/Dec/09 22:21;gdusbabek;662.diff;https://issues.apache.org/jira/secure/attachment/12429227/662.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,36:38.3,,,no_permission,,,,,,,,,,,,19808,,,Fri Jan 01 12:35:18 UTC 2010,,,,,,0|i0g0bb:,91481,,,,,,,,,,,31/Dec/09 22:22;gdusbabek;Removes wayward call to apply().,31/Dec/09 22:36;jbellis;+1,"31/Dec/09 22:48;gdusbabek;r894951 (trunk), r894952 (0.5).","01/Jan/10 12:35;hudson;Integrated in Cassandra #310 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/310/])
    Remove duplicated write (). Patch by Gary Dusbabek, reviewed by Jonathan Ellis.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RingCache code breaks,CASSANDRA-566,12441228,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,11/19/2009 21:21,3/12/2019 14:12,3/13/2019 22:24,11/19/2009 22:26,0.5,,,,0,,,,,,The code that creates AbstractReplicationStrategy in RingCache is broken.,,,,,,,,,,,,,,,,,,,,19/Nov/09 21:23;junrao;issue566.patchev1;https://issues.apache.org/jira/secure/attachment/12425528/issue566.patchev1,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,32:01.8,,,no_permission,,,,,,,,,,,,19758,,,Thu Nov 19 22:26:29 UTC 2009,,,,,,0|i0fzpz:,91385,,,,,,,,,,,19/Nov/09 21:23;junrao;Attach a patch.,19/Nov/09 21:32;jbellis;+1,19/Nov/09 22:26;junrao;committed to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getRangeSlice returns keys from outside the desired range,CASSANDRA-763,12455276,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,2/4/2010 6:35,3/12/2019 14:12,3/13/2019 22:24,2/6/2010 4:14,0.6,,,,0,,,,,,,,,,,,,,,,,CASSANDRA-342,,,,,,,,,05/Feb/10 20:05;jbellis;ASF.LICENSE.NOT.GRANTED--0001-add-Range.intersectsWith.txt;https://issues.apache.org/jira/secure/attachment/12435000/ASF.LICENSE.NOT.GRANTED--0001-add-Range.intersectsWith.txt,05/Feb/10 20:05;jbellis;ASF.LICENSE.NOT.GRANTED--0002-have-RangeSliceCommand-take-Range-or-Bounds-client-bou.txt;https://issues.apache.org/jira/secure/attachment/12435001/ASF.LICENSE.NOT.GRANTED--0002-have-RangeSliceCommand-take-Range-or-Bounds-client-bou.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,21:18.9,,,no_permission,,,,,,,,,,,,19854,,,Wed Feb 17 17:54:46 UTC 2010,,,,,,0|i0g0xb:,91580,,,,,,,,,,,"05/Feb/10 20:04;jbellis;the semantics of client start/end keys passed to get_range_slice and the tokens in a Range object are different.  CASSANDRA-759 and CASSANDRA-758 tried to add options to getRangeSlice to be able to handle both but the two are still too different.  consider for instance the case where start=end -- in the former case this is a range with exactly one (potential) key included; in the latter it is a wrapping range including the entire ring.

this partially backs out 758 and 759 and instead adds a Bounds class (that shares a common superclass w/ Range); getRangeSlice can then take either a Bounds (which has the semantics of the old start/end pair) or a Range, which is the case CASSANDRA-342 is interested in, and can now use intersectsWith to make sure we're only getting each key once.","05/Feb/10 21:21;stuhood; * Are we trying not to break network compatibility in 0.6? This changes RangeSliceCommand
 * I'm worried that since the AbstractBounds class doesn't implement any methods, we are basically using the subclasses as boolean values: ('is the start included?' == x instanceof Bounds). To make the subclasses worthwhile, we need to see a path toward actually encapsulating the intersection logic in them: for instance, StorageProxy.restrictBounds should be implemented in terms of an intersection between two AbstractBounds objects.

In general, this looks like an improvement, so if we can see a way in the future to encapsulate more logic in bounds, I'd be fine with giving it a +1.","05/Feb/10 21:43;jbellis;> Are we trying not to break network compatibility in 0.6

Nope, glad you are thinking about that but we already moved gossip to TCP which means no.  (Seems like a long time ago but yes that will be 0.6 :)

> we need to see a path toward actually encapsulating the intersection logic in them: for instance

You're right.  I will make that change.",06/Feb/10 04:14;jbellis;committed w/ above changes,"17/Feb/10 17:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix failing CompactionsPurgeTest,CASSANDRA-719,12445969,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,1/19/2010 21:28,3/12/2019 14:12,3/13/2019 22:24,1/22/2010 1:12,0.6,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,21/Jan/10 23:51;jbellis;719.txt;https://issues.apache.org/jira/secure/attachment/12431079/719.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,03:02.8,,,no_permission,,,,,,,,,,,,19835,,,Fri Jan 22 12:36:21 UTC 2010,,,,,,0|i0g0nj:,91536,,,,,,,,,,,"19/Jan/10 21:28;jbellis;(see CASSANDRA-710 for genesis.  Note that this test has been silently broken for we don't know how long, something in the other tests in CompactionsTest were covering it up.)","21/Jan/10 23:51;jbellis;invalidate cache after compaction in test so we read what's in the new sstable instead of the cache

alternatively we could fix this by making compaction invalidate the cache, but i think this is a bad idea; the system is designed so that to the client it makes no difference if the tombstones are GC'd or not since we hide them, so there is no hurry to uncache things.  And compaction (when we are doing a lot of extra io) is a particularly bad time to throw away the whole cache.  Just leave it to be invalidated eventually through natural causes.",22/Jan/10 00:03;lenn0x;+1,22/Jan/10 01:12;jbellis;committed,"22/Jan/10 12:36;hudson;Integrated in Cassandra #331 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/331/])
    invalidate cache after compaction so we read what's in the new sstable instead of the cache
patch by jbellis; reviewed by goffinet for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multi-get slice failing Nullpointer Exception,CASSANDRA-689,12445261,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,1/12/2010 6:36,3/12/2019 14:12,3/13/2019 22:24,1/13/2010 19:11,0.6,,,,0,,,,,,"Noticed this in trunk

ERROR [pool-1-thread-40] 2010-01-11 22:13:55,333 Cassandra.java (line 960) Internal error processing multiget_slice
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:510)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:375)
        at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:81)
        at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:164)
        at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:237)
        at org.apache.cassandra.service.CassandraServer.multiget_slice(CassandraServer.java:209)
        at org.apache.cassandra.service.Cassandra$Processor$multiget_slice.process(Cassandra.java:952)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:842)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:506)
        ... 11 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.filter.SliceQueryFilter.filterSuperColumn(SliceQueryFilter.java:70)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:809)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:750)
        at org.apache.cassandra.db.Table.getRow(Table.java:398)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:59)
        at org.apache.cassandra.service.StorageProxy$weakReadLocalCallable.call(StorageProxy.java:691)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
",,,,,,,,,,,,,,,,,,,,13/Jan/10 06:28;jbellis;ASF.LICENSE.NOT.GRANTED--0001-fix-enabling-disabling-row-cache.txt;https://issues.apache.org/jira/secure/attachment/12430101/ASF.LICENSE.NOT.GRANTED--0001-fix-enabling-disabling-row-cache.txt,13/Jan/10 06:28;jbellis;ASF.LICENSE.NOT.GRANTED--0002-add-missing-gcBefore-parameter-to-removeDeleted-clone-.txt;https://issues.apache.org/jira/secure/attachment/12430102/ASF.LICENSE.NOT.GRANTED--0002-add-missing-gcBefore-parameter-to-removeDeleted-clone-.txt,13/Jan/10 06:28;jbellis;ASF.LICENSE.NOT.GRANTED--0003-fix-missing-update-of-local-deletion-time-from-a-while.txt;https://issues.apache.org/jira/secure/attachment/12430103/ASF.LICENSE.NOT.GRANTED--0003-fix-missing-update-of-local-deletion-time-from-a-while.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,22:09.1,,,no_permission,,,,,,,,,,,,19819,,,Thu Jan 14 12:35:00 UTC 2010,,,,,,0|i0g0h3:,91507,,,,,,,,,,,12/Jan/10 13:22;jbellis;looks like a bug I introduced w/ the mmap code.  I don't suppose you have a query that can reproduce it?,12/Jan/10 22:41;jbellis;I can reproduce the problem; it's definitely from the cache patches (not mmap),"13/Jan/10 06:29;jbellis;03
    fix missing update of local deletion time from a while ago

02
    add missing gcBefore parameter to removeDeleted; clone SC from cache before modifying

01
    fix enabling/disabling row cache (which exposes rowcache to tests) and fix obvious NPE bugs

","13/Jan/10 07:28;lenn0x;Applied to our unstable cluster, will update ticket when further testing is complete.",13/Jan/10 17:36;lenn0x;+1,13/Jan/10 19:11;jbellis;committed,"14/Jan/10 12:35;hudson;Integrated in Cassandra #323 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/323/])
    fix missing update of local deletion time introduced in #658.
patch by jbellis; reviewed by goffinet for 
add missing gcBefore parameter to removeDeleted; clone SC from cache before modifying
patch by jbellis; reviewed by goffinet for 
fix enabling/disabling row cache.
patch by jbellis; reviewed by goffinet for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_range_slice NPE,CASSANDRA-578,12441537,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,11/24/2009 1:30,3/12/2019 14:12,3/13/2019 22:24,11/25/2009 23:16,0.5,,,,0,,,,,,"If I call get_range_slice with arguments in the SliceRange structure, then it seems to NPE.  I think it only does it when there is nothing in the range specified in the column slice start and end.


ERROR - Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:55)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Row.addColumnFamily(Row.java:96)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1469)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)
        ... 4 more
ERROR - Fatal exception in thread Thread[ROW-READ-STAGE:8,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:55)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Row.addColumnFamily(Row.java:96)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1469)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)
        ... 4 more
",,,,,,,,,,,,,,,,,,,,25/Nov/09 19:35;jbellis;0003-allow-serializing-null-CF-add-get_range_slice-test.patch;https://issues.apache.org/jira/secure/attachment/12426136/0003-allow-serializing-null-CF-add-get_range_slice-test.patch,24/Nov/09 02:57;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-578-r-m-unused-Row-code-and-move-table-vari.txt;https://issues.apache.org/jira/secure/attachment/12425928/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-578-r-m-unused-Row-code-and-move-table-vari.txt,24/Nov/09 02:57;jbellis;ASF.LICENSE.NOT.GRANTED--0002-make-Row-contain-a-single-final-CF-reference.txt;https://issues.apache.org/jira/secure/attachment/12425929/ASF.LICENSE.NOT.GRANTED--0002-make-Row-contain-a-single-final-CF-reference.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,58:18.8,,,no_permission,,,,,,,,,,,,19763,,,Thu Nov 26 12:34:02 UTC 2009,,,,,,0|i0fzsn:,91397,,,,,,,,,,,"24/Nov/09 02:58;jbellis;02
    make Row contain a single, final CF reference

01
    r/m unused Row code, and move table variable into callers rather than serializing it redundantly
","24/Nov/09 03:18;dispalt;Still get this error with the patches applied.

DEBUG - range_slice
DEBUG - reading org.apache.cassandra.db.RangeSliceCommand@4ef18d37 from 31@/127.0.0.1
ERROR - Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:53)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:58)
        at org.apache.cassandra.db.RowSerializer.serialize(Row.java:92)
        at org.apache.cassandra.db.RangeSliceReply.getReply(RangeSliceReply.java:53)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:46)
        ... 4 more
ERROR - Fatal exception in thread Thread[ROW-READ-STAGE:11,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:53)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:58)
        at org.apache.cassandra.db.RowSerializer.serialize(Row.java:92)
        at org.apache.cassandra.db.RangeSliceReply.getReply(RangeSliceReply.java:53)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:46)
        ... 4 more
",24/Nov/09 04:20;jbellis;can you reproduce against the default set of columnfamilies?,"24/Nov/09 04:31;dispalt;Let me try to do that.

just fyi in org.apache.cassandra.db.row.RowSerializer row.cf is null which is causing the NPE.","24/Nov/09 04:36;jbellis;I can reproduce it here, I'm good.","24/Nov/09 04:50;jbellis;03
    allow serializing null CF; add get_range_slice test exercising this","24/Nov/09 05:09;dispalt;I am getting a couple errors that I wasn't getting before and I think its related.


2009-11-24_05:06:54.65928 java.lang.NullPointerException
2009-11-24_05:06:54.65928       at org.apache.cassandra.db.Row.digest(Row.java:75)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:103)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:453)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758)
2009-11-24_05:06:54.65928       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)
2009-11-24_05:06:54.65928       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
2009-11-24_05:06:54.65928       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-24_05:06:54.65928       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-24_05:06:54.65928       at java.lang.Thread.run(Thread.java:636)
2009-11-24_05:06:56.82921 ERROR - Internal error processing get_slice
2009-11-24_05:06:56.82921 java.lang.NullPointerException
2009-11-24_05:06:56.82921       at org.apache.cassandra.db.Row.digest(Row.java:75)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:103)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:453)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758)
2009-11-24_05:06:56.82921       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)
2009-11-24_05:06:56.82921       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
2009-11-24_05:06:56.82921       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-24_05:06:56.82921       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-24_05:06:56.82921       at java.lang.Thread.run(Thread.java:636)



2009-11-24_05:06:57.54647 java.lang.NullPointerException
2009-11-24_05:06:57.54647       at org.apache.cassandra.db.Row.digest(Row.java:75)
2009-11-24_05:06:57.54647       at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:85)
2009-11-24_05:06:57.54647       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
2009-11-24_05:06:57.54647       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-24_05:06:57.54647       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-24_05:06:57.54647       at java.lang.Thread.run(Thread.java:636)
2009-11-24_05:06:57.62647 ERROR - Error in ThreadPoolExecutor
2009-11-24_05:06:57.62647 java.lang.NullPointerException
2009-11-24_05:06:57.62647       at org.apache.cassandra.db.Row.digest(Row.java:75)
2009-11-24_05:06:57.62647       at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:85)
2009-11-24_05:06:57.62647       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
2009-11-24_05:06:57.62647       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-24_05:06:57.62647       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-24_05:06:57.62647       at java.lang.Thread.run(Thread.java:636)
2009-11-24_05:06:57.62647 ERROR - Fatal exception in thread Thread[ROW-READ-STAGE:3,5,main]
2009-11-24_05:06:57.62647 java.lang.NullPointerException
2009-11-24_05:06:57.62647       at org.apache.cassandra.db.Row.digest(Row.java:75)
2009-11-24_05:06:57.62647       at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:85)
2009-11-24_05:06:57.62647       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
2009-11-24_05:06:57.62647       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-24_05:06:57.62647       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-24_05:06:57.62647       at java.lang.Thread.run(Thread.java:636)
",24/Nov/09 05:16;jbellis;updated patch 03,"24/Nov/09 06:01;dispalt;

2009-11-24_06:00:42.63108 ERROR - Internal error processing get_slice
2009-11-24_06:00:42.63108 java.lang.NullPointerException
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:130)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:453)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758)
2009-11-24_06:00:42.63108       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)
2009-11-24_06:00:42.63108       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
2009-11-24_06:00:42.63108       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-11-24_06:00:42.63108       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-11-24_06:00:42.63108       at java.lang.Thread.run(Thread.java:636)
",24/Nov/09 13:45;jbellis;updated 03 again,"25/Nov/09 07:01;dispalt;I now seem to be getting tihs.


2009-11-25_06:42:26.20349 java.lang.NullPointerException                                                                                                                                
2009-11-25_06:42:26.20349       at org.apache.cassandra.db.ColumnFamily.delete(ColumnFamily.java:252)                                                                                   
2009-11-25_06:42:26.20349       at org.apache.cassandra.db.ColumnFamily.resolve(ColumnFamily.java:402)                                                                                  
2009-11-25_06:42:26.20349       at org.apache.cassandra.db.Row.resolve(Row.java:62)                                                                                                     
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:122)                                                             
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:46)                                                              
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:88)                                                                
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:477)                                                                          
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:381)                                                                        
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)                                                               
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:170)                                                                      
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.CassandraServer.multigetSliceInternal(CassandraServer.java:245)                                                         
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:208)                                                                     
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:758)                                                               
2009-11-25_06:42:26.20349       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)                                                                         
2009-11-25_06:42:26.20349       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)                                                             
2009-11-25_06:42:26.20349       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)                                                                      
2009-11-25_06:42:26.20349       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)                                                                      
2009-11-25_06:42:26.20349       at java.lang.Thread.run(Thread.java:636)


And I am not sure if this is related but it seems new too:

java.lang.RuntimeException: Unable to load comparator class ''.  probably this means you have obsolete sstables lying around                                  
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:136)                                                       
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:107)                                                          
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:110)                                                                                      
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:122)                                                                    
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:98)                                                                     
2009-11-25_06:55:56.50945       at org.apache.cassandra.service.ReadResponseResolver.isDataPresent(ReadResponseResolver.java:154)                                                       
2009-11-25_06:55:56.50945       at org.apache.cassandra.service.QuorumResponseHandler.response(QuorumResponseHandler.java:97)                                                           
2009-11-25_06:55:56.50945       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:37)                                                                     
2009-11-25_06:55:56.50945       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)                                                                        
2009-11-25_06:55:56.50945       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)                                                                      
2009-11-25_06:55:56.50945       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)                                                                      
2009-11-25_06:55:56.50945       at java.lang.Thread.run(Thread.java:636)                                                                                                                
2009-11-25_06:55:56.50945 Caused by: java.lang.ClassNotFoundException:                                                                                                                  
2009-11-25_06:55:56.50945       at java.lang.Class.forName0(Native Method)                                                                                                              
2009-11-25_06:55:56.50945       at java.lang.Class.forName(Class.java:186)                                                                                                              
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:132)                                                       
2009-11-25_06:55:56.50945       ... 11 more                                                                                                                                             
2009-11-25_06:55:56.50945 ERROR - Fatal exception in thread Thread[RESPONSE-STAGE:2,5,main]                                                                                             
2009-11-25_06:55:56.50945 java.lang.RuntimeException: Unable to load comparator class ''.  probably this means you have obsolete sstables lying around                                  
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:136)                                                       
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:107)                                                          
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:110)                                                                                      
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:122)                                                                    
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:98)                                                                     
2009-11-25_06:55:56.50945       at org.apache.cassandra.service.ReadResponseResolver.isDataPresent(ReadResponseResolver.java:154)                                                       
2009-11-25_06:55:56.50945       at org.apache.cassandra.service.QuorumResponseHandler.response(QuorumResponseHandler.java:97)                                                           
2009-11-25_06:55:56.50945       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:37)                                                                     
2009-11-25_06:55:56.50945       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)                                                                        
2009-11-25_06:55:56.50945       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)                                                                      
2009-11-25_06:55:56.50945       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)                                                                      
2009-11-25_06:55:56.50945       at java.lang.Thread.run(Thread.java:636)                                                                                                                
2009-11-25_06:55:56.50945 Caused by: java.lang.ClassNotFoundException:                                                                                                                  
2009-11-25_06:55:56.50945       at java.lang.Class.forName0(Native Method)                                                                                                              
2009-11-25_06:55:56.50945       at java.lang.Class.forName(Class.java:186)                                                                                                              
2009-11-25_06:55:56.50945       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:132)                                                       
2009-11-25_06:55:56.50945       ... 11 more                ","25/Nov/09 17:04;dispalt;It seems for the comparator problem the """" is getting read as \u0000.    But then if I make that change I start getting:

2009-11-25_17:00:45.31534 INFO - Exception was generated at : 11/25/2009 17:00:45 on thread RESPONSE-STAGE:2                     
2009-11-25_17:00:45.31534                                                                                                        
2009-11-25_17:00:45.31534 java.io.EOFException                                                                                   
2009-11-25_17:00:45.31534       at java.io.DataInputStream.readFully(DataInputStream.java:197)                                   
2009-11-25_17:00:45.31534       at java.io.DataInputStream.readUTF(DataInputStream.java:609)                                     
2009-11-25_17:00:45.31534       at java.io.DataInputStream.readUTF(DataInputStream.java:564)                                     
2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ColumnFamilySerializer.readComparator(ColumnFamilySerializer.java:124)
2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:107)   
2009-11-25_17:00:45.31534       at org.apache.cassandra.db.RowSerializer.deserialize(Row.java:110)                               
2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:122)             
2009-11-25_17:00:45.31534       at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:98)              
2009-11-25_17:00:45.31534       at org.apache.cassandra.service.ReadResponseResolver.isDataPresent(ReadResponseResolver.java:154)
2009-11-25_17:00:45.31534       at org.apache.cassandra.service.QuorumResponseHandler.response(QuorumResponseHandler.java:97)    
2009-11-25_17:00:45.31534       at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:37)              
2009-11-25_17:00:45.31534       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)                 
2009-11-25_17:00:45.31534       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)               
2009-11-25_17:00:45.31534       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)               
2009-11-25_17:00:45.31534       at java.lang.Thread.run(Thread.java:636)","25/Nov/09 17:09;jbellis;the comparator problem is because we are changing Row internals which affects the commit log.  the simplest is to just wipe out the commitlogs, otherwise, go back to the old version and do a nodeprobe flush to clear them out before patching

working on a fix for the other NPE now",25/Nov/09 19:35;jbellis;updated patch 03 with fix for latest NPE,25/Nov/09 21:37;dispalt;I think that did it!  Thank you so much!,25/Nov/09 23:16;jbellis;committed,"26/Nov/09 12:34;hudson;Integrated in Cassandra #269 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/269/])
    allow serializing null CF; add get_range_slice test exercising this
patch by jbellis; tested by Dan Di Spaltro for 
make Row contain a single, final CF reference
patch by jbellis; tested by Dan Di Spaltro for 
r/m unused Row code, and move table variable into callers rather than serializing it redundantly
patch by jbellis; tested by Dan Di Spaltro for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
r/m `Leader` from the web interface,CASSANDRA-143,12424739,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/6/2009 18:30,3/12/2019 14:12,3/13/2019 22:24,5/6/2009 22:40,0.3,,,,0,,,,,,this confuses many people when they see nobody is Leader.  in reality it is an unused leftover from an earlier time.,,,,,,,,,,,,,,,,,,,,06/May/09 22:08;jbellis;143.patch;https://issues.apache.org/jira/secure/attachment/12407402/143.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:16.3,,,no_permission,,,,,,,,,,,,19564,,,Thu May 07 13:35:06 UTC 2009,,,,,,0|i0fx4v:,90966,,,,,,,,,,,06/May/09 22:27;urandom;Yep. Looks good. +1,06/May/09 22:40;jbellis;committed,"07/May/09 13:35;hudson;Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])
    r/m orphaned Leader code.  patch by jbellis; reviewed by Eric Evans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
regression in CF.digest causes NPE when CF contains no columns,CASSANDRA-149,12424824,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/7/2009 15:58,3/12/2019 14:12,3/13/2019 22:24,5/7/2009 16:18,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,07/May/09 15:59;jbellis;149.patch;https://issues.apache.org/jira/secure/attachment/12407545/149.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,15:58.9,,,no_permission,,,,,,,,,,,,19568,,,Thu May 07 18:44:38 UTC 2009,,,,,,0|i0fx67:,90972,,,,,,,,,,,07/May/09 16:15;urandom;+1,07/May/09 16:18;jbellis;committed,"07/May/09 18:44;hudson;Integrated in Cassandra #67 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/67/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Thrift interface needs a  Ruby namespace,CASSANDRA-327,12431829,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,eweaver,eweaver,eweaver,7/30/2009 17:38,3/12/2019 14:12,3/13/2019 22:24,8/10/2009 2:49,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,09/Aug/09 16:09;eweaver;CASSANDRA-327-2.diff;https://issues.apache.org/jira/secure/attachment/12415998/CASSANDRA-327-2.diff,30/Jul/09 17:40;eweaver;CASSANDRA-327.diff;https://issues.apache.org/jira/secure/attachment/12415042/CASSANDRA-327.diff,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,59:42.4,,,no_permission,,,,,,,,,,,,19637,,,Mon Aug 10 14:35:53 UTC 2009,,,,,,0|i0fy9b:,91148,,,,,,,,,,,05/Aug/09 22:59;beisenberg;works for me,"05/Aug/09 23:42;jbellis;Is the bug mentioned in this diff reported to the thrift project?

+1 after linking the thrift bug report.","06/Aug/09 00:21;nzkoz;looks good to me, definitely annoying how it's structured at present",06/Aug/09 21:01;schacon;works for me,09/Aug/09 16:09;eweaver;Generate patch with git.,09/Aug/09 16:14;eweaver;http://issues.apache.org/jira/browse/THRIFT-556,10/Aug/09 02:49;jbellis;committed,"10/Aug/09 14:35;hudson;Integrated in Cassandra #163 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/163/])
    add Ruby namespace. patch by Evan Weaver; reviewed by Michael Koziarski and Scott Chacon for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrap broken in 0.4.1,CASSANDRA-501,12438644,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,10/20/2009 23:59,3/12/2019 14:12,3/13/2019 22:24,11/3/2009 20:04,0.4,,,,0,,,,,,Bootstrap fails with NPE in 0.4.1 when you start a node with the -b option,all,,,,,,,,,,,,,,,,,,,21/Oct/09 00:03;sandeep_tata;bootstrapfix-v1.patch;https://issues.apache.org/jira/secure/attachment/12422749/bootstrapfix-v1.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,59:48.1,,,no_permission,,,,,,,,,,,,19725,,,Tue Nov 03 20:04:32 UTC 2009,,,,,,0|i0fzbj:,91320,,,,,,,,,,,"21/Oct/09 00:03;sandeep_tata;1. Fixes null token getting passed to Bootstapper
2. Correctly includes new nodes in src-target calculation (changes in tokenMetada for echoing writes probably broke this)
3. Fixed unit test","21/Oct/09 00:04;sandeep_tata;Applicable for 0.4.1, not useful for trunk.

trunk runs:

for (int i=0; i< targets_.length; i++)
{
                tokenMetadata_.update(tokens_[i], targets_[i], false);
}

before calling getRangesWithSourceTarget(), so it won't have this problem. We should probably fix the unit test so we catch this in the future.

","21/Oct/09 15:59;jbellis;I don't follow the reasoning behind cloneTokenEndPointMapIncludingBootstrapNodes.  If we include bootstrap-in-progress nodes in the source ranges, we could try to bootstrap from nodes that don't have all the data yet.  Better to bootstrap only from the original nodes; if there is overlap w/ existing bootstrapers, then we have some inefficiency, but that is better than not getting all the data.","03/Nov/09 03:21;jbellis;Sandeep, how is testing looking on this?","03/Nov/09 18:36;sandeep_tata;My 1->2 node tests were fine, I'm waiting to hear from the other team trying this. 
If you want to go ahead and commit this for 0.4.2, that's fine. I'll be able to run a bunch of comprehensive tests after Friday, and submit other fixes if needed.","03/Nov/09 20:04;jbellis;ok, committed this to 0.4 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Antlr checks broken,CASSANDRA-125,12424321,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,johanoskarsson,johanoskarsson,4/30/2009 17:01,3/12/2019 14:12,3/13/2019 22:24,4/30/2009 17:05,0.3,,,,0,,,,,,It seems CASSANDRA-105 broke the antlr checks to see if it needs to run or not,,,,,,,,,,,,,,,,,,,,30/Apr/09 17:02;johanoskarsson;CASSANDRA-125.patch;https://issues.apache.org/jira/secure/attachment/12406931/CASSANDRA-125.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,05:03.8,,,no_permission,,,,,,,,,,,,19558,,,Fri May 01 13:09:51 UTC 2009,,,,,,0|i0fx13:,90949,,,,,,,,,,,30/Apr/09 17:02;johanoskarsson;This corrects the reference to the generated code.,30/Apr/09 17:05;jbellis;committed,"01/May/09 13:09;hudson;Integrated in Cassandra #55 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/55/])
    fix antlr check.  patch by johano; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make jmx count/latency stuff actually useful,CASSANDRA-144,12424751,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/6/2009 20:20,3/12/2019 14:12,3/13/2019 22:24,5/6/2009 22:27,,,,,0,,,,,,"i just track those since ""the beginning of time"" it is not very useful for monitoring.  
(say things are fine for days, then something breaks -- you have to overcome the days of fine-ness in the avg before you can see anything wrong.)

option 1: reset the counters after each request for data.  problem: then i can't have multiple collectors (which i don't need atm but might be nice to have).
option 2: keep the last N data points and compute an average on demand, but that's more complicated than just doing a += for each op.

going to go with option 2.",,,,,,,,,,,,,,,,,,,,06/May/09 21:36;jbellis;144.patch;https://issues.apache.org/jira/secure/attachment/12407396/144.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,20:00.0,,,no_permission,,,,,,,,,,,,19565,,,Thu May 07 13:35:06 UTC 2009,,,,,,0|i0fx53:,90967,,,,,,,,,,,"06/May/09 22:20;urandom;The patch as attached causes test failures, but as you pointed out the on IRC, the following fixes that.

--- a/src/java/org/apache/cassandra/utils/TimedStatsDeque.java
+++ b/src/java/org/apache/cassandra/utils/TimedStatsDeque.java
@@ -19,7 +19,7 @@ public class TimedStatsDeque extends AbstractStatsDeque
     private void purge()
     {
         long now = System.currentTimeMillis();
-        while (deque.peek().timestamp < now - period)
+        while (!deque.isEmpty() && deque.peek().timestamp < now - period)
         {
             deque.remove();
         }

+1",06/May/09 22:27;jbellis;committed w/ above diff,"07/May/09 13:35;hudson;Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])
    clean up more ad-hoc timing message and move to mbeans.  add TimedStatsDeque to
simplify tracking load stats over a one-minute window.
patch by jbellis; reviewed by Eric Evans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
trivial bug in Marshal comparators,CASSANDRA-319,12431533,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,7/27/2009 20:04,3/12/2019 14:12,3/13/2019 22:24,7/28/2009 20:50,,,,,0,,,,,,"comparing X to [] where X is not [] should be 1, not -1
",,,,,,,,,,,,,,,,,,,,27/Jul/09 20:08;jbellis;319.patch;https://issues.apache.org/jira/secure/attachment/12414655/319.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,13:09.8,,,no_permission,,,,,,,,,,,,19634,,,Tue Jul 28 20:50:06 UTC 2009,,,,,,0|i0fy7j:,91140,,,,,,,,,,,"27/Jul/09 20:08;jbellis;this patch also:

removes special-casing of [] in UTF8, since [] already deserializes to """"

removes TODOs hoping to r/m special-casing []; we'll always need that to support slice/finish sentinel values in get_slice","27/Jul/09 20:13;stuhood;Assuming that it passes the rest of the unit tests, this patch looks alright. But rather than removing the TODOs, I would turn them into comments, since they document confusing behaviour.","27/Jul/09 20:17;jbellis;committed with comment to AbstractType:

 * Note that empty byte[] are used to represent ""start at the beginning""
 * or ""stop at the end"" arguments to get_slice, so the Comparator
 * should always handle those values even if they normally do not
 * represent a valid byte[] for the type being compared.
",28/Jul/09 20:50;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
race condition prevents startup under Xen vm,CASSANDRA-97,12423605,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,4/23/2009 17:47,3/12/2019 14:12,3/13/2019 22:24,4/29/2009 2:06,0.3,,,,0,,,,,,Reported by Soo Hwan Park on the mailing list.,,,,,,,,,,,,,,,,,,,,24/Apr/09 03:21;jbellis;0001-r-m-unused-code-do-minor-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12406324/0001-r-m-unused-code-do-minor-cleanup.patch,24/Apr/09 03:21;jbellis;0002-r-m-unnecessary-race-prone-cancelled-read-write-ke.patch;https://issues.apache.org/jira/secure/attachment/12406325/0002-r-m-unnecessary-race-prone-cancelled-read-write-ke.patch,24/Apr/09 03:22;jbellis;0003-renaming-and-final-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12406326/0003-renaming-and-final-cleanup.patch,28/Apr/09 21:13;jbellis;0004.patch.2;https://issues.apache.org/jira/secure/attachment/12406694/0004.patch.2,24/Apr/09 03:33;jbellis;cassandra_race_condition_in_xen.patch;https://issues.apache.org/jira/secure/attachment/12406329/cassandra_race_condition_in_xen.patch,,,,,,,,,5,,,,,,,,,,,,,,,,,,,43:53.5,,,no_permission,,,,,,,,,,,,19548,,,Wed Apr 29 02:06:41 UTC 2009,,,,,,0|i0fwuv:,90921,,,,,,,,,,,"24/Apr/09 03:35;jbellis;The race condition patch is Soo's original one.

But rather than proliferate hacks on top of hacks I think it's best to remove all the unnecessary scaffolding around the Selector entirely.

03
    renaming and final cleanup

02
    r/m unnecessary & race-prone cancelled/read/write key sets (a main reason to use
    select is that select does this for you!)

01
    r/m unused code & do minor cleanup
",27/Apr/09 19:08;jbellis;here is the thread where the bug was reported: http://groups.google.com/group/cassandra-user/browse_thread/thread/1617bb8fd6e80041/bb3fa9cf12535162,"27/Apr/09 22:43;urandom;First off, I think these patches (000[1-3]) look good. I don't see a reason for the key state collections that currently exist; IMO this is how the code should look, +1 for applying it.

However, I still believe there is a race condition here. Or rather, I am experiencing what seems to be a race condition between the select() call in SelectorManager.run and the invocation of SelectorManager.register() from the main thread.

Is no one else seeing this?


","28/Apr/09 21:10;jbellis;committed patches 1-3.

added patch 4: start SelectorManager after all the selector registration is done, to workaround a bug in some linux environments.  (Debian etch under vmware player or under EC2 seem to exhibit this problem consistently.)  the bug symptom is, the selector register hangs indefinitely even though the select() call is only for 100ms (after which it should stop blocking and let the register through).","28/Apr/09 21:12;jbellis;oops, needed to rebase post-113.","28/Apr/09 21:41;urandom;+1 for 0004.patch.2, that seems to do the trick.",29/Apr/09 02:06;jbellis;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileUtils.getUsedDiskSpace is inaccurate,CASSANDRA-394,12434112,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sammy.yu,sammy.yu,sammy.yu,8/27/2009 1:27,3/12/2019 14:12,3/13/2019 22:24,8/27/2009 17:23,0.4,,,,0,,,,,,"The disk space calculation calculates the disk space used by the data directories.  However it only looks at the first level.  This information is propagated by gossip and displayed in nodeprobe.
 ",,,,,,,,,,,,,,,,,,,,27/Aug/09 01:33;sammy.yu;0001-CASSANDRA-394-Calculate-disk-space-utilization-for-d.patch;https://issues.apache.org/jira/secure/attachment/12417838/0001-CASSANDRA-394-Calculate-disk-space-utilization-for-d.patch,27/Aug/09 16:24;sammy.yu;0002-CASSANDRA-394-Calculate-disk-space-utilization-for-d.patch;https://issues.apache.org/jira/secure/attachment/12417894/0002-CASSANDRA-394-Calculate-disk-space-utilization-for-d.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,02:51.0,,,no_permission,,,,,,,,,,,,19668,,,Fri Aug 28 15:08:27 UTC 2009,,,,,,0|i0fynz:,91214,,,,,,,,,,,27/Aug/09 15:02;jbellis;doesn't BinaryMemtableLoader belong over in CASSANDRA-337?,"27/Aug/09 16:24;sammy.yu;oops, new patch without BMT",27/Aug/09 17:23;jbellis;committed,"28/Aug/09 15:08;hudson;Integrated in Cassandra #180 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/180/])
    fix getUsedDiskSpace to look at data directories recursively.  Patch by Sammy Yu; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
forceFlush skips flush when there are pending operations,CASSANDRA-141,12424716,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/6/2009 15:33,3/12/2019 14:12,3/13/2019 22:24,5/19/2009 3:16,0.4,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,06/May/09 22:39;jbellis;141-2.patch;https://issues.apache.org/jira/secure/attachment/12407409/141-2.patch,06/May/09 15:34;jbellis;141.patch;https://issues.apache.org/jira/secure/attachment/12407364/141.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,48:30.5,,,no_permission,,,,,,,,,,,,19563,,,Thu May 07 14:55:33 UTC 2009,,,,,,0|i08c2n:,46555,,,,,,,,,,,06/May/09 16:48;urandom;+1,06/May/09 17:04;jbellis;committed,06/May/09 22:26;jbellis;still happens (less frequently),06/May/09 22:39;jbellis;kill race condition DEAD,"07/May/09 13:35;hudson;Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])
    force flush when there are pending ops on the memtable executor even when none have finished yet.  this fixes test case intermittent failures.  patch by jbellis; reviewed by Eric Evans for 
",07/May/09 14:55;urandom;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race in ChecksumManager.instance(),CASSANDRA-230,12427844,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,stuhood,stuhood,6/14/2009 21:59,3/12/2019 14:12,3/13/2019 22:24,6/15/2009 16:20,0.4,,,,0,,,,,,There is a minor race condition in ChecksumManager.instance(). Patch attached.,,,,,,,,,,,,,,,,,,,,15/Jun/09 14:14;jbellis;230.patch;https://issues.apache.org/jira/secure/attachment/12410675/230.patch,14/Jun/09 22:07;stuhood;checksum-manager-race.patch;https://issues.apache.org/jira/secure/attachment/12410606/checksum-manager-race.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,14:32.3,,,no_permission,,,,,,,,,,,,19603,,,Tue Jun 16 12:34:38 UTC 2009,,,,,,0|i0fxnz:,91052,,,,,,,,,,,15/Jun/09 14:14;jbellis;r/m CM and CRAF,"15/Jun/09 14:57;jbellis;I think removing ChecksumManager and ChecksumRandomAccessFile is a better fix, because
 1. they are unused
 2. CRAF is a copy/paste modification of BufferedRAF
 3. checksum-per-block is the wrong approach; we want checksum-per-column (and per-CF-index) instead so we can checksum reads easily",15/Jun/09 16:08;stuhood;Agreed. I didn't realize that code was unused.,15/Jun/09 16:20;jbellis;committed 230.patch,"16/Jun/09 12:34;hudson;Integrated in Cassandra #110 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/110/])
    r/m CM and CRAF.  patch by jbellis; reviewed by Stu Hood for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Thrift interface uses reserved keyword ""end""",CASSANDRA-247,12428493,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stephenjudkins,stephenjudkins,stephenjudkins,6/20/2009 22:53,3/12/2019 14:12,3/13/2019 22:24,6/22/2009 14:48,0.4,,,,0,,,,,,"The definition for get_slice_by_name_range has an argument named ""end"".

According to https://issues.apache.org/jira/browse/THRIFT-434, this will soon become an illegal keyword in Thrift definitions due to its use as a reserved keyword in Ruby.

Currently, attempting to use the Ruby interface results in syntax errors unless the interface definition is changed prior to generation.",,,,,,,,,,,,,,,,,,,,20/Jun/09 22:58;stephenjudkins;CASSANDRA-247.patch;https://issues.apache.org/jira/secure/attachment/12411324/CASSANDRA-247.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,48:18.8,,,no_permission,,,,,,,,,,,,19608,,,Tue Jun 23 12:34:55 UTC 2009,,,,,,0|i0fxrr:,91069,,,,,,,,,,,"20/Jun/09 22:58;stephenjudkins;This patch changes argument for get_slice_by_name_range from ""end"" to ""finish"" in interface definition.

I'm fairly certain this won't break much of anything? Correct me if I'm wrong.","22/Jun/09 14:48;jbellis;Committed, thanks!

(Accidentally attributed patch to Stu Hood in the commit message.  Sorry, must be Monday. :)","23/Jun/09 12:34;hudson;Integrated in Cassandra #117 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/117/])
    rename ""end"" parameter to ""finish"" to avoid conflict w/ ruby keyword.  patch by Stu Hood; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The initial arrive time should not be set to zero,CASSANDRA-14,12419528,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,zhu han,zhu han,3/26/2009 15:02,3/12/2019 14:12,3/13/2019 22:24,4/22/2009 18:40,0.3,,,,0,,,,,,"In line 253 of src.org.apache.cassandra.gms .FailureDetector, the initial arrive time should not be zero. Otherwise, the failure detector would report the new joining node as dead because the phi is positive infinity when the mean of arrive time is zero. 

A proper value should be set as the initial value. I use half of the gossip period as the initial value,  whose default configuration is 1000ms. ",,,1800,1800,,0%,1800,1800,,,,,,,,,,,,22/Apr/09 16:16;jbellis;14-part-2.patch;https://issues.apache.org/jira/secure/attachment/12406148/14-part-2.patch,22/Apr/09 16:09;jbellis;14.patch;https://issues.apache.org/jira/secure/attachment/12406147/14.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,44:16.5,,,no_permission,,,,,,,,,,,,19517,,,Wed Apr 22 18:40:56 UTC 2009,,,,,,0|i0fwdb:,90842,,,,,,,,,,,10/Apr/09 14:44;jbellis;I have seen this cause problems -- when I restart all the nodes in my test cluster at close to the same time sometimes they will partition even though there is nothing wrong with the network.,10/Apr/09 15:30;avinash.lakshman@gmail.com;Actually this will happen for the initial start maybe for a few seconds. But it will stabilize very very soon. In any system in this nature one could think of this as a barrier synchronization point where one waits for a few seconds for things to stabilize. It takes about less than 10 secs in a 100 node cluster for all this to kinda settle down. In my opinion a non-issue.,"10/Apr/09 16:12;jbellis;I'm talking minutes, not seconds.

Is it possible that more nodes ameliorates any problems?","10/Apr/09 17:26;avinash.lakshman@gmail.com;Hmm. That is weird. Because I have small, 5 node test cluster, and it definitely doesn't take minutes. Maybe the recovery is taking time. Nodes do not join the cluster untill recovery is complete.",22/Apr/09 16:09;jbellis;avoid setting the initial arrival time (and hence the mean arrival time until the next value) to zero,22/Apr/09 17:00;jbellis;part 2 cleans up FD by using a Deque instead of a List.,"22/Apr/09 18:00;junrao;comment for this patch: arrivalIntervals_.remove(0); in FailureDetector should be arrivalIntervals_.remove();

The patch looks fine to me otherwise.
","22/Apr/09 18:36;jbellis;ooh, very right.  thanks.",22/Apr/09 18:40;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table.java has mixed line endings.,CASSANDRA-333,12431932,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jmhodges,jmhodges,jmhodges,7/31/2009 20:37,3/12/2019 14:12,3/13/2019 22:24,7/31/2009 20:43,,,,,0,,,,,,Looks like r799684 added some \r\n's to Table.java.,,,,,,,,,,,,,,,,,,,,31/Jul/09 20:38;jmhodges;table_line_endings.patch;https://issues.apache.org/jira/secure/attachment/12415178/table_line_endings.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,43:00.5,,,no_permission,,,,,,,,,,,,19638,,,Sat Aug 01 12:34:01 UTC 2009,,,,,,0|i0fyan:,91154,,,,,,,,,,,31/Jul/09 20:38;jmhodges;Patch to convert all of Table.java to unix line endings.,31/Jul/09 20:43;jbellis;committed,"01/Aug/09 12:34;hudson;Integrated in Cassandra #154 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/154/])
    unmix line endings in Table.java.  patch by Jeff Hodges; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming fails on windows because file is not closed before it is renamed,CASSANDRA-630,12443151,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,12/11/2009 22:04,3/12/2019 14:12,3/13/2019 22:24,12/11/2009 22:15,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,11/Dec/09 22:07;jbellis;630.patch;https://issues.apache.org/jira/secure/attachment/12427781/630.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,13:02.0,,,no_permission,,,,,,,,,,,,19789,,,Fri Dec 11 22:15:09 UTC 2009,,,,,,0|i0g047:,91449,,,,,,,,,,,"11/Dec/09 22:07;jbellis;check rename return code, and close before renaming",11/Dec/09 22:13;stuhood;+1 Looks good.,11/Dec/09 22:15;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mapreduce support is broken,CASSANDRA-1700,12478945,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,jeromatron,jeromatron,11/3/2010 1:16,3/12/2019 14:12,3/13/2019 22:24,11/16/2010 15:16,0.6.9,0.7.0 rc 1,,,0,,,,,,Running from a vanilla download of beta3 src.  Tried the word count example and it's broken.  Attaching the stack trace.,,,,,,,,,,,,,,,,,,,,03/Nov/10 05:08;stuhood;0001-Add-testcase-for-wrapping-range-on-member-token.patch;https://issues.apache.org/jira/secure/attachment/12458713/0001-Add-testcase-for-wrapping-range-on-member-token.patch,03/Nov/10 03:36;jeromatron;server_log.txt;https://issues.apache.org/jira/secure/attachment/12458706/server_log.txt,03/Nov/10 01:16;jeromatron;stacktrace.txt;https://issues.apache.org/jira/secure/attachment/12458699/stacktrace.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,23:50.8,,,no_permission,,,,,,,,,,,,20265,,,Tue Nov 16 16:05:59 UTC 2010,,,,,,0|i0g6un:,92540,jbellis,jbellis,,,,,,,,,03/Nov/10 03:23;jbellis;The server stacktrace would be more useful than the client one.,03/Nov/10 03:36;jeromatron;Attaching server-side errors.,"03/Nov/10 03:55;jbellis;possibly caused by CASSANDRA-1442 which happened during beta3.

looks like the assert
{code}
        assert range instanceof Bounds
               || (!((Range)range).isWrapAround() || range.right.equals(StorageService.getPartitioner().getMinimumToken()))
               : range;
{code}

is failing.  it may no longer be a valid assert.",03/Nov/10 05:08;stuhood;Somehow in all those testcases on 1442 I didn't test the one case that Hadoop uses: a wrapping range centered on a member token. Attaching a patch to add a testcase and fix.,"03/Nov/10 14:09;jbellis;committed, thanks!",16/Nov/10 15:05;jbellis;also broken on 0.6 since we backported https://issues.apache.org/jira/browse/CASSANDRA-1722,16/Nov/10 15:16;jbellis;backported fix in r1035656,"16/Nov/10 16:05;hudson;Integrated in Cassandra-0.6 #5 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/5/])
    backport CASSANDRA-1700
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
antrl generated files should not be included in the source release,CASSANDRA-316,12431459,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,7/26/2009 15:33,3/12/2019 14:12,3/13/2019 22:24,11/9/2009 21:19,0.4,0.5,Legacy/Tools,,0,,,,,,Files located under src/gen-java should not be included in the source distribution since they are removed on clean and generated on build.,,,,,,,,,,,,,,,,,,,,09/Nov/09 21:01;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-316-don-t-ship-antrl-generated-files-in-sour.txt;https://issues.apache.org/jira/secure/attachment/12424396/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-316-don-t-ship-antrl-generated-files-in-sour.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,53:24.2,,,no_permission,,,,,,,,,,,,19632,,,Mon Nov 09 21:19:12 UTC 2009,,,,,,0|i0fy6v:,91137,,,,,,,,,,,09/Nov/09 18:53;jbellis;is this still a problem?,09/Nov/09 19:49;urandom;Yes. :(,09/Nov/09 19:53;jbellis;would it be easier to just use svn export to get a clean tree for the source release?,"09/Nov/09 20:53;urandom;If I understand what you're asking, then probably not in the long run (because you'd be adding an extra (manual )step), as opposed to having it all rolled up into ""ant release"".",09/Nov/09 21:03;urandom;For some reason I remembered it being more complicated than this.,09/Nov/09 21:03;jbellis;http://subclipse.tigris.org/svnant.html maybe? :),09/Nov/09 21:12;jbellis;+1 for patch,09/Nov/09 21:19;urandom;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
anticompaction return value is broken,CASSANDRA-431,12435132,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,9/8/2009 17:14,3/12/2019 14:12,3/13/2019 22:24,9/14/2009 18:05,0.5,,,,0,,,,,,"it ""returns"" a list of filenames generated by the anticompact.  better to return sstablereader objects -- currently cleanup() doesn't deal with the list correctly",,,,,,,,,,,,,,,,,,,,10/Sep/09 17:33;jbellis;431.patch;https://issues.apache.org/jira/secure/attachment/12419197/431.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,02:50.3,,,no_permission,,,,,,,,,,,,19687,,,Tue Sep 15 14:01:16 UTC 2009,,,,,,0|i0fyvz:,91250,,,,,,,,,,,14/Sep/09 18:02;lenn0x;+1,14/Sep/09 18:05;jbellis;committed,"15/Sep/09 14:01;hudson;Integrated in Cassandra #198 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/198/])
    make anticompaction return a list of the split-out sstables, instead of a mostly-useless boolean
patch by jbellis; reviewed by goffinet for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counter AES performance issue,CASSANDRA-2235,12499563,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,2/24/2011 8:08,3/12/2019 14:12,3/13/2019 22:24,3/16/2011 15:12,0.8 beta 1,,,,0,,,,,,"We noticed tonight when trying out AES for Counters in trunk, there is a serious performance issue when inlining the SSTables. We found that the way we are seeking in the file, BRAF keeps flushing out its buffer of 8MB, and we call dfile.sync() on every row. We are finalizing a patch to write a new SSTable on rebuild, instead of inlining. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,47:36.7,,,no_permission,,,,,,,,,,,,20518,,,Wed Mar 16 15:12:38 UTC 2011,,,,,,0|i0ga5r:,93076,,,,,,,,,,,"24/Feb/11 14:47;jbellis;bq. We found that the way we are seeking in the file, BRAF keeps flushing out its buffer of 8MB, and we call dfile.sync() on every row

fixed in CASSANDRA-2218",16/Mar/11 15:12;slebresne;Marking resolved since either CASSANDRA-2218 or CASSANDRA-2288 fixed this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unhelpful exception when failing to set keyspace,CASSANDRA-2251,12499749,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,2/25/2011 19:27,3/12/2019 14:12,3/13/2019 22:24,2/25/2011 22:06,,,,,0,,,,,,"If you fail to set the keyspace, {{ThriftValidation.validateColumnFamily()}} raises an {{AssertionError}}, which remotely results in a {{TApplicationException}}. ",,,0,0,,0%,0,0,,,,,,,,,,,,25/Feb/11 21:35;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2251-raise-IRE-for-null-keyspace.txt;https://issues.apache.org/jira/secure/attachment/12471976/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2251-raise-IRE-for-null-keyspace.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,59:10.3,,,no_permission,,,,,,,,,,,,20525,,,Fri Feb 25 22:53:30 UTC 2011,,,,,,0|i0ga9b:,93092,,,,,,,,,,,25/Feb/11 19:59;jbellis;maybe we should move the check into getKeyspace?  there's a bunch of callers that don't call vCF afterwards.,"25/Feb/11 21:36;urandom;bq. maybe we should move the check into getKeyspace? there's a bunch of callers that don't call vCF afterwards.

That'll work too.  New patch attached.",25/Feb/11 21:42;jbellis;+1,25/Feb/11 22:06;urandom;committed.,"25/Feb/11 22:53;hudson;Integrated in Cassandra #745 (See [https://hudson.apache.org/hudson/job/Cassandra/745/])
    raise IRE for null keyspace

Patch by eevans; reviewed by jbellis for CASSANDRA-2251
raise IRE for null keyspace argument

Patch by eevans; reviewed by jbellis for CASSANDRA-2251
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
turn on log4j output during tests,CASSANDRA-64,12422263,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,4/7/2009 16:38,3/12/2019 14:12,3/13/2019 22:24,4/7/2009 19:19,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,07/Apr/09 18:58;jbellis;CASSANDRA-64-v2.patch;https://issues.apache.org/jira/secure/attachment/12404870/CASSANDRA-64-v2.patch,07/Apr/09 16:51;jbellis;CASSANDRA-64.patch;https://issues.apache.org/jira/secure/attachment/12404847/CASSANDRA-64.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,08:37.0,,,no_permission,,,,,,,,,,,,19531,,,Tue Apr 07 19:19:52 UTC 2009,,,,,,0|i0fwnj:,90888,,,,,,,,,,,07/Apr/09 16:51;jbellis;add log4j.properties for testing.  output is to build/test/logs/system.log,07/Apr/09 18:58;jbellis;rebase to current,"07/Apr/09 19:08;junrao;patch looks fine. Ran ant test cleanly.
",07/Apr/09 19:19;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
regression prevents recognizing local reads,CASSANDRA-512,12438933,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/23/2009 15:26,3/12/2019 14:12,3/13/2019 22:24,10/23/2009 17:26,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,23/Oct/09 15:29;jbellis;512.patch;https://issues.apache.org/jira/secure/attachment/12423023/512.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,24:38.0,,,no_permission,,,,,,,,,,,,19730,,,Sat Oct 24 12:34:37 UTC 2009,,,,,,0|i0fzdz:,91331,,,,,,,,,,,23/Oct/09 17:24;brandon.williams;+1,23/Oct/09 17:26;jbellis;committed,"24/Oct/09 12:34;hudson;Integrated in Cassandra #237 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/237/])
    fix regression recognizing local reads from 828148.
patch by jbellis; reviewed by Brandon Williams for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add PendingTasks in nodeprobe,CASSANDRA-285,12429873,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sammy.yu,sammy.yu,sammy.yu,7/8/2009 22:55,3/12/2019 14:12,3/13/2019 22:24,7/9/2009 16:19,0.4,,Legacy/Tools,,0,,,,,,"Add the newly added PendingTasks metric into nodeprobe cfstat.
",,,,,,,,,,,CASSANDRA-173,,,CASSANDRA-253,,,,,,09/Jul/09 00:03;sammy.yu;0001-Added-getPendingTask-metric-to-nodeprobe-CASSANDRA.patch;https://issues.apache.org/jira/secure/attachment/12412936/0001-Added-getPendingTask-metric-to-nodeprobe-CASSANDRA.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:09.1,,,no_permission,,,,,,,,,,,,19620,,,Fri Jul 10 12:35:56 UTC 2009,,,,,,0|i0fxzz:,91106,,,,,,,,,,,08/Jul/09 22:58;sammy.yu;Ticket that added the new metric,09/Jul/09 00:21;phatduckk;+1 0001-Added-getPendingTask-metric-to-nodeprobe-CASSANDRA.patch,"09/Jul/09 16:19;urandom;committed, thanks!","10/Jul/09 12:35;hudson;Integrated in Cassandra #133 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/133/])
    add PendingTasks attribute to NodeProbe cfstats output

Patch by Sammy Yu; reviewed by Arin Sarkissian for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
migrate to junit,CASSANDRA-162,12425156,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/11/2009 23:55,3/12/2019 14:12,3/13/2019 22:24,5/12/2009 15:28,,,,,0,,,,,,junit has a fork option which will allow our tests to be 100% independent of each other instead of sorta kinda independent.,,,,,,,,,,,,,,,,,,,,11/May/09 23:56;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-162-migrate-to-JUnit-4.6.txt;https://issues.apache.org/jira/secure/attachment/12407831/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-162-migrate-to-JUnit-4.6.txt,11/May/09 23:56;jbellis;ASF.LICENSE.NOT.GRANTED--0002-extract-tests-that-could-conflict-with-each-other-to-a.txt;https://issues.apache.org/jira/secure/attachment/12407832/ASF.LICENSE.NOT.GRANTED--0002-extract-tests-that-could-conflict-with-each-other-to-a.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,13:44.2,,,no_permission,,,,,,,,,,,,19577,,,Wed May 13 09:26:38 UTC 2009,,,,,,0|i0fx93:,90985,,,,,,,,,,,12/May/09 01:18;jbellis;you'll want to put the junit4 jar in lib/ to run the tests w/ this applied.  (I'm using 4.6.),"12/May/09 10:13;johanoskarsson;I won't have time to review the patch this week unfortunately, but +1 on moving to junit4","12/May/09 15:04;sandeep_tata;I get failures for namesort, timesort, testcompaction. I'm getting these even on a clean checkout of trunk -- so this is unrelated to the move to junit.

RackUnawareStrategyTest fails with no runnable methods. (Everything in it seems to be commented out).

I'm +1 after fixing RackUnawareStrategyTest (taking a quick look at the code, the other failures seem unrelated to the junit move).


","12/May/09 15:18;sandeep_tata;BTW, the tests take approx 2x the time on my machine. 
I think that's an acceptable price to pay for independent tests.
","12/May/09 15:28;jbellis;yeah, agreed about the speed.

I created CASSANDRA-163 for RackUnawareStrategyTest which is also unrelated to the move -- it wasn't getting run by testng b/c it was in the wrong directory before.

committed the basic migration.","13/May/09 09:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    extract tests that could conflict with each other to a separate test class (= gets own jvm)
patch by jbellis; reviewed by Sandeep Tata for 
migrate to JUnit 4.6.  patch by jbellis; reviewed by Sandeep Tata for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not all column families are created,CASSANDRA-1038,12463375,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,brandon.williams,brandon.williams,4/29/2010 21:17,3/12/2019 14:12,3/13/2019 22:24,5/5/2010 15:35,0.7 beta 1,,,,0,,,,,,"It seems that not all column families will be created via system_add_keyspace in some cases.  To reproduce:

Run stress.py (with CASSANDRA-1033) inserts (I used 1M) against standard columns.  During this run, both Standard1 and Super1 will be created.

Run stress.py again, this time against super columns.  Due to CASSANDRA-1036 no errors will be visible to the client but can be observed in the log.

You can switch the order and stress supers first, in which case Standard1 will not exist.  If you call describe_keyspace on Keyspace1, it will show both CFs even though only one will work.
",,,,,,,,,,,,,,,,,,,,03/May/10 22:22;gdusbabek;0001-restrict-how-and-when-CFMetaData-objects-are-added-t.patch;https://issues.apache.org/jira/secure/attachment/12443512/0001-restrict-how-and-when-CFMetaData-objects-are-added-t.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,22:20.5,,,no_permission,,,,,,,,,,,,19968,,,Wed May 05 15:35:49 UTC 2010,,,,,,0|i0g2mf:,91855,,,,,,,,,,,"03/May/10 22:22;gdusbabek;The problem was that when the keyspace creation was reattempted, identical CFMetaData objects were created that obliterated the older, correct cfids.

I could have put a check in the private CFMetaData constructor that threw an exception, but decided I'd rather have explicit control over cfIdMap instead.",05/May/10 15:24;jbellis;+1,"05/May/10 15:35;gdusbabek;I forgot to include the ticket id in the commit msg, so hudson isn't going to adorn us with the svn rev.  It's r941349 for the curious.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SStableSliceIterator leaks FDs,CASSANDRA-1416,12472125,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,8/20/2010 20:33,3/12/2019 14:12,3/13/2019 22:24,8/20/2010 20:43,0.7 beta 2,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,20/Aug/10 20:34;jbellis;1416.txt;https://issues.apache.org/jira/secure/attachment/12452661/1416.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,39:36.1,,,no_permission,,,,,,,,,,,,20127,,,Sat Aug 21 11:14:31 UTC 2010,,,,,,0|i0g4xb:,92228,brandon.williams,brandon.williams,,,,,,,,,20/Aug/10 20:39;brandon.williams;+1,20/Aug/10 20:43;jbellis;committed,"21/Aug/10 11:14;hudson;Integrated in Cassandra #518 (See [https://hudson.apache.org/hudson/job/Cassandra/518/])
    fix FD leak in single-row slicepredicate queries.
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1416
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quorum calculation code needs to use polymorphic Strategy method for determining replica counts,CASSANDRA-1804,12491833,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,12/1/2010 23:01,3/12/2019 14:11,3/13/2019 22:24,12/2/2010 1:49,0.7.0 rc 2,,,,0,,,,,,"Keyspace.replication_factor is only valid for SimpleStrategy and ONTS, we shouldn't use it directly anywhere.

Related: CASSANDRA-1263 would remove the temptation to do the Wrong Thing.",,,,,,,,,,,,,,,,,,,,01/Dec/10 23:23;jbellis;1804.txt;https://issues.apache.org/jira/secure/attachment/12465089/1804.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,10:11.8,,,no_permission,,,,,,,,,,,,20320,,,Sat Dec 11 07:35:16 UTC 2010,,,,,,0|i0g7if:,92647,jhermes,jhermes,,,,,,,,,"02/Dec/10 00:10;jhermes;+1.
(An optional improvement would be to leave getRF in ARS abstract, and to provide dummy implementations in the child strategies to be rewritten at a later date (namely for #1263)).",02/Dec/10 01:49;jbellis;committed,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix RackUnawareStrategyTest,CASSANDRA-163,12425227,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/12/2009 15:27,3/12/2019 14:11,3/13/2019 22:24,5/12/2009 21:34,,,,,0,,,,,,it's been failing for forever but it was in the wrong directory so testng didn't run it.  I've moved it to the right directory for the junit migration but commented out.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,26:38.2,,,no_permission,,,,,,,,,,,,19578,,,Wed May 13 09:26:38 UTC 2009,,,,,,0|i0fx9b:,90986,,,,,,,,,,,12/May/09 21:34;jbellis;fixed.,"13/May/09 09:26;hudson;Integrated in Cassandra #73 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/73/])
    fix RackUnawareStrategyTest -- endpoint asserts 'host' is an ip address (to make sure we're not mixing hostnames in again) so create a suitable fake IP for the test.  patch by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fat client example cannot find schema,CASSANDRA-1002,12462441,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,johanoskarsson,johanoskarsson,4/19/2010 14:59,3/12/2019 14:10,3/13/2019 22:24,4/26/2010 22:04,0.7 beta 1,,,,0,,,,,,"Running the client example in contrib shows that it cannot find the schema, possibly caused by CASSANDRA-44.

Throws this error:
Exception in thread ""main"" java.lang.IllegalArgumentException: Unknown ColumnFamily Standard1 in keyspace Keyspace1
	at org.apache.cassandra.config.DatabaseDescriptor.getComparator(DatabaseDescriptor.java:1123)
	at org.apache.cassandra.db.ColumnFamily.getComparatorFor(ColumnFamily.java:437)
	at ClientOnlyExample.testWriting(ClientOnlyExample.java:52)
	at ClientOnlyExample.main(ClientOnlyExample.java:169)
",,,,,,,,,,,,,,,,,,,,26/Apr/10 20:48;gdusbabek;0001-modify-migrations-to-respect-client-only-mode.patch;https://issues.apache.org/jira/secure/attachment/12442892/0001-modify-migrations-to-respect-client-only-mode.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,49:56.6,,,no_permission,,,,,,,,,,,,19949,,,Mon Apr 26 21:45:33 UTC 2010,,,,,,0|i0g2ef:,91819,,,,,,,,,,,"26/Apr/10 20:49;gdusbabek;I dislike all of the 'if-clientMode' stuff, but Migrations weren't designed to separate schema-state from schema-storage.","26/Apr/10 21:11;jbellis;so this makes clientmode not make changes to local storage, is there anything else i should be noticing?","26/Apr/10 21:20;gdusbabek;No, that's basically it.",26/Apr/10 21:45;jbellis;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
contrib/pig's cassandra.yaml is out of date,CASSANDRA-1326,12470290,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jeromatron,jeromatron,jeromatron,7/27/2010 19:39,3/12/2019 14:10,3/13/2019 22:24,7/27/2010 19:50,0.7 beta 1,,,,0,,,,,,It just needs updating.,,,,,,,,,,,,,,,,,,,,27/Jul/10 19:39;jeromatron;fix_pig.patch;https://issues.apache.org/jira/secure/attachment/12450620/fix_pig.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,50:20.0,,,no_permission,,,,,,,,,,,,20081,,,Wed Jul 28 12:52:00 UTC 2010,,,,,,0|i0g4dj:,92139,,,,,,,,,,,27/Jul/10 19:50;brandon.williams;Committed.,"28/Jul/10 12:52;hudson;Integrated in Cassandra #502 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/502/])
    Update contrib/pig's cassandra.yaml for trunk.  Patch by Jeremy Hanna reviewed by brandonwilliams for CASSANDRA-1326
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update stress.py to thrift api changes,CASSANDRA-1148,12465842,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,johanoskarsson,johanoskarsson,johanoskarsson,6/1/2010 9:35,3/12/2019 14:10,3/13/2019 22:24,6/1/2010 13:28,0.7 beta 1,,,,0,,,,,,The stress.py file was not updated to the CASSANDRA-1070 changes,,,,,,,,,,,,,,,,,,,,01/Jun/10 09:42;johanoskarsson;CASSANDRA-1148.patch;https://issues.apache.org/jira/secure/attachment/12446008/CASSANDRA-1148.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,16:42.6,,,no_permission,,,,,,,,,,,,20009,,,Tue Jun 01 13:28:15 UTC 2010,,,,,,0|i0g3af:,91963,,,,,,,,,,,01/Jun/10 09:42;johanoskarsson;Wraps the timestamp in a Clock object.,01/Jun/10 13:16;jbellis;+1,01/Jun/10 13:28;johanoskarsson;Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
secondary indexes aren't created on pre-existing or streamed data,CASSANDRA-2244,12499629,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,2/24/2011 19:29,3/12/2019 14:10,3/13/2019 22:24,3/3/2011 17:42,0.7.4,,Feature/2i Index,,0,,,,,,"The repaired node neither receives indexes from the replicas, nor does it generate them afterwards.  The same bug prevents generation of new indexes against existing data.",,,57600,57600,,0%,57600,57600,,,,,,,,,,,,03/Mar/11 17:04;jbellis;2244-v2.txt;https://issues.apache.org/jira/secure/attachment/12472561/2244-v2.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,31:39.4,,,no_permission,,,,,,,,,,,,20523,,,Thu Mar 03 17:55:22 UTC 2011,,,,,,0|i0ga7r:,93085,brandon.williams,brandon.williams,,,,,,,,,28/Feb/11 07:31;jbellis;Doesn't repair use normal Streaming?  That should take care of generating the index CFs.,"01/Mar/11 23:33;brandon.williams;I added some logging, it does do something after streaming:

{noformat}
 INFO 23:27:32,050 Opening /var/lib/cassandra/data/Keyspace1/Standard1-f-1
 INFO 23:27:34,843 Opening /var/lib/cassandra/data/Keyspace1/Standard1-f-2
 INFO 23:27:35,232 Opening /var/lib/cassandra/data/Keyspace1/Standard1-f-3
 INFO 23:27:35,246 Building index for ColumnFamilyStore(table='Keyspace1', columnFamily='Standard1') [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-f-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-f-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-f-3-Data.db')] [java.nio.HeapByteBuffer[pos=0 lim=2 cap=2]]
 INFO 23:29:53,448 Finished streaming session 19283352579402862 from /10.179.65.102
{noformat}

During the ~2.5 minutes between the last 2 lines it appeared to generate the index, however flushing afterwards shows no index was generated:

{noformat}
 INFO 23:32:06,591 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1299022015229.log', position=12763)
 INFO 23:32:06,592 Enqueuing flush of Memtable-LocationInfo@1459852990(35 bytes, 1 operations)
 INFO 23:32:06,593 Writing Memtable-LocationInfo@1459852990(35 bytes, 1 operations)
 INFO 23:32:06,623 Completed flushing /var/lib/cassandra/data/system/LocationInfo-f-3-Data.db (89 bytes)
{noformat}


","03/Mar/11 16:34;jbellis;The ""don't bother flushing dropped CFs"" check was unaware that index CFs are never part of the global CF metadata.

(I think that check is race-prone to begin with, so we should probably drop it and do something more robust, but for now it's better than nothing -- it will *usually* prevent creating new sstables for dropped CFs.)",03/Mar/11 17:04;jbellis;Gary points out that migration does acquire the flushlock.,"03/Mar/11 17:04;jbellis;patch adds test that catches the problem, and adds fix to CFS.","03/Mar/11 17:42;brandon.williams;+1, committed","03/Mar/11 17:55;hudson;Integrated in Cassandra-0.7 #342 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/342/])
    CFS correctly flushes index CFs.
Patch by jbellis, reviewed by brandonwilliams for CASSANDRA-2244
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assert when using CL.EACH_QUORUM,CASSANDRA-2254,12499882,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,segy,segy,segy,2/28/2011 0:42,3/12/2019 14:10,3/13/2019 22:24,2/28/2011 1:50,0.7.4,,,,0,,,,,,"When using the NetworkTopology strategy, I am able to write using CL.LOCAL_QUORUM. When I attempt to write using CL.EACH_QUORUM, an assert is hit in DatacenterSyncWriteResponseHandler. Tracing the call through to the NetworkTopology code, it seems that this particular handler is only used when CL = EACH_QUORUM, yet the code asserts. ",RH Linux,,,,,,,,,,,,,,,,,,,28/Feb/11 00:55;segy;0001-Fixing-CASSANDRA-2254-DatacenterSyncWriteResponseHea.patch;https://issues.apache.org/jira/secure/attachment/12472132/0001-Fixing-CASSANDRA-2254-DatacenterSyncWriteResponseHea.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,50:32.9,,,no_permission,,,,,,,,,,,,20527,,,Mon Feb 28 02:06:24 UTC 2011,,,,,,0|i0ga9z:,93095,jbellis,jbellis,,,,,,,,,"28/Feb/11 00:55;segy;Patch that changes the assert to match what I believe is the intent of the check, basically s/LOCAL_QUORUM/EACH_QUORUM/.","28/Feb/11 01:50;jbellis;committed, thanks!","28/Feb/11 02:06;hudson;Integrated in Cassandra-0.7 #326 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/326/])
    fix assert in DatacenterSyncWriteResponseHandler
patch by Mark Guzman; reviewed by jbellis for CASSANDRA-2254
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When loading an AbstractType that does not include an instance field, an unhelpful exception is raised making diagnosis difficult",CASSANDRA-1242,12468314,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,eonnen,eonnen,eonnen,7/1/2010 1:21,3/12/2019 14:10,3/13/2019 22:24,7/1/2010 22:21,0.7 beta 1,,,,0,,,,,,"0.7.0 changes the contract for creating AbstractTypes. A custom AbstractType defined against 0.6.x will be incompatible and the error messaging around why the comparator is invalid is obtuse and non-obvious. Specifically, when porting a valid AbstractType from 0.6.x to 0.7 that does not include a public static instance field, the thrift system_add_column_family call will throw an exception whose only message is:

InvalidRequestException(why:instance)

No log messages are generated from the server as to the issue so the root cause is non obvious to developers.

I marked as Major because types defined against 0.6.x did not require an ""instance"" field so at a minimum migration of AbstractTypes to 0.7 should document the change in what is expected of AbstractTypes.

Patch attached for better logging and to create a more helpful exception for better communication to the client as to the issue.
",,,,,,,,,,,,,,,,,,,,01/Jul/10 01:23;eonnen;CASSANDRA-1242.patch;https://issues.apache.org/jira/secure/attachment/12448465/CASSANDRA-1242.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:11.7,,,no_permission,,,,,,,,,,,,20048,,,Fri Jul 02 13:01:50 UTC 2010,,,,,,0|i0g3v3:,92056,,,,,,,,,,,"01/Jul/10 22:21;jbellis;committed, thanks!

(I note in our defense that NEWS.txt has mentioned the AbstractType change for several weeks now though :)","02/Jul/10 12:47;hudson;Integrated in Cassandra #483 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/483/])
    add friendlier ConfigurationException for malformed AbstractTypes.  patch by Erik Onnen; reviewed by jbellis for CASSANDRA-1242
",02/Jul/10 13:01;jeromatron;Good call Erik - it needed some good feedback like this when people get bitten by the change.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix regression in CL.ALL read,CASSANDRA-2094,12497408,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,kelvin,kelvin,kelvin,2/2/2011 1:14,3/12/2019 14:10,3/13/2019 22:24,2/2/2011 14:33,0.7.1,,,,0,,,,,,"regression:
- digest message object re-used across multiple hosts.

problem:
- shared message id, so the first digest response received will remove the callback for all others.",,,,,,,,,,,,,,,,CASSANDRA-2038,,,,02/Feb/11 01:16;kelvin;0001-fix-bug-in-2038-via-4826e8c8.patch;https://issues.apache.org/jira/secure/attachment/12470007/0001-fix-bug-in-2038-via-4826e8c8.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,33:46.5,,,no_permission,,,,,,,,,,,,20438,,,Wed Feb 02 14:52:10 UTC 2011,,,,,,0|i0g99z:,92933,jbellis,jbellis,,,,,,,,,02/Feb/11 01:15;kelvin;caught by distributed testing.,02/Feb/11 01:16;kelvin;do not re-use digest message object.,"02/Feb/11 01:31;kelvin;future reference: please test w/ RF=3, which is a more interesting case for CL.ALL.","02/Feb/11 14:33;jbellis;Thanks for catching that, Kelvin!  committed.

(FWIW this is a regression from CASSANDRA-1959.)","02/Feb/11 14:52;hudson;Integrated in Cassandra-0.7 #234 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/234/])
    remove digestMessage reuse to fix regression from #1959
patch by Kelvin Kakugawa; reviewed by jbellis for CASSANDRA-2094
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system_create_cf() makes a SuperCF when column_type is Standard and subcolumn_comparator_type is present,CASSANDRA-1826,12492539,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,thobbs,thobbs,12/6/2010 20:39,3/12/2019 14:10,3/13/2019 22:24,12/6/2010 21:57,0.7.0 rc 2,,,,0,,,,,,"If you create a CF with system_create_column_family() and the CfDef has column_type = 'Standard' and subcomparator_type is present, it creates a SuperCF.  I would expect an InvalidRequestException, instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,57:42.8,,,no_permission,,,,,,,,,,,,20328,,,Mon Dec 06 21:57:42 UTC 2010,,,,,,0|i0g7nb:,92669,,,,,,,,,,,06/Dec/10 21:57;jbellis;fixed in rc2 by patches for CASSANDRA-1773 and CASSANDRA-1813,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException from o.a.c.db.ReplicateOnWriteTask,CASSANDRA-1903,12494050,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,kelvin,urandom,urandom,12/25/2010 0:27,3/12/2019 14:10,3/13/2019 22:24,12/27/2010 23:25,0.8 beta 1,,,,0,,,,,,"I'm seeing a whole lot of these when writing to a node.
 
{noformat}
java.lang.NullPointerException
	at org.apache.cassandra.db.ReplicateOnWriteTask.run(ReplicateOnWriteTask.java:97)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{noformat}

I don't think it will be difficult to reproduce, but the script I'm using is attached.

I bisected the source tree and and http://svn.apache.org/viewvc?view=rev&revision=1052356 seems to the culprit (a merge from 0.7).  Maybe CASSANDRA-1530?",,,,,,,,,,,,,,,,,,,,27/Dec/10 23:23;gdusbabek;1903.diff;https://issues.apache.org/jira/secure/attachment/12467020/1903.diff,25/Dec/10 00:27;urandom;thrift-test.py;https://issues.apache.org/jira/secure/attachment/12466952/thrift-test.py,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,12:21.8,,,no_permission,,,,,,,,,,,,20363,,,Tue Dec 28 01:10:01 UTC 2010,,,,,,0|i0g84f:,92746,,,,,,,,,,,"27/Dec/10 22:24;urandom;Upon further examination...

The null value here is row.cf, it's null because the read came up empty, so this looks like a race to me (the read is beating the write).  But... the bigger question is, why have we added a read to the write path? This seems very wrong to me.

Also, what does ReplicateOnWriteTask do that the existing replication doesn't? How come we're doing both?  This all seems to have changed as part of the counters implementation, so it would be great if someone involved with that could sound off on this.","27/Dec/10 23:12;kelvin;R-O-W is optional.

The goal is to provide faster convergence for CL.ONE writes.",27/Dec/10 23:25;gdusbabek;committed.,27/Dec/10 23:48;urandom;So the NullPointerExceptions are normal?,"28/Dec/10 01:10;hudson;Integrated in Cassandra #642 (See [https://hudson.apache.org/hudson/job/Cassandra/642/])
    set DEFAULT_REPLICATE_ON_WRITE to false. patch by gdusbabek, reviewed by eevans. CASSANDRA-1903
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-topology.properties cannot reside inside jar file,CASSANDRA-2036,12496455,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,michaelsembwever,michaelsembwever,michaelsembwever,1/23/2011 20:19,3/12/2019 14:10,3/13/2019 22:24,1/24/2011 15:59,0.7.1,,,,0,,,,,,"PropertyFileSnitch cannot load the cassandra-topology.properties if it is located inside a jar file.

At startup cassandra will print and exit
[ERROR] 20:50:01  Fatal error: Unable to read cassandra-topology.properties
Bad configuration; unable to start server

It seems FBUtilities.resourceToFIle(..) can only be used for loading plain files.

The attached patch solves the problem. It uses the standard java approach for loading a resource stream...
",,,,,,,,,,,,,,,,,,,,24/Jan/11 07:45;michaelsembwever;CASSANDRA-2036.patch;https://issues.apache.org/jira/secure/attachment/12469125/CASSANDRA-2036.patch,23/Jan/11 20:33;michaelsembwever;CASSANDRA-2036.patch;https://issues.apache.org/jira/secure/attachment/12469101/CASSANDRA-2036.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,40:47.8,,,no_permission,,,,,,,,,,,,20408,,,Mon Jan 24 16:38:54 UTC 2011,,,,,,0|i0g8xj:,92877,jbellis,jbellis,,,,,,,,,24/Jan/11 05:40;jbellis;Should we also allow loading from a URL the way we do cassandra.yaml in DatabaseDescriptor?,"24/Jan/11 06:52;michaelsembwever;There currently is no way of specifying the path/url to the cassandra-topology.properties, it is just presumed to be at the root of your classpath.

But the one cassandra-topology.properties file can serve not just every node in the cluster, but all nodes in all clusters (useful when test environments run multiple nodes/clusters per server, so it does make sense to allow loading from a URL. 

I can make a new patch for this (although i think this issue becomes an improvement then).","24/Jan/11 07:45;michaelsembwever;second revision. with improvements so ""-Dcassandra.propertyFileSnitch=<url>"" can be specified on the command line (in a similar manner to ""-Dcassadra.config"".","24/Jan/11 15:59;jbellis;committed the first patch, and created CASSANDRA-2040 for a more general solution to the URL problem in 0.8","24/Jan/11 16:38;hudson;Integrated in Cassandra-0.7 #194 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/194/])
    load PFS properties with ResourceAsStream
patch by Michael SembWever; reviewed by jbellis for CASSANDRA-2036
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes for multi-datacenter writes,CASSANDRA-2051,12496702,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ivancso,jbellis,jbellis,1/25/2011 16:57,3/12/2019 14:10,3/13/2019 22:24,1/25/2011 20:14,0.7.1,,,,0,,,,,,"Copied from CASSANDRA-982:

    * Message::removeHeader
      message.setHeader(RowMutation.FORWARD_HEADER, null) throws NullPointerException

    * db/RowMutationVerbHandler::forwardToLocalNodes
      set correct destination address for sendOneWay

    * response(ReadResponse result) added to DatacenterReadCallback
      otherwise ReadCallback will process local results and condition will be never signaled in DatacenterReadCallback

    * FORWARD header removed in StorageProxy::sendMessages if dataCenter equals to localDataCenter
      (if a non local DC processed before local DC FORWARD header will be set when unhintedMessage used in sendToHintedEndpoints. one instance of Message used for unhintedMessage)
",,,,,,,,,,,,,,,,,,,,25/Jan/11 17:17;jbellis;2051-2.txt;https://issues.apache.org/jira/secure/attachment/12469299/2051-2.txt,25/Jan/11 16:58;jbellis;rep_fix_02.patch;https://issues.apache.org/jira/secure/attachment/12469297/rep_fix_02.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,49:26.0,,,no_permission,,,,,,,,,,,,20414,,,Tue Jan 25 20:29:38 UTC 2011,,,,,,0|i0g90v:,92892,jbellis,jbellis,,,,,,,,,25/Jan/11 16:58;jbellis;Ivan's patch,25/Jan/11 17:17;jbellis;patch 2 re-organizes the loops in SP::sendMessages to make ivan's fix a little more clear.  (with the side benefit that we now call String.equals once per DC instead of once per message.),25/Jan/11 17:19;jbellis;committed everything else from ivan's patch in r1063361,"25/Jan/11 18:49;hudson;Integrated in Cassandra-0.7 #207 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/207/])
    ","25/Jan/11 19:55;ivancso;patch 2 much better. thanks Jonathan. ;)

i pulled latest trunk and applied patch in 2051-2.txt.

it seems that communication between nodes and DCs works as expected.","25/Jan/11 20:04;tjake;Looks good +1,  thanks for testing this ivan",25/Jan/11 20:14;jbellis;committed.  thanks Ivan and Jake!,"25/Jan/11 20:29;hudson;Integrated in Cassandra-0.7 #210 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/210/])
    clean out forward headers from message between loops
patch by ivancso and jbellis; reviewed by tjake for CASSANDRA-2051
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make cache saving less contentious,CASSANDRA-2053,12496713,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,nickmbailey,nickmbailey,1/25/2011 19:15,3/12/2019 14:10,3/13/2019 22:24,2/5/2011 21:18,0.7.1,,,,0,,,,,,"The current default for saving key caches is every hour.  Additionally the default timeout for flushing memtables is every hour.  I've seen situations where both of these occuring at the same time every hour causes enough pressure on the node to have it drop messages and other nodes mark it dead.  This happens across the cluster and results in flapping.

We should do something to spread this out. Perhaps staggering cache saves/flushes that occur due to timeouts.",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,01/Feb/11 17:48;jbellis;2053.txt;https://issues.apache.org/jira/secure/attachment/12469946/2053.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,48:59.8,,,no_permission,,,,,,,,,,,,20415,,,Sat Feb 05 21:52:47 UTC 2011,,,,,,0|i0g91b:,92894,nickmbailey,nickmbailey,,,,,,,,,01/Feb/11 17:48;jbellis;Moves CacheWriter to a top-level class and runs it on the CompactionManager executor.  Reads and writes udpated to use BRAF cache-non-polluting mode.,05/Feb/11 20:08;nickmbailey;+1,"05/Feb/11 21:18;jbellis;committed.

also increased default key cache save period from 1h to 4h.","05/Feb/11 21:52;hudson;Integrated in Cassandra-0.7 #251 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/251/])
    cache writing moved to CompactionManager to reduce i/o contention and updated to use non-cache-polluting writes
patch by jbellis; reviewed by nickmbailey for CASSANDRA-2053
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"nosetests are busted because ""Not enough live nodes to support this keyspace""",CASSANDRA-1360,12470834,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,8/4/2010 20:25,3/12/2019 14:10,3/13/2019 22:24,8/5/2010 18:25,0.7 beta 1,,,,0,,,,,,"the keyspaces created by nosetests should all have RF=1.

I'm pretty sure some of the keyspaces can be removed altogether.",,,,,,,,,,,,,,,,,,,,04/Aug/10 20:44;gdusbabek;0001-remove-unneccessary-keyspaces-from-nosetests.patch;https://issues.apache.org/jira/secure/attachment/12451264/0001-remove-unneccessary-keyspaces-from-nosetests.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,22:35.3,,,no_permission,,,,,,,,,,,,20104,,,Sun Aug 08 13:52:23 UTC 2010,,,,,,0|i0g4kv:,92172,,,,,,,,,,,04/Aug/10 20:44;gdusbabek;Removes Keyspace3 and Keyspace4.,05/Aug/10 18:22;jbellis;+1,"08/Aug/10 13:52;hudson;Integrated in Cassandra #510 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/510/])
    remove unneccessary keyspaces from nosetests. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1360
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GossipTimerTask stops running if an Exception occurs,CASSANDRA-1289,12469488,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,wadey,wadey,7/16/2010 20:29,3/12/2019 14:10,3/13/2019 22:24,7/18/2010 2:10,0.6.4,,,,0,,,,,,"The GossipTimerTask run() method has a try/catch around its body, but it re-throws all Exceptions as RuntimeExceptions. This causes the GossipTimerTask to no longer run (due to the way the underlying Java Timer implementation works), stopping the periodic gossip status checks.

Combine this problem with a bug like CASSANDRA-757 (not yet fixed in 0.6.x) and you get into a state where the server keeps running, but gossip is no longer occurring, preventing node addition / removal from happening.

I see two potential choices:
1) Log the error but don't re-throw it so that the GossipTimerTask will continue to run on its next interval.
2) Shutdown the server, since continuing to run without gossip subtly breaks other functionality / knowledge of other nodes.",,,,,,,,,,,,,,,,,,,,17/Jul/10 22:02;brandon.williams;1289.txt;https://issues.apache.org/jira/secure/attachment/12449769/1289.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,34:35.5,,,no_permission,,,,,,,,,,,,20060,,,Sun Jul 18 02:09:51 UTC 2010,,,,,,0|i0g45j:,92103,,,,,,,,,,,"17/Jul/10 21:34;brandon.williams;Patch to catch the exception and log it, as suggested in CASSANDRA-757","18/Jul/10 02:09;jbellis;committed w/ changes since it was simple:

uses .error instead of .warn

uses .error(message, exception) so the entire stack trace will be logged",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnavailableException after bootstrapping a previously decommissioned node,CASSANDRA-2163,12498566,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,slebresne,slebresne,2/14/2011 19:31,3/12/2019 14:10,3/13/2019 22:24,2/15/2011 8:16,0.8 beta 1,,,,0,,,,,,"In trunk (not in 0.7), if I boostrap a node, decommission it and boostrap it back (after having clean all data directory but on same ip), I get UnavailableException on read. 
I've bisected the regression to r1067508.",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,58:32.8,,,no_permission,,,,,,,,,,,,20474,,,Tue Feb 15 08:15:52 UTC 2011,,,,,,0|i0g9pb:,93002,,,,,,,,,,,"14/Feb/11 21:58;brandon.williams;Unable to repro with r1070628, but I did make some gossiper changes there forward-porting CASSANDRA-2115 so maybe I struck gold.",15/Feb/11 08:15;slebresne;I confirm I cannot repro now either. The forward port of CASSANDRA-2115 apparently fixed whatever was wrong.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OPP makes HH unhappy,CASSANDRA-1439,12472648,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,jbellis,jbellis,8/26/2010 21:47,3/12/2019 14:10,3/13/2019 22:24,8/27/2010 19:42,0.7 beta 2,,,,0,,,,,,"as reported multiple times on the mailing list and IRC:

ERROR [HINTED-HANDOFF-POOL:1] 2010-08-26 15:58:20,310 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[HINTED-HANDOFF-POOL:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:169)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:41)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:199)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:78)
        at org.apache.cassandra.db.HintedHandOffManager$1.runMayThrow(HintedHandOffManager.java:296)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:483)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:165)
        ... 11 more",,,,,,,,,,,,,,,,,,,,27/Aug/10 19:04;brandon.williams;1439.txt;https://issues.apache.org/jira/secure/attachment/12453261/1439.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,04:38.9,,,no_permission,,,,,,,,,,,,20139,,,Fri Aug 27 19:42:34 UTC 2010,,,,,,0|i0g52f:,92251,,,,,,,,,,,27/Aug/10 19:04;brandon.williams;Patch to always use UTF-8 for hint keys.,27/Aug/10 19:27;jbellis;+1 (w/ update to CHANGES),27/Aug/10 19:42;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot Start Cassandra Under Windows,CASSANDRA-948,12461065,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,nberardi,nberardi,4/2/2010 21:22,3/12/2019 14:10,3/13/2019 22:24,5/5/2010 20:53,0.6.2,0.7 beta 1,,,0,windows,wont-start,,,,"I get the following error when I try to launch the RC of Cassandra from the command line:

Starting Cassandra Server
Listening for transport dt_socket at address: 8888
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/CassandraDaemon
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.CassandraDaemon
        at java.net.URLClassLoader$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
Could not find the main class: org.apache.cassandra.thrift.CassandraDaemon.  Program will exit.","Windows 7 (NT 6.1) 64-bit, Java 6 Update 19 64-bit, Cassandra 0.6 RC",,,,,,,,,,,,,,,,,,,20/Apr/10 21:09;gdusbabek;cassandra-with-fixes.bat;https://issues.apache.org/jira/secure/attachment/12442349/cassandra-with-fixes.bat,02/Apr/10 21:34;nberardi;cassandra.bat;https://issues.apache.org/jira/secure/attachment/12440652/cassandra.bat,04/May/10 21:54;niarck;ccassandra.bat;https://issues.apache.org/jira/secure/attachment/12443649/ccassandra.bat,02/Apr/10 21:24;nberardi;storage-conf.xml;https://issues.apache.org/jira/secure/attachment/12440650/storage-conf.xml,20/Apr/10 21:07;gdusbabek;trunk-CASSANDRA-948.txt;https://issues.apache.org/jira/secure/attachment/12442348/trunk-CASSANDRA-948.txt,,,,,,,,,5,,,,,,,,,,,,,,,,,,,19:30.0,,,no_permission,,,,,,,,,,,,19934,,,Wed May 05 20:53:39 UTC 2010,,,,,,0|i0g22f:,91765,,,,,,,,,,,02/Apr/10 21:24;nberardi;Here is my storage-conf.xml file.,02/Apr/10 21:34;nberardi;Here is a copy of my cassandra.bat file.,03/Apr/10 15:19;rodrigoap;Did you run ant before? Have you asked for help on the users mailing list before creating this issue?,"03/Apr/10 18:06;nberardi;This was an already pre built binary downloaded from the Cassandra site, so I didn't run the ant build tool.  

This appears to be related to data storage paths I set, because if I switch the paths back to the default UNIX paths.  Everything runs fine. 

These paths that I have set worked fine in the 0.5.1 build of Cassandra.  Please see my attached config file for more details.","19/Apr/10 04:44;jbellis;Mark, are we broken again on Windows?","20/Apr/10 14:22;reldan;Hi guys. 
User skanga from http://www.sodeso.nl/?p=80 resolve the problem such way: 

The substitution in the batch file did not work for me as shown below.

C:\Java\servers\Apache-Cassandra-0.6.0-beta2\bin>cassandra
Invalid parameter - P:
Starting Cassandra Server
Listening for transport dt_socket at address: 8888
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/CassandraDaemon
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.CassandraDaemon
at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.cassandra.thrift.CassandraDaemon. Program will exit.

C:\Java\servers\Apache-Cassandra-0.6.0-beta2\bin>cassandra.bat

I fixed it in cassandra.bat by replacing

REM Shorten lib path for old platforms
subst P: ""%CASSANDRA_HOME%\lib""
P:
set CLASSPATH=P:\

for %%i in (*.jar) do call :append %%i
goto okClasspath

:append
set CLASSPATH=%CLASSPATH%;P:\%*
goto :eof

WITH

REM For each jar in the CASSANDRA_HOME lib directory call append to build the CLASSPATH variable.
for %%i in (%CASSANDRA_HOME%\lib\*.jar) do call :append %%~fi
goto okClasspath

:append
set CLASSPATH=%CLASSPATH%;%1%2
goto :eof
","20/Apr/10 16:20;nberardi;Thanks Eldar,

I know you are trying to be helpful, but as I indicated in my comments above the issue has to do with the CommitLogDirectory and DataFileDirectory when changed from a UNIX path to a Windows path.  

This issue appears to be solved. Because I took my config file from above and started it on 0.6.1 this morning.  

Nick","20/Apr/10 21:07;gdusbabek;This fixes the problem in trunk and 0.6.  

I will commit the change after someone tests and gives it a +1 review.",20/Apr/10 21:09;gdusbabek;Including a patched version of the batch file for the patch-averse.,"22/Apr/10 00:48;mgreene;Using the cassandra-with-fixes.bat I was able to start cassandra 0.6.1 on windows. However, this error did appear in the log:

log4j:ERROR Could not read configuration file [null\log4j.properties].
java.io.FileNotFoundException: null\log4j.properties (The system cannot find the path specified)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.<init>(FileInputStream.java:106)
        at java.io.FileInputStream.<init>(FileInputStream.java:66)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:306)
        at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:324)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:62)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:177)
log4j:ERROR Ignoring configuration file [null\log4j.properties].",22/Apr/10 01:16;gdusbabek;Thanks Mark.  Can you tell me where you keep log4j.properties in relation to everything else?,"22/Apr/10 01:27;mgreene;@Gary Sure....for me it's C:\dev\apache-cassandra-0.6.1\conf\log4j.properties.

","28/Apr/10 04:01;astrouk;Hi Gary and Mark,
I have just tried running cassandra-with-fixes.bat on fresh installation of 0.6.1 and have received an original error. 
It is Windows 2003 Server R2 32-bit and JDK 6.0.20 if it makes any sense.
Thank you.

Alex","28/Apr/10 11:44;mgreene;@Alexander Hint: You may want to give the details, perhaps the stack trace or the log line that is this ""original error"" :-)","28/Apr/10 18:14;astrouk;Sure, Mark.  I have meant an error reported by Nick. I am trying to launch cassandra from command line and receive the following output:

C:\Program Files\apache-cassandra-0.6.1\bin>cassandra-with-fixes.bat
Starting Cassandra Server
Listening for transport dt_socket at address: 8888
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/
thrift/CassandraDaemon
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.Cassand
raDaemon
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.cassandra.thrift.CassandraDaemon.  Pro
gram will exit.

This is all I have.

Thank you","29/Apr/10 01:10;astrouk;I got it. CLASSPATH variable breaks at whitespace in a folder name (""Program Files"" in my case). I have just moved it to a different folder, ""Cassandra"", and it works fine.

Thank you.",29/Apr/10 01:19;gdusbabek;Thanks.  I'll treat that as a +1.  I'll add some documentation in a few places about the space-in-classpath problem and then commit this.,29/Apr/10 01:26;jbellis;http://lkamal.blogspot.com/2006/12/setting-java-classpath-option-with.html,"30/Apr/10 22:20;astrouk;Thank you, Gary. However log4j error described above by Mark still occurs. See below:

C:\Cassandra\apache-cassandra-0.6.1\bin>cassandra.bat
Starting Cassandra Server
Listening for transport dt_socket at address: 8888
log4j:ERROR Could not read configuration file [null\log4j.properties].
java.io.FileNotFoundException: null\log4j.properties (The system cannot find the
 path specified)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.<init>(FileInputStream.java:106)
        at java.io.FileInputStream.<init>(FileInputStream.java:66)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurato
r.java:306)
        at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.
java:324)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.jav
a:62)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java
:177)","03/May/10 14:49;gdusbabek;cassandra-with-fixes.bat (and all the previous versions afaict) already quote the classpath.  My windows VM is ill and I don't feel like doctoring it atm, so if there are any windows users willing to take this one up and work on it, that would be great.",04/May/10 21:54;niarck;my version of the batch file without the space-in-classpath problem and the logj4 error,05/May/10 20:19;akapuya;Fixed by adding CASSANDRA_HOME environment in Windows,"05/May/10 20:35;gdusbabek;from IRC:  n]> gdusbabek, ccassandra.bat is the one working.

I'm committing it.
",05/May/10 20:53;gdusbabek;Thanks for the patch Leif!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli cannot connect ,CASSANDRA-1333,12470395,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,arya,arya,7/29/2010 0:06,3/12/2019 14:10,3/13/2019 22:24,7/29/2010 1:49,0.7 beta 1,,,,0,,,,,,"I cannot connect to any of my nodes using Cassandra-CLI. I think this has happened about 2 weeks ago:

[agoudarzi@cas-test1 bin]$ cassandra-cli --host 10.50.26.132 --port 9160 --debug
Exception retrieving information about the cassandra node, check you have connected to the thrift port.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:369)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:295)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:202)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_describe_cluster_name(Cassandra.java:1117)
	at org.apache.cassandra.thrift.Cassandra$Client.describe_cluster_name(Cassandra.java:1103)
	at org.apache.cassandra.cli.CliMain.connect(CliMain.java:164)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:255)
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
[default@unknown] exit                     

However using Thrift PHP Client I have no problem connecting and executing describe_cluster_name().

I have configured Cassandra RPC port and IP as follows:

# The address to bind the Thrift RPC service to
rpc_address: 10.50.26.132
# port for Thrift to listen on
rpc_port: 9160

Steps to Reproduce:
1. Start from a clean setup;
2. Run py_stress to insert some keys and create the default keyspace;
3. Try connecting using cassandra-cli like command above. You'll get the Exception.
","CentOS 5.2
trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,49:02.5,,,no_permission,,,,,,,,,,,,20086,,,Mon Nov 22 02:07:24 UTC 2010,,,,,,0|i0g4f3:,92146,,,,,,,,,,,"29/Jul/10 00:17;arya;More Info:

The server log says my client is old! But I am using the latest build from trunc:

ERROR [pool-1-thread-22] 2010-07-28 17:09:54,688 CustomTThreadPoolServer.java (line 175) Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Missing version in readMessageBegin, old client?
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:211)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2587)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
",29/Jul/10 01:49;jbellis;fixed in r980285.  thanks!,"21/Nov/10 13:05;jpartogi;Is this broken in 0.7.0-beta3 again?

I am getting this error when trying to connect from CLI:
[default@unknown] connect localhost/9160 
Exception retrieving information about the cassandra node, check you have connected to the thrift port.

Cassandra is running and thrift is bound to port 9160",22/Nov/10 02:07;jbellis;No. Probably you are running server in un-framed mode; cli defaults to trying to connect w/ framed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incomplete sstables for System keyspace are not scrubbed before opening it,CASSANDRA-1564,12475568,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/1/2010 3:18,3/12/2019 14:10,3/13/2019 22:24,10/1/2010 20:30,0.7 beta 3,,,,0,,,,,,(From CASSANDRA-1477),,,,,,,,,,,,,,,,,,,,01/Oct/10 03:18;jbellis;1564.txt;https://issues.apache.org/jira/secure/attachment/12456079/1564.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,16:11.6,,,no_permission,,,,,,,,,,,,20203,,,Sat Oct 02 12:56:27 UTC 2010,,,,,,0|i0g607:,92403,gdusbabek,gdusbabek,,,,,,,,,01/Oct/10 20:16;gdusbabek;+1,01/Oct/10 20:30;jbellis;committed,"02/Oct/10 12:56;hudson;Integrated in Cassandra #553 (See [https://hudson.apache.org/hudson/job/Cassandra/553/])
    scrub System keyspace before opening it
patch by jbellis; reviewed by gdusbabek for CASSANDRA-1564
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
index scan should treat not-present columns as not matching index expressions,CASSANDRA-1745,12479922,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,11/14/2010 18:49,3/12/2019 14:10,3/13/2019 22:24,11/17/2010 20:11,0.7.0 rc 1,,,,0,,,,,,"As reported on the mailing list,

{code}
I created a column family and added index on column A,B,C. 

Now I insert three rows. 

row1 : A=123, B=456, C=789 
row2 : A=123, C=789 
row3 : A=123, B=789, C=789 

Now if I perform an indexed query for A=123 and B=456, both row1 and row2 are returned. 
{code}",,,,,,,,,,,,,,,,,,,,14/Nov/10 18:53;jbellis;1745.txt;https://issues.apache.org/jira/secure/attachment/12459567/1745.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,55:54.5,,,no_permission,,,,,,,,,,,,20287,,,Thu Nov 18 00:16:35 UTC 2010,,,,,,0|i0g74n:,92585,stuhood,stuhood,,,,,,,,,"17/Nov/10 19:55;stuhood;+1
null is a tricky beast, but since we don't have a ""not equal"" expression yet, this appears to be the correct behaviour.",17/Nov/10 20:11;jbellis;committed,"18/Nov/10 00:16;hudson;Integrated in Cassandra-0.7 #13 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/13/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid dropping messages off the client request path,CASSANDRA-1676,12478591,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/28/2010 15:36,3/12/2019 14:10,3/13/2019 22:24,11/1/2010 4:07,0.6.7,0.7.0 rc 1,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,31/Oct/10 00:31;jbellis;1676-v2.txt;https://issues.apache.org/jira/secure/attachment/12458496/1676-v2.txt,28/Oct/10 15:37;jbellis;1676.txt;https://issues.apache.org/jira/secure/attachment/12458255/1676.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,36:21.3,,,no_permission,,,,,,,,,,,,20254,,,Fri Jan 07 10:01:25 UTC 2011,,,,,,0|i0g6pb:,92516,stuhood,stuhood,,,,,,,,,"29/Oct/10 04:31;jbellis;Brandon, can you beat up a cluster badly enough to start dropping repair messages like this, now that http://issues.apache.org/jira/browse/CASSANDRA-1677 shows what exactly we're dropping?  AS mentioned in CASSANDRA-1674 we're not really sure what is happening here so I don't want to commit this until we understand that better.",31/Oct/10 00:31;jbellis;rebased and updated post-CASSANDRA-1685 to never drop INTERNAL_RESPONSE messages,"31/Oct/10 21:36;stuhood;It doesn't look like CASSANDRA-1685 actually made it in to trunk, so this doesn't compile.",31/Oct/10 23:11;jbellis;patch is against 0.7.,01/Nov/10 03:24;stuhood;+1,"01/Nov/10 04:07;jbellis;committed, w/ the slight modification for 0.6 of not dropping any _RESPONSE calls at all since CASSANDRA-1685 isn't backported there.  should be a mostly academic difference since REQUEST_RESPONSE should be essentially instantaneous.","07/Jan/11 10:01;rantav;This bug fixes the ""hanged bootstrap"" problem some which some clusters would see when bootstrapping new nodes. 
See the following threads for more details 
http://www.mail-archive.com/user@cassandra.apache.org/msg07106.html
http://www.mail-archive.com/user@cassandra.apache.org/msg08379.html
http://www.mail-archive.com/user@cassandra.apache.org/msg08441.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Discard Commitlog Exception,CASSANDRA-936,12460748,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,3/30/2010 21:47,3/12/2019 14:10,3/13/2019 22:24,4/5/2010 19:54,0.6.1,,,,0,,,,,,"2010-03-30_21:19:02.31041 java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
2010-03-30_21:19:02.31041 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
2010-03-30_21:19:02.31041 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2010-03-30_21:19:02.31041 	at java.lang.Thread.run(Thread.java:636)
2010-03-30_21:19:02.31041 Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
2010-03-30_21:19:02.31041 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2010-03-30_21:19:02.31041 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2010-03-30_21:19:02.31041 	... 2 more
2010-03-30_21:19:02.31041 Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:358)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.ColumnFamilyStore$1.runMayThrow(ColumnFamilyStore.java:371)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2010-03-30_21:19:02.31041 	... 6 more
2010-03-30_21:19:02.31041 Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:350)
2010-03-30_21:19:02.31041 	... 8 more
2010-03-30_21:19:02.31041 Caused by: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1269983937410.log', position=8780)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:378)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:72)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:344)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2010-03-30_21:19:02.31041 	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLogExecutorService.process(CommitLogExecutorService.java:113)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLogExecutorService.access$200(CommitLogExecutorService.java:35)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.db.commitlog.CommitLogExecutorService$1.runMayThrow(CommitLogExecutorService.java:67)
2010-03-30_21:19:02.31041 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2010-03-30_21:19:02.31041 	... 1 more",,,,,,,,,,,,,,,,,,,,05/Apr/10 19:20;jbellis;936.txt;https://issues.apache.org/jira/secure/attachment/12440784/936.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,35:44.7,,,no_permission,,,,,,,,,,,,19926,,,Mon Apr 05 19:54:29 UTC 2010,,,,,,0|i0g1zr:,91753,,,,,,,,,,,"01/Apr/10 16:35;jbellis;Thought I had this figured out but I was wrong.

Can you turn on debug logging for the commitlog and include the log up to the next exception?

You can do this w/ JConsole under the service.StorageService MBean, Operations group, setLog4jLevel(org.apache.cassandra.db.commitlog, DEBUG)
","02/Apr/10 18:02;dispalt;2010-04-02_04:12:48.85690 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178196299.log); dirty is 1, 
2010-04-02_04:12:48.87690 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178421319.log); dirty is 1, 
2010-04-02_04:12:48.87690 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178736305.log); dirty is 1, 5, 15, 
2010-04-02_04:12:48.88690 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178971505.log); dirty is 1, 5, 15, 
2010-04-02_04:12:48.88690 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179198585.log); dirty is 1, 5, 15, 
2010-04-02_04:12:48.99689 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179421825.log); dirty is 1, 5, 7, 9, 15, 2010-04-02_04:12:49.01689 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179734722.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:12:49.04689 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179957332.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:12:49.05689 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180177252.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:12:49.16688 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180484668.log); dirty is 1, 9, 15, 
2010-04-02_04:12:49.21688 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180714597.log); dirty is 1, 6, 9, 12, 15, 16, 
2010-04-02_04:12:49.23688 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180931705.log); dirty is 1, 3, 5, 6, 9, 12, 14, 15, 16, 
2010-04-02_04:12:49.28688 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270181242081.log); dirty is 1, 3, 4, 5, 6, 9, 11, 12, 15, 16, 
2010-04-02_04:12:49.29688 DEBUG - Marking replay position 67674133 on commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270181459011.log)2010-04-02_04:14:33.32220 INFO - Rollup4h has reached its threshold; switching in a fresh Memtable at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181459011.log', position=117622522)
2010-04-02_04:14:33.33219 INFO - Enqueuing flush of Memtable(Rollup4h)@1686682813
2010-04-02_04:14:33.34219 INFO - Writing Memtable(Rollup4h)@1686682813
2010-04-02_04:14:36.60205 INFO - Completed flushing /var/lib/cassandra/data/MonitorApp/Rollup4h-2483-Data.db2010-04-02_04:14:36.60205 DEBUG - discard completed log segments for CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181459011.log', position=117622522), column family 16. CFIDs are {system: TableMetadata({LocationInfo: 0, HintsColumnFamily: 1, }), MonitorApp: TableMetadata({AppCounter: 2, Rollup5m: 3, Rollup20m: 4, TextChangeLog: 5, Rollup12h: 6, CheckDetails: 7, TextArchive: 8, StatusArchive: 9,
 NumericArchive: 10, Rollup30m: 11, Rollup1d: 12, StatusChangeLog: 15, ChangeLog: 13, MetricSummary: 14, Rollup60m: 17, Rollup4h: 16, RollupBookeeper: 18, }), }
2010-04-02_04:14:36.60205 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270083637654.log); dirty is 0, 2010-04-02_04:14:36.67204 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270159986913.log); dirty is 10, 2010-04-02_04:14:36.72204 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178196299.log); dirty is 1, 
2010-04-02_04:14:36.74204 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178421319.log); dirty is 1, 
2010-04-02_04:14:36.80204 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178736305.log); dirty is 1, 5, 15, 2010-04-02_04:14:36.83204 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178971505.log); dirty is 1, 5, 15, 
2010-04-02_04:14:36.88204 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179198585.log); dirty is 1, 5, 15, 
2010-04-02_04:14:36.91203 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179421825.log); dirty is 1, 5, 7, 9, 15, 
2010-04-02_04:14:36.96203 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179734722.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:14:37.01203 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179957332.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:14:37.05203 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180177252.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:14:37.18202 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180484668.log); dirty is 1, 9, 15, 
2010-04-02_04:14:37.32202 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180714597.log); dirty is 1, 6, 9, 12, 15, 2010-04-02_04:14:37.35201 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180931705.log); dirty is 1, 3, 5, 6, 9, 12, 14, 15, 
2010-04-02_04:14:37.38201 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270181242081.log); dirty is 1, 3, 4, 5, 6, 9, 11, 12, 15, 
2010-04-02_04:14:37.40201 DEBUG - Marking replay position 117622522 on commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270181459011.log)
2010-04-02_04:14:37.69200 INFO - Rollup12h has reached its threshold; switching in a fresh Memtable at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181459011.log', position=130228458)
2010-04-02_04:14:37.70200 INFO - Enqueuing flush of Memtable(Rollup12h)@298933940
2010-04-02_04:14:37.70200 INFO - Writing Memtable(Rollup12h)@298933940
2010-04-02_04:14:39.40192 INFO - Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1270181679401.log
2010-04-02_04:14:39.44192 INFO - Rollup1d has reached its threshold; switching in a fresh Memtable at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169)2010-04-02_04:14:39.45192 INFO - Enqueuing flush of Memtable(Rollup1d)@46303223
2010-04-02_04:14:40.10189 INFO - Completed flushing /var/lib/cassandra/data/MonitorApp/Rollup12h-2511-Data.db
2010-04-02_04:14:40.10189 INFO - Writing Memtable(Rollup1d)@46303223
2010-04-02_04:14:40.10189 DEBUG - discard completed log segments for CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181459011.log', position=130228458), column family 6. CFIDs are {system: T
ableMetadata({LocationInfo: 0, HintsColumnFamily: 1, }), MonitorApp: TableMetadata({AppCounter: 2, Rollup5m: 3, Rollup20m: 4, TextChangeLog: 5, Rollup12h: 6, CheckDetails: 7, TextArchive: 8, StatusArchive: 9, 
NumericArchive: 10, Rollup30m: 11, Rollup1d: 12, StatusChangeLog: 15, ChangeLog: 13, MetricSummary: 14, Rollup60m: 17, Rollup4h: 16, RollupBookeeper: 18, }), }2010-04-02_04:14:40.10189 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270083637654.log); dirty is 0, 
2010-04-02_04:14:40.20189 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270159986913.log); dirty is 10, 
2010-04-02_04:14:40.25188 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178196299.log); dirty is 1, 
2010-04-02_04:14:40.27188 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178421319.log); dirty is 1, 
2010-04-02_04:14:40.30188 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178736305.log); dirty is 1, 5, 15, 
2010-04-02_04:14:40.40188 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270178971505.log); dirty is 1, 5, 15, 
2010-04-02_04:14:40.44187 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179198585.log); dirty is 1, 5, 15, 
2010-04-02_04:14:40.47187 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179421825.log); dirty is 1, 5, 7, 9, 15, 
2010-04-02_04:14:40.50187 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179734722.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:14:40.52187 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270179957332.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:14:40.54187 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180177252.log); dirty is 1, 5, 9, 15, 
2010-04-02_04:14:40.56187 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180484668.log); dirty is 1, 9, 15, 
2010-04-02_04:14:40.67186 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180714597.log); dirty is 1, 9, 12, 15, 
2010-04-02_04:14:40.68186 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270180931705.log); dirty is 1, 3, 5, 9, 12, 14, 15, 
2010-04-02_04:14:40.76186 DEBUG - Not safe to delete commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270181242081.log); dirty is 1, 3, 4, 5, 9, 11, 12, 15, 
2010-04-02_04:14:40.76186 DEBUG - Marking replay position 130228458 on commit log CommitLogSegment(/var/lib/cassandra/commitlog/CommitLog-1270181459011.log)
2010-04-02_04:14:43.51174 INFO - Completed flushing /var/lib/cassandra/data/MonitorApp/Rollup1d-2568-Data.db
2010-04-02_04:14:43.51174 DEBUG - discard completed log segments for CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169), column family 12. CFIDs are {system: TableMetadata({LocationInfo: 0, HintsColumnFamily: 1, }), MonitorApp: TableMetadata({AppCounter: 2, Rollup5m: 3, Rollup20m: 4, TextChangeLog: 5, Rollup12h: 6, CheckDetails: 7, TextArchive: 8, StatusArchive: 9, NumericArchive: 10, Rollup30m: 11, Rollup1d: 12, StatusChangeLog: 15, ChangeLog: 13, MetricSummary: 14, Rollup60m: 17, Rollup4h: 16, RollupBookeeper: 18, }), }
2010-04-02_04:14:43.52174 ERROR - Error in executor futuretask
2010-04-02_04:14:43.52174 java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2010-04-02_04:14:43.52174       at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
2010-04-02_04:14:43.52174       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
2010-04-02_04:14:43.52174       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2010-04-02_04:14:43.52174       at java.lang.Thread.run(Thread.java:636)
2010-04-02_04:14:43.52174 Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169)
2010-04-02_04:14:43.52174       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
2010-04-02_04:14:43.52174       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2010-04-02_04:14:43.52174       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2010-04-02_04:14:43.52174       ... 2 more
2010-04-02_04:14:43.52174 Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:358)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.ColumnFamilyStore$1.runMayThrow(ColumnFamilyStore.java:371)
2010-04-02_04:14:43.52174       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2010-04-02_04:14:43.52174       ... 6 more
2010-04-02_04:14:43.52174 Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask.get(FutureTask.java:111)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:350)
2010-04-02_04:14:43.52174       ... 8 more
2010-04-02_04:14:43.52174 Caused by: java.lang.AssertionError: discard called on obsolete context CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1270181679401.log', position=169)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:378)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:72)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:344)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
2010-04-02_04:14:43.52174       at java.util.concurrent.FutureTask.run(FutureTask.java:166)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLogExecutorService.process(CommitLogExecutorService.java:113)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLogExecutorService.access$200(CommitLogExecutorService.java:35)
2010-04-02_04:14:43.52174       at org.apache.cassandra.db.commitlog.CommitLogExecutorService$1.runMayThrow(CommitLogExecutorService.java:67)
2010-04-02_04:14:43.52174       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2010-04-02_04:14:43.52174       ... 1 more
",02/Apr/10 18:04;dispalt;Those are the log lines before the exception with the commitlog debugging on full.,"05/Apr/10 19:20;jbellis;Okay, here's what's happening.

The assert in question is
        assert context.position > context.getSegment().getHeader().getPosition(id) : ""discard called on obsolete context "" + context;

The first argument, context.position, is ""the position in the current commitlog segment at the time the currently-finishing flush began.""  The second argument is ""the position we wrote in the commitlog segment header showing where the position was for last-completed flush.""  If the current flush position is less than the last flush position, then the last flush potentially deleted segments whose data wasn't turned into sstables yet, which would be Very Bad.  If the positions are equal, that would mean we're flushing a CF that hasn't had any new data since the last one, which isn't supposed to happen either.

But, there's a discontinuity in the position measuring, and that's when a new commitlog context is swapped in.  Let me give a diagram here of the CL segement's contents:

{code}
XXXXXXXXXXXXXXYYYYYYYYYYYYYYZZZZZZZZZZZZZ
0             H             F            etc
{code}

The first H bytes are the header.  From H to position F represents the first mutation written to the fresh segment.

Normally, if you force a memtable to flush after a new commitlog segment is rolled in, one of two things will happen:

(1) you will flush with a context position of H, and a header lastFlushedPosition of 0 (the initial value) or
(2) you will flush with a context position of F or higher, and a header lastFlushedPosition of H (after some writes occur)

But, if the flush context gets measured at H before any writes happen, THEN A WRITE OCCURS DURING THE FLUSH BEFORE THE DISCARD PHASE, you will have context position == lastFlushedPosition == H, because making a write calls turnOn which edits lastFlushedAt, even though no flushes have occurred, because lastFlushedAt is also semantically startReplayAt, and 0 is not a valid place to start mutation replay.  So in the latest stacktrace, H == 169, and it's failing in exactly this scenario.

The least invasive fix for this in 0.6 is to change the assert from > to >=.
",05/Apr/10 19:49;gdusbabek;+1,05/Apr/10 19:54;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system_drop_keyspace can cause a node to be unstartable,CASSANDRA-1203,12467270,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,mbryant,mbryant,6/17/2010 21:28,3/12/2019 14:10,3/13/2019 22:24,7/1/2010 16:52,0.7 beta 1,,,,0,,,,,,"calling thriftClient_.system_drop_keyspace(keyspaceName) on a newly created keyspace, then stopping the node renders the node unstartable. Results in the following stacktrace:

10/06/17 14:23:16 ERROR thrift.CassandraDaemon: Fatal exception during initialization
java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at java.io.DataInputStream.readUTF(DataInputStream.java:592)
	at java.io.DataInputStream.readUTF(DataInputStream.java:547)
	at org.apache.cassandra.config.KSMetaData.deserialize(KSMetaData.java:92)
	at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:75)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:422)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:103)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:221)

my repro:

start new node with empty data directory
create a new keyspace
drop the keyspace
attempt to restart the node, notice that it fails to start.
",Mac OS X 10.6.3 (10D573) / Darwin 10.3.0,,,,,,,,,,,,,,,,,,,22/Jun/10 22:00;gdusbabek;0001-handle-schema-loading-when-a-node-has-had-all-keyspa.patch;https://issues.apache.org/jira/secure/attachment/12447752/0001-handle-schema-loading-when-a-node-has-had-all-keyspa.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,04:17.1,,,no_permission,,,,,,,,,,,,20033,,,Fri Jul 02 12:47:06 UTC 2010,,,,,,0|i0g3mn:,92018,,,,,,,,,,,"01/Jul/10 16:04;jbellis;+1

(s/No tables where/No schema definitions were/)","02/Jul/10 12:47;hudson;Integrated in Cassandra #483 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/483/])
    handle schema loading when a node has had all keyspaces dropped. Patch by gdusbabek, reviewed by jbellis. CASSANDRA-1203.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row iteration can stomp start-of-row mark,CASSANDRA-1130,12465398,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jigneshdhruv,jigneshdhruv,5/25/2010 18:13,3/12/2019 14:10,3/13/2019 22:24,6/14/2010 18:06,0.7 beta 1,,,,0,,,,,,"Hello,

I am trying to use TTL (timeToLive) feature in SuperColumns.
My usecase is:
- I have a SuperColumn and 3 subcolumns.
- I try to expire data after 60 seconds.

While Cassandra is up and running, I am successfully able to push and read data without any problems. Data compaction and all occurs fine. After inserting say about 100000 records, I stop Cassandra while data is still coming.

On startup Cassandra throws an exception and won't start up. (This happens 1 in every 3 times). Exception varies like:
- EOFException while reading data
- negative value encountered exception
- Heap Space Exception

Cassandra simply won't start up.

Again I get this problem only when I use TTL with SuperColumns. There are no issues with using TTL with regular Columns.

I tried to diagnose the problem and it seems to happen on startup when it sees a Column that is marked Deleted and its trying to read data. Its off by some bytes and hence all these exceptions.

Caused by: java.io.IOException: Corrupt (negative) value length encountered
        at org.apache.cassandra.utils.FBUtilities.readByteArray(FBUtilities.java:317)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:84)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:336)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:285)
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.getNextBlock(SSTableSliceIterator.java:235)
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.pollColumn(SSTableSliceIterator.java:195)
        ... 18 more


Let me know if you need more information.

Thanks,
Jignesh",,,,,,,,,,,,,,,,,,,,14/Jun/10 16:53;slebresne;0001-Allow-for-multiple-mark-on-a-file.patch;https://issues.apache.org/jira/secure/attachment/12447039/0001-Allow-for-multiple-mark-on-a-file.patch,14/Jun/10 16:53;slebresne;0002-Unit-test-for-row-iteration.patch;https://issues.apache.org/jira/secure/attachment/12447040/0002-Unit-test-for-row-iteration.patch,27/May/10 21:21;jigneshdhruv;TestSuperColumnTTL.java;https://issues.apache.org/jira/secure/attachment/12445706/TestSuperColumnTTL.java,27/May/10 20:22;slebresne;TestSuperColumnTTL.java;https://issues.apache.org/jira/secure/attachment/12445696/TestSuperColumnTTL.java,28/May/10 17:00;slebresne;cassandra_0.6-Allow_multiple_mark_on_file.diff;https://issues.apache.org/jira/secure/attachment/12445795/cassandra_0.6-Allow_multiple_mark_on_file.diff,,,,,,,,,5,,,,,,,,,,,,,,,,,,,51:10.6,,,no_permission,,,,,,,,,,,,20002,,,Tue Jun 22 14:08:08 UTC 2010,,,,,,0|i0g36f:,91945,,,,,,,,,,,"25/May/10 18:51;slebresne;If you have some script that allows to reproduce, that would be awesome. Alternatively, if you're able to locate the culprit data file and if the infos are not sensible, providing the file could help.

Are you using latest trunk ?","25/May/10 19:06;jigneshdhruv;Yes I am using the latest source code from trunk.

I have a small java application that deals with creating schema and populating data.

This is what I was able to debug till now:
The error occurs during deserialization in ColumnSerializer.

There is an extra int byte that needs to be read before ColumnSerializer.java:84. Value of this extra int byte is ""4"". Not sure what it stands for.

I am not sure from where that byte is set. After reading that byte, I get the localDeletionTime value.

Also this happens when the DELETION_MASK is set on a record. It works fine for records with EXPIRATION_MASK. I am thinking that converting a record from EXPIRY to DELETED is causing this error at startup or something like that.

Let me know if you need more information.

But you should be able to reproduce this if you have a SuperColumn and their subColumns with TTL. Stop and start cassandra after loading some data. It consistently fails to startup 1 out of every 3 times.

Jignesh
","25/May/10 20:37;jigneshdhruv;OK. I think I narrowed it down further..

The bug may be in ""org/apache/cassandra/db/filter/SSTableSliceIterator.java:getNextBlock() method.

See the while loop on line 233.
Out here, its reading 1 column at a time.

As I said before, the problem is when an Column of Type ExpiringColumn becomes DeletedColumn when time has expired.

In that case, once the Supercolumn whose subcolumns are of type ""DELETED"" are read in this while loop, there are some extra bytes that needs to be skipped but instead it goes in the second iteration in the while loop and tries to read the next column and thats where all the problem starts.

Shouldn't the while loop just read one Column at a time and then exit. That is what it does when it reads all the bytes. If I put a ""break statement"" in the end of while loop after reading a column all works fine as the extra bytes are skipped during the next read of a Column.

I am not sure what is the purpose of this while loop? but if we break after reading  1 column at a time, all works fine and cassandra starts up smoothly.

This looks similar to issue
https://issues.apache.org/jira/browse/CASSANDRA-1073

Jignesh",25/May/10 21:19;jigneshdhruv;I did some more testing with the patch that I suggested i.e. skipping extra bytes after reading the column and exiting the while loop worked fine. I am now able to load and start cassandra with SuperColumn data with TTL without any problems.,25/May/10 21:30;jbellis;this sounds like something you could build a unit test for?,"25/May/10 21:40;slebresne;But they shouldn't be extra bytes to skip. An expired column becomes a deletedColumn only 
after everything is deserialized. It is expected that we read all and everything that is written.

The while loop you're referring to is here to read all the column (or super columns) that falls 
into one given index range (the index is sparse). If you exit the while loop, you will just potentially 
skip some columns (super columns in your example). That it makes cassandra start may just be 
that it skips the problematic parts. 

I'll try to reproduce this tomorrow (but if you want and can share your test code to make that 
easier, feel free to :)). ","26/May/10 09:22;slebresne;Sorry but I don't seem able to reproduce this.
I've tried a simple test, inserting supercolumns with 100 colums in them, 
each having a TTL (that I varied from 10 seconds to like 3 minutes).
I typically let it insert over 10000 super columns  (so around 1 millions ttled 
columns) and kill it. I run cassandra again, let it compact, kill it again, run again, 
start insertion again, etc... I tried like 20 times, no crashes whatsoever.

I've tried with a trunk of a week or so ago and then with trunk from 1 hour ago. 
Sounds like you have no problem reproducing on your side so .. I don't know.

If you could somehow come up with a unit test that make it crashes or a small 
script test that triggers it, that would be amazing.","26/May/10 17:29;jigneshdhruv;I checked out the latest source code this morning and I am still able to reproduce it.

My usecase is:
- Start Cassandra
- Keep adding SuperColumns with 3 subcolumns within each SuperColumn. Each subcolumn expires in 35 seconds.
- Let cassandra run until you see statements  like ""Deleted  files""
- Stop cassandra and try to start and it will give you all the exceptions that I am talking about.

Also I believe ExpiringColumn contains some more data compared to DeletedColumn. Correct? In my testing I found that length of each DeletedColumn was similar to ExpiringColumn and once a complete DeletedColumn record was read there were some more extra bytes at the end of the record which is causing all this issue?

When you convert a ExpiringColumn to DeletedColumn, is it in place replacement or the old record is marked for deletion by just changing the EXPIRING_MASK to DELETED_MASK.

I will try to produce a junit test case. But one needs to still stop cassandra when one sees some files being deleted. At that point you will see the error that I am talking about.

Jignesh","27/May/10 20:22;slebresne;Sorry but I'm still unable to reproduce.

I'm attaching a small insert script (in java) that, as far as I can see,
seems to do what you say triggers the bug. Could you look if this fails 
on your side. If I haven't understand the steps correctly, would you mind 
updating this test so that it reproduce the bug you see ?
(the script uses raw thrift and requires a very recent trunk)

{quote}
Also I believe ExpiringColumn contains some more data compared to DeletedColumn. Correct? In my testing I found that length of each
DeletedColumn was similar to ExpiringColumn and once a complete DeletedColumn record was read there were some more extra bytes at the 
end of the record which is causing all this issue?
When you convert a ExpiringColumn to DeletedColumn, is it in place replacement or the old record is marked for deletion by just 
changing the EXPIRING_MASK to DELETED_MASK.
{quote}

File on disk are not updated in place. So we never change on disk an
ExpiringColumn to a DeletedColumn. There only is a small optimisation in the
deserialization code that, after having fully deserialize an ExpiringColumn,
will return an equivalent DeletedColumn if the column is expired. But we always
read exactly what we have written (or it's a bug).","27/May/10 21:21;jigneshdhruv;Hello,

I am still able to reproduce the problem consistently. I have attached my JUNIT Test case which reproduces it 1 out of 3 times.

Streps to Reproduce:
- Run it until it inserts atleaset 200000 records or until you see a ""Deleted files"" message on your console.
- Stop cassandra while the data is still coming in.
- Start Cassandra and you should get exceptions.

If you do not get exceptions, without deleting the index repeat the above steps.

Let me know if you are able to reproduce the problem.

Thanks,
Jignesh","27/May/10 22:08;slebresne;Ok, I'm able to reproduce with your test.
I'll have a look at it, thanks","28/May/10 16:01;slebresne;Attached fiie should fix the problem. 

The problem is unrelated to TTL per se but a problem in row iterations so the 
ticket title can be a bit misleading.
Citing irc: 
  ""a columnGroupReader mark the file when it is created. Then it reset() when getting a next block.
   but with the way the row iteration works, a new columnGroupReader is created (and mark the file) before 
   the previous one has retrieved it's block
  (it's because computeNext() create the next SSTableSliceIterator before getReduced() had retrieved the actual 
   column of the previous one)""
The patch allow for each columnGroupReader to have it's own mark on the file

Btw, I was unable to reproduce previously because it's the cache preloading that 
trigged the error and I did not use one in my first tests. 

Thanks Jignesh for helping find this one.",28/May/10 17:00;slebresne;Attached version of the patch rebased against 0.6,"28/May/10 17:32;slebresne;Attaching a unit test for trunk that fails without the patch and passes with it.

It doesn't fail in 0.6 though. I suspect this is because getRangeSlice doesn't 
work in the same way in 0.6. So I'm not sure how to reproduce in 0.6. But I'll 
have a better at how it works in 0.6 and see if I can have a unit test for it too.","28/May/10 17:54;jigneshdhruv;Excellent. I guess the changes are in trunk. I will check it out.

Thanks,
Jignesh",30/May/10 01:41;jbellis;This bug is only present in 0.7.,14/Jun/10 16:08;jbellis;Sorry for the delay applying.  Can you rebase to trunk please?,"14/Jun/10 16:53;slebresne;No problem, attaching rebased patches","14/Jun/10 18:06;jbellis;committed, thanks!","22/Jun/10 04:06;jbellis;weird, I totally remember committing this (and editing SSTNI to make it apply) but it didn't make it to svn.  Probably I forgot git-svn dcommit somehow.

really committed this time.","22/Jun/10 14:08;hudson;Integrated in Cassandra #473 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/473/])
    fix race condition in SSTable*Iterator.
patch by Sylvain Lebresne; reviewed by jbellis for CASSANDRA-1130
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lack of subcomparator_type will corrupt the keyspace in Thrift system_add_keyspace(),CASSANDRA-1122,12465309,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,arya,arya,5/24/2010 21:27,3/12/2019 14:10,3/13/2019 22:24,5/26/2010 19:04,0.7 beta 1,,,,0,,,,,,"I had a problem earlier where I create a Keyspace by reading the default yaml config shipped with the above cassandra package and after parsing and creating the keyspace with PHP Thrift system_add_keysapce command, I would get 'TException: Error: Internal error processing describe_keyspace ' when trying to get describe_keyspace. In cassandra-cli I get the same error.

The cassandra log shows a null pointed exception:
ERROR [pool-1-thread-39] 2010-05-24 14:17:35,204 Cassandra.java (line 1943) Internal error processing describe_keyspace
java.lang.NullPointerException
	at org.apache.cassandra.thrift.CassandraServer.describe_keyspace(CassandraServer.java:476)
	at org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspace.process(Cassandra.java:1939)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1276)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

I traced down the problem to where I define cassandra_CfDef. When the type is Super and subcomparator_type is not set, Thrift code does not set it to any default value but blank """". The command system_add_keyspace() runs with no problem. Whatever happens in Cassandra afterwards, will create a useless keyspace.

Here is my code snippet to generate the exception:

 <?php
$GLOBALS['THRIFT_ROOT'] = '/usr/share/php/Thrift';
require_once $GLOBALS['THRIFT_ROOT'].'/packages/cassandra/Cassandra.php';
require_once $GLOBALS['THRIFT_ROOT'].'/packages/cassandra/cassandra_types.php';
require_once $GLOBALS['THRIFT_ROOT'].'/transport/TSocket.php';
require_once $GLOBALS['THRIFT_ROOT'].'/protocol/TBinaryProtocol.php';
require_once $GLOBALS['THRIFT_ROOT'].'/transport/TFramedTransport.php';
require_once $GLOBALS['THRIFT_ROOT'].'/transport/TBufferedTransport.php';

try {
  // Make a connection to the Thrift interface to Cassandra
  $socket = new TSocket('127.0.0.1', 9160);
  $transport = new TBufferedTransport($socket, 1024, 1024);
  $protocol = new TBinaryProtocolAccelerated($transport);
  $client = new CassandraClient($protocol);
  $transport->open();

  //Try creating some keyspace/column family defs
  $ks = new cassandra_KsDef();
  $ks->name = 'agoudarzi_Keyspace1';
  $ks->strategy_class = 'org.apache.cassandra.locator.RackUnawareStrategy';
  $ks->replication_factor = '1';
  
  //Now add a column family to it
  $cf = new cassandra_CfDef();
  $cf->name = 'Super3';
  $cf->table = 'agoudarzi_Keyspace1';
  $cf->column_type = 'Super';
  $cf->comparator_type = 'LongType';
  $cf->row_cache_size = '0';
  $cf->key_cache_size = '50';
  $cf->comment = 'A column family with supercolumns, whose column names are Longs (8 bytes)';
  $ks->cf_defs[] = $cf;

  $client->system_add_keyspace($ks);
  
  sleep(2);
  
  //Try to check if keyspace description is good
  $client->set_keyspace('agoudarzi_Keyspace1');
  $rs = $client->describe_keyspace('agoudarzi_Keyspace1');

  $transport->close();

} catch (TException $tx) {
   print 'TException: '.$tx->why. ' Error: '.$tx->getMessage() . ""\n"";
}
?>

I think a default subcomparator should be set by thrift or other defensive method be used to prevent this problem.

Please investigate. 

Thanks.","CentOS 5.2 - Linux 2.6.18-164.15.1.el5 #1 SMP Wed Mar 17 11:30:06 EDT 2010 x86_64 x86_64 x86_64 GNU/Linux
apache-cassandra-2010-05-21_13-27-42
Thrift 2.0 with patch https://issues.apache.org/jira/browse/THRIFT-780
PHP 5.2",,,,,,,,,,,,,,,,,,,26/May/10 18:33;gdusbabek;0001-CFMeta.subcolumncomparator-should-default-to-BytesTy.patch;https://issues.apache.org/jira/secure/attachment/12445574/0001-CFMeta.subcolumncomparator-should-default-to-BytesTy.patch,24/May/10 21:29;arya;test_cass.php;https://issues.apache.org/jira/secure/attachment/12445388/test_cass.php,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,36:08.0,,,no_permission,,,,,,,,,,,,19999,,,Thu May 27 12:47:29 UTC 2010,,,,,,0|i0g34v:,91938,,,,,,,,,,,24/May/10 21:29;arya;The PHP Code to reproduce this exception. ,"26/May/10 18:36;gdusbabek;Before CASSANDRA-44, we assumed defaulted CF.subcolumcomparator to BytesType if it wasn't specified on a supercolumn.  We can't have a conditional default in thrift, so make it right on the server.

Includes a system test that could reproduce the error prior to the patch.",26/May/10 18:36;jbellis;+1,"27/May/10 12:47;hudson;Integrated in Cassandra #447 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/447/])
    CFMeta.subcolumncomparator should default to BytesType if cftype==Super. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1122
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dropping column families and keyspaces races with compaction and flushing,CASSANDRA-1631,12477746,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,10/19/2010 14:10,3/12/2019 14:10,3/13/2019 22:24,11/5/2010 21:12,0.7 beta 3,,,,0,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1585,,,,19/Oct/10 21:14;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-fix-drop-race-with-compaction.txt;https://issues.apache.org/jira/secure/attachment/12457596/ASF.LICENSE.NOT.GRANTED--v1-0001-fix-drop-race-with-compaction.txt,19/Oct/10 21:14;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0002-fix-drop-race-with-flush.txt;https://issues.apache.org/jira/secure/attachment/12457597/ASF.LICENSE.NOT.GRANTED--v1-0002-fix-drop-race-with-flush.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,29:24.7,,,no_permission,,,,,,,,,,,,20226,,,Fri Nov 05 21:12:51 UTC 2010,,,,,,0|i0g6f3:,92470,jbellis,jbellis,,,,,,,,,19/Oct/10 21:29;jbellis;i don't think 02 actually prevents an update that is making its way through the system pre-drop from causing a flush after the lock is released.  probably setting the memtable to frozen while we hold the lock would fix this.,"19/Oct/10 21:37;gdusbabek;The null check in CFS.maybeSwitchMemtable was intended to handle that, i.e., if the flush goes through after the drop, it ends up doing nothing because the CF def can't be found.","20/Oct/10 01:45;jbellis;ah, right.  +1","20/Oct/10 15:14;hudson;Integrated in Cassandra #571 (See [https://hudson.apache.org/hudson/job/Cassandra/571/])
    fix drop race with flush. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1631
fix drop race with compaction. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1631
","04/Nov/10 15:56;jbellis;This is still a bug with schema updates to an existing CF, since reloadCf is doing a unload/init cycle.  So flushing + compaction is an issue there as well.  Here is a stacktrace from during an index creation where it stubbed its toe on an incomplete sstable from an in-progress compaction (path names anonymized):

{code}
 INFO [CompactionExecutor:1] 2010-11-02 16:31:00,553 CompactionManager.java (line 224) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-6-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-7-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-8-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-9-Data.db')]
...
ERROR [MigrationStage:1] 2010-11-02 16:31:10,939 ColumnFamilyStore.java (line 244) Corrupt sstable Standard1-tmp-e-10-<>=[Data.db, Index.db]; skipped
java.io.EOFException
        at org.apache.cassandra.utils.FBUtilities.skipShortByteArray(FBUtilities.java:308)
        at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:231)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:286)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:202)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:235)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:443)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:431)
        at org.apache.cassandra.db.Table.initCf(Table.java:335)
        at org.apache.cassandra.db.Table.reloadCf(Table.java:343)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.applyModels(UpdateColumnFamily.java:89)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:158)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:672)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
...
 INFO [CompactionExecutor:1] 2010-11-02 16:31:31,970 CompactionManager.java (line 303) Compacted to Standard1-tmp-e-10-Data.db.  213,657,983 to 213,657,983 (~100% of original) bytes for 626,563 keys.  Time: 31,416ms.
{code}","04/Nov/10 19:57;gdusbabek;This isn't going to be as simple as it was last time (shifting work on to the CompactionManager and blocking).  Part of the unload/init code, you guessed it, shifts work of the CompactionManager and blocks (index creation to be precise).  

I'm looking into ways of doing this that don't involve deadlock or refactoring large pieces of code.  Part of the problem is that we've started treating the CompactionManager as a way to synchronize access to sstables.  The problem with that is that it is very course and the jobs submitted to it do a lot more things that pure FS work.",05/Nov/10 15:03;gdusbabek;Realized streaming can be affected by drop/renames as well.,05/Nov/10 21:12;jbellis;created CASSANDRA-1715 for these new bugs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra does not expire data after setting timeToLive argument for each value,CASSANDRA-1109,12464905,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jigneshdhruv,jigneshdhruv,jigneshdhruv,5/19/2010 15:57,3/12/2019 14:10,3/13/2019 22:24,5/19/2010 23:38,0.7 beta 1,,,,1,,,,,,"Hello,

I downloaded latest cassandra source code from svn trunk. I wanted to test expire data functionality. Using Thrift API, I set timeToLive parameter for each fieldValue, however cassandra ignored it and did not expire any data.

I debugged cassandra's source code and found a bug in src/java/org/apache/cassandra/db/RowMutation.java.

In RowMutation.addColumnOrSuperColumnToRowMutation() method, QueryPath was not setting timeToLive argument. I updated RowMutation.java locally and tested it and then my data expired after 'n' number of seconds.

I wanted to have this fix in the trunk also.

Index: src/java/org/apache/cassandra/db/RowMutation.java
===================================================================
--- src/java/org/apache/cassandra/db/RowMutation.java   (revision 946222)
+++ src/java/org/apache/cassandra/db/RowMutation.java   (working copy)
@@ -295,12 +295,12 @@
         {
             for (org.apache.cassandra.thrift.Column column : cosc.super_column.columns)
             {
-                rm.add(new QueryPath(cfName, cosc.super_column.name, column.name), column.value, column.timestamp);
+                rm.add(new QueryPath(cfName, cosc.super_column.name, column.name), column.value, column.timestamp, column.ttl);
             }
         }
         else
         {
-            rm.add(new QueryPath(cfName, null, cosc.column.name), cosc.column.value, cosc.column.timestamp);
+            rm.add(new QueryPath(cfName, null, cosc.column.name), cosc.column.value, cosc.column.timestamp, cosc.column.ttl);
         }
     }


Thanks,
Jignesh",,,,,,,,,,,,,,,,,,,,19/May/10 17:23;slebresne;0001-System-test-for-1109.patch;https://issues.apache.org/jira/secure/attachment/12444959/0001-System-test-for-1109.patch,19/May/10 16:02;jigneshdhruv;CASSANDRA-1109.patch;https://issues.apache.org/jira/secure/attachment/12444953/CASSANDRA-1109.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,58:32.2,,,no_permission,,,,,,,,,,,,19996,,,Thu May 20 12:42:38 UTC 2010,,,,,,0|i0g31z:,91925,,,,,,,,,,,19/May/10 16:02;jigneshdhruv;Patch Submitted.,"19/May/10 16:58;slebresne;Indeed, good catch. Thanks

+1 on the patch","19/May/10 17:23;slebresne;And because everything's look better with a system test, adding one",19/May/10 23:38;jbellis;committed.  thanks!,"20/May/10 12:42;hudson;Integrated in Cassandra #441 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/441/])
    test + fix for expiring columns.  patch by Jignesh Dhruv and Sylvain Lebresne for CASSANDRA-1109
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe_ring() throws on single node clusters and/or probably clusters without a replication factor,CASSANDRA-1111,12465029,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,dccwilliams,dccwilliams,5/20/2010 14:12,3/12/2019 14:10,3/13/2019 22:24,5/28/2010 19:28,0.6.3,0.7 beta 1,,,0,describe_ring,exception,replication,,,"You use Thrift to call describe_ring() on a cluster with only a single node. The Thrift connection is broken, and the system.log shows the exception that has been thrown:

ERROR [pool-1-thread-15] 2010-05-20 13:15:24,753 TThreadPoolServer.java (line 259) Error occurred during processing of message.
java.lang.RuntimeException: No replica strategy configured for L1AbuseReports
        at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:246)
        at org.apache.cassandra.service.StorageService.constructRangeToEndPointMap(StorageService.java:457)
        at org.apache.cassandra.service.StorageService.getRangeToAddressMap(StorageService.java:443)
        at org.apache.cassandra.service.StorageService.getRangeToEndPointMap(StorageService.java:433)
        at org.apache.cassandra.thrift.CassandraServer.describe_ring(CassandraServer.java:628)
        at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.process(Cassandra.java:1781)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1125)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:637)",Mac OS X Snow Leopard,,2700,2700,,0%,2700,2700,,,,,,,,,,,,28/May/10 18:28;gdusbabek;1111-0.6.txt;https://issues.apache.org/jira/secure/attachment/12445800/1111-0.6.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:06.2,,,no_permission,,,,,,,,,,,,687,,,Mon May 31 09:59:46 UTC 2010,,,,,,0|i0g32f:,91927,,,,,,,,,,,"21/May/10 16:27;gdusbabek;I'm having difficulty reproducing this problem in branches/cassandra-0.6.  What I comment out <ReplicaPlacementStrategy>, I get the following error on startup:

ERROR 11:26:08,593 Fatal error: Missing replicaplacementstrategy directive for Keyspace1
Bad configuration; unable to start server

I get a similar error when I configure a replicaplacementstrategy, but leave the replicationfactor out.

Is it possible to work up a short test that duplicates this problem?",24/May/10 14:31;gdusbabek;Closing until we better understand how to duplicate the problem.,"27/May/10 04:48;tholzer;I can reproduce this with the following:

Linux & Python 2.6.4

{noformat}
from thrift.transport.TSocket import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocol
from cassandra.Cassandra import Client

if __name__ == ""__main__"":
    tsocket = TSocket('localhost', 9160)
    tsocket.open()
    tprotocol = TBinaryProtocol(tsocket)
    client = Client(tprotocol)
    keyspaces = client.describe_keyspaces()
    for keyspace in keyspaces:
        print ""%s"" % client.describe_ring(keyspace)
{noformat}

Output:
{noformat}
[TokenRange(end_token='107294900513650794844962875501795914878', start_token='107294900513650794844962875501795914878', endpoints=['127.0.0.1'])]
Traceback (most recent call last):
  File ""test/t1.py"", line 13, in <module>
    print ""%s"" % client.describe_ring(keyspace)
  File ""cassandra/Cassandra.py"", line 964, in describe_ring
    return self.recv_describe_ring()
  File ""cassandra/Cassandra.py"", line 975, in recv_describe_ring
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
  File ""thrift/protocol/TBinaryProtocol.py"", line 126, in readMessageBegin
    sz = self.readI32()
  File ""thrift/protocol/TBinaryProtocol.py"", line 203, in readI32
    buff = self.trans.readAll(4)
  File ""thrift/transport/TTransport.py"", line 58, in readAll
    chunk = self.read(sz-have)
  File ""thrift/transport/TSocket.py"", line 94, in read
    raise TTransportException(type=TTransportException.END_OF_FILE, message='TSocket read 0 bytes')
thrift.transport.TTransport.TTransportException: TSocket read 0 bytes
{noformat}

Server log:
{noformat}
ERROR 16:40:55,288 Error occurred during processing of message.
java.lang.RuntimeException: No replica strategy configured for system
        at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:246)
        at org.apache.cassandra.service.StorageService.constructRangeToEndPointMap(StorageService.java:457)
        at org.apache.cassandra.service.StorageService.getRangeToAddressMap(StorageService.java:443)
        at org.apache.cassandra.service.StorageService.getRangeToEndPointMap(StorageService.java:433)
        at org.apache.cassandra.thrift.CassandraServer.describe_ring(CassandraServer.java:628)
        at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.process(Cassandra.java:1781)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1125)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
{noformat}
",27/May/10 12:48;gdusbabek;Will re-examine using supplied steps.,"28/May/10 18:29;gdusbabek;describe_ring(""system"") should be treated as an invalid request since that keyspace has no ring.",28/May/10 18:47;jbellis;+1,"31/May/10 09:59;dccwilliams;Hi, fwiw was planning to use describe_ring() in new Pelops Java client library to discover the nodes in a cluster i.e. connection pool starts with a few known nodes then discovers all.

Problem occurred on single node setup on my laptop but I'm guessing that without this working there would be no way to ""know"" there is only one node.

In such cases TokenRange will always map to 1/the same node, but that's still useful information, at least for Pelops",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyRecordReader returns duplicate rows,CASSANDRA-1042,12463494,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,joosto,joosto,5/1/2010 16:18,3/12/2019 14:10,3/13/2019 22:24,7/19/2010 19:53,0.6.5,,,,0,hadoop,mapreduce,,,,"There's a bug in ColumnFamilyRecordReader that appears when processing a single split (which happens in most tests that have small number of rows), and potentially in other cases.  When the start and end tokens of the split are equal, duplicate rows can be returned.

Example with 5 rows:
token (start and end) = 53193025635115934196771903670925341736

Tokens returned by first get_range_slices iteration (all 5 rows):
 16955237001963240173058271559858726497
 40670782773005619916245995581909898190
 99079589977253916124855502156832923443
 144992942750327304334463589818972416113
 166860289390734216023086131251507064403

Tokens returned by next iteration (first token is last token from
previous, end token is unchanged)
 16955237001963240173058271559858726497
 40670782773005619916245995581909898190

Tokens returned by final iteration  (first token is last token from
previous, end token is unchanged)
 [] (empty)

In this example, the mapper has processed 7 rows in total, 2 of which
were duplicates.

",,,,,,,,,,,,,,,,,,,,25/Jun/10 23:41;jeromatron;1042-0_6.txt;https://issues.apache.org/jira/secure/attachment/12448109/1042-0_6.txt,10/Jul/10 19:30;jbellis;1042-test.txt;https://issues.apache.org/jira/secure/attachment/12449166/1042-test.txt,19/Jul/10 15:49;jbellis;1042-v2.txt;https://issues.apache.org/jira/secure/attachment/12449851/1042-v2.txt,27/May/10 16:29;jeromatron;CASSANDRA-1042-trunk.patch.txt;https://issues.apache.org/jira/secure/attachment/12445670/CASSANDRA-1042-trunk.patch.txt,27/May/10 16:29;jeromatron;Cassandra-1042-0_6-branch.patch.txt;https://issues.apache.org/jira/secure/attachment/12445671/Cassandra-1042-0_6-branch.patch.txt,20/May/10 23:32;jeromatron;cassandra.tar.gz;https://issues.apache.org/jira/secure/attachment/12445117/cassandra.tar.gz,01/Jul/10 16:33;jeromatron;duplicate_keys.rtf;https://issues.apache.org/jira/secure/attachment/12448502/duplicate_keys.rtf,,,,,,,7,,,,,,,,,,,,,,,,,,,02:11.6,,,no_permission,,,,,,,,,,,,19970,,,Thu Aug 05 12:40:54 UTC 2010,,,,,,0|i0g2nb:,91859,,,,,,,,,,,"01/May/10 17:02;jbellis;This sounds like something we could make a unit test for, without having to get Hadoop itself involved.","03/May/10 16:38;cbiocca;The basic issue is that the thrift server's return value is sorted by the absolute value of the tokens, while the CassandraRecordReader assumes that the order is the one given by traversal of the range (that is, we get the smallest value greater than start_token in first position, and the greatest value smaller than or equal to end_token in last position. 
Now I don't know which is correct, as the API docs I've looked at don't suggest which order is supposed to be returned, but if the server's implementation is correct, then the record reader needs to iterate over the returned tokens to figure out which one is actually the last token for iteration purposes. Otherwise, switching the server's implementation to return keys in the iteration order will work.","14/May/10 15:02;jeromatron;Hmm, I was able to reproduce this with the contrib/word_count piece on trunk.  It appears to double count rows in ranges that have a single row as well as those that have more - in this case 1000.","14/May/10 22:58;jeromatron;Just some more data - when I use an OrderPreservingPartioner, the word count works fine...","17/May/10 22:25;jeromatron;For the describe_splits call it makes, it returns 3 sub splits, the first of which is a wrapping split.  Sounds like it's buggy on the server side.  Will check with Jonathan.","18/May/10 15:11;jeromatron;Jonathan:

Inputs to client.describe_splits() - ColumnFamilyInputSplit:185:
range.start_token: 85469146195799762548951268272529359452
range.end_token: 85469146195799762548951268272529359452
splitsize: 65536

output:
splits - arraylist<String>:
0: 85469146195799762548951268272529359452
1: 85469146195799762548951268272529359452

Seems like that is the bug right there, but not familiar enough with what it's supposed to do in that case?

Btw, there are only 4 rows in the CF.",18/May/10 21:45;jeromatron;Appears to be something server related in the splits themselves.,"20/May/10 23:32;jeromatron;In order to facilitate reproducing the problem, I'm attaching my cassandra data directory tar/gzed up.

There are 4 rows in the cassandra instance.  If you modify WordCountSetup to change TEST_COUNT from 4 to 1, then run WordCount, you will find that Cassandra trunk will count 7 occurrences instead of 4.  You can also debug on the line I mentioned previously to see what describe_splits receives and then outputs.

Just wanted to facilitate reproducing the problem.","26/May/10 23:05;jeromatron;Adding a patch that does the following:

1. Removes an ordering section in StorageProxy that messes with the wrapping range for a get_range_slice call - thereby messing up the order of the records returned.  That led to having the initial wrapping range returned in token order instead of wrapping order.  So there was a second call going from last token as far as natural ordering goes, all the way to the initial start token.  So if the server's token were 5, and there were 10 tokens, it would list 1-10, then 1-5 again.  With this fix, the return order of the tokens is 6-10, then 1-5, which is correct - the order of the wrapped range, then in token order.

2. A few instances of token.toString should have been TokenFactory.toString(token) - fixed.

3. There was a method in StorageService - getStringEndpointMap - that is never call - removed that.

4. Updated WordCountSetup with the latest trunk to use new Clock(System.currentTimeMillis())",26/May/10 23:08;jeromatron;Tx to Stu Hood for helping me narrow this down.,26/May/10 23:12;jeromatron;To clarify: to fix the problem - this removes some ordering in StorageProxy.getRangeIterator since getRestricedRanges should already have returned the right thing.,"27/May/10 02:03;jbellis;Looks good to me.  Nice work, Jeremy and Stu.

Can you submit a version against 0.6 branch too?",27/May/10 16:29;jeromatron;Attaching patches for 0.6-branch and trunk.,27/May/10 16:31;jeromatron;Jonathan - I updated the trunk patch to not add a couple of unused imports that snuck in while I was messing with WordCount.,27/May/10 16:34;jeromatron;Also - I didn't remove StorageService.getStringEndpointMap in the 0.6 branch version because CassandraServer.get_string_property still calls it.  get_string_property was removed on trunk as part of CASSANDRA-965,"27/May/10 18:59;jbellis;committed, thanks!",27/May/10 19:50;jbellis;done,25/Jun/10 18:51;jbellis;re-opening in light of CASSANDRA-1198,25/Jun/10 23:41;jeromatron;Unwrapped the tokens in the first place ensuring that the splits would not contain wraps.  Works fine now.,25/Jun/10 23:41;jeromatron;Attaching new patch.,25/Jun/10 23:43;jeromatron;The patch should apply cleanly to 0.7/trunk as well,26/Jun/10 00:30;stuhood;+1,"26/Jun/10 00:33;jbellis;do we have a theory as to why wrapped ranges should cause bugs?

i worry that if we're just trying code out and it seems to work, that we may not be fixing the real problem","26/Jun/10 00:48;jeromatron;Good point.

From what I could tell in this instance, it would go through the input splits and on the last input split, it would have an incorrect last value.  So it would go back through and take that value to the end of the input list.  I would imagine that is where it had wrapped.  I'm not sure why it had the incorrect last value as the last value in the wrapped input split though.  If someone is wiser than I in these matters, please chime in.  But it appears that normalizing how the splits are done so one split does not wrap internally, it solves the problem.

To reproduce easily and with a small dataset: If you don't apply the patch and run the word_count_setup with only 10 values for text3, usually that will be enough to manifest the problem when running wordcount.

Also, I might think that if the wrap could be detected when creating the splits, as with this patch, then it makes sense that wrapping could be detected when reading the rows in the ColumnFamilyRecordReader.  That could be another way to resolve it.  But I think it's sixes when it comes to the solution.

Like I said, I'm not certain why that incorrect ordering happens on the wrapped split.",01/Jul/10 16:33;jeromatron;Adding the output of word count with duplicate tokens.  It appears to happen when the input split contains a wrapped key range.  That's why the updated patch splits wrapped key ranges (fixing the problem).,"06/Jul/10 20:16;jeromatron;Sorry if this is redundant but pasting in a thought we had a while ago that motivated the attached patch.  If we make sure that the splits are always in ring order and never wrap, it solves the problem.

""Token ranges may also wrap -- that is, the end token may be less than the start one. Thus, a range from keyX to keyX is a one-element range, but a range from tokenY to tokenY is the full ring.""

It does not say what order they will be in when it wraps.  Some clients assume that the ordering is natural order while the hadoop client interactions assume that it will be ring order.

For example:
-- a list of tokens (1,2,3,4,5,6,7,8,9)
-- a get_range_slice call with start_token = 5, end_token = 5
Natural order meaning token order from start to finish, returning the results (1,2,3,4,5,6,7.8,9).
Ring order or wrapping order meaning it would return the results (5,6,7,8,9,1,2,3,4).","06/Jul/10 20:20;jbellis;the ""correct"" order when tokens are involved is ring order

(when start_key is used instead of start_token, you can't have a wrapping range so it should be moot)","10/Jul/10 18:07;jbellis;patching CFIF isn't the answer, we need any client using the API to get the right results","10/Jul/10 19:30;jbellis;it seems that the root of the problem is, as Jeremy said, rows getting returned in token order instead of ring order.  if, in joost's original example, the rows were returned in order of

99079589977253916124855502156832923443
144992942750327304334463589818972416113
166860289390734216023086131251507064403
16955237001963240173058271559858726497
40670782773005619916245995581909898190

then doing an extra query for (40670782773005619916245995581909898190, 53193025635115934196771903670925341736]

would return the desired result of nothing.

but I am unable to reproduce this behavior in a unit test (against 0.6 branch, attached).  trying jeremy's data dir (also against 0.6 branch), I get ""java.io.IOException: Found system table files, but they couldn't be loaded. Did you change the partitioner?"" ","12/Jul/10 01:11;jbellis;Jeremy pointed out that the sorting that removed by the original patch here is sorting in raw token order rather than taking into account the requested start token.  I think that's our problem, although I'm not sure why my unit test isn't running into that.","19/Jul/10 14:41;jbellis;ah, the unit test hits CFS directly instead of going through StorageProxy (where the sort happens)...","19/Jul/10 15:49;jbellis;v2 attached:

 - removes wrapped-range handling from CFS.getRangeSlice, since StorageProxy always unwraps first
 - adds (initially failing) system test exercising wrapped-range path
 - adds sorting of unwrapped, restricted ranges relative to the original query range [this is the bug fix]
",19/Jul/10 19:38;jeromatron;+1,19/Jul/10 19:53;jbellis;committed,05/Aug/10 12:40;jbellis;Backported a related fix from CASSANDRA-1156 to 0.6.5 in r982580,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid Response count 4,CASSANDRA-937,12460750,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,3/30/2010 22:00,3/12/2019 14:10,3/13/2019 22:24,4/1/2010 1:48,0.6.1,,,,0,,,,,,"2010-03-30_21:59:04.64973 ERROR - Error in ThreadPoolExecutor
2010-03-30_21:59:04.64973 java.lang.AssertionError: invalid response count 4
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ReadResponseResolver.<init>(ReadResponseResolver.java:54)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.doReadRepair(ConsistencyManager.java:89)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:75)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:60)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
2010-03-30_21:59:04.64973 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2010-03-30_21:59:04.64973 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2010-03-30_21:59:04.64973 	at java.lang.Thread.run(Thread.java:636)
2010-03-30_21:59:04.64973 ERROR - Fatal exception in thread Thread[RESPONSE-STAGE:5,5,main]
2010-03-30_21:59:04.64973 java.lang.AssertionError: invalid response count 4
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ReadResponseResolver.<init>(ReadResponseResolver.java:54)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.doReadRepair(ConsistencyManager.java:89)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.handleDigestResponses(ConsistencyManager.java:75)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.service.ConsistencyManager$DigestResponseHandler.response(ConsistencyManager.java:60)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:35)
2010-03-30_21:59:04.64973 	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
2010-03-30_21:59:04.64973 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2010-03-30_21:59:04.64973 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2010-03-30_21:59:04.64973 	at java.lang.Thread.run(Thread.java:636)
",replication factor is set to 3,,,,,,,,,,,,,,,,,,,31/Mar/10 20:51;jbellis;937-2.txt;https://issues.apache.org/jira/secure/attachment/12440403/937-2.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,05:37.6,,,no_permission,,,,,,,,,,,,19927,,,Thu Apr 01 01:48:24 UTC 2010,,,,,,0|i0g1zz:,91754,,,,,,,,,,,30/Mar/10 22:05;jbellis;were you moving nodes around at all?,"30/Mar/10 22:09;dispalt;No, just restarting machines","30/Mar/10 22:20;jbellis;I think this in doReadRepair is the culprit

            replicas_.add(FBUtilities.getLocalAddress());

to make up for

                endpoints.remove(FBUtilities.getLocalAddress());

in ReadVerbHandler.

The problem is, in the case where local node isn't actually a replica for the read in question it will cause this bug.","31/Mar/10 20:51;jbellis;Repair is always invoked by a node that has a copy of the data, or thinks it should (that is how it gets the ""initial row"" to compare digests against), but if the definition of who should have replicas changes either from bootstrap or during rebuilding of the ring after a cluster restart then local node could be an ""extra"" replica at repair time.

This patch changes repair to keep the replica set constant throughout the process.  It also fixes repair to start comparing digests immediately rather than waiting for all responses first for no reason, and avoids an extra read from the local replica, re-using the one that we've been comparing digests with.",31/Mar/10 21:27;gdusbabek;looks good. +1,01/Apr/10 01:48;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spurious Gossip Up/Down and IO Errors,CASSANDRA-800,12456474,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,kingryan,kingryan,2/16/2010 19:48,3/12/2019 14:10,3/13/2019 22:24,2/22/2010 16:36,0.5,,,,0,,,,,,"We're seeing a lot of nodes flapping. It appears to possibly be a race condition in Gossip.

on 10.209.23.110

WARN [MESSAGING-SERVICE-POOL:2] 2010-02-13 01:18:22,976 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.209.23.110:7000 remote=/10.209.23.80:52720]
WARN [MESSAGING-SERVICE-POOL:1] 2010-02-13 01:18:22,976 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.209.23.110:7000 remote=/10.209.23.80:36128]
 WARN [MESSAGING-SERVICE-POOL:2] 2010-02-13 01:18:22,977 TcpConnection.java (line 485) Exception was generated at : 02/13/2010 01:18:22 on thread MESSAGING-SERVICE-POOL:2
Reached an EOL or something bizzare occured. Reading from: /10.209.23.80 BufferSizeRemaining: 16
java.io.IOException: Reached an EOL or something bizzare occured. Reading from: /10.209.23.80 BufferSizeRemaining: 16
    at org.apache.cassandra.net.io.StartState.doRead(StartState.java:44)
    at org.apache.cassandra.net.io.ProtocolState.read(ProtocolState.java:39)
    at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:95)
    at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:445)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)


on 10.209.23.80 about the same time


ERROR [pool-1-thread-4751] 2010-02-13 01:17:12,261 Cassandra.java (line 1096) Internal error processing batch_insert
java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:848)
    at java.util.HashMap$KeyIterator.next(HashMap.java:883)
    at java.util.AbstractCollection.addAll(AbstractCollection.java:305)
    at java.util.HashSet.<init>(HashSet.java:100)
    at org.apache.cassandra.gms.Gossiper.getLiveMembers(Gossiper.java:173)
    at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:120)
    at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:78)
    at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1186)
    at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
    at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
    at org.apache.cassandra.service.CassandraServer.batch_insert(CassandraServer.java:445)
    at org.apache.cassandra.service.Cassandra$Processor$batch_insert.process(Cassandra.java:1088)
    at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)


just before that:

INFO [Timer-1] 2010-02-13 01:17:12,070 Gossiper.java (line 194) InetAddress /10.209.21.223 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,257 Gossiper.java (line 194) InetAddress /10.209.21.217 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,257 Gossiper.java (line 194) InetAddress /10.209.21.216 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,258 Gossiper.java (line 194) InetAddress /10.209.21.215 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,258 Gossiper.java (line 194) InetAddress /10.209.23.82 is now dead.


and just after that:

INFO [Timer-1] 2010-02-13 01:17:12,261 Gossiper.java (line 194) InetAddress /10.209.23.81 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,293 Gossiper.java (line 194) InetAddress /10.209.23.79 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,304 Gossiper.java (line 194) InetAddress /10.209.21.204 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,307 Gossiper.java (line 194) InetAddress /10.209.21.197 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,308 Gossiper.java (line 194) InetAddress /10.209.21.245 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,309 Gossiper.java (line 194) InetAddress /10.209.21.242 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,310 Gossiper.java (line 194) InetAddress /10.209.23.106 is now dead.
INFO [GMFD:1] 2010-02-13 01:17:26,780 Log4jLogger.java (line 41) 02/13/2010 01:17:26 - Remaining bytes zero. Stopping deserialization in EndPointState.
INFO [GMFD:1] 2010-02-13 01:17:26,784 Gossiper.java (line 543) InetAddress /10.209.21.204 is now UP
INFO [GMFD:1] 2010-02-13 01:17:26,785 Gossiper.java (line 543) InetAddress /10.209.23.106 is now UP
INFO [GMFD:1] 2010-02-13 01:17:26,786 Gossiper.java (line 543) InetAddress /10.209.21.197 is now UP
INFO [GMFD:1] 2010-02-13 01:17:26,800 Gossiper.java (line 543) InetAddress /10.209.21.216 is now UP
INFO [GMFD:1] 2010-02-13 01:17:41,808 Gossiper.java (line 543) InetAddress /10.209.21.217 is now UP
INFO [GMFD:1] 2010-02-13 01:17:41,823 Gossiper.java (line 543) InetAddress /10.209.21.223 is now UP
INFO [GMFD:1] 2010-02-13 01:17:41,823 Gossiper.java (line 543) InetAddress /10.209.21.215 is now UP


We're on 298a0e66ba66c5d2a1e5d4a70f2f619ae3fbf72a from git.apache.org, which claims to be:

git-svn-id: https://svn.apache.org/repos/asf/incubator/cassandra/branches/cassandra-0.5@9035",,,,,,,,,,,,,,,,,,,,21/Feb/10 12:30;jbellis;800.txt;https://issues.apache.org/jira/secure/attachment/12436488/800.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,53:54.0,,,no_permission,,,,,,,,,,,,19869,,,Mon Feb 22 16:36:00 UTC 2010,,,,,,0|i0g15j:,91617,,,,,,,,,,,"16/Feb/10 19:53;jbellis;the IOException is the same as #657 and is harmless (fixed in trunk, not going to be fixed in 0.5).

the ConcurrentModificationException  may be causing the deadness problem.  (it also might be related to CASSANDRA-757 but the stacktrace is different.)","17/Feb/10 02:41;jbellis;Ryan added in IRC:

""this may be the root of the problems I was describing above -- some threads may be dying due to OOM""

I'm skeptical that OOM could cause CME though.","17/Feb/10 03:40;kingryan;I'm skeptical about it too, but I've seen stranger effects from OOME's. We've made some config changes to (hopefully) reduce heap size pressure. I'll let you know if that improves the situation or now.","17/Feb/10 20:15;kingryan;After reducing the heap pressure these errors appear to have gone away. I think it would be reasonable to attribute this behavior to hitting OOME's, which killed some threads, but not all of them.

I think it would be best to kill the server when we hit an OOME.","17/Feb/10 20:46;gdusbabek;OOME would appear in the logs.  I think the JVM is loaded and isn't making the right decisions about which threads to service.  I've been able to duplicate these exact errors on my dev machine when I spin up 4 cassandra instances, set RF=3 and send it a heavy write load.

The gossip thread is rather important in that if it gets stalled when the node isn't really down and the cluster is under a heavy write load, it could lead to writes getting sent to other (already loaded) nodes causing a cascade of failures.  

We might want to see about putting it in it's own thread group and giving it a higher priority.","17/Feb/10 21:04;jbellis;Gary: CME is a correctness issue though, priority shouldn't affect that.

Ryan: OOME is indeed configured to kill the server by default in CassandraDaemon:

            public void uncaughtException(Thread t, Throwable e)
            {
                logger.error(""Fatal exception in thread "" + t, e);
                if (e instanceof OutOfMemoryError)
                {
                    System.exit(100);
                }

if we have a catch-all statement somewhere that is neutering that, the stacktrace should make that obvious.","17/Feb/10 21:35;kingryan;gary-

we did see OOME's in the logs:

ERROR [pool-1-thread-5016] 2010-02-13 02:50:05,872 CassandraDaemon.java (line 71) Fatal exception in thread Thread[pool-1-thread-5016,5,main]
java.lang.OutOfMemoryError: Java heap space
 WARN [MESSAGING-SERVICE-POOL:2] 2010-02-13 02:46:24,194 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.209.23.111:7000 remote=/10.209.23.84:37322]
ERROR [pool-1-thread-4994] 2010-02-13 02:45:29,807 CassandraDaemon.java (line 71) Fatal exception in thread Thread[pool-1-thread-4994,5,main]
java.lang.OutOfMemoryError: Java heap space
ERROR [main] 2010-02-13 02:45:17,044 CassandraDaemon.java (line 184) Exception encountered during startup.
ERROR [MESSAGE-DESERIALIZER-POOL:1] 2010-02-13 02:45:17,044 DebuggableThreadPoolExecutor.java (line 162) Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:154)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.OutOfMemoryError: Java heap space",21/Feb/10 12:30;jbellis;Fix for OOME not killing the server attached,"22/Feb/10 16:36;jbellis;i'm going to close this as a dupe of CASSANDRA-757 even though they are different errors, since the right fix for 757 will be using a concurrent structure, which will fix any other CMEs too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cannot get_slice from CF defined with CompareWith=""TimeUUIDType""",CASSANDRA-377,12433405,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,eweaver,eweaver,8/19/2009 1:53,3/12/2019 14:10,3/13/2019 22:24,8/19/2009 16:28,,,,,0,,,,,,"      <Keyspace Name=""Multiblog"">      
        <KeysCachedFraction>0.01</KeysCachedFraction>
        <ColumnFamily CompareWith=""TimeUUIDType"" Name=""Blogs""/>
        <ColumnFamily CompareWith=""TimeUUIDType"" Name=""Comments""/>
      </Keyspace>

>> multiblog.insert(:Comments, ""test"", {UUID.new => 'I like this cat'})
=> nil
>> multiblog.get(:Comments, ""test"")
Thrift::ApplicationException: Thrift::ApplicationException

Server said:

DEBUG - get_slice_from
ERROR - Internal error processing get_slice
org.apache.cassandra.db.marshal.MarshalException: UUIDs must be exactly 16 bytes
	at org.apache.cassandra.db.marshal.TimeUUIDType.getString(TimeUUIDType.java:48)
	at org.apache.cassandra.db.SliceFromReadCommand.toString(SliceFromReadCommand.java:71)
	at java.lang.String.valueOf(String.java:2827)
	at java.lang.StringBuilder.append(StringBuilder.java:115)
	at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:602)
	at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:320)
	at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:92)
	at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:173)
	at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:213)
	at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:572)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:560)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
",,,,,,,,,,,,,,,,,,,,19/Aug/09 15:06;eweaver;CASSANDRA-377.diff;https://issues.apache.org/jira/secure/attachment/12417025/CASSANDRA-377.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,02:58.2,,,no_permission,,,,,,,,,,,,19659,,,Thu Aug 20 13:05:50 UTC 2009,,,,,,0|i0fykf:,91198,,,,,,,,,,,19/Aug/09 01:55;eweaver;Maybe start/finish aren't allowed to be empty strings anymore?,"19/Aug/09 01:59;eweaver;Yeah...let me know if

get_slice(""Multiblog"", ""test"", <CassandraThrift::ColumnParent column_family:""Comments"">, <CassandraThrift::SlicePredicate slice_range:<CassandraThrift::SliceRange start:"""", finish:"""", reversed:false, count:100>>, 1)

is supposed to work","19/Aug/09 02:05;eweaver;if I send nil:

<CassandraThrift::SlicePredicate slice_range:<CassandraThrift::SliceRange start:nil, finish:nil, reversed:false, count:100>>

I get:

ERROR - Internal error processing get_slice
java.lang.NullPointerException
	at org.apache.cassandra.db.marshal.TimeUUIDType.getString(TimeUUIDType.java:46)
	at org.apache.cassandra.db.SliceFromReadCommand.toString(SliceFromReadCommand.java:71)
	at java.lang.String.valueOf(String.java:2827)
	at java.lang.StringBuilder.append(StringBuilder.java:115)
",19/Aug/09 03:02;jbellis;looks like setting logger to INFO will work around this so it doesn't try to tostring an empty start,"19/Aug/09 03:33;eweaver;oh, it's the logger killing it? easy fix i guess. will try.","19/Aug/09 16:28;jbellis;Applied, with similar fixes for LexicalUUID and Long types.  Also fixed the Command toStrings to use the subcolumn comparators when toStringing slices of subcolumns (that is why the test suite didn't hit this bug before).","20/Aug/09 13:05;hudson;Integrated in Cassandra #173 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/173/])
    use the subcomparator when toStringing slice commands on subcolumns.  this exposes a couple bugs: fix getString in non-string types to accept byte[0], and fix a test to send a long to a LongType subcolumn.  patch by jbellis and Evan Weaver for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InstanceAlreadyExistsException,CASSANDRA-650,12444066,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,12/23/2009 6:18,3/12/2019 14:10,3/13/2019 22:24,12/23/2009 15:02,0.5,,,,0,,,,,,"Was testing out 05 last night, and got this error, can't boot up a node because of it. I'll be happy to debug if more info is needed, it's quite late here.

ERROR [main] 2009-12-22 21:52:14,167 StorageService.java (line 129) Exception was generated at : 12/22/2009 21:52:14 on thread main
javax.management.InstanceAlreadyExistsException: org.apache.cassandra.concurrent:type=CONSISTENCY-MANAGER
java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.concurrent:type=CONSISTENCY-MANAGER
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.<init>(DebuggableThreadPoolExecutor.java:63)
        at org.apache.cassandra.service.StorageService.<init>(StorageService.java:151)
        at org.apache.cassandra.service.StorageService.instance(StorageService.java:125)
        at org.apache.cassandra.locator.RackAwareStrategy.<init>(RackAwareStrategy.java:47)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:244)
        at org.apache.cassandra.service.StorageService.<init>(StorageService.java:233)
        at org.apache.cassandra.service.StorageService.instance(StorageService.java:125)
        at org.apache.cassandra.service.CassandraServer.<init>(CassandraServer.java:58)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:93)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:135)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.concurrent:type=CONSISTENCY-MANAGER
        at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:453)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1484)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:963)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.<init>(DebuggableThreadPoolExecutor.java:59)
        ... 18 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,02:36.5,,,no_permission,,,,,,,,,,,,19802,,,Wed Dec 23 15:02:36 UTC 2009,,,,,,0|i0g08n:,91469,,,,,,,,,,,23/Dec/09 15:02;jbellis;fixed in r893539.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Doing a descending range still returns columns in ascending order,CASSANDRA-196,12426123,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,jbellis,jbellis,5/22/2009 15:25,3/12/2019 14:10,3/13/2019 22:24,7/22/2009 21:14,0.4,,,,0,,,,,,"If I do

        result = table.getSliceFrom(row, ""Standard1:col5"", false, 2);
        cf = result.getColumnFamily(""Standard1"");

I expect to get back columns in the order 5, 4, 3 (at the thrift level, it's turned into a list) but instead I get 3, 4, 5 because using a CF as the return vehicle re-sorts them by the standard comparator.

The simplest solution is to allow user-defined column ordering as in CASSANDRA-185 and always return columns in that order (i.e., remove the ascending bool).  This also allows us to make the columngroup fetching more efficient in the best case (deserializing one column at a time instead of a group at a time).

OTOH using one index to allow fetching items relatively efficiently in either directly is cool.  But my gut says it's relatively uncommon to want to access in both directions at once, and even more uncommon to not be able to do a reverse() on the client side (because of data volume, for instance).  Forcing a separate CF for this special case of a special case might be worth the tradeoff.",,,,,,,,,,,,,,,,,,,,07/Jul/09 20:13;sandeep_tata;196-systest.patch;https://issues.apache.org/jira/secure/attachment/12412781/196-systest.patch,21/Jul/09 16:11;junrao;issue196.patchv3;https://issues.apache.org/jira/secure/attachment/12414111/issue196.patchv3,22/Jul/09 16:32;junrao;issue196.patchv4;https://issues.apache.org/jira/secure/attachment/12414221/issue196.patchv4,20/Jul/09 20:33;junrao;issue_196.patchv2;https://issues.apache.org/jira/secure/attachment/12414033/issue_196.patchv2,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,11:25.0,,,no_permission,,,,,,,,,,,,19589,,,Wed Jul 22 21:14:02 UTC 2009,,,,,,0|i0fxgf:,91018,,,,,,,,,,,"22/May/09 16:11;junrao;To fix the ordering of the returned result, we can reverse the returned list just before returning the thrift call, if the order is descending.

In terms of I/O, it is always better to fetch a group of columns from SSTable (instead of a column at a time). We can think about deserielizing a column at a time from the buffer. Doing this for descending order is bit difficult, but may still be possible.
","22/May/09 16:48;jbellis;Yes, we can add a bandaid there, but ""iterate in descending order, to add to a sortedset in ascending order, to reverse before sending back to the client"" isn't exactly awesome.  In general exposing an API that looks to the public like it is low-cost but is really relatively high-cost is a bad idea.

(Yes, we should always do buffered reads, which is why I specifically said we want to _deserialize_ one at a time"" :)",07/Jul/09 20:13;sandeep_tata;System test for descending slices.,"20/Jul/09 20:33;junrao;Attache a patch, including a fix + the system test that Sandeep added.
","21/Jul/09 13:32;jbellis;Thanks!  Can you rebase to trunk?  (Yep, I did try. :)",21/Jul/09 16:11;junrao;attache patchv3. rebased to trunk.,"21/Jul/09 22:32;jbellis;add(0) is a bad operation to do on a ArrayList, we should probably either use a deque or just do Collections.reverse at the end of the adds.

other than that it looks fine to me.

can you add a test slicing super subcolumns too?","22/Jul/09 16:32;junrao;Attache patch v4. Changed to collections.reverse and added a sc test. rebased to trunk.
",22/Jul/09 21:14;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
truncate is not secondary index-aware,CASSANDRA-1747,12479981,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,11/15/2010 17:44,3/12/2019 14:10,3/13/2019 22:24,11/17/2010 23:10,0.7.0 rc 1,,Feature/2i Index,,0,,,,,,we need to drop the index data files as well as the base CF ones on truncate.,,,,,,,,,,,,,,,,,,,,17/Nov/10 22:05;jbellis;1747-v2.txt;https://issues.apache.org/jira/secure/attachment/12459834/1747-v2.txt,17/Nov/10 19:39;jbellis;1747.txt;https://issues.apache.org/jira/secure/attachment/12459818/1747.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,16:13.1,,,no_permission,,,,,,,,,,,,20288,,,Thu Nov 18 00:16:35 UTC 2010,,,,,,0|i0g753:,92587,gdusbabek,gdusbabek,,,,,,,,,"17/Nov/10 19:39;jbellis;patch that marks index files compacted as well as base CF.

should be safe b/c we flush index memtables at the same time as base data (while holding the flush lock).",17/Nov/10 21:16;gdusbabek;Will this futz with the liveSize information in SSTableTracker?,"17/Nov/10 21:19;jbellis;in the sense that markCompacted updates liveSize, yes","17/Nov/10 21:54;gdusbabek;right, but you're sending it sstables that it doesn't track that belong to the indexes.  Won't that give an incorrect size?","17/Nov/10 22:05;jbellis;ah, you are right.  it needs to markCompacted each group w/ the cfs it came from.  v2 attached.",17/Nov/10 22:21;gdusbabek;+1 on v2.,"17/Nov/10 23:10;jbellis;committed, thanks","18/Nov/10 00:16;hudson;Integrated in Cassandra-0.7 #13 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/13/])
    truncate includes secondary indexes
patch by jbellis; reviewed by gdusbabek for CASSANDRA-1747
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
indexed writes fail with exception,CASSANDRA-1402,12471824,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,urandom,urandom,8/17/2010 17:16,3/12/2019 14:10,3/13/2019 22:24,9/13/2010 4:46,0.7 beta 2,,,,0,,,,,,"Indexed writes fail with an ArrayIndexOutOfBoundsException depending on the length of the key:


{noformat}
java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: -95
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -95
	at org.apache.cassandra.db.Table.apply(Table.java:379)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
	at org.apache.cassandra.service.StorageProxy$1.runMayThrow(StorageProxy.java:194)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}

The patch that follows updates ColumnFamilyStoreTest to demonstrate.",,,,,,,,,,,,,,,,,,,,17/Aug/10 20:31;jbellis;1402.txt;https://issues.apache.org/jira/secure/attachment/12452310/1402.txt,17/Aug/10 17:17;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1402-make-test-case-fail-with-ArrayIndexOutO.txt;https://issues.apache.org/jira/secure/attachment/12452294/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1402-make-test-case-fail-with-ArrayIndexOutO.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,31:06.2,,,no_permission,,,,,,,,,,,,20119,,,Wed Aug 18 13:13:51 UTC 2010,,,,,,0|i0g4u7:,92214,,,,,,,,,,,17/Aug/10 20:31;jbellis;that was embarassing.,17/Aug/10 20:56;urandom;LGTM. +1,"18/Aug/10 13:13;hudson;Integrated in Cassandra #517 (See [https://hudson.apache.org/hudson/job/Cassandra/517/])
    fix sharded lock hashing on index write path.  patch by jbellis; reviewed by eevans for CASSANDRA-1402
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ERROR [MIGRATION-STAGE:1] Previous Version Mistmatch,CASSANDRA-1384,12471466,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,arya,arya,8/12/2010 22:04,3/12/2019 14:10,3/13/2019 22:24,8/16/2010 19:26,0.7 beta 2,,,,0,,,,,,"I fired up a 3 node cluster. I created few keyspaces using API and inserted to them with no problem. Now I tried to add more CFs to one of those existing Keyspaces in a loop. I got the following exception:

ERROR [MIGRATION-STAGE:1] 2010-08-12 14:46:40,493 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[MIGRATION-STAGE:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: Previous version mismatch. cannot apply.
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: Previous version mismatch. cannot apply.
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	... 2 more
Caused by: org.apache.cassandra.config.ConfigurationException: Previous version mismatch. cannot apply.
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:101)
	at org.apache.cassandra.db.DefinitionsUpdateResponseVerbHandler$1.runMayThrow(DefinitionsUpdateResponseVerbHandler.java:70)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 6 more

The above exception is logged in the log of node in which I send the request to and not other seeds. It is noteworthy that my schem_agreement is stuck in a disagreed state:

Array
(
    [1775e847-a658-11df-960f-7d867dfef3ae] => Array
        (
            [0] => 10.50.26.134
        )

    [163d874a-a65b-11df-aef0-d73a63bafff3] => Array
        (
            [0] => 10.50.26.133
        )

    [14869031-a658-11df-8553-930ba61048ac] => Array
        (
            [0] => 10.50.26.132
        )

)

And this does not change. Affect is that some keyspaces would not respond to reads any more giving Internal Error:

ERROR [pool-1-thread-26] 2010-08-12 14:50:57,034 Cassandra.java (line 2988) Internal error processing batch_mutate
java.lang.AssertionError
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:91)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1289)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1277)
	at org.apache.cassandra.service.StorageProxy.mutateBlocking(StorageProxy.java:193)
	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:474)
	at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:438)
	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:2980)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2499)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

In my CF creation, I block for CF creation of the same name and not different names. 

Please advice.
","CentOS 5.2
Trunc August 12th, 2010 at 1:30pm",,,,,,,,,,,,,,,,,,,13/Aug/10 16:06;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-trap-ConfigExceptions-so-they-don-t-become-RTEs.txt;https://issues.apache.org/jira/secure/attachment/12452032/ASF.LICENSE.NOT.GRANTED--v1-0001-trap-ConfigExceptions-so-they-don-t-become-RTEs.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,03:25.6,,,no_permission,,,,,,,,,,,,20113,,,Tue Aug 17 12:59:51 UTC 2010,,,,,,0|i0g4q7:,92196,,,,,,,,,,,"13/Aug/10 15:03;gdusbabek;I think the problem is that the exception is not being handled properly.  Cassandra is throwing a RuntimeException, which is bad because this is a totally recoverable situation.","13/Aug/10 16:09;gdusbabek;Arya: I wasn't able to replicate your problem, but I think I understand it enough to provide this fix.  Can you please apply and test it?

Basically, a ConfigurationException which Cassandra can recover from is percolating up and getting re-thrown as a RuntimeException, which is bad.","13/Aug/10 16:41;jbellis;what causes the first type?

+                                    logger.info(""Migration not applied "" + ex.getMessage());

should that be error instead of info?","13/Aug/10 18:09;gdusbabek;>what causes the first type?
Trying to apply the same migration twice.  This happens as a result of gossip.  I was dropping them silently before, but figured a log message would be ok.",13/Aug/10 19:27;jbellis;let's comment that and move it to debug then.  +1 otherwise,13/Aug/10 19:46;gdusbabek;ok.  I'm going to hold off on committing this until I hear back from Arya.,13/Aug/10 22:46;arya;I updated my trunc with revision #985305 which includes your change and looks good to me. I tried it few times and I could not get the exception any more. +1,"14/Aug/10 12:48;hudson;Integrated in Cassandra #514 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/514/])
    revert last change (committed wrong branch CASSANDRA-1384)
",16/Aug/10 19:26;gdusbabek;committed.,"17/Aug/10 12:59;hudson;Integrated in Cassandra #516 (See [https://hudson.apache.org/hudson/job/Cassandra/516/])
    trap ConfigExceptions so they don't become RTEs. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1384
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unable to get entire supercolumn,CASSANDRA-508,12438856,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/22/2009 22:11,3/12/2019 14:10,3/13/2019 22:24,10/29/2009 22:05,0.4,,,,0,,,,,,"./interface/gen-py/cassandra/Cassandra-remote -h localhost:9160 get Keyspace1 o:movie ""ColumnPath('movie', 'all')"" ConsistencyLevel.ONE

gets transformed into

SliceByNamesReadCommand(table='Keyspace1', key='o:movie', columnParent='QueryPath(columnFamilyName='movie', superColumnName='[B@c68059', columnName='null')', columns=[all,])

which is wrong, sCN should be null",,,,,,,,,,,,,,,,,,,,23/Oct/09 15:23;jbellis;508.patch;https://issues.apache.org/jira/secure/attachment/12423022/508.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,04:50.9,,,no_permission,,,,,,,,,,,,19728,,,Thu Oct 29 22:05:36 UTC 2009,,,,,,0|i0fzd3:,91327,,,,,,,,,,,23/Oct/09 15:23;jbellis;add test & fix,"28/Oct/09 15:04;stuhood;Looks good, thanks Jonathan.",29/Oct/09 22:05;jbellis;committed to 0.4 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError SSTableSliceIterator.java:126,CASSANDRA-866,12458569,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,btoddb,btoddb,3/9/2010 18:24,3/12/2019 14:10,3/13/2019 22:24,4/13/2010 2:22,0.6.1,,,,0,,,,,,"also seeing these, using cassandra-0.6.0-beta2/

2010-03-09 07:37:57,683 ERROR [ROW-READ-STAGE:77] [CassandraDaemon.java:78] Fatal exception in thread Thread[ROW-READ-STAGE:77,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:126)
        at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:59)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:851)
        at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:748)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:773)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:740)
        at org.apache.cassandra.db.Table.getRow(Table.java:381)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:56)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:80)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,,,,,10/Apr/10 13:06;jbellis;ASF.LICENSE.NOT.GRANTED--866.txt;https://issues.apache.org/jira/secure/attachment/12441337/ASF.LICENSE.NOT.GRANTED--866.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,56:21.2,,,no_permission,,,,,,,,,,,,19896,,,Tue Apr 13 02:22:40 UTC 2010,,,,,,0|i0g1k7:,91683,,,,,,,,,,,"09/Mar/10 18:56;jbellis;are you missing the ""caused by"" part of the exception?  this looks like CASSANDRA-857 to me","09/Mar/10 19:09;btoddb;unfortunately i no longer have the log.  i don't believe i missed a caused by, but i'll keep an eye out for this error to happen again",09/Mar/10 20:28;btoddb;Actually i did have the logs and there is no caused by for these AssertionErrors,"16/Mar/10 16:28;btoddb;i still see these a lot ... here is a slightly different stack trace:

2010-03-16 06:57:20,191 ERROR [HINTED-HANDOFF-POOL:1] [DebuggableThreadPoolExecutor.java:94] Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:126)
        at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:59)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:800)
        at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:697)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:722)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:689)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:122)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:250)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:80)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:280)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
","19/Mar/10 21:35;jbellis;how big are the index files corresponding to your sstables?  in particular, do you have any > 2 GB?","22/Mar/10 22:10;btoddb;yes ...

-rw-rw-r-- 1 bburruss bburruss 2506278504 Mar 22 14:09 /data/cassandra-data/data/uds/bucket-2858-Index.db
-rw-rw-r-- 1 bburruss bburruss  170769982 Mar 22 14:24 /data/cassandra-data/data/uds/bucket-3150-Index.db
-rw-rw-r-- 1 bburruss bburruss  170040554 Mar 22 14:39 /data/cassandra-data/data/uds/bucket-3155-Index.db
-rw-rw-r-- 1 bburruss bburruss   45092660 Mar 22 14:41 /data/cassandra-data/data/uds/bucket-3156-Index.db
-rw-rw-r-- 1 bburruss bburruss    5758530 Mar 22 14:47 /data/cassandra-data/data/uds/bucket-3161-Index.db
-rw-rw-r-- 1 bburruss bburruss    5775322 Mar 22 14:52 /data/cassandra-data/data/uds/bucket-3166-Index.db
-rw-rw-r-- 1 bburruss bburruss    1454824 Mar 22 14:53 /data/cassandra-data/data/uds/bucket-3167-Index.db
-rw-rw-r-- 1 bburruss bburruss    1439313 Mar 22 14:54 /data/cassandra-data/data/uds/bucket-3168-Index.db
-rw-rw-r-- 1 bburruss bburruss    1448648 Mar 22 14:54 /data/cassandra-data/data/uds/bucket-3169-Index.db
-rw-rw-r-- 1 bburruss bburruss    1447836 Mar 22 14:55 /data/cassandra-data/data/uds/bucket-3170-Index.db
-rw-rw-r-- 1 bburruss bburruss    5778333 Mar 22 14:56 /data/cassandra-data/data/uds/bucket-3171-Index.db
-rw-rw-r-- 1 bburruss bburruss    1473373 Mar 22 14:56 /data/cassandra-data/data/uds/bucket-3172-Index.db
-rw-rw-r-- 1 bburruss bburruss    1457463 Mar 22 14:57 /data/cassandra-data/data/uds/bucket-3173-Index.db
-rw-rw-r-- 1 bburruss bburruss    1462000 Mar 22 14:58 /data/cassandra-data/data/uds/bucket-3174-Index.db
-rw-rw-r-- 1 bburruss bburruss    1451259 Mar 22 14:59 /data/cassandra-data/data/uds/bucket-3175-Index.db
-rw-rw-r-- 1 bburruss bburruss    5829897 Mar 22 14:59 /data/cassandra-data/data/uds/bucket-3176-Index.db
-rw-rw-r-- 1 bburruss bburruss   22964898 Mar 22 15:00 /data/cassandra-data/data/uds/bucket-3177-Index.db
-rw-rw-r-- 1 bburruss bburruss    1475776 Mar 22 15:00 /data/cassandra-data/data/uds/bucket-3178-Index.db
-rw-rw-r-- 1 bburruss bburruss    1462095 Mar 22 15:01 /data/cassandra-data/data/uds/bucket-3179-Index.db
-rw-rw-r-- 1 bburruss bburruss    1453720 Mar 22 15:02 /data/cassandra-data/data/uds/bucket-3180-Index.db
-rw-rw-r-- 1 bburruss bburruss    1337336 Mar 22 15:03 /data/cassandra-data/data/uds/bucket-3181-Index.db
-rw-rw-r-- 1 bburruss bburruss    5718888 Mar 22 15:03 /data/cassandra-data/data/uds/bucket-3182-Index.db
-rw-rw-r-- 1 bburruss bburruss    1489499 Mar 22 15:04 /data/cassandra-data/data/uds/bucket-3183-Index.db
-rw-rw-r-- 1 bburruss bburruss    1520616 Mar 22 15:05 /data/cassandra-data/data/uds/bucket-3184-Index.db
","22/Mar/10 22:24;jbellis;Can you test w/ latest 0.6 svn branch, to see if the fix in CASSANDRA-857 for large index files helps?  (this did not make it into beta3 unfortunately)",22/Mar/10 22:41;btoddb;trying it now,"22/Mar/10 23:43;btoddb;slightly different, but still getting error

2010-03-22 16:39:26,164 ERROR [ROW-READ-STAGE:22] [CassandraDaemon.java:78] Fatal exception in thread Thread[ROW-READ-STAGE:22,5,main]
java.lang.AssertionError: DecoratedKey(151727172804471108783669089800305019461, vmguest85__791890602)
        at org.apache.cassandra.db.filter.SSTableNamesIterator.<init>(SSTableNamesIterator.java:56)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:830)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:750)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:719)
        at org.apache.cassandra.db.Table.getRow(Table.java:381)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:56)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:80)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
","26/Mar/10 19:59;jbellis;What is the difference between queries that work, and queries that do not?

(I'm assuming you do have some queries that work.)

Do the problematic ones start working when you force DiskAccessMode to standard?","26/Mar/10 20:16;btoddb;all of my queries are the same - i'm doing straight get, put, deletes.  one column family, one column.  get a ""bucket"" of data, put a ""bucket"" of data.

i don't know which ones are working at which ones don't.  i haven't tried correlating the AssertionError back to a particular request.  i can say that not all gets fail.

last night i saw a bunch of AssertionErrors and then eventually the node OOM exception.  what assertion is failing?

here's my column family definition.

      <ColumnFamily CompareWith=""BytesType"" Name=""bucket""
                    RowsCached=""20%""
                    KeysCached=""0%""
                    />
",26/Mar/10 20:30;jbellis;I've committed an improved assertion to the 0.6 branch that will give the key and filename involved so we can have a look and see what's going on there.,"26/Mar/10 20:31;jbellis;Please re-test with that, and if the given data file is small enough, zip it up w/ its index and filter and send it to my gmail address.  Otherwise we can work out another way to test.","28/Mar/10 02:13;jbellis;Bumping to 0.6.1.

In the meantime, if you are affected by this switching DiskAccessMode to standard should work around the bug.","28/Mar/10 02:18;btoddb;I'm out of the office.  Back on 4/5/2010.
","01/Apr/10 19:20;brandon.williams;I tried reproducing this with the data set provided.  I couldn't reproduce at all on a 64bit jvm, and I tried on a 32bit, which also succeeded with the disk mode set to auto.  I had a theory that this was caused by forcing mmap mode on 32bits, but that actually raises a different exception:

Exception in thread ""main"" java.io.IOException: Map failed
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:803)
        at org.apache.cassandra.io.SSTableReader.mmap(SSTableReader.java:206)
        at org.apache.cassandra.io.SSTableReader.<init>(SSTableReader.java:152)
        at org.apache.cassandra.io.SSTableReader.<init>(SSTableReader.java:216)
        at org.apache.cassandra.io.SSTableReader.open(SSTableReader.java:121)
        at org.apache.cassandra.io.SSTableReader.open(SSTableReader.java:112)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:304)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:329)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:373)
Caused by: java.lang.OutOfMemoryError: Map failed
        at sun.nio.ch.FileChannelImpl.map0(Native Method)
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:800)
        ... 8 more


Todd, can you give us more details about your environment?","01/Apr/10 19:28;jbellis;the most important question is, ""what key is failing.""  (this is part of the error message, now.)","01/Apr/10 19:28;jbellis;... and does it fail reproducibly, and if so, what query can we use to repro.",06/Apr/10 16:21;jbellis;Todd: can you update?,"06/Apr/10 16:50;btoddb;64bit CentOS  2.6.18

single column family, only one column.  simple get, put, deletes.  would you like my storage-conf.xml?

i don't know what key, but it is reproducible.  i could make it happen simply by starting the node.  during startup i will see the exception.  it doesn't happen until some amount of data is in the database.  in other words, it runs fine until either the number of keys or the amount of data crosses some threshold.

i can't repo it at the moment as the cluster is shared and i needed to erase data and work on something else.  but i can try again probably later this week.","06/Apr/10 21:57;jbellis;Great, I'm glad we downloaded a 72 GB test corpus that you blew away. :(

The key is logged with every exception message in latest 0.6 code.  From there I would hope that it would be easy for you to track down what command is causing it.  If not, turning on debug logging on the Cassandra would get that information for you.
","07/Apr/10 23:20;thuske;I think we are seeing this error as well.

ERROR [ROW-READ-STAGE:17] 2010-04-07 21:04:36,861 CassandraDaemon.java (line 78) Fatal exception in thread Thread[ROW-READ-STAGE:17,5,main]
java.lang.AssertionError: DecoratedKey(112702293498592974518440554745446764341, 4452215176) != DecoratedKey(112702294131127473014102114707935791646, 11667886399) in /data2/data/Twitter/Statuses-4327-Data.db
	at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:127)
	at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:59)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:830)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:750)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:719)
	at org.apache.cassandra.db.Table.getRow(Table.java:381)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:59)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:80)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

We're using the RandomPartitioner, and a replication factor of 3, the relevant SSTable is 146GB, and the index file is 18GB.  This assertion only occurs on the primary node, whereas the data is returned correctly from the other 2 when asked directly.  We had been running with 0.6beta2 and a DiskAccessMode of ""auto"" which set the mode to mmap.  We've since upgraded to 0.6rc1, and changed the DAM to ""standard"", but the error still occurs.

From cassandra-cli, connected to the primary node:
cassandra> get Twitter.Statuses['11667886399']           
...RpcTimeout seconds later...
Exception Internal error processing get_slice

Connected to an unrelated node:
cassandra> get Twitter.Statuses['11667886399']
...RpcTimeout seconds later...
Exception null

We have a number of these errors, and for each one, the token for the key read off disk is numerically very close to that of the requested key:
DecoratedKey(79954008423729056632733094063639634941, 8307722089) != DecoratedKey(79954009259417349514057411741701713236, 6153489921)
DecoratedKey(80379343112914474054622124950679452815, 3768834481) != DecoratedKey(80379344845214890401447094271897994174, 11653098762)
DecoratedKey(80779546346447228478584703771606621740, 8569013773) != DecoratedKey(80779548447419746111153057677392388488, 11764962920)
DecoratedKey(82844158105946904890171938436663575930, 4464394167) != DecoratedKey(82844159691805202566257389052615565753, 6874217428)
DecoratedKey(84289221786768678402050522652258180028, 6057213014) != DecoratedKey(84289224459791176731103465787486907832, 9899742458)
DecoratedKey(86300142475818241616994325294057703135, 5832470418) != DecoratedKey(86300142677962509841823563793986169119, 11448189666)
DecoratedKey(92092914035356318872651100377285498797, 3473992372) != DecoratedKey(92092915436251199165174090571909314827, 8851204821)
DecoratedKey(94745421400532682848559427993237380460, 9893103006) != DecoratedKey(94745431874154322071681265706426033545, 10438638106)","08/Apr/10 01:51;jbellis;Tim, can you make that sstable (+ index + filter) available for us to debug locally?","08/Apr/10 21:34;btoddb;awright, big daddy ... reproduced again, and data is saved ;)

2010-04-07 21:21:10,520 ERROR [ROW-READ-STAGE:4] [CassandraDaemon.java:78] Fatal exception in thread Thread[ROW-READ-STAGE:4,5,main]
java.lang.AssertionError: DecoratedKey(147587030576932389559405437065042613628, vmguest85__-1131008275) != DecoratedKey(147587045543996727516366006491335105792, vmguest85__-888697030) in /data/cassandra-data/data/uds/bucket-1439-Data.db
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.<init>(SSTableSliceIterator.java:127)
        at org.apache.cassandra.db.filter.SSTableSliceIterator.<init>(SSTableSliceIterator.java:59)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:63)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:830)
        at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:727)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:752)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:719)
        at org.apache.cassandra.db.Table.getRow(Table.java:381)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:56)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:70)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

emailed jonathan an http site to download data files that reproduce this problem.

AssertionError happened on node 105 (see ring below), and according to the keys shown in above exception, 105 is the primary for the key.  ReplicationFactor = 3 so 102 and 103 are replicas, right?  i can use the cassandra-cli to retrieve the value on the key's replica nodes, just not on the primary node.

Address       Status     Load          Range                                      Ring
                                       170141183460469231731687303715884105728
192.168.132.102Up         41.64 GB      42535295865117307932921825928971026431     |<--|
192.168.132.103Up         41.33 GB      85070591730234615865843651857942052863     |   |
192.168.132.104Up         41.52 GB      127605887595351923798765477786913079295    |   |
192.168.132.105Up         36.86 GB      170141183460469231731687303715884105728    |-->|
","08/Apr/10 23:31;kingryan;Jonathan- 

We can't easily make that data available because it contains private data from some of our users. Sorry. :(","10/Apr/10 13:06;jbellis;Patch attached, with unit test.

Thanks to Todd for getting us a reproducible test case!",12/Apr/10 15:38;btoddb;i'll verify the fix with my dataset today by grabbing the tip of 0.6 branch and applying patch.  stay tuned,"12/Apr/10 16:25;gdusbabek;encodedUTF8Length doesn't account for 4-byte chars:

0x00-0x7f : 1 byte
0x80-0x7ff: 2 bytes
0x800-0xffff: 3 bytes
>=0x10000 (technically to 0x10ffff): 4 bytes.","12/Apr/10 17:55;jbellis;DataInput.writeUTF, which is what keys are written with, only uses 1-3 bytes.  http://java.sun.com/javase/6/docs/api/java/io/DataInput.html",12/Apr/10 19:00;gdusbabek;+1,"12/Apr/10 20:13;btoddb;I have been running with the patch for about 2 hrs now and haven't seen any AssertionErrors.  I would have previously seen them by now.  I'll keep watching things, but it looks good to me.","12/Apr/10 23:18;jmhodges;We're pushing out a version of the current cassandra-0.6 branch with this patch applied. A canary machine is currently freaking out. We're going to move forward and hope it's just this particular machine, but is there any hope of getting this patch rebased to 0.6-rc1 (a.k.a. 0.6-final)?","13/Apr/10 02:22;jbellis;committed.  it will be in 0.6.1.

no, not taking requests to rebase vs other revisions, but it should be fairly straightforward, code churn has been low in 0.6 post-rc1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
File descriptor leak in CommitLog,CASSANDRA-313,12431274,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,7/23/2009 17:47,3/12/2019 14:10,3/13/2019 22:24,7/23/2009 17:58,0.4,,,,0,,,,,,"There is a file descriptor leak in CommitLog.java.  On systems with a default ulimit of 1024, this causes Cassandra to eventually crash due to too many open files.  A descriptor appears to be leaked at each memtable rotation.","Debian lenny, 64 bit, openjdk-1.6.0",,300,300,,0%,300,300,,,,,,,,,,,,23/Jul/09 17:49;brandon.williams;patch;https://issues.apache.org/jira/secure/attachment/12414355/patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:15.5,,,no_permission,,,,,,,,,,,,19630,,,Thu Jul 23 17:58:15 UTC 2009,,,,,,0|i0fy67:,91134,,,,,,,,,,,23/Jul/09 17:49;brandon.williams;This simple patch fixes the file descriptor leak.,23/Jul/09 17:58;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE aborts streaming operations for keyspaces with hyphens ('-') in their names,CASSANDRA-1377,12471303,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,bhoyt,bhoyt,8/11/2010 14:54,3/12/2019 14:10,3/13/2019 22:24,8/12/2010 21:40,0.6.5,0.7 beta 2,,,0,,,,,,"When streaming starts for operations such as repair or bootstrap, it will fail due to an NPE if they rows are in a keyspace that has a hyphen in its name.  One workaround for this issue would be to not use keyspace names containing hyphens.  It would be even nicer if streaming worked for keyspace names with hyphens, since keyspaces named like that seem to be fine in all other ways.

To reproduce:
 1. With a multi-node ring, load up a keyspace with a hyphen in its name
 2. Add some data to that keyspace
 3. nodetool repair

Expected results:
Repair operations complete normally

Actual results:
Repair operations don't complete normally.  The stacktrace below is correlated with the repair request.  

 INFO [AE-SERVICE-STAGE:1] 2010-06-30 14:11:29,744 AntiEntropyService.java (line 619) Performing streaming repair of 1 ranges to /10.255.0.20 for (my-keyspace,AColumnFamily)
ERROR [MESSAGE-DESERIALIZER-POOL:1] 2010-06-30 14:11:30,034 DebuggableThreadPoolExecutor.java (line 101) Error in ThreadPoolExecutor
java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInitiateVerbHandler.getNewNames(StreamInitiateVerbHandler.java:154)
        at org.apache.cassandra.streaming.StreamInitiateVerbHandler.doVerb(StreamInitiateVerbHandler.java:76)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)",,,,,,,,,,,,,,,,,,,,24/Aug/10 12:59;gdusbabek;1377-0.6-check-for-dashes.txt;https://issues.apache.org/jira/secure/attachment/12452925/1377-0.6-check-for-dashes.txt,12/Aug/10 17:03;gdusbabek;1377-0.6.txt;https://issues.apache.org/jira/secure/attachment/12451925/1377-0.6.txt,12/Aug/10 17:02;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-disallow-invalid-ks-cf-names.txt;https://issues.apache.org/jira/secure/attachment/12451923/ASF.LICENSE.NOT.GRANTED--v1-0001-disallow-invalid-ks-cf-names.txt,24/Aug/10 03:02;eonnen;CAS-1377-1.patch;https://issues.apache.org/jira/secure/attachment/12452887/CAS-1377-1.patch,24/Aug/10 03:42;eonnen;CAS-1377-2.patch;https://issues.apache.org/jira/secure/attachment/12452889/CAS-1377-2.patch,23/Aug/10 21:51;eonnen;CAS-1377.patch;https://issues.apache.org/jira/secure/attachment/12452863/CAS-1377.patch,,,,,,,,6,,,,,,,,,,,,,,,,,,,40:49.1,,,no_permission,,,,,,,,,,,,20110,,,Tue Aug 24 13:40:09 UTC 2010,,,,,,0|i0g4on:,92189,,,,,,,,,,,12/Aug/10 02:40;gdusbabek;We currently disallow hyphens in CF names.  It sounds like we need to disallow it in KS names too.,12/Aug/10 05:15;jbellis;I vote for disallowing everything but \w characters - [a-zA-Z0-9_] ,"12/Aug/10 17:03;gdusbabek;0001 is for trunk, 1377-0.6 is for 0.6.",12/Aug/10 18:36;jbellis;+1,"14/Aug/10 12:48;hudson;Integrated in Cassandra #514 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/514/])
    disallow invalid ks+cf names. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1377
","21/Aug/10 11:14;hudson;Integrated in Cassandra #518 (See [https://hudson.apache.org/hudson/job/Cassandra/518/])
    CHANGES.txt and NEWS.txt update explaining the ramifications of CASSANDRA-1377
",23/Aug/10 19:00;gdusbabek;eonnen in #cassandra has made a compelling argument that committing this to 0.6 breaks some backwards compatibility.  I am inclined to agree if the amount of work to handle dashes during streaming is trivial.,"23/Aug/10 19:11;jbellis;We really need some ""reserved"" characters in keyspace/CF names.  You can make a case that restricting them to \w is going too far in the other direction, but hyphens have always been reserved, and letting them pass in keyspaces was definitely a bug that was going to bite us (see: this issue).",23/Aug/10 19:15;jbellis;(One of the reasons I'd like to restrict to \w is that makes us not have to deal with people reporting bugs from Thrift strings being utf8-encoded in some languages and not in others.),"23/Aug/10 21:48;eonnen;We have a multi-tenant deployment hosting multiple customers, each with multiple deployments of our upstream software (think test/prod). For each customer deployment, we've had a UUID identifying the instance since before the dawn of time, or at least since before Cassandra :)

Up until this change, using a keyspace-UUID mapping worked perfectly, especially after set_keyspace was added to the lower-level client API which allowed us to have pools for a given customer with different customers able to have different throughput to a point.

In hopes of getting this relaxed just a bit to allow ""-"", I've attached a fix for the bug in 0.6.0 tested with a keyspace name of ""e610eed7-c6be-449b-ad2c-562f35d75528"" which is a Type4 UUID. If you can broaden the limit here just a bit, we'll be good. I don't think it's all that unrealistic that users will want to have a keyspace correspond to real artifacts in their system (although I don't feel the same about CF names). This would at least broaden things just enough to allow UUIDs at that level.

While I understand the need to limit what goes into the name of the keyspace and why, it's too restrictive for my needs and I'll argue against it at least and try and patch my way out of it as long as you'll listen.

Happy to do the same for 0.7.0 if you're receptive.","23/Aug/10 21:51;eonnen;Fixes NPE with ""-"" in Keyspace names, re-allows using them at the DatabaseDescriptor.","24/Aug/10 02:08;jbellis;committed Erik's patch to 0.6 and trunk.

I still think we need to make some explicit rules about reserved characters but I agree that 0.6.5 is not the place to make that change.","24/Aug/10 03:02;eonnen;This patch lightens the restriction on KS names to allow ""-"" in addition to \\w bringing in line the functionality w/ the 0.6 patch submitted earlier.",24/Aug/10 03:05;eonnen;Thanks Jonathan. I added one additional patch to relax things slightly in 0.7 trunk. Tested with a nodetool loadbalance and nodetool move with a two node ring and both worked fine. Lots of changes in the streaming code between 0.6 and 0.7 and it looks like the broken code didn't move forward into 0.7 near as I can tell in reading though it.,24/Aug/10 03:42;eonnen;Forgot to attach test in CAS-1377-1.patch,24/Aug/10 12:59;gdusbabek;Puts hyphen check back into CF definition.,24/Aug/10 13:37;jbellis;+1 hyphens-in-CF check,24/Aug/10 13:40;gdusbabek;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tombstone records in Cassandra are not being deleted,CASSANDRA-507,12438736,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,rrabah,rrabah,10/21/2009 19:29,3/12/2019 14:10,3/13/2019 22:24,10/22/2009 15:41,0.5,,,,0,,,,,,"I am running into problems with get_key_range.
My command is client.get_key_range(""Keyspace1"", ""DatastoreDeletionSchedule"",
                   """", """", 25, ConsistencyLevel.ONE);

After a lot of deletes on the datastore, I am getting 

ERROR [pool-1-thread-36] 2009-10-19 17:24:28,223 Cassandra.java (line
770) Internal error processing get_key_range
java.lang.RuntimeException: java.util.concurrent.TimeoutException:
Operation timed out.
       at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:560)
       at org.apache.cassandra.service.CassandraServer.get_key_range(CassandraServer.java:595)
       at org.apache.cassandra.service.Cassandra$Processor$get_key_range.process(Cassandra.java:766)
       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:609)
       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
       at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.TimeoutException: Operation timed out.
       at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:97)
       at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:556)
       ... 7 more

Turns out that the compaction code removes tombstones, and it runs whenever you have
enough sstable fragments. As an optimization, if there is
only one version of a row it will just copy it to the new sstable.
This means it won't clean out tombstones, which is causing this problem.


",,,,,,,,,,,,,,,,,,,,21/Oct/09 20:01;jbellis;507.patch;https://issues.apache.org/jira/secure/attachment/12422843/507.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,01:47.5,,,no_permission,,,,,,,,,,,,19727,,,Fri Oct 23 12:34:41 UTC 2009,,,,,,0|i0fzcv:,91326,,,,,,,,,,,"21/Oct/09 20:01;jbellis;this patch fixes the bug in trunk.

unfortunately the risk/benefit of backporting this to the 0.4 branch is past my threshold of comfort.",22/Oct/09 03:23;junrao;patch looks good to me. Can you add a test case for this?,22/Oct/09 15:41;jbellis;committed with test,"23/Oct/09 12:34;hudson;Integrated in Cassandra #236 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/236/])
    all rows go through deserialize/removeDeleted so we can GC tombstones.
patch by jbellis; reviewed by junrao for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_key_range problems when a node is down,CASSANDRA-440,12435431,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,simongsmith,simongsmith,9/11/2009 13:37,3/12/2019 14:10,3/13/2019 22:24,9/15/2009 2:54,0.4,0.5,,,0,,,,,,"I'm running Cassandra on 5 nodes using the
OrderPreservingPartitioner, and have populated Cassandra with 78
records, and I can use get_key_range via Thrift just fine.  Then, if I
manually kill one of the nodes (if I kill off node #5), the node (node
#1) which I've been using to call get_key_range will timeout and the
error:

 Thrift: Internal error processing get_key_range

The Cassandra output traceback:

ERROR - Encountered IOException on connection:
java.nio.channels.SocketChannel[closed]
java.net.ConnectException: Connection refused
       at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
       at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:592)
       at org.apache.cassandra.net.TcpConnection.connect(TcpConnection.java:349)
       at org.apache.cassandra.net.SelectorManager.doProcess(SelectorManager.java:131)
       at org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:98)
WARN - Closing down connection java.nio.channels.SocketChannel[closed]
ERROR - Internal error processing get_key_range
java.lang.RuntimeException: java.util.concurrent.TimeoutException:
Operation timed out.
       at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:573)
       at org.apache.cassandra.service.CassandraServer.get_key_range(CassandraServer.java:595)
       at org.apache.cassandra.service.Cassandra$Processor$get_key_range.process(Cassandra.java:853)
       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:606)
       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
       at java.lang.Thread.run(Thread.java:675)
Caused by: java.util.concurrent.TimeoutException: Operation timed out.
       at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:97)
       at org.apache.cassandra.service.StorageProxy.getKeyRange(StorageProxy.java:569)
       ... 7 more


The error starts as soon as the downed node #5 goes down and lasts
until I restart the downed node #5.

bin/nodeprobe cluster is accurate (it knows quickly when #5 is down,
and when it is up again)

Since I set the replication set to 3, I'm confused as to why (after
the first few seconds or so) there is an error just because one host
is down temporarily.

(Jonathan Ellis and I discussed this on the mailing list, let me know if more information is needed.)",64-bit 4GB Rackspace-cloud boxes running FC11 (saw problem on 32-bit platform as well),,,,,,,,,,,,,,,,,,,11/Sep/09 15:36;jbellis;440.patch;https://issues.apache.org/jira/secure/attachment/12419312/440.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,36:55.1,,,no_permission,,,,,,,,,,,,19690,,,Tue Sep 15 02:54:29 UTC 2009,,,,,,0|i0fyxz:,91259,,,,,,,,,,,"11/Sep/09 15:36;jbellis;refactor findSuitableEndpoint to throw UnavailableException (instead of letting callers error out with NPE) when no replica is alive.  also check for endpoint live-ness in getNextEndpoint.

patch is against trunk but should also apply to 0.4.","14/Sep/09 22:37;simongsmith;Jonathan:

I tried out the patch you attached above, I applied it to 0.4, and it works for me.  Now, as soon as I take a node down, there may be one or two seconds of the thrift-internal error, the timeout (which I totally expect, and this is obviously OK) but as soon as the host doing the querying can see the node is down, the error stops, and valid output is given by the get_key_range query again.  And there isn't any disruption when the node comes back up.

Thanks! 

Simon Smith",15/Sep/09 02:54;jbellis;committed to 0.4 and 0.5 branches,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair using abnormally large amounts of disk space,CASSANDRA-1674,12478581,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,jbellis,jbellis,10/28/2010 14:38,3/12/2019 14:10,3/13/2019 22:24,11/18/2010 1:06,0.6.9,0.7.0 rc 1,,,1,,,,,,"I'm watching a repair on a 7 node cluster.  Repair was sent to one node; the node had 18G of data.  No other node has more than 28G.  The node where the repair initiated is now up to 261G with 53/60 AES tasks outstanding.

I have seen repair take more space than expected on 0.6 but nothing this extreme.

Other nodes in the cluster are occasionally logging
WARN [ScheduledTasks:1] 2010-10-28 08:31:14,305 MessagingService.java (line 515) Dropped 7 messages in the last 1000ms

The cluster is quiesced except for the repair.  Not sure if the dropped messages are contributing the the disk space (b/c of retries?).",,,,,,,,,,,,,,,,,,,,17/Nov/10 19:30;stuhood;0001-Only-repair-the-intersecting-portion-of-a-differing-ra.txt;https://issues.apache.org/jira/secure/attachment/12459816/0001-Only-repair-the-intersecting-portion-of-a-differing-ra.txt,17/Nov/10 19:30;stuhood;for-0.6-0001-Only-repair-the-intersecting-portion-of-a-differing-ra.txt;https://issues.apache.org/jira/secure/attachment/12459817/for-0.6-0001-Only-repair-the-intersecting-portion-of-a-differing-ra.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,47:41.4,,,no_permission,,,,,,,,,,,,20253,,,Thu Nov 18 01:46:01 UTC 2010,,,,,,0|i0g6ov:,92514,jbellis,jbellis,,,,,,,,,"28/Oct/10 14:38;jbellis;(this is rc1 snapshot, btw)","28/Oct/10 15:40;jbellis;bq. WARN [ScheduledTasks:1] 2010-10-28 08:31:14,305 MessagingService.java (line 515) Dropped 7 messages in the last 1000ms

Created CASSANDRA-1676 for this.","28/Oct/10 15:47;stuhood;I haven't tested the theory, but various messages are executed in the single threaded MISC stage.","28/Oct/10 16:05;jbellis;it doesn't have to be single threaded to drop messages, it just has to not run the verbhandler until RPC_TIMEOUT after the message was received.

In practice though I don't see any place where the verbhandler would be blocking or otherwise slow, including the ones running on MISC.","01/Nov/10 04:08;jbellis;so we have CASSANDRA-1685 and CASSANDRA-1676 committed now, but I don't think those are the root cause here since for them to cause lots of disk space usage repair would have to retry dropped requests which I don't see it doing.  (is this correct?)","12/Nov/10 03:10;jbellis;Here is one way to reproduce something similar, at least.

I start with 1 node, and put 1M rows on it.  Then I add a 2nd node, then a third, then run cleanup.  So I have 25% 50% 25% of the 1M rows on those 3 machines:

{code}
$ nodetool -h localhost -p 8080 ring
Address         Status State   Load            Token                                       
                                       164074424718159380631425626216484638578    
127.0.0.2       Up     Normal  95.39 MB        36509681143663337709904175976843745575      
127.0.0.1       Up     Normal  190.72 MB       121466565577718822332569289829746132598     
127.0.0.3       Up     Normal  95.4 MB         164074424718159380631425626216484638578     
{code}

I increase the RF to 2, and run repair against .2:

{code}
Address         Status State   Load            Token                                       
                                       164074424718159380631425626216484638578    
127.0.0.2       Up     Normal  381.41 MB       36509681143663337709904175976843745575      
127.0.0.1       Up     Normal  381.41 MB       121466565577718822332569289829746132598     
127.0.0.3       Up     Normal  190.75 MB       164074424718159380631425626216484638578     
{code}

Let's call the ranges of data that .2, .1, and .3 originally had R, G, and B.  Post repair, .2 should have RB but it has RGB.  Node 1 should have GB but it also has RGB.

Cleanup puts things in the expected state:
{code}
Address         Status State   Load            Token                                       
                                       164074424718159380631425626216484638578    
127.0.0.2       Up     Normal  190.74 MB       36509681143663337709904175976843745575      
127.0.0.1       Up     Normal  285.61 MB       121466565577718822332569289829746132598     
127.0.0.3       Up     Normal  190.75 MB       164074424718159380631425626216484638578     
{code}

This is reproducible against 0.6 as well as 0.7.","17/Nov/10 02:03;stuhood;After finding the differences for all data held on both nodes, repair was not limiting the repaired area to the ranges that the nodes had in common. jbellis: In your example above, because the nodes had no data in common to start with, the MerkleTree was determining that they were different for (0,0], but rather than intersecting the different range with the range of responsibility they had in common, it was repairing all of (0,0].

Attaching a patch for 0.7 to only repair the intersecting ranges.","17/Nov/10 02:04;stuhood;I started rebasing this for 0.6, but noticed some odd behaviour in the 0.6 branch: in what cases would SS.getLocalToken not equal the RHS of SS.getLocalPrimaryRange?","17/Nov/10 02:35;chipdude;Surely something that can overflow disks, even when below 50% usage, is worthy of fix before 0.7.0 is released...","17/Nov/10 03:10;stuhood;Chip: agreed. While it was broken before 0.7, it's a small enough fix that we should try to get it in 0.7.0","17/Nov/10 08:56;jbellis;bq. in what cases would SS.getLocalToken not equal the RHS of SS.getLocalPrimaryRange? 

at a guess, some test is mocking up a TokenMetadata that isn't consistent w/ localtoken",17/Nov/10 19:30;stuhood;Patches for 0.6 and 0.7/trunk: ready for review.,"17/Nov/10 20:08;jbellis;committed, but I think there is a bug.  With a similar setup to the above (200K keys instead of 1M), the pre-RF change setup is

{code}
Address         Status State   Load            Token                                       
                                       106239986353888428655683112465158427815    
127.0.0.2       Up     Normal  37.97 MB        21212647344528771789748883276744400257      
127.0.0.3       Up     Normal  18.98 MB        63523312719601176253752035031089272162      
127.0.0.1       Up     Normal  19.05 MB        106239986353888428655683112465158427815     
{code}

post-repair is
{code}
Address         Status State   Load            Token                                       
                                       106239986353888428655683112465158427815    
127.0.0.2       Up     Normal  57.01 MB        21212647344528771789748883276744400257      
127.0.0.3       Up     Normal  56.94 MB        63523312719601176253752035031089272162      
127.0.0.1       Up     Normal  19.07 MB        106239986353888428655683112465158427815     
{code}

So eyeballing it looks reasonable.  But when I kill node 2 and run

{code}$ python contrib/py_stress/stress.py -n 200000 -o read{code}

I get a ton of key-not-found exceptions, indicating that not all the data on 2 got replicated to 3.","17/Nov/10 20:37;hudson;Integrated in Cassandra-0.6 #7 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/7/])
    limit repaired ranges to what the nodes have in common
patch by Stu Hood; reviewed by jbellis for CASSANDRA-1674
","18/Nov/10 01:06;jbellis;the not-found was because i only repaired .2, but the RF increase meant requests to .1 thought they could find the .3 data locally.  not a bug.  Thanks to Stu for pointing this out on IRC.",18/Nov/10 01:46;stuhood;This still needs to be committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
file descriptor leak in getKeyRange,CASSANDRA-552,12440657,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,11/13/2009 19:57,3/12/2019 14:10,3/13/2019 22:24,11/13/2009 21:28,0.5,,,,0,,,,,,"paste from mailing list:

Cassandra reported the following:

WARN [GMFD:1] 2009-11-12 16:07:24,961 MessagingService.java (line 393)
Exception was generated at : 11/12/2009 16:07:24 on thread GMFD:1
Too many open files
java.net.SocketException: Too many open files
       at sun.nio.ch.Net.socket0(Native Method)
       at sun.nio.ch.Net.socket(Unknown Source)
       at sun.nio.ch.DatagramChannelImpl.<init>(Unknown Source)
       at sun.nio.ch.SelectorProviderImpl.openDatagramChannel(Unknown
Source)
       at java.nio.channels.DatagramChannel.open(Unknown Source)
       at
org.apache.cassandra.net.UdpConnection.init(UdpConnection.java:49)
       at
org.apache.cassandra.net.MessagingService.sendUdpOneWay(MessagingService.java:388)
       at
org.apache.cassandra.gms.GossipDigestSynVerbHandler.doVerb(Gossiper.java:889)
       at
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
       at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown
Source)
       at java.lang.Thread.run(Unknown Source)

Lsof reports that the java process has 65486 files open (I have ulimit
-n 65535 set in cassandra.in.sh).  Many of the lsof entries include a
trailing '(deleted)' comment after the file path.

This appears to be similar to CASSANDRA-283.  Anyone have a work around
for this?  Would a forced GC take care of the ones marked deleted?

Here is my sample code to count the number of keys:

public class CClient
{
   public static void main(String[] args)
   throws TException, InvalidRequestException, UnavailableException,
UnsupportedEncodingException, NotFoundException
   {
       TTransport tr = new TSocket(""localhost"", 9160);
       TProtocol proto = new TBinaryProtocol(tr);
       Cassandra.Client client = new Cassandra.Client(proto);
       tr.open();

       int     count = 0;
       int     block = 1000;
       String  key   = "" "";

       while (true)
       {
           List<String> list = client.get_key_range(""Keyspace1"",
               ""Standard1"", key, ""~"", block, ConsistencyLevel.ONE);
           int size = list.size();
           if (size == 0)
               break;
           count += size;
           key = list.get(size - 1) + '~';
           System.out.println(""Count: "" + Integer.toString(count));
      }
       tr.close();
   }
}",,,,,,,,,,,,,,,,,,,,13/Nov/09 20:00;jbellis;552.patch;https://issues.apache.org/jira/secure/attachment/12424887/552.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,17:27.0,,,no_permission,,,,,,,,,,,,19752,,,Sat Nov 14 12:33:58 UTC 2009,,,,,,0|i0fzmv:,91371,,,,,,,,,,,"13/Nov/09 21:17;elsif;Wow, that was fast!  The patch fixes the problem.

Thanks,
elsif","13/Nov/09 21:27;jbellis;committed to trunk.

0.4 code is different enough that this patch will not apply.  (It shouldn't be necessary, either, from my reading of the code, but if 0.4 does indeed leak FDs I don't think fixing it is worth the effort with 0.5 coming soon.)","14/Nov/09 12:33;hudson;Integrated in Cassandra #258 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/258/])
    make iterator closeable, again
patch by jbellis; tested by ""elsif"" for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null pointer exception in doIndexing(ColumnIndexer.java:142),CASSANDRA-458,12436600,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,teodor,teodor,9/25/2009 13:29,3/12/2019 14:10,3/13/2019 22:24,9/26/2009 12:54,0.4,0.5,,,0,,,,,,"INFO - Saved Token not found. Using 17570558338530880605478324248305304996
INFO - Cassandra starting up...
INFO - Standard1 has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(Standard1)@15830327
INFO - Sorting Memtable(Standard1)@15830327
INFO - Writing Memtable(Standard1)@15830327
INFO - Completed flushing /spool/cassandra/data/Keyspace1/Standard1-1-Data.db
INFO - Standard1 has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(Standard1)@22655307
INFO - Sorting Memtable(Standard1)@22655307
INFO - Writing Memtable(Standard1)@22655307
ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logFutur
eExceptions(DebuggableThreadPoolExecutor.java:95)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExe
cute(DebuggableThreadPoolExecutor.java:82)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:887)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.ColumnIndexer.doIndexing(ColumnIndexer.java:1
07)
        at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:62
)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeWithIndexes(C
olumnFamilySerializer.java:78)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:22
2)
        at org.apache.cassandra.db.ColumnFamilyStore$2$1.run(ColumnFamilyStore.j
ava:934)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:885)
        ... 2 more
INFO - Standard1 has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(Standard1)@14600171
INFO - Sorting Memtable(Standard1)@14600171
INFO - Writing Memtable(Standard1)@14600171
INFO - Completed flushing /spool/cassandra/data/Keyspace1/Standard1-3-Data.db 

How to reproduce: Run perl script pointed below, three at once.  In short, script just inserts a row and immediately removes it.
#!/usr/local/bin/perl
use lib qw(/usr/local/cassandra/interface/gen-perl/Cassandra /usr/local/cassandra/interface/gen-perl);
use strict;

use Cassandra;

use Thrift::Socket;
use Thrift::BinaryProtocol;
use Thrift::FramedTransport;
use Thrift::BufferedTransport;

use Data::Dumper;
use Time::HiRes qw( gettimeofday tv_interval );
use Getopt::Std;
my %opt;
getopts('iu:t:rn:', \%opt);

my $socket = Thrift::Socket->new('localhost', 9160);
   $socket->setSendTimeout(1000);
   $socket->setRecvTimeout(5000);
my $transport =  Thrift::BufferedTransport->new($socket, 1024, 1024);
my $protocol = Thrift::BinaryProtocol->new($transport);
my $client = Cassandra::CassandraClient->new($protocol);

$transport->open();


eval {
    my $id=0;
    for(;;) {
        $id++;
        my $PID = sprintf(""%040lld"", int(1000000 * rand()));
        $client->batch_insert(
            'Keyspace1',
            $PID,
            {
                'Standard1' => _makeColumnList ({
                    map {
                        $_=>'0'x(int(1 + 100 * rand()))
                    } (0..int(1+10*rand()))
                })
            },
            Cassandra::ConsistencyLevel::ONE
        );
 
        $client->remove(
            'Keyspace1',
            $PID,
            Cassandra::ColumnPath->new({
                column_family=>'Standard1',
            }),
            time(),
            Cassandra::ConsistencyLevel::ONE
        );
        print ""$id\n"" if ($id%100 == 0);
    }
};
 
die Dumper($@) if ($@);
 
$transport->close();
sub _makeColumnList($$) {
    my ($row) = @_;
 
    my @cfmap;
 
    foreach my $k (keys %$row) {
        push @cfmap, Cassandra::ColumnOrSuperColumn->new({
            column=>Cassandra::Column->new({
                name=>$k,
                value=>$row->{$k},
                timestamp=>time(),
            })
        });
    }
    die if $#cfmap < 0;
    return \@cfmap;
}

","FreeBSD 7.2, diablo-jdk-1.6.0.07.02_5, snapshot cassandra-0.4.0-final at 24/09/2009 ",,,,,,,,,,,,,,,,,,,25/Sep/09 18:11;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-458.txt;https://issues.apache.org/jira/secure/attachment/12420585/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-458.txt,25/Sep/09 18:11;jbellis;ASF.LICENSE.NOT.GRANTED--0002-r-m-unused-byte-tracking-code.txt;https://issues.apache.org/jira/secure/attachment/12420586/ASF.LICENSE.NOT.GRANTED--0002-r-m-unused-byte-tracking-code.txt,25/Sep/09 18:11;jbellis;ASF.LICENSE.NOT.GRANTED--0003-columns-may-be-empty-if-the-only-pre-flush-op-was-a-CF.txt;https://issues.apache.org/jira/secure/attachment/12420587/ASF.LICENSE.NOT.GRANTED--0003-columns-may-be-empty-if-the-only-pre-flush-op-was-a-CF.txt,25/Sep/09 13:31;teodor;test.pl;https://issues.apache.org/jira/secure/attachment/12420556/test.pl,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,54:20.1,,,no_permission,,,,,,,,,,,,19697,,,Sun Sep 27 12:34:22 UTC 2009,,,,,,0|i0fz27:,91278,,,,,,,,,,,25/Sep/09 13:31;teodor;Script to reproduce a problem,"25/Sep/09 14:54;jbellis;Sorry, I'm not very familiar with perl.  Any ideas how to solve this?

$ perl test.pl
Can't locate Bit/Vector.pm in @INC
","25/Sep/09 15:04;teodor;From root:
# perl -MCPAN -e shell 
and then
cpan[1]> install Bit::Vector

or use your  packet manager. BTW, Bit::Vector is a prerequisite of thrift",25/Sep/09 15:06;urandom;aptitude install libbit-vector-perl,25/Sep/09 15:07;jbellis;ok.  i have 3 copies running.  they are each past 10000.  how long should this take?,"25/Sep/09 15:13;teodor;Several minutes on my notebook, near 20000-50000 records usually.","25/Sep/09 15:20;jbellis;ok, reproduces for me.  thanks!","25/Sep/09 18:20;jbellis;03
    columns may be empty if the only pre-flush op was a CF-level remove op.  fix, with test.

02
    r/m unused byte-tracking code

01
    r/m unnecessary SortedFlushable class.  Keeping a 2nd reference to the Flushable in question,
    when it is only used to pass back to that Flushable for the write phase, was unnecessary & confusing.

Patch 03 is the bug fix against 0.4 and 0.5.  Can you verify that this fixes the problem for you?

Patches 02 and 01 are just for 0.5.","25/Sep/09 18:55;teodor;Thank you a lot, it's working for me. Although patch looks too obvious and simple :)","26/Sep/09 05:34;teodor;After full night run all is good. But I saw discussion named ""commit logs are not deleted"" and yesterday I updated svn to include http://svn.apache.org/viewvc?view=rev&revision=819004 and rebuilded cassandra with  0003-columns-may-be-empty-if-the-only-pre-flush-op-was-a-CF.txt patch
And I  face to the same issue with commitlog directory: files never deleted and its modification time is not in the past:
% ls -l /spool/cassandra/commitlog 
/spool/cassandra/commitlog:
total 4460240
-rw-r--r--  1 teodor  wheel  134217855 Sep 26 09:20 CommitLog-1253912758033.log
-rw-r--r--  1 teodor  wheel  134218092 Sep 26 09:20 CommitLog-1253913622766.log
-rw-r--r--  1 teodor  wheel  134217873 Sep 26 09:20 CommitLog-1253914489217.log
-rw-r--r--  1 teodor  wheel  134217828 Sep 26 09:20 CommitLog-1253915364409.log
-rw-r--r--  1 teodor  wheel  134218028 Sep 26 09:20 CommitLog-1253916234925.log
-rw-r--r--  1 teodor  wheel  134218470 Sep 26 09:20 CommitLog-1253917113214.log
-rw-r--r--  1 teodor  wheel  134218152 Sep 26 09:20 CommitLog-1253917968814.log
-rw-r--r--  1 teodor  wheel  134217926 Sep 26 09:20 CommitLog-1253918815999.log
-rw-r--r--  1 teodor  wheel  134218135 Sep 26 09:20 CommitLog-1253919704092.log
-rw-r--r--  1 teodor  wheel  134218195 Sep 26 09:20 CommitLog-1253920594414.log
-rw-r--r--  1 teodor  wheel  134218081 Sep 26 09:20 CommitLog-1253921493770.log
-rw-r--r--  1 teodor  wheel  134218658 Sep 26 09:20 CommitLog-1253922402945.log
-rw-r--r--  1 teodor  wheel  134217814 Sep 26 09:20 CommitLog-1253923245407.log
-rw-r--r--  1 teodor  wheel  134218227 Sep 26 09:20 CommitLog-1253924129426.log
-rw-r--r--  1 teodor  wheel  134218131 Sep 26 09:20 CommitLog-1253925014538.log
-rw-r--r--  1 teodor  wheel  134218027 Sep 26 09:20 CommitLog-1253925890662.log
-rw-r--r--  1 teodor  wheel  134218069 Sep 26 09:20 CommitLog-1253926743631.log
-rw-r--r--  1 teodor  wheel  134218421 Sep 26 09:20 CommitLog-1253927613273.log
-rw-r--r--  1 teodor  wheel  134218410 Sep 26 09:20 CommitLog-1253928473213.log
-rw-r--r--  1 teodor  wheel  134217909 Sep 26 09:20 CommitLog-1253929324973.log
-rw-r--r--  1 teodor  wheel  134217928 Sep 26 09:20 CommitLog-1253930275986.log
-rw-r--r--  1 teodor  wheel  134217996 Sep 26 09:20 CommitLog-1253931167174.log
-rw-r--r--  1 teodor  wheel  134217760 Sep 26 09:20 CommitLog-1253932029445.log
-rw-r--r--  1 teodor  wheel  134218328 Sep 26 09:20 CommitLog-1253932891947.log
-rw-r--r--  1 teodor  wheel  134217793 Sep 26 09:20 CommitLog-1253933740244.log
-rw-r--r--  1 teodor  wheel  134217834 Sep 26 09:20 CommitLog-1253934614337.log
-rw-r--r--  1 teodor  wheel  134217758 Sep 26 09:20 CommitLog-1253935459196.log
-rw-r--r--  1 teodor  wheel  134217875 Sep 26 09:20 CommitLog-1253936347082.log
-rw-r--r--  1 teodor  wheel  134218323 Sep 26 09:20 CommitLog-1253937268917.log
-rw-r--r--  1 teodor  wheel  134217908 Sep 26 09:20 CommitLog-1253938116400.log
-rw-r--r--  1 teodor  wheel  134218158 Sep 26 09:20 CommitLog-1253938991211.log
-rw-r--r--  1 teodor  wheel  134217778 Sep 26 09:20 CommitLog-1253939833974.log
-rw-r--r--  1 teodor  wheel  134217822 Sep 26 09:20 CommitLog-1253940711231.log
-rw-r--r--  1 teodor  wheel  134218030 Sep 26 09:20 CommitLog-1253941558463.log
-rw-r--r--  1 teodor  wheel     524288 Sep 26 09:20 CommitLog-1253942415880.log

I saw discussion named ""commit logs are not deleted"" and yesterday I updated svn to  revision 819092 and rebuild cassandra with  	0003-columns-may-be-empty-if-the-only-pre-flush-op-was-a-CF.txt patch","26/Sep/09 12:54;jbellis;committed patches from this ticket.

commitlog removal is a separate issue.","27/Sep/09 12:34;hudson;Integrated in Cassandra #210 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/210/])
    r/m unused byte-tracking code
patch by jbellis; tested for  by Teodor Sigaev
r/m unnecessary SortedFlushable class.  Keeping a 2nd reference to the Flushable in question,
when it is only used to pass back to that Flushable for the write phase, was unnecessary & confusing.
patch by jbellis; tested for  by Teodor Sigaev
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven build broken because it does not include all local dependencies,CASSANDRA-588,12442018,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,adamfisk,adamfisk,adamfisk,12/1/2009 1:26,3/12/2019 14:10,3/13/2019 22:24,12/1/2009 4:20,0.5,,,,0,,,,,,The maven build is broken because it's missing the reardencommerce and the simple json jars from the local jars it includes.,,,1200,1200,,0%,1200,1200,,,,,,,,,,,,01/Dec/09 01:27;adamfisk;pom_patch.diff;https://issues.apache.org/jira/secure/attachment/12426482/pom_patch.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,20:39.6,,,no_permission,,,,,,,,,,,,19768,,,Sat Dec 05 12:34:24 UTC 2009,,,,,,0|i0fzuv:,91407,,,,,,,,,,,01/Dec/09 01:27;adamfisk;Just adds in the two missing jars.,"01/Dec/09 01:29;adamfisk;I'm a little unclear on how the submit patch workflow is designed, but I attached the patch to the ticket.","01/Dec/09 04:20;urandom;Nope, attaching the patch works just fine. Thanks Adam!","05/Dec/09 12:34;hudson;Integrated in Cassandra #278 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/278/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
move daemon to framed transport (thrift),CASSANDRA-241,12428401,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,6/19/2009 19:43,3/12/2019 14:10,3/13/2019 22:24,8/10/2009 18:57,0.4,,,,0,,,,,,"The framed transports in thrift wrap the underlying transport to prepend the message size as a 4 byte value.  There are purported benefits to buffering, but the main purpose of these wrappers is to allow non-blocking servers to perform reads without deserialization. Of course, if the server transport is framed, the client's must be as well, and vice versa, (framed and non-framed transports are incompatible). 

CassandraDaemon is currently a threaded server with the default transport, I believe we should change it to being framed, for compatibility with non-blocking clients (this actually came up during an attempt to use a Twisted client).

This will break all existing client apps, (even if fixing them is trivial).",,,,,,,,,,,,,,,,,,,,19/Jun/09 19:44;urandom;241.txt;https://issues.apache.org/jira/secure/attachment/12411254/241.txt,09/Aug/09 22:30;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-241-optional-support-for-framed-transport.txt;https://issues.apache.org/jira/secure/attachment/12416009/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-241-optional-support-for-framed-transport.txt,10/Aug/09 17:08;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-241-optional-support-for-framed-transport.txt;https://issues.apache.org/jira/secure/attachment/12416089/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-241-optional-support-for-framed-transport.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,58:24.3,,,no_permission,,,,,,,,,,,,19607,,,Fri Nov 13 18:19:33 UTC 2009,,,,,,0|i0fxqf:,91063,,,,,,,,,,,"19/Jun/09 19:58;jbellis;Does Thrift give a meaningful error if you try connecting w/ non-framed client to framed Cassandra server or does it just silently do nonsense?

If the latter, please post to -dev and -users that this change is going to happen Monday in trunk and then I will commit it.
",19/Jun/09 20:00;urandom;It silently does nonsense. I will post to the lists.,"19/Jun/09 20:24;urandom;Also, once this is committed, http://wiki.apache.org/cassandra/ClientExamples will need to be updated accordingly.",19/Jun/09 21:57;junrao;could you explain a bit more what exactly are non-blocking clients?,22/Jun/09 14:17;urandom;A non-blocking client is one that uses a non-blocking socket and select/poll to determine when it is ready to read/write from/to the server.,"27/Jun/09 22:46;jbellis;I think the consensus on the list was that we should make the transport to use an option given that the following Thrift languages lack the Framed option.

C#
Cocoa
Haskell
Ocaml
Smalltalk","21/Jul/09 23:16;urandom;Just to provide some update:

There are a couple of reasons why I've been avoiding a configuration option to enable/disable framed transport.

1. Whether or not the transport is framed or not is an implementation detail of Thrift, it sucks that we would be forced to expose this to our users.

2. Until THRIFT-210 is complete (better yet, THRIFT-538), there won't be any one setting that will let you run a server compatible with all clients. For example, with a framed transport enabled C# clients currently aren't possible, conversely without a framed transport, Twisted clients are left out in the cold.
 
It has been my hope that the Thrift team would close at least THRIFT-210 (there is a patch attached), so that we could accommodate all of our users with a framed transport (and without requiring it to be configured).

However, considering how long THRIFT-210 has been open (and considering the complete lack of response to THRIFT-538), it may in fact be necessary to add an option so that our users can at least choose what (not  )to support.

",08/Aug/09 03:13;jbellis;Should we move this to 0.5?,08/Aug/09 05:28;euphoria;It shouldn't be too difficult to make framed an option.  I think we should add the option to 0.4 (keeping the existing non-framed as default) until further evaluation for 0.5.,09/Aug/09 02:50;jbellis;I guess it's up to Eric or other people who need Framed.  I'm okay either way.,09/Aug/09 02:58;urandom;I'll follow up with a patch shortly.,"10/Aug/09 15:15;jbellis;can you add boolean sanity checking like this?

            String syncRaw = xmlUtils.getNodeValue(""/Storage/CommitLogSync"");
            if (!""false"".equals(syncRaw) && !""true"".equals(syncRaw))
            {
                // Bool.valueOf will silently assume false for values it doesn't recognize
                throw new ConfigurationException(""Unrecognized value for CommitLogSync.  Use 'true' or 'false'."");
            }
            commitLogSync_ = Boolean.valueOf(syncRaw);
","10/Aug/09 15:50;urandom;I was trying to make this as optional as possible, and the pattern cited above doesn't support that.

For the vast majority of users this doesn't make any sense whatsoever; tweaking it is more likely to break, rather than enable. I really didn't even want to draw the sort of attention to it that an example entry in the sample config provides (and I'm still sort of wishing it was less ""documented"").

However, since I already feel ""beaten"" by this issue I'll update the patch if you feel it should be mandatory. :)",10/Aug/09 18:50;jbellis;+1,"10/Aug/09 18:54;euphoria;+1, looks good.  Thanks for accommodating, Eric.",10/Aug/09 18:57;urandom;committed.,"11/Aug/09 12:57;hudson;Integrated in Cassandra #164 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/164/])
     optional support for framed transport
","13/Nov/09 18:19;esteve;THRIFT-210 is fixed as of rev 835006. As for THRIFT-538, I think nobody is working on it, the Cocoa, Haskell, Ocaml and Smalltalk ports are not actively maintained, AFAIK.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.NegativeArraySizeException being thrown for large column names,CASSANDRA-460,12436674,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,tjake,tjake,9/27/2009 2:07,3/12/2019 14:10,3/13/2019 22:24,9/28/2009 17:47,0.4,0.5,,,0,,,,,,"When inserting large columns I'm getting this stacktrace in the cassandra log:

ERROR [ROW-MUTATION-STAGE:3] 2009-09-26 18:57:02,589 DebuggableThreadPoolExecutor.java (line 110) Error in ThreadPoolExecutor
java.lang.NegativeArraySizeException
        at org.apache.cassandra.db.ColumnSerializer.readName(ColumnSerializer.java:46)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:345)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:313)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:88)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:313)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:323)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:276)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:59)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [ROW-MUTATION-STAGE:3] 2009-09-26 18:57:02,589 CassandraDaemon.java (line 71) Fatal exception in thread Thread[ROW-MUTATION-STAGE:3,5,main]
java.lang.NegativeArraySizeException
        at org.apache.cassandra.db.ColumnSerializer.readName(ColumnSerializer.java:46)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:345)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:313)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:88)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:313)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:323)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:276)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:59)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
 INFO [ROW-MUTATION-STAGE:90] 2009-09-26 18:57:03,183 ColumnFamilyStore.java (line 367) TermVectors has reached its threshold; switching in a fresh Memtable
 INFO [R",linux,,,,,,,,,,,,,,,,,,,28/Sep/09 15:55;jbellis;460-2.patch;https://issues.apache.org/jira/secure/attachment/12420707/460-2.patch,27/Sep/09 03:16;jbellis;460.patch;https://issues.apache.org/jira/secure/attachment/12420645/460.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,55:47.2,,,no_permission,,,,,,,,,,,,19699,,,Tue Sep 29 12:34:59 UTC 2009,,,,,,0|i0fz2n:,91280,,,,,,,,,,,"27/Sep/09 03:10;tjake;The exception is reproducible with this column name:

lopadotemachoselachogaleokranioleipsanodrimhypotrimmatosilphioparaomelitokatakechymenokichlepikossyphophattoperisteralektryonoptekephalliokigklopeleiolagoiosiraiobaphetraganopterygon",27/Sep/09 03:28;tjake;+1 fixed my issue,"28/Sep/09 15:55;jbellis;Committed first patch, which fixes the internal name serialization for column names longer than 128 bytes but less than 64k.

This one adds sanity checking to provide nice error messages if a name is larger than 64k.","28/Sep/09 17:23;sammy.yu;+1 460-2.patch
",28/Sep/09 17:47;jbellis;committed to 0.4 and 0.5,"29/Sep/09 12:34;hudson;Integrated in Cassandra #212 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/212/])
    fix bit math on column name read
patch by jbellis; tested by T Jake Luciani for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read repair happens on every quorum read,CASSANDRA-462,12436783,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,edmond,edmond,9/28/2009 21:28,3/12/2019 14:10,3/13/2019 22:24,9/30/2009 21:23,0.4,,,,0,,,,,,"I have a 3 node cluster with a replication factor of 2, running on 0.4
RC1.  I've set both my read and write consistency levels to use a
quorum.

I'm observing that quorum reads keep invoking read repair and log
DigestMismatchExceptions from the StorageProxy.  In the absence of any
additional inserts, I'd expect that read repair would happen at most
once before the 2 nodes responsible for the data both have fresh views
of the data.

Here's what I see in my debug log for one machine on two consecutive
quorum reads for the data.  I get similar messages when querying any
of the 3 nodes.  Similar messages are logged on subsequent queries for
the exact same row/column.  The issue happens when reading both
supercolumns or columns.  Restarting the cluster has no effect.

DEBUG [pool-1-thread-1] 2009-09-26 00:26:20,317 CassandraServer.java
(line 305) multiget
DEBUG [pool-1-thread-1] 2009-09-26 00:26:20,360 StorageProxy.java
(line 375) strongread reading data for
SliceByNamesReadCommand(table='Analytics', key='test',
columnParent='QueryPath(columnFamilyName='Domain',
superColumnName='null', co\
lumnName='null')', columns=[www.ooyala.com,]) from 17@172.16.130.130:7000
DEBUG [pool-1-thread-1] 2009-09-26 00:26:20,365 StorageProxy.java
(line 382) strongread reading digest for
SliceByNamesReadCommand(table='Analytics', key='test',
columnParent='QueryPath(columnFamilyName='Domain',
superColumnName='null', \
columnName='null')', columns=[www.ooyala.com,]) from 18@172.16.130.131:7000
DEBUG [ROW-READ-STAGE:1] 2009-09-26 00:26:20,380 ReadVerbHandler.java
(line 100) Read key test; sending response to
EEF5BCFF-D592-F1DE-6DEE-B74029218A29@172.16.130.130:7000
DEBUG [RESPONSE-STAGE:1] 2009-09-26 00:26:20,387
ResponseVerbHandler.java (line 34) Processing response on a callback
from EEF5BCFF-D592-F1DE-6DEE-B74029218A29@172.16.130.130:7000
DEBUG [RESPONSE-STAGE:2] 2009-09-26 00:26:20,449
ResponseVerbHandler.java (line 34) Processing response on a callback
from EEF5BCFF-D592-F1DE-6DEE-B74029218A29@172.16.130.131:7000
DEBUG [pool-1-thread-1] 2009-09-26 00:26:20,474
ReadResponseResolver.java (line 84) Response deserialization time : 0
ms.
DEBUG [pool-1-thread-1] 2009-09-26 00:26:20,474
ReadResponseResolver.java (line 84) Response deserialization time : 0
ms.
 INFO [pool-1-thread-1] 2009-09-26 00:26:20,475 StorageProxy.java
(line 411) DigestMismatchException: test
DEBUG [ROW-READ-STAGE:2] 2009-09-26 00:26:20,477 ReadVerbHandler.java
(line 100) Read key test; sending response to 19@172.16.130.130:7000
DEBUG [RESPONSE-STAGE:3] 2009-09-26 00:26:20,478
ResponseVerbHandler.java (line 34) Processing response on a callback
from 19@172.16.130.130:7000
DEBUG [RESPONSE-STAGE:4] 2009-09-26 00:26:20,480
ResponseVerbHandler.java (line 34) Processing response on a callback
from 19@172.16.130.131:7000
DEBUG [pool-1-thread-1] 2009-09-26 00:26:20,481
ReadResponseResolver.java (line 84) Response deserialization time : 0
ms.
DEBUG [pool-1-thread-1] 2009-09-26 00:26:20,481
ReadResponseResolver.java (line 84) Response deserialization time : 0
ms.
 INFO [pool-1-thread-1] 2009-09-26 00:26:20,482
ReadResponseResolver.java (line 148) resolve: 1 ms.
DEBUG [pool-1-thread-2] 2009-09-26 00:27:22,099 CassandraServer.java
(line 305) multiget
DEBUG [pool-1-thread-2] 2009-09-26 00:27:22,100 StorageProxy.java
(line 375) strongread reading data for
SliceByNamesReadCommand(table='Analytics', key='test',
columnParent='QueryPath(columnFamilyName='Domain',
superColumnName='null', co\
lumnName='null')', columns=[www.ooyala.com,]) from 224@172.16.130.130:7000
DEBUG [pool-1-thread-2] 2009-09-26 00:27:22,100 StorageProxy.java
(line 382) strongread reading digest for
SliceByNamesReadCommand(table='Analytics', key='test',
columnParent='QueryPath(columnFamilyName='Domain',
superColumnName='null', \
columnName='null')', columns=[www.ooyala.com,]) from 225@172.16.130.131:7000
DEBUG [ROW-READ-STAGE:1] 2009-09-26 00:27:22,103 ReadVerbHandler.java
(line 100) Read key test; sending response to
CD1A7545-F759-1CA7-4D17-87FA4A16E2E4@172.16.130.130:7000
DEBUG [RESPONSE-STAGE:1] 2009-09-26 00:27:22,103
ResponseVerbHandler.java (line 34) Processing response on a callback
from CD1A7545-F759-1CA7-4D17-87FA4A16E2E4@172.16.130.130:7000
DEBUG [RESPONSE-STAGE:2] 2009-09-26 00:27:22,107
ResponseVerbHandler.java (line 34) Processing response on a callback
from CD1A7545-F759-1CA7-4D17-87FA4A16E2E4@172.16.130.131:7000
DEBUG [pool-1-thread-2] 2009-09-26 00:27:22,108
ReadResponseResolver.java (line 84) Response deserialization time : 1
ms.
DEBUG [pool-1-thread-2] 2009-09-26 00:27:22,108
ReadResponseResolver.java (line 84) Response deserialization time : 0
ms.
 INFO [pool-1-thread-2] 2009-09-26 00:27:22,109 StorageProxy.java
(line 411) DigestMismatchException: test
DEBUG [ROW-READ-STAGE:2] 2009-09-26 00:27:22,114 ReadVerbHandler.java
(line 100) Read key test; sending response to 226@172.16.130.130:7000
DEBUG [RESPONSE-STAGE:3] 2009-09-26 00:27:22,114
ResponseVerbHandler.java (line 34) Processing response on a callback
from 226@172.16.130.130:7000
DEBUG [RESPONSE-STAGE:4] 2009-09-26 00:27:22,205
ResponseVerbHandler.java (line 34) Processing response on a callback
from 226@172.16.130.131:7000
DEBUG [pool-1-thread-2] 2009-09-26 00:27:22,206
ReadResponseResolver.java (line 84) Response deserialization time : 0
ms.
DEBUG [pool-1-thread-2] 2009-09-26 00:27:22,206
ReadResponseResolver.java (line 84) Response deserialization time : 0
ms.
 INFO [pool-1-thread-2] 2009-09-26 00:27:22,207
ReadResponseResolver.java (line 148) resolve: 1 ms.",,,,,,,,,,,,,,,,,,,,29/Sep/09 22:47;jbellis;462-0-4.patch;https://issues.apache.org/jira/secure/attachment/12420852/462-0-4.patch,29/Sep/09 22:33;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-462-formatting-cleanup.txt;https://issues.apache.org/jira/secure/attachment/12420846/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-462-formatting-cleanup.txt,29/Sep/09 22:33;jbellis;ASF.LICENSE.NOT.GRANTED--0002-logging.txt;https://issues.apache.org/jira/secure/attachment/12420847/ASF.LICENSE.NOT.GRANTED--0002-logging.txt,29/Sep/09 22:33;jbellis;ASF.LICENSE.NOT.GRANTED--0003-compute-digest-correctly-using-byte-contents-instea.txt;https://issues.apache.org/jira/secure/attachment/12420848/ASF.LICENSE.NOT.GRANTED--0003-compute-digest-correctly-using-byte-contents-instea.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,35:20.1,,,no_permission,,,,,,,,,,,,19701,,,Thu Oct 01 12:34:51 UTC 2009,,,,,,0|i0fz33:,91282,,,,,,,,,,,28/Sep/09 21:35;jbellis;Do you see the same behavior when you run w/ replication factor of 3? (Wiping out your data and commitlog & restarting is the easiest way to change replication atm.)  I know there is at least one group testing 3 nodes / replication 3.,"28/Sep/09 22:20;edmond;The same behavior happens on repeated reads w/ a replication factor of 3:

DEBUG [pool-1-thread-10] 2009-09-28 22:13:02,058 CassandraServer.java (line 305) multiget
DEBUG [pool-1-thread-10] 2009-09-28 22:13:02,059 StorageProxy.java (line 375) strongread reading data for SliceByNamesReadCommand(table='Analytics', key='00NGRxOjiHjQzlRIts5DqWugci8H2uQU', columnParent='QueryPath(columnFamilyN\
ame='timestamp', superColumnName='[B@1ec278b5', columnName='null')', columns=[country:US,]) from 837@172.16.130.130:7000
DEBUG [pool-1-thread-10] 2009-09-28 22:13:02,059 StorageProxy.java (line 382) strongread reading digest for SliceByNamesReadCommand(table='Analytics', key='00NGRxOjiHjQzlRIts5DqWugci8H2uQU', columnParent='QueryPath(columnFamil\
yName='timestamp', superColumnName='[B@1ec278b5', columnName='null')', columns=[country:US,]) from 838@172.16.130.131:7000
DEBUG [pool-1-thread-10] 2009-09-28 22:13:02,059 StorageProxy.java (line 382) strongread reading digest for SliceByNamesReadCommand(table='Analytics', key='00NGRxOjiHjQzlRIts5DqWugci8H2uQU', columnParent='QueryPath(columnFamil\
yName='timestamp', superColumnName='[B@1ec278b5', columnName='null')', columns=[country:US,]) from 838@172.16.130.129:7000
DEBUG [ROW-READ-STAGE:7] 2009-09-28 22:13:02,061 ReadVerbHandler.java (line 100) Read key 00NGRxOjiHjQzlRIts5DqWugci8H2uQU; sending response to 48325865-7324-FA54-7789-7ABAD95244B8@172.16.130.130:7000
DEBUG [RESPONSE-STAGE:2] 2009-09-28 22:13:02,061 ResponseVerbHandler.java (line 34) Processing response on a callback from 48325865-7324-FA54-7789-7ABAD95244B8@172.16.130.130:7000
DEBUG [RESPONSE-STAGE:4] 2009-09-28 22:13:02,063 ResponseVerbHandler.java (line 34) Processing response on a callback from 48325865-7324-FA54-7789-7ABAD95244B8@172.16.130.131:7000
DEBUG [pool-1-thread-10] 2009-09-28 22:13:02,063 ReadResponseResolver.java (line 84) Response deserialization time : 0 ms.
DEBUG [pool-1-thread-10] 2009-09-28 22:13:02,063 ReadResponseResolver.java (line 84) Response deserialization time : 0 ms.
 INFO [pool-1-thread-10] 2009-09-28 22:13:02,063 StorageProxy.java (line 411) DigestMismatchException: 00NGRxOjiHjQzlRIts5DqWugci8H2uQU
DEBUG [ROW-READ-STAGE:8] 2009-09-28 22:13:02,065 ReadVerbHandler.java (line 100) Read key 00NGRxOjiHjQzlRIts5DqWugci8H2uQU; sending response to 839@172.16.130.130:7000
DEBUG [RESPONSE-STAGE:1] 2009-09-28 22:13:02,065 ResponseVerbHandler.java (line 34) Processing response on a callback from 839@172.16.130.130:7000
DEBUG [RESPONSE-STAGE:3] 2009-09-28 22:13:02,066 ResponseVerbHandler.java (line 34) Processing response on a callback from 839@172.16.130.129:7000
DEBUG [RESPONSE-STAGE:3] 2009-09-28 22:13:02,066 ResponseVerbHandler.java (line 34) Processing response on a callback from 839@172.16.130.131:7000
DEBUG [pool-1-thread-10] 2009-09-28 22:13:02,067 ReadResponseResolver.java (line 84) Response deserialization time : 0 ms.
DEBUG [pool-1-thread-10] 2009-09-28 22:13:02,067 ReadResponseResolver.java (line 84) Response deserialization time : 0 ms.
 INFO [pool-1-thread-10] 2009-09-28 22:13:02,067 ReadResponseResolver.java (line 148) resolve: 0 ms.


","29/Sep/09 21:51;jbellis;I can reproduce on my test cluster (5 nodes, 3 replicas).","29/Sep/09 22:34;jbellis;03
    compute digest correctly, using byte[] contents instead of toString. switch to MD5 for non-wtfery.

02
    logging

01
    formatting + cleanup
",29/Sep/09 22:47;jbellis;version of 03 that applies to 0.4 branch.  please test this one if you are using 0.4,"30/Sep/09 21:23;jbellis;From the ML: ""I patched in your change and no longer see a read
repair for every quorum read in the debug logs.  I also reran a load
test consisting of only simple read operations, and my quorum read
throughput has doubled now that the extraneous read repairs aren't
occurring.""

Applied the -0-4 patch to 0.4 branch, and 01,02,03 patches to trunk.","01/Oct/09 12:34;hudson;Integrated in Cassandra #214 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/214/])
    logging for digests.  patch by jbellis for 
formatting + cleanup.  patch by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compactions might remove tombstones without removing the actual data,CASSANDRA-604,12442530,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,rrabah,rrabah,12/5/2009 0:12,3/12/2019 14:10,3/13/2019 22:24,12/8/2009 18:45,0.5,,,,0,,,,,,"I was looking at the code for compaction, and noticed that when we are doing compactions during the normal course of
Cassandra, we call:

           for (List<SSTableReader> sstables :
getCompactionBuckets(ssTables_, 50L * 1024L * 1024L))
           {
               if (sstables.size() < minThreshold)
               {
                   continue;
               }
               other wise docompactions...

where getCompactionBuckets puts in buckets very small files, or files
that are 0.5-1.5 of each other's sizes. It will only compact those if
they are >= minimum threshold which is 4 by default.
So far so good. Now how about this scenario, I have an old entry that
I inserted long time ago and that was compacted into a 75MB file.
There are fewer 75MB files than 4. I do many deletes, and I end with 4
extra sstable files filled with tombstones, each about 300 MB large.
These 4 files are compacted together and in the compaction code, if
the tombstone is there we don't copy it over to the new file. Now
since we did not compact the 75MB files, but we compacted the
tombstone files, that leaves us with the tombstone gone, but
the data still intact in the 75MB file. If we compacted all the
files together I don't think that would be a problem, but since we
only compact 4, this potentially leaves data not cleaned.",Cent-OS,,,,,,,,,,,,,,,,,,,08/Dec/09 06:58;jbellis;604.patch;https://issues.apache.org/jira/secure/attachment/12427304/604.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,36:32.7,,,no_permission,,,,,,,,,,,,19775,,,Wed Dec 09 12:42:41 UTC 2009,,,,,,0|i0fzyf:,91423,,,,,,,,,,,"06/Dec/09 18:04;rrabah;Thinking about it some more, there is another case when data can be lost. In the above case the file containing the tombstone was compacted by itself before the data file.
The second case is that the file containing the data is compacted by itself before the tombstone is compacted. 

So in both cases, it seems like the only viable solution I can think of, is to only remove the tombstones when every single SSTable file for the column family is compacted (I.E. major compaction). Otherwise, the tombstone should stick around.

Does that make sense?","06/Dec/09 21:36;jbellis;I don't follow.

All the tombstone does is suppress earlier versions.  And all compaction does is write out a new sstable containing only the newest version.  So compacting data w/o the tombstone will either yield (1) a single version that is newer than the tombstone, in which case the tombstone will be ignored on reads, or (2) a single version older than the tombstone, in which case it will still be supressed.  Either way correctness is preserved.","07/Dec/09 00:33;rrabah;What I meant was there are 2 orders of compactions that might happen that would lead to the clean up of tombstones but not data:

Case 1) I described first, data is in sstable 1, tombstone in sstable 2. sstables 2-5 are compacted producing sstable 6, and sstable 6 has no tombstone. That leaves us with sstable 1 data, sstable 6 no tombstone --> Probably bad. In this case tombstones are compacted before the data is ever compacted.

Case 2) data is in sstable 1, tombstone is in sstable 5. sstable 1-4 get compacted to sstable 6 that has data, so now we have sstable 5 tombstone, sstable 6 data. sstable 5,7,8,9 are compacted producing sstable 10. sstable 6 data, sstable 10 no tombstone --> Probably bad.  In this case, data was compacted first, then tombstones. 

Both cases can probably be fixed in the same manner, but just wanted to point out all scenarios which I can think of that can cause this problem. 


","07/Dec/09 00:55;jbellis;That makes sense.  I agree that only GCing tombstones during major compactions (or, other compactions that happen to include all sstables) is the easiest fix.","08/Dec/09 06:58;jbellis;patch to only GC when compacting all sstables, as outlined above.

Ramzi to review?",08/Dec/09 18:33;rrabah;Looks great. I will test the patch with a higher volume of inserts and deletes and let you know how the test goes.,08/Dec/09 18:45;jbellis;committed.  let us know if your testing uncovers any problems.,"09/Dec/09 12:42;hudson;Integrated in Cassandra #282 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/282/])
    only GC when compacting all sstables, to avoid situations where the data a tombstone is intended to supress is in an sstable that is not part of the compaction set.
patch by jbellis; reviewed by Ramzi Rabah for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra 0.5 version throttles and sometimes kills traffic to a node if you restart it.,CASSANDRA-651,12444117,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,rrabah,rrabah,12/23/2009 17:35,3/12/2019 14:10,3/13/2019 22:24,12/30/2009 23:20,0.5,,,,1,,,,,,"From the cassandra user message board: 
""I just recently upgraded to latest in 0.5 branch, and I am running
into a serious issue. I have a cluster with 4 nodes, rackunaware
strategy, and using my own tokens distributed evenly over the hash
space. I am writing/reading equally to them at an equal rate of about
230 reads/writes per second(and cfstats shows that). The first 3 nodes
are seeds, the last one isn't. When I start all the nodes together at
the same time, they all receive equal amounts of reads/writes (about
230).
When I bring node 4 down and bring it back up again, node 4's load
fluctuates between the 230 it used to get to sometimes no traffic at
all. The other 3 still have the same amount of traffic. And no errors
what so ever seen in logs. "" ",latest in 0.5 branch,,,,,,,,,,,,,,,,,,,29/Dec/09 18:58;gdusbabek;651-v2.patch;https://issues.apache.org/jira/secure/attachment/12429078/651-v2.patch,29/Dec/09 23:09;gdusbabek;651-v3.patch;https://issues.apache.org/jira/secure/attachment/12429102/651-v3.patch,30/Dec/09 20:36;jbellis;651-v4.patch;https://issues.apache.org/jira/secure/attachment/12429168/651-v4.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,00:42.6,,,no_permission,,,,,,,,,,,,19803,,,Thu Dec 31 12:38:20 UTC 2009,,,,,,0|i0g08v:,91470,,,,,,,,,,,"23/Dec/09 17:39;rrabah;More info:
 I do see that Node X.X.X.X is dead, and
Node X.X.X.X has restarted.

This show up on all the 3 other servers:
 INFO [Timer-1] 2009-12-22 20:38:43,738 Gossiper.java (line 194)
InetAddress /10.6.168.20 is now dead.

Node /10.6.168.20 has restarted, now UP again
 INFO [GMFD:1] 2009-12-22 20:43:12,812 StorageService.java (line 475)
Node /10.6.168.20 state jump to normal

","23/Dec/09 17:43;rrabah;This is definitely a regression in 0.5. I tested out 0.4.2 and it works perfectly fine, and load goes back up to 100% on the restarted node. ","28/Dec/09 22:00;brandon.williams;I was able to reproduce this in a 4 node setup as well.  The recovered node does not appear to receive any writes after rejoining the cluster, and I receive TimedOutExceptions in the client.   I was able to write directly to the recovered node and things appeared to work, however after a while a different node OOM'd.  I examined the dump in MAT and it shows org.apache.cassandra.net.MessagingService occupies 72.53% of the available heap, followed by java.util.concurrent.LinkedBlockingQueue using 11.87%.","29/Dec/09 03:34;steel_mental;Confirm this issue by 8 nodes tests,

After read system.log, I found after one node down and up again, some other nodes will not establish tcp connection to it(on tcp port 7000 ) forever! 
And read request sent to it (into Pending-Writes because socket channel is closed) will not sent to ethernet forever(from observing tcpdump).

maybe PendingWrites Queue consume lots memory and OOM (yes, it not the recovered node, but  the node who try to send request to recovered node!)

It's seems when recovered node going down, some other node's socket channel was reset , after it come back, these socket channel remain closed, forever
","29/Dec/09 14:26;jbellis;Brandon Williams said: ""I see what's happening with 651 -- TcpConnectionManager keeps trying to reuse a closed connection and never opens new ones to the recovered node""

sounds like a regression from CASSANDRA-488 to me.",29/Dec/09 18:45;gdusbabek;Patched into trunk.  Link the gossip and messaging service so that invalid connection pools can be shutdown when a node goes offline.,29/Dec/09 18:46;gdusbabek;Patch is for trunk.,"29/Dec/09 18:51;jbellis;relying on FD to notice is not going to work, though, since FD is not instantaneous (and cannot be made so).  [edit: that is, a node could die and come back, or be partitioned and be available again, quickly enough that FD does not notice but old connections are still invalid.] 

can we have it attempt to reconnect when it encounters an error sending instead?",29/Dec/09 18:58;gdusbabek;The last patch diffed in the wrong direction.  This one is correct.,"29/Dec/09 19:09;jbellis;that said, it could be good to have the FD *in addition* to the other, so that if a node goes down for a while that doesn't have much traffic, we don't lose the first attempted message once it's back up unnecessarily.","29/Dec/09 19:54;brandon.williams;I can still reproduce the issue with this patch applied.  I'm receiving the following traceback:

ERROR - Fatal exception in thread Thread[TCP Selector Manager,5,main]
java.lang.AssertionError
        at org.apache.cassandra.net.TcpConnectionManager.destroy(TcpConnectionManager.java:85)
        at org.apache.cassandra.net.TcpConnection.errorClose(TcpConnection.java:319)
        at org.apache.cassandra.net.TcpConnection.connect(TcpConnection.java:364)
        at org.apache.cassandra.net.SelectorManager.doProcess(SelectorManager.java:143)
        at org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:107)

Because ackCon is already null.  With the assert removed, I'm unable to reproduce.","29/Dec/09 23:09;gdusbabek;Updated to include replacing calls to destroy() with shutdown().  Chances are if one TC is crappy, the other is not going to be useful either.","30/Dec/09 01:02;brandon.williams;+1, no longer reproducible with this patch.  The recovered node begins receiving writes normally.","30/Dec/09 13:40;gdusbabek;Jaako and I had a discussion in which we agreed that it would be better to have MessagingService implement IEndPointStateChangeSubscriber and subscribe to the gossiper rather than just implement IFailureDetector.  This would have provided a way to basically turn connection pools on and off and would allow writes to fail a bit faster.

I implemented that this morning.  It's a few more lines of code and all it really buys us is more descriptive error messages.  We'll just have to put up with errored writes until gossip takes the failed node out of the ring.","30/Dec/09 18:51;rrabah;+1 from me too, this seems to have fixed the problem. ","30/Dec/09 19:01;jbellis;I still think we should not rely on FD, or we will still hit this bug with short-lived partitions (which do occur in the wild).

something like brandon's throwing an exception if not connected or awaiting connection.

I'm still baffled that write() apparently doesn't throw when the connection dies...  Are we missing something there?","30/Dec/09 19:27;jbellis;My mistake: write _was_ throwing, but clearing the old conn out was not working.  Still trying to understand why.","30/Dec/09 19:30;gdusbabek;MessagingService.sendOneWay inconveniently swallows errors, so StorageProxy is never wise about them.","30/Dec/09 20:28;jbellis;Got it: the problem is that sendOneWay calls shutdown on SocketException, not errorClose.  so the part that really matters in gary's fix is adding the nulling out to shutdown.","30/Dec/09 20:35;jbellis;this version gets rid of the formely-problematic-and-now-redundant SocketException block, and renames TCM.shutdown to reset.",30/Dec/09 22:06;gdusbabek;Reviewed.  +1 on the v4 patch.,30/Dec/09 23:20;gdusbabek;Fixed in trunk and 0.5 branch.  Patch by Gary Dusbabek and Jonathan Ellis.  Reviewed by same.,"31/Dec/09 12:38;hudson;Integrated in Cassandra #309 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/309/])
      TcpConnectionManager was holding on to disconnected connections, giving the false indication they were being used.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insert/Get Contention,CASSANDRA-724,12446007,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,1/20/2010 8:26,3/12/2019 14:10,3/13/2019 22:24,2/4/2010 19:59,0.6,,,,0,,,,,,"We tried out the socket io patch in CASSANDRA-705, tested the latest JVM of b18 for 1.6. Still seeing very strange insert times. We see this with get_slices as well but it's easy to reproduce with batch_insert. I wonder if its related to Memtable contention, it's pretty easy to see the slow times when you restart the test script attached. We are running this on a 7 node cluster, <1% cpu. Consistency Level of 1.

Results
---------------------
Slow insert test.10882 0.203548192978
Slow insert test.18005 0.203876972198
Slow insert test.21154 0.204496860504
Slow insert test.22054 0.0444049835205
Slow insert test.26445 0.201545000076",,,,,,,,,,,,,,,,,,,,04/Feb/10 19:04;jbellis;ASF.LICENSE.NOT.GRANTED--0001-decouple-periodic-sync-mode-from-commit-log-append.txt;https://issues.apache.org/jira/secure/attachment/12434856/ASF.LICENSE.NOT.GRANTED--0001-decouple-periodic-sync-mode-from-commit-log-append.txt,04/Feb/10 19:04;jbellis;ASF.LICENSE.NOT.GRANTED--0002-replace-gc-after-each-compaction-w-gc-before-compactio.txt;https://issues.apache.org/jira/secure/attachment/12434857/ASF.LICENSE.NOT.GRANTED--0002-replace-gc-after-each-compaction-w-gc-before-compactio.txt,04/Feb/10 19:04;jbellis;ASF.LICENSE.NOT.GRANTED--0003-only-gc-if-there-are-undeleted-sstables-that-gc-ing-co.txt;https://issues.apache.org/jira/secure/attachment/12434858/ASF.LICENSE.NOT.GRANTED--0003-only-gc-if-there-are-undeleted-sstables-that-gc-ing-co.txt,20/Jan/10 23:25;jbellis;debug.patch;https://issues.apache.org/jira/secure/attachment/12430946/debug.patch,20/Jan/10 08:27;lenn0x;test_case.py;https://issues.apache.org/jira/secure/attachment/12430857/test_case.py,,,,,,,,,5,,,,,,,,,,,,,,,,,,,48:30.7,,,no_permission,,,,,,,,,,,,19838,,,Wed Feb 17 17:54:47 UTC 2010,,,,,,0|i0g0on:,91541,,,,,,,,,,,"20/Jan/10 08:29;lenn0x;Just an observation, we see this happening on 99% of keys that need to send a remote write.","20/Jan/10 21:48;jbellis;I can reproduce this on a single-node setup, so I think it is possible you are seeing two effects: one from the messagingservice stack (Brandon points out that this happens much more frequently w/o CASSANDRA-705, than with it), and one from the commitlog sync (which I can reproduce on a single node system).","20/Jan/10 23:11;jbellis;this addresses latency from waiting for commitlog append to finish (which will be delayed if commitlog is busy syncing).  in batch mode we have to wait because that is part of our contract, but in periodic mode we do not.

705 will be committed soon and that will address that.
","20/Jan/10 23:25;jbellis;patch to add debug timing info if you want to investigate further.

there does seem to be occasional latency spikes inside ColumnFamilyStore.apply that I do not yet understand.

when cpus are busy w/ compaction latency increases.  no real surprise there.

thrift sometimes adds 10s of ms of latency according to the differences b/t what my python client sees and what CassandraServer sees.  the java side of thrift does call setTcpNoDelay(true), but the python side does not -- the equivalent would be, setsockopt(SOL_TCP, TCP_NODELAY, 1).  that is probably the culprit.
","21/Jan/10 21:07;lenn0x;We didn't see much change, I think applying debug is going to be required. It ranges from 45ms to 800ms sometimes.","22/Jan/10 22:56;jbellis;Brandon's did some more testing and found that the System.gc() we request (to allow cleaning up obsolete sstables after a compaction) is the culprit.

Maybe it's time to experiment w/ the g1 garbage collector: http://java.sun.com/javase/technologies/hotspot/gc/g1_intro.jsp

Alternatively, one workaround might be to only issue the gc() request if we're within some percent of the disk filling up (we can use File.getUsableSpace / File.getTotalSpace for that)","26/Jan/10 23:27;jbellis;patches 02 and 03 will reduce your System.gc frequency (as long as you have spare disk space):

03
    only gc if there are undeleted sstables that gc-ing could free

02
    replace gc after each compaction w/ gc before compaction/flush only if we need it for the file space

01
    decouple periodic sync mode from commit log append [original patch posted]

",04/Feb/10 17:23;jbellis;rebased again,"04/Feb/10 19:49;brandon.williams;+1, much improved for me:

Slow insert test.15910 0.0661840438843
Slow insert test.37799 0.073842048645
Slow insert test.38254 0.0541589260101
Slow insert test.46248 0.0541749000549
Slow insert test.56482 0.0474050045013
Slow insert test.70314 0.0435261726379
Slow insert test.76370 0.0660541057587
Slow insert test.170684 0.0553348064423
Slow insert test.170685 0.0560541152954
Slow insert test.202273 0.0667309761047

I also confirmed w/verbose:gc that the long gc pauses related to compaction/deletion are gone.",04/Feb/10 19:59;jbellis;committed,"17/Feb/10 17:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sending random data crashes thrift service,CASSANDRA-475,12437517,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,zznate,urandom,urandom,10/7/2009 14:44,3/12/2019 14:10,3/13/2019 22:24,7/21/2010 16:27,0.7 beta 1,,,,1,,,,,,"Use dd if=/dev/urandom count=1 | nc $host 9160 as a handy recipe for shutting a cassandra instance down. 

Thrift has spoken (see THRIFT-601), but ""Don't Do That"" is probably an insufficient answer for our users. ",,,,,,,,,,,,,,,,,,,,11/Jul/10 01:05;zznate;trunk-475-config.txt;https://issues.apache.org/jira/secure/attachment/12449181/trunk-475-config.txt,14/Jul/10 06:17;zznate;trunk-475-src-3.txt;https://issues.apache.org/jira/secure/attachment/12449424/trunk-475-src-3.txt,20/Jul/10 18:52;zznate;trunk-475-src-4.txt;https://issues.apache.org/jira/secure/attachment/12449957/trunk-475-src-4.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,42:11.4,,,no_permission,,,,,,,,,,,,19710,,,Wed Jul 21 16:27:41 UTC 2010,,,,,,0|i0eoqn:,83770,,,,,,,,,,,"10/Mar/10 19:42;jbellis;We don't have the resources to devote to properly fixing THRIFT-601, so we'll have to close this as wontfix until or unless something changes there.

If you need to expose Cassandra to untrusted sources, I suggest helping with Avro integration.  (Ask Eric for how, he is the closest to that.)","10/Jun/10 15:24;jbellis;THRIFT-601 has reportedly been fixed.  Not sure how painful their fix is for us, we should have a look.",02/Jul/10 14:49;paixaop;Do you know if their patch has been committed to to cassandra's trunk?,02/Jul/10 15:02;jbellis;this ticket is open precisely to incorporate a newer thrift and make any other necessary changes to fix the problem,"09/Jul/10 05:25;jbellis;looks like we need to set a transport max length and a protocol read length (besides upgrading to a new thrift jar).

unclear if these would prohibit using large binary column values.  may need to make them configurable.  (would transport length be read length + some overhead?  or is transport length just a buffer size that we can leave at some reasonably safe value and is transparent to the rest?)","10/Jul/10 01:06;zznate;TFramedTransport maxLength is just a frame buffer size. It defaults to 2^31-1 in the latest thrift. Im not sure tweaking that buys us much given the following. 

TBinaryProtocol.readLength is what needs to be set. *NOTE*: The value provided here will effectively be the max column size (there is no overhead due to thrift's delimitation granularity). 

This should be configurable with a sane default. How about thrift_max_message_length with a default of 1 (in MB). (This is the default max_packet_size in mysql, so might be easier to grok if we match that). 


","10/Jul/10 03:02;jbellis;so readLength is the max size of a single field, and maxLength the max size of the entire rpc call?

is maxLength actually controlling a byte[] size somewhere, or not?  if it is then we do need to set it, or if the first 4 bytes correspond to a large int we still OOM.","10/Jul/10 05:33;zznate;Yes, maxLength does directly control the size of the input byte[] on TFramedTransport, so it is succeptible to the same issue. 

I would like to change thrift_framed_transport to thrift_framed_transport_size with 0 (in MB as well) as the default indicating TSocket over TFramedTransport","10/Jul/10 06:30;zznate;Looks like the thrift folks have changed the parameterization of TBase: THRIFT-759

Unfortunately, this beat our patch for THRIFT-601 by a couple of days. Thoughts?","10/Jul/10 14:18;jbellis;If you're asking ""is this worth having to rewrite a bunch of our code"" then the answer is ""yes, but this is why we upgrade thrift version so infrequently.""","10/Jul/10 16:50;zznate;We are agreed on the importance of this. The changes on the surface seem trivial, since Comparator is widely used already. My concerns here are strictly schedule related given the process overhead involved in touching so many files in active development. 

I'm going to have to put this down temporarily, until I can focus on this and make the change a one-shot deal (or as close to that as possible). 

EDIT: I just added CASSANDRA-1266 to track the jar upgrade separately so we don't muddy the waters on this issue.",10/Jul/10 17:05;zznate;I can continue with the config modifications required if the changes mentioned above are cool,"11/Jul/10 01:05;zznate;- trunk-475-config.txt updates the yaml config
- trunk-475-src.txt updates java src

EDIT: I dropped in a default setting of 256mb in the Converter when someone had framed transport configured in storage-conf. Was not sure what all to do there except add a sane default and a warning message. ",14/Jul/10 02:22;jbellis;do we need a sanity check that frame size must be < message length?,"14/Jul/10 03:42;zznate;I thought about that, but it's pretty clear exception wise when you go over the frame size that it is a transport issue","14/Jul/10 03:54;zznate;Come to think, nice to have completeness-wise and was easy enough to add. 

trunk-475-src-2.txt superseeds trunk-475-src.txt ","14/Jul/10 05:18;jbellis;hmm, why isn't that check firing with the default frame of 256 and length of 1?","14/Jul/10 06:17;zznate;I'm gonna blame that one on your central-TX weather melting my brain. 

trunk-475-src-3.txt replaces trunk-475-src-2.txt, fixes comparisson induced by mild heat stroke. ",14/Jul/10 14:01;jbellis;is thrift smart enough to re-use those byte[] buffers for multiple requests?  allocating byte[] is expensive in java.,"14/Jul/10 22:38;zznate;Both TBinaryProtocol and TFramedTransport use the limits in checks only. The size used to construct the byte[] is pulled from the first couple bytes of any thrift message. When junk was sent, this size was getting intepreted as extremely large values.","15/Jul/10 01:22;jbellis;That makes sense.

That would be an obvious optimization for them to make, then -- allocate a buffer at the max size, and re-use it.",15/Jul/10 01:22;jbellis;committed,"15/Jul/10 13:42;brandon.williams;I'm occasionally getting this error with stress.py after doing a few million inserts:

ERROR 13:30:12,158 Thrift error occurred during processing of message.
org.apache.thrift.TException: Message length exceeded: 32
        at org.apache.thrift.protocol.TBinaryProtocol.checkReadLength(TBinaryProtocol.java:384)
        at org.apache.thrift.protocol.TBinaryProtocol.readBinary(TBinaryProtocol.java:361)
        at org.apache.cassandra.thrift.Column.read(Column.java:498)
        at org.apache.cassandra.thrift.ColumnOrSuperColumn.read(ColumnOrSuperColumn.java:351)
        at org.apache.cassandra.thrift.Mutation.read(Mutation.java:346)
        at org.apache.cassandra.thrift.Cassandra$batch_mutate_args.read(Cassandra.java:16780)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:3041)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2531)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)","15/Jul/10 19:04;jhermes;Hudson also found -this- an error in build 492.
I assume the hadoop test that failed found it by doing a lot of inserts as above.","15/Jul/10 20:34;zznate;checkReadLength in TBinaryProtocol looks like it is treating the readLength attribute as an instance variable and is therefore slowly getting decremented on each call!

I'm verifying my findings w/ thrift folks now",15/Jul/10 20:46;zznate;THRIFT-820 created and patch submitted. Waiting on their review and feedback.,"16/Jul/10 13:02;hudson;Integrated in Cassandra #493 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/493/])
    ","20/Jul/10 18:52;zznate;Resets the input and output protocol on each after each successful call to process in CustomTThreadPoolServer

passes nosetests and stress.py with 5 million rows",20/Jul/10 20:30;brandon.williams;Verified this solves the issue and checked for any adverse performance effects.  Committed.,20/Jul/10 20:59;jbellis;is this going to hurt performance?  (not sure how heavyweight getProtocol is),20/Jul/10 21:05;brandon.williams;I checked the performance and couldn't see any noticeable difference.  The extra garbage might minorly exacerbate CASSANDRA-1014.,"21/Jul/10 12:50;hudson;Integrated in Cassandra #496 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/496/])
    Reset the input and output protocol on each after each successful call.  Patch by Nate McCall, reviewed by brandonwilliams for CASSANDRA-475
","21/Jul/10 16:15;jigneshdhruv;Hello,

I am using the latest source code from trunk. SVN 961952

After few hundred thousand inserts cassandra crashes and is throwing 2 different types of exceptions:
The first one being:
org.apache.thrift.transport.TTransportException
at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:369)
at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:295)
at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:202)
at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:960)
at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:944)
at com.cbsi.pi.rtss.data.cassandra.CassandraDataManager.insert(CassandraDataManager.java:107)
at com.cbsi.pi.rtss.service.bulk.BulkThread.run(BulkThread.java:59)
at java.lang.Thread.run(Unknown Source)

and the second one being:
org.apache.thrift.transport.TTransportException: java.net.SocketException: Software caused connection abort: socket write error
at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:147)
at org.apache.thrift.transport.TFramedTransport.flush(TFramedTransport.java:156)
at org.apache.cassandra.thrift.Cassandra$Client.send_set_keyspace(Cassandra.java:441)
at org.apache.cassandra.thrift.Cassandra$Client.set_keyspace(Cassandra.java:430)
at com.cbsi.pi.rtss.data.cassandra.CassandraDataManager.insert(CassandraDataManager.java:106)
at com.cbsi.pi.rtss.service.bulk.BulkThread.run(BulkThread.java:59)
at java.lang.Thread.run(Unknown Source)
Caused by: java.net.SocketException: Software caused connection abort: socket write error
at java.net.SocketOutputStream.socketWrite0(Native Method)
at java.net.SocketOutputStream.socketWrite(Unknown Source)
at java.net.SocketOutputStream.write(Unknown Source)
at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
at java.io.BufferedOutputStream.write(Unknown Source)
at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:145)
... 6 more

I was not getting this error yesterday but this morning when I updated my svn trunk I got update for following 2 files:
U src/java/org/apache/cassandra/thrift/CustomTThreadPoolServer.java
U src/java/org/apache/cassandra/scheduler/RoundRobinScheduler.java

One more thing.

Before I did a SVN update this morning, I checkout thrift and applied the patch suggested by Nate in bug THRIFT-820 https://issues.apache.org/jira/browse/THRIFT-820. When I rebuild thrift jar file and used it, I had no crashes or exceptions yesterday.

But today with/without thrift patch, I am getting exceptions mentioned above.

Thanks,
Jignesh

and that is causing the problem.",21/Jul/10 16:27;jigneshdhruv;Apologize for the confusion. I did another svn update and picked bunch of new files. I am not getting any cassandra crashes or exceptions any more.,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointer during a get_range_slice,CASSANDRA-623,12443008,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,12/10/2009 18:03,3/12/2019 14:10,3/13/2019 22:24,12/11/2009 3:58,0.5,,,,0,,,,,,"I keep running into this exception on a get_range_slice command.  It seems to be a regression because it didn't happen in r886010 but does happen in r888913

2009-12-10_17:17:22.56200 ERROR - Error in ThreadPoolExecutor
2009-12-10_17:17:22.56200 java.lang.RuntimeException: java.lang.NullPointerException
2009-12-10_17:17:22.56200       at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:53)
2009-12-10_17:17:22.56200       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
2009-12-10_17:17:22.56200       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-12-10_17:17:22.56200       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-12-10_17:17:22.56200       at java.lang.Thread.run(Thread.java:636)
2009-12-10_17:17:22.56200 Caused by: java.lang.NullPointerException
2009-12-10_17:17:22.56200       at org.apache.cassandra.dht.OrderPreservingPartitioner$1.compare(OrderPreservingPartitioner.java:45)
2009-12-10_17:17:22.56200       at org.apache.cassandra.dht.OrderPreservingPartitioner$1.compare(OrderPreservingPartitioner.java:42)
2009-12-10_17:17:22.56200       at org.apache.cassandra.db.ColumnFamilyStore.getKeyRangeRaw(ColumnFamilyStore.java:1465)
2009-12-10_17:17:22.56200       at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1506)
2009-12-10_17:17:22.56200       at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:39)
2009-12-10_17:17:22.56200       ... 4 more
2009-12-10_17:17:22.57200 ERROR - Fatal exception in thread Thread[ROW-READ-STAGE:96,5,main]
2009-12-10_17:17:22.57200 java.lang.RuntimeException: java.lang.NullPointerException
2009-12-10_17:17:22.57200       at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:53)
2009-12-10_17:17:22.57200       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
2009-12-10_17:17:22.57200       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-12-10_17:17:22.57200       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-12-10_17:17:22.57200       at java.lang.Thread.run(Thread.java:636)
2009-12-10_17:17:22.57200 Caused by: java.lang.NullPointerException
2009-12-10_17:17:22.57200       at org.apache.cassandra.dht.OrderPreservingPartitioner$1.compare(OrderPreservingPartitioner.java:45)
2009-12-10_17:17:22.57200       at org.apache.cassandra.dht.OrderPreservingPartitioner$1.compare(OrderPreservingPartitioner.java:42)
2009-12-10_17:17:22.57200       at org.apache.cassandra.db.ColumnFamilyStore.getKeyRangeRaw(ColumnFamilyStore.java:1465)
2009-12-10_17:17:22.57200       at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1506)
2009-12-10_17:17:22.57200       at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:39)
2009-12-10_17:17:22.57200       ... 4 more",r888913  RF=2 3 node cluster,,,,,,,,,,,,,,,,,,,10/Dec/09 22:25;jbellis;623.patch;https://issues.apache.org/jira/secure/attachment/12427645/623.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,39:48.7,,,no_permission,,,,,,,,,,,,19785,,,Fri Dec 11 03:58:47 UTC 2009,,,,,,0|i0g02n:,91442,,,,,,,,,,,"10/Dec/09 22:39;stuhood;+1
Looks good to me.",11/Dec/09 00:57;dispalt;Works great.  Thank you Jonathan!,11/Dec/09 03:58;jbellis;committed to 0.5 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make iterator-based read code the One True Path,CASSANDRA-287,12429949,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,7/9/2009 15:14,3/12/2019 14:10,3/13/2019 22:24,7/14/2009 21:17,0.4,,,,0,,,,,,"Since CASSANDRA-172 we've had two read paths; the old, ad-hoc path based on the faulty assumption that we could skip checking older sstables if we got a hit earlier in the path (fixed in CASSANDRA-223 but still bearing the marks of its origin) and the new iterator-based path.

This makes all read operations go through the iterator path, which cleans things up enormously and sets the stage for CASSANDRA-139.

I introduce a new QueryFilter interface, which has 3 main methods:

    /**
     * returns an iterator that returns columns from the given memtable
     * matching the Filter criteria in sorted order.
     */
    public abstract ColumnIterator getMemColumnIterator(Memtable memtable);

    /**
     * returns an iterator that returns columns from the given SSTable
     * matching the Filter criteria in sorted order.
     */
    public abstract ColumnIterator getSSTableColumnIterator(SSTableReader sstable) throws IOException;

    /**
     * subcolumns of a supercolumn are unindexed, so to pick out parts of those we operate in-memory.
     * @param superColumn
     */
    public abstract void filterSuperColumn(SuperColumn superColumn);

The first two are for pulling out indexed top-level columns, from a memtable or an sstable, respectively.

If the query is on subcolumns of a supercolumn, which are unindexed, CFS.getColumnFamily does an indexed Name filter on the supercolumn, then asks filterSuperColumn on the primary QueryFilter to pick out the parts the user is requesting.",,,,,,,,,,,,,,,,,,,,10/Jul/09 15:25;jbellis;0007-fixes.patch;https://issues.apache.org/jira/secure/attachment/12413127/0007-fixes.patch,10/Jul/09 15:26;jbellis;0008-fix-empty-CF-handling-should-always-be-null.patch;https://issues.apache.org/jira/secure/attachment/12413128/0008-fix-empty-CF-handling-should-always-be-null.patch,09/Jul/09 15:16;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-287-r-m-unused-and-dangerous-RowReadComman.txt;https://issues.apache.org/jira/secure/attachment/12413023/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-287-r-m-unused-and-dangerous-RowReadComman.txt,09/Jul/09 15:16;jbellis;ASF.LICENSE.NOT.GRANTED--0002-rename-lock_-sstableLock_.txt;https://issues.apache.org/jira/secure/attachment/12413024/ASF.LICENSE.NOT.GRANTED--0002-rename-lock_-sstableLock_.txt,09/Jul/09 15:16;jbellis;ASF.LICENSE.NOT.GRANTED--0003-refactor-out-QueryFilter-SliceQueryFilter.txt;https://issues.apache.org/jira/secure/attachment/12413025/ASF.LICENSE.NOT.GRANTED--0003-refactor-out-QueryFilter-SliceQueryFilter.txt,09/Jul/09 15:16;jbellis;ASF.LICENSE.NOT.GRANTED--0004-replace-namesfilter-with-NamesQueryFilter.-mv-filter.txt;https://issues.apache.org/jira/secure/attachment/12413026/ASF.LICENSE.NOT.GRANTED--0004-replace-namesfilter-with-NamesQueryFilter.-mv-filter.txt,09/Jul/09 15:16;jbellis;ASF.LICENSE.NOT.GRANTED--0005-nuke-timefilter.txt;https://issues.apache.org/jira/secure/attachment/12413027/ASF.LICENSE.NOT.GRANTED--0005-nuke-timefilter.txt,09/Jul/09 15:16;jbellis;ASF.LICENSE.NOT.GRANTED--0006-add-IdentityQueryFilter-finish-removing-IFilter.txt;https://issues.apache.org/jira/secure/attachment/12413028/ASF.LICENSE.NOT.GRANTED--0006-add-IdentityQueryFilter-finish-removing-IFilter.txt,,,,,,8,,,,,,,,,,,,,,,,,,,04:28.4,,,no_permission,,,,,,,,,,,,19622,,,Fri Jul 10 17:49:00 UTC 2009,,,,,,0|i0fy0f:,91108,,,,,,,,,,,"09/Jul/09 15:17;jbellis;(01 and 02 are automated refactorings.)

06
    add IdentityQueryFilter; finish removing IFilter

05
    add TimeQueryFilter and nuke Timefilter

04
    replace namesfilter with NamesQueryFilter.  mv filter code into separate package.

03
    refactor out QueryFilter, SliceQueryFilter

02
    rename lock_ -> sstableLock_

01
    CASSANDRA-287 r/m unused (and dangerous) RowReadCommand
","09/Jul/09 23:04;junrao;Review comments:
1. 0006 patch breaks compilation in unit test (merge conflicts included in the patch)
2. We should remove unused CMD_TYPE_GET_ROW,GET_SLICE AND SLICE_BY_RANGE in ReadCommand and renumber the rest.
3. SystemTable.StorageMetadata, 2nd line, change IdentityQueryFilter  to NamesQueryFilter.
4. What happens to those updateReadStatistics calls in various getRow in Table? Are they just got removed?
5. Need to add @override to all overridden methods in various filters class.
","10/Jul/09 00:19;jbellis;thanks for having a look, Jun.

1. oops. one too many rebases before submitting.  fixed.
2. fixed; if I'm feeling extra energetic I'll make it an enum
3. hmm, I'll need to take a closer look at this one -- making that change breaks SystemTableTest
4. yes, so I'm cheating a bit here to make the merge easier -- this is partially done in https://issues.apache.org/jira/browse/CASSANDRA-272 and will be completed in another patch
5. IMO it's bad form to @override an abstract method; are there others that I omitted?",10/Jul/09 15:25;jbellis;07 fixes points 1 and 2,"10/Jul/09 15:26;jbellis;08 fixes point 3.  the problem was inconsistent handling of empty CFs -- to be consistent with the rest of cassandra, following the original FB conventions, an empty CF should be represented with a null reference.

(IMO this is the wrong convention to pick but changing that is outside my scope for this ticket.)",10/Jul/09 17:12;junrao;The new patch looks good.,10/Jul/09 17:49;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TCP writes get stuck,CASSANDRA-220,12427319,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,junrao,junrao,6/8/2009 4:17,3/12/2019 14:10,3/13/2019 22:24,6/23/2009 17:57,0.4,,,,0,,,,,,"In our test cluster, I observed that on some nodes, TCP writes get accumulated in TcpConnection.pendingWrites. However, the selector never gets a ready to write event. As a result, all writes get stuck. This is because write(message) and doPendingWrites() are not fully synchronized.  This following situation can happen:

1. write(message) adds stuff to pendingWrites.
2. a ready to write event happens; in doPendingWrites() buffered requests are written to socket
3. another write request happens, in write(message), the test for pendingWrites.isEmpty() is false
4. doPendingWrites() finishes writing all buffered request to socket
5. in write(message), the new request is added to pendingWrites

Now, ready to write events will never happen again and all write requests get stuck in pendingWrites.",,,,,,,,,,,,,,,,,,,,08/Jun/09 15:02;jbellis;220-2.patch;https://issues.apache.org/jira/secure/attachment/12410136/220-2.patch,12/Jun/09 23:16;junrao;220-3.patch;https://issues.apache.org/jira/secure/attachment/12410523/220-3.patch,23/Jun/09 16:29;junrao;220-4.patch;https://issues.apache.org/jira/secure/attachment/12411547/220-4.patch,23/Jun/09 17:48;junrao;220-5.patch;https://issues.apache.org/jira/secure/attachment/12411554/220-5.patch,08/Jun/09 04:22;junrao;issue220.patchv1;https://issues.apache.org/jira/secure/attachment/12410103/issue220.patchv1,,,,,,,,,5,,,,,,,,,,,,,,,,,,,02:15.5,,,no_permission,,,,,,,,,,,,19599,,,Tue Jun 23 17:57:15 UTC 2009,,,,,,0|i0fxlr:,91042,,,,,,,,,,,"08/Jun/09 04:22;junrao;Attache a patch to trunk by expanding the synchronization scope in TcpConnection.doPendingWrites().

We need to patch to 0.3 branch too.

Also, we need to revisit the code in TcpConnection. Under heavy writes, is it better to keep accumulating writes until we run out of memory, or to put a cap on the write buffer and simply block and possibly timeout the writer.","08/Jun/09 15:02;jbellis;Committed to 0.3.  Will merge to trunk.

Here is another patch to clean up TcpConnection a little.  Look okay?",08/Jun/09 15:22;junrao;The new patch looks fine to me.,08/Jun/09 15:36;jbellis;committed,"12/Jun/09 23:16;junrao;Haven't beaten this one to death yet. There is another potential synchronization problem with respect to TcpConnection.connect. The following could happen:

1. Thread 1: TcpConnection.write(message) checks the channel and finds it not connected yet.
2. Thread 2: Sockect is connected and triggers TcpConnection.connect; pendingWrites is tested to be empty and ""interested in write"" bit is not set
3. Thread 1: adds message to pendingWrites

Now, pendingWrites is not empty and the ""read for write"" event is never going to happen since we didn't register an interest for it. All subsequent writes get stuck afterward.

Attach a patch in 220-3. It synchronizes in TcpConnection.connect too. Should patch 0.3 branch too.
","13/Jun/09 00:32;jbellis;I know nio is sexier, but maybe it would be better to rip it out and just go with thread-per-socket.

http://java.dzone.com/articles/avoid-nio-get-better-throughpu (read the linked slides)","17/Jun/09 18:33;jbellis;on -3, my only suggestion is, maybe we should just synchronize the whole method for simplicity since it is only going to be called once.

what do you think?","23/Jun/09 16:29;junrao;It took me a while to dig this out. Under heavy load and large column values, we still saw lockups in tcp connection. Here is the problem. The following code that sets the interest ops seems innocent, but it's the source of the problem. The reason is that this operation is not atomic. Another thread could sneak in between the reading of the ops and the setting of it. As a result, some wrong bits could be set.
   key_.interestOps(key_.interestOps() | SelectionKey.OP_READ)

This is a sequence that demonstrates how we can lose the OP_READ bit forever and thus jam the read channel:
1. Thread 1: we want to write a message and in write(Message) we are about to turn on OP_WRITE because the message can't be written in one shot.
2. Thread 2: a read comes in and in read(SelectionKey), we turn off OP_READ and submit the read request to ReadWorkItem in Thread 3.
3. Thread 1: read interestOps and see OP_READ as off.
4. Thread 3: finished processing the read request and turn OP_READ on
5. Thread 1: resumes and turn on OP_WRITE. However, by doing that, we also turned off OP_READ. The read channel is thus blocked forever after this.

Patch-4 makes changing interestOps atomic by synchronizing on the selection key.

Patch-4 also includes the earlier fix for the synchronization problem in connect. I left the fix as it is instead of making connect() a synchronized method. This way, it makes clear which part of the code in connect really requires synchronization.
","23/Jun/09 16:42;jbellis;looks good.

can you patch the other uses of interestOps in UDPConnection and HttpConnection too?",23/Jun/09 17:48;junrao;Attach patch-5 that fixes similar problems in HttpConnection and UdpConnection.,23/Jun/09 17:57;jbellis;committed to 0.3 branch and merged to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift validation bugs,CASSANDRA-266,12429153,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,eweaver,eweaver,6/30/2009 4:36,3/12/2019 14:10,3/13/2019 22:24,6/30/2009 22:08,0.4,,,,0,,,,,,"Server does not raise on invalid insert into regular ColumnFamily

DEBUG - Applying RowMutation(table='Twitter', key='8', modifications=[ColumnFamily(Statuses [body:false:5@1246336092])])
DEBUG - RowMutation(table='Twitter', key='8', modifications=[ColumnFamily(Statuses [body:false:5@1246336092])]) applied. 

OK

DEBUG - Applying RowMutation(table='Twitter', key='8', modifications=[ColumnFamily(Statuses [fhwagads:body:false:5@1246336111])])
DEBUG - RowMutation(table='Twitter', key='8', modifications=[ColumnFamily(Statuses [fhwagads:body:false:5@1246336111])]) applied.  

Not ok... ""Statuses:fhwagads:body"" insert should have failed. For example:

java.lang.IllegalArgumentException: Column Family Statuses:fhwagads:fhwagads:body in invalid format. Must be in <column family>:<column> format.

Also:

You can request an array of values via get_slice_by_names, and you can request an array of columns via get_super_slice_by_names, but you can't request an array of values through a supercolumn via either one. ""Ideally"" get_slice_by_names should allow a supercolumn specification like below:

InvalidRequestException: Column Family StatusRelationships:user_timelines is invalid. ",,,,,,,,,,,,,,,,,,,,30/Jun/09 16:26;jbellis;266.patch;https://issues.apache.org/jira/secure/attachment/12412179/266.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,42:24.1,,,no_permission,,,,,,,,,,,,19615,,,Wed Jul 01 13:41:16 UTC 2009,,,,,,0|i0fxvr:,91087,,,,,,,,,,,30/Jun/09 15:42;jbellis;(unbork my copy/paste job),"30/Jun/09 22:00;eweaver;Looks ok except for two issues:

* validateColumnPathOrParent has no callpoints.

* in validateColumnPathOrParent,
   +        else if (values.length != 3)
  should be
  +        else if (values.length > 3)
",30/Jun/09 22:08;jbellis;committed w/ suggested fixes,"01/Jul/09 13:41;hudson;Integrated in Cassandra #125 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/125/])
    more consistent checking of thrift parameters; fixes multiple bugs.
patch by jbellis; reviewed by Evan Weaver for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wait for bootstrap gossip to propagate,CASSANDRA-575,12441457,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jaakko,jaakko,jaakko,11/23/2009 13:29,3/12/2019 14:10,3/13/2019 22:24,11/23/2009 19:43,0.5,,,,0,,,,,,We should wait for some (RING_DELAY) time after gossiping bootstrap token to give other nodes time to update pending ranges accordingly.,,,,,,,,,,,,,,,,,,,,23/Nov/09 13:30;jaakko;575.patch;https://issues.apache.org/jira/secure/attachment/12425837/575.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,44:00.2,,,no_permission,,,,,,,,,,,,19761,,,Mon Nov 23 19:44:00 UTC 2009,,,,,,0|i0fzrz:,91394,,,,,,,,,,,23/Nov/09 13:30;jaakko;add RING_DELAY sleep to startBootstrap,23/Nov/09 19:44;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
batch insert failing with TokenMetadata AssertionError,CASSANDRA-722,12445995,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jaakko,dispalt,dispalt,1/20/2010 4:38,3/12/2019 14:10,3/13/2019 22:24,1/22/2010 0:01,0.5,,,,0,,,,,,"I get this during the course of an insert.

ERROR [pool-1-thread-17] 2010-01-20 03:50:40,517 Cassandra.java (line 1096) Internal error processing batch_insert
java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.getToken(TokenMetadata.java:212)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:129)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:76)
        at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1183)
        at org.apache.cassandra.service.StorageProxy.insert(StorageProxy.java:101)
        at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:470)
        at org.apache.cassandra.service.CassandraServer.batch_insert(CassandraServer.java:445)
        at org.apache.cassandra.service.Cassandra$Processor$batch_insert.process(Cassandra.java:1088)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)","r900058  4 node cluster, 1 more bootstrapping",,,,,,,,,,,,,,,,,,,21/Jan/10 01:57;jaakko;722.patch;https://issues.apache.org/jira/secure/attachment/12430977/722.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,42:16.6,,,no_permission,,,,,,,,,,,,19837,,,Fri Jan 22 00:01:08 UTC 2010,,,,,,0|i0g0o7:,91539,,,,,,,,,,,20/Jan/10 04:42;jbellis;have you done any node movement in this cluster?,"20/Jan/10 04:57;dispalt;I have used loadbalance quite a few times.  I have never done a decommision or a move command explicitly.

A machine does seem to be stalled in the middle of bootstrapping.  Trying to get intellij setup, to dig deeper on to what that machine is doing.","20/Jan/10 05:16;jaakko;Seems there's a bug in getHintedMapForEndpoints. 

That bootsrapping node is probably considered dead by other nodes

This is what's happening:
(1) Node bootstraps -> it has pending ranges, but it is not member of token ring
(2) During write, pending ranges match -> node is added to write endpoints
(3) Node is down -> a hinted target will be searched for
(4) getToken explodes because endpoint is not member

There are two things that need to be considered:
(1) We probably should ignore endpoints that are not members when looking for hinted targets
(2) That assert in getToken is a bit outdated I think. Nodes may come and go during the time ARS and friends are looking for write targets. It might very well happen that a node was removed between getting natural endpoints and coming back to get hinted targets. There's a comment saying ""don't want to return nulls"", but perhaps we'll need to reconsider this?
","20/Jan/10 05:41;jbellis;(1) definitely
(2) Under what conditions does it make sense to ask for Token of a node that is not a ring member?","20/Jan/10 06:07;jaakko;(2) it does not make sense, but it might happen. Suppose a node is natural endpoint of a write. StorageProxy gets these in mutate, and then passes the same list to getHintedEndpointMap. If one of these natural endpoints is removed in between getNaturalEndpoints and finding hinted targets, there will be a call to getToken for non-existing endpoint. It does not help even if we insert isMember check right before getToken, as there is still small window that could cause this.

Basically the same can happen wherever we use getToken","20/Jan/10 06:14;jbellis;We could fix that by picking a random node to start our scan of potential hint destinations; there's nothing magical about starting w/ endpoint+1.  In fact picking random is better, since currently having a node down degrades performance more than it should since all the hints go to the same node.","20/Jan/10 14:20;jaakko;Is it really OK to send hints to random targets? That will scatter hinted data around the cluster as each hinted write may go to different location. Might not be problematic, but feels like we're going around the problem instead of fixing the cause. Something would certainly be gained by having the load spread amongst nodes, but isn't something also lost if (potentially) all nodes in the cluster stream hinted data instead of one node?
","20/Jan/10 14:31;jbellis;no, we don't rely on hint location.

but actually using sortByProximity is probably best of all.","20/Jan/10 15:12;jaakko;yeah, proximity seems good.

Attached patch ignores dead bootstrappers and looks for hinted locations based on proximity (list of addresses is from gossiper.liveMembers).

This couples ARS with Gossiper, but don't know if that matters so much as it used FD already before.

Edit: this patch has not been tested much, have to do some more testing tomorrow.",20/Jan/10 15:28;jbellis;+1,"20/Jan/10 21:01;dispalt;Is there a way to remove the failed bootstrapped node from the cluster, decommission obviously doesn't work...

Or should this patch fix things?",20/Jan/10 21:30;jbellis;this will fix your batch_insert problem,"20/Jan/10 21:49;dispalt;Should I add another bug for the failed bootstrapping problem?  

The node seems to be getting data; like it thinks its part of the cluster and datagrowth is that of a normal working node.  However, it doesn't have all the data (should be at least 50g and it 3 gb) and if I try to nodeprobe ring it doesn't show up.","21/Jan/10 01:36;jaakko;new version. skip non-members also when considering hinted location.
","21/Jan/10 01:39;jaakko;> Is there a way to remove the failed bootstrapped node from the cluster, decommission obviously doesn't work...

Actually there is an ""undocumented"" feature in removetoken command that it will clear the token from bootstrapping as well. If you know the token, remove it and that should take care of it.

Probably need to provide better tools to investigate/poke bootstrapping and leaving tokens.
",21/Jan/10 01:47;dispalt;I think jonathan mentioned that I would need the patch rom 644 is that correct?,"21/Jan/10 01:57;jaakko;accidentally submitted a japanese version, this should be better.
","21/Jan/10 01:59;jaakko;yeah, 644 is needed to remove it from gossip, but it will be removed from token metadata even without 644, which is what matters in this case.
",22/Jan/10 00:01;jaakko;committed to 0.5.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFMetaData id gets out of sync,CASSANDRA-1403,12471843,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,8/17/2010 20:32,3/12/2019 14:10,3/13/2019 22:24,8/17/2010 21:44,0.7 beta 2,,,,0,,,,,,"stand up two nodes.
load a KS + cf on A.
add another CF on A.
Let the cluster quiesce.
add a CF on B.

You get the out of sync error.  I'm pretty sure this is because AddColumnFamily doesn't CFM.fixMaxId() like AddKeyspace does.",,,,,,,,,,,,,,,,,,,,17/Aug/10 20:58;gdusbabek;ASF.LICENSE.NOT.GRANTED--v0-0001-fix-max-id-after-adding-a-column-family.txt;https://issues.apache.org/jira/secure/attachment/12452322/ASF.LICENSE.NOT.GRANTED--v0-0001-fix-max-id-after-adding-a-column-family.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,20:34.7,,,no_permission,,,,,,,,,,,,20120,,,Wed Aug 18 13:13:50 UTC 2010,,,,,,0|i0g4uf:,92215,,,,,,,,,,,"17/Aug/10 21:20;stuhood;+1
I expect that cfId generation is going to need to change significantly to allow for concurrent schema changes.",17/Aug/10 21:40;gdusbabek;It will need to act like a distributed counter.,"18/Aug/10 13:13;hudson;Integrated in Cassandra #517 (See [https://hudson.apache.org/hudson/job/Cassandra/517/])
    fix max id after adding a column family. patch by gdusbabek, reviewed by stuhood. CASSANDRA-1403
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when calling get_range_slice(),CASSANDRA-643,12443739,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,vomjom,vomjom,12/18/2009 9:53,3/12/2019 14:10,3/13/2019 22:24,12/21/2009 17:29,0.5,,,,0,,,,,,"This bug affects 0.5.0beta2.

In the SlicePredicate struct, column_names is defined as ""optional"", and so can be null.

get_range_slice() passes the ""predicate.getColumn_names()"" to ""ThriftValidation.validateColumns()"" without first checking if it's null, resulting in the following exception:

java.lang.NullPointerException
	at org.apache.cassandra.service.ThriftValidation.validateColumns(ThriftValidation.java:162)
	at org.apache.cassandra.service.ThriftValidation.validateColumns(ThriftValidation.java:181)
	at org.apache.cassandra.service.CassandraServer.get_range_slice(CassandraServer.java:562)
	at org.apache.cassandra.service.Cassandra$Processor$get_range_slice.process(Cassandra.java:1024)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

Strangely enough, calling it from python doesn't generate this exception, but calling it from C++ does, with the following code:
shared_ptr<TTransport> transport(new TSocket(host, port));
shared_ptr<TProtocol> proto(new TBinaryProtocol(transport));
CassandraClient client(proto);
transport->open();

ColumnParent column_parent;
column_parent.column_family = ""Test"";
SlicePredicate slice_predicate;

std::vector<KeySlice> keys;
client.get_range_slice(keys, keyspace, column_parent, slice_predicate, """", """", 100, ONE);


The fix is trivial, just check that predicate.getColumn_names() is not null in line 562 of CassandraServer.java",Linux,,,,,,,,,,,,,,,,,,,18/Dec/09 21:00;jbellis;643.patch;https://issues.apache.org/jira/secure/attachment/12428481/643.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,00:09.8,,,no_permission,,,,,,,,,,,,19796,,,Mon Dec 21 17:29:39 UTC 2009,,,,,,0|i0g073:,91462,,,,,,,,,,,"18/Dec/09 21:00;jbellis;range and column_names are both optional, but exactly one must be present.  (thrift IDL does not provide a way to specify that more rigorously.)

attached patch will return an InvalidRequestException if a SlicePredicate does not conform to this.","21/Dec/09 16:56;vomjom;+1

Thanks.",21/Dec/09 17:29;jbellis;committed to 0.5 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HHOM goes into infinite loop, wasting cpu",CASSANDRA-715,12445884,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,1/19/2010 5:03,3/12/2019 14:10,3/13/2019 22:24,1/19/2010 18:38,0.6,,,,0,,,,,,"To replicate: take a host down, cause hints to it, wait for HHOM to kick in

The issue is line 201 of HHOM:
startColumn = keyColumn.name(); // repeating the last as the first is fine since we just deleted it

That comment is false.  The column may not have been deleted, since the endpoint could still be down.  This causes HHOM to go into an infinite loop trying to deliver hints to a down host.","debian lenny amd64 OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",,,,,,,,,,,,,,,,,,,19/Jan/10 17:05;jbellis;715.txt;https://issues.apache.org/jira/secure/attachment/12430771/715.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,41:24.8,,,no_permission,,,,,,,,,,,,19831,,,Fri Jan 22 12:36:20 UTC 2010,,,,,,0|i0g0mn:,91532,,,,,,,,,,,"19/Jan/10 15:41;jbellis;until this is fixed, running nodeprobe cleanup on each live node & restarting it should fix this (by removing undelivered hints forcibly).  depending on your replication factor you may have hints that are not removed by cleanup; in that case you can remove the hint files from data/system/*Hint*.","19/Jan/10 18:10;brandon.williams;+1, infinite loop no longer occurs.",19/Jan/10 18:38;jbellis;committed,"22/Jan/10 12:36;hudson;Integrated in Cassandra #331 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/331/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Count parameter has no effect on subcolumns.,CASSANDRA-356,12432633,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,eweaver,eweaver,8/9/2009 18:58,3/12/2019 14:09,3/13/2019 22:24,8/10/2009 15:29,0.4,,,,0,,,,,,"I send count=1: 

[""get_slice"", ""Twitter"", ""test_get_super_sub_keys_with_count"", <CassandraThrift::ColumnParent column_family:""StatusRelationships"", super_column:""user_timelines"">, <CassandraThrift::SlicePredicate slice_range:<CassandraThrift::SliceRange start:"""", finish:"""", is_ascending:true, count:1>>, 1]

The server says:

weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_count', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@3def3b56', columnName='null')', start='', finish='', isAscending=true, count=1)

But I get back two items:

{[<Cassandra::UUID#13699890>,
  ""v1""]=>nil,
 [<Cassandra::UUID#13699880>,
  ""v3""]=>nil}

Incidentally, the debug output value for ""superColumnName"" is not correct.",,,,,,,,,,,,,,,,,,,,10/Aug/09 14:04;jbellis;0002-v2.patch;https://issues.apache.org/jira/secure/attachment/12416068/0002-v2.patch,10/Aug/09 14:37;jbellis;0002-v3.patch;https://issues.apache.org/jira/secure/attachment/12416073/0002-v3.patch,10/Aug/09 03:36;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-356-rename-clean-up-collectColumns-methods.txt;https://issues.apache.org/jira/secure/attachment/12416014/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-356-rename-clean-up-collectColumns-methods.txt,10/Aug/09 03:36;jbellis;ASF.LICENSE.NOT.GRANTED--0002-use-the-existing-collectReducedColumns-api-to-make-sub.txt;https://issues.apache.org/jira/secure/attachment/12416015/ASF.LICENSE.NOT.GRANTED--0002-use-the-existing-collectReducedColumns-api-to-make-sub.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,37:22.8,,,no_permission,,,,,,,,,,,,19649,,,Tue Aug 11 12:57:31 UTC 2009,,,,,,0|i0fyfr:,91177,,,,,,,,,,,"10/Aug/09 03:37;jbellis;02
    use the existing collectReducedColumns api to make subcolumn slices conform as expected to filter semantics

01
    CASSANDRA-356 rename, clean up collectColumns methods
","10/Aug/09 03:37;eribeiro;QueryPath's toString() tries to print two byte arrays: superColumnName and columnName. Well, if you wrap them with new String(<field-name>) then the JVM will use the machine's default CharSet to convert the byte array to String (already printing the memory address).

","10/Aug/09 03:44;jbellis;yeah, QP doesn't have access to comparator or subcolumncomparator.  nor does it know the table so it can't cheat and look them up globally...","10/Aug/09 03:46;eweaver;Getting farther. It returns the right number of rows now, but isAscending has no effect.",10/Aug/09 05:07;eweaver;Now also blocked on CASSANDRA-357.,10/Aug/09 14:04;jbellis;patch that also fixes !asc case,10/Aug/09 14:23;jbellis;meh.  that's still not quite right.  hang on.,"10/Aug/09 14:37;jbellis;ok, this one works w/ the 357 test cases too",10/Aug/09 15:24;eweaver;ship it!,10/Aug/09 15:29;jbellis;committed,"11/Aug/09 12:57;hudson;Integrated in Cassandra #164 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/164/])
    use the existing collectReducedColumns api to make subcolumn slices conform as expected to filter semantics
patch by jbellis; reviewed by Evan Weaver for 
rename, clean up collectColumns methods
patch by jbellis; reviewed by Evan Weaver for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
supercolumn indexing is broken,CASSANDRA-341,12432213,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,8/4/2009 22:37,3/12/2019 14:09,3/13/2019 22:24,8/5/2009 2:39,0.4,,,,0,,,,,,SC.serializedSize() is broken and has been for some time.  all indexes generated with that are invalid and using them can cause bogus data and/or EOFExceptions being returned.,,,,,,,,,,,,,,,,,,,,04/Aug/09 22:42;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-341-fix-NPE-on-iterator-close.txt;https://issues.apache.org/jira/secure/attachment/12415538/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-341-fix-NPE-on-iterator-close.txt,04/Aug/09 22:42;jbellis;ASF.LICENSE.NOT.GRANTED--0002-fix-SC.serializedSize.txt;https://issues.apache.org/jira/secure/attachment/12415539/ASF.LICENSE.NOT.GRANTED--0002-fix-SC.serializedSize.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,32:15.7,,,no_permission,,,,,,,,,,,,19643,,,Wed Aug 05 12:35:06 UTC 2009,,,,,,0|i0fycf:,91162,,,,,,,,,,,05/Aug/09 02:32;lenn0x;+1. I tested this on a small dataset and it works properly now.,05/Aug/09 02:39;jbellis;committed,"05/Aug/09 12:35;hudson;Integrated in Cassandra #158 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/158/])
    fix SC.serializedSize.  patch by jbellis; reviewed by Chris Goffinet for 
fix NPE on iterator close.  patch by jbellis; reviewed by Chris Goffinet for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_range_slices doesn't return key from start_key in KeyRange any more,CASSANDRA-1733,12479768,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,arya,arya,11/12/2010 2:51,3/12/2019 14:09,3/13/2019 22:24,11/13/2010 3:19,0.7.0 rc 1,,,,0,,,,,,"The following is a test case which used to work for me before, but after upgrading to trunk it fails:

-Insert 50 or so keys to StandardByUUID1 CF in Keyspace1
-Try using get_range_slices with a KeyRange that has one of those keys you inserted earlier as its start_key with a count of 20

You will get 20 rows, but you won't get the row with specified start_key

Expected Result: you should get 20 rows back with the specified start_key as one of the rows

I am suing Random Partitioner with RF2 and CL1. I understand that Random Partitioner will not give you keys in order, but the behavior before at least returned a random subset including the row with specified start_key.

Please investigate.

-Arya

","Cent OS 5.4
Cassandra Trunk from November 11th at 2pm",,,,,,,,,,,,,,,,,,,13/Nov/10 03:13;tjake;1733_v1.txt;https://issues.apache.org/jira/secure/attachment/12459515/1733_v1.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,30:16.1,,,no_permission,,,,,,,,,,,,20281,,,Sat Nov 13 16:26:08 UTC 2010,,,,,,0|i0g71z:,92573,,,,,,,,,,,"12/Nov/10 17:30;stuhood;The ranges to query are properly generated in StorageProxy:
{quote}
[228...,0] are [[228...,333...], (333...,0]]
{quote}

But ColumnFamilyStore.getRangeSlice creates DecoratedKeys from the tokens of the Range with null keys: null keys now sort after valid keys, so the first key isn't properly included.","12/Nov/10 17:33;stuhood;Also, we need a testcase this time.","12/Nov/10 19:08;jbellis;I would think it would make more sense for null keys to sort before valid keys, so that a token X would match all rows with that token rather than none.",12/Nov/10 22:11;tjake;Why do we have a test ColumnFamilyStoreTest:testSkipStartKey() which validates the first key isn't included? ,"12/Nov/10 22:37;arya;I traced it to revision 906272 which is a result of CASSANDRA-759. Although, it says it will skip the start_key and that was committed in Feb, but I have been using Cassandra Trunk since April and it always worked the opposite. !!","13/Nov/10 01:51;jbellis;bq. Why do we have a test ColumnFamilyStoreTest:testSkipStartKey() which validates the first key isn't included? 

A Range object (which Hadoop splits generate) is start-exclusive.  A Bounds object (which normal user scan queries generate) is start-inclusive.","13/Nov/10 03:11;tjake;Patch includes fix and test case.

ColumnFamilyStore creates a startWith and stopAt DK with null key. 
This will need to be addressed in CASSANDRA-1034

1) For range queries, changed the check to skip start key to be only on token.  

2) Since null key sorts last,  I changed startWith to use a empty byte buffer as key which sorts before all other keys.

","13/Nov/10 03:19;jbellis;This has passed my level of comfort for 0.7.0.  Fixed by reverting CASSANDRA-1720.  Let's fix it ""right"" for CASSANDRA-1034 in trunk and possibly backport to 0.7.1 later.","13/Nov/10 16:26;tjake;Yeah, that's good.  You may want to keep add the unit test so we have one here on out",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scanning multiple CFs at once for range queries is a misfeature,CASSANDRA-280,12429721,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,7/7/2009 15:38,3/12/2019 14:09,3/13/2019 22:24,7/7/2009 20:02,0.4,,,,0,,,,,,the CF is the atom of storage.  It is unlikely that mixing keys from multiple CFs will be useful.,,,,,,,,,,,,,,,,,,,,07/Jul/09 19:31;jbellis;280-2.patch;https://issues.apache.org/jira/secure/attachment/12412777/280-2.patch,07/Jul/09 16:09;jbellis;280.patch;https://issues.apache.org/jira/secure/attachment/12412752/280.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,47:46.9,,,no_permission,,,,,,,,,,,,19617,,,Wed Jul 08 12:47:08 UTC 2009,,,,,,0|i0fxyv:,91101,,,,,,,,,,,07/Jul/09 16:09;jbellis;fairly straightforward patch.  we just have to change the arg propagation and un-loop a couple sections of code in Table.,07/Jul/09 19:31;jbellis;fix system tests (oops),07/Jul/09 19:47;urandom;Looks good. +1,07/Jul/09 20:02;jbellis;committed,"08/Jul/09 12:47;hudson;Integrated in Cassandra #131 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/131/])
    make get_key_range act on a single CF.
patch by jbellis; reviewed by Eric Evans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompareSubcolumnsWith= has no effect,CASSANDRA-357,12432635,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,eweaver,eweaver,8/9/2009 19:59,3/12/2019 14:09,3/13/2019 22:24,8/11/2009 2:11,0.4,,,,0,,,,,,"CompareSubcolumnsWith= has no effect.

          <ColumnFamily CompareWith=""UTF8Type"" CompareSubcolumnsWith=""TimeUUIDType"" ColumnType=""Super"" Name=""StatusRelationships"" />  

I insert:

<[<Cassandra::UUID#13700550 time: Sun Jan 24 00:40:32 -0800 1971>,
 <Cassandra::UUID#13700530 time: Wed Feb 16 09:21:04 -0800 1972>,
 <Cassandra::UUID#13700540 time: Wed Apr 03 03:42:08 -0700 1974>,
 <Cassandra::UUID#13700520 time: Tue Jul 04 14:24:16 -0700 1978>,
 <Cassandra::UUID#13700560 time: Mon Jan 05 10:48:32 -0800 1987>]> 

But:

    keys = @twitter.get(:StatusRelationships, key, ""user_timelines"").keys

Responds with:

<[<Cassandra::UUID#13700560 time: Mon Jan 05 10:48:32 -0800 1987>,
 <Cassandra::UUID#13700550 time: Sun Jan 24 00:40:32 -0800 1971>,
 <Cassandra::UUID#13700540 time: Wed Apr 03 03:42:08 -0700 1974>,
 <Cassandra::UUID#13700530 time: Wed Feb 16 09:21:04 -0800 1972>,
 <Cassandra::UUID#13700520 time: Tue Jul 04 14:24:16 -0700 1978>]>.

PS. The debug log says:

weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_ranges', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@370410a7', columnName='null')', start='', finish='', isAscending=true, count=100)

",,,,,,,,,,,CASSANDRA-356,,,,,,,,,10/Aug/09 16:24;jbellis;357-3.patch;https://issues.apache.org/jira/secure/attachment/12416086/357-3.patch,10/Aug/09 14:40;jbellis;357-v2.patch;https://issues.apache.org/jira/secure/attachment/12416074/357-v2.patch,10/Aug/09 04:21;jbellis;357.patch;https://issues.apache.org/jira/secure/attachment/12416016/357.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,15:25.5,,,no_permission,,,,,,,,,,,,19650,,,Tue Aug 11 12:57:30 UTC 2009,,,,,,0|i0fyfz:,91178,,,,,,,,,,,"09/Aug/09 20:15;jbellis;it looks like your client is returning a result as a hash.  isn't your client's hashing going to screw with the ordering, even if cassandra's is correct?

I'd check the raw thrift results -- I don't see anything obviously wrong on the cassandra code and this is a path that the system tests check.","09/Aug/09 20:44;eweaver; I have ordered hashes. Thrift returns:

[<CassandraThrift::ColumnOrSuperColumn column:<CassandraThrift::Column name:""\023\201@\0000\242\021\305\233\274a\252\a\300\325\355"", value:""v5"", timestamp:1249850151372872>>,
 <CassandraThrift::ColumnOrSuperColumn column:<CassandraThrift::Column name:""\023\201@\000N\377\021\263\204\020\370\224\245\341\202\034"", value:""v1"", timestamp:1249850151372872>>,
 <CassandraThrift::ColumnOrSuperColumn column:<CassandraThrift::Column name:""\023\201@\000\342\206\021\266\212\320\027J\207\000!\351"", value:""v3"", timestamp:1249850151372872>>,
 <CassandraThrift::ColumnOrSuperColumn column:<CassandraThrift::Column name:""\023\201@\000\200,\021\264\226\217\311~\024\333qD"", value:""v2"", timestamp:1249850151372872>>,
 <CassandraThrift::ColumnOrSuperColumn column:<CassandraThrift::Column name:""\023\201@\000\247:\021\273\2244\265\307*\375\200\237"", value:""v4"", timestamp:1249850151372872>>]

If I add in TimeUUIDType:

    public int compare(byte[] o1, byte[] o2)
    {
+        if (true) { throw new MarshalException(""Crap""); }
        long t1 = LexicalUUIDType.getUUID(o1).timestamp();
        long t2 = LexicalUUIDType.getUUID(o2).timestamp();
        return t1 < t2 ? -1 : (t1 > t2 ? 1 : 0);
    }

it's never thrown in any place that I can find, at least.
",10/Aug/09 04:21;jbellis;here you go.  test is longer than the actual fix :),"10/Aug/09 04:55;eweaver;Fixes one bug, but causes another.

DEBUG - weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_count', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@2bb57fd1', columnName='null')', start='', finish='', isAscending=false, count=1)
ERROR - Internal error processing get_slice
java.nio.BufferUnderflowException
  at java.nio.Buffer.nextGetIndex(Buffer.java:480)
  at java.nio.HeapByteBuffer.getLong(HeapByteBuffer.java:387)
  at org.apache.cassandra.db.marshal.LexicalUUIDType.getUUID(LexicalUUIDType.java:11)
  at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:10)
  at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:5)
  at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:80)
  at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1391)
  at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1365)
  at org.apache.cassandra.db.Table.getRow(Table.java:589)
  at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
  at org.apache.cassandra.service.StorageProxy.weakReadLocal(StorageProxy.java:609)
  at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:320)
  at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:94)
  at org.apache.cassandra.service.CassandraServer.getSlice(CassandraServer.java:175)
  at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:220)
  at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:587)
  at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:575)
  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:637)

[9:47pm] jbellis: evn: the fix is to duplicate the length==0 special cases from LexicalUUIDType.compare in TimeUUIDType
[9:47pm] jbellis: but i need to figure out why my test case isn't tripping that bug
[9:47pm] jbellis: but my brain is done for the night.  bedtime.

","10/Aug/09 05:17;eweaver;While you're at it, can you see if LongType has the same problems?","10/Aug/09 14:40;jbellis;v2 fixes comparing with byte[0].

requires latest 356 patch for tests to pass.",10/Aug/09 14:40;jbellis;LongType looks fine with 356 + 357 patches.,"10/Aug/09 15:23;eweaver;
I now get an order/limit/range error. Range nil..@uuids[2] and @uuids[2]..nil both return the first item in the subcolumn.

  1) Failure:
test_get_super_sub_keys_with_ranges(CassandraTest) [./test/cassandra_test.rb:109]:
<{<Cassandra::UUID#13817770 time: Wed Feb 16 09:21:04 -0800 1972, usecs: 0 jitter: 7897980574650157968>=>
  ""v2""}> expected but was
<{[<Cassandra::UUID#13748260 time: Sun Jan 24 00:40:32 -0800 1971, usecs: 0 jitter: 10815637326122044813>,
  ""v1""]=>nil}>.

DEBUG - weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_ranges', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@38990402', columnName='null')', start='?@?,????aB?', finish='', isAscending=true, count=1)

","10/Aug/09 16:24;jbellis;respect ""start"" filter argument for subcolumns.  applies on top of 357-v2","10/Aug/09 19:47;eweaver;Still broken.  Here is my test case (elements of the @uuids array are in increasing order by time):

  def test_get_super_sub_keys_with_ranges              
    @twitter.insert(:StatusRelationships, key, 
      {'user_timelines' => {
        @uuids[1] => 'v1', 
        @uuids[2] => 'v2', 
        @uuids[3] => 'v3',
        @uuids[4] => 'v4', 
        @uuids[5] => 'v5'}})

    keys = @twitter.get(:StatusRelationships, key, ""user_timelines"").keys
    assert_equal keys.sort, keys    
    assert_equal({@uuids[1] => 'v1'}, @twitter.get(:StatusRelationships, key, ""user_timelines"", :finish => @uuids[2], :count => 1))
 
    # FAILS ON NEXT LINE
    assert_equal({@uuids[2] => 'v2'}, @twitter.get(:StatusRelationships, key, ""user_timelines"", :start => @uuids[2], :count => 1))

    assert_equal 4, @twitter.get(:StatusRelationships, key, ""user_timelines"", :start => @uuids[2], :finish => @uuids[5]).size

Here is the result:

  1) Failure:
test_get_super_sub_keys_with_ranges(CassandraTest) [./test/cassandra_test.rb:110]:
<{<Cassandra::UUID#13756560 time: Wed Feb 16 09:21:04 -0800 1972, usecs: 0 jitter: 3921283106724136851>=>
  ""v2""}> expected but was
<{[<Cassandra::UUID#13696690 time: Sun Jan 24 00:40:32 -0800 1971, usecs: 0 jitter: 2275131151012163974>,
  ""v1""]=>nil}>.

The server said:

DEBUG - get_slice_from
DEBUG - weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_ranges', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@2eb89c06', columnName='null')', start='?@?,??X?Y?v', finish='', isAscending=true, count=1)

","10/Aug/09 20:17;jbellis;I added this to the patched test_time_uuid in test_server.py:

        p = SlicePredicate(slice_range=SliceRange(L[2].bytes, '', True, 1))
        column_parent = ColumnParent('Super4', 'sc1')
        slice = [result.column
                 for result in client.get_slice('Keyspace2', 'key1', column_parent, p, ConsistencyLevel.ONE)]
        assert slice == [Column(L[2].bytes, 'value2', 2)], slice

this passes.  I'm not sure how this differs from yours.
","11/Aug/09 00:10;eweaver;It works! My checkout was broken.

Ship it!",11/Aug/09 02:11;jbellis;fixed,"11/Aug/09 12:57;hudson;Integrated in Cassandra #164 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/164/])
    respect ""start"" filter argument for subcolumns.
patch by jbellis; reviewed by Even Weaver for 
fix typo breaking CompareSubcolumnsWith.  fix timeuuid compare with byte[0].
patch by jbellis; reviewed by Evan Weaver for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bootstrapping does not work properly using multiple DataFileDirectory,CASSANDRA-716,12445889,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,david.pan,david.pan,1/19/2010 6:55,3/12/2019 14:09,3/13/2019 22:24,1/21/2010 23:13,0.5,,,,0,,,,,,"I was adding a new machine A which has 2 DataFileDirectories into the ring. The A will throw exception while bootstrapping.
DEBUG [MESSAGING-SERVICE-POOL:4] 2010-01-19 11:43:32,837 ContentStreamState.java (line 88) Removing stream context /home/store0/data/pic/raw_data-tmp-1-Data.db:209833142
 WARN [MESSAGING-SERVICE-POOL:4] 2010-01-19 11:43:32,837 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local
=/10.81.37.65:7000 remote=/10.81.42.26:10418]
 WARN [MESSAGING-SERVICE-POOL:4] 2010-01-19 11:43:32,837 TcpConnection.java (line 485) Exception was generated at : 01/19/2010 11:43:32 on thread MESSAGING-SERVICE-POOL:4
java.io.IOException: rename failed of /home/store0/data/pic/raw_data-1-Filter.db
java.io.IOError: java.io.IOException: rename failed of /home/store0/data/pic/raw_data-1-Filter.db
        at org.apache.cassandra.io.SSTableWriter.rename(SSTableWriter.java:154)
        at org.apache.cassandra.io.SSTableWriter.renameAndOpen(SSTableWriter.java:162)
        at org.apache.cassandra.io.Streaming$StreamCompletionHandler.onStreamCompletion(Streaming.java:284)
        at org.apache.cassandra.net.io.ContentStreamState.handleStreamCompletion(ContentStreamState.java:108)
        at org.apache.cassandra.net.io.ContentStreamState.read(ContentStreamState.java:90)
        at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:96)
        at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:445)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: rename failed of /home/store0/data/pic/raw_data-1-Filter.db
        at org.apache.cassandra.utils.FBUtilities.renameWithConfirm(FBUtilities.java:306)
        at org.apache.cassandra.io.SSTableWriter.rename(SSTableWriter.java:150)
        ... 9 more


I traced the exception and maybe found the reason.
StreamInitiateVerbHandler::doVerb() will create 3 temporary files(index, filter, data) for each ssTable. The name for each file is generated by getNewFileNameFromOldContextAndNames(). This method will generate a file name and a path for each ssTable, but the path is generated with DatabaseDescriptor.getDataFileLocationForTable() which will return different path for ech call when we configure multi-DataFileDirectory. 
eg: the ssTable raw_data-1 may have 3 temporary files : 
/home/store0/data/pic/raw_data-tmp-1-Index.db
/home/store1/data/pic/raw_data-tmp-1-Filter.db
/home/store0/data/pic/raw_data-tmp-1-Data.db

After receiving all data, StreamCompletionHandler::onStreamCompletion() will rename all temporary files and this method think all ssTable files will have the same path as data.db file. 
            if (streamContext.getTargetFile().contains(""-Data.db""))
            {
               ......
                try
                {
                    SSTableReader sstable = SSTableWriter.renameAndOpen(streamContext.getTargetFile());
                    ......
                }
                ......
            }
Then the renameAndOpen() will throw that exception.



","storage-conf.xml:
  <CommitLogDirectory>/home/store1/commitlog</CommitLogDirectory>
  <DataFileDirectories>
      <DataFileDirectory>/home/store0/data</DataFileDirectory>
      <DataFileDirectory>/home/store1/data</DataFileDirectory>
  </DataFileDirectories>
  <CalloutLocation>/home/store1/cassandra/callouts</CalloutLocation>
  <StagingFileDirectory>/home/store1/cassandra/staging</StagingFileDirectory>",,86400,86400,,0%,86400,86400,,,,,,,,,,,,21/Jan/10 22:08;gdusbabek;0001-rename-DD.getDAtaFileLocationForTable-to-something-m.patch;https://issues.apache.org/jira/secure/attachment/12431070/0001-rename-DD.getDAtaFileLocationForTable-to-something-m.patch,21/Jan/10 22:08;gdusbabek;0002-ensure-all-files-for-an-sstable-are-streamed-to-the-.patch;https://issues.apache.org/jira/secure/attachment/12431071/0002-ensure-all-files-for-an-sstable-are-streamed-to-the-.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,36:24.4,,,no_permission,,,,,,,,,,,,19832,,,Thu Jan 21 23:13:04 UTC 2010,,,,,,0|i0g0mv:,91533,,,,,,,,,,,"19/Jan/10 07:04;david.pan;sorry, my version is 0.5-rc1.",21/Jan/10 13:36;jbellis;(CASSANDRA-730 has another example.),21/Jan/10 22:08;gdusbabek;ensures that all files for a single sstable end up in the same directory.  The same round-robin approach is used in the case of multiple sstables/keyspaces.,21/Jan/10 22:14;jbellis;+1,"21/Jan/10 23:13;gdusbabek;r901902 (0.5), r901915 (trunk)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missing lib/license/google-collect-1.0-rc1.jar.LICENSE,CASSANDRA-294,12430412,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,7/14/2009 19:44,3/12/2019 14:09,3/13/2019 22:24,7/14/2009 20:00,0.4,,,,0,,,,,,The license file for lib/google-collect-1.0-rc1.jar is missing. Patch to follow.,,,,,,,,,,,,,,,,,,,,14/Jul/09 19:45;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-294-add-license-information-for-google-colle.txt;https://issues.apache.org/jira/secure/attachment/12413471/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-294-add-license-information-for-google-colle.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,52:38.6,,,no_permission,,,,,,,,,,,,19624,,,Wed Jul 15 12:40:20 UTC 2009,,,,,,0|i0fy1z:,91115,,,,,,,,,,,14/Jul/09 19:46;urandom;seconds?,14/Jul/09 19:52;jbellis;+1,14/Jul/09 19:59;euphoria;+1,14/Jul/09 20:00;urandom;committed,"15/Jul/09 12:40;hudson;Integrated in Cassandra #138 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/138/])
    add license information for google-collect

Patch by eevans; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BootstrapTest occasionally fails,CASSANDRA-395,12434219,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,jbellis,jbellis,8/27/2009 20:45,3/12/2019 14:09,3/13/2019 22:24,8/28/2009 3:06,0.4,,,,0,,,,,,"sample failure:

    [junit] Testcase: testAntiCompaction1(org.apache.cassandra.db.BootstrapTest):       Caused an ERROR
    [junit] /home/jonathan/projects/cassandra/git-trunk/build/test/cassandra/data/Keyspace1/Standard1-2-Data.db (No such file or directory)                                                                                               
    [junit] java.io.FileNotFoundException: /home/jonathan/projects/cassandra/git-trunk/build/test/cassandra/data/Keyspace1/Standard1-2-Data.db (No such file or directory)                                                                
    [junit]     at java.io.RandomAccessFile.open(Native Method)                                                      
    [junit]     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)                                        
    [junit]     at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)                                         
    [junit]     at org.apache.cassandra.io.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)        
    [junit]     at org.apache.cassandra.io.FileStruct.<init>(FileStruct.java:49)                                     
    [junit]     at org.apache.cassandra.io.SSTableReader.getFileStruct(SSTableReader.java:321)                       
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.initializePriorityQueue(ColumnFamilyStore.java:655)     
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.doFileAntiCompaction(ColumnFamilyStore.java:911)        
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.doAntiCompaction(ColumnFamilyStore.java:814)            
    [junit]     at org.apache.cassandra.db.BootstrapTest.testAntiCompaction(BootstrapTest.java:63)                   
    [junit]     at org.apache.cassandra.db.BootstrapTest.testAntiCompaction1(BootstrapTest.java:72)                  

I have seen someone else mention this on IRC too.  Most of the time, the test passes.",,,,,,,,,,,,,,,,,,,,28/Aug/09 01:25;sandeep_tata;395.patch;https://issues.apache.org/jira/secure/attachment/12417946/395.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,43:25.1,,,no_permission,,,,,,,,,,,,19669,,,Fri Aug 28 15:07:40 UTC 2009,,,,,,0|i0fyo7:,91215,,,,,,,,,,,"28/Aug/09 00:43;sandeep_tata;Ah this is a fun bug -- looks like this happens if the following sequence occurs:

1. Call to flush memtable
2. Compaction submitted (from storeLocation after flushing memtable) but not yet completed.
3. CFS.doAntiCompaction reads keys from ssTables_
4. Compaction completes
5. SSTableReader.getApproximateKeyCount(files) now points to files that don't exist!

We have to prevent compaction from messing up the list of files that anticompaction needs to work on to build the new file.
I didn't expect this would happen in a short Junit test. Need to see why compaction is getting triggered.
","28/Aug/09 00:55;sandeep_tata;Quick Fix: Change MemtableObjectCountInMillions to 0.0002. This allows memtables to be bigger and avoids kicking off a compaction before the anticompaction can complete. BootstrapTest will no longer exhibit intermittent failures. (verified)

But that's just a band-aid -- the real solution is to synchronize compaction & anticompaction correctly.","28/Aug/09 01:25;sandeep_tata;Patch to call doAntiCompaction in the tests the same way we do in the actual code -- by submitting it to the MinorCompactionManager so it serializes with other compaction tasks.

Ran the tests a 200 times -- I don't see failures anymore.",28/Aug/09 03:06;jbellis;excellent.  committed,"28/Aug/09 15:07;hudson;Integrated in Cassandra #180 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/180/])
    call doAntiCompaction in the tests the same way we do in the actual code -- by submitting it to the MinorCompactionManager so it serializes with other compaction tasks.  patch by Sandeep Tata; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New node is attempting to bootstrap to itself,CASSANDRA-536,12440313,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,rays,rays,11/10/2009 18:38,3/12/2019 14:09,3/13/2019 22:24,11/12/2009 3:37,0.5,,,,0,,,,,,"I took an existing testing node and deleted the data, and commitlog directories and enabled AutoBootstrap in the config file and restarted cassandra and saw the following error in the log file which indicates to me that its attempting to bootstrap off of itself and failing. All nodes are running the same version of trunk.

DEBUG - Beginning bootstrap process
DEBUG [Thread-7] 2009-11-10 13:20:31,915 BootStrapper.java (line 98) Beginning bootstrap process
DEBUG - Binding thrift service to /0.0.0.0:9160
DEBUG [main] 2009-11-10 13:20:31,915 CassandraDaemon.java (line 101) Binding thrift service to /0.0.0.0:9160
 INFO [main] 2009-11-10 13:20:31,915 CassandraDaemon.java (line 141) Cassandra starting up...
DEBUG [Thread-7] 2009-11-10 13:20:31,966 BootStrapper.java (line 104) Sending BootstrapMetadataMessage to /10.2.4.110 for (36566327313263876483692170869705586748,68939025851256836916907001051563673941]
 WARN [Thread-7] 2009-11-10 13:20:31,966 MessagingService.java (line 468) Running on default stage - beware
DEBUG [Thread-7] 2009-11-10 13:20:31,966 StorageService.java (line 153) Added /10.2.4.110 as a bootstrap source
DEBUG [MESSAGE-DESERIALIZER-POOL:2] 2009-11-10 13:20:31,966 BootstrapMetadataVerbHandler.java (line 52) Received a BootstrapMetadataMessage from /10.2.4.110
ERROR [MESSAGE-DESERIALIZER-POOL:2] 2009-11-10 13:20:31,966 CassandraDaemon.java (line 71) Fatal exception in thread Thread[MESSAGE-DESERIALIZER-POOL:2,5,main]
java.lang.AssertionError
	at org.apache.cassandra.dht.BootstrapMetadataVerbHandler.doVerb(BootstrapMetadataVerbHandler.java:55)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
","FreeBSD 7.2-RELEASE amd64
Diablo Java HotSpot(TM) 64-Bit Server VM (build 10.0-b23, mixed mode)",,,,,,,,,,,,,,,,,,,11/Nov/09 23:21;jbellis;536.patch;https://issues.apache.org/jira/secure/attachment/12424671/536.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:54.5,,,no_permission,,,,,,,,,,,,19743,,,Thu Nov 12 12:34:56 UTC 2009,,,,,,0|i0fzjb:,91355,,,,,,,,,,,11/Nov/09 23:21;jbellis;fixes attached.,12/Nov/09 03:28;jaakko;+1,12/Nov/09 03:37;jbellis;committed,"12/Nov/09 12:34;hudson;Integrated in Cassandra #256 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/256/])
    avoid making local node part of the token ring until bootstrap completes; fix other buglets
patch by jbellis; reviewed by Jaakko Laine for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error with utf8 columnfamilies,CASSANDRA-493,12438228,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/15/2009 18:44,3/12/2019 14:09,3/13/2019 22:24,10/20/2009 0:29,0.4,,,,0,,,,,,"From the mailing list:

ERROR [ROW-MUTATION-STAGE:2935] 2009-10-15 17:32:52,518
DebuggableThreadPoolExecutor.java (line 85) Error in
ThreadPoolExecutor
java.lang.IllegalArgumentException: The name should match the name of
the current column or super column
       at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:208)
       at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:200)
       at org.apache.cassandra.db.ColumnFamily.addColumns(ColumnFamily.java:127)
       at org.apache.cassandra.db.Memtable.resolve(Memtable.java:156)
       at org.apache.cassandra.db.Memtable.put(Memtable.java:139)
       at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:450)
       at org.apache.cassandra.db.Table.apply(Table.java:608)
       at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:205)
       at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:79)
       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)
ERROR [ROW-MUTATION-STAGE:2935] 2009-10-15 17:32:52,519
CassandraDaemon.java (line 71) Fatal exception in thread
Thread[ROW-MUTATION-STAGE:2935,5,main]
java.lang.IllegalArgumentException: The name should match the name of
the current column or super column
       at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:208)
       at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:200)
       at org.apache.cassandra.db.ColumnFamily.addColumns(ColumnFamily.java:127)
       at org.apache.cassandra.db.Memtable.resolve(Memtable.java:156)
       at org.apache.cassandra.db.Memtable.put(Memtable.java:139)
       at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:450)
       at org.apache.cassandra.db.Table.apply(Table.java:608)
       at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:205)
       at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:79)
       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:39)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)

Stopping and starting the cluster gives me something similar:

ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException:
java.lang.IllegalArgumentException: The name should match the name of
the current column or super column
at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
at java.util.concurrent.FutureTask.get(FutureTask.java:83)
at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logFutureExceptions(DebuggableThreadPoolExecutor.java:95)
at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor.afterExecute(DebuggableScheduledThreadPoolExecutor.java:50)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: The name should match
the name of the current column or super column
at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:208)
at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:200)
at org.apache.cassandra.db.ColumnFamily.addColumns(ColumnFamily.java:127)
at org.apache.cassandra.db.ColumnFamily.resolve(ColumnFamily.java:408)
at org.apache.cassandra.db.ColumnFamilyStore.merge(ColumnFamilyStore.java:477)
at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:1078)
at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:689)
at org.apache.cassandra.db.MinorCompactionManager$1.call(MinorCompactionManager.java:165)
at org.apache.cassandra.db.MinorCompactionManager$1.call(MinorCompactionManager.java:162)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:207)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
... 2 more",,,,,,,,,,,,,,,,,,,,16/Oct/09 19:53;jbellis;493.patch;https://issues.apache.org/jira/secure/attachment/12422396/493.patch,15/Oct/09 18:45;jbellis;493.patch;https://issues.apache.org/jira/secure/attachment/12422258/493.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,58:07.0,,,no_permission,,,,,,,,,,,,19719,,,Tue Oct 20 00:29:42 UTC 2009,,,,,,0|i0fz9r:,91312,,,,,,,,,,,15/Oct/09 18:45;jbellis;Fixes using non-utf8-aware comparison as a sanity check in SC.putColumn (to make sure we're actually merging the same column name),"15/Oct/09 22:58;urandom;This patch causes several of the tests to fail.

    [junit] Testcase: testRemoveSuperColumnWithNewData(org.apache.cassandra.db.RemoveSuperColumnTest):	Caused an ERROR
    [junit] null
    [junit] java.nio.BufferUnderflowException
    [junit] 	at java.nio.Buffer.nextGetIndex(Buffer.java:497)
    [junit] 	at java.nio.HeapByteBuffer.getLong(HeapByteBuffer.java:406)
    [junit] 	at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:40)
    [junit] 	at org.apache.cassandra.db.marshal.LongType.compare(LongType.java:27)
    [junit] 	at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:206)
    [junit] 	at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:200)
    [junit] 	at org.apache.cassandra.db.ColumnFamily.addAll(ColumnFamily.java:126)
    [junit] 	at org.apache.cassandra.db.Memtable.resolve(Memtable.java:162)
    [junit] 	at org.apache.cassandra.db.Memtable.put(Memtable.java:145)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:442)
    [junit] 	at org.apache.cassandra.db.Table.apply(Table.java:466)
    [junit] 	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:205)
    [junit] 	at org.apache.cassandra.db.RemoveSuperColumnTest.testRemoveSuperColumnWithNewData(RemoveSuperColumnTest.java:105)",15/Oct/09 23:04;jbellis;looks like those tests are using invalid supercolumn names and we weren't catching that before.  will fix.,"16/Oct/09 19:53;jbellis;My patch was at fault after all.  (Who woulda guessed. :)  The problem is that SC.getComparator is the comparator for the subcolumns, not the SC and its peers.  There is no easy way for the SC to get the comparator for its peers, so this patch just leaves that assert out.","20/Oct/09 00:29;jbellis;From the mailing list:

> I'm no longer running into the issue anymore. [with the patch]

committed to 0.4 and trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corruption in CommitLog,CASSANDRA-605,12442555,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,12/5/2009 23:30,3/12/2019 14:09,3/13/2019 22:24,12/11/2009 15:32,0.5,,,,0,,,,,,"We are seeing corruption in commit log files when cassandra gets shutdown sometimes. I can attach the commitlogs privately. Here is a stacktrace:

 INFO [main] 2009-12-05 14:59:25,946 RecoveryManager.java (line 64) Replaying /mnt/var/cassandra/commitlog/CommitLog-1259972135241.log, /mnt/var/cassandra/commitlog/CommitLog-1260050162791.log, /mnt/var/cassandra/commitlog/CommitLog-1260047770958.log, /mnt/var/cassandra/commitlog/CommitLog-1260052237605.log
ERROR [COMPACTION-POOL:1] 2009-12-05 14:59:37,436 DebuggableThreadPoolExecutor.java (line 120) Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.NumberFormatException: For input string: ""nan""
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:112)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NumberFormatException: For input string: ""nan""
        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1224)
        at java.lang.Float.valueOf(Float.java:388)
        at java.lang.Float.<init>(Float.java:489)
        at com.digg.cassandra.db.marshal.FloatStringType.compare(FloatStringType.java:72)
        at com.digg.cassandra.db.marshal.FloatStringType.compare(FloatStringType.java:19)
        at java.util.concurrent.ConcurrentSkipListMap$ComparableUsingComparator.compareTo(ConcurrentSkipListMap.java:606)
        at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:797)
        at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1640)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedStandard(ColumnFamilyStore.java:530)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeleted(ColumnFamilyStore.java:515)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:111)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:38)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:135)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:130)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:919)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:861)
        at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:663)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:180)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:177)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more",,,,,,,,,,,,,,,,,,,,11/Dec/09 04:41;jbellis;605.patch;https://issues.apache.org/jira/secure/attachment/12427675/605.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,24:54.0,,,no_permission,,,,,,,,,,,,19776,,,Fri Dec 11 15:32:22 UTC 2009,,,,,,0|i0fzyn:,91424,,,,,,,,,,,"07/Dec/09 17:24;jbellis;Chris indicated in IRC that this does look like it only occurs near the end of the commitlog, where fsync hasn't yet guaranteed that data is safe.",10/Dec/09 05:40;jbellis;adding a CRC to each log record should fix this.,11/Dec/09 04:41;jbellis;patch to add CRC,"11/Dec/09 05:52;jbellis;From IRC:

junrao: if that doesn't add much overhead during writes, then it's ok
driftx: if there's a [performance] difference, it's negligible",11/Dec/09 15:32;jbellis;committed to 0.5 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_range_slice() behavior is inconsistent with get_slice() and multiget_slice() when super_column is set in ColumnParent,CASSANDRA-649,12444014,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,vomjom,vomjom,12/22/2009 14:23,3/12/2019 14:09,3/13/2019 22:24,12/26/2009 13:43,0.5,,,,0,,,,,,"Here's an example using my python library ( http://github.com/vomjom/pycassa ):

>>> import pycassa
>>> connect = pycassa.connect()
>>> cf = pycassa.ColumnFamily(connect, 'Test Keyspace', 'Test Super', super=True)
>>> cf.insert('key1', {'2': {'sub3': 'val3', 'sub4': 'val4'}})

>>> cf.get('key1')
{'2': {'sub4': 'val4', 'sub3': 'val3'}}
>>> cf.get('key1', super_column='2')
{'sub4': 'val4', 'sub3': 'val3'}

>>> cf.multiget(['key1'])
{'key1': {'2': {'sub4': 'val4', 'sub3': 'val3'}}}
>>> cf.multiget(['key1'], super_column='2')
{'key1': {'sub4': 'val4', 'sub3': 'val3'}}

>>> list(cf.get_range())
[('key1', {'2': {'sub4': 'val4', 'sub3': 'val3'}})]
>>> list(cf.get_range(super_column='2'))
[('key1', {'2': {'sub4': 'val4', 'sub3': 'val3'}})]

In the last case, I expected:
[('key1', {'sub4': 'val4', 'sub3': 'val3'})]

If the super_column argument is supplied, then all of these make a ColumnParent with:

cp = ColumnParent(column_family=self.column_family, super_column=super_column)

Or basically, in the KeySlice returned by get_range_slice(), if super_column was set in the passed in the ColumnParent, the columns member of the KeySlice should be a list of respective SuperColumn.columns and not a list of SuperColumn.

Another way to describe the problem:
get_slice(), multiget_slice(), and get_range_slice() all return:
list<ColumnOrSuperColumn> in their return values in some way or another.

If super_column is set in the ColumnParent then:
1. get_slice() and multiget_slice() return the list of SuperColumn.columns each wrapped in a ColumnOrSuperColumn
2. The KeySlice in get_range_slice() returns a list of ONE SuperColumn wrapped in a ColumnOrSuperColumn",Linux,,,,,,,,,,,,,,,,,,,25/Dec/09 16:37;jbellis;649.patch;https://issues.apache.org/jira/secure/attachment/12428955/649.patch,25/Dec/09 12:07;vomjom;649_unit_test.patch;https://issues.apache.org/jira/secure/attachment/12428951/649_unit_test.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,39:18.0,,,no_permission,,,,,,,,,,,,19801,,,Sat Dec 26 13:43:08 UTC 2009,,,,,,0|i0g08f:,91468,,,,,,,,,,,24/Dec/09 20:39;jbellis;I'd be a lot more sure that this is a bug in Cassandra rather than an under-tested client library if you stuck to raw Thrift.,"24/Dec/09 22:41;jbellis;the patch for CASSANDRA-647 makes get_range_slice use the same convert-to-thrift format as get_slice and multiget_slice.  If that doesn't fix it, please add a Thrift test to test_server.py showing the problem.  (Running the thrift tests is described in http://wiki.apache.org/cassandra/HowToContribute)","25/Dec/09 12:07;vomjom;I've attached a patch to test_server.py to unit test the bug.

The old test case depended on the behavior that I indicated, so I replaced that with the expected behavior.

I tested the latest trunk and it seems to still have this issue.","25/Dec/09 16:37;jbellis;thanks for the test.

patch extracts thriftifyColumnFamily and applies to getSlice and get_slice_range","25/Dec/09 17:43;vomjom;+1

Works for me.

Thanks.",26/Dec/09 13:43;jbellis;committed to 0.5 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted Handoff CF contention,CASSANDRA-658,12444353,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,12/30/2009 2:44,3/12/2019 14:09,3/13/2019 22:24,1/7/2010 5:41,0.6,,,,0,,,,,,"Hinted handoff causes a lot of contention on the HH CF, causing insert speed to massively drop.  Most of the row mutation stage threads end up blocking on each other at Memtable.resolve.  This is because HH sends the hint to the closest node, which will always be the node handling the write.

To reproduce: start a cluster with even InitialTokens, and begin a constant stream of writes to one node, with an even key distribution. (I used 4 nodes and stress.py in random mode.)  Take a node down, and the insert rate begin to drop, eventually settling between 100-300/s and sustaining there.  Bringing the down node back up will restore the original insert rate.","debian lenny amd64 OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",,,,,,,,,,,,,,,,,,,01/Jan/10 04:02;jbellis;ASF.LICENSE.NOT.GRANTED--0001-use-throughput-and-op-count-instead-of-size-and-column.txt;https://issues.apache.org/jira/secure/attachment/12429240/ASF.LICENSE.NOT.GRANTED--0001-use-throughput-and-op-count-instead-of-size-and-column.txt,01/Jan/10 04:02;jbellis;ASF.LICENSE.NOT.GRANTED--0002-replace-sharded-row-locks-with-column-level-locking.txt;https://issues.apache.org/jira/secure/attachment/12429241/ASF.LICENSE.NOT.GRANTED--0002-replace-sharded-row-locks-with-column-level-locking.txt,01/Jan/10 04:02;jbellis;ASF.LICENSE.NOT.GRANTED--0003-r-m-unused-code.txt;https://issues.apache.org/jira/secure/attachment/12429242/ASF.LICENSE.NOT.GRANTED--0003-r-m-unused-code.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,44:23.6,,,no_permission,,,,,,,,,,,,19806,,,Fri Jan 08 12:38:36 UTC 2010,,,,,,0|i0g0af:,91477,,,,,,,,,,,"31/Dec/09 19:44;stuhood;Brilliant... I really like this change.

I think the main reason that we had an Atomic variable for size() in CF and SC was to perform this calculation, so perhaps that code should be considered dead, and removed. I noticed a bunch of subtle bugs in it last time I was looking anyway. The size() method can remain, and calculate the size for each call?","31/Dec/09 21:18;jbellis;We still want to know size for when we're serializing, so I've left that code alone for now.","31/Dec/09 21:57;stuhood;Removes the atomic size calculation from SuperColumn, which was not being updated for remove() anyway.","01/Jan/10 02:22;jbellis;this actually has a race for simple columns: if a value is preset for a given column name, and two threads (A and B) run the putIfAbsent part at the same time, then A goes into the synchronized block and changes the value, thread C could attempt addColumn and sync on the value put by A, while B syncs on the old value.

This should be fixable by using replace(), not put().","01/Jan/10 04:04;jbellis;new version 2 fixes the races and adds a similar fine-grained approach to SuperColumn (which is not really more expensive, since we're paying the price of using a Concurrent map implementation already).  This includes Stu's size-removal patch.  03 does more cleanup.","07/Jan/10 05:15;brandon.williams;+1, I replicated the exact scenario originally outlined and insert speed does not dramatically drop.",07/Jan/10 05:41;jbellis;committed,"07/Jan/10 13:08;hudson;Integrated in Cassandra #316 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/316/])
    replace sharded row locks with column-level locking
patch by jbellis; tested by Brandon Williams for 
use throughput and op count instead of size and column count to determine when to flush, greatly reducing the amount of synchronization required to insert
patch by jbellis; tested by Brandon Williams for 
","08/Jan/10 12:38;hudson;Integrated in Cassandra #317 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/317/])
    update release notes for config changes

Patch by eevans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable generation clash during compaction,CASSANDRA-418,12434696,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,sammy.yu,sammy.yu,9/3/2009 1:51,3/12/2019 14:09,3/13/2019 22:24,9/7/2009 14:50,0.4,,,,0,,,,,,"We found that one of our node started getting timeouts for get_slice.  Looking further we found that the CFS.ssTables_ references a SStable doesn't exist on the file system.

Walking down the log we see that the sstable in question 6038 is being compacted onto itself (in terms of filename file wise it is written to -tmp):
system.log.2009-09-01: INFO [MINOR-COMPACTION-POOL:1] 2009-09-01 23:50:07,553 ColumnFamilyStore.java (line 1067) Compacting 
[/mnt/var/cassandra/data/Digg/FriendActions-6037-Data.db,/mnt/var/cassandra/data/Digg/FriendActions-6038-Data.db,/mnt/var/cassandra/data/Digg/
FriendActions-6040-Data.db,/mnt/var/cassandra/data/Digg/FriendActions-6042-Data.db]
system.log.2009-09-01: INFO [MINOR-COMPACTION-POOL:1] 2009-09-01 23:51:43,727 ColumnFamilyStore.java (line 1209) Compacted to
/mnt/var/cassandra/data/Digg/FriendActions-6038-Data.db.  0/1010269806 bytes for 9482/9373 keys read/written.  Time: 96173ms.

It appears the generation number is generated by looking at the lowest number in the list of files to be compacted and adding 1.  In this scenario it is 6037+1=6038.
The code in CFS.doFileCompaction will remove the key and add the key back and remove the key again, hence the error we were seeing.

Should the generation number be generated via another way or should we update doFileCompaction to be smarter?

",,,,,,,,,,,,,,,,,,,,03/Sep/09 02:56;sammy.yu;0001-CASSANDRA-418-Use-monotonically-increasing-generatio.patch;https://issues.apache.org/jira/secure/attachment/12418464/0001-CASSANDRA-418-Use-monotonically-increasing-generatio.patch,03/Sep/09 13:51;sammy.yu;0002-CASSANDRA-418-Use-monotonically-increasing-generatio.patch;https://issues.apache.org/jira/secure/attachment/12418515/0002-CASSANDRA-418-Use-monotonically-increasing-generatio.patch,04/Sep/09 14:44;jbellis;418-2.patch;https://issues.apache.org/jira/secure/attachment/12418630/418-2.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,09:05.4,,,no_permission,,,,,,,,,,,,19680,,,Tue Sep 08 12:35:49 UTC 2009,,,,,,0|i0fyt3:,91237,,,,,,,,,,,"03/Sep/09 02:09;jbellis;the compaction code relies on the bucketizer to keep files of the same compaction-count (a bucket of sstables that have been compacted twice, one of sstables that have been compacted 3 times) so that you are never compacting sstables of consecutive generations -- all will have even numbers, or all odd.  something has broken that invariant.

rather than try to band-aid the bucketizer i think making the generation-generator more robust is the way to go.  this seems like a flimsy property to try to preserve.

my vote would be to simplify: just pick the next monotonically increasing int any time we need a new tmp sstable file, whether for flush, compaction, or bootstrap.  I.e. via CFS.getTempSSTableFileName, without the extra increment.

the reason historically that FB tried to be fancy is, they were trying to optimize away reading older sstables at all if the data being queried was found in a newer one.  the ""only new sstables get a number from the atomic int, and the compactions fit in between"" was to preserve this.  (then you sort on the generation number and higher ones are always newer.)

but that can't work (see CASSANDRA-223) so we always do a full merge across all sstables now.  so we can simplify this safely.","03/Sep/09 02:56;sammy.yu;Use monotonically increase generation number for newly compacted sstable.
","03/Sep/09 02:57;lenn0x;A note on the ML might be needed, with this bug it looks like we are going to have to dump our old data and re-import since we don't have a 100% way of figuring out what data is missing across the cluster.","03/Sep/09 02:57;sammy.yu;Should we also change CFS.getTempSSTableFileName to just increment fileIndexGenerator_ once?
","03/Sep/09 12:40;jbellis;> Should we also change CFS.getTempSSTableFileName to just increment fileIndexGenerator_ once? 

right, that's what i meant by ""w/o the extra increment.""","03/Sep/09 13:51;sammy.yu;Self contained patch that now increment the generation number by one
",03/Sep/09 15:04;jbellis;(also affects version 0.3 for the record),03/Sep/09 15:11;jbellis;committed,04/Sep/09 14:44;jbellis;missed some code.  this cleans up inaccurate comments and remaining double-increment code,04/Sep/09 17:57;sammy.yu;+1 looks good,07/Sep/09 14:50;jbellis;committed to 0.4 and 0.5,"08/Sep/09 12:35;hudson;Integrated in Cassandra #191 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/191/])
    clean up inaccurate comments; remaining double-increment code.
patch by jbellis; reviewed by Sammy Yu for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expected both token and generation columns,CASSANDRA-576,12441535,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,dispalt,dispalt,11/24/2009 1:14,3/12/2019 14:09,3/13/2019 22:24,11/30/2009 20:00,0.5,,,,0,,,,,,"I restarted cassandra and this happened, and kept repeating for a couple hours, seemingly never finishing.

2009-11-24_00:16:49.23400 INFO - Compacting [org.apache.cassandra.io.SSTableReader(path='/var/lib/cassandra/data/MonitorApp/Rollup20m-414-Data.db'),org.apache.cassandra.io.SSTableReader(path='/var/lib/cassandra/data/MonitorApp/Rollup20m-415-Data.db'),org.apache.cassandra.io.SSTableReader(path='/var/lib/cassandra/data/MonitorApp/Rollup20m-416-Data.db'),org.apache.cassandra.io.SSTableReader(path='/var/lib/cassandra/data/MonitorApp/Rollup20m-417-Data.db')]
2009-11-24_00:16:49.98396 ERROR - Exception encountered during startup.
2009-11-24_00:16:49.98396 java.lang.RuntimeException: Expected both token and generation columns; found ColumnFamily(LocationInfo [Token,])
2009-11-24_00:16:49.98396       at org.apache.cassandra.db.SystemTable.initMetadata(SystemTable.java:154)
2009-11-24_00:16:49.98396       at org.apache.cassandra.service.StorageService.start(StorageService.java:257)
2009-11-24_00:16:49.98396       at org.apache.cassandra.service.CassandraServer.start(CassandraServer.java:70)
2009-11-24_00:16:49.98396       at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:94)
2009-11-24_00:16:49.98396       at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:166)
2009-11-24_00:16:49.99396 Exception encountered during startup.
2009-11-24_00:16:49.99396 java.lang.RuntimeException: Expected both token and generation columns; found ColumnFamily(LocationInfo [Token,])
2009-11-24_00:16:49.99396       at org.apache.cassandra.db.SystemTable.initMetadata(SystemTable.java:154)
2009-11-24_00:16:49.99396       at org.apache.cassandra.service.StorageService.start(StorageService.java:257)
2009-11-24_00:16:49.99396       at org.apache.cassandra.service.CassandraServer.start(CassandraServer.java:70)
2009-11-24_00:16:49.99396       at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:94)
2009-11-24_00:16:49.99396       at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:166)
2009-11-24_00:16:50.83392 Listening for transport dt_socket at address: 8888",,,,,,,,,,,,,,,,,,,,24/Nov/09 02:37;jbellis;576.patch;https://issues.apache.org/jira/secure/attachment/12425927/576.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,19:01.1,,,no_permission,,,,,,,,,,,,19762,,,Mon Nov 30 20:00:05 UTC 2009,,,,,,0|i0fzs7:,91395,,,,,,,,,,,"24/Nov/09 01:19;jbellis;dispalt: I restarted cassandra, after using cleanup

that's the culprit, cleanup cleaned out the system table row; the token setting was in the commit log and got replayed so it was still there, but the generation was not.","24/Nov/09 02:37;jbellis;skip the system table (keyspace) for cleanup, it's local-only",24/Nov/09 02:55;stuhood;Looks good to me.,30/Nov/09 20:00;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"time-based slicing does not work correctly w/ ""historical"" memtables",CASSANDRA-223,12427409,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,6/8/2009 22:09,3/12/2019 14:09,3/13/2019 22:24,6/26/2009 17:26,0.4,,,,0,,,,,,"TimeFilter assumes that it is done as soon as it finds a column stamped earlier than what it is filtering on, but when you have a group of ""historical"" memtables whose columns were written in an arbitrary order this is not a safe assumption.

It is not even a safe assumption when dealing with a single memtable + sstable pair, as the attached new test shows.",,,,,,,,,,,,,,,,,,,,25/Jun/09 21:33;jbellis;223-2-v2.patch;https://issues.apache.org/jira/secure/attachment/12411864/223-2-v2.patch,25/Jun/09 21:24;jbellis;223-2.patch;https://issues.apache.org/jira/secure/attachment/12411863/223-2.patch,08/Jun/09 22:10;jbellis;223.patch;https://issues.apache.org/jira/secure/attachment/12410183/223.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,02:21.3,,,no_permission,,,,,,,,,,,,19600,,,Fri Jun 26 17:26:47 UTC 2009,,,,,,0|i0fxmf:,91045,,,,,,,,,,,"09/Jun/09 14:02;jbellis;Thinking about it more, the current behavior is sort of reasonable if you assume that timestamp values are strictly increasing.  I prefer not to rely on this because it's impossible to sync clocks with perfect accuracy, but it's a reasonable optimization to make and consistent with the rest of Cassandra's design.

In this case though, clock sync problems can lead to permanently inconsistent behavior -- different queries will not agree on what data the node contains.","09/Jun/09 17:02;junrao;The assumption that columns in an SSTable with a larger file name always have more recent timestamp also breaks in the following cases:
1. after SSTables are compacted (since the compaction works on a group of SSTables at a time)
2. hinted data is delivered
3. columns fixed through read repairs
","10/Jun/09 14:39;junrao;get_column is also designed to stop iterating SSTables as soon as the requested column is found. Therefore, it could suffer from the above problems too.
","22/Jun/09 15:51;junrao;Since it's hard to guarantee any timestamp ordering among memtable and sstables because of the reasons listed above, the only way that we can get the correct answer is do look at the memtable and ALL sstables (this is what HBase has been doing). This affects the following APIs.
get_column
get_slice_by_names
get_slice_by_name_range
get_slice_since

The implication is that those APIs will potentially run slower since there are more files to read. One can probably tune the performance by setting a proper compaction policy.
","22/Jun/09 16:04;jbellis;I came to the same conclusion.

One partial answer to the files-to-read is to change compaction to guarantee log(n) sstable files instead of the current ad-hoc behavior, where n is the maximum sstable ""generation"" number.  (Where ""generation"" is the number of compactions done.)

For each CF, when you flush, you compact until there is nothing already at the same generation to compact with.  For example,

flush 1: nothing to merge.  memtable becomes sstable-gen0
flush 2: there is already a sstable-gen0 so you merge.  now you have sstable-gen1
flush 3: no gen0, so you store there.  now you have sstable-gen0, sstable-gen1
flush 4: 0 and 1 exist, so you compact (with the new one) to sstable-gen2

etc.

Generation tracking can be done in the sstable filename.","22/Jun/09 18:40;junrao;Well, as for SSTable compaction, we probably can learn from Lucene. Overall, Lucene tries to keep on the order of log(n) index segments, where n is the total number of segments generated. It does that by keeping merging index segments (up to a merging factor) of about the same size. The current compaction code in cassandra seems to try to do that too. It's worth revisiting it though.
",25/Jun/09 21:24;jbellis;add brute-force fix for the bug: always merge in results from all memtables and sstables.  (this is what bigtable does.),25/Jun/09 21:33;jbellis;oops.  already had to rebase.,"26/Jun/09 17:02;junrao;The patch looks good to me.
",26/Jun/09 17:26;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make BinaryMemtable work,CASSANDRA-337,12432082,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lenn0x,jbellis,jbellis,8/3/2009 19:42,3/12/2019 14:09,3/13/2019 22:24,8/28/2009 16:28,0.4,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,28/Aug/09 01:18;lenn0x;0001-Refactored-and-merged-BMT-to-MT-codepath-v3.patch;https://issues.apache.org/jira/secure/attachment/12417945/0001-Refactored-and-merged-BMT-to-MT-codepath-v3.patch,25/Aug/09 03:37;lenn0x;0001-Working-version-of-BMT.patch;https://issues.apache.org/jira/secure/attachment/12417565/0001-Working-version-of-BMT.patch,26/Aug/09 15:47;lenn0x;0002-Added-flushAndshutdown-v2.patch;https://issues.apache.org/jira/secure/attachment/12417746/0002-Added-flushAndshutdown-v2.patch,27/Aug/09 17:29;lenn0x;0003-Removed-decorateKey-in-writer.append-in-BMT.patch;https://issues.apache.org/jira/secure/attachment/12417901/0003-Removed-decorateKey-in-writer.append-in-BMT.patch,11/Aug/09 14:19;jbellis;337-rebased.patch;https://issues.apache.org/jira/secure/attachment/12416204/337-rebased.patch,28/Aug/09 15:45;jbellis;337-v3.patch;https://issues.apache.org/jira/secure/attachment/12417996/337-v3.patch,03/Aug/09 20:03;jbellis;337.patch;https://issues.apache.org/jira/secure/attachment/12415399/337.patch,,,,,,,7,,,,,,,,,,,,,,,,,,,35:02.2,,,no_permission,,,,,,,,,,,,19640,,,Sat Aug 29 12:35:41 UTC 2009,,,,,,0|i0fybj:,91158,,,,,,,,,,,03/Aug/09 20:03;jbellis;my humble contribution to the cleanup: use CFS executor for BMt flushes,11/Aug/09 14:19;jbellis;rebased,11/Aug/09 16:35;urandom;+1,11/Aug/09 16:55;euphoria;I don't understand the scope of this issue and the submitted patches.  Can anyone elaborate?,"11/Aug/09 17:16;jbellis;BinaryMemtable is intended to allow writing pre-serialized data, but FB never really cleaned it up to make it useful for anyone but them.  Chris has done that and this ticket is to remind him to post diffs.

My patch just removes an unnecessary extra executor and class and has BMt use the CFS executor for flushes like normal Mt.",11/Aug/09 17:17;jbellis;committed the CFS patch.,"11/Aug/09 17:41;lenn0x;-1

This should be reverted. As my discussion with Jonathan,  we want to make the Binary Memtable Flusher thread configurable, and we wouldn't want to have it under the same path as cfStore, since that should be a different tunable. I'll have my patches ready later this week for Binary Memtable work we have done at Digg.","12/Aug/09 13:08;hudson;Integrated in Cassandra #165 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/165/])
    use CFS executor for BMt flushes.
patch by jbellis; reviewed by Eric Evans for 
",25/Aug/09 03:37;lenn0x;Working version of BMT that we are running at Digg. Hopefully I didn't miss anything. Wanted to post this before I forget about it :),"25/Aug/09 03:43;lenn0x;Example of using BMT:

http://github.com/lenn0x/Cassandra-Hadoop-BMT/tree/master
",25/Aug/09 21:07;jbellis;is there overlap b/t this and CASSANDRA-342?,"25/Aug/09 23:47;lenn0x;Looking at his code, no. I made some minor interface changes such as flushBinary() public but everything I have done affects the BMT work already there. He seems to be adding functionality, I have no Hadoop code in my BMT.","26/Aug/09 05:23;jmhodges;Yeah, this looks to be clear of any interaction with CASSANDRA-342.",26/Aug/09 06:39;lenn0x;Forgot to add the flushAndShutdown method,"26/Aug/09 13:57;johanoskarsson;I assume the two last patches are to be applied together? If I try to build trunk + those patches I get the following error:
    [javac] /home/cassandra/cassandra-trunk/src/java/org/apache/cassandra/net/MessagingService.java:505: cannot find symbol
    [javac] symbol  : method getConnections()
    [javac] location: class org.apache.cassandra.net.TcpConnectionManager
    [javac]             for(TcpConnection connection: entry.getValue().getConnections())
","26/Aug/09 15:47;lenn0x;Fixed, forgot the getConnections",27/Aug/09 17:29;lenn0x;Removed decorateKey() in writer.append,"27/Aug/09 20:17;jbellis;I still don't see any reason to use separate executors for BMT flush and normal MT.  In fact I was thinking normal MT should probably have threads = #disks.  (Why do you use 2x?)  I'd favor keeping a single executor, with configurable thread count.","27/Aug/09 21:23;lenn0x;I can live with merging the executors for BMT flush and normal MT. Though I want the ability to specify a min and max for the threads in storage-conf.xml. During an import, we would want the ability to have a certain quality of service (I want a min of 4 threads during normal usage, but when we start binary import, I want a higher thread count). 

Merging both executors makes it a little harder, but the benefit is less code/cruft to manage -- that's a win for sure. When we were benchmarking our 7 node cluster with 600-800Mb/s per node we saw lower memory usage/better throughput on the flushing by having 2 threads per disk.

If this is acceptable (above) I can submit a patch.","27/Aug/09 21:29;jbellis;> I want the ability to specify a min and max for the threads in storage-conf.xml

works for me.","28/Aug/09 01:18;lenn0x;Refactored, and merged BMT to MT codepath. Added FlushMinThreads and FlushMaxThreads.","28/Aug/09 15:48;jbellis;looking good.  here is my version with light cleanup done.

but I don't see a way to check if a flushandshutdown is complete, and it's safe to kill the node?

should we make           logger_.debug(""Shutdown invocation complete."");

run at info level instead of debug?  or something more sophisticated?",28/Aug/09 16:08;lenn0x;I think going to logger_.info would be fine. Me and Sammy plan on opening another ticket for better shutdown support in Cassandra. We could get more sophisticated there.,"28/Aug/09 16:28;jbellis;committed v3 w/ the log info change.

do you want to submit the demo from github for contrib/ in another ticket?",28/Aug/09 16:43;lenn0x;Sure thing. See CASSANDRA-398,"29/Aug/09 12:35;hudson;Integrated in Cassandra #181 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/181/])
    Fixes to make BinaryMemtable useful.  Highlights are configurable threads for [binary]memtable flushing and flushAndShutdown JMX/nodeprobe directive.
patch by Chris Goffinet; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock with SelectorManager.doProcess and TcpConnection.write,CASSANDRA-392,12434078,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,sammy.yu,sammy.yu,8/26/2009 18:35,3/12/2019 14:09,3/13/2019 22:24,8/31/2009 15:49,0.4,,,,0,,,,,,"We ran into a deadlock last night:
Name: MESSAGE-SERIALIZER-POOL:2
State: BLOCKED on sun.nio.ch.SelectionKeyImpl@2e257f1b owned by: TCP Selector Manager
Total blocked: 1  Total waited: 1

Stack trace: 
org.apache.cassandra.net.SelectionKeyHandler.turnOnInterestOps(SelectionKeyHandler.java:73)
org.apache.cassandra.net.TcpConnection.write(TcpConnection.java:186)
   - locked org.apache.cassandra.net.TcpConnection@5ab9f791
org.apache.cassandra.net.MessageSerializationTask.run(MessageSerializationTask.java:67)
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
java.lang.Thread.run(Thread.java:619)



Name: TCP Selector Manager
State: BLOCKED on org.apache.cassandra.net.TcpConnection@5ab9f791 owned by: MESSAGE-SERIALIZER-POOL:2
Total blocked: 2  Total waited: 0

Stack trace: 
org.apache.cassandra.net.TcpConnection.connect(TcpConnection.java:360)
org.apache.cassandra.net.SelectorManager.doProcess(SelectorManager.java:131)
   - locked sun.nio.ch.SelectionKeyImpl@2e257f1b
org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:98)


The SelectionManager.doProcess acquires a monitor on the SelectionKey and then calls methods such as TcpConnection.connect(SelectionKey key) which obtains a monitor for the TcpConnection object itself.  Another task eg: MessageSerializationTask can come along and call write(Message message) which obtains a monitor for the TCPConnection first and then on calls to turnOnInterestOps tries to obtain the monitor for the SelectionKey which causes the deadlock.


",,,,,,,,,,,,,,,,,,,,26/Aug/09 18:57;sammy.yu;0001-CASSANDRA-392-Moved-turnOnInterestOps-outside-of-syn.patch;https://issues.apache.org/jira/secure/attachment/12417771/0001-CASSANDRA-392-Moved-turnOnInterestOps-outside-of-syn.patch,26/Aug/09 22:05;junrao;issue392.patchv2;https://issues.apache.org/jira/secure/attachment/12417819/issue392.patchv2,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,05:17.4,,,no_permission,,,,,,,,,,,,19667,,,Tue Sep 01 12:35:59 UTC 2009,,,,,,0|i0fynj:,91212,,,,,,,,,,,"26/Aug/09 18:57;sammy.yu;For TcpConnection.write(Message) moved turnOnInterestOps outside of synchronized block so that call only depends on having the SelectionKey monitor.

","26/Aug/09 22:05;junrao;In SelectorManager.doProcess(), I don't see why we need to synchronize on each selection key any more. Within each of the process such as connect, read and write, we already synchronize on the selection key through turnOnInterestOps/turnOffInterestOps (which only holds a short lock on a selection key).

Attached is a patch that removes the selection key synchronization in SelectorManager(). Sammy, could you give it a try and see it works?","29/Aug/09 22:21;lenn0x;+1.

We tested this in production and its working good!. Let's ship it.",31/Aug/09 15:49;junrao;commited.,"01/Sep/09 12:35;hudson;Integrated in Cassandra #184 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/184/])
    Deadlock with SelectorManager.doProcess and TcpConnection.write; patch by Sammy Yu and junrao; reviewed by Chris Goffinet for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FBUtilities.bytesToHex and FBUtilities.hexToBytes are not inverses,CASSANDRA-490,12438133,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,10/14/2009 22:08,3/12/2019 14:09,3/13/2019 22:24,10/15/2009 2:11,0.4,0.5,,,0,,,,,,b != hexToBytes(bytesToHex(b)) <-- bad.,all,,,,,,,,,,,,,,,,,,,14/Oct/09 22:09;sandeep_tata;490-v1.patch;https://issues.apache.org/jira/secure/attachment/12422148/490-v1.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,11:24.2,,,no_permission,,,,,,,,,,,,19717,,,Thu Oct 15 02:11:24 UTC 2009,,,,,,0|i0fz93:,91309,,,,,,,,,,,14/Oct/09 22:09;sandeep_tata;fix + unit test,15/Oct/09 02:11;urandom;committed; thanks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
intellibootstrap,CASSANDRA-385,12433531,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,8/20/2009 3:59,3/12/2019 14:09,3/13/2019 22:24,10/7/2009 16:24,0.5,,,,0,,,,,,"ideally bootstrap mode should determine its new position on the ring by examining the Load of the existing nodes in the cluster.  (currently Load is just disk space used but making this pluggable is another ticket.)  having found the highest-load-node-that-is-not-participating-in-bootstrap, it should ask that node for a Token which would move half the keys over to the new node.

This is easily computed since we have a periodic sampling of keys in memory of all the keys on disk, and even SSTable.getIndexedKeys that merges all such keys.  So pick the midpoint, and turn it into a token (these are decorated keys so that is always possible).",,,,,,,,,,,,,,,,,,,,07/Oct/09 15:52;jbellis;0007-switch-to-messagingservice-to-get-bootstrap-token.patch;https://issues.apache.org/jira/secure/attachment/12421532/0007-switch-to-messagingservice-to-get-bootstrap-token.patch,07/Oct/09 15:52;jbellis;0008-rename-getMessagingInstance-instance-r-m-unused.patch;https://issues.apache.org/jira/secure/attachment/12421534/0008-rename-getMessagingInstance-instance-r-m-unused.patch,05/Oct/09 18:26;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-385-clean-up-loadinfo-and-SLB.txt;https://issues.apache.org/jira/secure/attachment/12421310/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-385-clean-up-loadinfo-and-SLB.txt,05/Oct/09 18:26;jbellis;ASF.LICENSE.NOT.GRANTED--0002-make-LoadInfo-a-double-instead-of-going-from-long-s.txt;https://issues.apache.org/jira/secure/attachment/12421311/ASF.LICENSE.NOT.GRANTED--0002-make-LoadInfo-a-double-instead-of-going-from-long-s.txt,05/Oct/09 18:26;jbellis;ASF.LICENSE.NOT.GRANTED--0003-move-javadoc-into-mbean.-inline-methods-where-wrappin.txt;https://issues.apache.org/jira/secure/attachment/12421312/ASF.LICENSE.NOT.GRANTED--0003-move-javadoc-into-mbean.-inline-methods-where-wrappin.txt,05/Oct/09 18:26;jbellis;ASF.LICENSE.NOT.GRANTED--0004-use-Strings-instead-of-Endpoints-in-jmx-methods.-merge.txt;https://issues.apache.org/jira/secure/attachment/12421313/ASF.LICENSE.NOT.GRANTED--0004-use-Strings-instead-of-Endpoints-in-jmx-methods.-merge.txt,05/Oct/09 18:26;jbellis;ASF.LICENSE.NOT.GRANTED--0005-add-getLoadMap-jmx-method-add-load-info-to-nodeprobe.txt;https://issues.apache.org/jira/secure/attachment/12421314/ASF.LICENSE.NOT.GRANTED--0005-add-getLoadMap-jmx-method-add-load-info-to-nodeprobe.txt,05/Oct/09 18:26;jbellis;ASF.LICENSE.NOT.GRANTED--0006-get-token-on-bootstrap-that-gives-us-half-of-the-keys.txt;https://issues.apache.org/jira/secure/attachment/12421315/ASF.LICENSE.NOT.GRANTED--0006-get-token-on-bootstrap-that-gives-us-half-of-the-keys.txt,,,,,,8,,,,,,,,,,,,,,,,,,,38:14.1,,,no_permission,,,,,,,,,,,,19664,,,Sun Oct 11 12:35:11 UTC 2009,,,,,,0|i0fylz:,91205,,,,,,,,,,,"20/Aug/09 04:38;eweaver;this would be super...would obviate the need for fancy LB for quite a while, at least for us.","24/Sep/09 22:19;jbellis;06
    get token on bootstrap that gives us half of the keys from the most heavily-loaded node.
    the ""splits"" list should also be useful for #342.

05
    add getLoadMap jmx method; add load info to nodeprobe ring

04
    use Strings instead of Endpoints in jmx methods. merge cluster info into ring.

03
    move javadoc into mbean.  inline methods where wrapping is not necessary to satisfy Demeter

02
    make LoadInfo a double instead of going from long -> string -> LoadInfo -> int

01
    clean up loadinfo and SLB
",05/Oct/09 15:36;jbellis;rebased,"05/Oct/09 17:14;sandeep_tata;I get a build error. Perhaps a missing patch file in the set?

build-project:
     [echo] apache-cassandra-incubating: /home/stata/cassandra/build.xml
    [javac] Compiling 246 source files to /home/stata/cassandra/build/classes
    [javac] /home/stata/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1177: cannot find symbol
    [javac] symbol  : method undecorateKey(java.lang.String)
    [javac] location: interface org.apache.cassandra.dht.IPartitioner
    [javac]             String key = partitioner_.undecorateKey(decoratedKeys.get(index));
    [javac]                                      ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 1 error
",05/Oct/09 18:26;jbellis;oops.  rebased better. :),"06/Oct/09 23:01;urandom;This looks good to me, but I do have one question/concern.

Introduced in 0006-get-token-on-bootstrap-that-gives-us-half-of-the-keys.txt, when a node bootstraps and hasn't been assigned a token, it selects one via StorageService.getBootstrapTokenFrom which uses Thrift to retrieve it as a property. 

My concern is that this is the first time we've used Thrift for anything that wasn't client oriented. Do we want to start now, or does it make sense to preserve that separation?

If nothing else, sticking with this approach will mean that work is needed to improve how  the Thrift service is bound to interfaces, (currently configured via ThriftAddress), because there is only one option that is guaranteed safe with this patch applied (0.0.0.0).","06/Oct/09 23:38;jbellis;there's a couple motivations there:

 - i believe this same method call will be a good fit for Hadoop asking for input splits, so it will still be client-facing as well.  the convention is safe, or will be :)
 - RPC is a better fit for this than messagingservice's callback-oriented approach, since we do want to block the bootstrap until getting an answer

how bad is the thriftAddress pain going to be?","07/Oct/09 15:53;jbellis;08
    rename getMessagingInstance -> instance; r/m unused methods from FBUtiltities

07
    switch to messagingservice to get bootstrap token

(will squash this into 06 for commit, but it's probably easy to review as a separate patch)
",07/Oct/09 16:12;urandom;Looks good to me. +1,07/Oct/09 16:24;jbellis;committed,"08/Oct/09 12:35;hudson;Integrated in Cassandra #221 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/221/])
    rename getMessagingInstance -> instance; r/m unused methods from FBUtiltities
patch by jbellis; reviewed by Eric Evans for 
get token on bootstrap that gives us half of the keys from the most heavily-loaded node. (the ""splits"" approach should also be useful for #342; adding it to Thrift is trivial)
patch by jbellis; reviewed by Eric Evans for 
add getLoadMap jmx method; add load info to nodeprobe ring
patch by jbellis; reviewed by Eric Evans for 
use Strings instead of Endpoints in jmx methods. merge cluster info into ring.
patch by jbellis; reviewed by Eric Evans for 
move javadoc into mbean.  inline methods where wrapping is not necessary to satisfy Demeter
patch by jbellis; reviewed by Eric Evans for 
make LoadInfo a double instead of going from long -> string -> LoadInfo -> int
patch by jbellis; reviewed by Eric Evans for 
clean up loadinfo and SLB
patch by jbellis; reviewed by Eric Evans for 
","11/Oct/09 12:35;hudson;Integrated in Cassandra #224 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/224/])
    add AutoBootstrap config option
patch by jbellis.  reviewed by goffinet for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail startup if <seeds> is empty,CASSANDRA-479,12437617,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,sammy.yu,jbellis,jbellis,10/8/2009 17:00,3/12/2019 14:09,3/13/2019 22:24,10/8/2009 18:38,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,08/Oct/09 18:07;sammy.yu;0001--CASSANDRA-479-require-a-minimum-of-one-seed-so-we.patch;https://issues.apache.org/jira/secure/attachment/12421643/0001--CASSANDRA-479-require-a-minimum-of-one-seed-so-we.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,35:07.5,,,no_permission,,,,,,,,,,,,19712,,,Fri Oct 09 12:35:07 UTC 2009,,,,,,0|i0fz6n:,91298,,,,,,,,,,,08/Oct/09 18:38;jbellis;committed,"09/Oct/09 12:35;hudson;Integrated in Cassandra #222 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/222/])
    Fail startup if <seeds> is empty.  patch by Sammy Yu; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NOTICE.txt out of data for third-party libs,CASSANDRA-411,12434558,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,9/1/2009 21:13,3/12/2019 14:09,3/13/2019 22:24,9/1/2009 22:25,0.4,0.5,,,0,,,,,,"The Apache License 2.0 requires that attribution be included in a top-level NOTICE file. For Cassandra, and our ASF originated libraries, the standard ""This product includes software developed by The Apache Software Foundation (http://www.apache.org/)."" is sufficient, but attributions are missing for flexjson, clhm, and google-collect.

Patch to follow shortly.",,,,,,,,,,,,,,,,,,,,01/Sep/09 22:03;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-411-update-NOTICE-for-for-third-party-libs.txt;https://issues.apache.org/jira/secure/attachment/12418304/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-411-update-NOTICE-for-for-third-party-libs.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,22:39.7,,,no_permission,,,,,,,,,,,,19676,,,Tue Sep 01 22:25:34 UTC 2009,,,,,,0|i0fyrj:,91230,,,,,,,,,,,"01/Sep/09 21:22;jbellis;CLHM is removed in CASSANDRA-405, btw.",01/Sep/09 22:03;urandom;attached patch adds attributions for google-collect and flexjson,01/Sep/09 22:08;jbellis;+1,01/Sep/09 22:25;urandom;commited.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
intermittent OneCompactionTest failure,CASSANDRA-184,12425622,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,5/15/2009 20:55,3/12/2019 14:09,3/13/2019 22:24,5/20/2009 14:40,0.4,,,,0,,,,,,"[junit] junit.framework.AssertionFailedError                                              
    [junit]     at org.apache.cassandra.io.SSTable.delete(SSTable.java:223)                   
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:1454)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:896)     
    [junit]     at org.apache.cassandra.db.OneCompactionTest.testCompaction(OneCompactionTest.java:47)    
    [junit]     at org.apache.cassandra.db.OneCompactionTest.testCompaction2(OneCompactionTest.java:60)   
",,,,,,,,,,,,,,,,,,,,18/May/09 15:47;jbellis;184-0.3.patch;https://issues.apache.org/jira/secure/attachment/12408387/184-0.3.patch,18/May/09 15:48;jbellis;184-trunk.patch;https://issues.apache.org/jira/secure/attachment/12408388/184-trunk.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,51:46.4,,,no_permission,,,,,,,,,,,,19587,,,Thu May 21 13:10:36 UTC 2009,,,,,,0|i0fxdr:,91006,,,,,,,,,,,"18/May/09 15:47;jbellis;OneCompactionTest is failing occasionally because 500 keys per CFS is actually triggering anautomatic compaction (since test flush threshold is only 20) and we were doing a non-threadsafe doCompaction for convenience: the failure occurs when our manual compaction begins mid-run of an automatic one, and the automatic deletesthe original sstable file first.  Fix by (a) dropping the number of keys so that OneCompactionTest lives up to its name (more are tested in ""CompactionsTest"") and (b) making the compactions call threadsafe by refactoring to allow a threshold parameter to MCM.submit.",18/May/09 15:48;jbellis;patch for trunk that applies on top of the one for 0.3.  additional cleanup.,"18/May/09 18:51;junrao;The patch looks fine to me. 

The following comment in CFS.storeLocation() confuses me:
        /* it's ok if compaction gets submitted multiple times while one is already in process.
           worst that happens is, compactor will count the sstable files and decide there are
           not enough to bother with. */

With this patch, there can't be mutiple ongoing compactions, right?
","18/May/09 19:11;jbellis;only one compaction can be in progress at a time, but flushes and compactions can happen concurrently (different executors) so a thread flushing CF A can schedule A for a compaction (i.e. submit the op on the MCM) even if A is currently being compacted.  the comment is explaining that when the MCM gets to that redundant compact the impact will be minimal.",20/May/09 14:40;jbellis;guess we are good here.  committed.,"21/May/09 13:10;hudson;Integrated in Cassandra #83 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/83/])
    more cleanup of compaction code.
patch by jbellis; reviewed by Jun Rao for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NameSortTest.testNameSort100 fails,CASSANDRA-1044,12463547,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,johanoskarsson,johanoskarsson,5/3/2010 9:09,3/12/2019 14:09,3/13/2019 22:24,5/22/2010 17:18,,,,,0,,,,,,"The hudson build is failing due to this test failing: NameSortTest.testNameSort100
http://hudson.zones.apache.org/hudson/job/Cassandra/422/testReport/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,35:42.9,,,no_permission,,,,,,,,,,,,19971,,,Mon May 03 13:35:42 UTC 2010,,,,,,0|i0g2nr:,91861,,,,,,,,,,,"03/May/10 13:35;gdusbabek;I went ahead and committed a change to increase the timeout to 60s to see if that helps.  That test takes nowhere near 30s on my development machine though, so I don't know what hudson's problem is.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli doesn't handle non-string column names well,CASSANDRA-1701,12478959,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jancona,jancona,jancona,11/3/2010 5:21,3/12/2019 14:09,3/13/2019 22:24,11/3/2010 16:42,,,,,0,,,,,,"cassandra-cli has several bugs when using column names that aren't strings. Attached is a patch that updates CliTest to show the problems and fixes CliClient by properly converting non-string column and sub-column values passed to the GET, SET and COUNT commands.",,,,,,,,,,,,,,,,,,,,03/Nov/10 05:22;jancona;cassandra-cli-non-string-column-names.patch;https://issues.apache.org/jira/secure/attachment/12458714/cassandra-cli-non-string-column-names.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:01.4,,,no_permission,,,,,,,,,,,,20266,,,Wed Nov 03 16:42:01 UTC 2010,,,,,,0|i0g6uv:,92541,xedin,xedin,,,,,,,,,"03/Nov/10 14:58;xedin;This looks good to me, all .getBytes() replaced with columnNameAsByteArray methods which is how it should be + tests added.","03/Nov/10 16:42;jbellis;committed.  thanks, Jim!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LICENSE.txt should mention lib/licenses,CASSANDRA-371,12433348,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,8/18/2009 16:44,3/12/2019 14:09,3/13/2019 22:24,9/11/2009 17:09,0.4,0.5,,,0,,,,,,"Per the discussion taking place on general@ (http://www.mail-archive.com/general@incubator.apache.org/msg22205.html)

Patch to follow.",,,,,,,,,,,,,,,,,,,,18/Aug/09 16:45;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-371-top-level-reference-of-lib-licenses.txt;https://issues.apache.org/jira/secure/attachment/12416885/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-371-top-level-reference-of-lib-licenses.txt,08/Sep/09 19:39;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-371-monolithic-license-documentation.txt;https://issues.apache.org/jira/secure/attachment/12418952/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-371-monolithic-license-documentation.txt,08/Sep/09 19:39;urandom;ASF.LICENSE.NOT.GRANTED--v2-0002-CASSANDRA-371-extra-NOTICE.txt-attributions.txt;https://issues.apache.org/jira/secure/attachment/12418953/ASF.LICENSE.NOT.GRANTED--v2-0002-CASSANDRA-371-extra-NOTICE.txt-attributions.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,21:49.6,,,no_permission,,,,,,,,,,,,19656,,,Fri Sep 11 17:09:26 UTC 2009,,,,,,0|i0fyj3:,91192,,,,,,,,,,,18/Aug/09 17:03;urandom;See: http://www.mail-archive.com/general@incubator.apache.org/msg22214.html,"20/Aug/09 21:38;urandom;A fair amount of discussion took place during and after the vote for 0.4.0-beta1, and in a separate (but related) discussion that followed.

http://www.mail-archive.com/general@incubator.apache.org/msg22150.html
http://www.mail-archive.com/general@incubator.apache.org/msg22207.html

Consensus in these discussions was that all license information should be included *in* LICENSE.txt, and attributions for all software that requires it should be in NOTICE.txt.

This was reenforced by a link to an existing document, and incubator documentation was updated to reflect this opinion.

http://www.apache.org/dev/release.html#distributing-code-under-several-licenses
http://incubator.apache.org/guides/releasemanagement.html#best-practice-license",10/Sep/09 16:21;jbellis;+1,11/Sep/09 17:09;urandom;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"get_slice calls do not close files when finished resulting in ""too many open files"" exceptions and rendering C unusable",CASSANDRA-1178,12466596,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mdennis,mdennis,mdennis,6/9/2010 19:47,3/12/2019 14:09,3/13/2019 22:24,6/11/2010 0:19,0.7 beta 1,,,,0,,,,,,"insert ~100K rows.  Read them back in a loop.  Notice ""too many open files"" exceptions in log.  SSTableSliceIterator is never closing the files.

",,,,,,,,,,,,,,,,,,,,09/Jun/10 19:49;mdennis;0001-trunk-1178.patch;https://issues.apache.org/jira/secure/attachment/12446719/0001-trunk-1178.patch,10/Jun/10 16:34;mdennis;0002-trunk-1178.patch;https://issues.apache.org/jira/secure/attachment/12446771/0002-trunk-1178.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,11:58.8,,,no_permission,,,,,,,,,,,,20022,,,Fri Jun 11 12:45:40 UTC 2010,,,,,,0|i0g3h3:,91993,,,,,,,,,,,"09/Jun/10 23:11;mortazavi;Out of curiosity . . . What is the point, in this patch, in switching import statements around . . . Is there a policy to re-order them? . . . To the extent I can tell, the order of import statements is consistent with what one sees in the other files?

The rest of the fix seems OK to me as long as the ""close"" method in the IColumnIterator interface to SSTableSliceIterator  is used properly to clean up resources . . . (although -- on a different plane -- objects that know, themselves, that they have nothing more to do are more friendly objects to use and harder to design) . . . 




","09/Jun/10 23:59;mdennis;intellij often automatically reorders imports and removes unused ones (they call it ""optimizing imports"").  I just let it do it's thing in this regard (to be honest I've just gotten in the habit of ignoring the import section).

At least for get_slice, close is properly called (as evident by the patch fixing the problem).   It may be the case that for other calls close is not correctly called, I didn't specifically look for such cases but 1) I haven't noticed a problem making other calls and 2) I didn't notice anything in the code while debugging this issue.

In general I agree that self contained objects are better but that doesn't necessarily fit in this case as the open file is really a system resource.  Given the way Java does GC, such resources should be explicitly released.

I discussed this particular case with jbellis and the thought for the original patch that introduced this bug was that some callers would want to reuse an existing file.  In such a case, they would be responsible for closing the file but that meant that SSTableSliceIterator shouldn't close it out from under them.","10/Jun/10 03:52;mortazavi;Sounds reasonable . . . 

It seems to me that he idea is that 

(1) If you pass a non-null file object reference to the constructor, you are the best judge of when to close it. 

(2) If you pass a null, instead, you will have to ask SSTableSliceIterator to clean up after you're done by calling ""close"" on it. 

This is then the semantics of this class when it comes to the file variable. 

I think it may be useful to include this contractual/semantic fact in the javadoc for this class. 

(Side note: It would probably be best not to allow IDE's to change the order of imports from the one that's common everywhere else unless there is a policy by this project to make such reordering. It is better to keep things consistent.)

",10/Jun/10 16:34;mdennis;0002-trunk-1178.patch has suggested JavaDoc changes,11/Jun/10 00:19;jbellis;committed,"11/Jun/10 12:45;hudson;Integrated in Cassandra #462 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/462/])
    fix FD leak.  patch by mdennis; reviewed by jbellis for CASSANDRA-1178
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup needs to remove secondary index entries,CASSANDRA-1916,12494258,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,12/29/2010 17:39,3/12/2019 14:09,3/13/2019 22:24,12/30/2010 16:35,0.7.1,,Feature/2i Index,,0,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,30/Dec/10 14:00;tjake;0003-add-unit-test.txt;https://issues.apache.org/jira/secure/attachment/12467167/0003-add-unit-test.txt,29/Dec/10 17:39;jbellis;ASF.LICENSE.NOT.GRANTED--0001-merge-doCleanup-doAntiCompaction.txt;https://issues.apache.org/jira/secure/attachment/12467123/ASF.LICENSE.NOT.GRANTED--0001-merge-doCleanup-doAntiCompaction.txt,29/Dec/10 17:39;jbellis;ASF.LICENSE.NOT.GRANTED--0002-replace-AntiCompactionIterator-w-per-sstable-iteration.txt;https://issues.apache.org/jira/secure/attachment/12467124/ASF.LICENSE.NOT.GRANTED--0002-replace-AntiCompactionIterator-w-per-sstable-iteration.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,33:05.2,,,no_permission,,,,,,,,,,,,19355,,,Thu Dec 30 19:37:30 UTC 2010,,,,,,0|i0g87b:,92759,tjake,tjake,,,,,,,,,29/Dec/10 17:40;jbellis;patches are against trunk,"29/Dec/10 18:33;tjake;Looks like the secondary index entried aren't actually removed, only tombstoned.  Are you planning them to be removed on next minor compaction?

","29/Dec/10 19:08;jbellis;bq. Looks like the secondary index entried aren't actually removed, only tombstoned

right.  sstables being immutable, and all that.

bq. Are you planning them to be removed on next minor compaction

yes.",30/Dec/10 14:00;tjake;Added test case for this change. It verifies the tombstones are added on cleanup,"30/Dec/10 14:11;jbellis;simplified getRing in test to ""return StorageService.instance.getTokenMetadata();"" :)

committed w/ addition of post-cleanup flush of 2ary indexes","30/Dec/10 19:37;hudson;Integrated in Cassandra-0.7 #137 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/137/])
    Fix CompactionManager regression from CASSANDRA-1916 and add a better
test and more docs to prevent in the future.
Patch by jbellis, reviewed by brandonwilliams for CASSANDRA-1922
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool move throws Assertion Error,CASSANDRA-1732,12479762,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,arya,arya,11/12/2010 0:34,3/12/2019 14:09,3/13/2019 22:24,11/19/2010 14:34,0.7.0 rc 1,,,,0,,,,,,"Started from a clean slate 3 node cluster. I first started 1 node and bootstrapped the second and third node into the cluster. I created some Keyspaces and inserted some test data, I ended up with this ring:

[agoudarzi@cas-test1 ~]$ nodetool --host localhost ring
Address         Status State   Load            Token                                       
                                       142685436305748685139980028665762955655    
10.50.26.133    Up     Normal  160.51 KB       57614844575514069274136376807820902791      
10.50.26.134    Up     Normal  160.51 KB       100150140440631377207058202736791929223     
10.50.26.132    Up     Normal  165.48 KB       142685436305748685139980028665762955655     

Now I wanted to test manual moving nodes to balanced tokens:
stage1:agoudarzi:~:$ python test.py 3
56713727820156410577229101238628035242
113427455640312821154458202477256070484
170141183460469231731687303715884105727

So I did nodetool move on 10.50.26.132:
[agoudarzi@cas-test1 ~]$ nodetool --host localhost move 56713727820156410577229101238628035242

All went fine. 
[agoudarzi@cas-test1 ~]$ nodetool --host localhost ring
Address         Status State   Load            Token                                       
                                       100150140440631377207058202736791929223    
10.50.26.132    Up     Normal  603.03 KB       56713727820156410577229101238628035242      
10.50.26.133    Up     Normal  15.18 MB        57614844575514069274136376807820902791      
10.50.26.134    Up     Normal  15.19 MB        100150140440631377207058202736791929223     

Now I wanted to move the second node 10.50.26.133:

[agoudarzi@cas-test2 ~]$ nodetool --host localhost move 113427455640312821154458202477256070484
Exception in thread ""main"" java.lang.AssertionError
	at org.apache.cassandra.service.StorageService.getLocalToken(StorageService.java:1128)
	at org.apache.cassandra.service.StorageService.startLeaving(StorageService.java:1527)
	at org.apache.cassandra.service.StorageService.move(StorageService.java:1666)
	at org.apache.cassandra.service.StorageService.move(StorageService.java:1641)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1449)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1284)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1382)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

 I am attacking the logs for my 3 nodes. For your reference I refer to these IPs as these nodes:

node1: 10.50.26.132
node2: 10.50.26.133
node3: 10.50.26.134

I have seen similar exception being thrown in CASSANDRA-1670.

Please investigate.

-Arya
","CentOS 5.4
Cassandra Trunk SVN Revision #1034158",,,,,,,,,,,,,,,,,,,19/Nov/10 01:52;tjake;1732_v1.txt;https://issues.apache.org/jira/secure/attachment/12459963/1732_v1.txt,12/Nov/10 00:36;arya;node1.log;https://issues.apache.org/jira/secure/attachment/12459409/node1.log,12/Nov/10 00:36;arya;node2.log;https://issues.apache.org/jira/secure/attachment/12459410/node2.log,12/Nov/10 00:36;arya;node3.log;https://issues.apache.org/jira/secure/attachment/12459411/node3.log,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,54:51.4,,,no_permission,,,,,,,,,,,,20280,,,Fri Nov 19 15:12:34 UTC 2010,,,,,,0|i0g71r:,92572,jbellis,jbellis,,,,,,,,,14/Nov/10 18:54;jbellis;Does this work on 0.6.8?,"19/Nov/10 01:52;tjake;Attached fix.

in 0.7 bootstrapping a clean node takes a slightly different code path due to the fact that there are no non-system CFs defined.  This new code wasn't setting the nodes token.

This seems to be the same issue as CASSANDRA-1738","19/Nov/10 14:34;jbellis;committed, combining tokenMetadata_.updateNormalToken + SystemTable.updateToken into SS.setToken call","19/Nov/10 15:12;hudson;Integrated in Cassandra-0.7 #16 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/16/])
    fix for bootstrap when no non-system tables are defined
patch by tjake; reviewed by jbellis for CASSANDRA-1732
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug removing supercolumn,CASSANDRA-84,12422909,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,4/15/2009 17:14,3/12/2019 14:08,3/13/2019 22:24,4/20/2009 14:26,0.3,,,,0,,,,,,The code to create a SuperColumn tombstone has a bug.,,,,,,,,,,,,,,,,,,,,15/Apr/09 19:09;jbellis;0001-make-remove-test-code-use-the-same-api-that-the-thri.patch;https://issues.apache.org/jira/secure/attachment/12405554/0001-make-remove-test-code-use-the-same-api-that-the-thri.patch,15/Apr/09 19:10;jbellis;0002-generate-supercolumn-tombstone-when-a-2-tuple-delete.patch;https://issues.apache.org/jira/secure/attachment/12405555/0002-generate-supercolumn-tombstone-when-a-2-tuple-delete.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,51:33.0,,,no_permission,,,,,,,,,,,,19541,,,Mon Apr 20 14:26:39 UTC 2009,,,,,,0|i0fwrz:,90908,,,,,,,,,,,"15/Apr/09 23:51;sandeep_tata;Looks good to me.

Nits:
1. I'd rather use  !columnFamily.isSuper() instead of DatabaseDescriptor.getColumnFamilyType(cfName).equals(""Standard"")
2. Messages from testng assertions seem a little friendlier than plain java assertions.

","16/Apr/09 01:53;jbellis;1. Good point.  I have a patch already changing this globally.

2. Noted, but I'm not editing that code here.","20/Apr/09 14:26;jbellis;committed (a while ago, forgot to update jira)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix contrib/word_count build in 0.7,CASSANDRA-1030,12463154,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jeromatron,jbellis,jbellis,4/27/2010 21:38,3/12/2019 14:08,3/13/2019 22:24,5/6/2010 17:23,0.7 beta 1,,,,0,,,,,,CASSANDRA-44 broke word_count setup (see CASSANDRA-1002) so CASSANDRA-992 and CASSANDRA-1029 can't easily be applied to 0.7 as-is.  This ticket will port those to 0.7 and add schema setup.,,,,,,,,,,,,,,,,,,,,04/May/10 20:58;jeromatron;1030.patch;https://issues.apache.org/jira/secure/attachment/12443632/1030.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,49:49.6,,,no_permission,,,,,,,,,,,,19965,,,Thu May 06 17:23:58 UTC 2010,,,,,,0|i0g2kn:,91847,,,,,,,,,,,30/Apr/10 17:49;jeromatron;Converted from using the fat client to using thrift rpc for the WordCountSetup.  Also fixed a few things that were amiss in the current hadoop stuff with @stuhood. Needs a patch for cassandra-1022 to be able to login on the wordcount itself.,03/May/10 13:50;jbellis;why do we still need the .yaml?,03/May/10 13:55;gdusbabek;I haven't reviewed the patch yet.,"03/May/10 14:30;johanoskarsson;Had a quick look at but unfortunately the word count contrib does not compile for me with the patch applied. 
The fix is trivial, but I can't seem to figure out the reason for changing the input format from SortedMap to a LinkedHashMap in the first place?",03/May/10 15:19;jeromatron;@johan: the reason why we changed from SortedMap to LinkedHashMap was that the TreeMap required a comparable object to instantiate (line 234 of ColumnFamilyRecordReader).  Stu suggested just changing it to the LHM so that it didn't require the comparable object.  I'm wondering what it needed to compile - it had compiled fine for me.,"03/May/10 15:19;jeromatron;@jonathan: I removed the need for the yaml file for the WordCountSetup class and tried to run it without it for the WordCount but it errored out.  I wasn't sure if that was just the way it had to be or if I could get rid of that.  I think this line (line 91) in WordCount needs to be updated to grab the dynamic mapping:

 this.columnName = context.getConfiguration().get(CONF_COLUMN_NAME);

but I'll take another look.",03/May/10 15:29;jeromatron;I see the compilation problem after updating - it looks like the patch for cassandra-1022 has been committed.  That changes the args required for a thrift rpc client login.  I'll update the patch and see about getting rid of the dependency on the .yaml at the same time.,"03/May/10 15:45;jbellis;""context"" is the hadoop job context, nothing to do w/ our yaml.","03/May/10 15:55;jeromatron;@johan: sorry - not because it requires a comparator, but that LinkedHashMap maintains the insertion order.  That was the reason.

@jonathan: right - sorry, yeah.  Was going to look at it and that was the first thing I skimmed that looked promising, but you're right.  Looking at it though.",03/May/10 17:15;jeromatron;@jonathan - I asked Johan about the error when the cassandra.yaml file is not present.  He said it looked like it's required because it gets the host address from the seed that's in the configuration file.  That's in the org.cassandra.hadoop... code - ColumnFamilyInputFormat line 196 for example.  Would you rather the host be configured by the client itself and remove the dependency on cassandra.yaml?,"03/May/10 18:48;jeromatron;Updated the patch with compilation fix from cassandra-1022 changes. Also updated ColumnFamilyInputFormat to use LinkedHashMap in addition to the other places where SortedMap had been used.

Waiting to find out if it would be good to have the client provide the host address or if we should still depend on cassandra.yaml for the seed to get to that.","03/May/10 21:17;jbellis;It looks like the main reason we use DatabaseDescriptor is to get comparator information so we can throw SortedMaps around.

Created CASSANDRA-1047 to clean this up.",04/May/10 20:58;jeromatron;Updated to make sure everything works in the wordcount - dynamically finds the comparator based on describing the keyspace itself.,"06/May/10 09:50;johanoskarsson;The patch looks good, but I get unexpected output. In the setup code we insert just one ""word1"" in the text1 column. When the word count runs it finds two instances of ""word1"" in the text1 column. This could be due to other changes in trunk, but worth verifying it is not this patch.","06/May/10 16:38;johanoskarsson;The problem with the output is most likely this one CASSANDRA-1042, so will commit this patch as is",06/May/10 17:02;jeromatron;yeah - it looks like the output is the same on 0.6.0's word count as the trunk cassandra-1030 patch output.  so it would seem to be something external that is causing odd output - hopefully cassandra-1042 will fix that.,06/May/10 17:23;johanoskarsson;Committed to trunk. Thanks Jeremy!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Binary Memtable does not invalidate cache,CASSANDRA-761,12455240,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,lenn0x,lenn0x,2/3/2010 23:00,3/12/2019 14:08,3/13/2019 22:24,3/10/2010 17:42,0.6,,,,0,,,,,,"If you have RowCache turned on for your CF, and you do a BMT import, rows are not invalidated in cache until you restart the node.",,,,,,,,,,,,,,,,,,,,23/Feb/10 21:39;jbellis;761-v2.txt;https://issues.apache.org/jira/secure/attachment/12436765/761-v2.txt,03/Feb/10 23:17;jbellis;761.txt;https://issues.apache.org/jira/secure/attachment/12434726/761.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,44:00.4,,,no_permission,,,,,,,,,,,,19853,,,Wed Mar 10 17:42:41 UTC 2010,,,,,,0|i0g0wv:,91578,,,,,,,,,,,"04/Feb/10 06:44;jbellis;... cache invalidation at BMT insertion time as shown in this patch actually won't work; a read could still pull the old version into the cache again, since BMT contents are not ""live"" to readers.

you'll need to add ""nodetool invalidate"" instead.","13/Feb/10 22:34;tzz;Could this be related to what I saw in CASSANDRA-764, where the row cache needs to be flushed before a bitmask query will work?  Seems like a similar issue and my fix is similar (except my fix takes the shotgun approach of invalidating the whole cache).",21/Feb/10 12:18;jbellis;should probably get at least the JMX stub in 0.6.0; then the nodeprobe update can be 0.6.1 if needed,23/Feb/10 21:39;jbellis;jmx invalidator attached,25/Feb/10 17:11;lenn0x;+1,01/Mar/10 21:31;jbellis;rebased & committed to 0.6 and trunk,"10/Mar/10 17:42;jbellis;Resolving as Fixed.  If someone wants to add this to nodetool, let's do that in another issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DeletingReferences not created for existing sstables on startup,CASSANDRA-772,12455525,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,2/6/2010 6:30,3/12/2019 14:08,3/13/2019 22:24,2/6/2010 18:22,0.6,,,,0,,,,,,Trunk does not initialize SSTableTracker properly.,,,,,,,,,,,,,,,,,,,,06/Feb/10 06:32;stuhood;772-remove-codepath.patch;https://issues.apache.org/jira/secure/attachment/12435066/772-remove-codepath.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,22:17.9,,,no_permission,,,,,,,,,,,,19857,,,Wed Feb 17 17:54:42 UTC 2010,,,,,,0|i0g0zb:,91589,,,,,,,,,,,06/Feb/10 06:32;stuhood;Removes the ability to add tables using the constructor.,"06/Feb/10 18:22;jbellis;+1, committed.  good catch!","17/Feb/10 17:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted handoff rows never get deleted,CASSANDRA-34,12421771,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,junrao,jbellis,jbellis,4/1/2009 13:50,3/12/2019 14:08,3/13/2019 22:24,5/5/2009 17:03,0.3,,,,0,,,,,,"from the list: ""after the hints are delivered, the hinted keys are deleted from the hinted CF only, but not from the application CF.""

Prashant verified that this is a bug that can't be fixed until deletes are fully working.

Note: when we fix this, see if we can do so w/o compromising the immediate-GC of the hinted CF keys.",,,,,,,,,,,,,,,,,,,,03/May/09 03:28;jbellis;0001-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12407102/0001-cleanup.patch,03/May/09 03:29;jbellis;0002-fix-HHM.patch;https://issues.apache.org/jira/secure/attachment/12407103/0002-fix-HHM.patch,05/May/09 15:30;jbellis;0003-tombstones-take-priority-over-non-tombstones-w-the.patch;https://issues.apache.org/jira/secure/attachment/12407253/0003-tombstones-take-priority-over-non-tombstones-w-the.patch,01/May/09 21:45;junrao;issue34.patch_v1;https://issues.apache.org/jira/secure/attachment/12407039/issue34.patch_v1,01/May/09 22:53;junrao;issue34.patch_v2;https://issues.apache.org/jira/secure/attachment/12407045/issue34.patch_v2,,,,,,,,,5,,,,,,,,,,,,,,,,,,,14:09.5,,,no_permission,,,,,,,,,,,,19522,,,Thu May 07 13:35:07 UTC 2009,,,,,,0|i0fwhb:,90860,,,,,,,,,,,01/May/09 17:14;junrao;Sending out hinted data needs correct RowMutation support.,"01/May/09 21:45;junrao;Attach a fix. About the patch.

1. Make sendMessage blocking.
2. Delete the rows in application CF after hinted data is sent. To do that, we need to collect the largest timestamp among columns in a CF and then delete the CF with the largest timestamp.
3. When a column is in a deleted CF and their timestamps are the same, the rule is that the deleted CF wins. This rule is needed for step 2 above. Change CFStore.removeDeleted according to this rule.",01/May/09 22:53;junrao;Patch v2. Added some description on hinted data gets delivered.,"02/May/09 19:13;jbellis;any idea what the purpose of this code in runHints is?  why flush if nothing changed?

            	if(hintedColumnFamily == null)
            	{
                    columnFamilyStore_.forceFlush();
            		return;
            	}
","02/May/09 19:34;jbellis;also, now that we have range queries, it seems that a normal CF would be a better fit for this than a single super CF with keys as supercolumns.  i guess that is a separate issue.","03/May/09 03:27;jbellis;committed the sendMessage fix.  I've reworked the rest substantially in two parts.

01 is just refactoring / cleanup.

02 includes your fix to deleteKey, and also:
 A make hint generation include a real timestamp so we can do meaningful deletes
 B call removeDeleted on the data we read locally to purge tombstones
 C because of (B) any supercolumn w/o subcolumns simply won't exist so we know we can skip re-deleting the endpoint data.  so deleteKey becomes deleteHintedData.
 D because deleted data is not immediately purged, increased the scheduled interval fro 20min to 60 to reduce the load of scanning the hint CF.

(for another ticket: since all this is purely local data and not subject to read repair we should find a way to GC it immediately post-delete.)

how does this look to you?","03/May/09 12:31;hudson;Integrated in Cassandra #57 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/57/])
    make sendMessage only return true after ack by recipient.
patch by Jun Rao; reviewed by jbellis for 
",03/May/09 14:11;jbellis;created CASSANDRA-128 for improvments beyond the scope of 0.3,"05/May/09 00:00;junrao;Looked at the new patch. Here are some comments.

1. Move the comments above sendMessage to the beginning of class.

2. There is compilation error in MinorCompactionManager because of the removed HintedHandOff class.

3. Your new code deletes a column using the same timestamp as the column to be deleted. This is a more fundamental question. If a column has a non-delete entry and a deleted entry with the same timestamp, which one wins? I don't think that we can rely on the ordering of the insertion/deletion. This is because the insertion and the deletion can end up in different SSTables. During compaction, FileStruct is sorted only by keys. Therefore, columns from different SSTables with the same key can come in arbitrary order. One solution is to modify CF.addColumn such that a deletion always wins when the column timestamp is the same. Not sure if there is any other implications, though.
",05/May/09 00:07;junrao;4. It's probably worthwhile to make intervalInMins_ in HHM configurable.,"05/May/09 15:30;jbellis;patch to make tombstones have higher precedence than non- for same timestamp in Column (and SuperColumn, to be consistent w/ C and CF)","05/May/09 15:31;jbellis;(incorporated changes for comments (1) and (2) into my patchset, not bothering resubmitting those unless you really want 'em)",05/May/09 15:32;jbellis;noted comment (4) on CASSANDRA-128,"05/May/09 16:38;junrao;Comments for the new patch.
1. In CFStore.removeDeleted(), we should add comments to explain how we resolve the conflicts among CF, SC, and C when the timestamps are the same. As time goes, we are likely to forget those decisions that we have made.

Other than that, the patch looks fine to me.
",05/May/09 17:03;jbellis;done and committed.,"07/May/09 13:35;hudson;Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connections are not reset if a node is restarted but we had not marked it down,CASSANDRA-2292,12500808,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,3/8/2011 19:49,3/12/2019 14:08,3/13/2019 22:24,3/8/2011 20:40,0.7.4,,,,0,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,08/Mar/11 20:32;slebresne;0001-Reset-connections-when-a-node-is-restarted-but-we-di-v2.patch;https://issues.apache.org/jira/secure/attachment/12473041/0001-Reset-connections-when-a-node-is-restarted-but-we-di-v2.patch,08/Mar/11 19:50;slebresne;0001-Reset-connections-when-a-node-is-restarted-but-we-di.patch;https://issues.apache.org/jira/secure/attachment/12473033/0001-Reset-connections-when-a-node-is-restarted-but-we-di.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,07:43.8,,,no_permission,,,,,,,,,,,,20545,,,Tue Mar 08 21:08:11 UTC 2011,,,,,,0|i0gaif:,93133,brandon.williams,brandon.williams,,,,,,,,,08/Mar/11 20:07;jbellis;any reason not to put the convict in markAlive?,08/Mar/11 20:32;slebresne;Attach v2 that call onDead for all subscriber instead of simply convict().,08/Mar/11 20:40;brandon.williams;Committed with comments clarified.,"08/Mar/11 21:08;hudson;Integrated in Cassandra-0.7 #358 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/358/])
    Notify subscribers that the node was dead if it restarts before we mark
it down.
Patch slebresne, reviewed by brandonwilliams for CASSANDRA-2292
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpdateKeyspace does not modify strategy or strategy_options (until restart),CASSANDRA-1762,12480464,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,11/19/2010 22:54,3/12/2019 14:08,3/13/2019 22:24,11/23/2010 19:29,0.7.0 rc 2,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,23/Nov/10 17:25;jbellis;1762.txt;https://issues.apache.org/jira/secure/attachment/12460282/1762.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,23:42.0,,,no_permission,,,,,,,,,,,,20299,,,Sat Dec 11 07:35:21 UTC 2010,,,,,,0|i0g78f:,92602,gdusbabek,gdusbabek,,,,,,,,,23/Nov/10 17:25;jbellis;Most of this patch is the encapsulation of ARS.replicationStrategy; the actual fix is in UpdateKeyspace and in ARS.createReplicationStrategy,23/Nov/10 19:23;gdusbabek;+1.,23/Nov/10 19:29;jbellis;committed,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compacted SSTables not properly removed,CASSANDRA-1544,12475051,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,9/24/2010 15:58,3/12/2019 14:08,3/13/2019 22:24,9/26/2010 22:27,0.7 beta 2,,,,0,,,,,,It looks like SSTableDeletingReference isn't doing its job... SSTable.conditionalDelete is never being called for sstables that have been marked compacted.,,,,,,,,,,,,,,,,,,,,25/Sep/10 23:42;stuhood;1544-add-compacted-component.diff;https://issues.apache.org/jira/secure/attachment/12455581/1544-add-compacted-component.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:06.3,,,no_permission,,,,,,,,,,,,20191,,,Mon Sep 27 13:10:07 UTC 2010,,,,,,0|i0g5vr:,92383,jbellis,jbellis,,,,,,,,,25/Sep/10 23:42;stuhood;Oopsie: the in-memory list of components didn't match the on-disk list.,26/Sep/10 22:27;jbellis;committed,"27/Sep/10 13:10;hudson;Integrated in Cassandra #548 (See [https://hudson.apache.org/hudson/job/Cassandra/548/])
    add compaction marker to in-memory list of components.  patch by Stu Hood; reviewed by jbellis for CASSANDRA-1544
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction regression doesn't delete tombstones in trunk,CASSANDRA-500,12438588,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/20/2009 16:21,3/12/2019 14:08,3/13/2019 22:24,10/20/2009 18:41,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,20/Oct/09 16:25;jbellis;500.patch;https://issues.apache.org/jira/secure/attachment/12422698/500.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,32:30.0,,,no_permission,,,,,,,,,,,,19724,,,Wed Oct 21 12:34:23 UTC 2009,,,,,,0|i0fzbb:,91319,,,,,,,,,,,20/Oct/09 18:32;urandom;looks good. +1,20/Oct/09 18:41;jbellis;committed,"21/Oct/09 12:34;hudson;Integrated in Cassandra #234 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/234/])
    add test catching regression; call removeDeleted during compaction.
patch by jbellis; reviewed by eevans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dropping column families doesn't clean up secondary indexes,CASSANDRA-1406,12471923,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,gdusbabek,gdusbabek,8/18/2010 16:58,3/12/2019 14:08,3/13/2019 22:24,8/27/2010 20:20,0.7 beta 2,,Feature/2i Index,,0,,,,,,,,,,,,,,,,,,,,,,,,,,27/Aug/10 18:22;gdusbabek;0003-extract-unload-out-of-drop.patch;https://issues.apache.org/jira/secure/attachment/12453257/0003-extract-unload-out-of-drop.patch,26/Aug/10 21:07;jbellis;ASF.LICENSE.NOT.GRANTED--0001-replace-CF-graveyard-with-CFS.removeAllSSTables-which-.txt;https://issues.apache.org/jira/secure/attachment/12453167/ASF.LICENSE.NOT.GRANTED--0001-replace-CF-graveyard-with-CFS.removeAllSSTables-which-.txt,26/Aug/10 21:07;jbellis;ASF.LICENSE.NOT.GRANTED--0002-include-secondary-indexes-in-CF-and-KS-renaming.txt;https://issues.apache.org/jira/secure/attachment/12453168/ASF.LICENSE.NOT.GRANTED--0002-include-secondary-indexes-in-CF-and-KS-renaming.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,41:05.5,,,no_permission,,,,,,,,,,,,20121,,,Fri Aug 27 20:20:55 UTC 2010,,,,,,0|i0g4v3:,92218,gdusbabek,gdusbabek,,,,,,,,,26/Aug/10 17:41;jbellis;patch replaces CF graveyard with CFS.removeAllSSTables (which is recursive to handle 2ary index files),"26/Aug/10 18:39;gdusbabek;putting the deletion in Table.dropCF() is risky wrt KS and CF renames.  It won't hurt anything right now because the delete tries to remove the old files.  If that side-effect ever went away we'd be deleting valid files.  As it stands with this patch we'd be trying to do cleanup of sstables that have had their files moved out from underneath them, which probably isn't the best thing to try.

Is this a good ticket to lump in that RenameKeyspace and RenameColumnFamily don't address secondary indices, or should that be a new one?","26/Aug/10 21:01;jbellis;otoh, requiring the caller who calls dropCf to also call other methods to finish the job is poor encapsulation, and implementing rename as drop + add is itself an implementation detail.  neither approach is completely satisfactory imo.

02 adds support for 2ary indexes in the rename methods.",26/Aug/10 21:07;jbellis;(tweaked DefsTest to not require renaming to leave an empty directory behind for the old name),27/Aug/10 18:22;gdusbabek;dropCf was intended to unload a CFS from a table instance as indicated in its comment.  0003 cleans the interface up so that renaming acts on files like drop does.,"27/Aug/10 18:23;gdusbabek;+1, but I'd like 0003 to be included.",27/Aug/10 20:20;jbellis;committed w/ 03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fixed remove is not exposed in the thrift interface,CASSANDRA-83,12422899,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,4/15/2009 16:20,3/12/2019 14:07,3/13/2019 22:24,4/16/2009 16:24,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,15/Apr/09 16:29;jbellis;83.patch;https://issues.apache.org/jira/secure/attachment/12405539/83.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,15:10.4,,,no_permission,,,,,,,,,,,,19540,,,Thu Apr 16 16:24:07 UTC 2009,,,,,,0|i0fwrr:,90907,,,,,,,,,,,"15/Apr/09 16:29;jbellis;block_for is binary in nature: 0 = do not block (return value is meaningless), > 0 = block for majority (return value is success)","16/Apr/09 16:15;junrao;Shouldn't block_for in the remove thrift api be bool instead of int? Other than that, the patch works fine.","16/Apr/09 16:18;jbellis;right -- I want it to be an int eventually though.  (1, majority, and all are values that all make sense there.)

guess you are right that it should be a bool until that's actually implemented though.",16/Apr/09 16:24;jbellis;committed w/ suggested change to `bool block` instead of `int block_for`,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LongType should be network-endian,CASSANDRA-384,12433527,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,eweaver,eweaver,8/20/2009 3:00,3/12/2019 14:07,3/13/2019 22:24,8/20/2009 20:38,0.4,,,,0,,,,,,that's all,,,,,,,,,,,,,,,,,,,,20/Aug/09 03:00;eweaver;CASSANDRA-384.diff;https://issues.apache.org/jira/secure/attachment/12417091/CASSANDRA-384.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,13:30.7,,,no_permission,,,,,,,,,,,,19663,,,Fri Aug 21 12:34:39 UTC 2009,,,,,,0|i0fylr:,91204,,,,,,,,,,,"20/Aug/09 03:13;euphoria;Looks like that's the JVM's natural endianness anyway.

+1","20/Aug/09 03:35;jbellis;does this pass system tests?

if it does we need a better system test, b/c test_server.py does a bunch of int64s in little endian :)","20/Aug/09 03:37;eweaver;dunno...i've never run them

i only rely on my gem tests right now","20/Aug/09 04:07;euphoria;It passes unit tests and system tests, or I wouldn't have +1'd :)
I can't necessarily vouch for the tests.",20/Aug/09 20:38;jbellis;committed w/ new unit test to catch endian problems and switching test_server.py to big endian,"21/Aug/09 12:34;hudson;Integrated in Cassandra #174 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/174/])
    change LongType to read longs in big endian order to be consistent with network order and the UUID types.
patch by jbellis and Evan Weaver; reviewed by Michael Greene for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyOutputFormat should use client API objects,CASSANDRA-1315,12470089,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,7/24/2010 18:20,3/12/2019 14:07,3/13/2019 22:24,8/25/2010 20:53,0.7 beta 2,,,,0,,,,,,"ColumnFamilyOutputFormat currently takes IColumns as its input, meaning that users need to understand Cassandra's internals reasonably well in order to use it, and need to hardcode things like the comparator type and clock type into their MapReduce jobs.

Instead, CFOutputFormat should take either Thrift or Avro objects, which are familiar interfaces for users.",,,,,,,,,,,CASSANDRA-1368,CASSANDRA-1342,,,,,,,,24/Aug/10 19:46;stuhood;0001-Use-Avro-objects-as-input-to-CFOutputFormat.patch;https://issues.apache.org/jira/secure/attachment/12452959/0001-Use-Avro-objects-as-input-to-CFOutputFormat.patch,24/Aug/10 19:46;stuhood;0002-Allow-multiple-mutations-per-key-to-arrive-during-in.patch;https://issues.apache.org/jira/secure/attachment/12452960/0002-Allow-multiple-mutations-per-key-to-arrive-during-in.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,08:43.0,,,no_permission,,,,,,,,,,,,20076,,,Wed Aug 25 20:53:11 UTC 2010,,,,,,0|i0g4b3:,92128,jbellis,jbellis,,,,,,,,,"26/Jul/10 19:49;stuhood;I've got a patch for this, but it needs more testing... should get it on here before midweek.","05/Aug/10 23:16;stuhood;Adding 1322 as a dependency, since both change approximately the same code.",05/Aug/10 23:17;stuhood;Ports the OutputFormat to Avro objects.,"08/Aug/10 07:07;stuhood;Add 0002 to allow mutations for keys to arrive during separate write calls, rather than clobbering existing keys.","12/Aug/10 19:08;jbellis;committed 0001

looks like 0002 brace placement is wrong, seems that the mutationsByKey.put(key, cfMutation) should happen if cfMutation != null as well.  similarly for the step below that.","12/Aug/10 19:24;jbellis;reverted commit of 0001 after looking at CASSANDRA-1368 more.

I'm less and less convinced that we're going to move go Avro as our ""main"" interface, and unless we are, we shouldn't be adding public dependencies on it.

I don't buy the argument that ""Hadoop people already know Avro"" because there's basically nothing here that's a standard Hadoop Avro class, and using a Thrift StreamingMutation class would be much the same as an Avro one.","13/Aug/10 16:08;stuhood;> and using a Thrift StreamingMutation class would be much the same as an Avro one.
In order to use Thrift, you would need to generate code for your dynamic language, and then distribute it to all of the nodes in your Hadoop cluster: either as a library that you update for each Cassandra version, or as a JAR'd script dependency. Not the end of the world, I suppose, but more difficult then distributing only the protocol file.

Having worked with Avro on a few tickets now, I'm willing to get behind it 100% as a replacement for Thrift.","13/Aug/10 22:14;jbellis;bq. In order to use Thrift, you would need to generate code for your dynamic language, and then distribute it to all of the nodes in your Hadoop cluster: either as a library that you update for each Cassandra version, or as a JAR'd script dependency. Not the end of the world, I suppose, but more difficult then distributing only the protocol file. 

How is that different from having to distribute the Avro library for whatever streaming processor language you are using, other than the codegen step?","13/Aug/10 22:23;stuhood;> other than the codegen step?
Just the codegen step. If you don't have root access to your cluster for instance, you would need to JAR the Thrift generated code and then use a relative import to pull it in.

If the real question we're debating is whether we are still considering switching Cassandra 100% to Avro before 1.0, then we should probably discuss that elsewhere. When I began this ticket, I was under the impression that that was a sure thing.","14/Aug/10 12:48;hudson;Integrated in Cassandra #514 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/514/])
    use Avro objects in ColumnFamilyOutputFormat.  patch by Stu Hood; reviewed by jbellis for CASSANDRA-1315
","16/Aug/10 20:20;jbellis;bq. if the real question we're debating is whether we are still considering switching Cassandra 100% to Avro before 1.0, then we should probably discuss that elsewhere. When I began this ticket, I was under the impression that that was a sure thing.

This has never been a sure thing, and we're way behind schedule on what we had planned.

Offering both Thrift and Avro here would be fine, that's similar to what our migration path to Avro was planned to be like anyway.  But even if we were going to switch 100% to Avro we're several releases away from where that should be the only interface.","23/Aug/10 15:43;stuhood;> Offering both Thrift and Avro here would be fine
Hm, I think I could get behind that... aside from proliferation concerns. We'd be signing up for a ThriftColumnFamilyOutputFormat and an AvroC..F..O..F.., one of which wrapped the other.","24/Aug/10 19:46;stuhood;Rebase the existing patchset for trunk: I'll be tackling the Avro/Thrift duality sometime soon, but this isn't it.","25/Aug/10 20:53;jbellis;Stu convinced me that due to the direction the Hadoop project is moving, demand for a Thrift api is likely to be low.  Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InvalidRequestException(why='') returned from system_add_keyspace when strategy_class not found,CASSANDRA-1556,12475384,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,9/29/2010 7:50,3/12/2019 14:21,3/13/2019 22:24,10/6/2010 18:47,0.7 beta 3,,Legacy/CQL,,0,,,,,,"In thrift/CassandraServer system_add_keyspace() the strategy_class string from the KsDef is used to load a class. The ClassNotFoundError is then caught and used to build an InvalidRequestException. If the strategy_class is missing or empty, the error returned to the client is 

(python)
InvalidRequestException: InvalidRequestException(why='')

or 

InvalidRequestException: InvalidRequestException(why='foo')",,,,,,,,,,,,,,,,,,,,06/Oct/10 02:20;amorton;1556-amorton.txt;https://issues.apache.org/jira/secure/attachment/12456463/1556-amorton.txt,01/Oct/10 21:29;jbellis;1556.txt;https://issues.apache.org/jira/secure/attachment/12456160/1556.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,29:58.1,,,no_permission,,,,,,,,,,,,20198,,,Tue Oct 12 14:03:56 UTC 2010,,,,,,0|i0g5yf:,92395,jbellis,jbellis,,,,,,,,,01/Oct/10 21:29;jbellis;Odd that the CNFE would not include a message.  Does this patch work better?,"06/Oct/10 02:11;amorton;I noticed system_update_keyspace uses FBUtilities.<T>classForName() which prints out pretty error messages, e.g. 

InvalidRequestException(why=""Unable to find keyspace replication strategy class 'InvalidStrategyClass': is the CLASSPATH set correctly?"")

Have created a patch to use that in both the thrift and avro CassandraServer, will upload.","06/Oct/10 02:20;amorton;modified CassandraServer for thrift and avro to use FBUtilities.classForName() to get the strategy class for system_add_keyspace, was already doing it for system_update_keyspace","06/Oct/10 18:47;jbellis;committed, thanks!","12/Oct/10 14:03;hudson;Integrated in Cassandra #563 (See [https://hudson.apache.org/hudson/job/Cassandra/563/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
word count contrib module broken as a result of removing Clock interface,CASSANDRA-1529,12474767,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,9/21/2010 22:50,3/12/2019 14:21,3/13/2019 22:24,9/22/2010 2:05,0.7 beta 2,,,,0,,,,,,Just needs a quick patch.,,,,,,,,,,,,,,,,,,,,21/Sep/10 22:51;jeromatron;0001-Remove-clock-usage.patch;https://issues.apache.org/jira/secure/attachment/12455205/0001-Remove-clock-usage.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,05:32.1,,,no_permission,,,,,,,,,,,,20181,,,Wed Sep 22 02:05:32 UTC 2010,,,,,,0|i0g5sf:,92368,jbellis,jbellis,,,,,,,,,22/Sep/10 02:05;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodeprobe help message is missing option to compact specific keyspace,CASSANDRA-1568,12475718,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,appodictic,appodictic,appodictic,10/3/2010 18:17,3/12/2019 14:21,3/13/2019 22:24,10/4/2010 14:36,0.7 beta 3,,Legacy/Tools,,0,,,,,,"{noformat}
-                ""%nAvailable commands: ring, info, version, cleanup, compact, cfstats, snapshot [snapshotname], clearsnapshot, "" +
-                ""tpstats, flush, drain, repair, decommission, move, loadbalance, removetoken [status|force]|[token], "" +
+                ""%nAvailable commands: ring, info, version, cleanup, compact [keyspacename], cfstats, snapshot [snapshotname], "" +
+                ""clearsnapshot, tpstats, flush, drain, repair, decommission, move, loadbalance, removetoken [status|force]|[token], "" +
{noformat}",,,,,,,,,,,,,,,,,,,,03/Oct/10 18:18;appodictic;cassandra-1568.patch.txt;https://issues.apache.org/jira/secure/attachment/12456254/cassandra-1568.patch.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,36:00.2,,,no_permission,,,,,,,,,,,,20205,,,Tue Oct 05 13:25:31 UTC 2010,,,,,,0|i0g613:,92407,jbellis,jbellis,,,,,,,,,03/Oct/10 18:18;appodictic;Corrects help message,"04/Oct/10 14:36;jbellis;committed, thanks!","05/Oct/10 13:25;hudson;Integrated in Cassandra #556 (See [https://hudson.apache.org/hudson/job/Cassandra/556/])
    mention optional keyspace name in nodetool compact help message.  patch by ecapriolo; reviewed by jbellis for CASSANDRA-1568
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraServiceDataCleaner doesn't remove subdirectories properly,CASSANDRA-1509,12474242,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,btoddb,btoddb,btoddb,9/16/2010 3:05,3/12/2019 14:21,3/13/2019 22:24,9/18/2010 10:38,0.7 beta 2,,,,0,,,,,,"CassandraServiceDataCleaner.cleanDir assumes all files in the directory are normal files, not directories.  Suggested fix is to change FileUtils.delete(dirFile.listFiles()) to FileUtils.deleteRecursive(f) to remove recursively which will delete all data files.
",,,,,,,,,,,,,,,,,,,,16/Sep/10 03:06;btoddb;patch-delete-recursive.txt;https://issues.apache.org/jira/secure/attachment/12454736/patch-delete-recursive.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,38:46.5,,,no_permission,,,,,,,,,,,,20172,,,Sat Sep 18 12:48:21 UTC 2010,,,,,,0|i0g5hz:,92321,jbellis,jbellis,,,,,,,,,16/Sep/10 03:06;btoddb;patch to change to a recursive delete which will remove directories and all their files,18/Sep/10 10:38;jbellis;committed,"18/Sep/10 12:48;hudson;Integrated in Cassandra #539 (See [https://hudson.apache.org/hudson/job/Cassandra/539/])
    make contrib CassandraServiceDataCleaner recursive.  patch by B. Todd Burruss; reviewed by jbellis for CASSANDRA-1509
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
backgrounding cli makes it crash,CASSANDRA-1875,12493522,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,12/17/2010 15:32,3/12/2019 14:21,3/13/2019 22:24,12/22/2010 18:31,0.7.0,,Legacy/Tools,,0,,,,,,"{code}
$ bin/cassandra-cli 
Welcome to cassandra CLI.

Type 'help;' or '?' for help. Type 'quit;' or 'exit;' to quit.
[default@unknown] 
[1]+  Stopped                 bin/cassandra-cli
$ fg
bin/cassandra-cli
Exception in thread ""main"" java.io.IOException: Interrupted system call
	at java.io.FileInputStream.read(Native Method)
	at jline.Terminal.readCharacter(Terminal.java:99)
	at jline.UnixTerminal.readVirtualKey(UnixTerminal.java:128)
	at jline.ConsoleReader.readVirtualKey(ConsoleReader.java:1453)
	at jline.ConsoleReader.readBinding(ConsoleReader.java:654)
	at jline.ConsoleReader.readLine(ConsoleReader.java:494)
	at jline.ConsoleReader.readLine(ConsoleReader.java:448)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:328)
{code}
",,,,,,,,,,,,,,,,,,,,22/Dec/10 17:30;xedin;CASSANDRA-1875.patch;https://issues.apache.org/jira/secure/attachment/12466818/CASSANDRA-1875.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,43:06.4,,,no_permission,,,,,,,,,,,,20354,,,Wed Dec 22 19:05:02 UTC 2010,,,,,,0|i0g7xz:,92717,jbellis,jbellis,,,,,,,,,"22/Dec/10 17:37;jbellis;Isn't this going to catch, say, EOF as well?",22/Dec/10 17:43;xedin;EOFException is a subclass of the IOException like others (http://download.oracle.com/javase/6/docs/api/java/io/IOException.html).,"22/Dec/10 17:52;jbellis;Right, so I'm saying, isn't this potentially broken if you're piping input to the cli from a text file?","22/Dec/10 18:06;xedin;No, this won't broke when piping (I'm doing that all the time) or reading from file. I have tried to use --file option and piping file before submitting this patch :)",22/Dec/10 18:31;jbellis;committed,"22/Dec/10 19:05;hudson;Integrated in Cassandra-0.7 #109 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/109/])
    fix cli crash after backgrounding
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-1875
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] use cross platform new lines in printf calls,CASSANDRA-1786,12491508,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,11/28/2010 20:51,3/12/2019 14:21,3/13/2019 22:24,11/29/2010 16:51,0.7.0 rc 2,,,,0,,,,,,"code uses printf(""\n"") for new lines, should use ""%n"" instead.",ubuntu10.10 java 1.6.17 ant 1.8,,3600,3600,,0%,3600,3600,,,,,,,,,,,,28/Nov/10 20:51;dbrosius@apache.org;use_crossplatform_newlines.diff;https://issues.apache.org/jira/secure/attachment/12464819/use_crossplatform_newlines.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,51:34.0,,,no_permission,,,,,,,,,,,,20314,,,Sat Dec 11 07:35:22 UTC 2010,,,,,,0|i0g7dz:,92627,jbellis,jbellis,,,,,,,,,"29/Nov/10 16:51;jbellis;committed, thanks!","11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column MetaData: Index_name should not be allowed if index_type is not set.,CASSANDRA-1759,12480444,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,11/19/2010 20:24,3/12/2019 14:21,3/13/2019 22:24,11/23/2010 16:57,0.7.0 rc 2,,,,0,,,,,,"Giving an indexName starts the automatic index creation process.
If indexType is not also set, then that process barfs.
If a name is present, a type must be also (but the reverse is not necessarily true).",,,,,,,,,,,,,,,,,,,,19/Nov/10 22:39;jhermes;1759-2.txt;https://issues.apache.org/jira/secure/attachment/12460067/1759-2.txt,19/Nov/10 21:37;jhermes;1759.txt;https://issues.apache.org/jira/secure/attachment/12460056/1759.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,57:25.1,,,no_permission,,,,,,,,,,,,20296,,,Sat Dec 11 07:35:14 UTC 2010,,,,,,0|i0g77r:,92599,jbellis,jbellis,,,,,,,,,"19/Nov/10 21:37;jhermes;Similar to the patch for CASSANDRA-1527, this validates the creation of ColumnDefs from YAML and then from thrift/avro.

For simplicity, both name and index need to be set or unset at the same time. I don't think anyone is complaining that it's too hard to introduce non-obvious bugs, so I'm being explicit here.","19/Nov/10 21:57;jbellis;Looks like you got a little too clever with the xors -- index_type set but index_name not, is valid.","19/Nov/10 22:39;jhermes;Whoops, I read CFMD:279 wrong. I thought it was a bug that we were auto-generating the name incorrectly. Back to previous logic.",22/Nov/10 16:08;jhermes;CASSANDRA-1761 is the bug that I originally caught and is separate from this one.,23/Nov/10 16:57;jbellis;committed,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Startup can fail if DNS lookup fails for seed node,CASSANDRA-1697,12478906,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,appodictic,appodictic,11/2/2010 16:21,3/12/2019 14:21,3/13/2019 22:24,11/2/2010 17:13,0.6.7,0.7.0 rc 1,,,0,,,,,,"This might fall into one of those WONT FIX scenarios, since not many things are going to work well with flaky DNS. This has only happened to me once, but I someone might be interested in the stack trace. In this case cdbsd01.hadoop.pvt is one of my seed nodes.

{noformat}
 INFO [main] 2010-11-02 11:52:00,141 CLibrary.java (line 43) JNA not found. Native methods will be disabled.
 INFO [main] 2010-11-02 11:52:00,421 DatabaseDescriptor.java (line 246) DiskAccessMode ismmap, indexAccessMode is mmap
ERROR [main] 2010-11-02 11:52:25,591 CassandraDaemon.java (line 232) Exception encountered during startup.
java.lang.ExceptionInInitializerError
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:72)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:214)
Caused by: java.lang.RuntimeException: java.net.UnknownHostException: cdbsd01.hadoop.pvt
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:551)
	... 2 more
Caused by: java.net.UnknownHostException: cdbsd01.hadoop.pvt
	at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
	at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:850)
	at java.net.InetAddress.getAddressFromNameService(InetAddress.java:1201)
	at java.net.InetAddress.getAllByName0(InetAddress.java:1154)
	at java.net.InetAddress.getAllByName(InetAddress.java:1084)
	at java.net.InetAddress.getAllByName(InetAddress.java:1020)
	at java.net.InetAddress.getByName(InetAddress.java:970)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:540)
	... 2 more
{noformat}
",,,,,,,,,,,,,,,,,,,,02/Nov/10 16:36;jbellis;1697.txt;https://issues.apache.org/jira/secure/attachment/12458646/1697.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,31:35.3,,,no_permission,,,,,,,,,,,,20264,,,Tue Nov 02 17:13:26 UTC 2010,,,,,,0|i0g6tz:,92537,gdusbabek,gdusbabek,,,,,,,,,"02/Nov/10 16:31;jbellis;Yeah, sorry -- this is why we recommend using IP addresses instead of hostnames for seeds.",02/Nov/10 16:36;jbellis;Patch to generate a more human-friendly error message attached.,02/Nov/10 17:06;gdusbabek;+1,02/Nov/10 17:13;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
logging error on C* startup: Nodes /10.194.241.188 and /10.194.241.188 have the same token 85070591730234615865843651857942052863,CASSANDRA-1666,12478413,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,mdennis,mdennis,10/27/2010 1:44,3/12/2019 14:21,3/13/2019 22:24,10/28/2010 14:47,0.7 beta 3,,,,0,,,,,,"When restarting my cluster, I noticed that a

{code}Nodes /10.194.241.188 and /10.194.241.188 have the same token 85070591730234615865843651857942052863{code} error message.  Notice the IPs are the same.  The cluster came up fine and nodetool ring shows all nodes up so it's likely just a logging error.
",EC2,,,,,,,,,,,,,,,,,,,28/Oct/10 11:42;jbellis;1666.txt;https://issues.apache.org/jira/secure/attachment/12458245/1666.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,44:54.3,,,no_permission,,,,,,,,,,,,20249,,,Thu Oct 28 14:47:46 UTC 2010,,,,,,0|i0g6n3:,92506,gdusbabek,gdusbabek,,,,,,,,,28/Oct/10 12:44;gdusbabek;+1,28/Oct/10 14:47;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Database Descriptor has log message that mashes words,CASSANDRA-1604,12477040,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,appodictic,appodictic,appodictic,10/11/2010 16:26,3/12/2019 14:21,3/13/2019 22:24,10/11/2010 16:45,0.6.7,0.7 beta 3,,,0,,,,,,"-                logger.info(""DiskAccessMode is"" + conf.disk_access_mode + "", indexAccessMode is "" + indexAccessMode );
+                logger.info(""DiskAccessMode is "" + conf.disk_access_mode + "", indexAccessMode is "" + indexAccessMode );",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,11/Oct/10 16:28;appodictic;cassandra-1604-1-patch.txt;https://issues.apache.org/jira/secure/attachment/12456872/cassandra-1604-1-patch.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,45:27.4,,,no_permission,,,,,,,,,,,,20212,,,Mon Oct 11 16:45:27 UTC 2010,,,,,,0|i0g693:,92443,jbellis,jbellis,,,,,,,,,11/Oct/10 16:45;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli 'use Keyspace user pass' breaks with SimpleAuth,CASSANDRA-2111,12497714,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,thobbs,thobbs,2/4/2011 20:41,3/12/2019 14:21,3/13/2019 22:24,2/10/2011 2:43,0.7.1,,Legacy/Tools,,0,,,,,,"If SimpleAuth is used and the -Daccess.properties... JVM options are passed in, the CLI's ""use Keyspace user 'password'"" command breaks.  However, if the --username and --password options are used, you can still authenticate.",,;05/Feb/11 16:03;xedin;7200,7200,0,7200,100%,7200,0,7200,,,,,,,,,,,10/Feb/11 00:34;xedin;CASSANDRA-2111-v2.patch;https://issues.apache.org/jira/secure/attachment/12470744/CASSANDRA-2111-v2.patch,05/Feb/11 16:03;xedin;CASSANDRA-2111.patch;https://issues.apache.org/jira/secure/attachment/12470370/CASSANDRA-2111.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,32:08.5,,,no_permission,,,,,,,,,,,,20448,,,Thu Feb 10 04:34:53 UTC 2011,,,,,,0|i0g9dr:,92950,thobbs,thobbs,,,,,,,,,"04/Feb/11 22:32;xedin;Looks like that right now without a valid user/password given by --username/--password you won't be able to connect the cassandra instance, that means that if you will try to use `use Keyspace <username> '<password>';` it will tell you that `Keyspace is not found` which is a wrong error message in this case. 

I will make following changes: 

a). add support for setting username/password at `connect host/port;` command; 
b). change error to show that you are not connected to node instead of '<keyspace> not found' for `use <keyspace>` (other commands show the right error);","05/Feb/11 16:03;xedin;`connect` command enhancement:
`connect host/port <username> '<password>';`

and changes from previous comment.

","10/Feb/11 00:22;thobbs;The normal 'help' output does not show that you can specify a user/pass combination with connect.  The 'help connect' output does, though.

Other than that, it looks good.",10/Feb/11 00:34;xedin;v2 fixes missing note about user/pass for connect command when just `help` is used without `connect` attribute. This patch could be applied to both trunk and cassandra-0.7 branches.,10/Feb/11 00:41;thobbs;+1,10/Feb/11 02:43;jbellis;committed,"10/Feb/11 04:34;hudson;Integrated in Cassandra-0.7 #273 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/273/])
    add cli support for setting username/password at 'connect' command
patch by Pavel Yaskevich; reviewed by thobbs for CASSANDRA-2111
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When the ANTLR code generation fails the build continues and ignores the failure,CASSANDRA-1850,12493071,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stephenc,stephenc,stephenc,12/13/2010 13:49,3/12/2019 14:21,3/13/2019 22:24,12/13/2010 19:48,0.7.0 rc 3,,,,0,,,,,,"When trying to tweak the build, I broke the ANTLR code generation tasks, but as I was not doing a full clean build the generated code was still present, so I missed the fact that the code generation was broken. It would be nice if the antlr tasks had failonerror=""true""",,,0,0,,0%,0,0,,,,,,,,,,,,13/Dec/10 13:50;stephenc;fail-build-if-codegen-fails.patch;https://issues.apache.org/jira/secure/attachment/12466133/fail-build-if-codegen-fails.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,48:40.9,,,no_permission,,,,,,,,,,,,20343,,,Mon Dec 13 20:16:14 UTC 2010,,,,,,0|i0g7sn:,92693,urandom,urandom,,,,,,,,,13/Dec/10 13:50;stephenc;patch to fix this issue,13/Dec/10 19:48;urandom;committed; thanks.,"13/Dec/10 20:08;hudson;Integrated in Cassandra-0.7 #74 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/74/])
    failonerror for Cli antlr generation target

Patch by Stephen Connolly; reviewed by eevans for CASSANDRA-1850
","13/Dec/10 20:16;hudson;Integrated in Cassandra #625 (See [https://hudson.apache.org/hudson/job/Cassandra/625/])
    failonerror for Cql and Cli antlr generation targets

Patch by Stephen Connolly; reviewed by eevans for CASSANDRA-1850
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java -k doesn't keep going,CASSANDRA-1973,12495398,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,1/12/2011 18:27,3/12/2019 14:21,3/13/2019 22:24,1/18/2011 20:22,0.7.1,,,,0,,,,,,"stress.java's -k option doesn't work correctly.  In the face of many errors, it ends up printing 'null' a bunch and then exiting.",,;16/Jan/11 13:25;xedin;3600,3600,0,3600,100%,3600,0,3600,,,,,,,,,,,18/Jan/11 20:10;xedin;CASSANDRA-1973-v2.patch;https://issues.apache.org/jira/secure/attachment/12468677/CASSANDRA-1973-v2.patch,16/Jan/11 13:24;xedin;CASSANDRA-1973.patch;https://issues.apache.org/jira/secure/attachment/12468490/CASSANDRA-1973.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,28:36.1,,,no_permission,,,,,,,,,,,,20387,,,Tue Jan 18 21:28:25 UTC 2011,,,,,,0|i0g8jb:,92813,brandon.williams,brandon.williams,,,,,,,,,"12/Jan/11 21:28;xedin;Can't reproduce issue on the latest stress build it just prints Key XXX not found. and then exits after all -n keys are done.

Tried on empty db to read 100000 keys - standard and super (on both Standard and Super CF).

Can you please provide a better description how to reproduce this issue?","12/Jan/11 21:30;brandon.williams;I was trying to insert a node to death.  As death approached, stress.java died.","12/Jan/11 21:33;xedin;Ok gotcha, I will take a look at this. Thank you!","18/Jan/11 19:48;brandon.williams;Better, but it looks like the exception isn't trickling up:

Error while inserting key 4686646 - null
Error while inserting key 4697766 - null
Error while inserting key 4297775 - null
Error while inserting key 0609162 - null
Error while inserting key 0003898 - null
Error while inserting key 2719911 - null
Error while inserting key 4308726 - null
Error while inserting key 4453292 - null
Error while inserting key 4203275 - null
Error while inserting key 2625669 - null
Error while inserting key 1342402 - null
Error while inserting key 1247940 - null

I'm pretty sure these were TimeoutExceptions so it'd be nice to have that reflected.","18/Jan/11 19:51;xedin;I can make it write exception class too e.g, ""Error while inserting key 1247940 - (ExceptionClass): null"", what do you think?","18/Jan/11 20:22;brandon.williams;v2 looks good, committed.  Thanks!","18/Jan/11 21:28;hudson;Integrated in Cassandra-0.7 #172 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/172/])
    Improve stress.java error handling.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-1973
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The writing of statistics in SSTableWrite.Builder has been mistakenly removed by #1072,CASSANDRA-1981,12495531,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,1/13/2011 19:27,3/12/2019 14:21,3/13/2019 22:24,1/13/2011 19:43,0.8 beta 1,,,,0,,,,,,Everything's in the summary,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,13/Jan/11 19:27;slebresne;0001-Add-back-writting-of-statistics-in-SSTableWriter.patch;https://issues.apache.org/jira/secure/attachment/12468283/0001-Add-back-writting-of-statistics-in-SSTableWriter.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,43:17.5,,,no_permission,,,,,,,,,,,,19353,,,Thu Jan 13 21:14:19 UTC 2011,,,,,,0|i0g8l3:,92821,tjake,tjake,,,,,,,,,13/Jan/11 19:43;tjake;Committed!,"13/Jan/11 21:14;hudson;Integrated in Cassandra #667 (See [https://hudson.apache.org/hudson/job/Cassandra/667/])
    turn back on sstable statistics generation after it was removed by #1072 mistakenly.

Patch by Sylvain Lebresne; reviewed by Jake Luciani for CASSANDRA-1981
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstablekeys silently ignores extra arguments,CASSANDRA-2150,12498191,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,2/10/2011 4:47,3/12/2019 14:21,3/13/2019 22:24,2/10/2011 5:35,0.7.1,,Legacy/Tools,,0,,,,,,"sstablekeys only passes arg $1 to SSTableExporter instead of passing all arguments, like sstable2json.  Only one SSTable is allowed as an argument, but this is normally detected in SSTableExporter.java.  By only passing the one argument, we end up silently ignoring the remaining arguments.",,,600,600,,0%,600,600,,,,,,,,,,,,10/Feb/11 05:03;thobbs;2150.txt;https://issues.apache.org/jira/secure/attachment/12470753/2150.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,36:07.0,,,no_permission,,,,,,,,,,,,20466,,,Thu Feb 10 06:47:11 UTC 2011,,,,,,0|i0g9mf:,92989,jbellis,jbellis,,,,,,,,,10/Feb/11 05:03;thobbs;Attached patch passes all args.,10/Feb/11 05:36;jbellis;committed,"10/Feb/11 06:47;hudson;Integrated in Cassandra-0.7 #274 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/274/])
    forward all arguments to sstablekeys
patch by thobbs; reviewed by jbellis for CASSANDRA-2150
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating a SuperColumnFamily other than BytesType results in incorrect comparator types ,CASSANDRA-1712,12479178,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,ceocoder,ceocoder,11/5/2010 8:00,3/12/2019 14:21,3/13/2019 22:24,11/8/2010 21:06,0.7.0 rc 1,,Legacy/Tools,,0,,,,,,"CF 1
    ColumnFamily: CFCli (Super)
      Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.UTF8Type
      Subcolumns sorted by: org.apache.cassandra.db.marshal.LongType

was created with cli using 

create column family CFCli with column_type= 'Super' and comparator= 'LongType' and subcomparator='UTF8Type'

 CF 2
 ColumnFamily: CFYaml (Super)
      Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.UTF8Type
      Subcolumns sorted by: org.apache.cassandra.db.marshal.LongType

was created with yaml using 

  column_families:
        - name: CFYaml
          column_type: Super
          compare_with: LongType
          compare_subcolumns_with: UTF8Type

In both cases Subcolumn comparator was defined as UTF8Type but CF was created with subcomparatortype of LongType

",ubuntu using 0.7.0 beta 3 bin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,03:57.0,,,no_permission,,,,,,,,,,,,20267,,,Mon Nov 08 21:06:06 UTC 2010,,,,,,0|i0g6xb:,92552,,,,,,,,,,,"05/Nov/10 08:03;mdennis;{quote}
(03:00:59 AM) ceocoder1: I tried adding column using cli and my client (scromium) each time it fails with ""shit"" can not be converted to Long where shit is value of subcolumn
(03:01:07 AM) ceocoder1: thanks
(03:01:27 AM) ceocoder1: i meant name of subcolumn
(03:02:03 AM) mdennis: that sounds a lot worse than just an output bug
(03:02:09 AM) ceocoder1: y
(03:02:12 AM) mdennis: it sounds like it actually reversed them
(03:02:19 AM) ceocoder1: regular columns are fine
(03:02:24 AM) ceocoder1: just supercolumns
(03:02:44 AM) mdennis: it's karma saying people shouldn't use super columns
{quote}","05/Nov/10 12:17;jbellis;what does ""show keyspaces"" on the cli say the comparators are?","05/Nov/10 17:54;ceocoder;Output of show keyspaces

Keyspace: system:
  Replication Factor: 1
  Column Families:
    ColumnFamily: IndexInfo
    ""indexes that have been completed""
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period: 0.0/0
      Key cache size / save period: 0.01/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
    ColumnFamily: Schema
    ""current state of the schema""
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period: 0.0/0
      Key cache size / save period: 0.01/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
    ColumnFamily: Migrations
    ""individual schema mutations""
      Columns sorted by: org.apache.cassandra.db.marshal.TimeUUIDType
      Row cache size / save period: 0.0/0
      Key cache size / save period: 0.01/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
    ColumnFamily: LocationInfo
    ""persistent metadata for the local node""
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period: 0.0/0
      Key cache size / save period: 0.01/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
    ColumnFamily: HintsColumnFamily (Super)
    ""hinted handoff data""
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType/org.apache.cassandra.db.marshal.BytesType
      Subcolumns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period: 0.0/0
      Key cache size / save period: 0.01/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
Keyspace: Skunk:
  Replication Factor: 1
  Column Families:
    ColumnFamily: CFYaml (Super)
      Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.UTF8Type
      Subcolumns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period: 1000.0/0
      Key cache size / save period: 10000.0/3600
      Memtable thresholds: 0.29/255/59
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
    ColumnFamily: CFCli (Super)
      Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.UTF8Type
      Subcolumns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period: 0.0/0
      Key cache size / save period: 200000.0/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
Keyspace: Lucandra:
  Replication Factor: 1
  Column Families:
    ColumnFamily: Documents
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period: 0.0/0
      Key cache size / save period: 200000.0/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
    ColumnFamily: TermInfo (Super)
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType/org.apache.cassandra.db.marshal.BytesType
      Subcolumns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period: 0.0/0
      Key cache size / save period: 200000.0/3600
      Memtable thresholds: 0.571875/122/60
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
","05/Nov/10 19:34;jbellis;Oh, I see.  It's printing the correct comparator/subcomparator types, then there is an additional line with the comparator type incorrectly listed as the subcomparator.  Fixed in r1031741.","05/Nov/10 19:59;ceocoder;For 

ColumnFamily: CFYaml (Super)
Columns sorted by: org.apache.cassandra.db.marshal.LongType/org.apache.cassandra.db.marshal.UTF8Type
Subcolumns sorted by: org.apache.cassandra.db.marshal.LongType

set CFYaml['newrow'][1234567890]['column'] = 'value'
'column' could not be translated into a LongType.

I think type of ['column'] is UTF8Type not sure why I'm getting translated into LongType error.
","05/Nov/10 20:51;jbellis;Pavel, could you have a look at this second part?",05/Nov/10 21:01;xedin;sure! I will start with this issue right after I will finish 1470.,"08/Nov/10 21:06;jbellis;this is working fine for me in latest 0.7 code:

{code}
[default@unknown] create keyspace KS1
cfff16aa-eb7b-11df-b5e1-e700f669bcfc
[default@unknown] use KS1
Authenticated to keyspace: KS1
[default@KS1] create column family CFCli with column_type= 'Super' and comparator= 'LongType' and subcomparator='UTF8Type'
d4d6684b-eb7b-11df-b5e1-e700f669bcfc
[default@KS1] set CFCli['newrow'][1234567890]['column'] = 'value'
Value inserted.
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove -Dcassandra from the command line that bin/cassandra generates,CASSANDRA-601,12442486,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,12/4/2009 18:07,3/12/2019 14:21,3/13/2019 22:24,12/5/2009 1:30,0.6,,,,0,,,,,,"None of the code accesses System.getProperty(""cassandra"").  There is no need for it in the run command.",,,,,,,,,,,,,,,,,,,,04/Dec/09 22:16;gdusbabek;601-remove-unused-property.patch;https://issues.apache.org/jira/secure/attachment/12426985/601-remove-unused-property.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,31:21.9,,,no_permission,,,,,,,,,,,,19772,,,Sat Dec 05 12:34:25 UTC 2009,,,,,,0|i0fzxr:,91420,,,,,,,,,,,"04/Dec/09 22:16;gdusbabek;removes the benign ""-Dcassandra"" runtime property.",05/Dec/09 01:31;lenn0x;commited,"05/Dec/09 12:34;hudson;Integrated in Cassandra #278 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/278/])
    Remove -Dcassandra from the command line that bin/cassandra generates patch by gdusbabek; reviewed by goffinet for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli describe table exception.,CASSANDRA-317,12431481,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gasolwu,gasolwu,gasolwu,7/27/2009 3:51,3/12/2019 14:21,3/13/2019 22:24,7/27/2009 4:15,0.4,,Legacy/Tools,,0,,,,,,"cassandra> show tables            
Table1
Guestbook
cassandra> describe table Table1
Exception null
java.lang.NumberFormatException: null
        at java.lang.Integer.parseInt(Integer.java:415)
        at java.lang.Integer.parseInt(Integer.java:497)
        at org.apache.cassandra.cli.CliClient.executeDescribeTable(CliClient.java:259)
        at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:77)
        at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:121)
        at org.apache.cassandra.cli.CliMain.processLine(CliMain.java:153)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:187)
cassandra>

compare to CassandraServer.describe_table, columnMap key is mismatch.
","freebsd 7.2
java version ""1.6.0_07""
cassandra trunk r798015
Diablo Java(TM) SE Runtime Environment (build 1.6.0_07-b02)
Diablo Java HotSpot(TM) 64-Bit Server VM (build 10.0-b23, mixed mode)",,,,,,,,,,,,,,,,,,,27/Jul/09 03:56;gasolwu;CASSANDRA-317.patch;https://issues.apache.org/jira/secure/attachment/12414580/CASSANDRA-317.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,15:58.6,,,no_permission,,,,,,,,,,,,670,,,Mon Jul 27 12:42:13 UTC 2009,,,,,,0|i0fy73:,91138,,,,,,,,,,,"27/Jul/09 04:15;jbellis;committed, thanks!","27/Jul/09 12:42;hudson;Integrated in Cassandra #150 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/150/])
    fix cli/thrift map key regression.  patch by Gasol Wu; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI addColumnFamily - setting read_repair_chance modifies the keys_cache_size instead,CASSANDRA-1399,12471759,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,rnirmal,rnirmal,rnirmal,8/16/2010 22:43,3/12/2019 14:21,3/13/2019 22:24,8/17/2010 15:20,0.7 beta 2,,Legacy/Tools,,0,,,,,,"case KEY_CACHE_SIZE:
                cfDef.setKey_cache_size(Double.parseDouble(mValue));
                break;

case READ_REPAIR_CHANCE:
                cfDef.setKey_cache_size(Double.parseDouble(CliUtils.unescapeSQLString(mValue)));
                break;

Also it would be good to add gc_grace_seconds for this operation.",,,,,,,,,,,,,,,,,,,,16/Aug/10 22:55;rnirmal;0001-1399-CliClient-fixed-read_repair_chance-case.patch;https://issues.apache.org/jira/secure/attachment/12452225/0001-1399-CliClient-fixed-read_repair_chance-case.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,20:32.8,,,no_permission,,,,,,,,,,,,20117,,,Wed Aug 18 13:13:47 UTC 2010,,,,,,0|i0g4tj:,92211,jbellis,jbellis,,,,,,,,,17/Aug/10 15:20;jbellis;committed,"18/Aug/10 13:13;hudson;Integrated in Cassandra #517 (See [https://hudson.apache.org/hudson/job/Cassandra/517/])
    fix setting read_repair_chance from CLI addColumnFamily.  patch by Nirmal Ranganathan; reviewed by jbellis for CASSANDRA-1399
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.py stdev option should be float not int,CASSANDRA-1262,12468940,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,oby1,oby1,7/9/2010 17:00,3/12/2019 14:21,3/13/2019 22:24,7/11/2010 0:01,0.6.4,,,,0,,,,,,"The option to set the standard deviation parameter for the gaussian key generator defaults to 0.1 but has a type of int in the option parser.  As a result, it's impossible to use non-integer standard deviation values when testing.",,,,,,,,,,,,,,,,,,,,09/Jul/10 21:28;oby1;cassandra-0.6-1262.txt;https://issues.apache.org/jira/secure/attachment/12449118/cassandra-0.6-1262.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,01:53.4,,,no_permission,,,,,,,,,,,,20053,,,Wed Jul 14 13:56:15 UTC 2010,,,,,,0|i0g3zj:,92076,,,,,,,,,,,09/Jul/10 21:26;oby1;First time contributor - figured this is a simple chance to go through the motions.,11/Jul/10 00:01;brandon.williams;Nice catch Oren!  Committed to 0.6 and trunk.,"14/Jul/10 13:56;hudson;Integrated in Cassandra #491 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/491/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cli does not support updating replicate_on_write,CASSANDRA-2236,12499565,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,2/24/2011 8:46,3/12/2019 14:21,3/13/2019 22:24,2/24/2011 18:11,0.8 beta 1,,,,0,,,,,,Add support for updating a column families replicate on write setting.,,,,,,,,,,,,,,,,,,,,24/Feb/11 08:47;lenn0x;0001-cli-support-replicate_on_write-via-update-CF.patch;https://issues.apache.org/jira/secure/attachment/12471815/0001-cli-support-replicate_on_write-via-update-CF.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,04:26.2,,,no_permission,,,,,,,,,,,,20519,,,Thu Feb 24 20:14:28 UTC 2011,,,,,,0|i0ga5z:,93077,xedin,xedin,,,,,,,,,24/Feb/11 08:47;lenn0x;Patch by Kelvin Kakugawa.,24/Feb/11 18:04;xedin;+1,"24/Feb/11 20:14;hudson;Integrated in Cassandra #742 (See [https://hudson.apache.org/hudson/job/Cassandra/742/])
    Cli does not support updating replicate_on_write. patch Kelvin Kakugawa; reviewed by Pavel Yaskevich for CASSANDRA-2236
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to start Cassandra in windows if P drive exsists,CASSANDRA-824,12457227,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,kazw,kazw,2/23/2010 22:02,3/12/2019 14:21,3/13/2019 22:24,2/24/2010 15:40,0.6,,,,0,Missing-Class,P-Drive,win32,windows,wont-start,"When running bin\cassandra.bat from main dir, Cassandra exits with:
Invalid parameter - P:
Starting Cassandra Server
Listening for transport dt_socket at address: 8888
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/service/CassandraDaemon
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.service.CassandraDaemon
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.cassandra.service.CassandraDaemon.  Program will exit.",This problem should affect any version of windows with a P drive.,,0,0,,0%,0,0,,,,,,,,,,,,24/Feb/10 08:05;wolfeidau;cassandra-bat-tidy.patch;https://issues.apache.org/jira/secure/attachment/12436832/cassandra-bat-tidy.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,05:47.1,,,no_permission,,,,,,,,,,,,19880,,,Wed Feb 24 15:40:43 UTC 2010,,,,,,0|i0g1av:,91641,,,,,,,,,,,"23/Feb/10 22:06;kazw;Changing the following lines in bin\cassandra.bat will fix this issue (Substitute ""Q"" for any available drive letter):

subst Q: ""%CASSANDRA_HOME%\lib""
Q:
set CLASSPATH=Q:\

for %%i in (*.jar) do call :append %%i
goto okClasspath

:append
set CLASSPATH=%CLASSPATH%;Q:\%*
goto :eof","24/Feb/10 08:05;wolfeidau;Patch to correct the issue.

I have removed the use of subst which  was mapping the P: this is unneccessary on newer windows releases.

I will give it a go on some test machines at work so far this tests fine on windows 7.","24/Feb/10 15:24;gdusbabek;That patch didn't work perfectly for me.  It wasn't including the jar files located in %CASSANDRA_HOME%/build/lib/jars.  Once I added that, everything was good.

+1 on this patch.","24/Feb/10 15:40;gdusbabek;r915824 (0.6), r915828 (trunk)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when no keyspaces section is found in yaml file,CASSANDRA-1080,12464357,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,5/12/2010 15:46,3/12/2019 14:21,3/13/2019 22:24,5/12/2010 16:15,0.7 beta 1,,,,0,,,,,,Everything's in the summary,,,,,,,,,,,,,,,,,,,,12/May/10 15:47;slebresne;0001-Fix-NPE-when-no-keyspaces-section-is-found-in-yaml.patch;https://issues.apache.org/jira/secure/attachment/12444307/0001-Fix-NPE-when-no-keyspaces-section-is-found-in-yaml.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,14:23.0,,,no_permission,,,,,,,,,,,,19986,,,Wed May 12 16:14:22 UTC 2010,,,,,,0|i0g2vj:,91896,,,,,,,,,,,12/May/10 16:14;gdusbabek;+1.  Thanks Sylvain!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadoc for thrift interface is not generated,CASSANDRA-997,12462357,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,hannes@helma.at,hannes@helma.at,hannes@helma.at,4/18/2010 15:07,3/12/2019 14:21,3/13/2019 22:24,5/24/2010 22:34,0.6.3,,Legacy/Documentation and Website,,0,,,,,,In both 0.6 and svn trunk no javadoc is generated for thrift-generated classes like org.apache.cassandra.thrift.Cassandra. The problem is that the wrong directory is included in the javadoc ant target (interface/thrift instead of interface/thrift/gen-java). ,,,,,,,,,,,,,,,,,,,,18/Apr/10 15:09;hannes@helma.at;ASF.LICENSE.NOT.GRANTED--thrift-javadoc.diff;https://issues.apache.org/jira/secure/attachment/12442111/ASF.LICENSE.NOT.GRANTED--thrift-javadoc.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,34:37.8,,,no_permission,,,,,,,,,,,,19948,,,Mon May 24 22:34:37 UTC 2010,,,,,,0|i0g2db:,91814,,,,,,,,,,,18/Apr/10 15:09;hannes@helma.at;Simple patch to use proper directory for thrift-generated classes in javadoc task.,24/May/10 22:34;urandom;committed; thanks Hannes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Table comments indicates expiry checking happens 10x times per minimum interval, but doesn't",CASSANDRA-2000,12495843,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,1/18/2011 1:24,3/12/2019 14:21,3/13/2019 22:24,1/18/2011 2:56,0.7.1,,,,0,,,,,,"Minor point, the there is a comment in the body of the Table constructor that claims it checks 10x as often to not miss the deadline by more than 10%. It seems to me that either the comment should be removed, or a change is necessary to make it true (trivial patch attached).",,,,,,,,,,,,,,,,,,,,18/Jan/11 01:25;scode;2000.txt;https://issues.apache.org/jira/secure/attachment/12468614/2000.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,56:41.5,,,no_permission,,,,,,,,,,,,20396,,,Tue Jan 18 08:33:39 UTC 2011,,,,,,0|i0g8pb:,92840,jbellis,jbellis,,,,,,,,,18/Jan/11 02:56;jbellis;you totally just wanted to get issue 2000 :),18/Jan/11 02:56;jbellis;committed,"18/Jan/11 08:33;hudson;Integrated in Cassandra-0.7 #170 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/170/])
    check more frequently for memtable expiration
patch by Peter Schuller; reviewed by jbellis for CASSANDRA-2000
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli formatted help width,CASSANDRA-1841,12492953,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,12/10/2010 16:04,3/12/2019 14:21,3/13/2019 22:24,12/14/2010 23:36,0.7.0 rc 3,,Legacy/Tools,,0,,,,,,"Most of cassandra-cli's help output justifies to 81 chars, just enough to cause line wrap on most default sized terminals.  It would improve appearance here if one character of the separating white space was removed so that it justified at 80 chars.",,,0,0,,0%,0,0,,,,,,,,,,,,13/Dec/10 22:06;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1841-justify-at-80-chars-instead-of-81.txt;https://issues.apache.org/jira/secure/attachment/12466170/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1841-justify-at-80-chars-instead-of-81.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,16:14.4,,,no_permission,,,,,,,,,,,,20339,,,Tue Dec 14 23:54:56 UTC 2010,,,,,,0|i0g7qn:,92684,,,,,,,,,,,14/Dec/10 18:16;jbellis;+1,14/Dec/10 23:36;urandom;committed,"14/Dec/10 23:54;hudson;Integrated in Cassandra-0.7 #82 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/82/])
    CASSANDRA-1841: justify at 80 chars (instead of 81)

Patch by eevans
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"clustertool get_endpoints needs key as String, not ByteBuffer",CASSANDRA-1833,12492646,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,kelvin,kelvin,kelvin,12/7/2010 18:12,3/12/2019 14:21,3/13/2019 22:24,1/19/2011 1:02,0.7.0 rc 3,,Legacy/Tools,,0,,,,,,java RMI does not serialize ByteBuffer; revert clustertool to use String for key,all environments,,,,,,,,,,,,,,,,,,,07/Dec/10 23:20;kelvin;CASSANDRA-1833.120710.patch;https://issues.apache.org/jira/secure/attachment/12465757/CASSANDRA-1833.120710.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,45:00.4,,,no_permission,,,,,,,,,,,,20333,,,Wed Jan 19 01:02:32 UTC 2011,,,,,,0|i0g7ov:,92676,jbellis,jbellis,,,,,,,,,07/Dec/10 18:14;kelvin;replace ByteBuffer w/ String,07/Dec/10 20:45;jbellis;trying to decode to UTF8 is going to break on a lot of keys.  you'll want to use hex and FBUtilities.hexToBytes.,07/Dec/10 20:46;jbellis;... actually probably the best solution would be to put it back to being a byte[] the way it was pre-beta3.,07/Dec/10 23:20;kelvin;modified to use byte[] (like beta-2),"08/Dec/10 00:57;jbellis;committed, w/ addition of hexToBytes for nodeprobe","11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ","18/Jan/11 23:25;kelvin;just discovered that bytesToHex doesn't work via cluster tool get_endpoints.  The CharSets.UTF_8 version does, though.","19/Jan/11 01:02;kelvin;It's actually fine.

When using the cluster tool, keys must be hex-encoded.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli still relies on cassandra.in.sh instead of cassandra-env.sh,CASSANDRA-1446,12472890,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,brandon.williams,brandon.williams,8/30/2010 22:45,3/12/2019 14:21,3/13/2019 22:24,8/31/2010 3:15,0.6.6,0.7 beta 2,,,0,,,,,,"When we switched to cassandra-env.sh, we neglected to change the cli as well.  This leads to people unable to launch to the client due to heap size, and not having any idea how to change the heap for the cli itself.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,26:09.8,,,no_permission,,,,,,,,,,,,20143,,,Sun Sep 12 19:39:05 UTC 2010,,,,,,0|i0g53z:,92258,,,,,,,,,,,"30/Aug/10 23:26;urandom;cassandra-cli never used the set of arguments from cassandra.in.sh which included (among other things) the heap size.  This was on purpose, the memory requirements of the CLI should be negligible, and there is definitely no correlation to the memory needed by the server. 

Do you know of a specific problem here? Can you provide any specifics?","31/Aug/10 00:06;brandon.williams;The specific problem I'm seeing here is that the user can launch cassandra itself, but doesn't have enough memory to launch cassandra-cli afterwards, and there isn't an intuitive way to change cassandra-cli's heap.  I see cassandra-cli's script specifically trying to include cassandra.in.sh, though, on second look, it does not appear to be doing anything with it.  Maybe we should just remove the cassandra.in.sh cruft.","31/Aug/10 02:57;urandom;{quote}
The specific problem I'm seeing here is that the user can launch cassandra itself, but doesn't have enough memory to launch cassandra-cli afterwards, and there isn't an intuitive way to change cassandra-cli's heap.
{quote}

Interesting.  I would have thought the default max heap size would be something... reasonable, but on my machine it seems to be 987MB (the hell?!).

Anyway, I just checked in a change that sets this to 256MB.  I don't know if that's the optimal value but it's considerably less than the almost-a-gig default.  The only reason it would ever need to be bigger than say 16MB is to buffer larger responses, and 256MB seems like a gracious plenty for any practical use of our CLI.

{quote}
I see cassandra-cli's script specifically trying to include cassandra.in.sh, though, on second look, it does not appear to be doing anything with it. Maybe we should just remove the cassandra.in.sh cruft.
{quote}

It is using it though, it's using it to setup the classpath; cassandra.in.sh is basically the common code for setting the classpath in all of those scripts.","31/Aug/10 03:02;brandon.williams;256M works for me, but let's put this in 0.6 too.",31/Aug/10 03:15;urandom;committed new max heap to 0.6 and trunk,"12/Sep/10 19:39;hudson;Integrated in Cassandra #533 (See [https://hudson.apache.org/hudson/job/Cassandra/533/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Outdated comment in default storage-conf.xml,CASSANDRA-461,12436720,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,lars_francke,lars_francke,9/28/2009 1:32,3/12/2019 14:20,3/13/2019 22:24,9/29/2009 4:03,0.5,,Legacy/Documentation and Website,,0,,,,,,"There is a comment for the ""Keyspace"" element

   ~ The default ColumnSort is Time for standard column families.  For
   ~ super column families, specifying ColumnSort is not supported; the 
   ~ supercolumns themselves are always name-sorted and their subcolumns
   ~ are always time-sorted.

This seems to be outdated as far as I can tell and could be deleted as the default ""CompareWith"" value (BytesType) is explained later for the ColumnFamily element which seems to be more appropiate. The limitation seems to have been lifted (again: as far as I can tell, I'm new to Cassandra).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,03:19.5,,,no_permission,,,,,,,,,,,,19700,,,Tue Sep 29 04:03:19 UTC 2009,,,,,,0|i0fz2v:,91281,,,,,,,,,,,29/Sep/09 04:03;jbellis;removed; thanks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Version name not set correctly,CASSANDRA-239,12428194,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,sandeep_tata,sandeep_tata,sandeep_tata,6/18/2009 0:41,3/12/2019 14:20,3/13/2019 22:24,6/18/2009 17:30,0.4,,Legacy/Tools,,0,,,,,,"""show version"" in the CLI should return 0.3.0, it currently returns 1",all,,,,,,,,,,,,,,,,,,,18/Jun/09 00:45;sandeep_tata;239.patch;https://issues.apache.org/jira/secure/attachment/12411016/239.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,30:18.7,,,no_permission,,,,,,,,,,,,19606,,,Thu Jun 18 17:30:18 UTC 2009,,,,,,0|i0fxpz:,91061,,,,,,,,,,,"18/Jun/09 00:46;sandeep_tata;Changed version constant in the code.
Figured this was better than adding a ""version number"" field in the conf file.",18/Jun/09 17:30;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix build of stress.java in trunk,CASSANDRA-2159,12498528,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,slebresne,slebresne,2/14/2011 13:19,3/12/2019 14:20,3/13/2019 22:24,2/14/2011 14:55,0.8 beta 1,,,,0,,,,,,Some lines in stress (java) seems to have missed a merge,,,,,,,,,,,,,,,,,,,,14/Feb/11 13:19;slebresne;0001-Fix-stress.java-build.patch;https://issues.apache.org/jira/secure/attachment/12470994/0001-Fix-stress.java-build.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,55:34.9,,,no_permission,,,,,,,,,,,,20472,,,Mon Feb 14 14:55:34 UTC 2011,,,,,,0|i0g9of:,92998,,,,,,,,,,,14/Feb/11 14:55;jbellis;fixed by merging contrib/ from 0.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
changelog doesn't mention snapshot support,CASSANDRA-372,12433357,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,8/18/2009 17:58,3/12/2019 14:20,3/13/2019 22:24,8/20/2009 17:10,0.4,,Legacy/Documentation and Website,,0,,,,,,anything else we need to add to 0.4 CHANGES.txt?,,,,,,,,,,,,,,,,,,,,18/Aug/09 22:54;jbellis;372.patch;https://issues.apache.org/jira/secure/attachment/12416930/372.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,34:39.8,,,no_permission,,,,,,,,,,,,19657,,,Fri Aug 21 12:34:39 UTC 2009,,,,,,0|i0fyjb:,91193,,,,,,,,,,,18/Aug/09 22:54;jbellis;if there are no other suggestions then here is a one-line patch.  to a text file.  for review.,"20/Aug/09 17:10;jbellis;I feel ridiculous nagging people to review this, so applied.","21/Aug/09 12:34;hudson;Integrated in Cassandra #174 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/174/])
    mention snapshot support in changelog
patch by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli.bat fails to start cli client,CASSANDRA-858,12458407,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mwn,mwn,mwn,3/7/2010 22:59,3/12/2019 14:20,3/13/2019 22:24,3/8/2010 17:36,0.6,,Legacy/Tools,,0,,,,,,"When one fetch latest and greates from svn - the cassandra.bat works. (like - the server starts and get ready to rock) but the cassandra-cli.bat fails.
java claims NoClassDefFoundErrror: jline/Completor
- after joining all four braincells I noticed that %CASSANDRA_HOME%\build\lib\jars\* is not part of the classpath that is setup inside the bat file. Adding this solved the issue.
",winxp / java 1.6 / svn @ 920147,,600,600,,0%,600,600,,,,,,,,,,,,07/Mar/10 23:00;mwn;CASSANDRA-858.patch;https://issues.apache.org/jira/secure/attachment/12438152/CASSANDRA-858.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:51.6,,,no_permission,,,,,,,,,,,,19893,,,Mon Mar 08 17:39:45 UTC 2010,,,,,,0|i0g1if:,91675,,,,,,,,,,,07/Mar/10 23:00;mwn;fix,"08/Mar/10 17:21;jbellis;wfm on trunk, fails to apply against 0.6 (we need it there too right?)",08/Mar/10 17:36;jbellis;got it to apply to 0.6.  committed.,"08/Mar/10 17:39;staffan ericsson;%CASSANDRA_HOME%\build\lib\jars\ does not exist in the 0.6-beta2 binary package.
Correct path should be %CASSANDRA_HOME%\lib\jars\ 



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debian packaging refers to now nonexistent DISCLAIMER.txt,CASSANDRA-1173,12466401,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yosh,yosh,yosh,6/7/2010 23:08,3/12/2019 14:20,3/13/2019 22:24,6/18/2010 18:52,0.6.3,,,,0,,,,,,Debian packaging refers to now nonexistent DISCLAIMER.txt. Trivial patch attached.,,,,,,,,,,,,,,,,,,,,07/Jun/10 23:08;yosh;no-more-disclaimer.patch;https://issues.apache.org/jira/secure/attachment/12446542/no-more-disclaimer.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,52:32.8,,,no_permission,,,,,,,,,,,,20017,,,Fri Jun 18 18:52:32 UTC 2010,,,,,,0|i0g3fz:,91988,,,,,,,,,,,07/Jun/10 23:08;yosh;Trivial patch,18/Jun/10 18:52;urandom;committed; thanks Manish!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowWarningThreshold doesn't allow values more than about 2GB,CASSANDRA-882,12458946,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,3/12/2010 15:08,3/12/2019 14:20,3/13/2019 22:24,3/12/2010 20:34,0.6,,,,0,,,,,,"Using values bigger than 2048 for RowWarningThreshold makes 
Cassandra not start with the message: 
  Fatal error: Row warning threshold must be a positive integer",,,,,,,,,,,,,,,,,,,,12/Mar/10 15:10;slebresne;882-fix_rowWarningThresholdLimit.diff;https://issues.apache.org/jira/secure/attachment/12438617/882-fix_rowWarningThresholdLimit.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,29:59.5,,,no_permission,,,,,,,,,,,,19900,,,Fri Mar 12 20:29:59 UTC 2010,,,,,,0|i0g1nr:,91699,,,,,,,,,,,12/Mar/10 20:29;gdusbabek;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mixed use of Stage's name. Must use public static field.,CASSANDRA-906,12459602,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,rodrigoap,rodrigoap,rodrigoap,3/19/2010 2:25,3/12/2019 14:20,3/13/2019 22:24,4/2/2010 13:23,0.7 beta 1,,,,0,correctness,,,,,"This line in StageManger is not using the public static field to reference the Stage's name:

        stages.put(RESPONSE_STAGE, multiThreadedStage(""RESPONSE-STAGE"", Runtime.getRuntime().availableProcessors()));

It should be:

        stages.put(RESPONSE_STAGE, multiThreadedStage(RESPONSE_STAGE, Runtime.getRuntime().availableProcessors()));
",,,,,,,,,,,,,,,,,,,,19/Mar/10 02:34;rodrigoap;CASSANDRA-906.patch;https://issues.apache.org/jira/secure/attachment/12439232/CASSANDRA-906.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,23:04.5,,,no_permission,,,,,,,,,,,,19912,,,Tue Apr 06 13:10:16 UTC 2010,,,,,,0|i0g1t3:,91723,,,,,,,,,,,02/Apr/10 13:23;jbellis;committed,"06/Apr/10 13:10;hudson;Integrated in Cassandra #399 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/399/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
using Integer.MAX_VALUE for executor keepalive time defeats the purpose of the SEDA-like stage divisions,CASSANDRA-805,12456560,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,2/17/2010 17:02,3/12/2019 14:20,3/13/2019 22:24,4/22/2010 21:06,0.7 beta 1,,,,0,,,,,,we should allow thread pools to shrink when they have excess capacity,,,,,,,,,,,,,,,,,,,,22/Apr/10 15:59;jbellis;805.txt;https://issues.apache.org/jira/secure/attachment/12442571/805.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,43:06.8,,,no_permission,,,,,,,,,,,,19872,,,Thu Apr 22 21:06:33 UTC 2010,,,,,,0|i0g16n:,91622,,,,,,,,,,,19/Mar/10 02:43;rodrigoap;Maybe a StageController attached to the Stage that monitors its throughput to do runtime tuning of the thread pool.,22/Apr/10 20:26;rschildmeijer;+1,22/Apr/10 21:06;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsynchResult does not respect TimeUnit in get(),CASSANDRA-1362,12470899,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,hemartin,hemartin,hemartin,8/5/2010 12:51,3/12/2019 14:20,3/13/2019 22:24,8/5/2010 13:12,0.7 beta 1,,,,0,,,,,,"When waiting for a blocking get in AsynchResult, the parameter {{TimeUnit tu}} is ignored. The passed parameter {{long timeout}} is assumed to be milliseconds. Attached you will find my quick fix to convert {{timeout}} to milliseconds with respect to {{tu}}.",,,,,,,,,,,,,,,,,,,,05/Aug/10 12:53;hemartin;trunk-982452.txt;https://issues.apache.org/jira/secure/attachment/12451326/trunk-982452.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,12:33.0,,,no_permission,,,,,,,,,,,,20105,,,Sun Aug 08 13:52:23 UTC 2010,,,,,,0|i0g4lb:,92174,jbellis,jbellis,,,,,,,,,05/Aug/10 12:52;hemartin;Patch to convert {{timeout}} parameter to milliseconds.,05/Aug/10 13:12;jbellis;committed to trunk,"08/Aug/10 13:52;hudson;Integrated in Cassandra #510 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/510/])
    add TimeUnit.convert call to AsyncResult.  patch by Martin Hentschel; reviewed by jbellis for CASSANDRA-1362
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DynamicEndpointSnitch should implement AbstractRackAwareSnitch, not AbstractEndpointSnitch",CASSANDRA-1349,12470719,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,8/3/2010 12:32,3/12/2019 14:20,3/13/2019 22:24,8/3/2010 15:50,0.7 beta 1,,,,0,,,,,,Everything's in the summary I believe,,,,,,,,,,,,,,,,,,,,03/Aug/10 12:33;slebresne;1349-Make-DynamicEndpointSnitch-implements-AbstractRackAw.patch;https://issues.apache.org/jira/secure/attachment/12451119/1349-Make-DynamicEndpointSnitch-implements-AbstractRackAw.patch,03/Aug/10 15:42;slebresne;1349-Remove-test-for-AbstractRackAwareSnitch-in-rack-awar.patch;https://issues.apache.org/jira/secure/attachment/12451129/1349-Remove-test-for-AbstractRackAwareSnitch-in-rack-awar.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,53:24.3,,,no_permission,,,,,,,,,,,,20095,,,Wed Aug 04 13:25:31 UTC 2010,,,,,,0|i0g4if:,92161,,,,,,,,,,,03/Aug/10 14:53;jbellis;the right fix is to remove the instanceof check in DatacenterShardStrategy instead ,03/Aug/10 15:49;slebresne;You're right. Attaching patch that just do that. I guess it would nice to get an InvalidRequestException when you try to insert a keyspace with a strategy incompatible with your snitch. But maybe the first move would be to make the snitch configuration per keyspace ?,"03/Aug/10 15:50;jbellis;committed
","03/Aug/10 15:55;jbellis;I don't think it makes sense for ""which node is closer"" to depend on what keyspace you're talking about","04/Aug/10 13:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    remove instanceof checks for AbstractRackAwareSnitch.  patch by Sylvain Lebresne; reviewed by jbellis for CASSANDRA-1349
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageService.getPartitioner() and QueryFilter.getColumnComparator() should be statically accessed,CASSANDRA-966,12461576,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tupshin,tupshin,tupshin,4/8/2010 19:34,3/12/2019 14:20,3/13/2019 22:24,4/9/2010 14:37,0.7 beta 1,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,08/Apr/10 19:35;tupshin;static_access.diff;https://issues.apache.org/jira/secure/attachment/12441201/static_access.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,37:47.2,,,no_permission,,,,,,,,,,,,19941,,,Tue Apr 13 12:41:32 UTC 2010,,,,,,0|i0g26f:,91783,,,,,,,,,,,"09/Apr/10 14:37;jbellis;committed, thanks!","13/Apr/10 12:41;hudson;Integrated in Cassandra #405 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/405/])
    access static members by class name.  patch by Tupshin Harper; reviewed by jbellis for CASSANDRA-966
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix counter bug (regression from svn commit r1068504),CASSANDRA-2155,12498391,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,2/11/2011 15:45,3/12/2019 14:20,3/13/2019 22:24,2/11/2011 16:43,0.8 beta 1,,,,0,,,,,,A line was mistakenly removed by the merge from 0.7 at r1068504,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,11/Feb/11 15:45;slebresne;0001-Fix-regression-from-svn-commit-1068504.patch;https://issues.apache.org/jira/secure/attachment/12470863/0001-Fix-regression-from-svn-commit-1068504.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,43:32.8,,,no_permission,,,,,,,,,,,,20469,,,Fri Feb 11 17:49:48 UTC 2011,,,,,,0|i0g9nj:,92994,,,,,,,,,,,11/Feb/11 16:43;jbellis;committed,"11/Feb/11 17:49;hudson;Integrated in Cassandra #724 (See [https://hudson.apache.org/hudson/job/Cassandra/724/])
    fix merge
patch by slebresne; reviewed by jbellis for CASSANDRA-2155
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ReadResponseResolver logs resolve data to info, should be debug",CASSANDRA-427,12434924,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,9/6/2009 5:42,3/12/2019 14:20,3/13/2019 22:24,9/14/2009 18:00,0.5,,,,0,,,,,,ReadResponseResolver resolve() function logs to info when being called. We should move this to debug. ,,,,,,,,,,,,,,,,,,,,06/Sep/09 05:44;lenn0x;0001-Moved-log-level-to-debug-from-info-when-resolve-is-c.patch;https://issues.apache.org/jira/secure/attachment/12418736/0001-Moved-log-level-to-debug-from-info-when-resolve-is-c.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,00:52.9,,,no_permission,,,,,,,,,,,,19683,,,Tue Sep 15 14:01:16 UTC 2009,,,,,,0|i0fyv3:,91246,,,,,,,,,,,14/Sep/09 18:00;jbellis;committed,"15/Sep/09 14:01;hudson;Integrated in Cassandra #198 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/198/])
    Move log level to debug from info when resolve() is called.  patch by Chris Goffinet; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update py_stress to reflect udpated KsDef params,CASSANDRA-1352,12470747,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,rnirmal,rnirmal,rnirmal,8/3/2010 18:11,3/12/2019 14:20,3/13/2019 22:24,8/3/2010 20:41,0.7 beta 1,,,,0,,,,,,"The following occurs while running stress.py. 

Traceback (most recent call last):
  File ""stress.py"", line 387, in <module>
    make_keyspaces()
  File ""stress.py"", line 160, in make_keyspaces
    client.system_add_keyspace(keyspace)
  File ""/Users/nirmal.ranganathan/Documents/workspace/cassandra.git/interface/thrift/gen-py/cassandra/Cassandra.py"", line 1288, in system_add_keyspace
    self.send_system_add_keyspace(ks_def)
  File ""/Users/nirmal.ranganathan/Documents/workspace/cassandra.git/interface/thrift/gen-py/cassandra/Cassandra.py"", line 1295, in send_system_add_keyspace
    args.write(self._oprot)
  File ""/Users/nirmal.ranganathan/Documents/workspace/cassandra.git/interface/thrift/gen-py/cassandra/Cassandra.py"", line 5745, in write
    oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
SystemError: Objects/dictobject.c:1562: bad argument to internal function

KsDef added an extra parameter and needs to be updated here.",,,,,,,,,,,,,,,,,,,,03/Aug/10 20:20;rnirmal;1352-v2.patch;https://issues.apache.org/jira/secure/attachment/12451152/1352-v2.patch,03/Aug/10 18:22;rnirmal;1352.patch;https://issues.apache.org/jira/secure/attachment/12451142/1352.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,37:14.7,,,no_permission,,,,,,,,,,,,20098,,,Wed Aug 04 13:25:34 UTC 2010,,,,,,0|i0g4j3:,92164,,,,,,,,,,,03/Aug/10 19:37;brandon.williams;This is the second time we've had to fix this.  Can we change the ks/cf definitions to use kwargs as much as possible to avoid any future problems?,"03/Aug/10 20:20;rnirmal;Good thought, I've updated v2 to use kwargs.",03/Aug/10 20:41;brandon.williams;Thanks!  Committed.,"04/Aug/10 13:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    Use keyword args for CfDef/KsDef to futureproof.  Patch by  Nirmal Ranganathan, reviewed by brandonwilliams for CASSANDRA-1352
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[patch] use long math, if long values are expected.",CASSANDRA-1785,12491506,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,11/28/2010 20:34,3/12/2019 14:20,3/13/2019 22:24,11/29/2010 16:48,0.7.0 rc 2,,,,0,,,,,,"code does math assuming the result is a long, but uses integer math. Might as well use long math to avoid possible truncation.",ubuntu10.10 java1.6.17 ant1.8,,3600,3600,,0%,3600,3600,,,,,,,,,,,,28/Nov/10 20:34;dbrosius@apache.org;use_long_math.diff;https://issues.apache.org/jira/secure/attachment/12464818/use_long_math.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,48:35.5,,,no_permission,,,,,,,,,,,,20313,,,Sat Dec 11 07:35:21 UTC 2010,,,,,,0|i0g7dr:,92626,jbellis,jbellis,,,,,,,,,"29/Nov/10 16:48;jbellis;commited, thanks!","11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spurious failures of o.a.c.db.NameSortTest:testNameSort100,CASSANDRA-1783,12491450,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,urandom,urandom,11/27/2010 16:09,3/12/2019 14:20,3/13/2019 22:24,12/11/2010 6:52,0.7.0 rc 3,,,,0,,,,,,"{noformat}
    [junit] Cobertura: Loaded information on 961 classes.
    [junit] Cobertura: Saved information on 961 classes.
    [junit] Testsuite: org.apache.cassandra.db.NameSortTest
    [junit] Testsuite: org.apache.cassandra.db.NameSortTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.NameSortTest:testNameSort100:	Caused an ERROR
    [junit] Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit]
{noformat}

See also: https://hudson.apache.org/hudson/job/Cassandra-0.7/33/console","Hudson, ubuntu2 (vesta.apache.org)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,52:24.4,,,no_permission,,,,,,,,,,,,20312,,,Sat Dec 11 06:52:24 UTC 2010,,,,,,0|i0g7db:,92624,,,,,,,,,,,11/Dec/10 06:52;jbellis;fixed in r1044570,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
isSuper flag in cfstore is wrongly set in 0.7,CASSANDRA-1054,12463809,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,5/5/2010 19:22,3/12/2019 14:20,3/13/2019 22:24,5/5/2010 19:40,0.7 beta 1,,,,0,,,,,,"In 0.7, following CASSANDRA-16, the isSuper in ColumnFamilyStore is not set correctly (if I'm correct). 
This is because when the model is applied (AddColumnFamily.applyModels()) the columnFamilyStore
is created before the call to DataDescriptor.setTableDefinition. But the createColumnFamilyStore() 
function retrieve the columnType. This thus always return a null that end up in a ""Super"".equals(null)
that always sets the flag to false.
That being said, the isSuper flag of columnFamilyStore is never used. 
I propose thus to get ride of this flag completely since if needed in the future, the column type can 
always be retrieved from the table and cfname directly (the attached patch do just that).",,,,,,,,,,,,,,,,,,,,05/May/10 19:24;slebresne;0001-Removing-unused-and-wrongly-set-isSuper-flag-in-cfst.patch;https://issues.apache.org/jira/secure/attachment/12443755/0001-Removing-unused-and-wrongly-set-isSuper-flag-in-cfst.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,40:40.3,,,no_permission,,,,,,,,,,,,19976,,,Wed May 05 19:40:40 UTC 2010,,,,,,0|i0g2pr:,91870,,,,,,,,,,,"05/May/10 19:40;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
size of row in spanned index entries does not include key bytes,CASSANDRA-1056,12463828,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,5/5/2010 22:54,3/12/2019 14:20,3/13/2019 22:24,5/10/2010 14:36,0.6.2,,,,0,,,,,,"from Anty on the mailing list,

In source code of 0.6.1 ,in SSTableWriter,
private void afterAppend(DecoratedKey decoratedKey, long dataPosition, int dataSize) throws IOException
    {
        String diskKey = partitioner.convertToDiskFormat(decoratedKey);
        bf.add(diskKey);
        lastWrittenKey = decoratedKey;
        long indexPosition = indexFile.getFilePointer();
        indexFile.writeUTF(diskKey);
        indexFile.writeLong(dataPosition);
        if (logger.isTraceEnabled())
            logger.trace(""wrote "" + decoratedKey + "" at "" + dataPosition);
        if (logger.isTraceEnabled())
            logger.trace(""wrote index of "" + decoratedKey + "" at "" + indexPosition);

        indexSummary.maybeAddEntry(decoratedKey, dataPosition, dataSize, indexPosition, indexFile.getFilePointer());
    }
the value of ""dataSize"" is the length of value( column family) ,not including the length of key.

but in  the method  loadIndexFile() of SStableReader
...
    else
                {
                    input.readUTF();
                    nextDataPosition = input.readLong();
                    input.seek(nextIndexPosition);
                }
                indexSummary.maybeAddEntry(decoratedKey, dataPosition, nextDataPosition - dataPosition, indexPosition, nextIndexPosition);
            }
            indexSummary.complete();


the value of nextDataPosition - dataPosition is the length of key and value ,not just the length of value .

",,,,,,,,,,,,,,,,,,,,05/May/10 22:55;jbellis;1056.txt;https://issues.apache.org/jira/secure/attachment/12443790/1056.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,19977,,,Mon May 10 14:36:38 UTC 2010,,,,,,0|i0g2q7:,91872,,,,,,,,,,,05/May/10 22:55;jbellis;unit test + fix,"10/May/10 14:34;jbellis;Anty writes on dev list: ""I have reviewed the patch,and have done test on my litter luster, all is good.""",10/May/10 14:36;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix low-hanging scoping and other issues picked up by findbugs,CASSANDRA-338,12432084,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,euphoria,euphoria,euphoria,8/3/2009 19:51,3/12/2019 14:20,3/13/2019 22:24,12/9/2009 1:40,0.6,,,,0,,,,,,"Findbugs currently reports 266 bugs against trunk.  Many of these are invalid, but many are genuine.  This issue will involve fixing the genuine ones that are fixable in time for 0.4.",,,,,,,,,,,,,,,,,,,,07/Aug/09 22:47;dehora;CASSANDRA-338-CCE.diff;https://issues.apache.org/jira/secure/attachment/12415901/CASSANDRA-338-CCE.diff,03/Aug/09 19:53;euphoria;cassandra-findbugs-1.diff;https://issues.apache.org/jira/secure/attachment/12415397/cassandra-findbugs-1.diff,03/Aug/09 20:27;euphoria;cassandra-findbugs-2.diff;https://issues.apache.org/jira/secure/attachment/12415405/cassandra-findbugs-2.diff,05/Aug/09 13:12;jbellis;findbugs-2nd-addendum.diff;https://issues.apache.org/jira/secure/attachment/12415599/findbugs-2nd-addendum.diff,05/Aug/09 04:58;euphoria;findbugs-2ndset-1.diff;https://issues.apache.org/jira/secure/attachment/12415576/findbugs-2ndset-1.diff,,,,,,,,,5,,,,,,,,,,,,,,,,,,,07:16.6,,,no_permission,,,,,,,,,,,,19641,,,Wed Dec 09 01:40:27 UTC 2009,,,,,,0|i0fybr:,91159,,,,,,,,,,,"03/Aug/09 19:53;euphoria;Attached patch brings count from 266 -> 250 bugs by fixing some scopes, removing some dead code, and properly implementing Serializable",03/Aug/09 20:07;jbellis;why make Row serializable?,"03/Aug/09 20:12;euphoria;Row already implemented the functionality for Serializable, it just didn't declare itself as such.  It should, because ReadResponse is Serializable and contains a Row.  Our serializer implementation for ReadResponse just knows about Row's serializer, but findbugs was compelling me to make Java know about it as well.","03/Aug/09 20:15;jbellis;ah, so the real bug is that ReadResponse implements Serializable, when it doesn't need to (relic of really old FB code)",03/Aug/09 20:27;euphoria;Incorporates jbellis's more rational way of solving the Row serialization problem.,03/Aug/09 20:32;jbellis;committed,"04/Aug/09 12:35;hudson;Integrated in Cassandra #157 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/157/])
    brings findbugs count from 266 -> 250 bugs by fixing some scopes, removing some dead code, and not implementing Serializable unnecessarily.  patch by Michael Greene; reviewed for  by jbellis
",04/Aug/09 20:05;jbellis;(does that resolve this issue?),"04/Aug/09 20:21;euphoria;Well, there are plenty of other ones to pick off in the report.  I'd rather keep this open until we're planning on wrapping up 0.4 rather than open up new issues for each set.  I have 2 more off the 250 in a local branch.","05/Aug/09 04:58;euphoria;This is a second set of fixes to potential problems caught by findbugs.  Brings bug count from 250 -> 239.

For what it's worth, about 150 of the remaining are out of our control through ANTLR or Thrift.","05/Aug/09 13:12;jbellis;addendup to patch 2:

instead of adding hashcode to classes that override equals unnecessarily, r/m the equals method.

instead of adding null checks to variables that Should Never Be Null, add assert != null.

look ok?",05/Aug/09 13:19;euphoria;Yes in both cases if the presumptions are true.,05/Aug/09 13:40;jbellis;committed,"06/Aug/09 13:05;hudson;Integrated in Cassandra #159 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/159/])
    fix more findbugs complaints.  patch by Michael Greene; reviewed by jbellis for 
",07/Aug/09 22:43;dehora;Fixes two bad classcasts in StorageService. But I'm wondering if this hasn't blown up to date whether the path ever gets executed?,07/Aug/09 22:47;dehora;Fixes two bad classcasts in StorageService.,"07/Aug/09 23:27;jbellis;applied (minus the unit tests -- the effort is appreciated, but it doesn't actually exercise any code in StorageService), thanks!

> I'm wondering if this hasn't blown up to date whether the path ever gets executed? 

It isn't.  Sandeep, is that code fragment (StorageService.forceHandoff) going to be useful at all for CASSANDRA-195?  If not let's nuke it.","08/Aug/09 12:35;hudson;Integrated in Cassandra #161 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/161/])
    fix broken casts.  patch by Bill de hOra; reviewed by jbellis for 
",09/Aug/09 02:49;jbellis;removing 0.4 tag since it's more open-ended than that,09/Aug/09 18:20;eribeiro;Nice to know that findbugs is being used to check the code base cleanness. I've posted and patched some issues discovered by findbugs in the past.,09/Nov/09 18:56;jbellis;should this be closed?,"09/Nov/09 20:00;euphoria;I have another round of a few that we can add in, but after that I
think it should.  There are always going to be small things to fix
that can be found by static analysis, but most of the things it picks
up now are out of our control.

","09/Nov/09 20:04;jbellis;ok, moving to release-after-0.5 so i remember to bug you in that time frame :)",09/Dec/09 01:40;jbellis;closing in favor of Stu's more recent findbugs tickets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RecoveryManager should ignore hidden files (like .DS_Store) created by some operating systems.,CASSANDRA-503,12438716,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,10/21/2009 15:20,3/12/2019 14:20,3/13/2019 22:24,10/21/2009 16:07,0.5,,,,0,,,,,,RecoveryManager should ignore hidden files (like .DS_Store) created by some operating systems.,,,,,,,,,,,,,,,,,,,,21/Oct/09 15:22;gdusbabek;cassandra-503-ignore_hidden_in_commit_logs.diff;https://issues.apache.org/jira/secure/attachment/12422811/cassandra-503-ignore_hidden_in_commit_logs.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,07:53.0,,,no_permission,,,,,,,,,,,,19726,,,Thu Oct 22 12:34:39 UTC 2009,,,,,,0|i0fzbz:,91322,,,,,,,,,,,21/Oct/09 15:22;gdusbabek;This patch makes it so hidden files are ignored.,"21/Oct/09 16:07;jbellis;committed, w/ braces changed to match cassandra code style (http://wiki.apache.org/cassandra/CodeStyle)","22/Oct/09 12:34;hudson;Integrated in Cassandra #235 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/235/])
    ignore hidden files in commitlog directory (useful for users on OS X, which likes to add files like "".DS_Store"").  patch by gdusbabek; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AntiEntropyService causes lots of errors to be logged during unit tests,CASSANDRA-657,12444335,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,jbellis,jbellis,12/29/2009 21:33,3/12/2019 14:20,3/13/2019 22:24,1/22/2010 19:03,0.6,,,,0,,,,,,"Some examples from a successful test run on trunk:

java.io.IOException: Reached an EOL or something bizzare occured. Reading from: /127.0.0.1 BufferSizeRemaining: 16
	at org.apache.cassandra.net.io.StartState.doRead(StartState.java:44)
	at org.apache.cassandra.net.io.ProtocolState.read(ProtocolState.java:39)
	at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:96)
	at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:444)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

ERROR [TCP Selector Manager] 2009-12-29 16:27:53,244 TcpConnection.java (line 363) Encountered IOException on connection: java.nio.channels.SocketChannel[closed]
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
	at org.apache.cassandra.net.TcpConnection.connect(TcpConnection.java:341)
	at org.apache.cassandra.net.SelectorManager.doProcess(SelectorManager.java:143)
	at org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:107)

It's not obvious to me if these are errors in AES, errors in the streaming code, errors caused because the test environment insufficiently resembles a live one, or errors because the test is asking the streaming code to do something that doesn't make sense, but AES is definitely at least immediately responsible.",,,,,,,,,,,,,,,,,,,,22/Jan/10 04:43;stuhood;657-short-lived-connections-for-streaming.patch;https://issues.apache.org/jira/secure/attachment/12431095/657-short-lived-connections-for-streaming.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,49:28.4,,,no_permission,,,,,,,,,,,,19805,,,Sat Jan 23 12:35:50 UTC 2010,,,,,,0|i0g0a7:,91476,,,,,,,,,,,"29/Dec/09 22:49;stuhood;Ah yea. I've seen this one before: it seems to happen after successful streaming sessions, but because streaming is a bit of a black box, I've never looked into it too deeply. We might want to make this depend on CASSANDRA-579, since changes there will likely expose these inconsistencies.","21/Jan/10 23:07;stuhood;Hmm, I haven't checked trunk, but this might have been fixed by #705. Mostly making this comment as a reminder to myself.","21/Jan/10 23:10;jbellis;i checked, and the errors are slightly different (duh :) but still present.","22/Jan/10 04:43;stuhood;The receive loop in IncomingTcpConnection expects that any type of connection will be long lived, but streaming connections only send a single header.","22/Jan/10 05:14;jbellis;I'd rather just let it EOF; that is less fragile if we decide to pool streaming connections later.

We can move the log message to trace if it bothers you. :)",22/Jan/10 19:03;jbellis;moved to trace in r902220,"23/Jan/10 12:35;hudson;Integrated in Cassandra #332 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/332/])
    move eofexception ST down to trace.  patch by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper convicts same node over and over.,CASSANDRA-695,12445431,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,1/13/2010 19:35,3/12/2019 14:20,3/13/2019 22:24,1/21/2010 22:23,0.6,,,,0,,,,,,"I can reproduce this in trunk or 0.5.  It is quite easy to see with logging set to DEBUG.
1.  Bring up several nodes.
2.  Kill one of them.

Gossip still continues to examine the failed node because it is in the ring, but gets convicted over and over at every check (see FailureDetector.interpret).  If this is expected, we should consider lowering the debug statement in MessagingService.convict to TRACE.",,,,,,,,,,,,,,,,,,,,21/Jan/10 22:20;gdusbabek;695.patch;https://issues.apache.org/jira/secure/attachment/12431073/695.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,46:05.6,,,no_permission,,,,,,,,,,,,19824,,,Fri Jan 22 12:36:21 UTC 2010,,,,,,0|i0g0if:,91513,,,,,,,,,,,"21/Jan/10 19:46;jbellis;From my reading this is Works As Intended.  Lowering level to trace is fine with me.  Go ahead and commit that, Gary.",21/Jan/10 22:23;gdusbabek;901893. Patch by Gary Dusbabek.,21/Jan/10 22:30;jbellis;+1,"22/Jan/10 12:36;hudson;Integrated in Cassandra #331 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/331/])
    lower MS.convict debug statement to trace. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor SliceRange documentation fix,CASSANDRA-1085,12464395,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yosh,yosh,yosh,5/12/2010 21:56,3/12/2019 14:20,3/13/2019 22:24,5/17/2010 14:39,0.6.2,,Legacy/Documentation and Website,,0,,,,,,"Minor doc typo, patch attached",,,,,,,,,,,,,,,,,,,,12/May/10 21:58;yosh;doc-fix.patch;https://issues.apache.org/jira/secure/attachment/12444348/doc-fix.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,39:19.8,,,no_permission,,,,,,,,,,,,19988,,,Mon May 17 14:39:19 UTC 2010,,,,,,0|i0g2wn:,91901,,,,,,,,,,,12/May/10 21:59;yosh;Patch applies to both trunk and cassandra-0.6 without trouble.,17/May/10 14:39;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Command line arguments inversion in clustertool,CASSANDRA-942,12460956,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,4/1/2010 16:36,3/12/2019 14:20,3/13/2019 22:24,4/2/2010 21:42,0.6.1,,Legacy/Tools,,0,,,,,,"The arguments (table and key) of the get_endpoints command of clustertool are in the wrong order
(the call to getNaturalEndpoints also have it's argument in the wrong order so it kinda works in the end
but the printing of the 'Key' is wrong.
",,,,,,,,,,,,,,,,,,,,01/Apr/10 16:37;slebresne;ClusterCmdArgumentsError.diff;https://issues.apache.org/jira/secure/attachment/12440523/ClusterCmdArgumentsError.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,42:39.2,,,no_permission,,,,,,,,,,,,19930,,,Fri Apr 02 21:42:39 UTC 2010,,,,,,0|i0g213:,91759,,,,,,,,,,,"01/Apr/10 16:37;slebresne;The patch also make it so that if it misses an argument, it crashes 
gracefully",02/Apr/10 21:42;urandom;committed w/ minor modifications; thanks Sylvain!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"config file option DiskAccessMode has no-op option ""mmap_index_only""",CASSANDRA-1241,12468307,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,rcoli,rcoli,6/30/2010 23:35,3/12/2019 14:04,3/13/2019 22:24,8/13/2010 17:12,0.7 beta 2,,Legacy/Documentation and Website,,0,,,,,,"Per http://wiki.apache.org/cassandra/StorageConfiguration :
""
Access mode. mmapped i/o is substantially faster, but only practical on a 64bit machine (which notably does not include EC2 ""small"" instances) or relatively small datasets. ""auto"", the safe choice, will enable mmapping on a 64bit JVM. Other values are ""mmap"", ""mmap_index_only"" (which may allow you to get part of the benefits of mmap on a 32bit machine by mmapping only index files) and ""standard"". (The buffer size settings that follow only apply to standard, non-mmapped i/o.)
""

The actual code referring to ""mmap_index_only"" is in src/java/org/apache/cassandra/config/DatabaseDescriptor.java :
""
 public static enum DiskAccessMode {
        auto,
        mmap,
        mmap_index_only,
        standard,
    }
...
    private static DiskAccessMode diskAccessMode;
    private static DiskAccessMode indexAccessMode;
...
            if (diskAccessMode == DiskAccessMode.auto)
            {
                diskAccessMode = System.getProperty(""os.arch"").contains(""64"") ? DiskAccessMode.mmap : DiskAccessMode.standard;
                indexAccessMode = diskAccessMode;
                logger.info(""Auto DiskAccessMode determined to be "" + diskAccessMode);
            }
            else if (diskAccessMode == DiskAccessMode.mmap_index_only)
            {
                diskAccessMode = DiskAccessMode.standard;
                indexAccessMode = DiskAccessMode.mmap;
            }
            else
            {
                indexAccessMode = diskAccessMode;
            }
""

As indicated by this snippet, IndexAccessMode is set to ""mmap"" if ""mmap_index_only"" is set in the conf file. However it does not appear that IndexAccessMode or getIndexAccessMode() are used by any other cassandra code.

""
~/repos/cassandra$ grep -ri indexAccessMode .
./src/java/org/apache/cassandra/config/DatabaseDescriptor.java:    private static Config.DiskAccessMode indexAccessMode;
./src/java/org/apache/cassandra/config/DatabaseDescriptor.java:                indexAccessMode = conf.disk_access_mode;
./src/java/org/apache/cassandra/config/DatabaseDescriptor.java:                indexAccessMode = Config.DiskAccessMode.mmap;
./src/java/org/apache/cassandra/config/DatabaseDescriptor.java:                indexAccessMode = conf.disk_access_mode;
./src/java/org/apache/cassandra/config/DatabaseDescriptor.java:    public static Config.DiskAccessMode getIndexAccessMode()
./src/java/org/apache/cassandra/config/DatabaseDescriptor.java:        return indexAccessMode;
""

If I understand the code correctly, this means that setting DiskAccessMode to ""mmap_index_only"" is functionally the same as setting it to ""standard."" As people might be tempted to try/test ""mmap_index_only"" as a mode in-between ""standard"" and ""mmap"" in order to mitigate concerns about https://issues.apache.org/jira/browse/CASSANDRA-1214, it would probably be good to either complete the feature or remove the configuration option. I am willing to submit a patch for the latter and fix the docs if that's the decision.
",,,,,,,,,,,,,,,,,,,,13/Aug/10 16:23;jbellis;1241.patch;https://issues.apache.org/jira/secure/attachment/12452034/1241.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,23:21.2,,,no_permission,,,,,,,,,,,,20047,,,Sat Aug 14 12:48:32 UTC 2010,,,,,,0|i0g3uv:,92055,stuhood,stuhood,,,,,,,,,13/Aug/10 16:23;jbellis;patch to restore use of mmap_index_only option,13/Aug/10 16:49;stuhood;+1,13/Aug/10 17:12;jbellis;committed,"14/Aug/10 12:48;hudson;Integrated in Cassandra #514 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/514/])
    restore use of mmap_index_only option.  patch by jbellis; reviewed by Stu Hood for CASSANDRA-1241
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove assumption that Key to Token is one-to-one,CASSANDRA-1034,12463287,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,stuhood,stuhood,4/29/2010 3:21,3/12/2019 14:04,3/13/2019 22:24,12/1/2011 10:14,1.1.0,,,,5,,,,,,"get_range_slices assumes that Tokens do not collide and converts a KeyRange to an AbstractBounds. For RandomPartitioner, this assumption isn't safe, and would lead to a very weird heisenberg.

Converting AbstractBounds to use a DecoratedKey would solve this, because the byte[] key portion of the DecoratedKey can act as a tiebreaker. Alternatively, we could make DecoratedKey extend Token, and then use DecoratedKeys in places where collisions are unacceptable.",,,,,,,,,,,CASSANDRA-1600,,,,,CASSANDRA-1205,,,,23/Nov/11 16:07;slebresne;0001-Generify-AbstractBounds-v2.patch;https://issues.apache.org/jira/secure/attachment/12504886/0001-Generify-AbstractBounds-v2.patch,30/Nov/11 10:44;slebresne;0001-Generify-AbstractBounds-v3.patch;https://issues.apache.org/jira/secure/attachment/12505606/0001-Generify-AbstractBounds-v3.patch,18/Nov/11 14:20;slebresne;0001-Generify-AbstractBounds.patch;https://issues.apache.org/jira/secure/attachment/12504214/0001-Generify-AbstractBounds.patch,23/Nov/11 16:07;slebresne;0002-Remove-assumption-that-token-and-keys-are-one-to-one-v2.patch;https://issues.apache.org/jira/secure/attachment/12504887/0002-Remove-assumption-that-token-and-keys-are-one-to-one-v2.patch,30/Nov/11 10:44;slebresne;0002-Remove-assumption-that-token-and-keys-are-one-to-one-v3.patch;https://issues.apache.org/jira/secure/attachment/12505607/0002-Remove-assumption-that-token-and-keys-are-one-to-one-v3.patch,18/Nov/11 14:20;slebresne;0002-Remove-assumption-that-token-and-keys-are-one-to-one.patch;https://issues.apache.org/jira/secure/attachment/12504215/0002-Remove-assumption-that-token-and-keys-are-one-to-one.patch,23/Nov/11 16:07;slebresne;0003-unit-test-v2.patch;https://issues.apache.org/jira/secure/attachment/12504888/0003-unit-test-v2.patch,30/Nov/11 10:44;slebresne;0003-unit-test-v3.patch;https://issues.apache.org/jira/secure/attachment/12505608/0003-unit-test-v3.patch,18/Nov/11 14:20;slebresne;0003-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12504216/0003-unit-test.patch,09/Nov/10 05:19;tjake;1034_v1.txt;https://issues.apache.org/jira/secure/attachment/12459138/1034_v1.txt,10/Oct/11 20:42;xedin;CASSANDRA-1034.patch;https://issues.apache.org/jira/secure/attachment/12498465/CASSANDRA-1034.patch,,,11,,,,,,,,,,,,,,,,,,,12:36.1,,,no_permission,,,,,,,,,,,,14985,,,Thu Dec 01 10:14:43 UTC 2011,,,,,,0|i0g2lj:,91851,jbellis,jbellis,,,,,,,,,"29/Apr/10 04:12;jbellis;The problem is that we are using DK both for routing and for local key sorting, partly because it's very convenient to be able to use the ""natural"" compareTo to compare those two kinds of DK.

If the only place we have DK with null key is for the routing case, then the right thing is to convert those usages to raw Tokens and make key non-optional in DK.

i have a nagging feeling that there are more complications though.","29/Apr/10 04:13;stuhood;MD5 collisions are rare enough, so somebody would probably have to write their own partitioner to trigger this.","09/Nov/10 04:54;jbellis;bq. it's very convenient to be able to use the ""natural"" compareTo to compare those two kinds of DK

In particular, we generate (Token, null) DKs for range scans, at least in part because Hadoop thinks in terms of TokenRanges instead of DecoratedKeyRanges.  (Presumably it is still ok to assume that a partitioner generates many more tokens than there are nodes in the cluster; if not, this would need to change.)

We might be able to still do this, if we just say that DK(T, null) always sorts before DK(T, non-null-key) for any given Token T.

I still suspect we're using DK in some places where Token is all we really need.",09/Nov/10 05:19;tjake;Discovered the same issue with a partitioner that shares tokens for many keys.  This patch fixes the issue. all tests pass.,"09/Nov/10 16:14;stuhood;Jake's patch is only a partial fix for this problem, so I've moved it to #1720. The core of this ticket is either: changes to the class hierarchy, or changes to ranges.","10/Nov/10 19:07;jbellis;Can you elaborate as to what else needs to be fixed?

As I said above, ""Presumably it is still ok to assume that a partitioner generates many more tokens than there are nodes in the cluster"" so I don't think we need to make Range a DK pair for granularity's sake.","10/Nov/10 19:52;stuhood;> Can you elaborate as to what else needs to be fixed?
I guess the larger problem here is that if a range query asks for 10 rows using a Token range, but there are 1000 rows sharing a particular token, which 10 rows do you return?","10/Nov/10 21:13;jbellis;Okay, so that is a problem because we create DK(Token, null) internally for range queries currently even when using the key-based API.

Once that is fixed I'm fine with saying ""the first 10"" (or if more convenient, ""that's undefined""), since only Hadoop or similar iterate-over-everything approaches should be using Token-based range queries at the API level.  (Again, I'm assuming that there are enough tokens to achieve sufficient granularity, iow, that the number of rows sharing a token is less than the InputSplit size.)","03/Mar/11 19:12;slebresne;Attaching a patch that I believe solves this. It makes Range accept both Token and DecoratedKey and makes those two compare together correctly.

It introduces a new marker interface (RingPosition) instead of making DecoratedKey extends Token (for reason explained in the comments of RingPosition but to sum up: I think it's cleaner).

The second patch attached is just a stupid partitioner that use for token the length of the key. It's just for testing and not meant for inclusion. But this shows that with the first patch, you can do correct range query that go from 'the middle of a token' to the 'middle of another one'.

An important note is that this breaks the serialization unit tests, because now an AbstractBounds can use decoratedKeys, and thus serialized AbstractBounds are incompatible with previous version. Not sure how to deal with that though, I though we had a plan for dealing with that but I'll admit I don't remember the details.",16/Mar/11 14:53;slebresne;Patch rebased,24/Mar/11 09:47;slebresne;Rebased patch with code for binary backward compatibility. This still needs the first part of CASSANDRA-2361 to fully pass the serialization unit tests.,25/Mar/11 17:43;jbellis;What is LengthPartitioner for?,"25/Mar/11 18:22;slebresne;Oh, it's just a stupid partitioner with tons of collision (and predictable ones) that I used for testing and attached so that other can test too. Not meant for inclusion.","25/Mar/11 18:58;jbellis;Initial feedback:

- I'm a fan of the RingPosition approach
- Less of a fan of pretending that Tokens and DK are equal if the token component of DK is equal.  Shouldn't we force caller to ask Token.equals(DK.token) if that's what they mean? As you pointed out in RP docstring, there is not an is-a relationship there.
- Should we add RP.isToken to encapsulate RP.asDecoratedKey.key == null checks?
- DK docstring is obsolete now
","25/Mar/11 19:24;slebresne;bq. Less of a fan of pretending that Tokens and DK are equal if the token component of DK is equal. Shouldn't we force caller to ask Token.equals(DK.token) if that's what they mean? As you pointed out in RP docstring, there is not an is-a relationship there.

The thing is, we need them to be equal for compareTo() (because we can't have token > keys nor token < keys, otherwise that would mess up our ranges). Then for the equals, the motivation is summed up by the Comparable documentation:
{noformat}
It is strongly recommended (though not required) that natural orderings be consistent with equals. This is so because sorted sets (and sorted maps) without explicit comparators behave ""strangely"" when they are used with elements (or keys) whose natural ordering is inconsistent with equals. In particular, such a sorted set (or sorted map) violates the general contract for set (or map), which is defined in terms of the equals method.
{noformat}
And I do fear that we would get something inconsistent at some point.
But I'm not a super fan either, just felt the less evil of the two choices.
I'm happy with suggestion though and I'll work out the other remarks.

","25/Mar/11 19:39;jbellis;I understand the Comparable docs, but 
- that's primarily concerned w/ compareTo + equals b/t members of the same class
- it's valid to say ""these are tied for sorting purposes, and yet they are not equal""

In other words I'm more worried about subtle bugs if we allow the equals, than if we don't. :)

The Map example is a good one -- if I set

map[token(1)] = foo
map[dk(1, 1)] = bar

I would expect two map entries, not one.  (If you want one, you explicitly use asToken, then there is no ambiguity.)

How about if we add an assert to both equals to make sure we don't pass in the other kind of object?","25/Mar/11 19:44;slebresne;You're right, I'm convinced, it's probably safer to have equals be a true equals.
I'll do the change.","26/Mar/11 10:13;stuhood;I was reaaally hoping we could subclass here... adding RingPosition leads to explicit conversions scattered all over that end up obscuring  implicit conversions.

The hairiest part of subclassing would be renaming all of our Token subclasses with DecoratedKey subclasses, but it cleans up unnecessary references: for example, for a DecoratedKey for ByteOrderPartitioner or LocalPartitioner you have:
{code}
DecoratedKey
   BytesToken token;
      ByteBuffer token;
   ByteBuffer key;
{code}
... while with subclassing you could save two object references:
{code}
DecoratedKey
   ByteBuffer keyAndToken;
{code}

Also, the Comparable dilemma is relatively straightforward with subclassing: Token implements Comparable<Token>, the subclasses override, call super.compare, and if their superclass is equal, fall back to instanceof(myclass) to see whether they can compare the key data.","30/Mar/11 15:21;slebresne;I realize that this is a little more subtle than I first though.

You just cannot compare a Token and a DecoratedKey simply, because a Token is actually a range of keys. Hence dealing with a Range that mixes Token and DecoratedKey correctly is doable, but a bit complicated (typically, it involves declaring multiple different comparison functions). To take quick example, consider that if you mix DK and Token, you must have the following that stands:
{noformat}
    (T(2), T(8)] should not contain DK(T(2), ""foo"") => DK(T(2), ""foo"") < T(2)
    [T(2), T(8)] should contain     DK(T(2), ""foo"") => DK(T(2), ""foo"") >= T(2)
{noformat}
So there is no way to write a compareTo() function dealing with both DK and token.

So I think that it will be simplest to not mix DK and Token in the same ranges. We'll have ranges of Token (for everything related to ring management) and ranges of DK (for rangeSlice and scan). This is what the patch does (and the patch 'generify' AbstractBounds, Range and Bounds (a fair part of the patch) to keep type information around and avoid unnecessary casts all over the place).

We still want to make a rangeSlice/scan over a range of token. To do that, we simply convert a range of Token to a range of DK. This involves declaring for a given token a smallest key and biggest key, and this in turn comes a slight complication related to the minimum token, but the detail are in the docstrings of the patch. I am reasonably confident on that new patch.

(Note that this patch is much bigger than the previous one, but this is mostly due to the generification of Range)",30/Mar/11 18:57;jbellis;Can you break the generification out into a separate patch?,01/Apr/11 17:08;slebresne;Generification broken into a separate patch + some tiny code style update,04/Apr/11 16:16;slebresne;Attaching v2 for my second patch. There was some failure in the unit tests for getRestrictedRanges. This fix and improves those test and fix a small bug related to the handling of the minimum value for DecoratedKey.,"04/Apr/11 16:18;jbellis;I think we are almost done.  A couple comments:

- DK.isEmpty seems like a bad method name for a Key object -- intuitively, keys are a specific point and should not contain other points except for the obvious identity case.  Would isMinimum be a better name? 
- I don't understand RP.toSplitValue or why DK would throw away information, when calling it.  More generally, I'm unclear why we would have null keys in DK -- shouldn't you use a Token, if you don't have key information?
- using MINIMUM_TOKEN for both sort-before-everything and sort-after-everything values has always been confusing.  Should we introduce a MAXIMUM_TOKEN value to clear that up?","04/Apr/11 21:44;slebresne;Reattaching v2, previous had a stupid mistake, sorry about that.","07/Apr/11 16:24;slebresne;bq. DK.isEmpty seems like a bad method name for a Key object – intuitively, keys are a specific point and should not contain other points except for the obvious identity case. Would isMinimum be a better name?

Actually I don't even like isEmpty for token, so in favor of isMinimum for both DK and token.

bq.  don't understand RP.toSplitValue or why DK would throw away information, when calling it. More generally, I'm unclear why we would have null keys in DK – shouldn't you use a Token, if you don't have key information?

Current patch don't allow to mix token and DK in a range/bounds (because that comes with its whole sets of complications). However getRestrictedRange must be able to break a range of DK based on a node token. So RP.toSplitValue() returns for a given token the value that splits the range: for a token range it's the token itself, but for a DK range, it's the largest DK having this token.
The null keys is related: even though we don't mix DK and token in range, we need to be able to have a range of DK that includes everything from x token to y token. Hence, for a given token t, we need two DK: the smallest DK having t and the biggest DK having t. In the patch, slightly but not totally randomly, I use DK(t, EMPTY_BB) for the smallest key and DK(t, null) for the biggest one, hence the ""need"" for null keys. 

bq. using MINIMUM_TOKEN for both sort-before-everything and sort-after-everything values has always been confusing. Should we introduce a MAXIMUM_TOKEN value to clear that up?

I think that would make wrapping stuffs more complicated. Because then what would be the difference between the following ranges: (MIN, MIN], (MAX, MAX], (MIN, MAX] and (MAX, MIN]. For DK, the code is already enforcing that the we only have one minimum key (that is DK(MIN, EMPTY_BB)) and never ever use DK(MIN, null) because that poses problems. I think a MAX token would make that worst. ","08/Apr/11 20:55;jbellis;bq. RP.toSplitValue() returns for a given token the value that splits the range: for a token range it's the token itself, but for a DK range, it's the largest DK having this token. The null keys is related: even though we don't mix DK and token in range, we need to be able to have a range of DK that includes everything from x token to y token

This feels messy and error-prone to me. I wonder if we haven't found the right approach yet.","09/Apr/11 20:07;stuhood;I agree that using null is a necessary solution here: you need a max value for keys, since they are essentially the ""child"" of a one-token-range. The key range is bounded (since it has parents), but the token range is not, so I agree with sylvain that a MAX_TOKEN is probably not necessary.

One way to remove toSplitValue would be to use DecoratedKey everywhere; DecoratedKey is a compound of the Token and the key blob. The equivalent of today's Token is a DecoratedKey for that token with a null key: it compares greater than all valid child keys, so it contains them.

I hope that it won't muddy the water, but the <empty> as min and <null> as max approach is the same one I took forthe first cut of the file-format, and it worked very well. You can use the min/max values to find the beginning or end of a child range. See [ColumnKey.java|https://github.com/stuhood/cassandra-old/blob/674/src/java/org/apache/cassandra/db/ColumnKey.java#L225]

EDIT: Actually... I'm not so sure about not having MAX_TOKEN... it might actually clean things up quite a bit. Any time a range ends with what use to be the min token, you can make a direct translation to MAX_TOKEN.","09/Jun/11 11:01;slebresne;Patch rebased, this is against trunk.","09/Jun/11 11:32;slebresne;bq. One way to remove toSplitValue would be to use DecoratedKey everywhere;

I'm not saying it's not possible, but I think this is overkill (in the changes it involves). Moreover, all the code that deals with topology really only care about token. That's the right abstraction for those part of the code. So I really (really) doubt using decorated key everywhere would be cleaner. Of course, anyone is free to actually do the experiment and prove me wrong. I also don't think it would remove the need for splitValue, it would just maybe call it differently.

bq. The equivalent of today's Token is a DecoratedKey for that token with a null key

This is only true today because we assume key and token are one-to-one. The goal is to change that. If multiple keys can have the same token (by definition the token is really the hash of a key), then the statement above is false. If a token correspond to an infinite set of key (with is the case with md5 btw, we just ignore it), then replacing a token by given key *cannot* work.

Overall, it could be that there is better way to do this, but having spend some time on this, I have a reasonable confidence on that it fixes the issue at hand without being too disruptive (which is not saying there isn't a few points here and there that couldn't be improved).",20/Aug/11 22:08;michaelsembwever;What's the status on this? This issue and its relations back to CASSANDRA-2878 are the only reason we're using OPP. I suspect other users setup with both cassandra and hadoop (or brisk) could be in the same boat. Not only does OPP leave an unbalanced ring (i've had a case where all data went to one node because the keys/tokens were longer than normal) it leaves poor performance to hadoop jobs as tasks requirement on data locality has become stricter (w/ CASSANDRA-2388). Apart from the plain preference to be using secondary indexes over OPP.,"21/Aug/11 04:20;jbellis;Status is, I'm hoping that someone comes up with a fix that doesn't look error prone.  Otherwise we'll probably end up with merging Sylvain's solution for 1.1.","03/Oct/11 14:08;xedin;I'm thinking of making Token an interface and implementing two classes RoutingToken(token) and QueryToken(token, key) so all current token implementations LocalToken, StringToken, BytesToken, BigIntegerToken are going to extend QueryToken. RoutingToken is going to be used for operations where we don't need a key - bootstrap, midpoint calculation, TokenMetadata class; QueryToken is going to be a replacement for DK, that will allow us to remove DK completely and operate only on the token basis. Thoughts?","03/Oct/11 14:23;slebresne;bq. Thoughts?

From your comment only I don't see right away what you are proposing besides a renaming of Token -> RoutingToken and DecoratedKey -> QueryToken.","10/Oct/11 20:42;xedin;Patch removes DK and IPartitioner.decorateKey(ByteBuffer key), which is replaced by IPartitioner.getToken(ByteBuffer key), Token now takes second parameter - ByteBuffer key. Most of the patch are replacements for DK -> Token and decorateKey -> getToken. All tests (test, test-compression, long-test) pass.

Rebased with the latest trunk (last commit 7624536ae7fea52bcf761c7dea212fe12d2f4586)","10/Oct/11 20:57;tjake;At first glancei  like this because it makes the Token first class and the key not required. cleaning up the code below.

{code}
-        DecoratedKey startWith = new DecoratedKey(range.left, null);
-        DecoratedKey stopAt = new DecoratedKey(range.right, null);
+        Token startWith = range.left;
+        Token stopAt = range.right;
{code}","11/Oct/11 11:42;slebresne;I have 2 major problems with that patch:

The first one is I really dislike the idea of merging DK into Token (I disliked the idea of merging Token into DK and that roughly the same idea).  First, I fail to see how this is of any help in solving what this ticket is trying to solve. Second, I think it's a very bad use of types. A Token is not a Key. By merging those together we just weaken our type hierarchy, thus getting less insurance from types. Typically, with this patch, a function that really want a DK, could get a Token, i.e getKey() is not guaranteed to return a valid key. Now I know, we are already using 'false' DK by feeding null as a key sometimes. Well, that is ugly and error prone. I don't think generalizing this everywhere while introducing a 300K patch is the right way to go, quite the contrary. Besides, it's inefficient. All the places were we do use only a Token, we'll now have a bigger structure with a useless pointer to the EMPTY_BYTE_BUFFER (granted this has probably little impact, but it's another sign that it's doing it wrong).

The second problem is this doesn't work. This DK->Token really just muddy the water but it doesn't solve anything. What we want is to fix the fact that the code identifies token and keys as a one to one mapping. In particular, this is forced in DK.compareTo(), which only compare the tokens, ignoring the keys.  Fixing that place is easy, and the patch does it, but it's really just a few lines change.

The real problem is that the code make the assumption that key <-> token is one to one in other places. So making DK.compareTo takes key into account breaks other parts. For instance, in RowIteratorFactory, we have this:
{noformat}
return startWith.compareTo(row.getKey()) <= 0
       && (stopAt.isEmpty() || row.getKey().compareTo(stopAt) <= 0);
{noformat}
and say that startWith and stopAt are token only. The semantic is that this is supposed to be inclusive on both bound. With the last patch, this would include keys having the startWith token, but *not* the ones having stopAt as token, because in the patch, a token compares strictly before all of the key having this token (concretely, the attached patch skips keys during range queries).

And this is not the only places in the code where this problem manifest, because this is the symptom of a larger problem. If more than one key can have the same token, then tokens are a range of keys.
If you ask for the range of tokens [1, 4], then you expect that it will return all the keys having token 1, 2, 3 and 4. That excludes having a token comparing strictly before all the keys having this token (or having it compare strictly after all the keys having it as token for that matter). Merging Token and DK just doesn't work.

At the risk of sounding cocky, I really encourage people to have another look at my patch. I do believe that once you've realized what solving this problem entails, it's a solution that strike a reasonable balance in fixing the problem without a entire rewrite of Cassandra.
","11/Oct/11 20:50;tjake;@Sylvain This is all really confusing and I agree the core of the ticket is to make key->token 1:1

The core of the problem initially was explained in CASSANDRA-1733

bq. A Range object (which Hadoop splits generate) is start-exclusive. A Bounds object (which normal user scan queries generate) is start-inclusive.

So by making Token the only way to deal with keys it feels like a more consistent api.  Since Key can be null it needs to be Token that becomes the primary internal class. 

In your impl we now have DK, Token, RingPosition which too me is more confusing than having one Token class.


","12/Oct/11 10:43;slebresne;I'm not sure I'm am completely clear, so allow me to try to improve that. I think there is two issues with the last patch that are largely orthogonal:
  # the patch is broken (again, this is largely not related to the shoving of DK into Token)
  # I believe shoving DK into Token is a bad, error-prone idea that have no tangible advantages

But let's me first focus on the first issue, if only because it involves no opinion whatsoever: I'm either right or wrong that it's broken (but i'm pretty sure I'm right). So let's be concrete and take an example.

The patch ""merges"" Token and DecoratedKey together, so let me take the following notation:
  * t(1, a) for what is the DecoratedKey a of token 1 in current but is is just a Token instance in the patch
  * t(1) for the 'pure' token 1. In other word it's a shortcut for t(1, EMPTY_BYTE_BUFFER) in the attached patch and correspond to just a Token in the current code.
(as a side note, the fact that I have to speak of DecoratedKey and 'pure' token to explain is imo yet another sign than melting everything into Token is a bad idea but I'm diverging)

Since when Token are compared, the token is compared then the key is on token equality, we have t\(n) < t(n, k) whatever the token n and key k are (since t\(n) is t(n, EMPTY_BYTE_BUFFER) and EMPTY_BYTE_BUFFER < k for any valid key k) .

Let's now take an example of multiple keys having the same token and say that I have the following keys in my cluster:
{noformat}
tokens |   1   |     2     |   3
keys   | a | b | c | d | e | f | g
{noformat}
In other words, a and b have the same token 1; c, d and e have the same token 2; ...

The goal for this ticket is to support that situation correctly. Sor for instance, we should have that:
   range_slice(start=t(1), end=t(3)) returns c, d, e, f and g
(because range_slice with tokens is start exclusive).  However, with the attached patch:
   range_slice(start=t(1), end=t(3)) will return a, b, c, d and e

The reason is fairly simple: we have that t(1) < t(1, k) for any k and t(3) < t(3, k) for any k.

Another way to put it is that it breaks our token ranges: if you have a node that owns Range(t(1), t(3)), it's supposed to not contains any key with token 1 and all keys with token 3, but it fails at both.

So it's broken. Now there is something we could be tempted to do. We could make it so that t\(n) > t(n, k) for any token n and any key k. But in turn that would break Bounds (i.e, start inclusive) of 'pure' tokens. I.e, Bounds(t(1), t(2)) is supposed to include all keys with token 1, but if t(1) > t(1, k) for any key k, it won't include it.

One could argue however that this is still solution because I *think* that right now we never really use a Bounds of 'pure' tokens (more precisely, the current code does it, but only in place where we are actually doing a range slice between keys). And I *think* that functions that take a startKey, when fed a 'pure' token only do start exclusive. So I suppose we could assert that we never create Bounds of Token and put some assert here and there (in SSTableReader.getPosition() for instance) and go with that.

But imho this is a bad idea. Because it's fragile and because this is ignoring a problem that may screw us up later. Why not fix it the right way now? What if tomorrow we do want to be able to query all the keys having a given token ? That is, what if we want to query [t(1), t(1)] ? We would not be able to, because if t(1) > t(1, k) for any k, then [t(1), t(1)] don't include anything.

Again, all this is because a token actually correspond to a set of keys (once you acknowledge multiple keys can have the same token), and so if you want to do things correctly, you need for a given token n to have a representation for both:
  * the smallest key having token n
  * the greatest key having token n

With that, you can query all the keys having token n. Without, you can't. That is what my patch does and I believe fairly strongly is the right way to do it.


Alright, that the first thing that a patch to this ticket must deal with. Then there is another thing: the current code only allow for AbstractBounds of Token (by typing), but we want for this patch that once you do a range_slice query with at startKey and endKey, you get a range of keys in ColumnFamilyStore.getRangeSlice(), so that you can precisely answer those queries. That means we must be able to construct AbstractBounds with keys in them. Note that it's basically just a typing problem.

The answer to that of this patch is show DK into Token. So yeah, it fixes that problem, but what I'm arguing is that:
  * It's error-prone and make coding *more* complicated. We're merging object that are not the same thing. Today if a methods takes a Token, you know it won't do anything at the granularity of keys (well today Token and keys have the same granularity but this ticket is supposed to change that). You lose that information if you merge DK and Token. And if a method takes a DecoratedKey, you know that it doesn't expect a Token (more precisely, Typing ensures it). Sure, we do already use a trick in a few places where we create 'fake' DK(null, key). But at the very least, when we do that, we know we're doing something weird, and we are extra careful that methods we call on that fake DK handle that case correctly. If everything is Token, now the signature for a lot of method will suggest it is ok to give a 'pure' Token. So what, all method should defensively assert this is not the case ? This is what types are for.
  * It's a ~300K patch. Sure it's mostly simple changes, but it's still that many changes that could introduce a typo somewhere that causes a bug.
  * It's a tad less efficient because each time we really only care about 'pure' Token (and there is quite a bunch of code that does that), we would allocate a slightly larger structure for no reason. And I'm pretty sure god kills a kitten each time you do that.

The solution to that very same type problem I'm proposing (in my patch) is instead simply to generalize AbstractBound slightly so you can have both AbstractBound of Token and of DecoratedKey. That sound very reasonable to me.  After all we should be able to have AbstractBounds of anything that implements Comparable right ? Well, as it turns out our implementation of AbstractBound needs a little more than that (because our ranges wraps, we need Comparable but with a minimum value for instance) and that is what RingPosition is for.  But it's only a marker interface, and if you look at the code it's actually used in a small number of places, so I admit I fail to see how this make thing much more complicated.","12/Oct/11 11:14;xedin;Can you please define what do you mean by ""pure token""? Aren't we supposed to generate token from key in all situations except initial token in config and middle point between tokens? So if you do a range slice using tokens instead of keys TokenFactory.fromString will force you to use correctly serialized token data which will also include key.

bq. It's error-prone and make coding more complicated. We're merging object that are not the same thing etc...

If token is generated from key than for me it's natural to have a key as member. The thing is that you are enable to create a ""pure"" token, Partitioner will always give you a Token with valid key except for midpoint method so if partitioner is used to generate tokens you are guaranteed to have a valid key in the resulting token instance.

bq. It's a ~300K patch. Sure it's mostly simple changes, but it's still that many changes that could introduce a typo somewhere that causes a bug.

The same thing I can say about your set of patches - it's 198 KB. Aren't we writing tests to catch such bugs?","12/Oct/11 12:23;slebresne;
bq. Can you please define what do you mean by ""pure token""?

In you patch, it's a Token whose key is EMPTY_BYTE_BUFFER (which is *not* a valid row key, hence the 'pure' token name).

bq. Aren't we supposed to generate token from key in all situations except initial token in config and middle point between tokens?

And? Is that not enough? There is tons of place in the code where we manipulate those tokens 'not created from a key' (all the distribution code basically, which is a big part of Cassandra). Moreover, there is also range_slice that accept a range of token.

bq. So if you do a range slice using tokens instead of keys TokenFactory.fromString will force you to use correctly serialized token data which will also include key.

To what is this supposed to be an answer ?

bq. If token is generated from key than for me it's natural to have a key as member. The thing is that you are enable to create a ""pure"" token, Partitioner will always give you a Token with valid key except for midpoint method so if partitioner is used to generate tokens you are guaranteed to have a valid key in the resulting token instance.

But it's not always generated from a key! There is nothing natural to a key member in all the Token object manipulated by TokenMetadata and other, since there is not such key.

bq. The same thing I can say about your set of patches - it's 198 KB. Aren't we writing tests to catch such bugs?

Well, in my patch, 148K of those are a type generification only (that's why I've separated it). Because generics are erased at runtime, as long as it compile, there is *NO* chance this can introduce a bug. As for trusting tests to catch bugs, I think it's being overconfident in tests. But in the end, I'm happily taking back that objection as this is by far the less important.


Let me try to put things graphically, everyone loves a graph: if I draw the set of all keys as this:
{noformat}
[-----------------------------------------------------------------------------[
{noformat}
i.e, the ring but as a line because I'm ignoring wrapping for this.

Now, if I display row keys (decorated or not, that doesn't matter, both are keys), I would have for instance:
{noformat}
[---------------------------|-------|---------------|---------------|---------[
                            k1      k2              k3              k4
{noformat}
A key is a point on the ring.

Now if keys and tokens are a 1 to 1 mapping, then it could be ok to say that a token is a point on the ring, but once it's not the case, then it looks like that:
{noformat}
                                t                     t'              t''
[-------------------------[*|*******|*]----------[**|****]-------[**|******]--[
                            k1      k2              k3              k4
{noformat}
where t is the token for both k1 and k2 (and an infinite number of other keys (actually finite because we're working on a computer)), t' the token of k3 (and an 'infinite' number of other keys), etc...

A token is intrinsically a range, a segment on the ring. Shoving DK and Token into the same class everywhere in the code is saying that we'll use the same class for a point and an interval. How can that be a good idea? How can that not backfire on us and be hard to work with, making it easy to introduce errors?
","12/Oct/11 12:33;tjake;bq. A token is intrinsically a range, a segment on the ring. 

But the whole point of the ticket is to remove this concept. Are you saying that can't be guaranteed?

This should be possible by making a equals consider the token AND key.  The problem with CASSANDRA-1733 is sometimes we don't specify a key since we have have Min token and an intrinsic Max token.  ","12/Oct/11 13:34;slebresne;bq. But the whole point of the ticket is to remove this concept. Are you saying that can't be guaranteed?

There is a misunderstanding. The whole point of this ticket is to *enforce* this concept. A token is a range, a segment on the ring, there is nothing we can do about it. It's like saying the point of the ticket is to remove the concept that a segment is different from a point.

I'm happy to discuss that, and that is clearly where we should start, but I'm pretty sure that the *problem* we want to fix is that the current code is pretending than a segment is equal to a point. The current code is pretending that a token t is the same thing than a key having this token. This only work if there is only one key have a given token, otherwise it's buggy, you identify all keys having the same token as equal, that is the problem.

And saying that you'll change the comparison of DK to include the key and pretending that a token is the same thing that some fictive key that as far as key comparison is concerned would be before any key having the token (which is *exactly* what Pavel's patch is doing) doesn't work either. As I've said earlier with examples.

I'm saying that the right way to fix is to make the code treats Token as a segment (because you know, that's what it is) and a key as a point. Now that, imho, is not of a debatable nature: it's either true or false (and imho clearly true but maybe i'm completely stupid). But at the very least we should agree on that first, even before thinking about how we will code it.

Then, once we agree on the problem, there is the question of how we do it. And then, my second argument is that shoving a token (a segment) and a (decorated) key (a point) into the same class (that we would happen to call Token) is, why probably ""possible"", likely an error-prone, confusing and frankly ugly idea. You can create a class representing both a segment and point, having it work correctly underneath and write code using that, but it will unlikely be beautiful nor easy to use. But it's ""possible"". ","12/Oct/11 13:46;jbellis;bq. The whole point of this ticket is to enforce this concept. A token is a range, a segment on the ring, there is nothing we can do about it.

Right.

Is this still a fair summary of why we want to fix this?

bq. the problem is that we are using DK both for routing and for local key sorting","12/Oct/11 14:31;tjake;My view is a Key requires a Token in our system. I understand that you cant keep multiple keys from mapping to the same token, still I would have liked to see the code deal with Tokens with (optional) keys then a mix of keys and tokens.  I see now this idea is broken in the sense that sorting a list of tokens means different things depending on the context (partitioner bounds vs user defined range)
","12/Oct/11 14:46;slebresne;{quote}
Is this still a fair summary of why we want to fix this?

the problem is that we are using DK both for routing and for local key sorting
{quote}
Hum, I would actually rephrase it with token instead of DK, in the sense that we don't really use DK for routing, DK is a key with it's token ""cached"" to speed up computing it, we're using only the token to route. The problem is we're also using only the token for local key sorting.

But while we could/should be using token to route and the DK for local key sorting, we still need to be able to handle local key *search* by token. And that is imho the difficulty of this ticket (if we always had an actual valid key to do local key search it would be much easier). And we need local search based on tokens because:
  * we allow range_slices on a range of tokens (so this translate ultimately to local search by token)
  * even for range_slices by keys, we still end up splitting the key range by a token in getRestrictedRanges, hence resulting ultimately to a local search by token.
Then the problem is that since a token is a segment and a key (what we're searching for) a point, we can't really compare those, in the sense that a key is not necessarily either stricly greater, equal, or stricly lesser than a token. So you do have to consider both the ""bounds"" of the token, which are now point and that you can compare to keys.","18/Nov/11 14:20;slebresne;Attaching rebased patch (against trunk).

I've slightly refactored the patches too. The first contains only the generification of AbstractBounds. It's obviously a bit ""dumb"" taken alone since it generify but doesn't allow anything else than tokens. The only other noticeable thing is the removal of the Range.compare() method (in favor of the compareTo method of Token). I have no idea what that method was about in the first place. The second patch does the rest of the work and has got some minor cleanups. I've also tried to add some new comments to make it more digestible.  I also include a third patch with a small unit test.

Having spend quite some time thinking about this issue, I do think that this is a good way to fix it, the alternative of allowing to mixing Token and DecoratedKey directly in a Range being (to have pursued it a bit before giving up) much more messy and error prone imho. Now I can't force anyone to like this solution but I also won't rebase this forever.","18/Nov/11 23:51;jbellis;{noformat}
+    public static final DecoratedKey minKey = new DecoratedKey(partitioner.getMinimumToken(), false);
{noformat}

I think I'd rather have these in the partitioner.  (I know partitioner is cluster-global right now but it still feels wrong to ""hoist"" something partitioner dependent out and make it static final.)

{noformat}
+        assert token != null && key != null;
{noformat}

This feels odd when we go ahead and construct DKs with null key anyway in the other constructor.

*Important*: I think my biggest problem with this patch is that a DK may or may not have a key that when given to the partitioner, results in the Token in the DK.  And there's nothing to show that is the case, except that key == null or Empty.  So we're still pretending a Token ""is"" a key, we've just made it more complicated.  Could we update the methods for whose benefits we're performing the Token -> DK conversion, to accept RingPosition instead?

{noformat}
+        return token.hashCode() + (key == null ? 0 : key.hashCode());
{noformat}

I don't see a good reason to not use a ""real"" hashcode implementation (Objects.hashCode is useful here).

{noformat}
+        // null is used as a 'end of range' marker, so DK(t, k) is always before DK(t, null) unless k == null
{noformat}

Still not a huge fan of using null to mean end of range, but I guess I don't have a better suggestion. There's clearly a lot of places in this patch where it's causing special case ugliness though, independent of its status as ""max.""

{noformat}
+        // minimunKey, see Token.upperBoundKey()
{noformat}

typo.  (both occurrences.)

{noformat}
-        T min = (T)current.partitioner.getMinimumToken();
+        T min = (T)current.left.minimumValue(current.partitioner);
{noformat}

I think the positives of making this Generic are outweighed by the negative of implying that minimum value for partitioner X depends on the RingPosition that is returning it.  I think I'd rather accept the casting ugliness of having a Partitioner method that does instanceof checks to return the appropriate type.  

*Serializer code*: How does DK, AB, etc. code deal w/ backwards compatibility issues?  Looks like some (AES) can get by with saying ""we don't support mixed-version streaming"" but others (IndexScanCommand) cannot.

{noformat}
+        assert left.compareTo(right) <= 0 || right.isMinimum(partitioner) : ""["" + left + "","" + right + ""]"";
{noformat}

What if we added a Partitioner reference so we could just ask isMinimum()?

","19/Nov/11 00:03;jbellis;bq. I think my biggest problem with this patch is that a DK may or may not have a key that when given to the partitioner, results in the Token in the DK.

Put another way: in my ideal world, DK.token would be purely an optimization to avoid calling partitioner.getToken(key) over and over.","21/Nov/11 16:54;slebresne;bq. Put another way: in my ideal world, DK.token would be purely an optimization to avoid calling partitioner.getToken(key) over and over.

I understand that, but I think there is two different things and I want to know exactly where the disagreement/problem is.

The first problem, which is imho the core of this ticket, is that the code needs to be able somehow to deal with things like (where I use k for keys and t for tokens, and the term range for either Range or Bounds):
  * Is key k in the range [k', t] (or (t', k''])? Because when you do a range_slice of [k', k''] and there is multiple nodes and [k', k''] spans multiple replica, we will end up requesting all keys in [k', t] (for some t) or (t', k''].
  * Is key k in range (t, t']? Because we're allowed to range query keys by a token range, but also a few other reason, like the fact that during validation compaction we hashes together keys within a token range.
Note that those are not trivial questions, because for instance [k', t], while we intuitively understand what it represents is a weird beast in that is a range a point and a segment?!

Or in other words, as much as I'd like the operations on Tokens and the ones on Keys to be two completely orthogonal sets of operation with no interaction whatsoever, it is not the case and we have to deal with it.

Dealing with the case where we need tokens and we have keys is trivial (we just call Key.getToken() and boom, we're back in the case with only tokens).

The problem is when we fundamentally work on keys, but have only token to start with. Today (i.e. before this ticket), we take a simplification by doing essentially the same thing that in the 'needs token but got keys' case by having a sort of Token.getKey() (it's more ugly in practice, we inline calls to new DecoratedKey(t, null), but that's the same thing). But doing that forces in itself the fact that key an token are in bijection and we want to lift that.

One solution could be to try to keep Token as long as we can, even in places where we really need a key and have the code deal with that. I can understand that on the surface that could look clean, but in practice the code to do that correctly would a pure nightmare. Just trying to implement a Range that would mix token and keys (like the [k', t] range above) is a complete mess.

So what this patch does is realizing that you could characterize the set of keys that a token t represents with only two keys: the smallest key having token t, and the biggest key having token t.

Now, supposing we agree on what is above, the rest is implementation details and that's probably a much simpler discussion. Note that above I'm not talking of DecoratedKey, only key. But the question is, how do you represent those two new keys (for each token). The patch uses special values of the key field of DK to deal with those. I can agree this is not the cleanest thing ever and I'm fine looking for a different encoding, but I just don't have a much better idea, and frankly I don't find that horrible either.

bq. I think I'd rather have these in the partitioner

Good idea.

bq. his feels odd when we go ahead and construct DKs with null key anyway in the other constructor.

The goal here is to avoid constructing one of the two 'fake' keys by accident For that the second constructor is dedicated to their construction and as the commnet says, you're not even supposed to use this second constructor, but use Token.{upper|lower}Bound instead. Actually, the assert should check for the EMPTY_BYTE_BUFFER.

bq. Could we update the methods for whose benefits we're performing the Token -> DK conversion, to accept RingPosition instead?

Frankly, and as argumented above, no, not without *huge* pain. We only do that conversion in places where we will have to do it at some point, and trying to push Tokens deeper would only serve in having operations that make no real sense for Tokens be able to actually deal with Token. As one example, we would have to make Range with a mix of Token and Keys, and frankly that will be a total mess to code.

bq. I don't see a good reason to not use a ""real"" hashcode implementation (Objects.hashCode is useful here)

Not sure I follow but ByteBuffer.hashCode() does hash the content of the buffer if that was what you meant.

bq. There's clearly a lot of places in this patch where it's causing special case ugliness though, independent of its status as ""max.""

Again, I would be open to better encoding. But is there really that much places? The patch tried to make it so that no code outside of DecoratedKey really have to deal with it. If not perfect, I actually think it's better contained that before the patch.

bq. I think the positives of making this Generic are outweighed by the negative of implying that minimum value for partitioner X depends on the RingPosition that is returning it. I think I'd rather accept the casting ugliness of having a Partitioner method that does instanceof checks to return the appropriate type.

I think you're right.

bq. Serializer code: How does DK, AB, etc. code deal w/ backwards compatibility issues?

Basically, old version only understand AbstractBounds of Token, while new version generates/accept AbstractBounds of either token, or keys. When old sends to new and keys are expected, new convert the range/bounds of token as range/bounds of keys. When new sends to old, it converts any range/bounds of keys to range/bounds of token.

bq. What if we added a Partitioner reference so we could just ask isMinimum()?

Do you mean to have the DK to have a reference to the partioner? If so, I agree that it's probably something we should, but it's nothing specific to that patch so I'd rather leave it to another ticket.
","21/Nov/11 18:16;jbellis;bq. in practice, we inline calls to new DecoratedKey(t, null)

Right.  I must be missing something crucial, because that's exactly what it looks like we're still doing in this patch, only with a special constructor.","21/Nov/11 21:17;slebresne;No, no, the patch does use the same think. I merely said that the patch does some attempt at a better encapsulation, as it seems better to use the Token.{upper|lower}BoundKey to creates those fake keys that inlining the call to the constructor all over the code (which we do now). It makes the use of null more of an internal detail of DecoratedKey (not completely, granted, but it's a little bit better). It also makes it simpler to check we don't accidentally construct a DK with a null key by accident (the goal of the assertion in the first DK constructor in the patch).

But let it be clear that I'm not making any claim that this patch ""cleans"" some ugliness in the current code. It mainly try to solve the problem at hand, which is basically to be able to do range_slices and getting the right result even when multiple keys have the same token.

This is not saying it wouldn't be good to fix any current ugliness at the same time if possible, but in truth, I don't find that using special DK to represent special keys is such an ugly hack (not either claiming it's super beautiful, I just don't have a particular hatred of this). Besides, I don't have tons of ideas to fix the issue at end (the priority) and make the code clearly cleaner. And I do think that whatever ugliness the current have, this patch doesn't make it worst.

Anyway, I'll try to see if I can improve the encapsulation of the Token.{upper|lower}BoundKey representation and see if I can come with something slightly cleaner.","21/Nov/11 21:28;jbellis;bq. the patch does some attempt at a better encapsulation, as it seems better to use the Token.{upper|lower}BoundKey to creates those fake keys that inlining the call to the constructor all over the code (which we do now). 

Okay, I'll buy that.  It's an awful lot of code churn for IMO a relatively minor win, but I see where you're going with that.

Help me understand this patchset a different way: which is the part without which CASSANDRA-1600 is impossible?","23/Nov/11 16:06;slebresne;bq. Help me understand this patchset a different way: which is the part without which CASSANDRA-1600 is impossible?

CASSANDRA-1600 requires that the row key range requested be known by CFS.getRangeSlice/search, while today it only gest the corresponding tokens.  We could possibly do what your first patch for CASSANDRA-1600 did and add the keys separately. You'll have to deal with wrapping and such, but that's probably doable.

What this patchset does is make getRangeSlice/search actually take keys, so this greatly simplify CASSANDRA-1600. But CASSANDRA-1600 is probably doable without this, it's just the logical first step before getting a clean implementation. Now for the specific parts, as said we need to be able to have keys for getRangeSlice/search, which basically require the bulk of this patchset (i.e. for CASSANDRA-1600, we could still have DecoratedKey.compareTo() to only compare the tokens and not the keys, but that's probably it)

But truth being told, CASSANDRA-1600 is by far not my main motivation for this. My main motivation is CASSANDRA-1684. For the latter, if we want to do it 'natively', we will have lots of key having the same token, so this ticket is an absolute requirement before even getting started. And there is also the problem of md5 collision :)
","23/Nov/11 16:07;slebresne;I've tried finding a better encapsulation for the 'fake' keys of the patch.  The idea being to restrict DK to 'true' row key, i.e. the ones that can be written on disk and create a new class (Token.KeyBound) to represent the two ""fake"" key for each token representing the smallest/biggest key having the token. To make it work together, they share the new RowPosition interface.

Some of the methods accepts a RowPosition (instead of DecoratedKey) to indicate that it can accept a 'fake' key for purpose of selecting true keys.  So for instance SSTableReader.getPosition() accepts a RowPosition. However, SSTableReader.getCachedPosition() only accepts a DK, because the key cache can only contain a ""true"" row key.

Anyway, I actually end up liking this. With that, we never ever create a DK with a null key (nor even an empty one, which wouldn't be a true key either).  This is more clear and avoids mistakes. Unfortunately the patch got bigger :(
","28/Nov/11 14:49;jbellis;+1 on the KeyBound approach.  This is exactly what I was hoping for.

Returning to a minor point:

bq. bq. I don't see a good reason to not use a ""real"" hashcode implementation (Objects.hashCode is useful here)

bq. Not sure I follow but ByteBuffer.hashCode() does hash the content of the buffer if that was what you meant.

I mean that straight addition is a weak hashcode combination since X + Y is the same as Y + X.  ""return Objects.hashCode(X, Y)"" is an easy way to do it ""right"" with no more code than the weak approach.  Doesn't matter much here but it's good practice imo.

Another nit: should we be using a enum for RowPosition.kind?

Meta observation: I'm glad we're not doing this a week before freeze. :)","29/Nov/11 15:10;slebresne;bq. I mean that straight addition is a weak hashcode combination since X + Y is the same as Y + X. ""return Objects.hashCode(X, Y)"" is an easy way to do it ""right"" with no more code than the weak approach. Doesn't matter much here but it's good practice imo.

Make sense. I made the DK hashcode be only based on the key hashcode though (since the token is just a cached value for getToken() :)). The hashCode method of Token.KeyBound don't use Objects.hasCode(), but I really think that in that case it doesn't matter at all and it avoids the boxing of the boolean. I can change it though if that's the only problem remaining.

bq. Another nit: should we be using a enum for RowPosition.kind?

What do you mean exactly by that? Are you talking of the kind use in RowPositionSerializer? To have an enum to distinguish between DK and Token.KeyBound instance of doing the instanceof? If so why not, but I'm not sure it buys us anything.","29/Nov/11 15:24;jbellis;Okay, we can skip the hashcode change if you're worried about boxing.

Yes, that's what I'm referring to for ""kind.""  Seeing code like ""if kind == 0"" means I have to go back to the kind method to see what a return value of 0 means.","30/Nov/11 10:44;slebresne;Attaching v3, rebased and using an enum for the RowPosition kind. I could have changed a few {{assert key instanceof DecoratedKey}} by {{assert key.kind() == RowPosition.Kind.ROW_KEY}} I suppose, but I prefered keeping the instanceof since each time the next line do a cast to DK, so this feels more coherent like that.",30/Nov/11 14:05;jbellis;+1,"01/Dec/11 09:25;hudson;Integrated in Cassandra #1229 (See [https://builds.apache.org/job/Cassandra/1229/])
    remove assumption that key and token are in bijection
patch by slebresne; reviewed by jbellis for CASSANDRA-1034

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1208993
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/client/RingCache.java
* /cassandra/trunk/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/DecoratedKey.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/HintedHandOffManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/IndexScanCommand.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RangeSliceCommand.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowIteratorFactory.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowPosition.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndexSearcher.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/keys/KeysSearcher.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/marshal/LocalByPartionerType.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/AbstractBounds.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/AbstractByteOrderedPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/AbstractPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/BootStrapper.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/Bounds.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/IPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/LocalPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/RandomPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/Range.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/RingPosition.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/Token.java
* /cassandra/trunk/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java
* /cassandra/trunk/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/IndexSummary.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableBoundedScanner.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableScanner.java
* /cassandra/trunk/src/java/org/apache/cassandra/locator/AbstractReplicationStrategy.java
* /cassandra/trunk/src/java/org/apache/cassandra/locator/TokenMetadata.java
* /cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/AntiEntropyService.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageServiceMBean.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamIn.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamOut.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamRequestMessage.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamingRepairTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/ThriftValidation.java
* /cassandra/trunk/src/java/org/apache/cassandra/tools/BulkLoader.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/MerkleTree.java
* /cassandra/trunk/test/unit/org/apache/cassandra/Util.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/CleanupTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/KeyCollisionTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/SerializationsTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/dht/AbstractBoundsTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/dht/BootStrapperTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/dht/PartitionerTestCase.java
* /cassandra/trunk/test/unit/org/apache/cassandra/dht/RangeTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/io/CompactSerializerTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableReaderTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java
* /cassandra/trunk/test/unit/org/apache/cassandra/service/MoveTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/service/SerializationsTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/service/StorageProxyTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/streaming/SerializationsTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java
",01/Dec/11 10:14;slebresne;Committed \o/
Failed to delete commitlog when restarting service,CASSANDRA-1348,12470702,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,vjevdokimov,vjevdokimov,8/3/2010 8:42,3/12/2019 14:04,3/13/2019 22:24,9/7/2010 16:25,0.6.6,0.7 beta 2,,,0,,,,,,"When restarting any Cassandra node we've got exception:

ERROR 09:42:27,869 Exception encountered during startup.
java.io.IOException: Failed to delete C:\cassandra\data\commitlog\CommitLog-1280817512228.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:45)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:177)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
	at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:57)
	at org.apache.cassandra.contrib.windows.service.WindowsService.start(Unknown Source)
	at org.apache.cassandra.contrib.windows.service.WindowsService.main(Unknown Source)
 INFO 09:42:27,869 Exception encountered during startup.
 INFO 09:42:27,869 Cassandra Service Finished: Tue Aug 03 09:42:27 EEST 2010

Aftrer exception was thrown and Cassandra didn't started, within commitlog directory there's only one file: CommitLog-1280817512228.log
and no CommitLog-1280817512228.log.header. Looks like CommitLog-1280817512228.log was a new file, not the last one after stopping service.

After few restarts .log file is deleted and Cassandra is working fine until next restart.",Windows Server 2008 R2 64bit,,,,,,,,,,,,,,,,,,,07/Sep/10 15:24;jbellis;1348-2.txt;https://issues.apache.org/jira/secure/attachment/12454023/1348-2.txt,03/Aug/10 15:15;jbellis;1348.txt;https://issues.apache.org/jira/secure/attachment/12451125/1348.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,16:27.7,,,no_permission,,,,,,,,,,,,20094,,,Tue Sep 07 16:25:49 UTC 2010,,,,,,0|i0g4i7:,92160,gdusbabek,gdusbabek,,,,,,,,,03/Aug/10 15:16;jbellis;windows gets pissy when you try to delete an open file.  this patch closes it before the delete.,03/Aug/10 15:38;gdusbabek;+1,03/Aug/10 15:48;jbellis;committed,"04/Aug/10 13:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    close commitlog reader before deleting it
patch by jbellis; reviewed by gdusbabek for CASSANDRA-1348
","03/Sep/10 08:49;vjevdokimov;Still could happen with 0.7 beta 1. Not always, but happens.",03/Sep/10 14:18;jbellis;new stacktrace?,"04/Sep/10 07:58;vjevdokimov; INFO 13:38:04,311 Received start command
 INFO 13:38:04,311 Cassandra Service Starting: Fri Sep 03 13:38:04 CEST 2010
 INFO 13:38:04,467 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 13:38:04,592 Sampling index for D:\cassandra\data\data\system\Statistics-e-501-<>
 INFO 13:38:04,639 Sampling index for D:\cassandra\data\data\system\Statistics-e-502-<>
 INFO 13:38:04,670 Sampling index for D:\cassandra\data\data\system\Schema-e-1-<>
 INFO 13:38:04,670 Sampling index for D:\cassandra\data\data\system\Schema-e-2-<>
 INFO 13:38:04,670 Sampling index for D:\cassandra\data\data\system\Schema-e-3-<>
 INFO 13:38:04,701 Sampling index for D:\cassandra\data\data\system\Migrations-e-1-<>
 INFO 13:38:04,701 Sampling index for D:\cassandra\data\data\system\Migrations-e-2-<>
 INFO 13:38:04,701 Sampling index for D:\cassandra\data\data\system\Migrations-e-3-<>
 INFO 13:38:04,717 Sampling index for D:\cassandra\data\data\system\LocationInfo-e-9-<>
 INFO 13:38:04,717 Sampling index for D:\cassandra\data\data\system\LocationInfo-e-10-<>
 INFO 13:38:04,717 Sampling index for D:\cassandra\data\data\system\LocationInfo-e-11-<>
 INFO 13:38:04,717 Sampling index for D:\cassandra\data\data\system\HintsColumnFamily-e-24-<>
 INFO 13:38:04,732 Sampling index for D:\cassandra\data\data\system\HintsColumnFamily-e-25-<>
 INFO 13:38:04,732 Sampling index for D:\cassandra\data\data\system\HintsColumnFamily-e-26-<>
 INFO 13:38:04,732 Sampling index for D:\cassandra\data\data\system\HintsColumnFamily-e-28-<>
 INFO 13:38:04,732 Sampling index for D:\cassandra\data\data\system\HintsColumnFamily-e-30-<>
 INFO 13:38:04,732 Sampling index for D:\cassandra\data\data\system\HintsColumnFamily-e-33-<>
 INFO 13:38:04,763 Loading schema version 6e908998-aa17-11df-bf55-cacedb26d926
 INFO 13:38:05,013 Sampling index for D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-25-<>
 INFO 13:38:07,588 Sampling index for D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-150-<>
 INFO 13:38:10,837 Sampling index for D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-279-<>
 INFO 13:38:14,550 Sampling index for D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-412-<>
 INFO 13:38:17,747 Sampling index for D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-461-<>
 INFO 13:38:18,948 Creating new commitlog segment E:/cassandra/data/commitlog\CommitLog-1283513898948.log
 INFO 13:38:18,964 Deleted D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-470-Data.db
 INFO 13:38:18,964 Deleted D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-479-Data.db
 INFO 13:38:18,979 Deleted D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-492-Data.db
 INFO 13:38:18,979 Deleted D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-497-Data.db
 INFO 13:38:18,979 Sampling index for D:\cassandra\data\data\BehavioralTargeting\CookieTrackingSetupMemberships-e-498-<>
 INFO 13:38:20,118 Replaying E:\cassandra\data\commitlog\CommitLog-1283513833929.log, E:\cassandra\data\commitlog\CommitLog-1283513898948.log
 INFO 13:38:20,134 E:\cassandra\data\commitlog\CommitLog-1283513833929.log.header incomplete, missing or corrupt.  Everything is ok, don't panic.  CommitLog will be replayed from the beginning
 INFO 13:38:20,150 Finished reading E:\cassandra\data\commitlog\CommitLog-1283513833929.log
 INFO 13:38:20,150 Finished reading E:\cassandra\data\commitlog\CommitLog-1283513898948.log
 INFO 13:38:20,150 switching in a fresh Memtable for Statistics at CommitLogContext(file='E:/cassandra/data/commitlog\CommitLog-1283513898948.log', position=592)
 INFO 13:38:20,165 Enqueuing flush of Memtable-Statistics@1582759704(15004 bytes, 4 operations)
 INFO 13:38:20,165 Writing Memtable-Statistics@1582759704(15004 bytes, 4 operations)
 INFO 13:38:20,508 Completed flushing D:\cassandra\data\data\system\Statistics-e-503-Data.db
 INFO 13:38:20,508 Recovery complete
ERROR 13:38:20,508 Exception encountered during startup.
java.io.IOException: Failed to delete E:\cassandra\data\commitlog\CommitLog-1283513898948.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:45)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:178)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
	at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:57)
	at org.apache.cassandra.contrib.windows.service.WindowsService.start(Unknown Source)
	at org.apache.cassandra.contrib.windows.service.WindowsService.main(Unknown Source)
 INFO 13:38:20,555 Exception encountered during startup.
 INFO 13:38:20,555 Cassandra Service Finished: Fri Sep 03 13:38:20 CEST 2010
","07/Sep/10 15:24;jbellis;it logs ""Finished reading"" which means it's successfully closed the reader, so there's nothing in Cassandra blocking the delete.  patch -2 makes failure to delete a non-fatal error.",07/Sep/10 15:27;jbellis;backported try/finally from original patch to 0.6.  (failure to delete is already non-fatal there.),07/Sep/10 15:49;gdusbabek;+1 on the new patch.,07/Sep/10 16:25;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool drain attempts to delete a deleted file,CASSANDRA-1408,12471949,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,jhermes,jhermes,8/18/2010 21:49,3/12/2019 14:04,3/13/2019 22:24,12/10/2010 17:36,0.6.9,0.7 beta 2,,,0,,,,,,"Running `nodetool drain` presented me with a pretty stack-trace.
The drain itself finished successfully and nothing showed up in the system.log.

{noformat}
$ bin/nodetool -h 127.0.0.1 -p 8080 drain
Exception in thread ""main"" java.lang.AssertionError: attempted to delete non-existing file CommitLog-1282166457787.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:40)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:178)
	at org.apache.cassandra.service.StorageService.drain(StorageService.java:1653)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
{noformat}",sun-jdk-1.6/Ubuntu 10.04,,,,,,,,,,,,,,,,,,,09/Dec/10 22:50;brandon.williams;1408-0.6.txt;https://issues.apache.org/jira/secure/attachment/12465949/1408-0.6.txt,07/Sep/10 14:15;jbellis;1408.txt;https://issues.apache.org/jira/secure/attachment/12454016/1408.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,57:08.4,,,no_permission,,,,,,,,,,,,20123,,,Fri Dec 10 18:48:42 UTC 2010,,,,,,0|i0g4vj:,92220,gdusbabek,gdusbabek,,,,,,,,,18/Aug/10 21:57;jbellis;this may be a problem in 0.6 as well.,02/Sep/10 22:42;jbellis;why are we even calling CommitLog.recover() during drain?,"03/Sep/10 12:54;gdusbabek;No good reason that I can think of.  At the time I remember the complaint was that shutting down left uncommitted updates in the CL, but flushing solves that.","03/Sep/10 14:26;jbellis;I think the recover can be necessary b/c we don't shut down mutations until after the flushes.  Attached patch fixes ordering there (and adds wait on postFlushExecutor for maximum correctness).

(As for the error on recover() itself, that is/will be addressed in CASSANDRA-1348.)",07/Sep/10 13:45;gdusbabek;patch needs rebase.,07/Sep/10 14:15;jbellis;rebased,07/Sep/10 15:52;gdusbabek;+1,07/Sep/10 16:27;jbellis;committed,"19/Sep/10 02:30;phuongcsa;java.lang.AssertionError: attempted to delete non-existing file Walls-e-25-Statistics.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:43)
        at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:135)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:174)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:296)
        at org.apache.cassandra.db.Table.<init>(Table.java:245)
        at org.apache.cassandra.db.Table.open(Table.java:104)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:461)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:105)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:98)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:216)


I have installed this patch, however bug is not resolved.

Cassandra 0.7 beta1, Ubuntu 10.04",09/Dec/10 22:35;brandon.williams;Reopened for backport to 0.6,"09/Dec/10 23:26;lenn0x;Brandon,

I verified this with 0.6 branch. Error occurs without patch. Applied with patch, no more errors. Thx!",10/Dec/10 16:33;jbellis;+1,10/Dec/10 17:36;brandon.williams;Committed.,"10/Dec/10 18:48;hudson;Integrated in Cassandra-0.6 #22 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/22/])
    correct ordering of drain operations so CL.recover is no longer necessary.  Patch by jbellis and brandonwilliams, reviewed by jbellis for CASSANDRA-1408
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable cleanup killed by IllegalStateException,CASSANDRA-1458,12473181,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,cgist,cgist,9/2/2010 19:51,3/12/2019 14:04,3/13/2019 22:24,9/7/2010 18:33,0.6.6,0.7 beta 2,,,0,,,,,,"Compacted SSTables were not being deleted even after a forced GC. The following stack traces were observed:

ERROR [SSTABLE-CLEANUP-TIMER] 2010-09-01 15:54:07,254 CassandraDaemon.java (line 85) Uncaught exception in thread Thread[SSTABLE-CLEANUP-TIMER,5,main]
java.lang.IllegalStateException: Task already scheduled or cancelled
        at java.util.Timer.sched(Timer.java:380)
        at java.util.Timer.schedule(Timer.java:192)
        at org.apache.cassandra.io.sstable.SSTableDeletingReference$CleanupTask.run(SSTableDeletingReference.java:86)
        at java.util.TimerThread.mainLoop(Timer.java:534)
        at java.util.TimerThread.run(Timer.java:484)

ERROR [SSTABLE-DELETER] 2010-09-01 16:20:22,587 CassandraDaemon.java (line 85) Uncaught exception in thread Thread[SSTABLE-DELETER,5,main]
java.lang.IllegalStateException: Timer already cancelled.
        at java.util.Timer.sched(Timer.java:376)
        at java.util.Timer.schedule(Timer.java:192)
        at org.apache.cassandra.io.sstable.SSTableDeletingReference.cleanup(SSTableDeletingReference.java:70)
        at org.apache.cassandra.io.sstable.SSTableReader$1$1.run(SSTableReader.java:85)
        at java.lang.Thread.run(Thread.java:636)

If the SSTableDeletingReference$CleanupTask cannot delete a file, it reschedules itself for later. TimerTasks (which CleanupTask subclasses) are intended to be scheduled only once and will cause an IllegalStateException in the timer when it tries to schedule itself again. The exception causes timer to effectively cancel itself and the next attempt to schedule a task will cause an IllegalStateException in the SSTABLE-DELETER.

It appears this could be fixed by scheduling a new CleanupTask instead of the same one that failed (SSTableDeletingReference.java:86).","trunk from 2010-08-31
Linux 2.6.18-164.2.1.el5.plus #1 SMP x86_64
OpenJDK 64-Bit Server VM (build 1.6.0-b09)
",,,,,,,,,,,,,,,,,,,07/Sep/10 14:41;jbellis;1458.txt;https://issues.apache.org/jira/secure/attachment/12454019/1458.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:09.2,,,no_permission,,,,,,,,,,,,20151,,,Tue Sep 07 18:33:33 UTC 2010,,,,,,0|i0g56n:,92270,stuhood,stuhood,,,,,,,,,02/Sep/10 22:27;jbellis;what was causing the original failure to delete?,"02/Sep/10 22:49;jbellis;I suspect retrying the delete is simply bogus -- if there is a race condition that would make the delete fail, we need to fix that; I can't think of another reason delete would succeed later after failing initially.

btw, are you running Windows?",03/Sep/10 02:14;cgist;The cause of the failed delete is unknown. I was running Linux as in the updated environment.,07/Sep/10 14:41;jbellis;patch attached,07/Sep/10 14:44;jbellis;(patch is against 0.7; I will backport to 0.6 as well),07/Sep/10 18:33;jbellis;+1'd by Stu in IRC. committed to 0.6 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleAuthenticator MD5 support,CASSANDRA-1447,12472895,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,rnirmal,stuhood,stuhood,8/31/2010 1:06,3/12/2019 14:04,3/13/2019 22:24,9/29/2010 4:54,0.6.6,0.7 beta 2,,,0,,,,,,"...is broken, or not working as expected. Needs a look before 0.7.0.",,,,,,,,,,,,,,,,,,,,20/Sep/10 20:15;rnirmal;0001-Fixed-MD5-support-in-SimpleAuthenticator.patch;https://issues.apache.org/jira/secure/attachment/12455074/0001-Fixed-MD5-support-in-SimpleAuthenticator.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:42.7,,,no_permission,,,,,,,,,,,,20144,,,Wed Sep 29 04:54:32 UTC 2010,,,,,,0|i0g547:,92259,jbellis,jbellis,,,,,,,,,"08/Sep/10 01:52;stuhood;From Peter Nerg on CASSANDRA-1474:
{quote}
When running the SimpleAuthenticator in MD5 mode it will try to compare the plain text password with a MD5 digest from the passwd.properties file.
However the password in the property file is already a MD5 digest thus it's now digested twice.
Code snippet
""
case MD5:
authenticated = MessageDigest.isEqual(password.getBytes(), MessageDigest.getInstance(""MD5"").digest(props.getProperty(username).getBytes()));
break;
""
{quote}",25/Sep/10 23:58;jbellis;committed,"26/Sep/10 12:47;hudson;Integrated in Cassandra #547 (See [https://hudson.apache.org/hudson/job/Cassandra/547/])
    fix SimpleAuthenticator MD5 support
patch by Nirmal Ranganathan; reviewed by jbellis for CASSANDRA-1447
",28/Sep/10 17:13;jbellis;re-opening for backport to 0.6,29/Sep/10 04:54;jbellis;done for 0.6.6 too,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
suggest avoiding broken openjdk6 on Debian as build-dep,CASSANDRA-1575,12475803,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,scode,scode,10/4/2010 20:28,3/12/2019 14:04,3/13/2019 22:24,10/6/2010 21:13,0.6.6,0.7 beta 3,Packaging,,0,,,,,,"I ran into this myself and then today someone was reporting having the same problem on IRC; there is a packaging bug in openjdk6 in lenny:

   http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=501487

The effect is that when ant tries to download files over SSL, it fails complaining about:

   ""java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty""

It turns out this works fine with the Sun JVM. I'm attaching a patch which makes Cassandra build on both lenny and squeeze; however, I am not sure whether other platforms may be negatively affected. The patch just requires an openjdk sufficiently new that the lenny openjdk won't quality. If there are other platforms where we do want an older openjdk, this patch might break that.

In addition, I removed the ""java6-sdk"" as a sufficient dependency because that resolved to openjdk-6-jdk on lenny.

I think it's a good idea to consider changing this just to decrease the initial threshold of adoption for those trying to build from source.

So: This does fix the build issue on lenny, and doesn't seem to break squeeze, but I cannot promise anything about e.g. ubuntu.

For the record, I'm also attaching a small self-contained test case which, when run, tries to download one of the offending pom files. It can be used to easily test weather the SSL download with work with a particular JVM.",Debian lenny,,,,,,,,,,,,,,,,,,,04/Oct/10 20:30;scode;Trunk1575Test.java;https://issues.apache.org/jira/secure/attachment/12456317/Trunk1575Test.java,04/Oct/10 20:29;scode;trunk-1575.txt;https://issues.apache.org/jira/secure/attachment/12456316/trunk-1575.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,13:08.0,,,no_permission,,,,,,,,,,,,20209,,,Wed Oct 06 22:33:17 UTC 2010,,,,,,0|i0g62n:,92414,urandom,urandom,,,,,,,,,"04/Oct/10 21:44;scode;And I should stress that this will presumably only help normalized build environments that install build dependencies as required. It doesn't help a user trying to 'ant build', nor a user trying to 'debuild' and happens to have other JDK:s installed, but I'm not sure how to address that in a clean fashion.

Maybe add a note under ""requirements"" in the README?

(I realize this is catering to a very specific platform, but lenny is presumably pretty common.)","06/Oct/10 21:13;urandom;First off, thanks for the report, and the background research on it.

To summarize this issue for others, the openjdk-6 package in Lenny is missing the cacerts keystore needed to establish ""trust"" with SSL enabled servers.  I'm guessing this is because it was stripped from Sun's original code dump, because later versions of the package depend on ca-certificates-java which simply maintains a keystore made up of the Debian installed CAs.

Where this creates a problem for Cassandra is in the retrieval of build dependencies with Ivy, where those deps are located on SSL-enabled remote servers. This _only_ occurs on Lenny though, later versions are fine.

As to the attached patch, I'm not convinced that the cure here isn't worse than the disease.  Here' s why:

* The problem is only with building a Debian source package, and only on Lenny.  I believe this to be a small subset of all users.
* The situation isn't impossible for those that want to build the source package on Lenny.  They simply need to install sun-java6 first (or set it to default using update-alternatives if openjdk-6 is already installed).
* The attached patch will result in an uninstallable package for anyone who doesn't have the non-free repository enabled.  This is everyone who went through the default installation process.
* Unattended installs of sun-java6 (think chef, puppet, et. al.) are difficult at best because the package prompts for user acceptance of the license.
* If possible, we want to use the same packaging for all versions of Debian and derivatives, and there has been a lot of talk of removing the sun packages from archives. 

I think it'd be better to simply document this at http://wiki.apache.org/cassandra/DebianPackaging and leave things as they are.  If you disagree, feel free to reopen the report.","06/Oct/10 22:33;scode;Sounds reasonable.

That said, maybe the set of people who would try 'ant build' on lenny is significantly larger than those building Debian packages with debuild. For those, a note in README might be helpful.

But again I realize this is catering to a very specific problem. Maybe it's just not worth it.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
only attempt to set size on Linux (for portability),CASSANDRA-1508,12474213,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,9/15/2010 20:53,3/12/2019 14:04,3/13/2019 22:24,9/16/2010 13:43,0.7 beta 2,,Packaging,,0,,,,,,"-Xss128k causes the JVM to refuse to start or crash on 64 bit FreeBSD 8 (this goes for two wildly differing openjdk 1.6:es and for the current openjdk7 branch). Attaching patch to only pass -Xss on Linux.

The motivation here is that out-of-the-box behavior is important for first-comers, and for people in production on a non-Linux platform where -Xss128k would work are presumably committed enough that they can tweak this themselves.
",,,,,,,,,,,,,,,,,,,,15/Sep/10 20:54;scode;trunk-1508.txt;https://issues.apache.org/jira/secure/attachment/12454698/trunk-1508.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,43:28.7,,,no_permission,,,,,,,,,,,,20171,,,Fri Sep 17 13:55:35 UTC 2010,,,,,,0|i0g5hr:,92320,brandon.williams,brandon.williams,,,,,,,,,"16/Sep/10 13:43;brandon.williams;Committed.  Out of curiosity, was it crashing due the the recursion limit being exceeded?","16/Sep/10 14:45;scode;To be honest I have not investigated (due to the nature of the crash there's no crashdump or anything to investigate stack trace). I believe it's a general issue with the JDK on the platform. Even just ""java -Xss128k"" (without running anything but trying to elicit help) crashes.

My suspicion is that it is simply something general having more to do with a non-default stack size, or some arbitrary minimum limit, rather than actually eating the stack in Java code. But it's speculation at this point.
","17/Sep/10 13:55;hudson;Integrated in Cassandra #538 (See [https://hudson.apache.org/hudson/job/Cassandra/538/])
    only attempt to set thread stack size on Linux.  Patch by Peter Schuller, reviewed by brandonwilliams for CASSANDRA-1508
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.py's multiget option sends increasingly inefficient queries as more test data is inserted,CASSANDRA-1520,12474520,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,zznate,zznate,9/19/2010 5:58,3/12/2019 14:04,3/13/2019 22:24,9/20/2010 18:38,0.7 beta 2,,,,0,,,,,,"MultiGetter's key list sizes should be broken up better for more efficient queries. Setting an initial value that breaks up the key list into N sub lists (where N is the number of threads) yielded more efficient queries. (The choice of thread count here was a stop-gap for demonstration purposes. End result should probably be chunk-size config option with a sane default).

Pre patch:
---
python stress.py -o multiget -t 25 -n 250000 -c 5
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
6,0,6000,8.6109764576,10
10,0,4000,18.6666852832,20
17,0,7000,27.4705835751,30
23,0,6000,36.6091703971,41
25,0,2000,41.8415510654,42

Post patch:
---
python mstress.py -o multiget -t 25 -n 250000 -c 5
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
172,17,6880,1.44215503127,10
314,14,5680,1.8667214538,20
466,15,6080,1.69888155084,31
624,15,6320,1.55442555947,41
625,0,40,0.0914790630341,41",,,,,,,,,,,,,,,,,,,,19/Sep/10 22:21;brandon.williams;1520-v2.txt;https://issues.apache.org/jira/secure/attachment/12454999/1520-v2.txt,19/Sep/10 05:59;zznate;1520.patch;https://issues.apache.org/jira/secure/attachment/12454965/1520.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,01:57.3,,,no_permission,,,,,,,,,,,,20177,,,Mon Sep 20 18:38:29 UTC 2010,,,,,,0|i0g5kf:,92332,brandon.williams,brandon.williams,,,,,,,,,19/Sep/10 07:28;zznate;Moved priority down to minor. ,"19/Sep/10 21:01;brandon.williams;Committed as is, since multiget was fairly poor.  Number of threads as a stopgap probably isn't too bad, but ultimately I think providing a config option for the number of keys is best, so the user knows exactly what's going on.","19/Sep/10 22:17;brandon.williams;Updated patch to reuse the rangecount variable so we don't have to use another option, and also apply the logic to supercolumns.","20/Sep/10 14:37;hudson;Integrated in Cassandra #541 (See [https://hudson.apache.org/hudson/job/Cassandra/541/])
    Optimize multiget in stress.py.  Patch by Nate McCall, reviewed by brandonwilliams for CASSANDRA-1520
","20/Sep/10 18:29;zznate;1520v2 looks good as well. new -g usage works well to find the slice sweet spot. 
For the record: had to apply patch to file previous to commit though, thus some merge handjiving will be needed.",20/Sep/10 18:38;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RedHat init script status is a no-op,CASSANDRA-1628,12477654,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,addumb,addumb,addumb,10/18/2010 17:37,3/12/2019 14:04,3/13/2019 22:24,10/19/2010 0:10,0.6.7,0.7 beta 3,,,0,,,,,,"The current bare-bones init script is a little too bare-bones. The ""status"" argument make a successful no-op, which can be pretty misleading.",RedHat-like Linux,,,,,,,,,,,,,,,,,,,18/Oct/10 18:03;addumb;redhat-status-usage.patch;https://issues.apache.org/jira/secure/attachment/12457465/redhat-status-usage.patch,18/Oct/10 17:38;addumb;redhat-status.patch;https://issues.apache.org/jira/secure/attachment/12457461/redhat-status.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,14:04.5,,,no_permission,,,,,,,,,,,,20223,,,Tue Oct 19 14:15:02 UTC 2010,,,,,,0|i0g6ef:,92467,brandon.williams,brandon.williams,,,,,,,,,18/Oct/10 17:38;addumb;Do a pretty minimal status check.,18/Oct/10 18:03;addumb;Also include the status option in the Usage section.,"18/Oct/10 18:14;zznate;Works as advertised: 
# /etc/init.d/cassandra status
cassandra (pid  1101) is running...

After shutdown:
# /etc/init.d/cassandra status
cassandra is stopped
",19/Oct/10 00:10;brandon.williams;Committed.,"19/Oct/10 14:15;hudson;Integrated in Cassandra #570 (See [https://hudson.apache.org/hudson/job/Cassandra/570/])
    Redhat init script prints the status when asked.  Patch by Adam Gray, reviewed by brandonwilliams for CASSANDRA-1628.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Queries on system keyspace over thrift api all fail,CASSANDRA-1649,12478089,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,thepaul,thepaul,10/22/2010 16:37,3/12/2019 14:04,3/13/2019 22:24,10/22/2010 21:17,0.7 beta 3,,Legacy/CQL,,0,,,,,,"as far as I can tell, any calls to get, get_slice, get_range_slices, get_count, etc on any ColumnFamily in the ""system"" keyspace results in an error like the following:

{noformat}
ERROR 16:29:41,278 Internal error processing get
java.lang.AssertionError: No replica strategy configured for system
        at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:315)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1459)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1447)
        at org.apache.cassandra.service.StorageService.findSuitableEndpoint(StorageService.java:1493)
        at org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:245)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:224)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:131) 
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:324)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:2655)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

Might be only when this is done over the thrift api.  I don't even know how to use the avro api or query in any other way.  But at least this sort of thing used to work around a week ago.","Debian Squeeze, cassandra svn HEAD (r1026380)",,,,,,,,,,,,,,,,,,,22/Oct/10 20:55;thepaul;1649-system-test.txt;https://issues.apache.org/jira/secure/attachment/12457870/1649-system-test.txt,22/Oct/10 18:48;jbellis;1649.txt;https://issues.apache.org/jira/secure/attachment/12457859/1649.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,48:20.2,,,no_permission,,,,,,,,,,,,20239,,,Sat Oct 23 12:49:05 UTC 2010,,,,,,0|i0g6j3:,92488,thepaul,thepaul,,,,,,,,,22/Oct/10 18:48;jbellis;moves strategy creation into Table instantiation so it can't be out of sync,22/Oct/10 20:55;thepaul;An addition to test/system/test_thrift_server.py which makes sure queries to the system keyspace can be made,"22/Oct/10 21:17;jbellis;test passes both with and without this patch, so the problem must be subtle, but I still think this patch has a good chance of stopping it.  committed.","23/Oct/10 12:49;hudson;Integrated in Cassandra #574 (See [https://hudson.apache.org/hudson/job/Cassandra/574/])
    move strategy creation into Table instantiation so it can't be out of sync
patch by jbellis; tested by Paul Cannon for CASSANDRA-1649
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"get_range_slices always returns super columns that's been removed/restored, regardless of count value in slicerange",CASSANDRA-1591,12476722,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,hujn,hujn,10/7/2010 0:06,3/12/2019 14:04,3/13/2019 22:24,11/22/2010 19:29,0.6.9,0.7.0 rc 1,,,0,,,,,,"I'm seeing cases where the count in slicerange predicate is not respected. This is only happening for super columns. I'm running Cassandra 0.6.4 in a single node.

Steps to reproduce, using the Keyspace1.Super1 CF:
* insert three super columns, bar1 bar 2, and bar3, under the same key
* delete bar1
* insert bar1 again
* run a get_range_slices on Super1, with start=bar1, finish=bar3, and count=1
* I expected only bar1 to be returned, but both both bar1 and bar2 are returned. bar3 isn't, though. so count is somewhat respected.

perl code to reproduce is attached
when I tried the same test on a standard CF it worked. only super CF seem to have this problem.","CentOS 5.4, single Cassandra node ",,,,,,,,,,,,,,,,,,,15/Oct/10 21:25;thobbs;1591-0.6-reproduce.txt;https://issues.apache.org/jira/secure/attachment/12457299/1591-0.6-reproduce.txt,22/Nov/10 18:43;jbellis;1591.txt;https://issues.apache.org/jira/secure/attachment/12460194/1591.txt,07/Oct/10 00:10;hujn;test.pl;https://issues.apache.org/jira/secure/attachment/12456559/test.pl,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,25:04.3,,,no_permission,,,,,,,,,,,,19368,,,Mon Nov 22 22:01:02 UTC 2010,,,,,,0|i0g65z:,92429,tjake,tjake,,,,,,,,,15/Oct/10 21:25;thobbs;Attached patch at least partially reproduces using system tests.,"22/Nov/10 18:43;jbellis;the deleted supercolumn w/ a live subcolumn wasn't being included in the count.  this patch adds an isLive method to fix this:

+     * For a simple column, live == !isMarkedForDelete.
+     * For a supercolumn, live means it has at least one subcolumn whose timestamp is greater than the
+     * supercolumn deleted-at time.
",22/Nov/10 19:18;tjake;LGTM +1,"22/Nov/10 22:01;hudson;Integrated in Cassandra-0.6 #11 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/11/])
    fix live-column-count of slice ranges including tombstoned supercolumn with live subcolumn
patch by jbellis and Tyler Hobbs; reviewed by tjake for CASSANDRA-1591
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RPM spec file should create saved_caches directory,CASSANDRA-1662,12478253,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,drevell,drevell,drevell,10/25/2010 18:07,3/12/2019 14:04,3/13/2019 22:24,10/26/2010 13:06,0.7.0 rc 2,,Packaging,,0,,,,,,"After installing the 0.6.6 RPM from rpm.riptano.com, the directories /var/lib/cassandra/data and /var/lib/cassandra/commitlog exist, but /var/lib/cassandra saved_caches does not exist. Cassandra fails to startup with ""java.io.IOException: unable to mkdirs /var/lib/cassandra/saved_caches"".

After manually creating /var/lib/cassandra/saved_caches, Cassandra can start.",CentOS 5 64-bit,,,,,,,,,,,,,,,,,,,25/Oct/10 18:08;drevell;spec-0.6.6.diff;https://issues.apache.org/jira/secure/attachment/12457986/spec-0.6.6.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,06:20.4,,,no_permission,,,,,,,,,,,,20246,,,Wed Oct 27 12:49:21 UTC 2010,,,,,,0|i0g6m7:,92502,brandon.williams,brandon.williams,,,,,,,,,"26/Oct/10 13:06;brandon.williams;Committed since this is the correct thing to do, however it's unclear to me why cassandra couldn't make the directory since it should own /var/lib/cassandra.","27/Oct/10 12:49;hudson;Integrated in Cassandra #578 (See [https://hudson.apache.org/hudson/job/Cassandra/578/])
    RPM spec file creates saved_caches directory.  Patch by Dave Revell, reviewed by brandonwilliams for CASSANDRA-1662
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
o.a.c.dht.AbstractBounds missing serialVersionUID,CASSANDRA-1686,12478703,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,10/29/2010 22:00,3/12/2019 14:04,3/13/2019 22:24,10/29/2010 22:12,0.6.7,0.7.0 rc 1,Legacy/Tools,,0,,,,,,"o.a.c.dht.AbstractBounds does not have a  serialVersionUID set, and as a result, tools that make use of getRangeToEndpointMap() on the StorageService mbean must be from the exact same build or they fail with an java.io.InvalidClassException.  This is very inconvenient.",,,,,,,,,,,,,,,,,,,,29/Oct/10 22:01;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1686-assign-a-serialVersionUID.txt;https://issues.apache.org/jira/secure/attachment/12458455/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1686-assign-a-serialVersionUID.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,04:24.0,,,no_permission,,,,,,,,,,,,20259,,,Wed Nov 03 20:51:29 UTC 2010,,,,,,0|i0g6rj:,92526,jbellis,jbellis,,,,,,,,,29/Oct/10 22:04;jbellis;+1,29/Oct/10 22:12;urandom;committed.,"30/Oct/10 12:48;hudson;Integrated in Cassandra #581 (See [https://hudson.apache.org/hudson/job/Cassandra/581/])
    CASSANDRA-1686: assign a serialVersionUID

Patch by eevans
",03/Nov/10 20:51;jbellis;committed to 0.6 branch as well,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gen-thrift-py can fail but claims success,CASSANDRA-1692,12478832,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mdennis,mdennis,mdennis,11/1/2010 19:53,3/12/2019 14:04,3/13/2019 22:24,11/1/2010 22:44,0.7.0 rc 1,,Legacy/Tools,,0,,,,,,"{code}
Buildfile: /home/mdennis/mdev/cassandra-0.7.0-beta2/build.xml

gen-thrift-py:
     [echo] Generating Thrift Python code from /home/mdennis/mdev/cassandra-0.7.0-beta2/interface/cassandra.thrift ....
     [exec] 
     [exec] [FAILURE:/home/mdennis/mdev/cassandra-0.7.0-beta2/interface/cassandra.thrift:374] error: identifier ONE is unqualified!
     [exec] Result: 1

BUILD SUCCESSFUL
Total time: 1 second
{code}

""BUILD SUCCESSFUL"" is not the phrase I would use to describe the outcome of the command in this case :P",,,,,,,,,,,,,,,,,,,,01/Nov/10 21:02;mdennis;1692-trunk.txt;https://issues.apache.org/jira/secure/attachment/12458582/1692-trunk.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,51:13.8,,,no_permission,,,,,,,,,,,,20261,,,Mon Nov 01 22:44:13 UTC 2010,,,,,,0|i0g6sv:,92532,jbellis,jbellis,,,,,,,,,"01/Nov/10 19:54;mdennis;To be clear, I know the problem that causes it to fail; the complaint is that it claimed success","01/Nov/10 20:51;jbellis;Do you know how to fix it?

""ant gen-thrift-py"" is a convenience for developers only, I'm not going to lose much sleep over it not knowing how to handle an out of date thrift compiler.","01/Nov/10 21:02;mdennis;yes, I know how to fix it (patch attached)",01/Nov/10 22:44;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliTest crashing intermittently,CASSANDRA-1648,12478071,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,10/22/2010 14:37,3/12/2019 14:04,3/13/2019 22:24,10/26/2010 23:20,0.7 beta 3,,Legacy/Tools,,0,,,,,,"About 50% of the time I get

{code}
$ ant test -Dtest.name=CliTest
...
    [junit] Test org.apache.cassandra.cli.CliTest FAILED (crashed)
{code}",,,,,,,,,,,,,,,,,,,,26/Oct/10 15:49;xedin;CASSANDRA-1648.patch;https://issues.apache.org/jira/secure/attachment/12458077/CASSANDRA-1648.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,06:40.9,,,no_permission,,,,,,,,,,,,20238,,,Wed Oct 27 08:44:46 UTC 2010,,,,,,0|i0g6iv:,92487,jbellis,jbellis,,,,,,,,,22/Oct/10 15:06;xedin;I got such a thing few times too - this happens because EmbeddedCassandraService fails to start sometimes for no reason... Thats why I though it would be better to ask user to start server by hand first...,26/Oct/10 23:05;jbellis;Does just extending CleanupHelper fix it?,"26/Oct/10 23:20;jbellis;Yes, CleanupHelper ftw.

Thanks for figuring out the problem!","27/Oct/10 08:44;xedin;Yes, this is what was needed and I overlooked that we have CleanupHelper, sorry...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New subcolumn resurrect deleted subcolumns,CASSANDRA-1734,12479769,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,wenjun@appoji.com,wenjun@appoji.com,11/12/2010 3:43,3/12/2019 14:04,3/13/2019 22:24,11/15/2010 14:24,0.7.0 rc 1,,,,0,,,,,,"The followings are the conversation I had with jbellis on IRC:
have a question on deletion of super column....deletion of SuperColumn works fine....but when I add any new sub-column, the 'old' sub-columns reappear..how can I tell they are tombstoned
the 'old' sub-columns still have all the data in place, including timestamp
and the timestamp is older than markDeletaAt of the super column
<jbellis> appoji: right.  that is expected.  the subcolumns won't be tombstoned, the supercolumn tombstone should supress them.
<jbellis> shouldn't*
<jbellis> appoji: but writing a new subcolumn should resurrect the others.  can you submit a test case?

I am able to reproduce it with cli as followings:

1.  create column family UserGroup with column_type = 'Super' and gc_grace=5 and comparator = 'AsciiType' and subcomparator = 'BytesType'
2. set UserGroup ['100'] ['memberList']['paul']=''
3. del UserGroup ['100'] ['memberList']
4. wait for 5 seconds and ""list UserGroup"" does not return any data, which is correct
5. set UserGroup ['100'] ['memberList']['andrew']=''
6. now ""list UserGroup"" returns 2 sub-columns ('paul' and 'andrew')

here is server log for step 6:

DEBUG 22:40:52,378 range_slice
DEBUG 22:40:52,379 RangeSliceCommand{keyspace='Appoji', column_family='UserGroup
1', super_column=null, predicate=SlicePredicate(slice_range:SliceRange(start:80
01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 28
 0C 00 01 0B 00 03 00 00 00 0A 55 73 65 72 47 72 6F 75 70 31 00 0C 00 02 0C 00 0
2 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 6
5 5F 73 6C 69 63 65 73 00 00 00 28 0C 00 01 0B 00 03 00 00 00 0A 55 73 65 72 47
72 6F 75 70 31 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, r
eversed:false, count:100)), range=[0,0], max_keys=100}
DEBUG 22:40:52,380 restricted single token match for query [0,0]
DEBUG 22:40:52,381 local range slice
DEBUG 22:40:52,382 collecting 0 of 100: SuperColumn(memberList -delete at 128953
3231194000- [616e64726577:false:0@1289533250404000,7061756c:false:0@128953321401
5000,])
DEBUG 22:40:52,383 scanned DecoratedKey(9839004666223652184852086760848439587, 3
13030)

",Windows7-64,,,,,,,,,,,,,,,,,,,14/Nov/10 09:36;jbellis;1734.txt;https://issues.apache.org/jira/secure/attachment/12459550/1734.txt,15/Nov/10 14:08;tjake;1734_v2.txt;https://issues.apache.org/jira/secure/attachment/12459603/1734_v2.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,36:59.2,,,no_permission,,,,,,,,,,,,19366,,,Mon Nov 15 14:24:15 UTC 2010,,,,,,0|i0g727:,92574,tjake,tjake,,,,,,,,,14/Nov/10 09:36;jbellis;patch to add removeDeleted call to getRangeSlices (list) as it is for getColumnFamily (get),"15/Nov/10 14:08;tjake;The patch doesn't account for already deleted rows, where cf if null.

I've addressed this in the next patch, +1 with this change.",15/Nov/10 14:24;jbellis;committed v2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cli ""list"" gives unhelpful error when not authenticated",CASSANDRA-1731,12479746,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,11/11/2010 20:32,3/12/2019 14:04,3/13/2019 22:24,11/11/2010 21:03,0.7.0 rc 1,,Legacy/Tools,,0,,,,,,"{code}
[default@unknown] list Userline
Using default limit of 100
null
{code}
",,,,,,,,,,,,,,,,,,,,11/Nov/10 20:44;xedin;CASSANDRA-1731.patch;https://issues.apache.org/jira/secure/attachment/12459379/CASSANDRA-1731.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,20279,,,Thu Nov 11 21:03:39 UTC 2010,,,,,,0|i0g71j:,92571,jbellis,jbellis,,,,,,,,,"11/Nov/10 21:03;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clear endpoint cache after updating keyspace metadata,CASSANDRA-1741,12479850,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,11/13/2010 4:51,3/12/2019 14:04,3/13/2019 22:24,11/15/2010 14:54,0.7.0 rc 1,,,,0,,,,,,"If the replication factor or strategy (or options) change, we need to clear the cache so it can be repopulated with the new options. ",,,,,,,,,,,,,,,,,,,,13/Nov/10 04:51;jbellis;1741.txt;https://issues.apache.org/jira/secure/attachment/12459516/1741.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,41:47.7,,,no_permission,,,,,,,,,,,,20285,,,Mon Nov 15 14:54:37 UTC 2010,,,,,,0|i0g73r:,92581,gdusbabek,gdusbabek,,,,,,,,,15/Nov/10 14:41;gdusbabek;+1,15/Nov/10 14:54;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix jna errno reporting,CASSANDRA-1694,12478855,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,11/2/2010 5:20,3/12/2019 14:04,3/13/2019 22:24,11/2/2010 18:29,0.6.7,0.7.0 rc 1,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,02/Nov/10 17:21;jbellis;1694-take-2.txt;https://issues.apache.org/jira/secure/attachment/12458652/1694-take-2.txt,02/Nov/10 05:59;jbellis;1694-v3.txt;https://issues.apache.org/jira/secure/attachment/12458613/1694-v3.txt,02/Nov/10 05:39;jbellis;jna-3.2.7.jar;https://issues.apache.org/jira/secure/attachment/12458611/jna-3.2.7.jar,02/Nov/10 05:20;jbellis;jna.patch;https://issues.apache.org/jira/secure/attachment/12458608/jna.patch,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,09:09.5,,,no_permission,,,,,,,,,,,,20262,,,Tue Nov 02 18:29:18 UTC 2010,,,,,,0|i0g6tb:,92534,brandon.williams,brandon.williams,,,,,,,,,02/Nov/10 05:55;jbellis;v2 adds support for crappy old jna versions,"02/Nov/10 06:09;brandon.williams;+1.  Needs jna 3.2.7 to compile, but will run (with inferior logging) with lesser jna versions.",02/Nov/10 06:12;jbellis;committed,"02/Nov/10 17:19;jbellis;This causes
{code}
ERROR 12:11:56,633 Exception encountered during startup.
java.lang.NoClassDefFoundError: com/sun/jna/LastErrorException
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:67)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
Caused by: java.lang.ClassNotFoundException: com.sun.jna.LastErrorException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
{code}

when jna is not present at runtime.  reverted for now.","02/Nov/10 17:22;jbellis;-take-2 patch adds a pre-emptive call to getLastError in an attempt to fix the problem a different way.
{code}
            // calling getLastError involves system calls that can reset errno, so you don't
            // want the first time you call it to be when you actually have an error to diagnose
{code}",02/Nov/10 17:27;brandon.williams;Take-2 has no effect for me regardless of jna version.,"02/Nov/10 18:29;jbellis;re-committed v3 with catches changed to look like this instead:
{code}
        catch (RuntimeException e)
        {
            if (!(e instanceof LastErrorException))
                throw e;

            ... handle errno ...
        }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix misuse of DataOutputBuffer.getData in AntiEntropyService,CASSANDRA-1729,12479731,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,11/11/2010 17:29,3/12/2019 14:04,3/13/2019 22:24,11/19/2010 17:06,0.6.9,,,,0,,,,,,"As reported by Schubert Zhang, AntiEntropyService is ignoring the length of the input buffer.
{code:java}
byte[] rowhash = FBUtilities.hash(""SHA-256"", row.key.key.getBytes(), row.buffer.getData());
{code}

While this isn't affecting our accuracy, it would break validation if we started reusing buffers in CompactedRow. This issue has already been fixed in 0.7.",,,,,,,,,,,,,,,,,,,,11/Nov/10 17:38;stuhood;0001-Remove-varargs-from-FBUtilities.hash-and-correctly-use.txt;https://issues.apache.org/jira/secure/attachment/12459367/0001-Remove-varargs-from-FBUtilities.hash-and-correctly-use.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,06:23.8,,,no_permission,,,,,,,,,,,,20277,,,Fri Nov 19 17:55:58 UTC 2010,,,,,,0|i0g713:,92569,jbellis,jbellis,,,,,,,,,19/Nov/10 17:06;jbellis;committed,"19/Nov/10 17:55;hudson;Integrated in Cassandra-0.6 #9 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/9/])
    Fix misuse of DataOutputBuffer.getData in AntiEntropyService
patch by Stu Hood; reviewed by jbellis for CASSANDRA-1729
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli help output should mention the need for semicolons,CASSANDRA-1770,12480726,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,thepaul,thepaul,11/23/2010 20:15,3/12/2019 14:04,3/13/2019 22:24,11/24/2010 16:25,0.7.0 rc 2,,Legacy/Tools,,0,,,,,,"When running cassandra-cli for the first time after upgrading beta3 -> rc1, I thought it was broken, because any command I typed (""?"", ""quit"", ""exit"", ""use system"", etc) just printed some empty space and sat there until I killed it. I didn't know it was a continuation prompt, or that semicolons were needed.

The startup banner message currently says:

{noformat}
Connected to: ""Test Cluster"" on localhost/9160
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
{noformat}

I believe that the example commands should have semicolons after them, to let the user know about the change.

Also, a ""?"" by itself should probably not require a semicolon.",Debian squeeze with 0.7.0~rc1 package installed,,,,,,,,,,,,,,,,,,,24/Nov/10 15:36;xedin;CASSANDRA-1770.patch;https://issues.apache.org/jira/secure/attachment/12460374/CASSANDRA-1770.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,25:06.2,,,no_permission,,,,,,,,,,,,20303,,,Sat Dec 11 07:35:10 UTC 2010,,,,,,0|i0g7a7:,92610,jbellis,jbellis,,,,,,,,,"23/Nov/10 20:18;thepaul;Oh yeah, and when a semicolon isn't given, the continuation line should have a secondary prompt, like "">  "" or ""...  "" or something.",24/Nov/10 16:25;jbellis;committed,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
handle replica unavailability in index scan,CASSANDRA-1755,12480376,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,jbellis,jbellis,11/18/2010 21:46,3/12/2019 14:04,3/13/2019 22:24,11/19/2010 15:17,0.7.0 rc 1,,,,0,,,,,,StorageProxy.scan should return UnavailableException to clients under this condition,,,,,,,,,,,,,,,,,,,,19/Nov/10 14:58;tjake;1755_v1.txt;https://issues.apache.org/jira/secure/attachment/12460013/1755_v1.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,37:34.0,,,no_permission,,,,,,,,,,,,20294,,,Fri Nov 19 15:37:33 UTC 2010,,,,,,0|i0g76v:,92595,jbellis,jbellis,,,,,,,,,"19/Nov/10 15:17;jbellis;committed, thanks!","19/Nov/10 15:37;hudson;Integrated in Cassandra-0.7 #17 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/17/])
    handle replica unavailability in index scan
patch by tjake; reviewed by jbellis for CASSANDRA-1755
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Indexes: Auto-generating the CFname may collide with user-generated names,CASSANDRA-1761,12480462,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,11/19/2010 22:51,3/12/2019 14:04,3/13/2019 22:24,3/15/2011 16:27,0.7.5,,,,0,,,,,,"{noformat}column_families:
  - name: CF
    comparator: BytesType
    column_metadata: 
      - name: foo
        index_name: 626172
        index_type: KEYS
      - name: bar
        index_type: KEYS{noformat}

Auto-generated versus user-supplied names collide in the YAML above. The code:
{code}cfname = parentCf + ""."" + (info.getIndexName() == null ? FBUtilities.bytesToHex(info.name) : info.getIndexName()){code}

From the first ColumnDefinition, we create cfname = ""CF.626172"" (from the fail clause of the ternany, user-supplied name)
From the second ColumnDefinition, we create cfname = ""CF.626172"" (from the pass clause of the ternary, we generate the name)

They're in hex form. This is possible, but fairly unlikely that someone will do this.",,,57600,57600,,0%,57600,57600,,,,,,,,,,,,15/Mar/11 00:11;jhermes;1761-0.7.txt;https://issues.apache.org/jira/secure/attachment/12473640/1761-0.7.txt,15/Mar/11 00:00;jhermes;1761.txt;https://issues.apache.org/jira/secure/attachment/12473638/1761.txt,15/Mar/11 00:03;jhermes;repro.cli;https://issues.apache.org/jira/secure/attachment/12473639/repro.cli,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,33:15.0,,,no_permission,,,,,,,,,,,,20298,,,Tue Mar 15 16:46:45 UTC 2011,,,,,,0|i0g787:,92601,jbellis,jbellis,,,,,,,,,20/Nov/10 00:05;jhermes;CF.bar -> CF.626172.,"16/Dec/10 22:33;jbellis;This will require some re-organization to solve.  Currently, ColumnDefinition index_name is just copied from CfDef and may be null; final index name is determined by CFMetaData.newIndexMetadata, which has no context as to what other indexes exist in the CF.

What we should do is create the final index name at the CassandraServer level, and validate at that point that it does not cause conflicts with existing ones.  (Also: index_name and index_type fields of CD become final.)

(This lets us return appropriate InvalidRequest exceptions to the client instead of failing with an internal error, too.)

Then by the time we get to addIndex/newIndexMetadata, all we need to do is a final sanity check as a defense against users violating the one-schema-change-at-a-time rule.","14/Mar/11 23:59;jhermes;Because the column_metadata has to be passed in full, it can be fully validated in ThriftValidation of the cf_def in the system_{update,add}_column_family and system_add_keyspace calls.

In related news, I also found out that we never validated CfDefs in system_update_column_family, so I'm surprised to say the least and glad it was found in an innocuous bug instead of something more serious.

As for the one-schema-change-at-a-time rule, this is now enforced by default with validateSchemaAgreement() calls in CassandraServer.","15/Mar/11 00:03;jhermes;Also attaching repro script.

Without patch you should see an error trying to register an MBean twice, and the first index will no longer work.
After patch you should see an IRE.","15/Mar/11 00:11;jhermes;1761.txt is for trunk.
1761-0.7.txt is for 0.7*.

(Slight difference in imports makes it not apply cleanly.)","15/Mar/11 16:27;jbellis;committed w/ some improvements:

- added a system test
- fix migration failure to throw InternalError back to client instead of InvalidRequestException (a bug found by the new test)
- use Set instead of List for .contains checks","15/Mar/11 16:46;hudson;Integrated in Cassandra-0.7 #382 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/382/])
    validate index names
patch by jhermes and jbellis for CASSANDRA-1761
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JNA Native check can throw a NoSuchMethodError,CASSANDRA-1760,12480451,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,11/19/2010 21:33,3/12/2019 14:04,3/13/2019 22:24,11/19/2010 21:49,0.6.9,0.7.0 rc 1,,,0,,,,,,"Looks like older versions of JNA have a different Native.register() method

From IRC:
hi, i'm having trouble starting cassandra up...the error is very bizarre: java.lang.NoSuchMethodError: com.sun.jna.Native.register(Ljava/lang/String;)V",,,,,,,,,,,,,,,,,,,,19/Nov/10 21:34;tjake;1760_v1.txt;https://issues.apache.org/jira/secure/attachment/12460055/1760_v1.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,49:34.9,,,no_permission,,,,,,,,,,,,20297,,,Fri Nov 19 21:49:34 UTC 2010,,,,,,0|i0g77z:,92600,jbellis,jbellis,,,,,,,,,19/Nov/10 21:43;tjake;it was jruby :(,19/Nov/10 21:49;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra deb should depend on adduser,CASSANDRA-1816,12492012,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,12/3/2010 18:04,3/12/2019 14:04,3/13/2019 22:24,12/4/2010 1:51,0.7.0 rc 2,,Packaging,,0,,,,,,"The cassandra debian package uses the addgroup and adduser commands in its postinst script, which are found in the 'adduser' package, but the cassandra debian package does not depend on adduser.   When a user does not already have adduser installed, this will lead to an error like:

{noformat}
Setting up cassandra (0.7.0~rc1) ...
/var/lib/dpkg/info/cassandra.postinst: 50: addgroup: not found
dpkg: error processing cassandra (--configure):
 subprocess installed post-installation script returned error exit status 127
{noformat}

Yes, this won't happen much, because systems without adduser installed are rare.  But adduser is not ""Essential: yes"", so it will happen sometimes in bare-bones VMs or development environments.",Debian squeeze,,,,,,,,,,,,,,,,,,,03/Dec/10 18:05;thepaul;cass-1816.txt;https://issues.apache.org/jira/secure/attachment/12465251/cass-1816.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,51:24.0,,,no_permission,,,,,,,,,,,,20326,,,Tue Dec 07 20:30:52 UTC 2010,,,,,,0|i0g7l3:,92659,urandom,urandom,,,,,,,,,04/Dec/10 01:51;urandom;committed; thanks!,"07/Dec/10 20:30;hudson;Integrated in Cassandra #615 (See [https://hudson.apache.org/hudson/job/Cassandra/615/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"debian initscript sometimes mistakenly thinks it failed, gives extraneous output",CASSANDRA-1772,12480761,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,11/24/2010 1:15,3/12/2019 14:04,3/13/2019 22:24,11/24/2010 23:11,0.7.0 rc 2,,Packaging,,0,,,,,,"On my test systems, which are all relatively slow VMs, the Cassandra debian initscript usually thinks it fails to start, even though the startup was successful.  It appears that jsvc forks the daemon process and exits, and the initscript check for the running Cassandra service occurs before the new daemon is able to initialize itself and create its pidfile.

On top of that, most invocations end up spitting out a small amount of garbage from /bin/ps, in addition to the typical ""Stopping Cassandra: cassandra."" log messages one sees if verbose=yes in /etc/default/rcS.  This is not very flattering.

Finally, the initscript should provide the ""status"" command to meet current LSB spec. The functionality is mostly complete already anyway, and it can be quite useful.",Debian Squeeze with cassandra 0.7.0~rc1 on a slicehost VM,,,,,,,,,,,,,,,,,,,24/Nov/10 01:31;thepaul;cass-add-status.patch.txt;https://issues.apache.org/jira/secure/attachment/12460327/cass-add-status.patch.txt,24/Nov/10 01:34;thepaul;cass-wait-for-start.patch.txt;https://issues.apache.org/jira/secure/attachment/12460328/cass-wait-for-start.patch.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,11:02.3,,,no_permission,,,,,,,,,,,,20305,,,Sat Dec 11 07:35:11 UTC 2010,,,,,,0|i0g7av:,92613,urandom,urandom,,,,,,,,,"24/Nov/10 01:31;thepaul;cass-add-status.patch adds a slightly more robust is_running check (ensures the command line of the process $(cat $PIDFILE) matches what we expect, instead of just checking that there is a process with that number).

It also gets rid of the extraneous ""ps"" output to the terminal, and adds the ""status"" command to the initscript.","24/Nov/10 01:34;thepaul;cass-wait-for-startup.patch.txt allows the ""start"" and ""restart"" actions to wait for up to 10 seconds (configurable) for the Cassandra service to start up fully.

This should eliminate the false failure messages.","24/Nov/10 23:11;urandom;committed, w/ discussed (minor )changes.","11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli craps its pants and dies when 'gc_grace_seconds' is used in cf creation,CASSANDRA-1549,12475251,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,9/27/2010 22:52,3/12/2019 14:04,3/13/2019 22:24,9/28/2010 1:40,0.7 beta 2,,Legacy/Tools,,0,,,,,,"{noformat}
trunk$ bin/cassandra-cli --host localhost
Connected to: ""Test Cluster"" on localhost/9160
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
[default@unknown] use Keyspace1
Authenticated to keyspace: Keyspace1
[default@Keyspace1] create column family cfname with gc_grace_seconds=86400
Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.cli.CliClient.executeAddColumnFamily(CliClient.java:817)
        at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:105)
        at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:230)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:302)
{noformat}

it's just a missing ""break;"" statement in CliClient.java.",encountered on Debian Squeeze with Cassandra from HEAD (r1001931),,300,300,,0%,300,300,,,,,,,,,,,,27/Sep/10 22:56;thepaul;0000-fix-cassandra-cli-gc_grace_seconds.patch;https://issues.apache.org/jira/secure/attachment/12455771/0000-fix-cassandra-cli-gc_grace_seconds.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,40:09.7,,,no_permission,,,,,,,,,,,,20195,,,Tue Sep 28 13:31:32 UTC 2010,,,,,,0|i0g5wv:,92388,jbellis,jbellis,,,,,,,,,27/Sep/10 22:56;thepaul;dumbest patch evar,28/Sep/10 01:40;jbellis;committed,"28/Sep/10 13:31;hudson;Integrated in Cassandra #549 (See [https://hudson.apache.org/hudson/job/Cassandra/549/])
    fix setting gc_grace_secondsvia CLI.
patch by Paul Cannon; reviewed by jbellis for CASSANDRA-1549
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal error from malformed remove with supercolumns,CASSANDRA-1866,12493352,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,jbellis,jbellis,12/15/2010 17:56,3/12/2019 14:04,3/13/2019 22:24,12/19/2010 3:40,0.6.9,0.7.0 rc 3,Legacy/CQL,,0,,,,,,"From the ML: 

""I just call ""remove"" method where ColumnPath structure has ""column_family"" and ""column"" members set (""super_column"" not set).""
{code}
ERROR 17:57:46,924 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.DeletedColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.DeletedColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:318)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:298)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:82)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:68)
        at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:344)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:355)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:269)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:194)
        at org.apache.cassandra.service.StorageProxy$1.runMayThrow(StorageProxy.java:197)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR 17:57:46,928 Fatal exception in thread Thread[MUTATION_STAGE:2,5,main]
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.DeletedColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.DeletedColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:318)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:298)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:82)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:68)
        at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:344)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:355)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:333)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:269)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:194)
        at org.apache.cassandra.service.StorageProxy$1.runMayThrow(StorageProxy.java:197)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{code}

Fix: add system test to catch this & raise invalidrequestexception from ThriftValidation",,,,,,,,,,,,,,,,,,,,17/Dec/10 18:40;thobbs;1866-0.6-test.txt;https://issues.apache.org/jira/secure/attachment/12466481/1866-0.6-test.txt,17/Dec/10 18:40;thobbs;1866-0.6.txt;https://issues.apache.org/jira/secure/attachment/12466480/1866-0.6.txt,16/Dec/10 22:22;thobbs;1866-0.7-test.txt;https://issues.apache.org/jira/secure/attachment/12466413/1866-0.7-test.txt,16/Dec/10 22:22;thobbs;1866-0.7.txt;https://issues.apache.org/jira/secure/attachment/12466412/1866-0.7.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,24:53.6,,,no_permission,,,,,,,,,,,,20350,,,Sun Dec 19 04:03:35 UTC 2010,,,,,,0|i0g7vz:,92708,jbellis,jbellis,,,,,,,,,16/Dec/10 22:24;thobbs;Patches also apply to trunk.,"17/Dec/10 15:34;jbellis;committed, thanks!",17/Dec/10 15:35;jbellis;can you backport to 0.6 too?,"17/Dec/10 17:17;hudson;Integrated in Cassandra-0.7 #94 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/94/])
    ",17/Dec/10 18:40;thobbs;0.6 versions attached.,"19/Dec/10 03:40;jbellis;committed, thanks","19/Dec/10 04:03;hudson;Integrated in Cassandra-0.6 #28 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/28/])
    backport CASSANDRA-1866 from 0.7
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
return invalidrequest when client attempts to create secondary index on supercolumns,CASSANDRA-1813,12491990,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,12/3/2010 14:52,3/12/2019 14:04,3/13/2019 22:24,12/3/2010 18:56,0.7.0 rc 2,,Feature/2i Index,,0,,,,,,,,,,,,,,,,,,,,,,,,,,03/Dec/10 16:13;jbellis;1813.txt;https://issues.apache.org/jira/secure/attachment/12465237/1813.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,03:29.1,,,no_permission,,,,,,,,,,,,20325,,,Sat Dec 11 07:35:18 UTC 2010,,,,,,0|i0g7kf:,92656,gdusbabek,gdusbabek,,,,,,,,,03/Dec/10 16:13;jbellis;also fixes assumption in validateCfDef that presence of subcomparator_type implies supercolumn-ness,03/Dec/10 17:03;gdusbabek;+1,03/Dec/10 18:56;jbellis;committed,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cli support index_type enum names in column_metadata,CASSANDRA-1810,12491944,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,12/2/2010 23:58,3/12/2019 14:04,3/13/2019 22:24,12/3/2010 14:26,0.7.0 rc 2,,Legacy/Tools,,0,,,,,,currently you have to write index_type: 0 when index_type: KEYS would be better,,,,,,,,,,,,,,,,,,,,03/Dec/10 10:38;xedin;CASSANDRA-1810.patch;https://issues.apache.org/jira/secure/attachment/12465222/CASSANDRA-1810.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,40:28.5,,,no_permission,,,,,,,,,,,,20324,,,Sat Dec 11 07:35:17 UTC 2010,,,,,,0|i0g7jr:,92653,jbellis,jbellis,,,,,,,,,03/Dec/10 10:40;xedin;Please review this after CASSANDRA-1809. Current version will support both - numeric and string values for index_type.,03/Dec/10 10:41;xedin;Work branch: cassandra-0.7 (latest commit was d6b1d581e9ad28ea2106462d306304e821a335f9),03/Dec/10 14:26;jbellis;committed,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mx4j does not load,CASSANDRA-1832,12492636,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,rantav,rstml,rstml,12/7/2010 16:28,3/12/2019 14:04,3/13/2019 22:24,12/8/2010 1:01,0.7.0 rc 3,,Legacy/Tools,,0,,,,,,"Adding mx4j-tools.jar (latest) to the library causes following exception:

{code}
 WARN 20:22:25,123 Could not start register mbean in JMX
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.cassandra.utils.Mx4jTool.maybeLoad(Mx4jTool.java:67)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:169)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)
Caused by: java.net.BindException: Address already in use
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:365)
	at java.net.ServerSocket.bind(ServerSocket.java:319)
	at java.net.ServerSocket.<init>(ServerSocket.java:185)
	at mx4j.tools.adaptor.PlainAdaptorServerSocketFactory.createServerSocket(PlainAdaptorServerSocketFactory.java:24)
	at mx4j.tools.adaptor.http.HttpAdaptor.createServerSocket(HttpAdaptor.java:672)
	at mx4j.tools.adaptor.http.HttpAdaptor.start(HttpAdaptor.java:478)
	... 9 more
{code}",CentOS 5.5,,,,,,,,,,,,,,,,,,,07/Dec/10 21:48;rantav;CASSANDRA-1832.patch;https://issues.apache.org/jira/secure/attachment/12465736/CASSANDRA-1832.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,40:07.0,,,no_permission,,,,,,,,,,,,20332,,,Sat Dec 11 07:35:18 UTC 2010,,,,,,0|i0g7on:,92675,jbellis,jbellis,,,,,,,,,"07/Dec/10 17:24;rstml;I did a bit more tests and here are some results which might help:

1. JMX port set to 9090 in cassandra-env.sh
2. On the machine where another service running on 8080 we get exception above
3. On the machine where no service running on 8080 we don't get any exception and MX4J runs on port 9090

Seems like something checks for port 8080 even though it is configured to run on 9090.","07/Dec/10 21:40;rantav;I think there's a confusion. 
There are two ports in business, one is the JMX port (default is 8080) and one is the MX4J port (default 8081)
If the JMX port is used when cassandra starts you see the following exception, which is different from what's pasted in this bug report:

apache-cassandra-0.7.0-rc1 $ bin/cassandra -f 
Error: Exception thrown by the agent : java.rmi.server.ExportException: Port already in use: 8080; nested exception is: 
	java.net.BindException: Address already in use

If mx4j's port, by default 8081, is used, then you see the report from above, which btw isn't fatal, the server operates correctly but without mx4j.

WARN 20:22:25,123 Could not start register mbean in JMX
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.cassandra.utils.Mx4jTool.maybeLoad(Mx4jTool.java:67)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:169)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:55)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:216)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:134)
Caused by: java.net.BindException: Address already in use
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:365)
	at java.net.ServerSocket.bind(ServerSocket.java:319)
	at java.net.ServerSocket.<init>(ServerSocket.java:185)
	at mx4j.tools.adaptor.PlainAdaptorServerSocketFactory.createServerSocket(PlainAdaptorServerSocketFactory.java:24)
	at mx4j.tools.adaptor.http.HttpAdaptor.createServerSocket(HttpAdaptor.java:672)
	at mx4j.tools.adaptor.http.HttpAdaptor.start(HttpAdaptor.java:478)
	... 9 more


So the problem in this case. I believe was that mx4j's port was bound to a different process.
To control the port used by mx4j use  -Dmx4jport=8082. See https://issues.apache.org/jira/browse/CASSANDRA-1068 for more details.

I think this is not a bug and recommend to close it as such.
I will, however, attach a patch for trunk to make this more obvious and add -Dmx4jport=8081 to conf/cassandra-env.sh",07/Dec/10 21:48;rantav;Patch that adds the variables MX4J_ADDRESS and MX4J_PORT to conf/cassandra-env.sh make configuration for mx4j obvious.,"07/Dec/10 22:03;rstml;You right Ran, I checked this machine again and I have another service listening on 8081.
For some reason I thought that MX4J uses same port.

With config options we can close it now.","08/Dec/10 01:01;jbellis;committed, thanks!","11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""null"" error creating CF from cli",CASSANDRA-1835,12492695,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,12/8/2010 1:37,3/12/2019 14:04,3/13/2019 22:24,12/10/2010 16:37,0.7.0 rc 3,,Legacy/Tools,,0,,,,,,"This fails with only ""null"" as the failure message:

{code}
create column family test1 with column_type = 'Super' and comparator = 'LongType' and column_metadata=[{column_name:a,validation_class:LongType}];
{code}",,,,,,,,,,,,,,,,,,,,08/Dec/10 10:35;xedin;CASSANDRA-1835.patch;https://issues.apache.org/jira/secure/attachment/12465789/CASSANDRA-1835.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,40:40.7,,,no_permission,,,,,,,,,,,,20335,,,Sat Dec 11 07:35:17 UTC 2010,,,,,,0|i0g7pb:,92678,jbellis,jbellis,,,,,,,,,09/Dec/10 20:08;jbellis;should we require subcomparator instead of guessing?  that may be better.,09/Dec/10 20:40;xedin;I think warning here will be a pretty good option but as you think the best...,"09/Dec/10 22:47;xedin;In other words I think we should keep it with warning, what do you say?",10/Dec/10 16:37;jbellis;committed,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cli executeGetWithConditions is not case-insensitive for CF names,CASSANDRA-1809,12491943,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,12/2/2010 23:57,3/12/2019 14:04,3/13/2019 22:24,12/3/2010 14:25,0.7.0 rc 2,,Legacy/Tools,,0,,,,,,"{code}
        String columnFamily = statement.getChild(0).getText();
{code}

is not being normalized for case...

I tried
{code}
-        String columnFamily = statement.getChild(0).getText();
+        String columnFamily = CliCompiler.getColumnFamily(statement.getChild(0), keyspacesMap.get(keySpace).cf_defs);
{code}

but that broke it, all gets returned null (missing exception message?)",,,,,,,,,,,,,,,,,,,,03/Dec/10 10:12;xedin;CASSANDRA-1809.patch;https://issues.apache.org/jira/secure/attachment/12465220/CASSANDRA-1809.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,12:54.8,,,no_permission,,,,,,,,,,,,20323,,,Sat Dec 11 07:35:15 UTC 2010,,,,,,0|i0g7jj:,92652,jbellis,jbellis,,,,,,,,,03/Dec/10 10:12;xedin;Tests are updated too.,03/Dec/10 10:41;xedin;Work branch: cassandra-0.7 (latest commit was d6b1d581e9ad28ea2106462d306304e821a335f9),03/Dec/10 14:25;jbellis;committed,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
heisenbug in SSTableExportTest,CASSANDRA-1892,12493807,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,jbellis,jbellis,12/21/2010 21:21,3/12/2019 14:04,3/13/2019 22:24,12/24/2010 18:23,0.7.0,,,,0,test,,,,,"{code}
    [junit] java.lang.IndexOutOfBoundsException: Index: 4, Size: 4
    [junit] 	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
    [junit] 	at java.util.ArrayList.get(ArrayList.java:322)
    [junit] 	at org.apache.cassandra.tools.SSTableExportTest.testExportSimpleCf(SSTableExportTest.java:130)
{code}",,,,,,,,,,,,,,,,,,,,24/Dec/10 18:08;tjake;1892.txt;https://issues.apache.org/jira/secure/attachment/12466946/1892.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,08:43.0,,,no_permission,,,,,,,,,,,,20357,,,Mon Dec 27 17:45:26 UTC 2010,,,,,,0|i0g81z:,92735,jbellis,jbellis,,,,,,,,,21/Dec/10 21:22;jbellis;most of the time this passes.,24/Dec/10 18:08;tjake;The test checks for an expiring column but the localDeleteTime is specified as now.  so if the test takes longer than a second to run it fails.  I verified this with a Thread.sleep call. The patch adds an extra 42 seconds to the expiring column.,"24/Dec/10 18:23;jbellis;committed, thanks!","27/Dec/10 17:45;hudson;Integrated in Cassandra-0.7 #118 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/118/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to get columns from super column in cassandra-cli (0.7-rc2),CASSANDRA-1899,12494000,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,xedin,xedin,12/23/2010 18:48,3/12/2019 14:04,3/13/2019 22:24,12/23/2010 18:57,0.7.0,,Legacy/Tools,,0,,,,,,"I'm using cassandra 0.7.0-rc2. 

When I tried to get column contents in a super column of Super CF like below;
{code}
get myCF['key']['scName'];
{code}

the client reply: supercolumn parameter is not optional for super CF user
","The cluster was made in cassandra-0.7.0-beta1 and the script was

create column family myCF with column_type='Super' and comparator='UTF8Type' AND subcomparator='UTF8Type';",,,,,,,,,,,,,,,,,,,23/Dec/10 18:49;xedin;super_columns_get.patch;https://issues.apache.org/jira/secure/attachment/12466908/super_columns_get.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,57:19.0,,,no_permission,,,,,,,,,,,,20361,,,Wed Dec 29 16:38:24 UTC 2010,,,,,,0|i0g83j:,92742,jbellis,jbellis,,,,,,,,,23/Dec/10 18:49;xedin;Tests added. work branch: trunk (latest commit 44f0214b1d0e0ab11ffa8d8df1055458e17e1e45),23/Dec/10 18:57;jbellis;committed,"23/Dec/10 19:46;hudson;Integrated in Cassandra-0.7 #114 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/114/])
    fix CLI get recognition of supercolumns
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-1899
",29/Dec/10 16:23;tcn;Just hit this one. Why 0.7.1 and not 0.7.0rc3 (rc4)?,"29/Dec/10 16:38;jbellis;We can't fix it in rc3 since rc3 is already released.

This isn't severe enough to justify an rc4.  If we do roll an rc4 for other reasons, it will be incorporated",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timed out reads not counted in metrics,CASSANDRA-1893,12493818,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,12/21/2010 23:07,3/12/2019 14:04,3/13/2019 22:24,12/22/2010 18:47,0.7.0,,,,0,,,,,,Reads that experience timeouts (or other exceptions) are not tracked in StorageProxy latencies.,,,,,,,,,,,,,,,,,,,,21/Dec/10 23:09;stuhood;0001-Move-metrics-into-finally-blocks.txt;https://issues.apache.org/jira/secure/attachment/12466775/0001-Move-metrics-into-finally-blocks.txt,21/Dec/10 23:09;stuhood;0002-Fully-expose-read-write-range-histograms-in-StoragePro.txt;https://issues.apache.org/jira/secure/attachment/12466776/0002-Fully-expose-read-write-range-histograms-in-StoragePro.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,47:22.1,,,no_permission,,,,,,,,,,,,20358,,,Wed Dec 22 19:05:02 UTC 2010,,,,,,0|i0g827:,92736,jbellis,jbellis,,,,,,,,,"21/Dec/10 23:09;stuhood;Patches to record all SP calls, and expos the histograms that we were recording anyway.","22/Dec/10 18:47;jbellis;committed, thanks!","22/Dec/10 19:05;hudson;Integrated in Cassandra-0.7 #109 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/109/])
    count timeouts in storageproxy latencies, and include latency
histograms in StorageProxyMBean
patch by Stu Hood; reviewed by jbellis for CASSANDRA-1893
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in get_slice quorum read,CASSANDRA-1883,12493608,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,kmueller,kmueller,12/18/2010 21:16,3/12/2019 14:04,3/13/2019 22:24,12/21/2010 21:37,0.6.9,0.7.0 rc 3,,,0,,,,,,"Getting this NPE as of the 2010-12-17 0.7 trunk.  Some data may be corrupt somewhere on a node.  It could be a null key somewhere.

ERROR [pool-1-thread-28] 2010-12-18 12:53:20,411 Cassandra.java (line 2707) Internal error processing get_slice
java.lang.NullPointerException
        at org.apache.cassandra.service.DigestMismatchException.<init>(DigestMismatchException.java:30)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:92)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:43)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:91)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:362)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:229)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:225)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:301)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:263)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:2699)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)",Linux Fedora 12 x86_64,,,,,,,,,,,,,,,,,,,20/Dec/10 05:23;jbellis;1883.txt;https://issues.apache.org/jira/secure/attachment/12466603/1883.txt,19/Dec/10 02:09;kmueller;digestmismatch-debug.txt;https://issues.apache.org/jira/secure/attachment/12466556/digestmismatch-debug.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,50:22.2,,,no_permission,,,,,,,,,,,,19360,,,Tue Dec 21 21:37:43 UTC 2010,,,,,,0|i0g7zr:,92725,tjake,tjake,,,,,,,,,18/Dec/10 21:26;kmueller;Additional information: one of the SSD raid0s went bad recently.  This may have produced weird data for one cassandra node.   ,"19/Dec/10 02:09;kmueller;this is a debug output from a node with this NPE happening around the same time.  If you need more from the log, I have the rest of it available",19/Dec/10 03:50;jbellis;Does this reproduce whenever you query a certain key?,"20/Dec/10 03:54;brandon.williams;Note that there was data here inserted from RC2, and then CASSANDRA-1847 was encountered causing the upgrade to trunk, so this may just be fallout from previous corruption.",20/Dec/10 05:23;jbellis;patch to fix NPE when two different digests arrive before the data read does,20/Dec/10 05:59;mdennis;+1,20/Dec/10 17:33;tjake;+1 this was a regression from CASSANDRA-1830,21/Dec/10 21:37;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ByteBufferUtil.clone shouldn't mutate the passed bytebuffer,CASSANDRA-1847,12493038,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,12/13/2010 1:57,3/12/2019 14:04,3/13/2019 22:24,12/13/2010 16:56,0.7.0 rc 3,,,,0,,,,,,"Currently ByteBufferUtil.clone uses .mark and .reset on the passed ByteBuffer.

This is fine when using thrift because no two ByteBuffer are being accessed at the same time, but this could be thread-unsafe if the same BB was passed concurrently.

Solandra is having problems with this (Solandra shares JVM with Cassandra).",,,,,,,,,,,,,,,,,,,,13/Dec/10 02:13;jbellis;1847-v2.txt;https://issues.apache.org/jira/secure/attachment/12466113/1847-v2.txt,13/Dec/10 15:04;tjake;1847-v3.txt;https://issues.apache.org/jira/secure/attachment/12466137/1847-v3.txt,13/Dec/10 16:44;jbellis;1847-v4.txt;https://issues.apache.org/jira/secure/attachment/12466146/1847-v4.txt,13/Dec/10 02:00;tjake;1847_v1.txt;https://issues.apache.org/jira/secure/attachment/12466111/1847_v1.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,13:25.9,,,no_permission,,,,,,,,,,,,20342,,,Mon Dec 13 17:14:37 UTC 2010,,,,,,0|i0g7rz:,92690,jbellis,jbellis,,,,,,,,,13/Dec/10 02:13;jbellis;v2 uses arraycopy instead of manual loop and enforces non-null,13/Dec/10 08:59;slebresne;+1 on v2,"13/Dec/10 14:00;tjake;Right, thats faster but I was thinking ahead for CASSANDRA-1714 we'd need to use this for putting a row into the cache and .array() only works with HeapByteBuffers.  ",13/Dec/10 14:49;jbellis;should we add an instanceof check for HeapByteBuffer then? seems like it's probably worth it. ,13/Dec/10 15:04;tjake;v3 checks if buffer isDirect(),"13/Dec/10 16:44;jbellis;BB.get(i) adds arrayoffset to the given index, so I think the loop in v3 is buggy.  v4 attached","13/Dec/10 16:52;tjake;ok took ""absolute"" too literally then :)
+1 v4",13/Dec/10 16:56;jbellis;committed,"13/Dec/10 17:14;hudson;Integrated in Cassandra-0.7 #72 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/72/])
    make ByteBufferUtil.clone thread-safe
patch by tjake and jbellis for CASSANDRA-1847
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMX threads leak in NodeProbe,CASSANDRA-1665,12478392,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,billa,billa,billa,10/26/2010 20:32,3/12/2019 14:04,3/13/2019 22:24,10/26/2010 23:12,0.7.0 rc 2,,Legacy/Tools,,0,,,,,,There is a JMX threads leak in NodeProbe.  It creates and uses a JMXConnector but never calls its close() method.  I am working on a patch which add a close() method to NodeProbe  that calls JMXConnector.close().,,,,,,,,,,,,,,,,,,,,26/Oct/10 21:59;billa;trunk-1665.txt;https://issues.apache.org/jira/secure/attachment/12458109/trunk-1665.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,12:00.6,,,no_permission,,,,,,,,,,,,20248,,,Tue Oct 26 23:12:00 UTC 2010,,,,,,0|i0g6mv:,92505,jbellis,jbellis,,,,,,,,,26/Oct/10 21:59;billa;I have added a close() method to NodeProbe.  I have also added a close() method that calls NodeProbe.close() to ClusterCmd since it creates its own private instance of NodeProbe in its constructors..,"26/Oct/10 23:12;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MutationTest of the distributed-test suite fails,CASSANDRA-1964,12495250,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,tawamudu,tawamudu,1/11/2011 14:08,3/12/2019 14:04,3/13/2019 22:24,1/19/2011 19:53,0.7.1,0.8 beta 1,,,0,,,,,,"MutationTest of the distributed-test test suite causes errors on trunk.

To reproduce, issue:

ant distributed-test -Dwhirr.config=<path_to_whirr_config_file>

from the project root.

relevant whirr configuration settings used:

whirr.service-name=cassandra
whirr.cluster-name=cassandra_test
whirr.instance-templates=4 cassandra
whirr.version=0.3.0-incubating-SNAPSHOT
whirr.location-id=us-west-1
whirr.image-id=us-west-1/ami-16f3a253
whirr.hardware-id=m1.large
whirr.blobstore.provider=s3
whirr.blobstore.container=tawamuducassandratests
whirr.provider=ec2
whirr.run-url-base=http://hoodidge.net/scripts/

Traceback:

distributed-test:   
     [echo] running distributed tests
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/mallen/Desktop/cassandra-trunk/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.MovementTest
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 446.65 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] SLF4J: Class path contains multiple SLF4J bindings.
    [junit] SLF4J: Found binding in [jar:file:/Users/mallen/Desktop/cassandra-trunk/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    [junit] SLF4J: Found binding in [jar:file:/Users/mallen/Desktop/cassandra-trunk/build/test/lib/jars/whirr-cli-0.3.0-incubating-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    [junit] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
    [junit]  WARN 12:12:46,654 over limit 471283/262144: wrote temp file
    [junit]  WARN 12:12:48,572 over limit 374979/262144: wrote temp file
    [junit]  WARN 12:12:50,701 over limit 892174/262144: wrote temp file
    [junit]  WARN 12:12:54,442 over limit 612358/262144: wrote temp file
    [junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.cassandra.MutationTest
    [junit] Tests run: 4, Failures: 0, Errors: 3, Time elapsed: 110.971 sec
    [junit] 
    [junit] Testcase: testInsert(org.apache.cassandra.MutationTest):    Caused an ERROR
    [junit] null
    [junit] NotFoundException()
    [junit]     at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6900)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:568)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:541)
    [junit]     at org.apache.cassandra.MutationTest.getColumn(MutationTest.java:210)
    [junit]     at org.apache.cassandra.MutationTest.testInsert(MutationTest.java:66)
    [junit] 
    [junit] 
    [junit] Testcase: testWriteAllReadOne(org.apache.cassandra.MutationTest):   Caused an ERROR
    [junit] null
    [junit] NotFoundException()
    [junit]     at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6900)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:568)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:541)
    [junit]     at org.apache.cassandra.MutationTest.getColumn(MutationTest.java:210)
    [junit]     at org.apache.cassandra.MutationTest.testWriteAllReadOne(MutationTest.java:87)
    [junit] 
    [junit] 
    [junit] Testcase: testWriteOneReadAll(org.apache.cassandra.MutationTest):   Caused an ERROR
    [junit] null
    [junit] TimedOutException()
    [junit]     at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15392)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:907)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:879)
    [junit]     at org.apache.cassandra.MutationTest.insert(MutationTest.java:202)
    [junit]     at org.apache.cassandra.MutationTest.testWriteOneReadAll(MutationTest.java:185)
    [junit] 
    [junit] 
    [junit] TEST org.apache.cassandra.MutationTest FAILED
    [junit] Tests FAILED

BUILD FAILED
/Users/mallen/Desktop/cassandra-trunk/build.xml:557: The following error occurred while executing this line:
/Users/mallen/Desktop/cassandra-trunk/build.xml:540: Some distributed test(s) failed.

Total time: 10 minutes 15 seconds
","OS X Version 10.6.6
java version ""1.6.0_22""
Java(TM) SE Runtime Environment (build 1.6.0_22-b04-307-10M3261)
Java HotSpot(TM) 64-Bit Server VM (build 17.1-b03-307, mixed mode)
Apache Ant version 1.8.1 compiled on September 21 2010",,,,,,,,,,CASSANDRA-2005,,,,,CASSANDRA-1986,,,,15/Jan/11 04:49;stuhood;0001-Add-a-builder-for-a-retrying-get-attempt-and-use-to-im.txt;https://issues.apache.org/jira/secure/attachment/12468443/0001-Add-a-builder-for-a-retrying-get-attempt-and-use-to-im.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:56.4,,,no_permission,,,,,,,,,,,,20385,,,Wed Jan 19 20:15:44 UTC 2011,,,,,,0|i0g8hb:,92804,xedin,xedin,,,,,,,,,"11/Jan/11 19:27;stuhood;I think we have a race condition on our hands. Trunk tests just passed for me.

I'll see if we can add more blocking.","14/Jan/11 04:59;stuhood;Attaching a fix for the first failing testcase: when doing a write at ONE and a read at ONE, we retry to ensure that the read eventually succeeds.

The other two failures are more interesting, and are not bugs in the testcase. Kelvin is investigating, but it appears that reads and writes at ALL are not blocking appropriately on the server side: he'll open a separate ticket for this tomorrow.",14/Jan/11 21:29;kelvin;bug found.,14/Jan/11 21:29;kelvin;may also be related.,"15/Jan/11 04:49;stuhood;Ready for review: CASSANDRA-1986 is closed, and we pass all tests consistently.

I was a little bit uncomfortable having to set such high retry timeouts in cases where we need to wait for gossip (in particular when waiting for the ""fail fast"" behaviour from CASSANDRA-1803 to kick in), but it is a bit of a necessary evil with cloud providers. I opened CASSANDRA-1988 to discuss ways to improve this behaviour.","15/Jan/11 05:45;stuhood;Ack... after a rebase, it looks like we're blocked again: this time by CASSANDRA-1989. These tests restart the nodes multiple times.","16/Jan/11 15:10;jbellis;With 1989 committed, is this patch good to go?","16/Jan/11 22:37;stuhood;+1
This is ready.",16/Jan/11 23:13;jbellis;Should have asked earlier -- for 0.7.1 or just trunk?  (I ask b/c CASSANDRA-1989 just affected trunk.),17/Jan/11 00:37;stuhood;Both of 0.7.1 and 0.8 should receive this fix.,"19/Jan/11 11:38;xedin;Tested on both trunk and cassandra-0.7 branches, patch fixes TimedOutException and NotFoundException in both cases, code looks good but maybe move RetryingAction to the BaseTest to make it reusable?","19/Jan/11 19:15;stuhood;> maybe move RetryingAction to the BaseTest to make it reusable
Good idea... I've actually done this in a separate issue: CASSANDRA-2005","19/Jan/11 19:45;xedin;Great, I have nothing to add then! :)",19/Jan/11 19:53;jbellis;committed,"19/Jan/11 20:15;hudson;Integrated in Cassandra-0.7 #178 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/178/])
    fix distributed-test MutationTest
patch by stuhood; reviewed by Pavel Yaskevich for CASSANDRA-1964
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken keyspace strategy_option with zero replicas,CASSANDRA-1924,12494336,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,thorcarpenter,thorcarpenter,12/30/2010 22:59,3/12/2019 14:04,3/13/2019 22:24,3/11/2011 18:18,0.7.4,,,,0,,,,,,"When a keyspace is defined that has strategy options specifying zero replicas should be place in a datacenter (e.g. DCX:0), an assert is violated for any insert and LOCAL_QUORUM reads fail.  I'm not sure if the issue is that there are no nodes in DCX or that I'm saying DCX shouldn't get any replicas, or a combination of the two.

The broken keyspace:

create keyspace KeyspaceDC1 with
  replication_factor = 1 and
  placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and
  strategy_options = [{DC1:1, DC2:0}];

The fixed keyspace:

create keyspace KeyspaceDC1 with
  replication_factor = 1 and
  placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and
  strategy_options = [{DC1:1}];


To reproduce:

* Install the 0.7rc3 rpm on a single node in ""DC1"".
* In cassandra.yaml set initial_token = 1 and specify PropertyFileSnitch.
* cassandra-topology.properties:

10.5.64.26=DC1:R1
default=DC2:R1

* Schema loaded via cassandra-cli:

create keyspace KeyspaceDC1 with
  replication_factor = 1 and
  placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and
  strategy_options = [{DC1:1, DC2:0}];

use KeyspaceDC1;

create column family TestCF with
  column_type = 'Standard' and
  comparator = 'BytesType' and
  keys_cached = 200000 and
  rows_cached = 2000 and
  gc_grace = 0 and
  read_repair_chance = 0.0;

* In cassandra-cli execute the following:

[default@unknown] use KeyspaceDC1;
Authenticated to keyspace: KeyspaceDC1
[default@KeyspaceDC1] set TestCF['some key']['some col'] = 'some value';
Internal error processing insert

* If you have asserts enabled, check system.log where you should find the assertion error: 

DEBUG [pool-1-thread-3] 2010-12-29 12:10:38,897 CassandraServer.java (line 362) insert
ERROR [pool-1-thread-3] 2010-12-29 12:10:38,906 Cassandra.java (line 2960) Internal error processing insert
java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.firstTokenIndex(TokenMetadata.java:392) 
        at org.apache.cassandra.locator.TokenMetadata.ringIterator(TokenMetadata.java:417)
        at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:95)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:99)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1411)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1394)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:109)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:442)
        at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:379)
        at org.apache.cassandra.thrift.Cassandra$Processor$insert.process(Cassandra.java:2952)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
        at java.lang.Thread.run(Thread.java:619)

* If you don't have asserts enabled, you should find that no errors are logged but LOCAL_QUORUM reads cause TimedOutExceptions on the client.
",,,7200,7200,,0%,7200,7200,,,,,,,,,,,,11/Mar/11 14:54;slebresne;0001-Ring-iterator-empty-list-test.patch;https://issues.apache.org/jira/secure/attachment/12473396/0001-Ring-iterator-empty-list-test.patch,11/Mar/11 13:43;slebresne;0002-NetworkTopologyStrategy-test-and-small-optim.patch;https://issues.apache.org/jira/secure/attachment/12473392/0002-NetworkTopologyStrategy-test-and-small-optim.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,11:09.7,,,no_permission,,,,,,,,,,,,20372,,,Fri Mar 11 23:07:03 UTC 2011,,,,,,0|i0g88v:,92766,jbellis,jbellis,,,,,,,,,"10/Mar/11 09:11;tommysdk;So the AssertionError occurs in TokenMetadata.firstTokenIndex where the token ring is expected to have a size greater than 0. A discovery I made is that the TokenMetadata.ringIterator method which makes the call to firstTokenIndex, also expects the ring to be non-empty if the includeMin parameter is true:
final boolean insertMin = (includeMin && !ring.get(0).equals(StorageService.getPartitioner().getMinimumToken())) ? true : false;

If the includeMin parameter is true and the ring is empty, it would raise a java.lang.IndexOutOfBoundsException: Index: 0, Size: 0. However, the method only seems to be called with includeMin == true from StorageProxy.getRestrictedRanges. As in the case of this issue, it is called from NetworkTopologyStrategy with a includeMin == false, thus failing at the assertion in TokenMetadata.firstTokenIndex since there are no TokenMetadata tokens present.

","11/Mar/11 13:43;slebresne;Indeed ringIterator() shouldn't fail when the argument token list is empty.

Attaching patch to fix this with a unit test.

The second patch attached adds a more high level unit test that tests the problem at the level of NetworkTopologyStrategy (while the first patch has a ringIterator test). It also include a tiny optimisation for NetworkTopologyStrategy that skip unnecessary steps when the number of replicas in the DC is 0. This optimisation would hide the actual problem so that's why it's attached separately.","11/Mar/11 14:18;jbellis;would it be simpler to say something like ""if ring.isEmpty() return Iterators.emptyIterator()""?","11/Mar/11 14:54;slebresne;You are right, it's a bit cleaner. Attaching new version of first patch using Iterators functions.","11/Mar/11 18:18;jbellis;committed w/o the NTS optimization.

(open to being convinced otherwise but my reasoning is that performance is not critical--since the result of calculateNE is cached by ARS.getNE--so we should not add special cases there that can hide bugs.)","11/Mar/11 23:07;hudson;Integrated in Cassandra-0.7 #375 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/375/])
    allow zero replicas in a NTSdatacenter
patch by slebresne; reviewed by jbellis for CASSANDRA-1924
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0.7 migrations/schema serializations are incompatible with trunk,CASSANDRA-2001,12495898,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,1/18/2011 14:03,3/12/2019 14:04,3/13/2019 22:24,1/18/2011 18:25,0.8 beta 1,,,,0,,,,,,"Two problems:
1. inserting replicate_on_write into the middle of the CfDef members created a problem with serialization.  
2. merging the genavro files created a strange namespacing problem.",,,,,,,,,,,,,,,,,,,,18/Jan/11 21:11;stuhood;0004-Set-a-default-for-rep-on-write-and-revert-0001.txt;https://issues.apache.org/jira/secure/attachment/12468682/0004-Set-a-default-for-rep-on-write-and-revert-0001.txt,18/Jan/11 17:19;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-use-the-writer-schema-and-not-the-current-schema-when-.txt;https://issues.apache.org/jira/secure/attachment/12468653/ASF.LICENSE.NOT.GRANTED--v1-0001-use-the-writer-schema-and-not-the-current-schema-when-.txt,18/Jan/11 17:19;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0002-get-rid-of-the-avro-db.migrations-classes-that-were-du.txt;https://issues.apache.org/jira/secure/attachment/12468654/ASF.LICENSE.NOT.GRANTED--v1-0002-get-rid-of-the-avro-db.migrations-classes-that-were-du.txt,18/Jan/11 17:19;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0003-fix-order-of-replicate_on_write-and-make-sure-it-s-not.txt;https://issues.apache.org/jira/secure/attachment/12468655/ASF.LICENSE.NOT.GRANTED--v1-0003-fix-order-of-replicate_on_write-and-make-sure-it-s-not.txt,18/Jan/11 20:09;gdusbabek;data.zip;https://issues.apache.org/jira/secure/attachment/12468676/data.zip,,,,,,,,,5,,,,,,,,,,,,,,,,,,,54:13.9,,,no_permission,,,,,,,,,,,,20397,,,Tue Jan 18 23:16:43 UTC 2011,,,,,,0|i0g8pj:,92841,eevans,eevans,,,,,,,,,18/Jan/11 14:03;gdusbabek;fwiw CASSANDRA-1923 contains unit tests that will catch these kinds of problems in the future.,18/Jan/11 17:20;gdusbabek;also fixes the incorrect schema that was used when deserializing an object.,"18/Jan/11 17:54;urandom;Just for posterity sake: CASSANDRA-926 moved  the remaining Avro records from o.a.c.avro elsewhere, to packages that made it obvious which components were still using them (but obviously, created a bug in the process).

+1 on this patchset though (with or without the o.a.c.db.migration.avro -> o.a.c.db.avro move).","18/Jan/11 18:24;hudson;Integrated in Cassandra #676 (See [https://hudson.apache.org/hudson/job/Cassandra/676/])
    fix order of replicate_on_write and make sure it's not null. patch by gdusbabek, reviewed by eevans. CASSANDRA-2001
get rid of the avro db.migrations classes that were duplicated elsewhere. patch by gdusbabek, reviewed by eevans. CASSANDRA-2001
use the writer schema and not the current schema when deserializing. patch by gdusbabek, reviewed by eevans. CASSANDRA-2001
",18/Jan/11 18:25;gdusbabek;committed,18/Jan/11 20:09;gdusbabek;schema from a vanilla 0.7 node.,"18/Jan/11 20:27;gdusbabek;If you bypass the ClassCastException that 926 introduced, you end up with an avro error:
org.apache.avro.AvroTypeException: Found {""type"":""record"",""name"":""CfDef"",""namespace"":""org.apache.cassandra.avro"",""fields"":[{""name"":""keyspace"",""type"":""string""},{""name"":""name"",""type"":""string""},{""name"":""column_type"",""type"":[""string"",""null""]},{""name"":""comparator_type"",""type"":[""string"",""null""]},{""name"":""subcomparator_type"",""type"":[""string"",""null""]},{""name"":""comment"",""type"":[""string"",""null""]},{""name"":""row_cache_size"",""type"":[""double"",""null""]},{""name"":""key_cache_size"",""type"":[""double"",""null""]},{""name"":""read_repair_chance"",""type"":[""double"",""null""]},{""name"":""gc_grace_seconds"",""type"":[""int"",""null""]},{""name"":""default_validation_class"",""type"":[""null"",""string""],""default"":null},{""name"":""min_compaction_threshold"",""type"":[""null"",""int""],""default"":null},{""name"":""max_compaction_threshold"",""type"":[""null"",""int""],""default"":null},{""name"":""row_cache_save_period_in_seconds"",""type"":[""int"",""null""],""default"":0},{""name"":""key_cache_save_period_in_seconds"",""type"":[""int"",""null""],""default"":3600},{""name"":""memtable_flush_after_mins"",""type"":[""int"",""null""],""default"":60},{""name"":""memtable_throughput_in_mb"",""type"":[""null"",""int""],""default"":null},{""name"":""memtable_operations_in_millions"",""type"":[""null"",""double""],""default"":null},{""name"":""id"",""type"":[""int"",""null""]},{""name"":""column_metadata"",""type"":[{""type"":""array"",""items"":{""type"":""record"",""name"":""ColumnDef"",""fields"":[{""name"":""name"",""type"":""bytes""},{""name"":""validation_class"",""type"":""string""},{""name"":""index_type"",""type"":[{""type"":""enum"",""name"":""IndexType"",""symbols"":[""KEYS""],""aliases"":[""org.apache.cassandra.config.avro.IndexType""]},""null""]},{""name"":""index_name"",""type"":[""string"",""null""]}]}},""null""]}]}, expecting {""type"":""record"",""name"":""CfDef"",""namespace"":""org.apache.cassandra.avro"",""fields"":[{""name"":""keyspace"",""type"":""string""},{""name"":""name"",""type"":""string""},{""name"":""column_type"",""type"":[""string"",""null""]},{""name"":""comparator_type"",""type"":[""string"",""null""]},{""name"":""subcomparator_type"",""type"":[""string"",""null""]},{""name"":""comment"",""type"":[""string"",""null""]},{""name"":""row_cache_size"",""type"":[""double"",""null""]},{""name"":""key_cache_size"",""type"":[""double"",""null""]},{""name"":""read_repair_chance"",""type"":[""double"",""null""]},{""name"":""gc_grace_seconds"",""type"":[""int"",""null""]},{""name"":""default_validation_class"",""type"":[""null"",""string""],""default"":null},{""name"":""min_compaction_threshold"",""type"":[""null"",""int""],""default"":null},{""name"":""max_compaction_threshold"",""type"":[""null"",""int""],""default"":null},{""name"":""row_cache_save_period_in_seconds"",""type"":[""int"",""null""],""default"":0},{""name"":""key_cache_save_period_in_seconds"",""type"":[""int"",""null""],""default"":3600},{""name"":""memtable_flush_after_mins"",""type"":[""int"",""null""],""default"":60},{""name"":""memtable_throughput_in_mb"",""type"":[""null"",""int""],""default"":null},{""name"":""memtable_operations_in_millions"",""type"":[""null"",""double""],""default"":null},{""name"":""id"",""type"":[""int"",""null""]},{""name"":""column_metadata"",""type"":[{""type"":""array"",""items"":{""type"":""record"",""name"":""ColumnDef"",""fields"":[{""name"":""name"",""type"":""bytes""},{""name"":""validation_class"",""type"":""string""},{""name"":""index_type"",""type"":[{""type"":""enum"",""name"":""IndexType"",""symbols"":[""KEYS""],""aliases"":[""org.apache.cassandra.config.avro.IndexType""]},""null""]},{""name"":""index_name"",""type"":[""string"",""null""]}],""aliases"":[""org.apache.cassandra.config.avro.ColumnDef""]}},""null""]},{""name"":""replicate_on_write"",""type"":[""boolean"",""null""]}],""aliases"":[""org.apache.cassandra.config.avro.CfDef""]}

basically, avro was expecting to see replicate_on_write.","18/Jan/11 21:11;stuhood;Attaching the patch discussed in IRC.

In summary: it is necessary to add a default with every new field, independent of whether you are using a union to make the field optional. Defaults are used in places where the writer's schema doesn't contain a field, but the reader's schema does. Unions are used to make fields optional: you can use a union together with a default to add a new optional field.",18/Jan/11 21:28;gdusbabek;backed out of the original fix and committed the one-liner that sets a default on CfDef.replicate_on_write.,"18/Jan/11 23:16;hudson;Integrated in Cassandra #677 (See [https://hudson.apache.org/hudson/job/Cassandra/677/])
    set a default for replicate_on_write. patch by stuhood and gdusbabek. CASSANDRA-2001
back out of 2001. patch by gdusbabek. CASSANDRA-2001
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when read repair is disabled,CASSANDRA-2010,12496038,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,1/19/2011 18:54,3/12/2019 14:04,3/13/2019 22:24,1/20/2011 2:34,0.7.1,,,,0,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,19/Jan/11 18:56;jbellis;2010.txt;https://issues.apache.org/jira/secure/attachment/12468775/2010.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,15:01.9,,,no_permission,,,,,,,,,,,,19350,,,Thu Jan 20 02:59:27 UTC 2011,,,,,,0|i0g8rj:,92850,tjake,tjake,,,,,,,,,19/Jan/11 19:01;jbellis;the problem is that messages and endpoints in the sendRR need to be the same length.  fix attached.,19/Jan/11 23:15;tjake;+1,20/Jan/11 02:34;jbellis;committed,"20/Jan/11 02:59;hudson;Integrated in Cassandra-0.7 #181 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/181/])
    fix messages/endpoints mismatch when RR is disabled
patch by jbellis; reviewed by tjake for CASSANDRA-2010
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli doesn't accept 'name' as a column name in column metadata when creating a column family,CASSANDRA-1995,12495723,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,thobbs,thobbs,1/16/2011 20:03,3/12/2019 14:04,3/13/2019 22:24,1/17/2011 16:00,0.7.1,,Legacy/Tools,,0,,,,,,"This fails:

create column family Countries with comparator=UTF8Type and column_metadata=[ {column_name: name, validation_class: UTF8Type} ];

This works:

create column family Countries with comparator=UTF8Type and column_metadata=[ {column_name: fooname, validation_class: UTF8Type} ];",,;17/Jan/11 13:34;xedin;10800,10800,0,10800,100%,10800,0,10800,,,,,,,,,,,17/Jan/11 13:34;xedin;CASSANDRA-1995.patch;https://issues.apache.org/jira/secure/attachment/12468551/CASSANDRA-1995.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,34:30.3,,,no_permission,,,,,,,,,,,,20394,,,Mon Jan 17 16:21:26 UTC 2011,,,,,,0|i0g8o7:,92835,jbellis,jbellis,,,,,,,,,"17/Jan/11 13:34;xedin;Work branch: cassandra-0.7, latest commit 2a3a497fd8b12160faf81edce1d7e2cbac953b95 (r/m unused code and improve formatting)

Also: tests added for this case and for help statements.",17/Jan/11 16:00;jbellis;committed,"17/Jan/11 16:21;hudson;Integrated in Cassandra-0.7 #167 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/167/])
    fix CLI parsing of ""cluster name.""
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-1995
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Word count example doesn't output the words correctly to cassandra.  It outputs spurious data past the length of the byte array.,CASSANDRA-1993,12495691,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jesseshieh,jesseshieh,jesseshieh,1/16/2011 1:38,3/12/2019 14:04,3/13/2019 22:24,1/16/2011 22:55,0.7.1,,,,0,,,,,,"To reproduce:
# start a local cassandra server e.g. sudo bin/cassandra -f
cd contrib/word_count
ant
bin/word_count_setup
bin/word_count

# check the data in cassandra, all looks fine because the words are all of the same length.
# change the data in cassandra to real words, rerun the mapreduce and you'll see some words have spurious characters written past their length
# this is because the word bytes are not terminated at their length",All,,3600,3600,,0%,3600,3600,,,,,,,,,,,,16/Jan/11 04:03;jbellis;1993-v2.txt;https://issues.apache.org/jira/secure/attachment/12468473/1993-v2.txt,16/Jan/11 01:43;jesseshieh;trunk-1993.txt;https://issues.apache.org/jira/secure/attachment/12468470/trunk-1993.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,03:15.5,,,no_permission,,,,,,,,,,,,20392,,,Sun Jan 16 23:14:51 UTC 2011,,,,,,0|i0g8nr:,92833,jbellis,jbellis,,,,,,,,,"16/Jan/11 04:03;jbellis;Thanks for the report!

v2 should provide a similar fix while avoiding an unnecessary intermediate copy.","16/Jan/11 04:59;jesseshieh;nice improvement =)

you might also be interested to know that the latest version of hadoop adds a method copyBytes to the Text object that should be able to replace getBytes and take care of this automatically for us.

see: http://svn.apache.org/viewvc/hadoop/common/trunk/src/java/org/apache/hadoop/io/Text.java?r1=953881&r2=1050070",16/Jan/11 22:55;jbellis;committed v2,"16/Jan/11 23:14;hudson;Integrated in Cassandra-0.7 #164 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/164/])
    fix copy bounds for word Text in wordcount demo
patch by Jesse Shieh and jbellis for CASSANDRA-1993
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update token metadata for NORMAL state,CASSANDRA-1934,12494667,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,nickmbailey,nickmbailey,1/5/2011 2:06,3/12/2019 14:04,3/13/2019 22:24,1/20/2011 21:37,0.7.1,,,,0,,,,,,"The handleStateNormal() method in StorageService.java doesn't update the tokenmetadata. This means if you try to decommission a node but for some reason it fails, and then you bring the node back up, all other nodes will see it in a 'Leaving' state. When the state jumps back to normal they should update the token metadata to reflect that.

This also means you won't be able to call 'removetoken' on that node, unless you restart another node in the cluster in order to put it back in a 'normal' state.",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,10/Jan/11 18:10;brandon.williams;1934.txt;https://issues.apache.org/jira/secure/attachment/12467903/1934.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,10:40.0,,,no_permission,,,,,,,,,,,,20376,,,Thu Jan 20 21:53:26 UTC 2011,,,,,,0|i0g8b3:,92776,nickmbailey,nickmbailey,,,,,,,,,10/Jan/11 18:10;brandon.williams;Patch to allow recovery to normal if we knew about the endpoint previously and its IP has not changed.,"20/Jan/11 20:12;jbellis;How does updating when endpoint.equals(currentNode) solve the problem?

It looks to me like the relevant path is

{code}
            logger_.info(String.format(""Nodes %s and %s have the same token %s.  %s is the new owner"",
                                       endpoint, currentNode, token, endpoint));
            tokenMetadata_.updateNormalToken(token, endpoint);
{code}

which is already doing The Right Thing.","20/Jan/11 20:16;brandon.williams;If you are comparing endpoint to currentNode and they are the same, the generation difference can never be > 0","20/Jan/11 20:22;jbellis;my point is that currentNode should always have the right view of itself anyway, it's the other nodes we are worried about (according to the issue description anyway). ","20/Jan/11 20:35;brandon.williams;currentNode (which failed decom and then restarted) does have the correct view of itself (normal) but
{code}
        else if (endpoint.equals(currentNode))
        {
            // nothing to do
        }
{code}

Prevents the other nodes from updating the state from 'Leaving' back to 'normal'.

We almost update the state here:
{code}
        else if (Gossiper.instance.compareEndpointStartup(endpoint, currentNode) > 0)
        {
            logger_.info(String.format(""Nodes %s and %s have the same token %s.  %s is the new owner"",
                                       endpoint, currentNode, token, endpoint));
            tokenMetadata_.updateNormalToken(token, endpoint);
            if (!isClientMode)
                SystemTable.updateToken(endpoint, token);
        }
{code}
But don't because the generations will always be equal (in other words, this code only handles a *different* node updating the state, not the same node returning)
",20/Jan/11 21:24;jbellis;I still don't get it -- endpoint.equals(currentNode) will never be true on the other nodes.,"20/Jan/11 21:29;jbellis;bq. I still don't get it - endpoint.equals(currentNode) will never be true on the other nodes. 

I was thinking that currentNode == localAddress but that is not the case.  It makes more sense now! :)",20/Jan/11 21:30;jbellis;+1,"20/Jan/11 21:30;brandon.williams;Yes it will.  endpoint is the node entering the ring, currentNode is the node that currently has the token passed in (which in this case will be endpoint's token also), but does not mean 'my address' to the node making the evaluation.  Thus, if we see the same endpoint and token, we do nothing in the current code, causing the state to never get updated.  ",20/Jan/11 21:32;jbellis;(renamed currentNode to currentOwner so I can remember the difference in the future.),20/Jan/11 21:37;brandon.williams;committed.,"20/Jan/11 21:53;hudson;Integrated in Cassandra-0.7 #186 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/186/])
    Update token metadata for NORMAL state when endpoint has not changed.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1934
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition during decommission,CASSANDRA-2072,12497003,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,1/28/2011 0:06,3/12/2019 14:04,3/13/2019 22:24,2/3/2011 20:17,0.7.1,,,,0,,,,,,"Occasionally when decommissioning a node, there is a race condition that occurs where another node will never remove the token and thus propagate it again with a state of down.  With CASSANDRA-1900 we can solve this, but it shouldn't occur in the first place.

Given nodes A, B, and C, if you decommission B it will stream to A and C.  When complete, B will decommission and receive this stacktrace:

ERROR 00:02:40,282 Fatal exception in thread Thread[Thread-5,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:62)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:387)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:91

At this point A will show it is removing B's token, but C will not and instead its failure detector will report that B is dead, and nodetool ring on C shows B in a leaving/down state.  In another gossip round, C will propagate this state back to A.",,,,,,,,,,,,,,,,,,,,28/Jan/11 20:28;brandon.williams;0001-announce-having-left-the-ring-for-RING_DELAY-on-deco.patch;https://issues.apache.org/jira/secure/attachment/12469696/0001-announce-having-left-the-ring-for-RING_DELAY-on-deco.patch,28/Jan/11 20:28;brandon.williams;0002-Improve-TRACE-logging-for-Gossiper.patch;https://issues.apache.org/jira/secure/attachment/12469697/0002-Improve-TRACE-logging-for-Gossiper.patch,31/Jan/11 19:44;brandon.williams;0003-Remove-endpoint-state-when-expiring-justRemovedEndpo.patch;https://issues.apache.org/jira/secure/attachment/12469851/0003-Remove-endpoint-state-when-expiring-justRemovedEndpo.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,15:17.5,,,no_permission,,,,,,,,,,,,20427,,,Thu Feb 03 20:47:49 UTC 2011,,,,,,0|i0g95j:,92913,gdusbabek,gdusbabek,,,,,,,,,"28/Jan/11 20:28;brandon.williams;Here is what is happening:

B sends LEFT to C, C calls removeEndpoint and drops the endpoint state.  B never gets to send to A (because it only waits 2s to announce, which can be just one round) and A still thinks it's LEAVING.  C sees B in a gossip digest from A, and  not knowing anything about it, calls requestAll, but A refuses to tell C anything about it because A has B in justRemovedEndpoints.  Eventually, QUARANTINE_DELAY expires and A unhelpfully propagates the LEAVING state back to C.

The obvious solution here is that B should announce LEFT for RING_DELAY, simply because it's the right thing to do as opposed to a one-off delay of 2 seconds.

However, this exposes a more subtle problem.  When removeEndpoint is called, we drop the state right away and track the endpoint in justRemovedEndpoints.  Instead, we should hold on to the state so it is still propagated in further gossip digests, and expire it when we expire justRemovedEndpoints.

Either of these changes is technically enough to solve this issue, but both together add an extra safeguard.  Changing where we expire the endpoint state is the more impacting of the two, however the gossip generation and version checks always prevent any negative consequences from doing this.",03/Feb/11 18:15;gdusbabek;+1,03/Feb/11 20:17;brandon.williams;Committed.,"03/Feb/11 20:47;hudson;Integrated in Cassandra-0.7 #244 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/244/])
    Fix race condition during decommission by announcing for RING_DELAY and
not removing endpoint state until removing the ep from
justRemovedEndpoints.
Patch by brandonwilliams, reviewed by gdusbabek for CASSANDRA-2072
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming occasionally makes gossip back up,CASSANDRA-2073,12497092,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,brandon.williams,brandon.williams,1/28/2011 20:47,3/12/2019 14:04,3/13/2019 22:24,1/31/2011 19:04,0.7.1,,,,0,,,,,,"Streaming occasionally makes gossip back up, causing nodes to mark each other as down even though the network is ok.  This appears to happen just after streaming has finished.  I noticed this in the course of working on CASSANDRA-2072, so decommission is one way to reproduce.  It seems to happen maybe one of fifteen or twenty tries, so it's fairly rare.",,,7200,7200,,0%,7200,7200,,,,,,,,,,,,29/Jan/11 00:55;jbellis;2073.txt;https://issues.apache.org/jira/secure/attachment/12469720/2073.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:47.1,,,no_permission,,,,,,,,,,,,20428,,,Mon Jan 31 19:04:16 UTC 2011,,,,,,0|i0g95r:,92914,gdusbabek,gdusbabek,,,,,,,,,28/Jan/11 21:21;gdusbabek;I see this regularly during repair operations.,"28/Jan/11 21:24;brandon.williams;I'm fairly certain this is the true cause of CASSANDRA-1730 and other ""gossip took longer than RING_DELAY"" bugs we've had.","28/Jan/11 21:28;jbellis;bq. Streaming occasionally makes gossip back up

On the StreamOut [source] side, the StreamIn [receiver] side, or both?","28/Jan/11 21:40;brandon.williams;Appears to be the receiver.  I just repro'd it with decom.  Nodes A, B, and C.  Decom B, streams to A and C complete, and afterwards A and C cannot gossip to each other for approximately 40s or so.  B did get the usual exception:

{noformat}
ERROR [Thread-6] 2011-01-28 21:23:08,720 AbstractCassandraDaemon.java (line 119) Fatal exception in thread Thread[Thread-6,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:62)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:387)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:91)
{noformat}

(I had CASSANDRA-2072 applied to avoid other problems) and I don't see any message about streaming completing on it, though A and C show StreamInSessions finishing right before the gossip outage.",28/Jan/11 21:41;brandon.williams;tpstats on both nodes showed one active gossip task and a bunch pending.,"28/Jan/11 21:51;jbellis;i wonder if we have a Thread.sleep(RING_DELAY) in code that gets called on the gossip path.

jstack during the pause?","28/Jan/11 22:09;brandon.williams;It looks like in my case, both nodes are stuck in the same spot:

{noformat}

""GossipStage:1"" prio=10 tid=0x0000000041644000 nid=0xece waiting on condition [0x00007fdbe861f000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00007fdc5ecf8828> (a java.util.concurrent.FutureTask$Sync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:218)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.db.HintedHandOffManager.deleteHintsForEndPoint(HintedHandOffManager.java:156)
    at org.apache.cassandra.service.StorageService.excise(StorageService.java:831)
    at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:787)
    at org.apache.cassandra.service.StorageService.onChange(StorageService.java:645)
    at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:742)
    at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:732)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:649)
    at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:68)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:70)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}

They are waiting on deleting hints, and then likely for the hints CF to compact.","28/Jan/11 22:15;jbellis;Bingo.  I don't see any reason why we'd want that to be blocking, do you?

If not we can just remove the .get().","28/Jan/11 22:16;jbellis;... however, we DO want the flush before compaction to be blocking.

possibly kicking the whole operation out to the StorageService task queue is the best move.

(in which case the .get() is a no-op and can still be removed.)","28/Jan/11 23:56;brandon.williams;I don't see any reason why it should block, no.  Also I can't repro with repair on a real cluster, so I think this is the only bug.",29/Jan/11 00:55;jbellis;attached,31/Jan/11 15:26;gdusbabek;+1,31/Jan/11 19:04;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LazilyCompactedRow doesn't add CFInfo to digest,CASSANDRA-2039,12496518,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,richardlow,richardlow,richardlow,1/24/2011 10:59,3/12/2019 14:04,3/13/2019 22:24,4/8/2011 21:39,0.8 beta 1,,,,0,,,,,,"LazilyCompactedRow.update doesn't add the CFInfo or columnCount to the digest, so the hash value in the Merkle tree does not include this data.  However, PrecompactedRow does include this.  Two consequences of this are:
* Row-level tombstones are not compared when using LazilyCompactedRow so could remain inconsistent
* LazilyCompactedRow and PrecompactedRow produce different hashes of the same row, so if two nodes have differing in_memory_compaction_limit_in_mb values, rows of size in between the two limits will have different hashes so will always be repaired even when they are the same.",,,,,,,,,,,,,,,,,,,,26/Jan/11 12:29;richardlow;trunk-2038-LazilyCompactedRowTest.txt;https://issues.apache.org/jira/secure/attachment/12469416/trunk-2038-LazilyCompactedRowTest.txt,26/Jan/11 18:24;richardlow;trunk-2038-v2.txt;https://issues.apache.org/jira/secure/attachment/12469454/trunk-2038-v2.txt,24/Jan/11 11:03;richardlow;trunk-2038.txt;https://issues.apache.org/jira/secure/attachment/12469140/trunk-2038.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,05:34.7,,,no_permission,,,,,,,,,,,,20410,,,Sat Feb 05 21:37:53 UTC 2011,,,,,,0|i0g8y7:,92880,jbellis,jbellis,,,,,,,,,"25/Jan/11 20:05;jbellis;this looks okay eyeballing it, but can you add a check to LazilyCompactedRowTest similar to assertBytes to make sure this stays fixed?","26/Jan/11 12:37;richardlow;I added assertDigest (see the patch trunk-2038-LazilyCompactedRowTest.txt), but it currently fails in testManyRows.  The reason is that the Bloom filters have different sizes, because getEstimatedColumnCount returns a value too large in LazilyCompactedRow.  There is probably no way round this without doing another pass on the data.

However, it isn't necessary to add the Bloom filter or indeed the index to the digest - they are determined by the data that comes later.  So could the header be excluded from the digest?",26/Jan/11 14:41;jbellis;That sounds good.  Let's do that in trunk to avoid breaking compatibility in 0.7.,"26/Jan/11 18:39;richardlow;The v2 patch does this (and supersedes the other two patches).  Now the updated LazilyCompactedRowTest passes, as do the other tests.

It now might be possible to compute the digest for LazilyCompactedRow in just one pass through the data - the first pass is now just used to calculate columnCount.  columnCount could be excluded from the digest (I added it just because it was there in PrecompactedRow).  However, it's used by isEmpty - what would be the impact of using estimatedColumnCount here instead?  Is estimatedColumnCount zero if and only if columnCount is zero?  Or does it matter is isEmpty sometimes returns false when it's true?","05/Feb/11 20:38;jbellis;Committed v2.  Thanks!

bq. what would be the impact of using estimatedColumnCount here instead

It would break the part of CompactionIterator that leaves out rows with no columns from the new SSTable.  ""estimated"" is the maximum possible number of columns in the new row, so it's ok to use it in the bloom filter, but not in the ""is this row empty post-merge"" check.","05/Feb/11 21:37;hudson;Integrated in Cassandra #710 (See [https://hudson.apache.org/hudson/job/Cassandra/710/])
    make PreCompactedRow and LazyCompactedRow digest computations match
patch by Richard Low and jbellis for CASSANDRA-2039
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI chokes on whitespace after semicolon when using -f,CASSANDRA-2031,12496345,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,mdennis,mdennis,1/21/2011 22:06,3/12/2019 14:04,3/13/2019 22:24,1/24/2011 5:39,0.7.1,,Legacy/Tools,,0,,,,,,"The CLI chokes on whitespace after the semicolon when a file is passed with -f

""... missing EOF at""",,;22/Jan/11 14:43;xedin;600,600,0,600,100%,600,0,600,,,,,,,,,,,22/Jan/11 14:43;xedin;CASSANDRA-2031.patch;https://issues.apache.org/jira/secure/attachment/12469045/CASSANDRA-2031.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,39:04.6,,,no_permission,,,,,,,,,,,,20406,,,Mon Jan 24 05:53:50 UTC 2011,,,,,,0|i0g8wf:,92872,mdennis,mdennis,,,,,,,,,24/Jan/11 02:37;mdennis;+1,24/Jan/11 05:39;jbellis;committed,"24/Jan/11 05:53;hudson;Integrated in Cassandra-0.7 #192 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/192/])
    trim cli input to avoid confusing parser
patch by Pavel Yaskevich; reviewed by mdennis for CASSANDRA-2031
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misuses of ByteBuffer absolute get (wrongfully adding arrayOffset to the index),CASSANDRA-1939,12494734,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,1/5/2011 16:21,3/12/2019 14:04,3/13/2019 22:24,1/7/2011 8:34,0.7.0,,,,0,,,,,,ByteBuffer.arrayOffset() should not be added to the argument of an absolute get. ,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,05/Jan/11 16:22;slebresne;0001-Remove-addition-of-arrayOffset-in-ByteBuffer-absolut.patch;https://issues.apache.org/jira/secure/attachment/12467553/0001-Remove-addition-of-arrayOffset-in-ByteBuffer-absolut.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,06:29.9,,,no_permission,,,,,,,,,,,,20377,,,Thu Jan 06 01:06:29 UTC 2011,,,,,,0|i0g8c7:,92781,jbellis,jbellis,,,,,,,,,06/Jan/11 01:06;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restart required to change cache_save_period,CASSANDRA-2100,12497510,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,nickmbailey,nickmbailey,2/2/2011 21:46,3/12/2019 14:04,3/13/2019 22:24,2/25/2011 20:31,0.7.3,,,,0,,,,,,"The cache_save_period is set in the schema for each column family.  However this value is only checked when a node starts up so changing this value isn't really dynamic.

We should actually change this when the schema changes instead of having to restart.",,,7200,7200,,0%,7200,7200,,,,,,,,,,,,21/Feb/11 23:22;jhermes;2100-2.txt;https://issues.apache.org/jira/secure/attachment/12471582/2100-2.txt,24/Feb/11 00:31;jhermes;2100-3.txt;https://issues.apache.org/jira/secure/attachment/12471792/2100-3.txt,10/Feb/11 20:28;jhermes;2100.txt;https://issues.apache.org/jira/secure/attachment/12470802/2100.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,28:41.5,,,no_permission,,,,,,,,,,,,20442,,,Fri Feb 25 21:02:43 UTC 2011,,,,,,0|i0g9bb:,92939,jbellis,jbellis,,,,,,,,,"10/Feb/11 20:28;jhermes;row/key cache save period in seconds now config-able at runtime via JMX (not just migrations).

On either migration or JMX-change path, the scheduling has been broken out. It first cancels the previous scheduled task (but will not interrupt currently saving caches) then schedules a new task with the correct frequency.",21/Feb/11 20:22;jbellis;can you rebase?,21/Feb/11 23:22;jhermes;Rebased.,23/Feb/11 04:50;jbellis;how does this handle updating when schema changes?,"23/Feb/11 23:57;jhermes;I thought that UpdateColumnFamily.applyModels() called CFS.reload() in all cases, but that only happens when the server is not in client mode.

CFMetaData.apply(avro CfDef) will need to call the update logic. Patch forthcoming.","24/Feb/11 00:31;jhermes;Slight surprise in testing due to 2172 changes, but tests correctly (save period is updated at runtime for both JMX and schema migrations (both client and non-client mode)).","25/Feb/11 19:00;jbellis;I'm not a fan of having side effects in CFMetaData like that.

I checked w/ Gary and his take was, ""The saving probably belongs in UpdateColumnFamily.  CFMetaData.apply() should only affect a single object—-nothing persistent.""","25/Feb/11 19:29;jhermes;2100-2.txt works fine when the schema changes.

`if (!clientMode) { CFS.reload() }` will reset the config appropriately, and it's good we don't do anything when clientMode because we don't apply() from migration in that case as well (and there would be nothing to reset).

From your comment I thought that something didn't work. Don't scare me like that. :P",25/Feb/11 20:31;jbellis;committed,"25/Feb/11 21:02;hudson;Integrated in Cassandra-0.7 #324 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/324/])
    add [get|set][row|key]cacheSavePeriod to JMX
patch by Jon Hermes; reviewed by jbellis for CASSANDRA-2100
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saved row cache continues to be read past max cache size,CASSANDRA-2082,12497246,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,1/31/2011 19:46,3/12/2019 14:04,3/13/2019 22:24,7/26/2011 18:58,1.0.0,,,,0,,,,,,"Scenario:
 - Node has a saved row cache of size n
 - node OOMs
 - Make row cache size = .5n to prevent OOM while we debug, restart node.
 - n items are still read from the row cache, making startup take twice as long as needed.


(This is intended as a straightforward bug, not as a hackish CASSANDRA-1966.)",,,,,,,,,,,,,,,,,,,,22/Jul/11 19:00;cburroughs;0001-v1-Only-load-up-to-capacity-items-into-the-cache.patch;https://issues.apache.org/jira/secure/attachment/12487465/0001-v1-Only-load-up-to-capacity-items-into-the-cache.patch,22/Jul/11 19:00;cburroughs;0002-v1-unit-test-for-CASSANDRA-2082.patch;https://issues.apache.org/jira/secure/attachment/12487466/0002-v1-unit-test-for-CASSANDRA-2082.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,58:40.9,,,no_permission,,,,,,,,,,,,20432,,,Tue Jul 26 19:19:38 UTC 2011,,,,,,0|i0g97b:,92921,jbellis,jbellis,,,,,,,,,22/Jul/11 19:00;cburroughs;Note that the second patch applies on top of CASSANDRA-1966.,"26/Jul/11 18:58;jbellis;Committed since the code involved is small, but I note that the applicability is limited because you can't change the cache setting via schema or JMX until the node has started successfully.  So it's only useful if you notice memory pressure and change the setting before the node OOMs.
","26/Jul/11 19:19;hudson;Integrated in Cassandra #975 (See [https://builds.apache.org/job/Cassandra/975/])
    stop reading cache after max size-to-save is reached
patch by Chris Burroughs; reviewed by jbellis for CASSANDRA-2082

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1151211
Files : 
* /cassandra/trunk/test/unit/org/apache/cassandra/db/RowCacheTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted Handoff and schema race,CASSANDRA-2083,12497250,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,1/31/2011 19:57,3/12/2019 14:04,3/13/2019 22:24,2/3/2011 23:34,0.7.1,,,,0,,,,,,"If a node is down while a keyspace/cf is created and then data is inserted into the CF causing other nodes to hint, when the down node recovers it will lose some hints until the schema propagates:

{noformat}
ERROR 19:59:28,264 Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1000
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:117)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:377)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:50)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:70)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 19:59:28,356 Applying migration 28e2e7a4-2d74-11e0-9b6b-cdc89135952c
{noformat}",,,28800,28800,,0%,28800,28800,,,,,,,,,,,,03/Feb/11 17:36;jbellis;2083-v3.txt;https://issues.apache.org/jira/secure/attachment/12470163/2083-v3.txt,03/Feb/11 20:07;brandon.williams;2083-v4.txt;https://issues.apache.org/jira/secure/attachment/12470180/2083-v4.txt,01/Feb/11 22:46;brandon.williams;2083.txt;https://issues.apache.org/jira/secure/attachment/12469992/2083.txt,02/Feb/11 21:10;brandon.williams;2083v2.txt;https://issues.apache.org/jira/secure/attachment/12470070/2083v2.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,25:28.9,,,no_permission,,,,,,,,,,,,20433,,,Fri Feb 04 00:04:40 UTC 2011,,,,,,0|i0g97j:,92922,gdusbabek,gdusbabek,,,,,,,,,01/Feb/11 22:46;brandon.williams;Patch to wait up to RING_DELAY for schema alignment before delivering hints.  Also ensures that we keep schema up to date in gossip.,02/Feb/11 14:25;jbellis;gossip propagation is laggy enough that we've traditionally used the special version rpc call for schema version checks instead.  we should probably keep doing that here.,"02/Feb/11 21:10;brandon.williams;v2 builds on v1, in that it keeps schema up to date in gossip.  In HHOM we first check if the schema matches in gossip so we can avoid the rpc in the common case of no schema change, then fall back to waiting up to RING_DELAY for agreement via rpc before delivering hints.","02/Feb/11 21:18;jbellis;So...  now that I think about it more, maybe gossip lag is a GOOD thing here: w/ the RPC, everyone will start hammering the recovered nodes w/ replay at the same time.  W/ gossip it will likely be staggered.","02/Feb/11 21:22;brandon.williams;Interesting point, although schema will probably match 99% of the time and they'll replay at the same time anyway.","03/Feb/11 17:34;jbellis;v3 only uses gossip to check for agreement, and adds a random sleep from 0 to 60 seconds if schemas agree to stagger delivery a little.",03/Feb/11 17:59;brandon.williams;Committed with the debug line indicating the check is in progress bumped to info.,"03/Feb/11 18:31;brandon.williams;Reverted.  This has problems, the least of which is passing tests.","03/Feb/11 20:07;brandon.williams;v4 is similar to v3, but fixes migration announcement in gossip correctly by splitting the active announcement (via rpc) from the passive announcement (via gossip), increases logging to indicate it is sleeping to stagger the hints, and makes sure the host is still alive after the sleep before beginning delivery.","03/Feb/11 20:16;hudson;Integrated in Cassandra-0.7 #243 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/243/])
    ",03/Feb/11 23:28;gdusbabek;+1,03/Feb/11 23:34;brandon.williams;Committed.,"04/Feb/11 00:04;hudson;Integrated in Cassandra-0.7 #245 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/245/])
    Fix race between HH and schema changes.
Patch by brandonwilliams, reviewed by gdusbabek for CASSANDRA-2083
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UUID generation when specifying date-times is broken,CASSANDRA-2099,12497499,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,2/2/2011 20:06,3/12/2019 14:04,3/13/2019 22:24,2/3/2011 17:43,0.8 beta 1,,,,0,cql,,,,,"o.a.c.utils.UUIDGen properly guards against a clock moving backward, but this makes the creation of UUIDs based on arbitrary date-times problematic.",,,0,0,,0%,0,0,,,,,,,,,,,,02/Feb/11 21:02;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2099-fix-UUIDv1-generation-for-non-default-d.txt;https://issues.apache.org/jira/secure/attachment/12470069/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2099-fix-UUIDv1-generation-for-non-default-d.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,34:39.3,,,no_permission,,,,,,,,,,,,20441,,,Thu Feb 03 17:43:34 UTC 2011,,,,,,0|i0g9b3:,92938,gdusbabek,gdusbabek,,,,,,,,,03/Feb/11 03:34;gdusbabek;+1,"03/Feb/11 04:23;hudson;Integrated in Cassandra #705 (See [https://hudson.apache.org/hudson/job/Cassandra/705/])
    fix UUIDv1 generation for non-default date-times

Patch by eevans; reviewed by gdusbabek for CASSANDRA-2099
",03/Feb/11 17:43;urandom;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InternalException on system_update_column_family if column_metadata is not assigned,CASSANDRA-2096,12497417,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,2/2/2011 3:44,3/12/2019 14:04,3/13/2019 22:24,2/8/2011 5:06,0.7.1,,,,0,,,,,,"Steps to reproduce:

Execute system_update_column_family without passing in column_metadata in CfDef object.

Error:


java.lang.NullPointerException
	at org.apache.cassandra.config.CFMetaData.convertToAvro(CFMetaData.java:827)
	at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:882)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:4518)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:3227)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,,,,,02/Feb/11 03:48;lenn0x;0001-Fix-internal-exception-when-not-passing-in-column_me.patch;https://issues.apache.org/jira/secure/attachment/12470015/0001-Fix-internal-exception-when-not-passing-in-column_me.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:55.2,,,no_permission,,,,,,,,,,,,20440,,,Tue Feb 08 04:41:17 UTC 2011,,,,,,0|i0g9af:,92935,gdusbabek,gdusbabek,,,,,,,,,03/Feb/11 14:21;gdusbabek;I think this needs a rebase. The patch doesn't apply to trunk or 0.7.,03/Feb/11 17:54;gdusbabek;+1,"07/Feb/11 18:21;hudson;Integrated in Cassandra-0.7 #254 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/254/])
    Fix internal exception when not passing in column_metadata over Thrift patch by goffinet; reviewed by gdusbabek for CASSANDRA-2096
",08/Feb/11 04:30;jbellis;needs to be merged to trunk [there are conflicts]: svn merge https://svn.apache.org/repos/asf/cassandra/branches/cassandra-0.7 -c r1068028,08/Feb/11 04:41;lenn0x;Merged from 0.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing logging for some exceptions,CASSANDRA-2061,12496895,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,stuhood,stuhood,1/27/2011 4:29,3/12/2019 14:04,3/13/2019 22:24,8/16/2011 20:53,1.0.0,,,,0,,,,,,"{quote}Since you are using ScheduledThreadPoolExecutor.schedule(), the exception was swallowed by the FutureTask.

You will have to perform a get() method on the ScheduledFuture, and you will get ExecutionException if there was any exception occured in run().{quote}",,,28800,28800,,0%,28800,28800,,,,,,,,,,,,28/Jan/11 15:32;jbellis;2061-0.7.txt;https://issues.apache.org/jira/secure/attachment/12469679/2061-0.7.txt,15/Aug/11 05:01;jbellis;2061-v3.txt;https://issues.apache.org/jira/secure/attachment/12490402/2061-v3.txt,28/Jan/11 15:33;jbellis;2061.txt;https://issues.apache.org/jira/secure/attachment/12469681/2061.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,36:19.0,,,no_permission,,,,,,,,,,,,20421,,,Tue Aug 16 21:23:12 UTC 2011,,,,,,0|i0g933:,92902,xedin,xedin,,,,,,,,,"27/Jan/11 15:36;jbellis;As near as I can tell, an exception thrown on a scheduled task will never kill the executor, just like in TPE.  I don't remember why the author [me] wrote that code -- probably because it was replacing Timer and TimerTask, and an uncaught exception in a TimerTask _will_ kill the timer.

Patch removes RetryingSTPE and replaces with DebuggableSTPE that has an afterExecute copied from DTPE.",27/Jan/11 15:44;jbellis;new patch also updates afterExecute in both classes to log error if default uncaught exception handler is null,27/Jan/11 15:46;jbellis;patch for 0.7,"27/Jan/11 20:44;stuhood;* 2061.txt doesn't completely remove RetryingSTPE.java, and doesn't replace the usage in CFStore
* 2061-0.7.txt doesn't apply to the 0.7 branch

Also, will we need a separate patch for trunk?","28/Jan/11 15:27;jbellis;bq. 2061.txt doesn't completely remove RetryingSTPE.java

that's just how svn diff works.

bq. and doesn't replace the usage in CFStore

fixed.
","28/Jan/11 15:32;jbellis;bq. 2061-0.7.txt doesn't apply to the 0.7 branch

fixed.  also applies to trunk.","31/Jan/11 05:03;stuhood;Based on anecdotal evidence (it exposed an exception I was expecting), this looks good. But it looks like we can probably merge Debuggable(Scheduled)ThreadPool... they are appear to be essentially identical now.","31/Jan/11 05:09;jbellis;From the STPE javadoc, it sounds like STPE is more heavyweight than TPE and you don't want to use the former when all you need API-wise is the latter.  I have not done the code diving to confirm this though.","31/Jan/11 05:12;jbellis;STPE also notes,

bq. While this class inherits from ThreadPoolExecutor, a few of the inherited tuning methods are not useful for it. In particular, because it acts as a fixed-sized pool using corePoolSize threads and an unbounded queue, adjustments to maximumPoolSize have no useful effect.

We've wanted bounded queues in the past, and we definitely still use growable pools in places, so that's another reason to keep both.",03/Feb/11 17:42;jbellis;committed,"03/Feb/11 20:04;jbellis;reverted because of DynamicEndpointSnitchTest failure.  Not sure what is going on there -- I suspect some scheduled task is taking too long and keeping the DES update from happening, but why that should be affected by this patch is obscure to me.","03/Feb/11 20:16;hudson;Integrated in Cassandra-0.7 #243 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/243/])
    ","15/Aug/11 05:00;jbellis;Figured out the problem.  Here's the new version of logExceptionsAfterExecute that fixes it:

{code}
     public static void logExceptionsAfterExecute(Runnable r, Throwable t)
     {
-        // exceptions wrapped by FutureTask
-        if (r instanceof FutureTask<?>)
+        // Check for exceptions wrapped by FutureTask.  We do this by calling get(), which will
+        // cause it to throw any saved exception.
+        //
+        // Complicating things, calling get() on a ScheduledFutureTask will block until the task
+        // is cancelled.  Hence, the extra isDone check beforehand.
+        if ((r instanceof Future<?>) && ((Future<?>) r).isDone())
         {
             try
             {
-                ((FutureTask<?>) r).get();
+                ((Future<?>) r).get();
             }
{code}",15/Aug/11 05:01;jbellis;v3 attached for trunk.,16/Aug/11 20:53;xedin;committed.,"16/Aug/11 21:23;hudson;Integrated in Cassandra #1027 (See [https://builds.apache.org/job/Cassandra/1027/])
    Fix missing logging for some exceptions
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-2061

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1158439
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/trunk/src/java/org/apache/cassandra/concurrent/RetryingScheduledThreadPoolExecutor.java
* /cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableScheduledThreadPoolExecutor.java
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/trunk/test/unit/org/apache/cassandra/locator/DynamicEndpointSnitchTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
removetoken after removetoken rf error fails to work,CASSANDRA-2129,12497902,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,mbulman,mbulman,2/7/2011 20:56,3/12/2019 14:04,3/13/2019 22:24,7/14/2011 21:26,0.8.2,,,,0,,,,,,"2 node cluster, a keyspace existed with rf=2.  Tried removetoken and got:

mbulman@ripcord-maverick1:/usr/src/cassandra/tags/cassandra-0.7.0$ bin/nodetool -h localhost removetoken 159559397954378837828954138596956659794
Exception in thread ""main"" java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1)

Deleted the keyspace, and tried again:

mbulman@ripcord-maverick1:/usr/src/cassandra/tags/cassandra-0.7.0$ bin/nodetool -h localhost removetoken 159559397954378837828954138596956659794
Exception in thread ""main"" java.lang.UnsupportedOperationException: This node is already processing a removal. Wait for it to complete.",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,27/May/11 20:28;brandon.williams;2129-v2.txt;https://issues.apache.org/jira/secure/attachment/12480689/2129-v2.txt,16/Mar/11 19:33;brandon.williams;2129.txt;https://issues.apache.org/jira/secure/attachment/12473835/2129.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,55:05.1,,,no_permission,,,,,,,,,,,,20457,,,Thu Jul 14 22:14:00 UTC 2011,,,,,,0|i0g9hj:,92967,xedin,xedin,,,,,,,,,"07/Feb/11 21:55;jbellis;Does removetoken force clear up the second problem?

We should allow reducing the nodes in the cluster below the RF count, just as we allow creating a keyspace with RF greater than the node count.  (In both cases, writes will be rejected until more nodes are added or RF is reduced.)","08/Feb/11 16:26;mbulman;No.  force and status both report no token removals in process.

tags/cassandra-0.7.0# bin/nodetool -h localhost removetoken status                                 
RemovalStatus: No token removals in process.

Restarting the node that was processing the removal clears up the confusion/issue.","16/Mar/11 19:33;brandon.williams;Patch to allow removing the token, and throw UE instead of an internal error when trying to insert and the number of endpoints is less than the RF.",17/Mar/11 02:14;jbellis;SP.mutate should be throwing UAE already (assureSufficientLiveNodes).  Why isn't that working?,"17/Mar/11 20:13;brandon.williams;Because it calls rs.getNaturalEndpoints first, which throws ISE and SP.mutate only catches IOException.  I'm a little uneasy with catching both IOE and ISE, or modifying the RS to throw IOE, but I'm not sure what the best solution is.  The patch as-is still has ISE problems, at least on describe_keyspace.","18/Mar/11 02:39;jbellis;what purpose does leaving the ISE in serve, at this point?  should we just remove it?",06/Apr/11 16:59;nickmbailey;Note: a similar error occurs when trying to do describe_ring on a cluster where rf < N.,29/Apr/11 19:14;jbellis;Possibly the same bug was reported on the user list: http://permalink.gmane.org/gmane.comp.db.cassandra.user/15803,"27/May/11 20:28;brandon.williams;Removing ISE almost got us all the way there, but there was a subtle bug in WRH.determineBlockFor being relative to the amount of endpoints, instead of the RF.  v2 removes ISE and addresses this problem.",14/Jul/11 21:20;xedin;+1,14/Jul/11 21:26;brandon.williams;Committed,"14/Jul/11 22:14;hudson;Integrated in Cassandra-0.8 #215 (See [https://builds.apache.org/job/Cassandra-0.8/215/])
    Allow RF to exceed the number of nodes (but disallow writes)
Patch by brandonwilliams, reviewed by Pavel Yaskevich for CASSANDRA-2129

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1146900
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/OldNetworkTopologyStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/SimpleStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/NetworkTopologyStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/AbstractReplicationStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/WriteResponseHandler.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java cardinality option parsing typo,CASSANDRA-2121,12497851,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,2/7/2011 14:49,3/12/2019 14:04,3/13/2019 22:24,2/9/2011 14:32,0.7.1,,,,0,,,,,,"Session.java
{noformat} 
            if (cmd.hasOption(""C""))
                cardinality = Integer.parseInt(cmd.getOptionValue(""t""));
{noformat} 

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,32:28.3,,,no_permission,,,,,,,,,,,,20453,,,Wed Feb 09 14:57:58 UTC 2011,,,,,,0|i0g9fr:,92959,jbellis,jbellis,,,,,,,,,"09/Feb/11 14:32;jbellis;committed, thanks!","09/Feb/11 14:57;hudson;Integrated in Cassandra-0.7 #265 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/265/])
    fix parsing of cardinality option
patch by Chris Burroughs; reviewed by jbellis for CASSANDRA-2121
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cli read_repair_chance input not validated,CASSANDRA-2146,12498154,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,cburroughs,cburroughs,2/9/2011 18:28,3/12/2019 14:03,3/13/2019 22:24,2/10/2011 18:51,0.7.2,,,,0,,,,,,"{noformat}
put(ColumnFamilyArgument.READ_REPAIR_CHANCE, ""Probability (0.0-1.0) with which to perform read repairs on CL.ONE reads"");
{noformat}

The input range is not enforced so 
{noformat}
create column family ... with ... read_repair_chance = 25;
{noformat}

Will result in
{noformat}
Keyspace: ks1:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
    Replication Factor: 3
  Column Families:
    ColumnFamily: cf1
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
...
      Read repair chance: 25.0
{noformat}


I am unsure if in practice this means RR chance 100%, or something surprising.  (I ran into this because read_repair_chance requires a leading 0 and ommiting it results in an unhelpful ""Command not found:"" message).
",,;10/Feb/11 16:54;xedin;1800,1800,0,1800,100%,1800,0,1800,,,,,,,,,,,10/Feb/11 16:53;xedin;CASSANDRA-2146.patch;https://issues.apache.org/jira/secure/attachment/12470782/CASSANDRA-2146.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,51:12.4,,,no_permission,,,,,,,,,,,,20463,,,Thu Feb 10 22:11:36 UTC 2011,,,,,,0|i0g9lj:,92985,jbellis,jbellis,,,,,,,,,10/Feb/11 18:51;jbellis;committed,"10/Feb/11 22:11;hudson;Integrated in Cassandra-0.7 #276 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/276/])
    validate read_repair_chance
patch by xedin; reviewed by jbellis for CASSANDRA-2146
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default concurrency values are improperly proportioned,CASSANDRA-1972,12495338,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,1/12/2011 7:04,3/12/2019 14:03,3/13/2019 22:24,1/14/2011 19:14,0.7.1,,,,0,,,,,,"The ""default""/""suggested"" {{concurrent_reads}} value is much too low. It assumes that CPU will be the bottleneck, rather than IO, and for most deployments, this will not be the case. Additionally it is better to be queued for IO in the kernel or on your device than in user space, because the former work to optimize queue order.

Additionally, reads are much cheaper than writes in terms of CPU time (since writes can experience contention due to retries), so while {{concurrent_writes}} should probably factor in the number of cores on the machine, {{concurrent_reads}} should probably be calculated purely by number of spindles.",,,,,,,,,,,,,,,,,,,,14/Jan/11 04:53;stuhood;0001-Recommendation-changes.txt;https://issues.apache.org/jira/secure/attachment/12468346/0001-Recommendation-changes.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,07:26.7,,,no_permission,,,,,,,,,,,,20386,,,Fri Jan 14 19:55:03 UTC 2011,,,,,,0|i0g8j3:,92812,jbellis,jbellis,,,,,,,,,"12/Jan/11 18:07;scode;I agree in general, unless the intent is to make out-of-the-box stress testing on memory-bound workloads optimized.

In particular I agree about being queued in the kernel, and even further down on devices or RAID controllers. Propagating outstanding I/O requests all the way down, within reason, is key to saturating the capacity of underlying spindles.","12/Jan/11 18:24;jbellis;Yes, this sounds like a pretty straightforward win.  Have you done testing on better values, Stu?","14/Jan/11 04:08;stuhood;Based on IO/seek bound tests on two machines with 4 and 12 drives in RAID-0 and RAID-10, we saw the best performance with around 20 threads per drive. Not everyone will be completely IO bound though (hopefully _something_ is in cache), so perhaps a better recommended value is 16 threads per drive. I'll write a new blurb for the config file.","14/Jan/11 19:14;jbellis;committed, thanks!","14/Jan/11 19:55;hudson;Integrated in Cassandra-0.7 #157 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/157/])
    update concurrent_reads default setting + comments
patch by Stu Hood; reviewed by jbellis for CASSANDRA-1972
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error saving cache on Windows,CASSANDRA-2207,12499223,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,tantra,tantra,2/21/2011 12:14,3/12/2019 14:03,3/13/2019 22:24,2/23/2011 14:10,0.7.3,,,,0,,,,,,"I launch clean cassandra 7.2 instalation, and after few days i look at system.log follow error (more then 10 times):


ERROR [CompactionExecutor:1] 2011-02-19 02:56:17,965 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.io.IOException: Unable to rename cache to F:\Cassandra\7.2\saved_caches\system-LocationInfo-KeyCache
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to rename cache to F:\Cassandra\7.2\saved_caches\system-LocationInfo-KeyCache
    at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:85)
    at org.apache.cassandra.db.CompactionManager$9.runMayThrow(CompactionManager.java:746)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    ... 6 more
",WindowsXP(SP3) 32 bit,,3600,3600,,0%,3600,3600,,,,,,,,,,,,21/Feb/11 21:27;jbellis;2207.txt;https://issues.apache.org/jira/secure/attachment/12471579/2207.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:04.7,,,no_permission,,,,,,,,,,,,20504,,,Wed Feb 23 15:18:09 UTC 2011,,,,,,0|i0g9z3:,93046,mdennis,mdennis,,,,,,,,,21/Feb/11 21:27;jbellis;Apparently Windows is won't let you rename over an existing file. Patch attached to explicitly delete the old one first.,23/Feb/11 07:31;mdennis;+1,23/Feb/11 14:10;jbellis;committed,"23/Feb/11 15:18;hudson;Integrated in Cassandra-0.7 #309 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/309/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saved-cache files are created for empty caches,CASSANDRA-2172,12498711,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mdennis,jbellis,jbellis,2/16/2011 2:10,3/12/2019 14:03,3/13/2019 22:24,2/16/2011 5:58,0.7.3,,,,0,,,,,,This results in a harmless EOFException on startup.,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,16/Feb/11 04:30;mdennis;2172-cassandra-0.7-v2.txt;https://issues.apache.org/jira/secure/attachment/12471149/2172-cassandra-0.7-v2.txt,16/Feb/11 02:18;jbellis;2172.txt;https://issues.apache.org/jira/secure/attachment/12471143/2172.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,30:35.4,,,no_permission,,,,,,,,,,,,20479,,,Wed Feb 16 06:16:58 UTC 2011,,,,,,0|i0g9rb:,93011,jbellis,jbellis,,,,,,,,,16/Feb/11 02:18;jbellis;patch to avoid creating file for empty cache,"16/Feb/11 04:30;mdennis;it's not safe to return null as some callers wait on the future.

v2 patch centralizes check and delete logic in one place and returns a valid future.",16/Feb/11 04:43;mdennis;noticed CASSANDRA-2174 while reviewing this,16/Feb/11 05:58;jbellis;committed v2,"16/Feb/11 06:16;hudson;Integrated in Cassandra-0.7 #281 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/281/])
    don't save empty caches
patch by mdennis; reviewed by jbellis for CASSANDRA-2172
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memtable_flush_after_mins setting not working,CASSANDRA-2183,12498922,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,chenyy,chenyy,2/17/2011 16:51,3/12/2019 14:03,3/13/2019 22:24,2/23/2011 4:57,0.7.3,,,,0,,,,,,"We have observed the behavior that memtable_flush_after_mins setting not working occasionally.   After some testing and code digging, we finally figured out what going on.
The memtable_flush_after_mins won't work on certain condition with current implementation in Cassandra.

In org.apache.cassandra.db.Table,  the scheduled flush task is setup by the following code during construction.

------------------------------------------------------------------------------------------------------------------
int minCheckMs = Integer.MAX_VALUE;
       
for (ColumnFamilyStore cfs : columnFamilyStores.values())  
{
    minCheckMs = Math.min(minCheckMs, cfs.getMemtableFlushAfterMins() * 60 * 1000);
}

Runnable runnable = new Runnable()
{
   public void run()
   {
       for (ColumnFamilyStore cfs : columnFamilyStores.values())
       {
           cfs.forceFlushIfExpired();
       }
   }
};
flushTask = StorageService.scheduledTasks.scheduleWithFixedDelay(runnable, minCheckMs, minCheckMs, TimeUnit.MILLISECONDS);
------------------------------------------------------------------------------------------------------------------------------

Now for our application, we will create a keyspacewithout without any columnfamily first.  And only add needed columnfamily later depends on request.

However, when keyspacegot created (without any columnfamily ), the above code will actually schedule a fixed delay flush check task with Integer.MAX_VALUE ms
since there is no columnfamily yet.

Later when you add columnfamily to this empty keyspace, the initCf() method in Table.java doesn't check whether the scheduled flush check task interval need
to be updated or not.   To fix this, we'd need to restart the Cassandra after columnfamily added into the keyspace. 

I would suggest that add additional logic in initCf() method to recreate a scheduled flush check task if needed.
",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,17/Feb/11 16:57;jbellis;2183.txt;https://issues.apache.org/jira/secure/attachment/12471294/2183.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,57:32.7,,,no_permission,,,,,,,,,,,,20486,,,Wed Feb 23 15:18:09 UTC 2011,,,,,,0|i0g9tr:,93022,scode,scode,,,,,,,,,"17/Feb/11 16:57;jbellis;The original approach of checking barely more often than we think is necessary is overengineering the problem; even with 1000s of CFs, checking every 10s would have no affect whatsoever on performance.  Patch attached.","21/Feb/11 22:25;scode;(was asked to review) I agree on the overengineering. Patch looks good to me. The obvious caveat of being limited to 10 second resolution should be utterly irrelevant for the purposes for which the time based flushing is intended.
",23/Feb/11 04:57;jbellis;committed,"23/Feb/11 15:18;hudson;Integrated in Cassandra-0.7 #309 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/309/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli --keyspace option doesn't work properly when used with authentication,CASSANDRA-2162,12498565,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jancona,jancona,jancona,2/14/2011 19:30,3/12/2019 14:03,3/13/2019 22:24,2/17/2011 21:22,0.7.3,,Legacy/Tools,,0,authentication,cli,,,,"The logic to select the keyspace is applied before authentication credentials are processed in cassandra-cli. This leads to a ""Keyspace FOO not found"" message at login for a keyspace that exists.
",,,,,,,,,,,,,,,,,,,,14/Feb/11 19:38;jancona;0001-Swap-order-of-authentication-and-keyspace-processing.patch;https://issues.apache.org/jira/secure/attachment/12471011/0001-Swap-order-of-authentication-and-keyspace-processing.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,32:02.3,,,no_permission,,,,,,,,,,,,20473,,,Thu Feb 17 21:38:45 UTC 2011,,,,,,0|i0g9p3:,93001,xedin,xedin,,,,,,,,,14/Feb/11 19:36;jancona;Swap the order of processing the authentication and keyspace parameters.,17/Feb/11 20:32;xedin;LGTM,17/Feb/11 21:22;jbellis;committed,"17/Feb/11 21:38;hudson;Integrated in Cassandra-0.7 #288 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/288/])
    Swap the order of processing the authentication and keyspace CLI arguments
patch by Jim Ancona; reviewed by Pavel Yaskevich for CASSANDRA-2162
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI does not use sub-comparator on Super CF `get`.,CASSANDRA-2136,12497984,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,xedin,xedin,2/8/2011 14:55,3/12/2019 14:03,3/13/2019 22:24,2/10/2011 18:48,0.7.2,,Legacy/Tools,,0,,,,,,"[default@foo] get foo[page-field];
=> (super_column=20110208,
    (column=82f4c650-2d53-11e0-a08b-58b035f3f60d, value=msg1,
timestamp=1297159430471000)
    (column=82f4c650-2d53-11e0-a08b-58b035f3f60e, value=msg2,
timestamp=1297159437423000)
    (column=82f4c650-2d53-11e0-a08b-58b035f3f60f, value=msg3,
timestamp=1297159439855000))
Returned 1 results.

[default@foo] get foo[page-field][20110208];
, value=msg1, timestamp=1297159430471000)
=> (column=???P-S???X?5??, value=msg2, timestamp=1297159437423000)
=> (column=???P-S???X?5??, value=msg3, timestamp=1297159439855000)
Returned 3 results.

[default@foo] get
foo[page-field][20110208][82f4c650-2d53-11e0-a08b-58b035f3f60e];
=> (column=???P-S???X?5??, value=msg2, timestamp=1297159437423000)","Column family settings:
       - name: foo
         column_type: Super
         compare_with: AsciiType
         compare_subcolumns_with: TimeUUIDType
         default_validation_class: AsciiType",;10/Feb/11 17:42;xedin;3600,3600,0,3600,100%,3600,0,3600,,,,,,,,,,,10/Feb/11 17:42;xedin;CASSANDRA-2136.patch;https://issues.apache.org/jira/secure/attachment/12470787/CASSANDRA-2136.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,48:09.9,,,no_permission,,,,,,,,,,,,20461,,,Thu Feb 10 22:11:36 UTC 2011,,,,,,0|i0g9jb:,92975,jbellis,jbellis,,,,,,,,,10/Feb/11 18:48;jbellis;committed,"10/Feb/11 22:11;hudson;Integrated in Cassandra-0.7 #276 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/276/])
    format subcolumn names with subcomparator
patch by xedin; reviewed by jbellis for CASSANDRA-2136
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tombstones not collected post-repair,CASSANDRA-2279,12500654,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,j.casares,j.casares,3/7/2011 17:42,3/12/2019 14:03,3/13/2019 22:24,4/8/2011 22:23,0.7.5,,Legacy/Tools,,0,,,,,,"The keys would only show up in sstables2json and look like this:

(root@aps4):/opt/cassandra/storage/queue/data/Panama Wed Feb 23 07:24:34am 
===> /opt/cassandra/bin/sstable2json Queue-2527-Data.db -k waq:publicMessageIndexingWorkArea:PUM8a65ce95-9d35-4941-928c-dd5965e8b29b 
2011-02-23 07:24:43,710 INFO [org.apache.cassandra.config.DatabaseDescriptor] - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap 
2011-02-23 07:24:43,972 INFO [org.apache.cassandra.io.SSTableReader] - Opening /opt/cassandra/storage/queue/data/Panama/Queue-2527-Data.db 
{ 
""waq:publicMessageIndexingWorkArea:PUM8a65ce95-9d35-4941-928c-dd5965e8b29b"": [] 
} 
(root@aps4):/opt/cassandra/storage/queue/data/Panama Wed Feb 23 07:24:44am 
===>

The steps that I took to reproduce it were:
Create a keyspace, column family, and a key
Delete the key on Node 1 using the cli (del cf['key'];)
Flush 
Repair on a cluster with more than 1 node
Wait GCSeconds 
Compact
And the empty row would appear on Node 2

However, when I was able to get rid of the empty rows, I was following these steps on a single machine: 
Create a keyspace, column family, and a key
Delete the key
Flush
Sample write (writing to some temporary key)
Deleting the attribute to that temporary key (not the entire key)
Flush
Compact

or these steps:
Create a keyspace, column family, and a key
Delete the key
Flush 
Wait GCseconds
Compact

",,,,,,,,,,,,,,,,,,,,14/Mar/11 18:18;slebresne;RowIteration-unit-tests.patch;https://issues.apache.org/jira/secure/attachment/12473595/RowIteration-unit-tests.patch,14/Mar/11 18:18;slebresne;fix-RowIteratorFactory.patch;https://issues.apache.org/jira/secure/attachment/12473596/fix-RowIteratorFactory.patch,14/Mar/11 22:27;j.casares;nodeA.txt;https://issues.apache.org/jira/secure/attachment/12473620/nodeA.txt,14/Mar/11 22:27;j.casares;nodeB.txt;https://issues.apache.org/jira/secure/attachment/12473621/nodeB.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,35:30.9,,,no_permission,,,,,,,,,,,,20537,,,Fri Apr 08 22:23:58 UTC 2011,,,,,,0|i0gafj:,93120,jbellis,jbellis,,,,,,,,,"09/Mar/11 15:35;slebresne;I'm not able to reproduce. And I don't see why repair would screw up with tombstones collection. Only compaction should remove them and repair shouldn't have anything to do with this. If sstable2json is the only one to show it, it's even weirder (that is, listing with the cli should also show an empty row if there was problem with tombstone collection).

Are you sure you compacted node 2 (on which the empty row showed up) ? Are you sure there wasn't a Queue-2527-Compacted file alongside Queue-2527-Data.db ? Are you sure node 2 local time wasn't highly out of sync with node 1 ?","11/Mar/11 10:30;slebresne;Ok, strongest hypothesis: this is a sign of CASSANDRA-2305.
Joaquim: was some row cache enabled on the incriminated CF ?","11/Mar/11 17:35;j.casares;The entire thing was reproduced on a fresh install and I was just using the standard Keyspace that already exists, I believe. I'll go through and follow my steps again and give a more detailed account.","14/Mar/11 13:44;slebresne;It's a bit of a shot in the dark since I'm not sure exactly how was produced and how to reproduce this, but I found a place in RowIteratorFactor where we don't handle a CF localDeletionTime correctly. Since RowIterator is used for repair and for sstable2json, it sounds like a promising candidate for this (and it's a legit problem even if not the one at hand here).

Patch is against 0.7.",14/Mar/11 14:28;jbellis;how hard would it be to create a unit test that catches this?,14/Mar/11 14:38;slebresne;Should be doable. I'll do that.,14/Mar/11 16:50;slebresne;Attaching the unit test.,14/Mar/11 17:09;jbellis;what's the significance of gc_grace_seconds: 2 in the test CF?  I don't see any sleeps in the test.,"14/Mar/11 17:19;slebresne;Sorry, that was me trying stuff and forgetting to remove it. I update the test to use an existing column with standard gc_grace","14/Mar/11 18:18;slebresne;There was actually 2 related bugs in RowIteratorFactory. Re-attaching new patch with the 2 unit tests and the fix (fixing both problem and simplifying, I think, RowIteratorFactory).","14/Mar/11 19:32;jbellis;that's a big improvement over the existing factory code!  committed.

we'll need a separate patch for 0.6 -- RowIteratorFactory doesn't exist (the analogous code is inline in getRangeSlice).","14/Mar/11 19:48;hudson;Integrated in Cassandra-0.7 #378 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/378/])
    fix tombstone handling in repair andsstable2json
patch by slebresne; reviewed by jbellis for CASSANDRA-2279
","14/Mar/11 22:27;j.casares;Here are all the commands that were run. I was messing around with T2 references and the bug didn't show up the first time, so I tried it again.

The second time it worked following the exact steps that I listed. I set the GCSeconds to be 30 minutes to wait for the repair to finish 100%.

This time however, the SSTables actually have the old values as well.","18/Mar/11 14:30;slebresne;bq. we'll need a separate patch for 0.6

I don't think 0.6 suffers from the problem fixed by the attached patch. It uses CFStore.getFamily() for range slices to do its bidding which handles correctly the column family deletion times as far as I can tell.

Which make me think that there could be something else at hand here. I'll have a look at Joaquin instructions to try to reproduce what he is seeing. ","30/Mar/11 22:11;j.casares;I just read this and was wondering if this may be the case:
http://wiki.apache.org/cassandra/Operations#Dealing_with_the_consequences_of_nodetool_repair_not_running_within_GCGraceSeconds

GCGraceSeconds was set to 30 minutes to allow the repair to finish and I waited 35 before running the compaction on the above steps.","08/Apr/11 22:06;jbellis;compaction should be runnable any time, as long as you do repair before gcgrace expires.
","08/Apr/11 22:15;slebresne;For the record, we did try to reproduce with Joaquin and weren't able to find anything wrong in there. In particular, in one instance when we though we had a column coming back from the dead, we were actually looking at a compacted sstable (which sstable2json won't avoid).

So I do not know if there is an actual problem here.","08/Apr/11 22:23;jbellis;Sounds good, thanks for looking into that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BRAF assertion error,CASSANDRA-2256,12499978,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,2/28/2011 21:29,3/12/2019 14:03,3/13/2019 22:24,3/3/2011 16:44,0.7.4,,,,0,,,,,,"While investigating CASSANDRA-2240 I ran into this:

{noformat}
java.lang.AssertionError
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.read(BufferedRandomAccessFile.java\
:230)
        at java.io.RandomAccessFile.readByte(RandomAccessFile.java:589)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:273)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:539)
{noformat}",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,02/Mar/11 22:09;xedin;CASSANDRA-2256-v2.patch;https://issues.apache.org/jira/secure/attachment/12472476/CASSANDRA-2256-v2.patch,02/Mar/11 19:30;xedin;CASSANDRA-2256.patch;https://issues.apache.org/jira/secure/attachment/12472452/CASSANDRA-2256.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,52:23.0,,,no_permission,,,,,,,,,,,,20529,,,Thu Mar 03 16:27:39 UTC 2011,,,,,,0|i0gaaf:,93097,jbellis,jbellis,,,,,,,,,"02/Mar/11 17:52;xedin;Test for the problem:
{code}
public void testAssertOnRead() throws IOException
    {
        BufferedRandomAccessFile file = createTempFile(""braf"");
        file.write(new byte[10]);
        file.sync();

        BufferedRandomAccessFile copy = new BufferedRandomAccessFile(file.getPath(), ""r"");

        copy.seek(15);
        copy.read();

        file.close();
        copy.close();
    }
{code}

This happens when you are trying to seek to position > file length on read-only file and then read (because fileLength is cached, method isEOF() does not work properly). I don't think that we should allow such seeks.","02/Mar/11 19:02;jbellis;Technically RAF.seek allows seeking beyond EOF and writing there (presumably the intervening space would be filled with 0?) but Cassandra doesn't use this and it's kind of a weird corner case.  There's a pretty strong assumption in BRAF that current <= EOF.

So, I would be okay with throwing EOFException if you try to seek past EOF.","02/Mar/11 19:36;jbellis;Is there a way to do this w/o calling channel.size() for each seek?  We seek twice for every row written, and channel.size() is fairly expensive.",02/Mar/11 19:41;xedin;There is no way to skip this unless we will check this only for read-only files.,"02/Mar/11 19:50;tjake;throwing EOF is good.

Regarding the check of length on every seek for writable files, I think you could change it to see if the seek is < bufferOffset + buffer.length before calling length()

","02/Mar/11 20:11;xedin;It is also half-solving, I think we can check that for read-only files and for writable we can leave this as is (makes perfect sense), any counter-argument?","02/Mar/11 20:19;tjake;The counter argument is we call seek() 2 times for every row in SSTableWriter.

So we need to make sure we don't call stat() unless we have no other choice.
Another approach would be to set fileLength based on the total number of bytes from the starting size, or 0 for a new file...","02/Mar/11 20:36;xedin;if we will be doing this for read-only condition will be 
{code}
if (fileLength != -1 && newPosition > fileLength)
    throw new EOFException();
{code}

this won't call channel.size() or do any expensive calculations like length()","02/Mar/11 20:45;tjake;Oh so you mean let the it throw an AssertionError if you try to seek beyond the end of a file in ""rw"" mode?",02/Mar/11 20:48;jbellis;Not a fan of relying on assert when we really want EOFException.,"02/Mar/11 20:55;xedin;Seems that we don't understand each other here :)

seek method will look like this:

{code}
    public void seek(long newPosition) throws IOException
    {
        if (newPosition < 0)
            throw new IllegalArgumentException(""new position should not be negative"");

        if (fileLength != -1 && newPosition > fileLength)
            throw new EOFException(""unable to seek past the end of the file in read-only mode."");

        current = newPosition;

        if (newPosition >= bufferOffset + validBufferBytes || newPosition < bufferOffset)
            reBuffer(); // this will set bufferEnd for us
    }
{code}

We set fileLength = -1 for ""rw"" mode and caching fileLength = channel.size() for ""r"" mode files, so condition ""fileLength != -1 && newPosition > fileLength"" will allow us to block seeking past the end of the file in ""r"" mode leaving ""rw"" untouched (which makes a good sense even if it's unused right now and lets us avoid calling length() for every seek()).","02/Mar/11 21:00;tjake;Right, but what happens if you try to seek past the end of a file in ""rw"" mode?",02/Mar/11 21:04;xedin;That will be allowed as it is right now but that won't create any problems because length of the file is dynamic in that case and isEOF() method will be working properly.,02/Mar/11 21:23;tjake;If that's tested and proven then I don't see an issue with using the code snippet above.,"02/Mar/11 21:27;xedin;Yes, it is. I will attach v2 patch asap.",03/Mar/11 16:06;jbellis;committed with additional test that writing past EOF actually works,"03/Mar/11 16:27;hudson;Integrated in Cassandra-0.7 #341 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/341/])
    throw EOFException when seeking past EOF in read-only mode
patch by Pavel Yaskevich; reviewed by tjake and jbellis for CASSANDRA-2256
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sstable2json dies with ""Too many open files"", regardless of ulimit",CASSANDRA-2304,12500973,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,alienth,alienth,3/10/2011 0:54,3/12/2019 14:03,3/13/2019 22:24,3/10/2011 17:13,0.7.4,,Legacy/Tools,,0,,,,,,"Running sstable2json on the attached sstable eventually results in the following:

{code}
Exception in thread ""main"" java.io.IOError: java.io.FileNotFoundException: /var/lib/cassandra/data/reddit/CommentSortsCache-f-9764-Data.db (Too many open files)
        at org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:68)
        at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:567)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:49)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:68)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
        at org.apache.cassandra.tools.SSTableExport.serializeRow(SSTableExport.java:187)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:355)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:377)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:390)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:448)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/reddit/CommentSortsCache-f-9764-Data.db (Too many open files)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:111)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:106)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:91)
        at org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:62)
{code}

Set my ulimit -n to 60000 and got the same result. Leaking file descriptors?",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,10/Mar/11 16:38;jbellis;2304.txt;https://issues.apache.org/jira/secure/attachment/12473282/2304.txt,10/Mar/11 16:56;xedin;CASSANDRA-2304-v2.patch;https://issues.apache.org/jira/secure/attachment/12473288/CASSANDRA-2304-v2.patch,10/Mar/11 01:06;alienth;sstable.tar.bz2;https://issues.apache.org/jira/secure/attachment/12473227/sstable.tar.bz2,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,38:02.5,,,no_permission,,,,,,,,,,,,20549,,,Thu Mar 10 17:40:56 UTC 2011,,,,,,0|i0gal3:,93145,xedin,xedin,,,,,,,,,10/Mar/11 01:06;alienth;Sstable which causes sstable2json to die. Grabbed from an 0.7.3 node.,"10/Mar/11 01:07;alienth;Output from lsof. Thousands of lines of the following:

{code}
java      1766       root 1080r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1081r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1082r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1083r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1084r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1085r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1086r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1087r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1088r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1089r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1090r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1091r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1092r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1093r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1094r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1095r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
{code}",10/Mar/11 16:38;jbellis;This is a bug with non-mmap'd I/O.,10/Mar/11 16:38;jbellis;patch to close column iterator and only do one pass per row,10/Mar/11 16:56;xedin;with number of the exported columns properly incremented. LGTM.,10/Mar/11 17:13;jbellis;committed v2,"10/Mar/11 17:40;hudson;Integrated in Cassandra-0.7 #370 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/370/])
    fix fd leak in sstable2json with non-mmap'd i/o
patch by jbellis; reviewed by Pavel Yaskevich for CASSANDRA-2304
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tombstoned rows not purged from cache after gcgraceseconds,CASSANDRA-2305,12500984,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,paladin8,paladin8,3/10/2011 4:39,3/12/2019 14:03,3/13/2019 22:24,4/10/2011 18:50,0.7.5,,,,0,,,,,,"From email to list:

I was wondering if this is the expected behavior of deletes (0.7.0). Let's say I have a 1-node cluster with a single CF which has gc_grace_seconds = 0. The following sequence of operations happens (in the given order):

insert row X with timestamp T
delete row X with timestamp T+1
force flush + compaction
insert row X with timestamp T

My understanding is that the tombstone created by the delete (and row X) will disappear with the flush + compaction which means the last insertion should show up. My experimentation, however, suggests otherwise (the last insertion does not show up).

I believe I have traced this to the fact that the markedForDeleteAt field on the ColumnFamily does not get reset after a compaction (after gc_grace_seconds has passed); is this desirable? I think it introduces an inconsistency in how tombstoned columns work versus tombstoned CFs. Thanks.",,;11/Mar/11 10:26;slebresne;7200,7200,0,7200,100%,7200,0,7200,,,,,,,,,,,11/Mar/11 10:24;slebresne;0001-Compaction-test.patch;https://issues.apache.org/jira/secure/attachment/12473375/0001-Compaction-test.patch,11/Mar/11 10:24;slebresne;0002-Invalidate-row-cache-on-compaction-purge.patch;https://issues.apache.org/jira/secure/attachment/12473376/0002-Invalidate-row-cache-on-compaction-purge.patch,14/Mar/11 14:07;slebresne;2305_2nd_patch.patch;https://issues.apache.org/jira/secure/attachment/12473565/2305_2nd_patch.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,01:28.9,,,no_permission,,,,,,,,,,,,20550,,,Sun Apr 10 23:17:06 UTC 2011,,,,,,0|i0galb:,93146,jbellis,jbellis,,,,,,,,,"10/Mar/11 04:41;paladin8;For a little more info, I think this only happens when you remove an entire row. If you delete specific columns, the tombstones are handled appropriately.","10/Mar/11 18:01;slebresne;This will also happen if you remove all the columns inside the row (even though you didn't issued a row deletion command).

The problem is that when you flush + compact and gcGrace has elapsed, if the row is empty (i.e. all tombstone have been collected), the row itself is collected. This means that when you issue the second wave of inserts, there is no trace whatsoever of the row.

That's why you are not supposed to have a gcGrace too low and why it is highly advised to use the current time as a timestamp. If so, the scenario above will never happen.

Best thing we can do is probably to edit http://wiki.apache.org/cassandra/DistributedDeletes to add that gcGrace should be such that no insert with a timestamp lower that a delete could reach any given node after gcGrace has elapsed.
","10/Mar/11 18:32;jbellis;bq. The problem is that when you flush + compact and gcGrace has elapsed, if the row is empty (i.e. all tombstone have been collected), the row itself is collected. This means that when you issue the second wave of inserts, there is no trace whatsoever of the row.

That's what's supposed to happen, but Jeffrey is saying that is NOT what he observes.","10/Mar/11 18:49;slebresne;Oups, my mistake. I somehow confused myself. I was not able to reproduce though, but I'll try harder tomorrow.","11/Mar/11 10:24;slebresne;I think this is due to row cache. We do not invalidate the row cache when a row is fully collected by compaction.

Jeffrey, can you confirm that you had some row cache enabled when doing your experiments ?

Attaching 2 patch against 0.7. The first one is a unit test showing the failure, the second one is the fix.",11/Mar/11 14:26;jbellis;committed,"11/Mar/11 20:57;paladin8;I actually don't have row cache enabled (I just checked cfstats to make sure), so I don't think that's the cause of my problem in particular. Here's some more info that may or may not be correct:

- When I run the compaction, in ColumnFamilyStore.removeDeletedStandard() I see that columns are being removed because of the c.timestamp() <= cf.getMarkedForDeleteAt() condition, which makes sense since I issued a delete on the entire row.
- However, after the compaction, I do the insert, and if I flush/compact again, I still see the columns being removed because of that condition. It seems like the markedForDeleteAt field on the ColumnFamily is persisting across the major compaction which I believe is hiding the newly inserted column.

Also, my initial steps to repro were not correct, which made it hard to figure out the root cause. Here is a proper repro:

- Create a CF with gc_grace_seconds = 0 and no row cache.
- Insert row X, col A with timestamp 0.
- Insert row X, col B with timestamp 2.
- Remove row X with timestamp 1 (expect col A to disappear, col B to stay).
- Wait 1 second.
- Force flush and compaction.
- Insert row X, col A with timestamp 0.
- Read row X, col A (see nothing).

Inserting row X, col B is necessary for this to repro because if all the columns in a row disappear, the ColumnFamily object goes away and the markedForDeleteAt field is reset. Only when a column still exists does the field persist across the compaction. Hope this helps!",11/Mar/11 20:57;paladin8;I believe the cause to be something else (see latest comment).,"11/Mar/11 23:07;hudson;Integrated in Cassandra-0.7 #375 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/375/])
    ","12/Mar/11 11:22;slebresne;Ok, I understand what you meant. I've created CASSANDRA-2317 with the fix since we have already committed a patch here. Thanks a lot for the report.","12/Mar/11 11:23;slebresne;Remarking this resolved, the follow up is in CASSANDRA-2317 instead.","14/Mar/11 14:00;slebresne;I'm reopening because the committed patch, while ok, is only a partial fix.","14/Mar/11 14:07;slebresne;The first patch was purging the cache when the full row is expired. But we don't remove expired tombstone from the cache.

Attaching a second patch to purge the cache. This has two purposes: 
  # avoid surprise for client getting tombstones back from query (either sstable2json or rangeSlice) even well after gc_grace and compaction has occured
  # reclaim some memory for row sitting in the cache for a very long time

Note that this patch introduces concurrent deletes, and as such SHOULDN'T be applied to a branch that do not have CASSANDRA-1559 (0.7 and 0.8 have it).

Patch is against 0.7",08/Apr/11 21:47;jbellis;+1,10/Apr/11 18:50;slebresne;Committed to 0.7 (r1090867) and a rebased version to trunk (r1090866),"10/Apr/11 19:07;hudson;Integrated in Cassandra-0.7 #429 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/429/])
    Purge tombstone from row cache (0.7 version)
patch by slebresne; reviewed by jbellis for CASSANDRA-2305
","10/Apr/11 23:17;hudson;Integrated in Cassandra #846 (See [https://hudson.apache.org/hudson/job/Cassandra/846/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CassandraStorage for pig checks for environment variable on mappers/reducers, but it should only need to be set on the machine launching pig.",CASSANDRA-2310,12501053,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,eldondev,eldondev,3/10/2011 19:14,3/12/2019 14:03,3/13/2019 22:24,3/10/2011 20:47,0.7.4,,,,0,,,,,,"Only error out if necessary pig settings have not previously been set in job config. CassandraStorage checks for environment variables on mappers/reducers, but it should only need to be set on the machine launching the pig jobs.",,,,,,,,,,,,,,,,,,,,10/Mar/11 19:16;eldondev;0001-Dont-fail-if-configs-already-set.patch;https://issues.apache.org/jira/secure/attachment/12473312/0001-Dont-fail-if-configs-already-set.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,30:41.8,,,no_permission,,,,,,,,,,,,20553,,,Fri Mar 11 03:47:16 UTC 2011,,,,,,0|i0gamf:,93151,brandon.williams,brandon.williams,,,,,,,,,"10/Mar/11 19:15;eldondev;If the environment variable is set, it wins. If not, and it's already stored in the job configuration, do nothing. If it doesn't exist at all, fail.","10/Mar/11 19:30;jeromatron;+1 - works where before it would error out in mapreduce mode (post storefunc). I did suggest maybe unifying the conditions and using an else or something to make it more concise, but that's a trivial thing.","10/Mar/11 20:47;brandon.williams;Committed, thanks!","11/Mar/11 03:47;hudson;Integrated in Cassandra-0.7 #373 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/373/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stress.java columns are bigger than advertised,CASSANDRA-2312,12501067,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,3/10/2011 21:33,3/12/2019 14:03,3/13/2019 22:24,3/11/2011 18:24,0.7.4,,Legacy/Tools,,0,,,,,,"Converting from bytes to hex makes the columns 4x larger than they should be.  (2x for conversion to hex, then another 2x for converting to UTF-16 which is the default String encoding.)
",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,10/Mar/11 21:35;jbellis;2312.txt;https://issues.apache.org/jira/secure/attachment/12473333/2312.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,09:41.4,,,no_permission,,,,,,,,,,,,20554,,,Fri Mar 11 23:07:02 UTC 2011,,,,,,0|i0gamv:,93153,xedin,xedin,,,,,,,,,10/Mar/11 21:35;jbellis;Patch to leave column values as raw bytes. Also adds comparator metadata for column names.,11/Mar/11 18:09;xedin;+1,11/Mar/11 18:24;jbellis;committed,"11/Mar/11 23:07;hudson;Integrated in Cassandra-0.7 #375 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/375/])
    fix stress.java column sizes
patch by jbellis; reviewed by Pavel Yaskevich for CASSANDRA-2312
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair hangs if one of the neighbor is dead,CASSANDRA-2290,12500799,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,3/8/2011 19:09,3/12/2019 14:03,3/13/2019 22:24,4/19/2011 12:54,0.7.5,0.8 beta 1,,,0,,,,,,"Repair don't cope well with dead/dying neighbors. There is 2 problems:

  # Repair don't check if a node is dead before sending a TreeRequest; this is easily fixable.
  # If a neighbor dies mid-repair, the repair will also hang forever.

The second point is not easy to deal with. The best approach is probably CASSANDRA-1740 however. That is, if we add a way to query the state of a repair, and that this query correctly check all neighbors and also add a way to cancel a repair, this would probably be enough.
",,,3600,3600,,0%,3600,3600,,,,,,,,,,CASSANDRA-1740,,09/Mar/11 13:16;slebresne;0001-Don-t-start-repair-if-a-neighbor-is-dead.patch;https://issues.apache.org/jira/secure/attachment/12473125/0001-Don-t-start-repair-if-a-neighbor-is-dead.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,32:12.6,,,no_permission,,,,,,,,,,,,20543,,,Tue Apr 19 14:04:21 UTC 2011,,,,,,0|i0gahz:,93131,jbellis,jbellis,,,,,,,,,"09/Mar/11 13:16;slebresne;Attaching patch for the first problem above. It checks that all neighbors are alive before attempting the repair. If not, it don't start the repair. Another option would be to still do the repair with whomever neighbor are alive (if any). But I think that refusing to repair is a saner default and I'm fine waiting that someone needs the second option before considering adding it.
","09/Mar/11 18:32;amorton;Not sure if this helps. I found a place where AES was hanging while testing failure during streaming transfer for CASSANDRA-2088 (against 0.7). I broke the FileStresmTask to only send one range and close the sending channel. 

The  IncomingStreamReader.readFile() got stuck in an infinite loop because it does not check the return from FileChannel.transferFrom(). It was returning 0 bytes read. Also the FileStreamTask does not check the bytes sent by transferTo()

While stuck in the loop the socket it was reading from was (127.0.0.1 was in the loop, .0.2 was sending) 
java      25371 aaron   73u  IPv4 0xffffff8010742ff8      0t0  TCP 127.0.0.1:7000->127.0.0.2:52759 (CLOSE_WAIT)

When I was debugging the socketChannel was still reporting it was open. 

Update: Modified FileStresmTask to call System.exit() after sending the first section and got the same result.",18/Apr/11 22:11;jbellis;+1 the check-neighbors patch,18/Apr/11 22:23;slebresne;Committed (to 0.7 and 0.8) the check-neighbor patch. I'm closing this as the rest of this issue is a duplicate CASSANDRA-2433 and could go there.,"19/Apr/11 00:25;jbellis;oops, need to fix AESTest now","19/Apr/11 10:28;hudson;Integrated in Cassandra #856 (See [https://hudson.apache.org/hudson/job/Cassandra/856/])
    ","19/Apr/11 10:39;slebresne;Tests fixed, sorry about that.","19/Apr/11 10:46;hudson;Integrated in Cassandra-0.8 #19 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/19/])
    Fix unit tests for CASSANDRA-2290
","19/Apr/11 14:04;hudson;Integrated in Cassandra-0.7 #447 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/447/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClientOnly mode is creating directories,CASSANDRA-2223,12499415,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,2/22/2011 22:36,3/12/2019 14:03,3/13/2019 22:24,2/23/2011 19:47,0.7.3,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,22/Feb/11 22:42;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-client-only-mode-shouldn-t-create-any-local-data.txt;https://issues.apache.org/jira/secure/attachment/12471666/ASF.LICENSE.NOT.GRANTED--v1-0001-client-only-mode-shouldn-t-create-any-local-data.txt,23/Feb/11 16:45;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0001-ClientOnlyExample-waits-for-schema-to-arrive-tries-to-.txt;https://issues.apache.org/jira/secure/attachment/12471748/ASF.LICENSE.NOT.GRANTED--v2-0001-ClientOnlyExample-waits-for-schema-to-arrive-tries-to-.txt,23/Feb/11 16:45;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0002-avoid-creating-local-files-when-in-fat-client-mode.txt;https://issues.apache.org/jira/secure/attachment/12471749/ASF.LICENSE.NOT.GRANTED--v2-0002-avoid-creating-local-files-when-in-fat-client-mode.txt,23/Feb/11 16:45;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0003-make-sure-fat-clients-gossip-their-schema.txt;https://issues.apache.org/jira/secure/attachment/12471750/ASF.LICENSE.NOT.GRANTED--v2-0003-make-sure-fat-clients-gossip-their-schema.txt,23/Feb/11 16:45;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0004-update-contrib_client-only-meta.txt;https://issues.apache.org/jira/secure/attachment/12471751/ASF.LICENSE.NOT.GRANTED--v2-0004-update-contrib_client-only-meta.txt,,,,,,,,,5,,,,,,,,,,,,,,,,,,,53:56.3,,,no_permission,,,,,,,,,,,,19346,,,Wed Feb 23 20:11:13 UTC 2011,,,,,,0|i0ga33:,93064,tjake,tjake,,,,,,,,,"23/Feb/11 15:53;tjake;The client_only example no longer works with this patch:

jake@Jake-Lucianis-MacBook-Pro(client_only)$ ./bin/client_only write
11/02/23 10:53:01 INFO config.DatabaseDescriptor: Loading settings from jar:file:/Users/jake/workspace/cassandra-git/contrib/client_only/build/client_only.jar!/cassandra.yaml
11/02/23 10:53:01 INFO config.DatabaseDescriptor: DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
11/02/23 10:53:01 INFO service.StorageService: Starting up client gossip
11/02/23 10:53:01 INFO gms.Gossiper: Node /127.0.0.1 has restarted, now UP again
11/02/23 10:53:02 INFO gms.Gossiper: InetAddress /127.0.0.1 is now dead.
11/02/23 10:53:02 INFO gms.Gossiper: InetAddress /127.0.0.1 is now UP
Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:66)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:61)
        at org.apache.cassandra.db.RowMutation.add(RowMutation.java:144)
        at org.apache.cassandra.db.RowMutation.add(RowMutation.java:152)
        at ClientOnlyExample.testWriting(ClientOnlyExample.java:68)
        at ClientOnlyExample.main(ClientOnlyExample.java:127)","23/Feb/11 17:07;tjake;v2 works great.  verified no dirs are created. +1

Only thing I've been seeing with client mode in general is this on startup (randomly)

11/02/23 12:03:46 INFO service.StorageService: Starting up client gossip
Exception in thread ""Thread-2"" java.io.IOError: java.io.EOFException
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:73)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:61)

Seems to cause no issues though...",23/Feb/11 19:47;gdusbabek;committed!,"23/Feb/11 20:11;hudson;Integrated in Cassandra-0.7 #312 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/312/])
    fat clients were creating local data. patch by gdusbabek, reviewed by tjake. CASSANDRA-2223
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column family deletion time is not always reseted after gc_grace,CASSANDRA-2317,12501241,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,3/12/2011 11:19,3/12/2019 14:03,3/13/2019 22:24,7/4/2011 14:36,1.0.0,,,,0,,,,,,"Follow up of CASSANDRA-2305.
Reproducible (thanks to Jeffrey Wang) by: 

Create a CF with gc_grace_seconds = 0 and no row cache.
Insert row X, col A with timestamp 0.
Insert row X, col B with timestamp 2.
Remove row X with timestamp 1 (expect col A to disappear, col B to stay).
Wait 1 second.
Force flush and compaction.
Insert row X, col A with timestamp 0.
Read row X, col A (see nothing).",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,14/Mar/11 13:47;slebresne;0001-Add-AbstractColumnContainer-to-factor-common-parts-o.patch;https://issues.apache.org/jira/secure/attachment/12473561/0001-Add-AbstractColumnContainer-to-factor-common-parts-o.patch,14/Mar/11 13:47;slebresne;0002-Add-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12473562/0002-Add-unit-test.patch,14/Mar/11 13:47;slebresne;0003-Reset-CF-and-SC-deletion-time-after-compaction.patch;https://issues.apache.org/jira/secure/attachment/12473563/0003-Reset-CF-and-SC-deletion-time-after-compaction.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,12:22.5,,,no_permission,,,,,,,,,,,,20558,,,Mon Jul 04 15:20:32 UTC 2011,,,,,,0|i0ganz:,93158,jbellis,jbellis,,,,,,,,,"12/Mar/11 11:20;slebresne;Adding patch against 0.7.

First patch is a unit test to reproduce the failure, patch 2 is the fix.",12/Mar/11 11:54;slebresne;I realize this fix don't take the cache into account. I'll attach an updated patch asap.,12/Mar/11 13:12;jbellis;How do you have CF objects around at all post-purge?,"14/Mar/11 13:47;slebresne;bq. How do you have CF objects around at all post-purge?

The problem is actually with cf objects that don't get fully purged. Those still retain their markedForDeleteAt and localDeletionTime after compaction even though it could be way past gc_grace. That is, a deletion can easily live way past gc_grace + compaction.

As it turns out, super columns also suffers for the same problem.

Because it felt a bit annoying to have to fix the problem in 2 places, and because that's not the first time that happens, the first attached is a refactoring one, that introduces an AbstractColumnContainer class that factor common code to ColumnFamily and SuperColumn.

The second patch introduces unit tests for column family and super columns and third patch is the fix. It introduces a new structure to hold both markedDeletedAt and localDeletionTime so that we are able to set both of those together atomically. This is necessary for the second part of CASSANDRA-2305.  I think that anyhow it was not fully correct to update them non atomically.

Note that the third patch depends on the patch for CASSANDRA-2279.

All patches are against 0.7.
","14/Mar/11 14:27;jbellis;Oh, so the problem is that we're not actually losing information (when a CF contains non-deleted data) that we may lose otherwise?

Is that really worth adding a bunch of complexity to ""fix?""","14/Mar/11 15:01;slebresne;This is clearly not of big importance.

I however think that we should fix this for coherence sake. Tombstones have side effects on client queries and tombstone collection too (it's not just an internal detail). As such, we should make the behavior as coherent as possible. That a deletion always has effect until the first compaction after gc_grace would be more coherent that the current status quo. The fact that Jeffrey was surprised and that we assume right away that it was a bug proves that, I think.

And the fix is actually fairly simple, even though my patch doesn't do it justice. The first patch is really just a refactoring that I think should have been done a long time ago. I'm happy to create a ticket for this specifically if we prefer, but I just think it is stupid to not factor that code.

The second part of the patch is to put markedDeletedAt and localDeletionTime in a common structure that we CAS for changes. Again, this is because right now I think we have a race condition when updating those two values. That is, we could end up with the markedDeleteAt of a given operation but the localDeletionTime of another one. This could also be put on another ticket (I just tend to be lazy so I've put everything here, sorry).

Once those are done, the fix for this specific ticket is really just the maybeResetDeletionTime() function.","13/Jun/11 19:16;jbellis;doesn't this mean that for a CF w/ no tombstone, we create a new deletioninfo every call to maybeReset?

bq. if (current.localDeletionTime > gcBefore || deletionInfo.compareAndSet(current, new DeletionInfo()))

otherwise, +1 for trunk.","22/Jun/11 22:33;paladin8;Does anyone know whether this is fixed in 0.8? We are thinking of upgrading soon, but I don't want to try to apply the 0.7 patch to 0.8...","22/Jun/11 23:19;jbellis;""Unresolved"" means not fixed anywhere yet.","04/Jul/11 14:37;slebresne;Committed to trunk (as I agree this should really go there).

bq. doesn't this mean that for a CF w/ no tombstone, we create a new deletioninfo every call to maybeReset?

You're right, I've included a current.localDeletionTime == Integer.MIN_VALUE in the condition to escape early in that case.","04/Jul/11 15:20;hudson;Integrated in Cassandra #948 (See [https://builds.apache.org/job/Cassandra/948/])
    Reset CF and SC deletion time after gc_grace
patch by slebresne; reviewed by jbellis for CASSANDRA-2317

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1142690
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamily.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/filter/QueryFilter.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowMutation.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/SuperColumn.java
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/test/unit/org/apache/cassandra/service/RowResolverTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/RowTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/IColumnContainer.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/compaction/CompactionsPurgeTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/AbstractColumnContainer.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilySerializer.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DecoratedKey equals() only tests Token,CASSANDRA-1720,12479458,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,11/9/2010 4:23,3/12/2019 14:03,3/13/2019 22:24,11/11/2010 2:52,0.7.0 rc 1,,,,0,,,,,,"I'm working on a new Partitioner for Lucandra that lets many keys share the same token.

When I use this partitioner SliceQueryFilter class returns all rows that match key with the same Token.  This isn't correct in my mind. 
Tokens should only be used to route a Key in the ring.  DecoratedKey equals() hashCode() and compare() should consider Token *and* Key

Thoughts?",,,,,,,,,,,,,,,,,,,,09/Nov/10 16:05;stuhood;1034_v1.txt;https://issues.apache.org/jira/secure/attachment/12459163/1034_v1.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,48:32.2,,,no_permission,,,,,,,,,,,,20271,,,Mon Feb 21 16:02:37 UTC 2011,,,,,,0|i0g6z3:,92560,stuhood,stuhood,,,,,,,,,09/Nov/10 04:48;jbellis;let's move this to CASSANDRA-1034,09/Nov/10 16:04;stuhood;+1 on the patch originally from 1034,11/Nov/10 02:52;jbellis;committed; see CASSANDRA-1034 for further comments,21/Feb/11 16:02;jbellis;(reverted b/c of CASSANDRA-1733),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading an empty commit log throw an exception,CASSANDRA-2285,12500768,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,3/8/2011 14:47,3/12/2019 14:03,3/13/2019 22:24,3/9/2011 20:52,0.7.4,,,,0,,,,,,"Start a one node cluster, shutdown within 10 seconds but after the node is started and the location infos has been flushed. Restart node, you'll get a 'EOFException: unable to seek past the end of the file in read-only mode.'",,,7200,7200,,0%,7200,7200,,,,,,,,,,,,09/Mar/11 12:49;slebresne;0001-skip-CL-recover-when-fully-data-was-fully-flushed-wi.patch;https://issues.apache.org/jira/secure/attachment/12473123/0001-skip-CL-recover-when-fully-data-was-fully-flushed-wi.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,11:57.1,,,no_permission,,,,,,,,,,,,20540,,,Wed Mar 09 20:52:47 UTC 2011,,,,,,0|i0gagv:,93126,jbellis,jbellis,,,,,,,,,08/Mar/11 14:48;slebresne;Attached patch against 0.7,"08/Mar/11 16:11;jbellis;I can't reproduce, probably because Cassandra always does a few writes to system tables post-startup.  Can you make a failing test?","08/Mar/11 16:22;slebresne;Yeah, when I say stop, that may involve killing the node mid-start.","09/Mar/11 12:49;slebresne;
I had of wrong understanding of what was going on. The problem is that we can have a commit log header with a replay position greater than the size of the commit log.

This happens if some data gets flushed before it had time to hit the actual log (thus only in periodic mode). Which in turn can happen because we use a FileOutputStream for the header, which will get sync to disk even if cassandra dies/is killed shortly afterwards (unless this is a system failure).

It's fairly unlikely to happen in real use, but it is fairly easy to reproduce (see description).

Attaching a patch using the correct condition as well as a test unit.
",09/Mar/11 20:52;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"login() request via Thrift/PHP fails with ""Unexpected authentication problem"" in cassandra log / ""Internal error processing login"" in Thrift",CASSANDRA-935,12460737,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,rschildmeijer,redsolar,redsolar,3/30/2010 20:22,3/12/2019 14:02,3/13/2019 22:24,4/3/2010 21:59,0.7 beta 1,,,,0,,,,,,"When issuing a login request via PHP Thrift with the following parameters:

$auth_request = new cassandra_AuthenticationRequest;
$auth_request->credentials = array (
    ""username"" => ""jsmith"",
     ""password"" => ""havebadpass"",
);
$client->login(""Keyspace1"", $auth_request);

I get an exception, with the following details

PHP Exception:
PHP Fatal error:  Uncaught exception 'TApplicationException' with message 'Internal error processing login' in /home/redsolar/html/includes/thrift/packages/cassandra/Cassandra.php:73

Cassandra log:

ERROR 13:00:53,823 Internal error processing login
java.lang.RuntimeException: Unexpected authentication problem
        at org.apache.cassandra.auth.SimpleAuthenticator.login(SimpleAuthenticator.java:113)
        at org.apache.cassandra.thrift.CassandraServer.login(CassandraServer.java:651)
        at org.apache.cassandra.thrift.Cassandra$Processor$login.process(Cassandra.java:1147)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1125)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.NullPointerException
        at java.io.FileInputStream.<init>(FileInputStream.java:133)
        at java.io.FileInputStream.<init>(FileInputStream.java:96)
        at org.apache.cassandra.auth.SimpleAuthenticator.login(SimpleAuthenticator.java:82)
        ... 7 more

File contents (all chmod 777 for testing):

""conf/access.properties""
Keyspace1=jsmith,Elvis Presley,dilbert

""conf/password.properties""
jsmith=havebadpass
Elvis\ Presley=graceland4evar
dilbert=nomoovertime","PHP 5.3, Cassandra 0.6.0-rc1 (as in current vote), CentOS 5.4 x64, 4-node cluster",,,,,,,,,,,,,,,,,,,31/Mar/10 19:13;tzz;CASSANDRA-935-check-properties.patch;https://issues.apache.org/jira/secure/attachment/12440379/CASSANDRA-935-check-properties.patch,31/Mar/10 18:56;tzz;CASSANDRA-935-check-properties.patch;https://issues.apache.org/jira/secure/attachment/12440378/CASSANDRA-935-check-properties.patch,31/Mar/10 18:07;rschildmeijer;CASSANDRA-935-v2.patch;https://issues.apache.org/jira/secure/attachment/12440370/CASSANDRA-935-v2.patch,03/Apr/10 07:04;rschildmeijer;CASSANDRA-935-v3.patch;https://issues.apache.org/jira/secure/attachment/12440672/CASSANDRA-935-v3.patch,30/Mar/10 20:52;rschildmeijer;CASSANDRA-935.patch;https://issues.apache.org/jira/secure/attachment/12440275/CASSANDRA-935.patch,,,,,,,,,5,,,,,,,,,,,,,,,,,,,24:22.1,,,no_permission,,,,,,,,,,,,19925,,,Tue Apr 06 13:10:16 UTC 2010,,,,,,0|i0g1zj:,91752,,,,,,,,,,,"30/Mar/10 20:24;jbellis;are you setting -DPASSWD_FILENAME_PROPERTY=path/to/access.properties ?  because that is what SimpleAuthenticator looks for.  (it should probably just try to load from classpath, that seems more java-ish.)","30/Mar/10 20:29;rschildmeijer;In fact the property keys are called passwd.properties and access.properties. 
    -Dpasswd.properties=<PATH>/passwd.properties
    -Daccess.properties=<PATH>access.properties

(The PASSWD_FILENAME_PROPERTY is just the name of the static constant)

","30/Mar/10 20:39;redsolar;Running cassandra with ""bin/cassandra -f -Dpasswd.properties=conf/passwd.properties -Daccess.properties=conf/access.properties"" solved the issue
","30/Mar/10 20:51;rschildmeijer;We should probably add a check that everything was properly defined and ""bail out"" otherwise",30/Mar/10 20:52;rschildmeijer;Verifies that access.properties and passwd.properties are defined if SimpleAuthenticator is used (in storage-conf.xml),"31/Mar/10 14:30;urandom;I think I'm -1 on this patch Roger. Those authenticators are pluggable, I'd rather that we didn't have implementation details leaking out of them.","31/Mar/10 14:49;rschildmeijer;I agree. 

I was considering a solution that augmented the IAuthenticator interface with methods like isAuthenticationRequired(), getAccessFileName(), getPasswordFileName, but that felt like polluting the interface with implementation specific details.

What speaks for the submitted solution (atleast to some extent) is that we are handling faulty configuration more correctly when using the built in/shipped authentication (SimpleAuthenticator).   ",31/Mar/10 14:57;urandom;I think the correct solution is just to make SimpleAuthenticator produce a clearer error message when it's been misconfigured.,31/Mar/10 15:08;jbellis;Could we add a validateConfiguration method to IAuthenticator?,31/Mar/10 15:28;urandom;works for me.,"31/Mar/10 18:56;tzz;What should validateConfiguration() do that login() should not do as well?

I'd rather add the necessary checks to login().  The performance penalty is negligible.  See attached patch.","31/Mar/10 19:01;jbellis;> What should validateConfiguration() do that login() should not do as well? 

fail the startup rather than breaking things when you try to query, which as we've seen here is the wrong approach.",31/Mar/10 19:13;tzz;How about checking in the constructor?  Attaching patch to do it this way.,"31/Mar/10 20:08;rschildmeijer;I totaly agree that we should do the validation as early as possible (prefer startup check instead of ""on query check"").
I think the IAuthenticator API is easier to get a correct implementation for using the two method version (see CASSANDRA-935-v2.patch) instead of relying on that implementors throws an exception in the ctor.","31/Mar/10 20:25;tzz;Either validateConfiguration() or in the constructor works.  Assuming your patch goes in, I would distinguish between the two kinds of configuration exception for the two possible missing items or show the actual values in the exception message, e.g.

String.format(""When using %s, properties %s (currently %s) and %s (currently %s) must be defined, ...)
","02/Apr/10 22:22;urandom;I think I prefer the validateConfiguration() approach.

Roger, can you use o.a.c.config.DatabaseDescriptor.ConfigurationException instead of javax.naming.ConfigurationException? I'm also having some trouble applying your patch to trunk/ (maybe it just needs to be rebased).
",03/Apr/10 07:04;rschildmeijer;Patch rebased (v3),03/Apr/10 21:59;urandom;committed; thanks Roger!,"06/Apr/10 13:10;hudson;Integrated in Cassandra #399 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/399/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
separate assignment of current keyspace from login(),CASSANDRA-1022,12463043,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,todd,urandom,urandom,4/26/2010 18:35,3/12/2019 14:02,3/13/2019 22:24,5/4/2010 16:01,0.7 beta 1,,,,0,,,,,,"With the completion of CASSANDRA-714, it is now a requirement that login() be called, even when using the AllowAllAuthenticator (effectively disabling auth), since this is how the current/connected keyspace is set. These two disparate functions (assigning keyspace and authentication) should be disentangled.

I propose that the keyspace argument be removed from calls to {{login()}}, and that a new method ({{use_keyspace(string)}}?), be added.
",,,,,,,,,,,,,,,,,,,,30/Apr/10 10:16;todd;CASSANDRA-1022.patch;https://issues.apache.org/jira/secure/attachment/12443277/CASSANDRA-1022.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,45:12.5,,,no_permission,,,,,,,,,,,,19961,,,Sat Jul 03 12:48:09 UTC 2010,,,,,,0|i0g2iv:,91839,,,,,,,,,,,26/Apr/10 18:45;jbellis;mild preference for set_keyspace here,"26/Apr/10 20:42;todd;Since this relates closely with CASSANDRA-714, I'll spend the time logically separating these.",30/Apr/10 10:16;todd;Patch attached.,"02/May/10 00:13;urandom;This has been committed with a few changes:

* Fixed some style nits, (braches on newlines, etc). See http://wiki.apache.org/cassandra/CodeStyle.
* Reinstated debug log statement in CassandraServer.login()
* Reinstated css_.debug test when printing stacktrace in the CLI
* Removed unused _login() function from system tests
* Reworded a few error messages.
* Updated contrib/utils/service/CassandraServiceTest for login() change
* Disabled system_add_keyspace() when authentication is enabled (login() requires a keyspace).
* Reset access level (forcing a new login) when keyspace is changed using set_keyspace().

Thanks Todd!
","04/May/10 14:53;tzz;Eric, please remember http://thread.gmane.org/gmane.comp.db.cassandra.user/1038/focus=1404 where you asked me to merge login() with setKeyspace() and now they are pulled apart again.  What has changed?  Shouldn't we be insisting on the ""one keyspace per connection"" concept?  Or has the philosophy changed?

If set_keyspace() is to be used, it should be the one returning an AccessLevel.  login() should return void.  In other words, the AccessLevel is per-keyspace, not per-user.

Also, login() should no longer throw AuthorizationException since it doesn't do authorization, while set_keyspace() should throw it.

Please let me know if I need to implement these changes or if someone else will do it or if there are any questions.

Thanks!","04/May/10 16:01;urandom;{quote}
Eric, please remember http://thread.gmane.org/gmane.comp.db.cassandra.user/1038/focus=1404 where you asked me to merge login() with setKeyspace() and now they are pulled apart again. What has changed? Shouldn't we be insisting on the ""one keyspace per connection"" concept? Or has the philosophy changed?
If set_keyspace() is to be used, it should be the one returning an AccessLevel. login() should return void. In other words, the AccessLevel is per-keyspace, not per-user.
{quote}

I remember; what has changed is (anecdotal) experience with how people are (or more importantly are not) using this, and how well the interface is holding up over time (read: it's not).

Personally,  I am  stronger in my convictions now that we should _not_ be rolling our own AAA, that this is not The Way, and I am seeking to make this (still experimental) API as optional as possible while working toward something better, (AVRO-341).

{quote}
Also, login() should no longer throw AuthorizationException since it doesn't do authorization, while set_keyspace() should throw it.
{quote}

{{set_keyspace()}} is the required call now, {{login()}} works exactly as before only it uses the keyspace specified in {{set_keyspace()}} instead of having one passed in (and {{set_keyspace()}} invalidates any previous {{login()}}).

This was the whole point of this issue, to divorce the function of assigning a keyspace from authentication.

{quote}
Please let me know if I need to implement these changes or if someone else will do it or if there are any questions.
{quote}

I don't see any changes that need to be implemented here.",04/May/10 16:29;tzz;Thanks for explaining.  I didn't understand your motivation earlier.,02/Jul/10 20:31;messi;Commit #940127 introduced a bug in CassandraServer (line 889 in trunk): if statement is empty because of semicolon.,02/Jul/10 21:54;jbellis;removed the semicolon.  thanks!,"03/Jul/10 12:48;hudson;Integrated in Cassandra #484 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/484/])
    r/m errant semicolon.  patch by Folke Behrens; reviewed by jbellis for CASSANDRA-1022
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during QuorumResponseHandler,CASSANDRA-864,12458566,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,btoddb,btoddb,3/9/2010 18:20,3/12/2019 14:02,3/13/2019 22:24,3/10/2010 17:18,0.6,,,,0,,,,,,"using cassandra-0.6.0-beta2/


2010-03-09 09:17:26,827 ERROR [pool-1-thread-675] [Cassandra.java:1166] Internal error processing get
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:68)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:470)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:401)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:101)
        at org.apache.cassandra.thrift.CassandraServer.multigetInternal(CassandraServer.java:309)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:274)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:1156)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1114)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619) ",,,,,,,,,,,,,,,,,,,,10/Mar/10 02:02;jbellis;864-v2.txt;https://issues.apache.org/jira/secure/attachment/12438352/864-v2.txt,09/Mar/10 22:55;jbellis;864.txt;https://issues.apache.org/jira/secure/attachment/12438339/864.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,55:00.0,,,no_permission,,,,,,,,,,,,19894,,,Wed Mar 10 17:18:47 UTC 2010,,,,,,0|i0g1jr:,91681,,,,,,,,,,,"09/Mar/10 18:54;jbellis;What's happening is, we're relying on the synchronization done by the caller of response() to make us thread-safe.  And this works fine _if_ responses arrive before the timeout; then the assumption that responses are never added after the wait in get completes holds true (since no responses may be added after the signal is fired).

But if the request times out, then the signal has not actually been fired, and a response may arrive during iteration of the messages and cause a CME, as seen here.","09/Mar/10 20:18;jbellis;Fix for IAsyncCallback concurrency problems.

I've also removed a bunch of ""if (condition.isSignaled()) return;"" statements that were premature optimizations at best and possibly counterproductive to performance.  Remember that after a successful signal, we remove the callback entry in the finally {} block, so this was only useful for callbacks that arrived (a) after the signal but (b) before the callback was removed.",09/Mar/10 21:02;rschildmeijer;+1,"09/Mar/10 22:55;jbellis;Heh, I went to commit and couldn't get the patch to apply to 0.6.  Then I realized I'd only submitted part of the patch (forgot to squash in my local git repo).

Full patch attached.",10/Mar/10 02:02;jbellis;fix IllegalStateException: Queue full,10/Mar/10 07:06;rschildmeijer;Reviewed once more. Still +1 :),10/Mar/10 13:54;gdusbabek;Was changing the for-loop to 'if (iter.hasNext())' in QRH.get() intentional?  It seems like a while-loop is required to keep the same semantics as the old code.,"10/Mar/10 13:58;jbellis;I believe the responses all have the same messageid; the for loop was a clunkier way of expressing ""remove the handler, but only if we actually got a response with which to grab the id""",10/Mar/10 14:27;gdusbabek;+1,"10/Mar/10 16:55;jbellis;actually ""responses all have the same messageid"" is only true for QRH, not WRH.  I will change both of them back to the way they were before.",10/Mar/10 17:18;jbellis;committed w/ that change,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in StorageService when cluster is first being created,CASSANDRA-1639,12477942,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,davew,davew,10/21/2010 0:40,3/12/2019 14:02,3/13/2019 22:24,10/22/2010 14:52,0.7 beta 3,,Legacy/Tools,,0,,,,,,"Saw this exception on the 0.7.0-beta2 version of cassandra right after bringing up a cluster and trying to get the number of live nodes.

java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService.stringify(StorageService.java:1151)
        at org.apache.cassandra.service.StorageService.getLiveNodes(StorageService.java:1138)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)


I fixed this by adding some null checks to the stringify methods 

    private Set<String> stringify(Collection<InetAddress> endpoints)
    {
        Set<String> stringEndpoints = new HashSet<String>();
        for (InetAddress ep : endpoints)
        {
        	if(ep != null) {
        		stringEndpoints.add(ep.getHostAddress());
        	}
        }
        return stringEndpoints;
    }

    private List<String> stringify(List<InetAddress> endpoints)
    {
        List<String> stringEndpoints = new ArrayList<String>();
        for (InetAddress ep : endpoints)
        {
        	if(ep != null) {
        		stringEndpoints.add(ep.getHostAddress());
        	}
        }
        return stringEndpoints;
    }

After adding those checks, then I got more reasonable/realistic errors from a different part of the code since the service wasn't up yet as the cluster was still initializing:

Caused by: javax.management.InstanceNotFoundException: org.apache.cassandra.service:type=StorageService
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:662)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
        at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:255)
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:233)
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:142)
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:878)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:263)
",Windows XP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,52:30.1,,,no_permission,,,,,,,,,,,,20230,,,Fri Oct 22 14:52:30 UTC 2010,,,,,,0|i0g6gv:,92478,,,,,,,,,,,22/Oct/10 14:52;jbellis;looks like this is fixed in the latest nightly -- should not be possible for getLiveNodes to pass a null list to stringify.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add then drop Keyspace without putting anything in it causes exception,CASSANDRA-1378,12471309,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,jjordan,jjordan,8/11/2010 15:33,3/12/2019 14:02,3/13/2019 22:24,8/13/2010 18:01,0.7 beta 2,,,,0,,,,,,"The following from python causes an exception on apache-cassandra-2010-08-10_13-08-19-bin.tar.gz and a bunch of earlier builds in the 0.7 line:
        socket = TSocket.TSocket(host, 9160)
        transport = TTransport.TFramedTransport(socket)
        protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
        client = Cassandra.Client(protocol)
        transport.open()
        try:
            client.describe_keyspace(dbName)
        except NotFoundException, e:
            keyspaceDef = KsDef(name=dbName,
 
strategy_class='org.apache.cassandra.locator.RackUnawareStrategy',
                                replication_factor=replicationFactor,
                                cf_defs=[])
            client.set_keyspace('system')
            client.system_add_keyspace(keyspaceDef)

        try:
            client.describe_keyspace(dbName)
            client.set_keyspace('system')
            client.system_drop_keyspace(dbName)
        except NotFoundException, e:
            pass

The system_drop_keyspace throws:
InvalidRequestException(why='java.util.concurrent.ExecutionException:
java.lang.NullPointerException')

If I put a system_add_column_family in the middle it doesn't crash.
I think this broke sometime after apache-cassandra-2010-07-06_13-27-21",Single node.  Both Linux and Windows.,,,,,,,,,,,,,,,,,,,12/Aug/10 14:33;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-handle-graveyard-cleanups-gracefully-when-there-is-not.txt;https://issues.apache.org/jira/secure/attachment/12451903/ASF.LICENSE.NOT.GRANTED--v1-0001-handle-graveyard-cleanups-gracefully-when-there-is-not.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,11:42.1,,,no_permission,,,,,,,,,,,,20111,,,Sat Aug 14 12:48:36 UTC 2010,,,,,,0|i0g4ov:,92190,,,,,,,,,,,"12/Aug/10 14:11;gdusbabek;full error:


ERROR [CompactionExecutor:1] 2010-08-12 08:59:31,088 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[CompactionExecutor:1,5,main]
java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
	at org.apache.cassandra.db.CompactionManager$CompactionExecutor.afterExecute(CompactionManager.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:90)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	... 2 more
ERROR [MIGRATION-STAGE:1] 2010-08-12 08:59:31,089 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[MIGRATION-STAGE:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at org.apache.cassandra.db.migration.Migration.cleanupDeadFiles(Migration.java:246)
	at org.apache.cassandra.db.migration.DropKeyspace.applyModels(DropKeyspace.java:86)
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
	at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:722)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	... 2 more
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.migration.Migration.cleanupDeadFiles(Migration.java:238)
	... 8 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:90)
	... 5 more
",13/Aug/10 16:44;jbellis;+1,"14/Aug/10 12:48;hudson;Integrated in Cassandra #514 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/514/])
    handle graveyard cleanups gracefully when there is nothing to delete. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1378
trap ConfigExceptions so they don't become RTEs. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1378
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra-cli doesn't use framed transport by default,CASSANDRA-1290,12469491,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,7/16/2010 20:54,3/12/2019 14:02,3/13/2019 22:24,7/16/2010 21:56,0.7 beta 1,,,,0,,,,,,"Spawned by CASSANDRA-475 .
The cli uses non-framed transport, which causes errors on connection that look like:

cli:{noformat}$ bin/cassandra-cli 
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
[default@unknown] connect 127.0.0.1/9160
Exception retrieving information about the cassandra node, check you have connected to the thrift port.{noformat}

cass:{noformat}ERROR 15:48:12,523 Thrift error occurred during processing of message.
org.apache.thrift.TException: Message length exceeded: 1684370275
	at org.apache.thrift.protocol.TBinaryProtocol.checkReadLength(TBinaryProtocol.java:384)
	at org.apache.thrift.protocol.TBinaryProtocol.readStringBody(TBinaryProtocol.java:350)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:213)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2519)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619){noformat}

Changing it to use framed transport fixes this, and it should be using framed transport by default regardless.",,,,,,,,,,,,,,,,,,,,16/Jul/10 21:42;jhermes;TRUNK-1290.txt;https://issues.apache.org/jira/secure/attachment/12449708/TRUNK-1290.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,57:56.8,,,no_permission,,,,,,,,,,,,20061,,,Mon Jul 19 14:45:04 UTC 2010,,,,,,0|i0g45r:,92104,,,,,,,,,,,"16/Jul/10 20:57;jbellis;there is a --framed option

should be switched to --unframed",16/Jul/10 20:58;jhermes;This is a one-line (line 32 to be exact) patch with some minor spatial formatting.,"16/Jul/10 21:42;jhermes;Yeah, good idea.
Changed the arg to unframed.",16/Jul/10 21:56;jbellis;committed,"19/Jul/10 14:45;hudson;Integrated in Cassandra #494 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/494/])
    make cli framed by default.  patch by Jon Hermes; reviewed by jbellis for CASSANDRA-1290
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bootstrapping does not work properly using multiple key space,CASSANDRA-673,12444764,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jaakko,steel_mental,steel_mental,1/6/2010 3:14,3/12/2019 14:02,3/13/2019 22:24,1/18/2010 18:35,0.5,,,,0,,,,,,"when use multiple key-spaces, and one key-space has no SSTable, then bootstrap may not work right.
Say nodes A, B, C, D have key spaces ""KS1"" and ""KS2"", KS1 is empty, now add empty node E into cluster.
Suppose E decide to drag data from A and B(bootstrap source), E will send range to A and B, and A, B will scan all key-spaces they got and send ack to E, which contains list of key-space name(StreamContextManager.StreamContext),
when E get ack from A and B, it scan this list, but when encounter first empty key-space, it will stop and remove node from bootstrap sources list:

StreamInitiateVerbHandler.doVerb:
......
                if (streamContexts.length == 0 && StorageService.instance().isBootstrapMode())
                {
                    if (logger.isDebugEnabled())
                        logger.debug(""no data needed from "" + message.getFrom());
                    StorageService.instance().removeBootstrapSource(message.getFrom());
                    return;
                }
......
If list of bootstrap sources is empty, E will finish bootstrapping

So, the result is: E get nothing from source A, B, even KS2 has lots of data.
","1.	cluster has at least one empty key-space
2.	one node be added into cluster to do bootstrap
",,,,,,,,,,,,,,,,,,,14/Jan/10 20:21;gdusbabek;0001-keep-track-of-table-and-node-while-bootstrapping.patch;https://issues.apache.org/jira/secure/attachment/12430290/0001-keep-track-of-table-and-node-while-bootstrapping.patch,15/Jan/10 14:05;gdusbabek;0002-do-not-include-system-tables-when-setting-bootstrap-.patch;https://issues.apache.org/jira/secure/attachment/12430402/0002-do-not-include-system-tables-when-setting-bootstrap-.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,21:12.5,,,no_permission,,,,,,,,,,,,19813,,,Mon Jan 18 18:35:15 UTC 2010,,,,,,0|i0g0dr:,91492,,,,,,,,,,,14/Jan/10 20:21;gdusbabek;Tracks bootstrap sources on a node+table basis. Puts the table name as a message header for the case when there are no stream contexts to extract it from.,14/Jan/10 20:21;gdusbabek;I decided I'd like to patch this one up to make rebasing whatever fix into my 620 changes less of a headache.,"15/Jan/10 13:25;jaakko;System table should probably not be included in the transfer?
",15/Jan/10 13:31;gdusbabek;Good catch.  I'll put together another patch.,16/Jan/10 07:52;jaakko;+1,"18/Jan/10 18:35;gdusbabek;r900469 (0.5)
r900499 (trunk)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FatClient removal causes ConcurrentModificationException,CASSANDRA-757,12455173,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,2/3/2010 13:33,3/12/2019 14:02,3/13/2019 22:24,4/27/2010 18:36,0.7 beta 1,,,,0,,,,,,"After using a fatclient and killing it, I later receive this ST on all nodes:

 INFO 16:04:58,999 FatClient /10.242.4.13 has been silent for 3600000ms, removing from gossip
ERROR 16:04:58,999 Fatal exception in thread Thread[Timer-1,5,main]
java.lang.RuntimeException: java.util.ConcurrentModificationException
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:96)
        at java.util.TimerThread.mainLoop(Timer.java:534)
        at java.util.TimerThread.run(Timer.java:484)
Caused by: java.util.ConcurrentModificationException
        at java.util.Hashtable$Enumerator.next(Hashtable.java:1048)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:382)
        at org.apache.cassandra.gms.Gossiper$GossipTimerTask.run(Gossiper.java:90)
        ... 2 more
","debian lenny amd64 OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",,,,,,,,,,,,,,,,,,,22/Apr/10 19:52;brandon.williams;0001_use_concurrent_structures.txt;https://issues.apache.org/jira/secure/attachment/12442616/0001_use_concurrent_structures.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,20:09.5,,,no_permission,,,,,,,,,,,,19852,,,Fri Jul 16 20:40:55 UTC 2010,,,,,,0|i0g0vz:,91574,,,,,,,,,,,04/Feb/10 19:20;jbellis;The good news is this looks like a pretty harmless heisenbug -- eventually (usually the next timer iteration) no changes will be made to the map during iteration and it will work.,08/Mar/10 22:47;brandon.williams;Patch to avoid concurrent modification.,"08/Mar/10 22:53;jbellis;were you able to reproduce before, and not w/ this patch?  because AFAIK using an iterator manually instead of a foreach only helps when the CME is from modifying the collection in the same loop, and you can replace that w/ iter.remove().  otherwise if the CME is happening from another thread modifying stuff you have to move to a Concurrent collection.","08/Mar/10 23:19;jbellis;i think what needs to happen is take all the structures in Gossiper and make them Concurrent equivalents, encapsulate any places where we're returning them to other objects directly, and audit the rest for correctness, because the original author basically ignored threadsafety entirely",19/Apr/10 21:25;brandon.williams;Patch to use concurrent structures and remove synchronization  in the Gossiper.,"22/Apr/10 15:08;jbellis;Jaakko hasn't replied so I'll take a stab at reviewing.

Can you rebase?  2 hunks are failing in Gossiper for me.",22/Apr/10 19:52;brandon.williams;Updated w/rebased patch.,"27/Apr/10 18:36;jbellis;committed.  also inlined the comparator, replaced the last synchronized methods in EndpointState w/ volatile fields, and removed getSortedApplicationStates.","16/Jul/10 20:35;wadey;This is a more serious bug than originally thought due to CASSANDRA-1289. When this exception gets thrown, it causes GossipTimerTask to stop running until the server is restarted. Because of this, I would recommend a backport to 0.6.x (I'll offer to do the backport as well). ","16/Jul/10 20:40;jbellis;backporting this to 0.6 doesn't work for me, but a hack to log exceptions would be ok.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
making cassandra-cli friendlier to scripts,CASSANDRA-1340,12470478,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,7/29/2010 22:43,3/12/2019 14:02,3/13/2019 22:24,7/30/2010 21:07,0.7 beta 1,,Legacy/Tools,,0,,,,,,"It is currently possible (and useful) to execute bulk commands with cassandra-cli using shell redirection. However, there is  no mechanism for handling errors, and the same output seen in an interactive session is echoed to the terminal.

The patch that follows accepts a new argument, (--batch), which:
* disables initialization of the history file
* suppresses output (stdout only)
* exits on error with status 2 for invalid syntax, 4 for invalid requests, and 8 for everything else.",,,,,,,,,,,,,,,,,,,,29/Jul/10 22:44;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1340.-cassandra-cli-batch-processing-mode.txt;https://issues.apache.org/jira/secure/attachment/12450870/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1340.-cassandra-cli-batch-processing-mode.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,37:28.8,,,no_permission,,,,,,,,,,,,20088,,,Sat Jul 31 12:51:02 UTC 2010,,,,,,0|i0g4gf:,92152,,,,,,,,,,,29/Jul/10 23:37;jbellis;+1,30/Jul/10 21:07;urandom;committed.,"31/Jul/10 12:51;hudson;Integrated in Cassandra #505 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/505/])
    CASSANDRA-1340. cassandra-cli: batch processing mode

Passing --batch on the command line causes normal output to be suppressed,
and errors to be fatal. Useful for executing bulk commands in a script ala:

   bin/cassandra-cli < commands.txt

Patch by eevans for CASSANDRA-1340
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when I start up the cassandra-cli, a ClassNotFoundException occured:java.lang.ClassNotFoundException: org.apache.cassandra.cli.CliMain",CASSANDRA-1236,12468055,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,ialand,ialand,6/28/2010 13:22,3/12/2019 14:02,3/13/2019 22:24,7/21/2010 16:18,0.6.4,,Legacy/Tools,,0,,,,,,"After start up the cassandra server, I went to the bin/ directory and run the cassandra-cli, but there's an Exception throwed out, I have set the CASSANDRA_HOME system variable,  I don't know why
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/cli/CliMain
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.cli.CliMain
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:276)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)",windows XP,,,,,,,,,,,,,,,,,,,11/Aug/10 04:59;l0s;cassandra-cli;https://issues.apache.org/jira/secure/attachment/12451744/cassandra-cli,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,04:15.9,,,no_permission,,,,,,,,,,,,20042,,,Thu Sep 08 13:02:21 UTC 2011,,,,,,0|i0g3tr:,92050,,,,,,,,,,,29/Jun/10 04:04;jbellis;Please do not change the issue metadata.,08/Jul/10 14:34;jbellis;can you reproduce in 0.6.3 gary?,21/Jul/10 16:18;gdusbabek;fixed in trunk and 0.6.4.,11/Aug/10 03:33;l0s;I am able to reproduce this in the 0.6.4 binary release.  I get the error when running cassandra-cli from the Cassandra directory as well as from the bin directory.,"11/Aug/10 03:56;gdusbabek;Thanks Carlos.  Do you have the know-how to fix the batch file?  If so, would you be willing to do this and attach it to this ticket?  I can take care of the rest.",11/Aug/10 04:59;l0s;Attached is a fix for the Bourne shell script.  It was tested on Cygwin/XP.,"11/Aug/10 05:04;l0s;Hi Gary, I actually did not realise that there was a batch file to launch the CLI but I took a look and figured out what needed to be done to the sh script.  I don't know enough about Windows batch programming to fix the bat file.  Anyway, thanks for pointing me in the right direction.  I was just trying to get through the GettingStarted guide on the Wiki.","08/Sep/11 08:25;smartree;Need the CASSANDRA_HOME system variable,
or Run the bat in CMD from Directory of ""CASSANDRA_HOME"">bin\cassandra-cli","08/Sep/11 13:02;jbellis;Smartree, 

1) please submit changes as a patch generated by svn diff

2) adding lib/*.jar is done by cassandra.in.sh (the CASSANDRA_INCLUDE searched for at the beginning of -cli).  There's no need to do it a second time in the cli script.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build does not handle ANTLR generated code properly,CASSANDRA-40,12421816,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,permellqvist,permellqvist,permellqvist,4/1/2009 20:46,3/12/2019 14:02,3/13/2019 22:24,4/2/2009 19:17,,,,,0,,,,,,"The default ant target does not trigger generation of code from Cli.g and Cql.g
The clean target does not remove generated files
The targets gen-cli-grammar and gen-cql-grammar do not reference current locations of Cli.g and Cql.g
Generated files are commited in svn repository (by mistake?)",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,01/Apr/09 20:50;permellqvist;diff.txt;https://issues.apache.org/jira/secure/attachment/12404376/diff.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,55:52.3,,,no_permission,,,,,,,,,,,,19525,,,Thu Apr 02 19:17:11 UTC 2009,,,,,,0|i0dxav:,79325,,,,,,,,,,,"01/Apr/09 20:50;permellqvist;Patch (generated by 'svn diff build.xml' on revision 760988)
Fixes mentioned issues with build.xml
Removing ANTLR-generated files from svn remains","01/Apr/09 20:55;jbellis;What does this do to Eclipse users?  Can you tell eclipse ""just build it with ant?""

(I don't use Eclipse but I think most others do.)","02/Apr/09 08:21;permellqvist;Eclipse has ant integration (so does IntelliJ and as far as I know NetBeans).
To trigger ant from eclipse http://help.eclipse.org/help33/index.jsp?topic=/org.eclipse.platform.doc.user/gettingStarted/qs-92_project_builders.htm

For Eclipse users that do not want to configure ant integration, running the gen- tasks once after checkout should be enough. After that the normal eclipse build cycle should work",02/Apr/09 12:27;jbellis;IMO we should move generated code out of the main src/ tree too but as I have said before I am not a java build guru.  What's the best practice in these situations?,"02/Apr/09 12:57;jbellis;I get errors running `ant clean build` after applying this patch:

gen-cli-grammar:
     [echo] Building Grammar /home/jonathan/projects/cassandra-new-svn/src/org/apache/cassandra/cli/Cli.g  ....
     [java] Exception in thread ""main"" java.lang.NoClassDefFoundError: org/antlr/stringtemplate/StringTemplateErrorListener
     [java] 	at org.antlr.Tool.main(Tool.java:67)
     [java] Caused by: java.lang.ClassNotFoundException: org.antlr.stringtemplate.StringTemplateErrorListener
     [java] 	at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
     [java] 	at java.security.AccessController.doPrivileged(Native Method)
     [java] 	at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
     [java] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
     [java] 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
     [java] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:252)
     [java] 	at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)
     [java] 	... 1 more
     [java] Java Result: 1
","02/Apr/09 15:38;junrao;I think it's convenient to include antlr-generated java files in the source tree. Not everybody will be modifying the CQL grammar. In a similar way, we already include thrift-generated java code in the source tree.","02/Apr/09 15:49;jbellis;That's because Thrift is, frankly, a bitch to build.

Building the antlr files should be easy since we just ship the relevant jars.  Or it should be. :)  I'm sure Per can fix the problem I'm seeing.","02/Apr/09 18:10;permellqvist;Jonathan - the NoClassDefFoundError is a separate issue (you would get the same problem even without the patch if you delete the generated sources).
I had to put some jars from an older revision back in the lib directory to run the gen-tasks

stringtemplate-3.0.jar
antlr-2.7.7.jar

They are both referenced in build.xml but were removed from the lib directory at some point.",02/Apr/09 19:17;jbellis;applied,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`show config file` in cli causes server to throw NPE ,CASSANDRA-129,12424519,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,sandeep_tata,jmhodges,jmhodges,5/4/2009 13:33,3/12/2019 14:02,3/13/2019 22:24,5/6/2009 0:35,0.3,,,,0,,,,,,"Booting up the cli and running the command ""show config file"" in the lastest from trunk (r771019) on a fresh and empty cassandra instance causes a NullPointerError to be thrown. By looking at the code (but being a not-so-hot java developer), it looks like the problem is simply DatabaseDescriptor.getConfigFileName() returning a null because configFileName_ never gets set anywhere in the code.

The error in question:

ERROR - Error occurred during processing of message.
java.lang.NullPointerException
	at java.io.FileInputStream.<init>(FileInputStream.java:133)
	at java.io.FileInputStream.<init>(FileInputStream.java:96)
	at org.apache.cassandra.service.CassandraServer.getStringProperty(CassandraServer.java:485)
	at org.apache.cassandra.service.Cassandra$Processor$getStringProperty.process(Cassandra.java:1294)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:860)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:713)

","OS X, Java 1.7 (via soylatte)",,,,,,,,,,,,,,,,,,,05/May/09 23:33;sandeep_tata;129.patch;https://issues.apache.org/jira/secure/attachment/12407296/129.patch,06/May/09 00:45;jmhodges;config_file_name.pach;https://issues.apache.org/jira/secure/attachment/12407302/config_file_name.pach,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,33:25.0,,,no_permission,,,,,,,,,,,,19560,,,Thu May 07 13:35:05 UTC 2009,,,,,,0|i0fx1z:,90953,,,,,,,,,,,"05/May/09 23:33;sandeep_tata;Reproduced the error.
The config file wasn't being initialized. This patch should fix that.","06/May/09 00:35;jbellis;applied, thanks",06/May/09 00:45;jmhodges;Patch from Sandeep for config flie plus unit test.,"06/May/09 00:46;jmhodges;Ah, shoot. Too slow at writing the test for this, I guess.",06/May/09 00:55;sandeep_tata;+1 :-),"06/May/09 02:00;jbellis;committed Jeff's test, too.  thanks!","07/May/09 13:35;hudson;Integrated in Cassandra #63 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/63/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
race with insufficiently constructed Gossiper,CASSANDRA-1160,12466186,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mdennis,jbellis,jbellis,6/4/2010 13:59,3/12/2019 14:02,3/13/2019 22:24,6/10/2010 17:06,0.6.3,0.7 beta 1,,,0,,,,,,"Gossiper.start needs to be integrated into the constructor.  Currently you can have threads using the gossiper instance before start finishes (or even starts?), resulting in tracebacks like this:

ERROR [GMFD:1] 2010-06-02 10:45:49,878 CassandraDaemon.java (line 78) Fatal exception in thread Thread[GMFD:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.net.Header.<init>(Header.java:56)
	at org.apache.cassandra.net.Header.<init>(Header.java:74)
	at org.apache.cassandra.net.Message.<init>(Message.java:58)
	at org.apache.cassandra.gms.Gossiper.makeGossipDigestAckMessage(Gossiper.java:294)
	at org.apache.cassandra.gms.Gossiper$GossipDigestSynVerbHandler.doVerb(Gossiper.java:935)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
ERROR [GMFD:2] 2010-06-02 10:45:49,880 CassandraDaemon.java (line 78) Fatal exception in thread Thread[GMFD:2,5,main]
java.lang.AssertionError
	at org.apache.cassandra.net.Header.<init>(Header.java:56)
	at org.apache.cassandra.net.Header.<init>(Header.java:74)
	at org.apache.cassandra.net.Message.<init>(Message.java:58)
	at org.apache.cassandra.gms.Gossiper.makeGossipDigestAckMessage(Gossiper.java:294)
	at org.apache.cassandra.gms.Gossiper$GossipDigestSynVerbHandler.doVerb(Gossiper.java:935)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)",,,,,,,,,,,,,,,,,,,,07/Jun/10 17:50;mdennis;0001-cassandra-0.6-1160.patch;https://issues.apache.org/jira/secure/attachment/12446505/0001-cassandra-0.6-1160.patch,08/Jun/10 16:04;mdennis;0002-cassandra-0.6-1160.patch;https://issues.apache.org/jira/secure/attachment/12446601/0002-cassandra-0.6-1160.patch,08/Jun/10 21:06;mdennis;0003-cassandra-0.6-1160.patch;https://issues.apache.org/jira/secure/attachment/12446628/0003-cassandra-0.6-1160.patch,10/Jun/10 15:39;mdennis;0004-cassandra-0.6-1160.patch;https://issues.apache.org/jira/secure/attachment/12446768/0004-cassandra-0.6-1160.patch,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,50:49.7,,,no_permission,,,,,,,,,,,,20012,,,Thu Jun 10 17:06:55 UTC 2010,,,,,,0|i0g3d3:,91975,,,,,,,,,,,"07/Jun/10 17:50;mdennis;moving start() to Gossiper.<init> causes failures in testClientOnlyMode and I didn't see a way to readily make client/server mode available to Gossiper.<init>

0001-cassandra-0.6-1160.patch delays creation of the Gossiper instance until start is called.  Any thread attempting to obtain a reference to Gossiper before it is started busy waits until it is fully initialized.

I'm not sure I actually like this solution, but thought I would submit for comments/feedback.

If we end up taking this solution, I'll submit a patch for trunk as well.","07/Jun/10 22:26;gdusbabek;What were the failures in testClientOnlyMode?  I'm sure they can be worked around.

I'm not a fan of Gossiper.preRegistrations.  I think it is a code smell that we're not spinning up Gossiper the right way.  What if we were to make Gossiper.start() blocking until we were sure it was in a healthy state (keep in mind the single-node cluster) and then make sure it was called before MessageService.listen()?","07/Jun/10 23:33;mdennis;I should have been more clear about that.  The failures in StorageServiceClientTest are because files from DatabaseDescriptor.getAllDataFileLocations() exist (the test asserts they do not).  This is because when I moved start into <init> I passed the same generation number that that initServer() uses, but initClient (used by the test) passes a generation number based on currentTimeMillis() which doesn't cause the file paths to exist.  It wasn't clear to me there was an easy way to determine what generation to use.

More importantly, this lead me to notice that both StorageService and StorageLoadBalancer are registered with Gossiper before start is called.  If start was moved into <init> I was afraid of introducing new race conditions (e.g. Gossiper starting up, doing things that would have resulted in publications and then later receiving the registration for such publications).

I agree that Gossiper is likely not being spun up correctly.  I think this applies to other things as well (in general things seem to have many interdependencies on boot.  I don't have anything specific to point at, but it feels like there are several race conditions that just happen to usually not be exhibited).

One solution I've used in the past that I like for this in general is that none of the singletons are inited in their own classes, but only by a BootInitializer of sorts that completely controls when the objects are inited/started/created/etc, in particular controlling the order things are done.  A single entry point into the system if you will.  This seems like some work from the point we're at now and probably a bit unnecessary.

Along those lines I had a similar patch that I didn't submit that took a third argument, List<IEndpointStateChangeSubsciber> initialSubscribers.  Each initialSubscriber was registered before the existing code in start was called.  initServer built a list of the StorageService and StorageLoadBalance instances and passed that to start to ensure none of the publications were missed by either service.  Like the 0001-cassandra-0.6-1160.patch references to .instance were replaced with getInstance() calls which busy waited (via yield) until start completed but this meant that StorageService was managing StorageLoadBalancer registrations and that didn't feel right, hence preRegistrations() (so each could manager their own registrations).

So, basically two issues/questions if Gossiper.start moves to Gossiper.<init>:

1) how does Gossiper.<init> get the correct generation number (server v client)?

2) what about missed publications because Gossiper was started before StorageService and/or StorageLoadBalancer got a chance to register?
",08/Jun/10 13:18;gdusbabek;I'm taking a closer look. In the mean time can you rebase your patch?,08/Jun/10 16:04;mdennis;0002-cassandra-0.6-1160.patch is against r952679,"08/Jun/10 18:36;mdennis;just FYI, if Gossiper.start() is moved before MessageService.listen() the unit tests show sporadic (~ 1 in 5) errors even though the tests still pass.



{code}
    [junit] ------------- Standard Error -----------------
    [junit] Exception in thread ""ACCEPT-/127.0.0.1"" java.lang.RuntimeException: java.nio.channels.ClosedChannelException
    [junit] 	at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:488)
    [junit] Caused by: java.nio.channels.ClosedChannelException
    [junit] 	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:130)
    [junit] 	at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    [junit] 	at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:477)
{code}

and

{code}
    [junit] Testsuite: org.apache.cassandra.service.AntiEntropyServiceTest
    [junit] Tests run: 10, Failures: 0, Errors: 0, Time elapsed: 3.282 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 11:18:22,111 Error in executor futuretask
    [junit] java.util.concurrent.ExecutionException: java.lang.NullPointerException
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    [junit] 	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    [junit] 	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:619)
    [junit] Caused by: java.lang.NullPointerException
    [junit] 	at java.util.AbstractCollection.addAll(AbstractCollection.java:303)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:149)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$Validator.call(AntiEntropyService.java:491)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	... 2 more
{code}","08/Jun/10 19:03;gdusbabek;Intermittent test failures indicate you have a timing problem.

The fact that the assertion (the original intermittent problem) fails because a static class member is null makes me think we're dealing with static initializers firing in the order we don't suspect.  

I'd start by getting the inner classes (the verb handlers) out of Gossiper and into their own classes.  Then there is the race between Gossiper and MessageService.  You could let the gossiper start before the MessageService starts listening, but have the GossipTimerTask check to make sure the MessageService is listening before it sends out any gossip requests.  This still isn't going to protect us in the case of a node getting restarted where other nodes already know about it and and diligently trying to contact what they thought was a dead node.
",08/Jun/10 21:06;mdennis;0003-cassandra-0.6-1160.patch as requested moves inner classes and Gossiper to top level classes and has Gossiper TimerTask busy wait until MessagingService is up,"10/Jun/10 13:15;gdusbabek;Last patch is looking good, except you should consider replacing the busy-wait Thread.yield() stuff with a CountDownLatch and a simple MessageService.awaitListen() call or something like that.  My reasoning is that Thread.yeild() behaves differently across platforms and can possibly starve threads running at a lower priority.",10/Jun/10 14:01;jbellis;also consider using SimpleCondition.,"10/Jun/10 14:31;gdusbabek;Wouldn't a Condition cause the waiter to block if somehow signalAll() (which in this case would only be called once) was called before await()?  I know it's not likely, but there isn't any such problem with a CountDownLatch.","10/Jun/10 14:57;jbellis;// [SimpleCondition] fulfils the Condition interface without spurious wakeup problems
// (or lost notify problems either: that is, even if you call await()
// _after_ signal(), it will work as desired.)
","10/Jun/10 15:14;gdusbabek;I see.  I figured SimpleCondition was something out of the JDK.

Spurious bikeshed comment:  SimpleCondition.set should be volatile, no?  otherwise set=true ins't guaranteed to be visible to any thread stuck in await().","10/Jun/10 15:27;jbellis;I believe the synchronized keyword takes care of that, but wait/notify are admittedly subtle.  I could be wrong.",10/Jun/10 15:39;mdennis;0004-cassandra-0.6-1160.patch uses a gate to wait until MessagingService is listening,10/Jun/10 15:40;gdusbabek;You're right. synchronized establishes happens-before with respect to object state.,10/Jun/10 17:06;gdusbabek;replaced the latch with a SimpleCondition and committed.  Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
validation of time uuid is incorrect,CASSANDRA-1910,12494185,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,dave111,dave111,12/28/2010 18:18,3/12/2019 14:02,3/13/2019 22:24,12/29/2010 19:45,0.6.9,0.7.0,,,0,,,,,,"It appears _TimeUUIDType_ (as of 12/9) is checking the wrong bits when validating a time UUID as version 1.

Per the comment and rfc4122, ""_version is bits 4-7 of byte 6_"", however validate() is actually checking the least significant bits:

> _if ((slice.get() & 0x0f) != 1)_

Sample java/hector code:

{code}
// displays ""version 1"" but validation fails
java.util.UUID uuid1 = java.util.UUID.fromString(""00000000-0000-1000-0000-000000000000"");
System.out.println(uuid1 + "" "" + uuid1.version());
TimeUUIDType.instance.validate(UUIDSerializer.get().toByteBuffer(uuid1));

// displays ""version 2"" but validation succeeds
java.util.UUID uuid2 = java.util.UUID.fromString(""00000000-0000-2100-0000-000000000000"");
System.out.println(uuid2 + "" "" + uuid2.version());
TimeUUIDType.instance.validate(UUIDSerializer.get().toByteBuffer(uuid2));
{code}

The issue can be seen with any UUID where the timestamp doesn't start with 1:

b54adc00-67f9-10d9-9669-0800200c9a66, (timestamp year 1776) version 1 fails
b54adc00-67f9-12d9-9669-0800200c9a66, (timestamp year 2233) version 1 fails
",,,,,,,,,,,,,,,,,,,,29/Dec/10 18:58;gdusbabek;1910-0.6.txt;https://issues.apache.org/jira/secure/attachment/12467129/1910-0.6.txt,29/Dec/10 18:40;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-examine-the-right-nibble-when-validating-TimeUUIDs.txt;https://issues.apache.org/jira/secure/attachment/12467127/ASF.LICENSE.NOT.GRANTED--v1-0001-examine-the-right-nibble-when-validating-TimeUUIDs.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,30:26.7,,,no_permission,,,,,,,,,,,,20367,,,Wed Dec 29 19:45:09 UTC 2010,,,,,,0|i0g85z:,92753,,,,,,,,,,,"29/Dec/10 17:30;jbellis;probably related: I have seen this error in the test suite --

{noformat}
    [junit] Testcase: testInvalidTimeUUID(org.apache.cassandra.db.marshal.TypeValidationTest):	FAILED
    [junit] Expected exception: org.apache.cassandra.db.marshal.MarshalException
    [junit] junit.framework.AssertionFailedError: Expected exception: org.apache.cassandra.db.marshal.MarshalException
{noformat}",29/Dec/10 18:44;gdusbabek;patch is for 0.7+trunk.  0.6 will be different.,29/Dec/10 18:58;gdusbabek;Implements proper TimeUUID validation in 0.6.,"29/Dec/10 19:04;jbellis;+1

let's commit to 0.6, 0.7.0, 0.7, and trunk","29/Dec/10 19:32;hudson;Integrated in Cassandra-0.6 #41 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/41/])
    examine the right nibble when validating TimeUUIDs. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1910
",29/Dec/10 19:45;gdusbabek;committed everywhere.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift client forwarding the null keys to the servers,CASSANDRA-308,12430972,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,vijay2win@yahoo.com,vijay2win@yahoo.com,7/20/2009 23:54,3/12/2019 14:02,3/13/2019 22:24,9/3/2009 15:59,0.4,,,,0,,,,,,"Thrift client is suppose to validate the input before it actually sends it to the server but it did not.... 

client logs are like the below: (Java client lib)
org.apache.thrift.TApplicationException: Internal error processing get_slice
        at org.apache.thrift.TApplicationException.read(TApplicationException.java:107)
        at org.apache.cassandra.service.Cassandra$Client.recv_get_slice(Cassandra.java:178)
        at org.apache.cassandra.service.Cassandra$Client.get_slice(Cassandra.java:154)
        at com.webex.dms.datastore.DataStoreRead.readObject(DataStoreRead.java:163)
        at com.webex.dms.repository.ReadDocument.load(ReadDocument.java:87)
        at com.webex.dms.repository.Document.readBasic(Document.java:307)

Server Logs are as below:
DEBUG [pool-1-thread-448] 2009-07-20 09:27:50,831 CassandraServer.java (line 172) get_slice_from
ERROR [pool-1-thread-448] 2009-07-20 09:27:50,837 Cassandra.java (line 844) Internal error processing get_slice
java.lang.NullPointerException
        at org.apache.cassandra.service.ThriftValidation.validateKey(ThriftValidation.java:18)
        at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:75)
        at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:181)
        at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:838)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:796)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)

","Centos 3.0, cassendra trunk, JVM 1.6, tomcat 1.6",,,,,,,,,,,,,,,,,,,02/Sep/09 21:58;jbellis;308-v2.patch;https://issues.apache.org/jira/secure/attachment/12418442/308-v2.patch,27/Aug/09 21:06;jbellis;308.patch;https://issues.apache.org/jira/secure/attachment/12417933/308.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,39:01.3,,,no_permission,,,,,,,,,,,,19628,,,Thu Sep 03 15:59:03 UTC 2009,,,,,,0|i0fy53:,91129,,,,,,,,,,,"14/Aug/09 15:39;jbellis;The real fix is for the server to do the validation; we don't want to rely on the client since there are so many implementations of such widely varying maturity.

I've attached a patch for this to THRIFT-562.","15/Aug/09 12:50;sbtourist;Jonathan,

so should it, in your opinion, be resolved by patching the CassandraServer?",15/Aug/09 13:36;jbellis;No.,"15/Aug/09 13:36;jbellis;To clarify: it should be resolved by having the Cassandra.java code generated by Thrift do the checking, not the hand-coded CassandraServer.java.","27/Aug/09 21:06;jbellis;Making ""not optional"" imply ""required"" was apparently too much of a leap for dreiss of Thrift.  (See THRIFT-562, THRIFT-455.)  This patch adds `required` boilerplate to all such fields.","28/Aug/09 15:08;hudson;Integrated in Cassandra #180 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/180/])
    update to thrift trunk and rename jar after the svn revision (806014).  inline our map typedef to work around regression introduced in THRIFT-144.  add slf4j dependencies (required since THRIFT-558).  regenerate thrift structs with new version.
patch by jbellis for , CASSANDRA-387
",28/Aug/09 17:56;urandom;+1,28/Aug/09 19:03;jbellis;committed,"29/Aug/09 12:35;hudson;Integrated in Cassandra #181 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/181/])
    add ""required"" to non-optional fields. Thrift sucks.
patch by jbellis; reviewed by Eric Evans for 
",02/Sep/09 21:57;jbellis;still not null checking method parameters,"02/Sep/09 21:58;jbellis;v2 adds null-checking to method parameters, as generated by the patch to THRIFT-575.","03/Sep/09 15:25;sammy.yu;+1
",03/Sep/09 15:59;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Using KeysCached=""xx%"" results in Key cache capacity: 1",CASSANDRA-1129,12465392,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,rantav,rantav,5/25/2010 17:23,3/12/2019 14:02,3/13/2019 22:24,6/1/2010 19:26,0.6.3,,,,0,,,,,,"I don't know if this is a general bug or only something related to my instance, but for me (v0.6.1) I've noticed that when defining KeysCached=""50%"" (or KeysCached=""100%"" and I didn't test other values with %) then cfstats reports Key cache capacity: 1

      <ColumnFamily CompareWith=""BytesType"" Name=""KvAds""
        KeysCached=""100%""
        RowsCached=""10000""
        />


                Column Family: KvAds
                SSTable count: 7
                Space used (live): 797535964
                Space used (total): 797535964
                Memtable Columns Count: 42292
                Memtable Data Size: 10514176
                Memtable Switch Count: 24
                Read Count: 2563704
                Read Latency: 4.590 ms.
                Write Count: 1963804
                Write Latency: 0.025 ms.
                Pending Tasks: 0
                Key cache capacity: 1
                Key cache size: 1
                Key cache hit rate: 0.0
                Row cache capacity: 10000
                Row cache size: 10000
                Row cache hit rate: 0.2206178354382234
                Compacted row minimum size: 386
                Compacted row maximum size: 9808
                Compacted row mean size: 616

I'll attach one of the sstable files from this CF",,,,,,,,,,,,,,,,,,,,29/May/10 05:42;jhermes;0001-CASSANDRA1129.patch;https://issues.apache.org/jira/secure/attachment/12445832/0001-CASSANDRA1129.patch,31/May/10 07:26;jhermes;0002-CASSANDRA1129.patch;https://issues.apache.org/jira/secure/attachment/12445907/0002-CASSANDRA1129.patch,01/Jun/10 15:33;jhermes;0003-CASSANDRA1129.patch;https://issues.apache.org/jira/secure/attachment/12446030/0003-CASSANDRA1129.patch,25/May/10 19:04;rantav;KvAds-84.zip;https://issues.apache.org/jira/secure/attachment/12445489/KvAds-84.zip,02/Jun/10 06:42;jhermes;TOTRUNK-2-CASSANDRA1129.patch;https://issues.apache.org/jira/secure/attachment/12446118/TOTRUNK-2-CASSANDRA1129.patch,02/Jun/10 00:50;jhermes;TOTRUNK-CASSANDRA1129.patch;https://issues.apache.org/jira/secure/attachment/12446084/TOTRUNK-CASSANDRA1129.patch,,,,,,,,6,,,,,,,,,,,,,,,,,,,37:06.4,,,no_permission,,,,,,,,,,,,20001,,,Fri Jun 04 02:38:45 UTC 2010,,,,,,0|i0g367:,91944,,,,,,,,,,,25/May/10 17:33;rantav;The smallest sstable file I could find of this CF.,"25/May/10 17:37;jbellis;can you include index and filter parts too, as a zip or tarball?","25/May/10 19:04;rantav;zip file with index, data and filter","27/May/10 13:38;jbellis;So to troubleshoot this you would

* check out the 0.6 branch
* add the CF definition above to your conf/storage-conf.xml
* unzip the sstable files into your data/Keyspace1 directory
* set a breakpoint in DatabaseDescriptor.getKeysCachedFor and see why it's calculating 1 instead of the number of rows in the sstable","29/May/10 05:42;jhermes;Status for first patch:
M       test/unit/org/apache/cassandra/db/CompactionsPurgeTest.java
M       test/conf/storage-conf.xml
M       src/java/org/apache/cassandra/db/ColumnFamilyStore.java
M       src/java/org/apache/cassandra/utils/FBUtilities.java
M       src/java/org/apache/cassandra/cache/InstrumentedCache.java
M       src/java/org/apache/cassandra/io/SSTableTracker.java

This bug was two bugs:
# FBUtilities#absoluteFromFraction(double,long) is a bit ambiguous. The method reads 100% as *absolute* 1, so the keyCacheCapacity is always going to be 1. This fix is already in 0.7.
# The bigger bug is that the keyCacheCapacity was not getting changed during runtime. There was a boolean capacityModified that controlled access to the capacity. It gets set true the first time the capacity is modified (read: creation of table/cfstore) and then never goes back to false. By removing this bool, the cache size gets updated on flushes and on compactions correctly.

Also included are tests in CompactionsPurgeTest that shows this fix works for both 50% and 100% (that the keyCacheCapacity changes dynamically).

The update is O(1), so it shouldn't matter for performance that it's being called on every flush and every compaction.","29/May/10 11:59;jbellis;committed the absoluteFromFraction fix.

for the rest, we need to preserve the boolean, but make it only get set when the cache capacity is set manually from JMX (see CASSANDRA-1079)","31/May/10 07:26;jhermes;Removed the FBUtil change from patch.

setCapacity(int) changed to setCapacity(int,bool) in both interface and object.
The boolean in the object is now capacityFrozen with the same logic as before.
setCapacity from the automatic update (flushing/compaction) pass in a false to not freeze the capacity afterward, setCapacity from the JMX command pass in a true to do so.

(To remedy a previous statement, the running time is not O(1) overall. It's O(|SSTables|) with constant time per table.)","01/Jun/10 14:15;jbellis;instead of overloading setCapacity to be the interface for both normal adjustments and manual overrides, let's split it up.  have setCapacity(value) be the mbean interface, setting a capacitySetManually boolean, and make a method updateCapacity(value) for internal use.  ","01/Jun/10 15:33;jhermes;capacitySetManually is in place.
setCapacity(int), updateCapacity(int) work as described above.","01/Jun/10 19:26;jbellis;committed, thanks!","01/Jun/10 19:31;jbellis;could you submit a version of this patch against trunk, too?","02/Jun/10 00:50;jhermes;Here's the patch for trunk.
The testconf.xml is now a testconf.yaml.
The test itself uses trunk-style clock, decorated keys.

io/SSTableTracker changes go to io/sstable/SSTableTracker.

The other changes patched easily.","02/Jun/10 03:34;jbellis;I'm getting test failures on trunk (but not 0.6):

    [junit] Testcase: testKeyCache50(org.apache.cassandra.db.CacheSizeTest):	FAILED
    [junit] 128
    [junit] junit.framework.AssertionFailedError: 128
    [junit] 	at org.apache.cassandra.db.CacheSizeTest.testKeyCache(CacheSizeTest.java:93)
    [junit] 	at org.apache.cassandra.db.CacheSizeTest.testKeyCache50(CacheSizeTest.java:49)

(I moved it to a separate CacheSizeTest class, in case the others in CompactionsPurgeTest were messing with it.  Didn't help.)","02/Jun/10 06:42;jhermes;All right, the difference is between SSTableReader.estimatedKeys() in 0.6 and RowIndexedReader.estimatedKeys() in 0.7 -- RIR increments the size once during estimation whereas SSTR does not.
Knowing this, I'm making the test explicitly catch 128/256.","02/Jun/10 06:53;jhermes;By the way, the RIR increment looks unintentional to me. It might be a bug; I can't see a reason why it gets incremented for CASS-777 when SSTableReader went to RIReader.
If this is the case, then the +1 can be reverted and the test can go back to catching 64/128.","04/Jun/10 02:38;jbellis;committed the original version for trunk, and removed the extra +1 from estimatedKeys.  Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memtable_throughput_in_mb can not support sizes over 2.2 gigs because of an integer overflow.,CASSANDRA-2158,12498478,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,trisk,trisk,2/13/2011 6:54,3/12/2019 14:02,3/13/2019 22:24,2/18/2011 22:10,0.7.4,,,,0,,,,,,"If memtable_throughput_in_mb is set past 2.2 gigs, no errors are thrown.  However, as soon as data starts being written it is almost immediately being flushed.  Several hundred SSTables are created in minutes.  I am almost positive that the problem is that when memtable_throughput_in_mb is being converted into bytes the result is stored in an integer, which is overflowing.

From memtable.java:

    private final int THRESHOLD;
    private final int THRESHOLD_COUNT;

...
this.THRESHOLD = cfs.getMemtableThroughputInMB() * 1024 * 1024;
this.THRESHOLD_COUNT = (int) (cfs.getMemtableOperationsInMillions() * 1024 * 1024);


NOTE:
I also think currentThroughput also needs to be changed from an int to a long.  I'm not sure if it is as simple as this or if this also is used in other places.",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,14/Feb/11 22:12;jbellis;2158.txt;https://issues.apache.org/jira/secure/attachment/12471026/2158.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,33:05.5,,,no_permission,,,,,,,,,,,,20471,,,Sat Mar 12 00:23:38 UTC 2011,,,,,,0|i0g9o7:,92997,brandon.williams,brandon.williams,,,,,,,,,"13/Feb/11 14:33;jbellis;While you are correct, you almost certainly shouldn't have throughput set that high, because if you are tuning things correctly you will hit your operations count limit first for 99.9% of workloads.","13/Feb/11 17:46;trisk;I think I have a usecase where larger memtables would help a lot.  I have a combination of fat columns that can update frequently, and I have lots of memory (currently 96 gb).  I know I could also handle this by putting more boxes in the cluster, but I think I can get a lot more out of the boxes I have.  I am experimenting with breaking up my cf into multiple ones to get the same effect as the bigger sstable.  So far it seems to perform well, but feels hacky.

Even if you decide to have a hard limit on the memtable size, it should probably fail loudly instead of generating hundreds of sstables.  With my understanding of the current defaults, any default install of cassandra with more than 32 gb of memory will default to this state and will be hard for new users to understand (32/2 -> 16 gig heap | 16/8 -> 2gb default CF memtable throughput).  I would much prefer the option than the hard limit though :).",14/Feb/11 22:12;jbellis;patch to make ints into longs and validate input,16/Feb/11 18:16;brandon.williams;+1,18/Feb/11 22:10;jbellis;committed,"18/Feb/11 22:51;hudson;Integrated in Cassandra-0.7 #296 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/296/])
    update memtable_throughput to be a long
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2158
","12/Mar/11 00:11;jbellis;Erik Forkalsrud commented on the mailing list,

{noformat}
It looks like the fix isn't entirely correct.  The bug is still in 0.7.3.   In Memtable.java, the line:
  THRESHOLD = cfs.getMemtableThroughputInMB() * 1024 * 1024;

should be changed to:
  THRESHOLD = cfs.getMemtableThroughputInMB() * 1024L * 1024L;

Here's some code that illustrates the difference:

   public void testMultiplication() {
       int memtableThroughputInMB = 2300;
       long thresholdA = memtableThroughputInMB * 1024 * 1024;
       long thresholdB = memtableThroughputInMB * 1024L * 1024L;
       System.out.println(""a="" + thresholdA + "" b="" + thresholdB);
   }
{noformat}

Made this change for 0.7.4","12/Mar/11 00:23;hudson;Integrated in Cassandra-0.7 #376 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/376/])
    fix memtable thresholds better
patch by Erik Foralsrud; reviewed by jbellis for CASSANDRA-2158
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
user created with debian packaging is unable to increase memlock,CASSANDRA-2169,12498679,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,2/15/2011 19:36,3/12/2019 14:02,3/13/2019 22:24,2/19/2011 22:54,0.7.3,,Packaging,,0,,,,,,"To reproduce:
- Install a fresh copy of ubuntu 10.04.
- Install sun's java6 jdk.
- Install libjna-java 3.2.7 into /usr/share/java.
- Install cassandra 0.7.0 from the apache debian packages.
- Start cassandra using /etc/init.d/cassandra
In the output.log there will be the following error:
{quote}
Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
{quote}
This shouldn't be as the debian package creates /etc/security/limits.d/cassandra.conf and sets the cassandra user's memlock limit to 'unlimited'.

I tried a variety of things including making the memlock unlimited for all users in /etc/security/limits.conf.  I was able to run cassandra using root with jna symbolically linked into /usr/share/cassandra from /usr/share/java, but I could never get the init.d script to work and get beyond that error.

Based on all the trial and error, I think it might have to do with the cassandra user itself, but my debian/ubuntu fu isn't as good as others'.",,,,,,,,,,,,,,,,,,,,18/Feb/11 21:14;jeromatron;2169-0_7.txt;https://issues.apache.org/jira/secure/attachment/12471439/2169-0_7.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,39:19.6,,,no_permission,,,,,,,,,,,,20478,,,Sat Feb 19 23:16:39 UTC 2011,,,,,,0|i0g9qn:,93008,urandom,urandom,,,,,,,,,15/Feb/11 19:46;jeromatron;Upgraded to 0.7.1 and still appears to have the same issue.,16/Feb/11 18:39;brandon.williams;What happens if you su to the cassandra user and check ulimit?,"16/Feb/11 18:50;jeromatron;""ulimit -l"" always returns 64.

I gave the cassandra user a shell so I can su to that user.  When I do, the memlock doesn't ever take - whether it's defined in /etc/security/limits.conf for the cassandra user, for all users '*', or in the /etc/security/limits.d/cassandra.conf.","16/Feb/11 18:53;brandon.williams;Just to be certain limits.conf is taking effect, have you tried rebooting the box since changing it?",16/Feb/11 18:56;jeromatron;I made sure it took effect for the user I originally logged in as - ubuntu for my ec2 instance and jeremy for my local vm.  It took effect for those users whenever I changed it.  I also tried rebooting.  That was the way it took effect for the ec2 server's ubuntu user iirc.  But it didn't take effect for the cassandra user.,"16/Feb/11 21:17;jeromatron;Based on a suggestion from jake and brandon, I added {quote}session required pam_limits.so{quote} to /etc/pam.d/common-session and rebooted.  That made it so the memlock value was set correctly.  However, for some reason, using /etc/init.d/cassandra still gives the memory locking error.  I su to the cassandra user and run {quote}/usr/sbin/cassandra -f{quote} and it is able to lock the memory.

For the pam setting, see http://posidev.com/blog/2009/06/04/set-ulimit-parameters-on-ubuntu/","18/Feb/11 05:10;thepaul;I grepped through jsvc's source, and I don't see any references to pam at all. So unless the jre has some special ""switch users and set up a pam session"" functionality I don't know about, jsvc isn't setting up a pam session when switching users.

This means limits.conf (and limits.d/cassandra.conf) are useless, except in what they define for root's resource limits.

If we want to use both limits.conf and jsvc, then hrm. Maybe we could switch users in the initscript using /bin/su, but afaik default debian and ubuntu systems all comment out pam_limits.so from /etc/pam.d/su , so that wouldn't work without monkeying with users' conffiles.

We could switch users in the initscript with sudo, but it's pretty hard to be sure the user hasn't done something funky with their sudoers file which would break our startup.

I can only come up with 2 halfway-decent options: both involve ditching limits.d/cassandra.conf.

1: just /bin/su in the initscript and do a 'ulimit -l unlimited' in the child before exec'ing jsvc.

2: implement limit setting and user switching in cassandra itself. is there any good way to do setrlimit() and setuid() in java?","18/Feb/11 05:19;jbellis;bq. 2: implement limit setting and user switching in cassandra itself. is there any good way to do setrlimit() and setuid() in java?

No, you'd have to write a JNI wrapper or use JNA.  (Which I guess is technically feasible since JNA is packaged for both deb and rpm but eww. :)","18/Feb/11 05:23;thepaul;Actually, durr, you could just do 'ulimit -l unlimited' in sh before switching users. It should stick. Sorry, I'm slow tonight.

Still might be a good idea to keep limits.d/cassandra.conf around, in case people want to su or sudo to the cassandra user for testing stuff.",18/Feb/11 16:21;jeromatron;Paul: so is that something that would go in the /debian configuration as a change?  Would you like me to try that in my test server and see if that resolves the problem?,18/Feb/11 18:49;jeromatron;so adding that manually to the /etc/init.d/cassandra script does the trick.  I will submit a patch that adds that.  Thanks guys!,18/Feb/11 21:14;jeromatron;One line patch to fix the ulimit stuff.,19/Feb/11 22:54;urandom;committed; thanks!,"19/Feb/11 23:16;hudson;Integrated in Cassandra-0.7 #299 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/299/])
    increase memlock at daemon startup

Patch by Jeremy Hanna; reviewed by eevans for CASSANDRA-2169
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide way to remove nodes from gossip entirely,CASSANDRA-644,12443778,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jaakko,jbellis,jbellis,12/18/2009 16:18,3/12/2019 14:02,3/13/2019 22:24,1/27/2010 21:19,0.6,,Legacy/Tools,,1,,,,,,"As reported in CASSANDRA-634, ""Now that we're gossiping about dead nodes as well, gossip digest continues to grow without boundary when nodes come and go. This information will never disappear as it will be propagated to new nodes no matter how old and obsolete it is. To counter this, we need some mechanism to (1) either remove dead node from endpointstateinfo or (2) at some point stop to gossip about it, or both.""

This is also seen when using ""fat clients"" that participate in the gossip ring; if a client leaves and does not come back it stays in the gossip forever.  (This can be confusing if the client does start up again, connecting to a _different_ cluster, but the old one notices it is back and starts gossiping to it again!)

I would prefer to leave management of these things explicit; 3 days is long enough that the fat client problem in particular needs another solution, and if it needs another solution then that can become the only solution. :)

So I would be in favor of removeToken clearing out gossip entries, and also adding a command to remove an endpoint from the gossip ring that does not have a token associated with it (like fat clients).  A command to ask ""what are all the known gossip hosts"" would also be useful, since nodeprobe ring only includes nodes w/ tokens.",,,,,,,,,,,,,,,,,,,,21/Jan/10 05:01;jaakko;644.patch;https://issues.apache.org/jira/secure/attachment/12430987/644.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,35:20.5,,,no_permission,,,,,,,,,,,,19797,,,Wed Jan 27 21:19:14 UTC 2010,,,,,,0|i0g07b:,91463,,,,,,,,,,,"21/Dec/09 12:35;jaakko;> connecting to a _different_ cluster, but the old one notices it is back and starts gossiping to it again!

Actually this cannot happen since gossip SYN carries cluster ID and only messages from the same cluster are handled.

I think removing fat clients automatically is OK. We could run a check periodically and remove all nodes from gossiper that have been silent for certain period of time, and do not have token associated with it. I'm not sure in what kind of situations this kind of clients are used, but if a node does not participate in storage and has been dead for an hour, there should be no harm in removing it. It will be back in gossip anyway immediately if it reappears. Cleaning up fat clients automatically would prevent gossip data from growing ""rapidly"" if there are many clients coming and going from different IP addresses.

As for ""proper"" storage nodes: These are likely to be more stable than clients, so probably it is best to have removetoken and/or other manual commands to take care of this.
","21/Dec/09 15:59;jbellis;> We could run a check periodically and remove all nodes from gossiper that have been silent for certain period of time, and do not have token associated with it.

that sounds fine.","13/Jan/10 22:47;jbellis;(I originally marked this as Improvement, but isn't decommission/removeToken not cleaning out from gossip a Bug?)","14/Jan/10 15:27;jaakko;yeah, that should probably be fixed. Attached patch should do the trick. I'll fix the other part tomorrow.","16/Jan/10 07:22;jaakko;patch attached:

- removeToken now removes node from gossip
- fat clients (nodes without token) are removed from gossip after 1 hour of inactivity
- added justRemovedEndPoints to gossip to prevent removed nodes from reappearing immediately. Removed nodes are kept here for RING_DELAY period, during which time new joins from them are ignored. It takes a while for remove token gossip to propagate to all nodes. During this time some nodes will continue to gossip about the just-now-being-removed-node, while others have already removed it.
- fixed a bug related to removeToken command which allowed node's own token being removed.
",18/Jan/10 16:22;jbellis;shouldn't this also r/m from FD.arrivalSamples?,"21/Jan/10 05:01;jaakko;yeah, I decided it is not worth the trouble, but you're right, it is better to remove from FD too. attached new version.
","22/Jan/10 15:37;jbellis;+1

are you comfortable committing this to 0.5 branch? ","23/Jan/10 00:01;jaakko;Before committing to 0.5, it would be good if other people tested this as well.",25/Jan/10 09:27;jaakko;Committed to trunk. Let's wait a while and commit to 0.5 if nothing breaks.,"25/Jan/10 12:43;hudson;Integrated in Cassandra #334 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/334/])
    fixed broken test. 
modify removetoken to remove node from gossip. remove fat clients automatically after 1h of inactivity. patch by jaakko, reviewed by jbellis. 
",25/Jan/10 14:06;jbellis;could you also update CHANGES?,27/Jan/10 21:19;jbellis;updated CHANGES and closed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool info NPE when node isn't fully booted,CASSANDRA-2270,12500386,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,wajam,wajam,3/4/2011 1:16,3/12/2019 14:02,3/13/2019 22:24,3/4/2011 20:43,0.7.4,,,,0,,,,,,"Running ""nodetool -h 127.0.0.1 info"" when the node is not yet ready throw a NPE.

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.gms.Gossiper.getCurrentGenerationNumber(Gossiper.java:313)
        at org.apache.cassandra.service.StorageService.getCurrentGenerationNumber(StorageService.java:1239)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:205)
",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,04/Mar/11 19:08;brandon.williams;2270.txt;https://issues.apache.org/jira/secure/attachment/12472697/2270.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,57:56.2,,,no_permission,,,,,,,,,,,,20534,,,Fri Mar 04 23:34:27 UTC 2011,,,,,,0|i0gadj:,93111,jbellis,jbellis,,,,,,,,,"04/Mar/11 14:57;jbellis;By ""not yet ready"" do you mean early in normal startup, or started up with explicitly not joining ring?","04/Mar/11 15:01;wajam;Early in normal startup, just run ""nodetool -h localhost info"" when the node its still opening index.",04/Mar/11 19:08;brandon.williams;Patch to not check the generation when gossip is not initialized.,04/Mar/11 19:12;jbellis;+1,04/Mar/11 20:43;brandon.williams;Committed.,"04/Mar/11 23:34;hudson;Integrated in Cassandra-0.7 #349 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/349/])
    Fix NPE in nodetool when gossip isn't initialized.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-2270
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
service.SerializationsTest failes under cobertura,CASSANDRA-2258,12500103,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,jbellis,jbellis,3/1/2011 21:39,3/12/2019 14:02,3/13/2019 22:24,3/15/2011 13:07,0.7.5,,Legacy/Testing,,0,,,,,,"ant codecoverage -Dtest.name=SerializationsTest gives

{noformat}
    [junit] Testcase: testTreeResponseRead(org.apache.cassandra.service.SerializationsTest):	Caused an ERROR
    [junit] java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124
    [junit] java.lang.RuntimeException: java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.deserialize(AntiEntropyService.java:634)
    [junit] 	at org.apache.cassandra.service.SerializationsTest.testTreeResponseRead(SerializationsTest.java:90)
    [junit] Caused by: java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124
    [junit] 	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:562)
    [junit] 	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1582)
    [junit] 	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1495)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1731)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.deserialize(AntiEntropyService.java:630)
{noformat}",,,,,,,,,,,,,,,,,,,,14/Mar/11 21:02;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-do-not-instrument-the-Token-classes.txt;https://issues.apache.org/jira/secure/attachment/12473614/ASF.LICENSE.NOT.GRANTED--v1-0001-do-not-instrument-the-Token-classes.txt,14/Mar/11 20:53;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-fix-cobertura-brokenness-by-forcing-serialVersionUIDs.txt;https://issues.apache.org/jira/secure/attachment/12473610/ASF.LICENSE.NOT.GRANTED--v1-0001-fix-cobertura-brokenness-by-forcing-serialVersionUIDs.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,04:15.7,,,no_permission,,,,,,,,,,,,20531,,,Wed Mar 16 00:31:14 UTC 2011,,,,,,0|i0gaav:,93099,,,,,,,,,,,"14/Mar/11 21:04;gdusbabek;Several problems here:
1.  AntiEntropyService is using serialized java objects.
2.  Cobertura doesn't preserve serialVersionUID during instrumentation.

I've attached two possible fixes, neither of them very impressive.

The first forces the serialVersionUID for all Token descendants to 1L.  This is bad because it breaks wire compatibility between $THIS_VERSION and 0.7.x (see CASSANDRA-1015 for why we shouldn't allow this).

The second disables instrumenting *Token.class during the instrumentation phase.  Upshot is that we don't get code coverage reports for those classes.","14/Mar/11 22:10;jbellis;bq. The second disables instrumenting *Token.class during the instrumentation phase. Upshot is that we don't get code coverage reports for those classes.

+1 this approach if you add a comment to build.xml for posterity :)",15/Mar/11 13:07;gdusbabek;committed,"15/Mar/11 16:46;hudson;Integrated in Cassandra-0.7 #382 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/382/])
    ","16/Mar/11 00:31;hudson;Integrated in Cassandra #781 (See [https://hudson.apache.org/hudson/job/Cassandra/781/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StringTokenizer throws NoSuchElementException,CASSANDRA-486,12437926,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,elubow,elubow,10/13/2009 1:40,3/12/2019 14:02,3/13/2019 22:24,10/13/2009 15:48,0.4,,,,0,,,,,,"---- cf format ----
<ColumnFamily CompareWith=""UTF8Type"" Name=""Users"" />
    'bar@baz.com': {
        email: 'bar@baz.com',
        person_id: '789',
        send_dates_2009-09-30: '2245',
        send_dates_2009-10-01: '2246',
    },
---- Relevant log lines ----
DEBUG - insertBlocking writing key schmidtcaroline71@yahoo.com to 3489502@[127.0.0.1:7000]
DEBUG - Applying RowMutation(table='Mailings', key='schmidtcaroline71@yahoo.com', modifications=[ColumnFamily(Users [email,person_id,send_dates_2009-09-30,])])
DEBUG - RowMutation(table='Mailings', key='schmidtcaroline71@yahoo.com', modifications=[ColumnFamily(User/Users-46-Data.db   : 914432
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-48-Data.db   : 949632
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-50-Data.db   : 984832
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-52-Data.db   : 1020032
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-54-Data.db   : 1055232
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-56-Data.db   : 1090432
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-58-Data.db   : 1125632
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-60-Data.db   : 1160832
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-62-Data.db   : 1196032
DEBUG - index size for bloom filter calc for file  : /nfsassets/mailings/db_tests/cassandra/data/Mailings/Users-64-Data.db   : 1231232
DEBUG - Expected bloom filter size : 1231232
ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.util.NoSuchElementException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logFutureExceptions(DebuggableThreadPoolExecutor.java:95)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor.afterExecute(DebuggableScheduledThreadPoolExecutor.java:50)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.util.NoSuchElementException
        at java.util.StringTokenizer.nextToken(StringTokenizer.java:349)
        at org.apache.cassandra.dht.RandomPartitioner$1.compare(RandomPartitioner.java:56)
        at org.apache.cassandra.dht.RandomPartitioner$1.compare(RandomPartitioner.java:40)
        at org.apache.cassandra.io.FileStruct.compareTo(FileStruct.java:80)
        at org.apache.cassandra.io.FileStruct.compareTo(FileStruct.java:33)
        at java.util.PriorityQueue.siftUpComparable(PriorityQueue.java:599)
        at java.util.PriorityQueue.siftUp(PriorityQueue.java:591)
        at java.util.PriorityQueue.offer(PriorityQueue.java:291)
        at java.util.PriorityQueue.add(PriorityQueue.java:268)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:1113)
        at org.apache.cassandra.db.ColumnFamilyStore.doCompaction(ColumnFamilyStore.java:689)
        at org.apache.cassandra.db.MinorCompactionManager$1.call(MinorCompactionManager.java:165)
        at org.apache.cassandra.db.MinorCompactionManager$1.call(MinorCompactionManager.java:162)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        ... 2 more
","[root@db5 ~]# java -version
java version ""1.6.0""
OpenJDK  Runtime Environment (build 1.6.0-b09)
OpenJDK 64-Bit Server VM (build 1.6.0-b09, mixed mode)
[root@db5 ~]# uname -a
Linux db5.shermanstravelmedia.com 2.6.18-92.1.18.el5 #1 SMP Wed Nov 12 09:19:49 EST 2008 x86_64 x86_64 x86_64 GNU/Linux",,,,,,,,,,,,,,,,,,,13/Oct/09 02:51;jbellis;486.patch;https://issues.apache.org/jira/secure/attachment/12421944/486.patch,13/Oct/09 01:53;elubow;master.txt;https://issues.apache.org/jira/secure/attachment/12421937/master.txt,13/Oct/09 01:53;elubow;part_cas_load.pl;https://issues.apache.org/jira/secure/attachment/12421938/part_cas_load.pl,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,51:08.1,,,no_permission,,,,,,,,,,,,19715,,,Tue Oct 13 15:48:03 UTC 2009,,,,,,0|i0fz87:,91305,,,,,,,,,,,13/Oct/09 01:53;elubow;The master.txt file is an example of the master.txt files that are normally used.  They generally have between 3 and 4 million lines all in that format.  A new one is generated every day.  The part_cas_load.pl Perl script loads up the master.txt line by line and creates an entry per user.,"13/Oct/09 02:51;jbellis;patch that prevents empty keys from being inserted, which is the root cause of the problem",13/Oct/09 15:48;jbellis;committed to 0.4 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error deleting files during bootstrap,CASSANDRA-681,12444942,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,brandon.williams,brandon.williams,1/7/2010 17:44,3/12/2019 14:02,3/13/2019 22:24,1/13/2010 16:06,0.5,,,,0,,,,,,"I started a 3 node cluster and proceeded to bootstrap a 4th node.  On one of the existing nodes I began to see tracebacks like this:

ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.IOException: Unable to delete /mnt/drive3/data/Keyspace1/stream/Standard1-158-Index.db after 10 tries
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:53)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: java.io.IOException: Unable to delete /mnt/drive3/data/Keyspace1/stream/Standard1-158-Index.db after 10 tries
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:13)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        ... 2 more
Caused by: java.io.IOException: Unable to delete /mnt/drive3/data/Keyspace1/stream/Standard1-158-Index.db after 10 tries
        at org.apache.cassandra.io.DeletionService$2.runMayThrow(DeletionService.java:45)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:9)
        ... 6 more

For various data, index, and filter files.","debian lenny amd64 OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",,,,,,,,,,,,,,,,,,,12/Jan/10 20:55;jbellis;681.patch;https://issues.apache.org/jira/secure/attachment/12430045/681.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,00:57.9,,,no_permission,,,,,,,,,,,,19816,,,Wed Jan 13 16:06:47 UTC 2010,,,,,,0|i0g0fb:,91499,,,,,,,,,,,"08/Jan/10 17:06;brandon.williams;I also received this while testing CASSANDRA-680, so it is not limited to bootstrap.",11/Jan/10 20:00;jbellis;were there any other errors (like the ones in CASSANDRA-657) in the logs?,12/Jan/10 20:55;jbellis;it looks like the problem was that Streaming.transferSSTables and StreamManager.finish were both attempting to delete the streamed file.  this patch removes the one from transferSSTables.,"12/Jan/10 23:06;brandon.williams;+1, error no longer appears",13/Jan/10 16:06;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make adaptive heap size calculation portable,CASSANDRA-1507,12474211,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,9/15/2010 20:27,3/12/2019 14:02,3/13/2019 22:24,9/15/2010 20:37,0.7 beta 2,,,,0,,,,,,"The adaptive heap size calculation is dependent on the 'free' tool which is typically available on Linux machines (GNU toolset) but not others.

I'm attaching a patch which makes FreeBSD specifically supported as well as falling back to static 1024m default if the operating system is unrecognized.
",,,,,,,,,,,,,,,,,,,,15/Sep/10 20:35;scode;trunk-1507-v2.txt;https://issues.apache.org/jira/secure/attachment/12454695/trunk-1507-v2.txt,15/Sep/10 20:29;scode;trunk-1507.txt;https://issues.apache.org/jira/secure/attachment/12454694/trunk-1507.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,37:49.5,,,no_permission,,,,,,,,,,,,20170,,,Fri Sep 17 13:55:35 UTC 2010,,,,,,0|i0g5hj:,92319,,,,,,,,,,,15/Sep/10 20:35;scode;Slightly modified patch. I accidentally included the version where FreeBSD was mis-spelled (I used it for testing the fallback case).,"15/Sep/10 20:37;brandon.williams;+1, committed.  Thanks!","17/Sep/10 13:55;hudson;Integrated in Cassandra #538 (See [https://hudson.apache.org/hudson/job/Cassandra/538/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
register AES verbs at SS start,CASSANDRA-717,12445948,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,1/19/2010 17:15,3/12/2019 14:02,3/13/2019 22:24,1/21/2010 21:22,0.6,,,,0,,,,,,"the reason we do all registration in one place is it prevents bugs like this one

ERROR - Error in ThreadPoolExecutor
java.lang.AssertionError: unknown verb TREE-RESPONSE-VERB
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR - Fatal exception in thread Thread[AE-SERVICE-STAGE:1,5,main]
java.lang.AssertionError: unknown verb TREE-RESPONSE-VERB
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
",,,,,,,,,,,,,,,,,,,,13/Jul/10 17:28;messi;0003-EnumMap-StorageService.Verb.patch.txt;https://issues.apache.org/jira/secure/attachment/12449369/0003-EnumMap-StorageService.Verb.patch.txt,19/Jan/10 19:45;jbellis;ASF.LICENSE.NOT.GRANTED--0001-mv-tree-and-gossip-verb-registration-into-StorageServi.txt;https://issues.apache.org/jira/secure/attachment/12430788/ASF.LICENSE.NOT.GRANTED--0001-mv-tree-and-gossip-verb-registration-into-StorageServi.txt,19/Jan/10 19:45;jbellis;ASF.LICENSE.NOT.GRANTED--0002-convert-verbs-to-enums.txt;https://issues.apache.org/jira/secure/attachment/12430789/ASF.LICENSE.NOT.GRANTED--0002-convert-verbs-to-enums.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,59:55.3,,,no_permission,,,,,,,,,,,,19833,,,Wed Jul 14 13:56:17 UTC 2010,,,,,,0|i0g0n3:,91534,,,,,,,,,,,"19/Jan/10 20:59;stuhood;+1... it's an improvement, although if we can find a good excuse to give the components of SS (AES, Gossiper, Streaming) an abstract base class and lifecycle, that would probably be ideal.",21/Jan/10 21:22;jbellis;rebased and committed,"22/Jan/10 12:36;hudson;Integrated in Cassandra #331 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/331/])
    convert verbs to enums
patch by jbellis; reviewed by Stu Hood for 
mv tree and gossip verb registration into StorageService
patch by jbellis; reviewed by Stu Hood for 
","13/Jul/10 17:28;messi;Use EnumMap for Map<StorageService.Verb, IVerbHandler>.",13/Jul/10 19:46;jbellis;committed,"14/Jul/10 13:56;hudson;Integrated in Cassandra #491 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/491/])
    Use EnumMap for verbHandlers.  patch by Folke Behrens; reviewed by jbellis for CASSANDRA-717
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra should chdir / when daemonizing,CASSANDRA-1718,12479400,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,thepaul,thepaul,11/8/2010 19:40,3/12/2019 14:02,3/13/2019 22:24,1/20/2011 16:11,0.7.1,,Packaging,,0,,,,,,"Common practice when daemonizing is to cd / to avoid pinning a filesystem.  For example, if the oper happens to start Cassandra (by itself, or with a manual jsvc invocation, or with the initscript) in /mnt/usb-storage, and there is something mounted there, then the oper will not be able to unmount the usb device that was mounted at that location, since the cassandra process has it open as its cwd.

evidence that this isn't being done already:

{noformat}
~% sudo lsof -p 9775 | awk '$4==""cwd""'
jsvc    9775 cassandra  cwd    DIR                8,1     4096 147675 /home/paul/packages/cassandra/trunk
{noformat}

(That instance was invoked using the Debian initscript.)

Obviously chdir(""/"") isn't necessary when not daemonizing, although it shouldn't hurt either.

If there are concerns about Cassandra having an ongoing ability to open filenames relative to its original working directory, then it should be sufficient just to do a ""cd /"" in the initscript before starting Cassandra.  That case, at least, is particularly important.","Debian squeeze, Cassandra 0.7.0-beta3 and trunk (r1032649)",,,,,,,,,,,,,,,,,,,12/Jan/11 16:32;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1718-switch-to-home-directory-on-startup.txt;https://issues.apache.org/jira/secure/attachment/12468136/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1718-switch-to-home-directory-on-startup.txt,14/Jan/11 20:21;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-1718-chdir-on-startup.txt;https://issues.apache.org/jira/secure/attachment/12468399/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-1718-chdir-on-startup.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,47:14.1,,,no_permission,,,,,,,,,,,,20270,,,Thu Jan 20 16:44:06 UTC 2011,,,,,,0|i0g6yn:,92558,,,,,,,,,,,08/Nov/10 19:47;jbellis;tagging fix-for 0.7.1 because I don't want to risk any more breakage for 0.7.0,"07/Dec/10 17:49;urandom;Huh. I thought jsvc was doing this (and I want to say that it _was_ at some point). It really seems like it ought to be.

bq. If there are concerns about Cassandra having an ongoing ability to open filenames relative to its original working directory, then it should be sufficient just to do a ""cd /"" in the initscript before starting Cassandra. That case, at least, is particularly important.

We should consider it a bug if anything here relies on relative paths (and I don't think it does).","11/Jan/11 20:00;jbellis;Note: this means hprof and err files will be created in /, since they are dumped in CWD.","12/Jan/11 15:58;thepaul;Maybe best would be to chdir() to cassandra's data directory, then. It should be ok to pin that. But definitely we want it to be predictable, not ""whatever directory the admin was in the last time she started it up"".","12/Jan/11 16:27;jbellis;bq. Maybe best would be to chdir() to cassandra's data directory

I like that idea.","12/Jan/11 16:34;urandom;Is there some way to tell the JVM to put its  detritus elsewhere?  Barring that, /var/lib/cassandra is an improvement over, ""wherever"".",12/Jan/11 16:42;thepaul;+1 this patch,"12/Jan/11 16:43;jbellis;bq. Is there some way to tell the JVM to put its detritus elsewhere?

http://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html lists -XX:HeapDumpPath but nothing for a JVM crash log that I see.",14/Jan/11 02:52;thepaul;-XX:ErrorFile ?,14/Jan/11 03:10;jbellis;Bingo.,20/Jan/11 16:11;urandom;Did.,"20/Jan/11 16:44;hudson;Integrated in Cassandra-0.7 #182 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/182/])
    chdir / on startup

Patch by eevans for CASSANDRA-1718
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Debian packaging should auto-detect the JVM, not require OpenJDK",CASSANDRA-1174,12466404,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,codahale,codahale,codahale,6/7/2010 23:22,3/12/2019 14:02,3/13/2019 22:24,6/11/2010 0:47,0.6.3,,,,0,debian,,,,,"The current init.d script for Debian-packaged Cassandra has the OpenJDK's JAVA_HOME hard-coded in, making it impossible to use sun-java6 without modifying the file. Ideally it should use the same sort of auto-detection logic used by other Debian-packaged Java projects to figure out which JVM it should use.

(I have a patch for this that I'll upload shortly.)",,,,,,,,,,,,,,,,,,,,08/Jun/10 01:14;codahale;0001-Use-tomcat6-s-JVM-detection-code-in-the-Debian-init..patch;https://issues.apache.org/jira/secure/attachment/12446555/0001-Use-tomcat6-s-JVM-detection-code-in-the-Debian-init..patch,11/Jun/10 00:30;codahale;02-0001-Use-tomcat6-s-JVM-detection-code-in-the-Debian-init.patch;https://issues.apache.org/jira/secure/attachment/12446820/02-0001-Use-tomcat6-s-JVM-detection-code-in-the-Debian-init.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,20:47.0,,,no_permission,,,,,,,,,,,,20018,,,Fri Jun 11 12:45:40 UTC 2010,,,,,,0|i0g3g7:,91989,,,,,,,,,,,08/Jun/10 01:14;codahale;Adds the JVM detection logic from Debian's tomcat6 package to the Debian init.d script.,"11/Jun/10 00:20;jbellis;We probably don't want to include jdk 1.5 directories on the search path, since we require 1.6","11/Jun/10 00:30;codahale;New patch, which chooses between the Sun JRE and the OpenJDK JRE, preferring Sun.","11/Jun/10 00:47;jbellis;committed

(tested and it works for me against openjdk still, but i'm a little puzzled as to why since it chops off the jre/ part of the old path)","11/Jun/10 12:45;hudson;Integrated in Cassandra #462 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/462/])
    allow using sun jdk w/ debian init script.  patch by Coda Hale; reviewed by jbellis for CASSANDRA-1174
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatabaseDescriptor static initialization circular reference when initialized through call to StorageService.instance.initClient ,CASSANDRA-1756,12480378,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,eonnen,eonnen,11/18/2010 21:58,3/12/2019 14:02,3/13/2019 22:24,11/19/2010 16:31,0.7.0 rc 1,,,,0,,,,,,"In trunk, attempting to invoke StorageService.instance.initClient results in an NPE due to static definition field ordering in StorageService and a circular reference from DatabaseDescriptor back into an uninitialized field (scheduledTasks). Changing the ordering of the static fields such that scheduledTasks is defined before the static partitioner fixes the issue.

I've also marked the scheduledTasks executor as final as it doesn't seem to make sense changing it.

All tests pass with this change locally.

I suspect this hasn't surfaced in tests as calling initServer first in the same JVM will allow later calls to initClient to see the correctly defined scheduledTasks fields.

I'm following the recommended way to do this from ClientOnlyExample, if this isn't the right way to initialize things let me know.
",,,,,,,,,,,,,,,,,,,,19/Nov/10 14:34;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-fix-cicular-initialization-problem-between-StorageServ.txt;https://issues.apache.org/jira/secure/attachment/12460006/ASF.LICENSE.NOT.GRANTED--v1-0001-fix-cicular-initialization-problem-between-StorageServ.txt,19/Nov/10 14:34;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0002-StorageService.initClient-unit-test-that-never-finishe.txt;https://issues.apache.org/jira/secure/attachment/12460007/ASF.LICENSE.NOT.GRANTED--v1-0002-StorageService.initClient-unit-test-that-never-finishe.txt,19/Nov/10 14:34;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0003-DynamicEndpointSnitch-shouldn-t-update-if-SS-isn-t-ini.txt;https://issues.apache.org/jira/secure/attachment/12460008/ASF.LICENSE.NOT.GRANTED--v1-0003-DynamicEndpointSnitch-shouldn-t-update-if-SS-isn-t-ini.txt,18/Nov/10 21:59;eonnen;CS-1756;https://issues.apache.org/jira/secure/attachment/12459948/CS-1756,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,42:29.2,,,no_permission,,,,,,,,,,,,20295,,,Fri Nov 19 20:05:42 UTC 2010,,,,,,0|i0g773:,92596,,,,,,,,,,,18/Nov/10 22:01;eonnen;Attached,"18/Nov/10 22:57;eonnen;I forgot to add the stack trace resulting in the NPE in the original bug, sorry about that.

Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:199)
	at io.eao.cassandra.http.Main.main(Main.java:24)
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:396)
	... 2 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:74)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:403)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:275)","19/Nov/10 13:42;gdusbabek;That patch does fix the circular reference but exposes other problems in the fat client.  I've attached a test case that never finishes.  I suspect over time we've introduced changes that couple a few of the singleton services together in undesirable ways.  

I'll spend some time looking at it this morning and see if I can make sense of it.","19/Nov/10 14:39;gdusbabek;0001 is Erik's patch
0002 is a test case that exposes an bug in the assumption DynamicEndpointSnitch makes about service initialization.
0003 fixes the bug.

Here is the chain of events.
StorageService.initClient() triggers static initializer in DD which instantiates a DynamicEndpointSnitch.  DES, as part of it's constructors schedule some tasks with SS.scheduledTasks which promptly starts executing them.  This causes MessagingService to initialize out of order which tries to access the not-fully-initialized SS, causing deadlock.  

SS cannot finish initializing until MS is done initializing, but MS is waiting on SS to do the same thing, near as I can tell.",19/Nov/10 14:43;jbellis;+1,19/Nov/10 16:31;gdusbabek;committed with a few changes.  It turns out System.exit() in a unit test is not good and I had to change the DES unit test to (rightfully) initialize StorageService.,"19/Nov/10 20:05;hudson;Integrated in Cassandra-0.7 #19 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/19/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json2sstable fails due to OutOfMemory,CASSANDRA-2189,12498991,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,skamio,skamio,2/18/2011 2:53,3/12/2019 14:02,3/13/2019 22:24,12/8/2011 16:39,0.8.9,1.0.6,Legacy/Tools,,0,,,,,,"I have a json file created with sstable2json for a column family of super column type. Its size is about 1.9GB. (It's a dump of all keys because I cannot find out how to specify keys to dump in sstable2json.)
When I tried to create sstable from the json file, it failed with OutOfMemoryError as follows.

 WARN 00:31:58,595 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
Exception in thread ""main"" java.lang.OutOfMemoryError: PermGen space
        at java.lang.String.intern(Native Method)
        at org.codehaus.jackson.util.InternCache.intern(InternCache.java:40)
        at org.codehaus.jackson.sym.BytesToNameCanonicalizer.addName(BytesToNameCanonicalizer.java:471)
        at org.codehaus.jackson.impl.Utf8StreamParser.addName(Utf8StreamParser.java:893)
        at org.codehaus.jackson.impl.Utf8StreamParser.findName(Utf8StreamParser.java:773)
        at org.codehaus.jackson.impl.Utf8StreamParser.parseLongFieldName(Utf8StreamParser.java:379)
        at org.codehaus.jackson.impl.Utf8StreamParser.parseMediumFieldName(Utf8StreamParser.java:347)
        at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName(Utf8StreamParser.java:304)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:140)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:93)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:65)
        at org.codehaus.jackson.map.deser.MapDeserializer._readAndBind(MapDeserializer.java:197)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:145)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:23)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:1261)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:517)
        at org.codehaus.jackson.JsonParser.readValueAs(JsonParser.java:897)
        at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:208)
        at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:197)
        at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:421)

So, what I had to is that split the json file with ""split"" command and modify them to be correct json file. Create sstable for each small json files.

Could you change json2sstable to avoid OutOfMemory?",linux,,3600,3600,,0%,3600,3600,,,,,,,,,,,,07/Dec/11 23:54;jbellis;2189-2.txt;https://issues.apache.org/jira/secure/attachment/12506542/2189-2.txt,18/Feb/11 06:58;jbellis;2189.txt;https://issues.apache.org/jira/secure/attachment/12471370/2189.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,00:27.0,,,no_permission,,,,,,,,,,,,20491,,,Thu Dec 08 17:37:14 UTC 2011,,,,,,0|i0g9v3:,93028,,,,,,,,,,,18/Feb/11 03:00;jbellis;That kind of looks like a jackson bug to me.  I'll ask Tatu.,18/Feb/11 06:58;jbellis;patch to disable interning.  (Thanks to Tatu for pointing me at the right Feature.),"21/Feb/11 21:13;jbellis;Shotaro, can you test the patch?",23/Feb/11 23:28;jbellis;committed,"23/Feb/11 23:46;hudson;Integrated in Cassandra-0.7 #313 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/313/])
    turnoff string interning in json2sstable
patch by jbellis for CASSANDRA-2189
","07/Dec/11 23:46;jbellis;This didn't actually fix the problem.  Tatu again:

""This calls configure on parser; but at that point the symbol table has already been created. So configure must be called on the factory first (and only need to be called once, really), and then settings will be passed as expected.""",07/Dec/11 23:54;jbellis;Patch attached to move configuration to the factory.,"08/Dec/11 16:39;jbellis;Customer testing indicates that this is a big improvement, especially when combined with an upgrade to Jackson 1.9.2.  I'll commit this patch to 0.8.9 and 1.0.6, and upgrade Jackson in trunk.","08/Dec/11 17:37;hudson;Integrated in Cassandra-0.8 #413 (See [https://builds.apache.org/job/Cassandra-0.8/413/])
    turn off string interning in json2sstable, take 2
patch by jbellis; tested by George Ciubotaru for CASSANDRA-2189

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211976
Files : 
* /cassandra/branches/cassandra-0.8
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/SSTableImport.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hyphenated index names cause problems,CASSANDRA-2196,12499061,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,sgpope,sgpope,2/18/2011 16:16,3/12/2019 14:02,3/13/2019 22:24,2/26/2011 1:19,0.7.3,,,,0,,,,,,"When inserting a large number of entries with batch_insert (100000) using thrift compiled into C# there's a NumberFormatException that occurs.

The first logged entry that tipped me off was this:
 INFO 10:53:52,171 Writing Memtable-TransactionLogs.client-hostname@350930888(1171371 bytes, 32787 o
perations)
ERROR 10:53:52,171 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NumberFormatException: For input string: ""tmp""
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NumberFormatException: For input string: ""tmp""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
        at java.lang.Integer.parseInt(Integer.java:449)
        at java.lang.Integer.parseInt(Integer.java:499)
        at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:154)
        at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:119)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:67)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:156)
        at org.apache.cassandra.db.Memtable.access$000(Memtable.java:49)
        at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:174)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more

 Which points to the suspect piece of code in Descriptor.java:154 (browse at https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/Descriptor.java)

 The file I believe it's trying to parse is mentioned in my logs as:

INFO 10:51:31,231 Compacted to C:\cassandra\apache-cassandra-0.7.2\bin\..\Storage\data\system\Index
Info-tmp-f-6-Data.db.  384 to 225 (~58% of original) bytes for 1 keys.  Time: 281ms.

 I'm new here, so I'm not sure what needs fixing here (the filename, or the parsing of it).","Cassandra 0.7.2

Windows 7 64-bit
java version ""1.6.0_23""
Java(TM) SE Runtime Environment (build 1.6.0_23-b05)
Java HotSpot(TM) 64-Bit Server VM (build 19.0-b09, mixed mode)
",,3600,3600,,0%,3600,3600,,,,,,,,,,,,18/Feb/11 19:12;jbellis;2196.txt;https://issues.apache.org/jira/secure/attachment/12471434/2196.txt,25/Feb/11 23:32;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2196-invoke-toString-instead-of-casting.txt;https://issues.apache.org/jira/secure/attachment/12471994/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2196-invoke-toString-instead-of-casting.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,49:39.9,,,no_permission,,,,,,,,,,,,20496,,,Sat Feb 26 02:40:31 UTC 2011,,,,,,0|i0g9wn:,93035,gdusbabek,gdusbabek,,,,,,,,,"18/Feb/11 16:49;jbellis;The bug is that it shouldn't be trying to open a file with ""tmp"" in the name.

I wonder if this is another one of those bugs where we have unix-oriented assumptions about opened files, that don't hold under Windows.  Can you doublecheck that there are no earlier ERROR or WARN lines in the log?","18/Feb/11 16:56;sgpope;No errors before that one, and the only warning before that is:
 WARN 10:50:25,748 Generated random token Token(bytes[c010b410364388921ed82a633849a3cc]). Random tok
ens will result in an unbalanced ring; see http://wiki.apache.org/cassandra/Operations
","18/Feb/11 18:32;jbellis;I see the problem. Your CF is named client-hostname, but - is an illegal character in columnfamily names.  Apparently the code that checks that got broken at some point.  How did you create the CF, through the CLI?",18/Feb/11 18:43;sgpope;I'm confused. My CFs are TransactionLogs and Terms. The CF in the log entry is the system-generated one. I created my CFs in code.,"18/Feb/11 18:52;jbellis;Ah, I thought ""TransactionLogs.client-hostname"" was KS.CF but it must be CF.indexname. ","18/Feb/11 19:00;sgpope;Sorry, yeah. I should've mentioned that client-hostname is one of my indexes.","18/Feb/11 19:12;jbellis;patch to keep invalid characters out of index names.

if you can afford to lose the data the easiest fix for this CF is to drop and recreate it.",18/Feb/11 19:15;sgpope;I can afford to lose it. Thanks!,18/Feb/11 19:28;gdusbabek;+1,"18/Feb/11 20:56;hudson;Integrated in Cassandra-0.7 #293 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/293/])
    validate index namesfor \w+
patch by jbellis; reviewed by gdusbabek for CASSANDRA-2196
",21/Feb/11 21:15;jbellis;committed,"25/Feb/11 23:31;urandom;r1072123 broke the CQL system tests with the following logged exception:

{noformat}
java.lang.ClassCastException: org.apache.avro.util.Utf8 cannot be cast to java.lang.String
        at org.apache.cassandra.db.migration.UpdateColumnFamily.<init>(UpdateColumnFamily.java:52)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:638)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1209)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4576)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:3235)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:188)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

(Trivial )patch attached.",25/Feb/11 23:37;gdusbabek;+1,"26/Feb/11 02:40;hudson;Integrated in Cassandra #746 (See [https://hudson.apache.org/hudson/job/Cassandra/746/])
    invoke toString() instead of casting

Patch by eevans; reviewed by gdusbabek for CASSANDRA-2196
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bug in CollatedOrderPreservingPartitioner.midpoint,CASSANDRA-519,12439223,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,jbellis,jbellis,10/27/2009 21:30,3/12/2019 14:02,3/13/2019 22:24,12/7/2009 21:17,0.5,,,,0,,,,,,"            assert FBUtilities.isEqualBits(MINIMUM.token, rbytes);

fails frequently when running the test added for CASSANDRA-517.  just revert patch 03 and it will fail several times out of 10 test runs (when range3 is a wrapping range, presumably)",,,,,,,,,,,,,,,,,,,,06/Dec/09 02:02;stuhood;519-biginteger-midpoint.diff;https://issues.apache.org/jira/secure/attachment/12427084/519-biginteger-midpoint.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,55:15.8,,,no_permission,,,,,,,,,,,,19733,,,Wed Dec 09 12:42:41 UTC 2009,,,,,,0|i0fzfj:,91338,,,,,,,,,,,"27/Oct/09 21:55;stuhood;IPartitioner.midpoint() doesn't currently require implementations to support wrapping ranges. I'll get to this eventually, but it doesn't look like midpoint is going to be necessary/helpful for 192, like I originally thought.",01/Dec/09 23:30;stuhood;I'll try and get to this one this week.,"06/Dec/09 02:02;stuhood;The implementations of midpoint in RPP and OPP were fairly optimized, but we would have needed to reimplement more of the operations in BigInteger in order to support wrapping ranges.

Instead, this patch reuses the BigInteger based midpoint implementation, and moves it into FBUtilities.midpoint. The partitioners convert their tokens into bit arrays represented as BigIntegers.","06/Dec/09 02:08;stuhood;Er, s/RPP/COPP/ in the previous comment.",07/Dec/09 21:17;jbellis;committed,"09/Dec/09 12:42;hudson;Integrated in Cassandra #282 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/282/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
initMetadata does not load partitioner from disk,CASSANDRA-1638,12477915,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,10/20/2010 20:23,3/12/2019 14:02,3/13/2019 22:24,10/29/2010 13:46,0.6.7,,,,0,,,,,,"The function initMetadata() in org/apache/cassandra/db/SystemTable.java does not add the PARTITIONER constant to columns. So when the NamesQueryFilter runs, it will never pull the value from disk. This makes this portion of the code always null:

        if (partitionerColumn == null)
        {
            Column c = new Column(PARTITIONER, partitioner.getBytes(""UTF-8""), TimestampClock.ZERO);
            cf.addColumn(c);
            logger.info(""Saved partitioner not found. Using "" + partitioner);
        }",,,,,,,,,,,,,,,,,,,,20/Oct/10 20:25;lenn0x;0001-Fixed-initMetadata-to-load-on-disk-PARTITIONER-confi.patch;https://issues.apache.org/jira/secure/attachment/12457706/0001-Fixed-initMetadata-to-load-on-disk-PARTITIONER-confi.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:33.2,,,no_permission,,,,,,,,,,,,20229,,,Fri Oct 29 13:46:07 UTC 2010,,,,,,0|i0g6gn:,92477,,,,,,,,,,,22/Oct/10 20:58;gdusbabek;+1,"29/Oct/10 13:46;gdusbabek;Committed. 

0.7 and trunk didn't need changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor cleanup & unused code removal,CASSANDRA-56,12422132,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,4/6/2009 15:47,3/12/2019 14:02,3/13/2019 22:24,4/6/2009 17:56,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,06/Apr/09 16:25;jbellis;0001-replace-.indexOf-1-with-.contains.patch;https://issues.apache.org/jira/secure/attachment/12404746/0001-replace-.indexOf-1-with-.contains.patch,06/Apr/09 16:25;jbellis;0002-r-m-redundant-or-unused-code.patch;https://issues.apache.org/jira/secure/attachment/12404747/0002-r-m-redundant-or-unused-code.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,47:48.4,,,no_permission,,,,,,,,,,,,19528,,,Mon Apr 06 17:56:56 UTC 2009,,,,,,0|i0fwlr:,90880,,,,,,,,,,,"06/Apr/09 16:47;johanoskarsson;+1 for patch 0001, much clearer.
+1 for 0002 too, but personally I don't mind explicitly setting variables to null. ColumnFamilyStore:955 is incorrectly indented, but it was like that before this patch too.",06/Apr/09 17:56;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loadbalance during gossip issues leaves cluster in bad state,CASSANDRA-1895,12493949,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,stuhood,stuhood,12/23/2010 7:31,3/12/2019 14:02,3/13/2019 22:24,1/16/2011 23:54,0.7.1,,,,0,,,,,,Running loadbalance against a node in a 4 node cluster leaves gossip in a wonky state.,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,03/Jan/11 22:39;brandon.williams;1895.txt;https://issues.apache.org/jira/secure/attachment/12467374/1895.txt,23/Dec/10 07:33;stuhood;logs.tgz;https://issues.apache.org/jira/secure/attachment/12466861/logs.tgz,23/Dec/10 07:33;stuhood;ring-views.txt;https://issues.apache.org/jira/secure/attachment/12466862/ring-views.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,31:16.8,,,no_permission,,,,,,,,,,,,20360,,,Mon Jan 17 00:37:15 UTC 2011,,,,,,0|i0g82n:,92738,,,,,,,,,,,23/Dec/10 07:33;stuhood;Attaching logs and {{nodetool ring}} output from after running loadbalance against 10.97.29.122.,"23/Dec/10 15:31;jbellis;trunk, or 0.7 branch?",23/Dec/10 17:56;nickmbailey;this looks very similar to CASSANDRA-1895,23/Dec/10 18:04;nickmbailey;It also looks like CASSANDRA-1829 :),"23/Dec/10 18:24;stuhood;This run is from trunk. I'm traveling today, but I should be able to try it against 0.7 when I get settled this evening.",23/Dec/10 19:59;brandon.williams;I'm unable to repro against 0.7.,"23/Dec/10 20:00;tjake;Based on the ring and .122 log it looks like you were missing fix for CASSANDRA-1829

If see this in the log: "" INFO [RMI TCP Connection(2)-10.97.29.122] 2010-12-22 01:33:47,328 StorageService.java (line 249) Bootstrap/move completed! Now serving reads.""

Then the local ring state would say Normal since finishBootstrap() calls setToken() which sets the ring state to normal. hmm.
","23/Dec/10 20:03;tjake;In fact, based on the log message above I can see you didn't have the fix in palace since the line is now 250 after the fix for the above message:

http://svn.apache.org/viewvc/cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java?r1=1043262&r2=1044034","25/Dec/10 06:34;stuhood;It looks like the patch you mentioned was only partially applied to trunk. The last chunk in the patch should have added {code}+        if(!bootstrapped)
+            setToken(token);{code}
...but didn't.","25/Dec/10 06:39;nickmbailey;Stu,

That  if check was removed in a later commit. The current code will call setToken() no matter what. The only difference is if the node bootstrapped, setToken will be called twice but that shouldn't be a problem and I don't think that is causing the bug here.","28/Dec/10 07:58;stuhood;I think the burden of proof is back on me now, so I'll try and reproduce this once I'm back in the office tomorrow.","29/Dec/10 02:24;stuhood;After a rebase and changes to the EC2 images I was using, I'm no longer able to reproduce this against either 0.7 or trunk.

Nick noticed some gossip related problems in the attached logs, so I'm going to chalk this up to _either_ a bad rebase, or problems related to bootstrapping during gossip problems. Nick: could you chime in with details, and whether you think those gossip issues might be worth pursuing?","29/Dec/10 19:34;nickmbailey;I just noticed that the logs on one of the machines contains a ""Removing token"" line followed by a new node line 3 times. Since Stu only called loadbalance once, I believe this is likely due to gossip issues, where a node completes the decom then gets gossip about the same node leaving and readds it.

I've also talked to one other person in irc who had a similar problem.  Every time a decommission happened, the node would get removed but added 30 seconds later (gossip timeout after removal) due to some other node in the cluster.  ","29/Dec/10 21:33;gdusbabek;We've had this problem before.  I think it was CASSANDRA-1467, but not sure.",30/Dec/10 20:40;stuhood;Is this the sort of problem where having a gossip generation might help? The node that readded an older node should have ignored the outdated gossip.,30/Dec/10 21:55;nickmbailey;We have generations for node states. The problem is after 30 seconds we assume gossip has propagated and forget about nodes that have been removed.  Then when we see the gossip again it looks like a brand new node.,30/Dec/10 22:10;jbellis;sounds like the inverse of CASSANDRA-1730,03/Jan/11 22:39;brandon.williams;Revival of my first try at CASSANDRA-1730: change the quarantine interval for justRemovedEndpoints to RING_DELAY * 2.,"03/Jan/11 23:03;nickmbailey;Would it be a good idea to make this configurable? I've seen nodes that have failed to process a REMOVED state for up to 45 minutes, although that was one extreme case. It might be useful in a case like that to temporarily configure this very high in order to process ring operations when the cluster is having issues otherwise.","03/Jan/11 23:08;brandon.williams;I'm guessing in the 45 minute case, what was happening was the state was being re-gossiped just outside of RING_DELAY many times, but finally it propagated fast enough.  I think the next step, if this isn't sufficient, is to expose RING_DELAY as a tunable.",15/Jan/11 05:11;jbellis;would it make more sense to change FatClientTimeout to QUARANTINE_DELAY / 2?,"16/Jan/11 18:47;brandon.williams;It wouldn't hurt, though we established in CASSANDRA-1730 the timeout was rather arbitrary.  It could help in the case of a 'flapping' bootstrap, though I tend to think there will be problems there in any case.",16/Jan/11 23:10;jbellis;let's commit w/ that change then,16/Jan/11 23:54;brandon.williams;Committed w/fatclient timeout change.,"17/Jan/11 00:37;hudson;Integrated in Cassandra-0.7 #165 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/165/])
    Set quarantine delay to RING_DELAY * 2
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1895
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Indexes: CF MBeans for automatic indexes are never unregistered when they are deleted.,CASSANDRA-1843,12492991,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,12/10/2010 23:33,3/12/2019 14:02,3/13/2019 22:24,12/11/2010 0:06,0.7.0 rc 3,,,,0,,,,,,"Add, delete, and add the same index and you should get a stacktrace to this effect:
{noformat}
java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=IndexColumnFamilies,keyspace=Keyspace1,columnfamily=Standard1.616765
  at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:259)
  at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:447)
  at org.apache.cassandra.db.ColumnFamilyStore.addIndex(ColumnFamilyStore.java:304)
  at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:193)
  at org.apache.cassandra.db.migration.UpdateColumnFamily.applyModels(UpdateColumnFamily.java:80)
  at org.apache.cassandra.db.migration.Migration.apply(Migration.java:171)
  at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:663)
  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
  at java.util.concurrent.FutureTask.run(FutureTask.java:138)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:662)
Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=IndexColumnFamilies,keyspace=Keyspace1,columnfamily=Standard1.616765
  at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:453)
  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1484)
  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:963)
  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)
  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)
  at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)
  at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:255)
  ... 11 more{noformat}
CFS.reload() manages index deletion, but never unregisters the MBeans it creates during initialization.",,,,,,,,,,,,,,,,,,,,11/Dec/10 00:03;jhermes;1843.txt;https://issues.apache.org/jira/secure/attachment/12466029/1843.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,06:34.5,,,no_permission,,,,,,,,,,,,20341,,,Mon Dec 13 20:16:15 UTC 2010,,,,,,0|i0g7r3:,92686,,,,,,,,,,,"11/Dec/10 00:03;jhermes;Someone already wrote this method and just forgot to call it.
Whoops.",11/Dec/10 00:06;brandon.williams;Committed.,11/Dec/10 05:13;jbellis;updated CHANGES,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ","13/Dec/10 20:16;hudson;Integrated in Cassandra #625 (See [https://hudson.apache.org/hudson/job/Cassandra/625/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfBoundException in StorageService.getAllRanges ,CASSANDRA-933,12460730,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,riffraff,riffraff,riffraff,3/30/2010 19:07,3/12/2019 14:02,3/13/2019 22:24,4/2/2010 13:20,0.6.1,,,,0,,,,,,"this was seen on 0.6-beta3 but it appears to be in trunk too, given the same code for getAllRanges. The problem appeared while accessing a bootstraping node via nodetool, giving the following stacktrace

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.get(ArrayList.java:324)
        at java.util.Collections$UnmodifiableList.get(Collections.java:1154)
        at org.apache.cassandra.service.StorageService.getAllRanges(StorageService.java:1133)
        at org.apache.cassandra.service.StorageService.getRangeToAddressMap(StorageService.java:440)
        at org.apache.cassandra.service.StorageService.getRangeToEndPointMap(StorageService.java:431)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
...


Basically, no test is performed for non-emptyness of the input token list. If such list should never be empty, I guess this should be explicit in the interface/javadoc, otherwise I'm attaching a patch & testcase (pretty silly code, but the test passes :) ) ",,,600,600,,0%,600,600,,,,,,,,,,,,30/Mar/10 19:09;riffraff;CASSANDRA-933.patch;https://issues.apache.org/jira/secure/attachment/12440265/CASSANDRA-933.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,20:11.5,,,no_permission,,,,,,,,,,,,19923,,,Fri Apr 02 13:20:11 UTC 2010,,,,,,0|i0g1z3:,91750,,,,,,,,,,,30/Mar/10 19:09;riffraff;simply return empty output on empty input. ,02/Apr/10 13:20;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodeprobe flush_binary using Order Preserving Partitioner,CASSANDRA-467,12436976,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,lenn0x,lenn0x,10/1/2009 2:35,3/12/2019 14:02,3/13/2019 22:24,10/1/2009 2:55,0.5,,,,0,,,,,,"DecoratedKey.toString is trying to be called on null.

Exception:

        at org.apache.cassandra.db.DecoratedKey.toString(DecoratedKey.java:91)
        at org.apache.cassandra.db.BinaryMemtable.writeSortedContents(BinaryMemtable.java:145)
        at org.apache.cassandra.db.ColumnFamilyStore$3$1.run(ColumnFamilyStore.java:950)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",,,,,,,,,,,,,,,,,,,,01/Oct/09 02:54;jbellis;467-2.patch;https://issues.apache.org/jira/secure/attachment/12420978/467-2.patch,01/Oct/09 02:53;jbellis;467.patch;https://issues.apache.org/jira/secure/attachment/12420977/467.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,55:31.1,,,no_permission,,,,,,,,,,,,19703,,,Thu Oct 01 12:34:51 UTC 2009,,,,,,0|i0fz47:,91287,,,,,,,,,,,"01/Oct/09 02:55;jbellis;from irc

> dispalt: jbellis: works now thanks","01/Oct/09 12:34;hudson;Integrated in Cassandra #214 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/214/])
    don't use token in decoration except for RandomPartioner.  patch by jbellis for ; tested by dispalt
make token, key in DecoratedKey public final in accordance with style guide.
patch by jbellis for ; tested by dispalt
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe_keyspace fails on the system table,CASSANDRA-481,12437635,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,brandon.williams,brandon.williams,10/8/2009 18:43,3/12/2019 14:02,3/13/2019 22:24,10/8/2009 22:31,0.5,,,,0,,,,,,"When the thrift call describe_keyspace is called on the system table, an NPE is raised:

ERROR - Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:174)
        at org.apache.cassandra.service.Cassandra$describe_keyspace_result.write(Cassandra.java:11109)
        at org.apache.cassandra.service.Cassandra$Processor$describe_keyspace.process(Cassandra.java:959)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:627)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
","debian lenny amd64, openjdk 6b11-9.1",,,,,,,,,,,,,,,,,,,08/Oct/09 19:40;jbellis;481.patch;https://issues.apache.org/jira/secure/attachment/12421653/481.patch,08/Oct/09 18:46;brandon.williams;stub_system_keyspace.patch;https://issues.apache.org/jira/secure/attachment/12421648/stub_system_keyspace.patch,08/Oct/09 18:44;brandon.williams;test_describe_keyspace.patch;https://issues.apache.org/jira/secure/attachment/12421647/test_describe_keyspace.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,40:57.9,,,no_permission,,,,,,,,,,,,628,,,Fri Oct 09 12:35:07 UTC 2009,,,,,,0|i0fz73:,91300,,,,,,,,,,,08/Oct/09 18:43;brandon.williams;Patch to test describe_keyspace,08/Oct/09 18:46;brandon.williams;Patch that stubs out the system table with fake values to prevent the NPE.  I do not know what the real values should be.,"08/Oct/09 19:40;jbellis;this patch incorporates Brandon's test and replaces the undocumented CF attribute aliases with a single human-readable ""comment"" attribute.",08/Oct/09 21:24;euphoria;jbellis's latest patch looks good to me and works + handy feature.  Why the commenting of the assertions? They both make sense and pass for me.,"08/Oct/09 21:27;jbellis;oops, assertions should have been left in.","08/Oct/09 21:33;euphoria;k, +1 on commit with assertions",08/Oct/09 22:31;jbellis;committed w/ asserts restored,"09/Oct/09 12:35;hudson;Integrated in Cassandra #222 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/222/])
    r/m undocumented term aliases in favor of a single 'comment' field.
patch by jbellis and Brandon Williams; reviewed by Michael Greene for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool cfstats does not update after adding new cfs through API,CASSANDRA-1385,12471485,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,arya,arya,8/13/2010 2:41,3/12/2019 14:02,3/13/2019 22:24,8/18/2010 19:16,0.7 beta 2,,,,0,,,,,,Start a 3 node cluster. Add a new Keyspace with API. Then add more CFs to that Keyspace. ndoetool cfstats will only show you the CF which was originally part of KsDef creation and not the CfDefs that were added later.,"CentOS 5.2
Trunc",,,,,,,,,,,,,,,,,,,18/Aug/10 16:26;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-make-CFS-constructor-responsible-for-registering-mbean.txt;https://issues.apache.org/jira/secure/attachment/12452414/ASF.LICENSE.NOT.GRANTED--v1-0001-make-CFS-constructor-responsible-for-registering-mbean.txt,18/Aug/10 17:03;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0001-make-CFS-responsible-for-registering-unregistering-mbe.txt;https://issues.apache.org/jira/secure/attachment/12452422/ASF.LICENSE.NOT.GRANTED--v3-0001-make-CFS-responsible-for-registering-unregistering-mbe.txt,18/Aug/10 19:11;gdusbabek;ASF.LICENSE.NOT.GRANTED--v4-0001-make-CFS-responsible-for-registering-unregistering-mbe.txt;https://issues.apache.org/jira/secure/attachment/12452436/ASF.LICENSE.NOT.GRANTED--v4-0001-make-CFS-responsible-for-registering-unregistering-mbe.txt,18/Aug/10 19:11;gdusbabek;ASF.LICENSE.NOT.GRANTED--v4-0002-remove-underscores-from-CFS-members.txt;https://issues.apache.org/jira/secure/attachment/12452437/ASF.LICENSE.NOT.GRANTED--v4-0002-remove-underscores-from-CFS-members.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,28:55.6,,,no_permission,,,,,,,,,,,,20114,,,Sat Aug 21 11:14:33 UTC 2010,,,,,,0|i0g4qf:,92197,,,,,,,,,,,"18/Aug/10 16:28;jbellis;do we want the ""hidden"" index CFSes exposed as mbeans?  if we do we probably want them in a different place in the jmx heirarchy to avoid confusion.","18/Aug/10 16:46;gdusbabek;They weren't previously, but I think it makes good sense to expose them.  It would be easy to segregate them by mbean name if we assume that any CFS with a LocalPartitioner is an internal and should be grouped in ""org.apache.cassandra.db:type=InternalCFS"" or something like that.  The other (more ugly) approach would be to use a flag to indicate hidden/internal CFSs.",18/Aug/10 16:49;jbellis;+1 LocalPartitioner approach,"18/Aug/10 18:16;jbellis;are the ""if (mbs.isRegistered(nameObj))"" checks actually necessary or just old code?

should we take this opportunity to s/ColumnFamilyStores/ColumnFamilies/ (and IndexCFS/IndexColumnFamlies) in the mbean names?","18/Aug/10 19:12;gdusbabek;If 'if' was old code.  

I also took the liberty of removing the underscores from member variables.",18/Aug/10 19:14;jbellis;+1,18/Aug/10 19:16;gdusbabek;fixed.,"21/Aug/10 11:14;hudson;Integrated in Cassandra #518 (See [https://hudson.apache.org/hudson/job/Cassandra/518/])
    CHANGES.txt for CASSANDRA-1385
remove underscores from CFS members. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1385
make CFS responsible for registering/unregistering mbeans. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1385
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion failure in MerkleTree,CASSANDRA-639,12443671,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,gdusbabek,gdusbabek,12/17/2009 18:46,3/12/2019 14:02,3/13/2019 22:24,12/18/2009 15:10,0.5,,,,0,,,,,,I'm running three nodes (I'll attach storage.conf).  I have a simple test that writes 100 rows of 100 cols each (15-20 byte names and values).  The assertion in MerkleTree.inc() is consistently failing.,,,,,,,,,,,,,,,,,,,,17/Dec/09 20:48;stuhood;639-off-by-one.diff;https://issues.apache.org/jira/secure/attachment/12428344/639-off-by-one.diff,17/Dec/09 20:23;stuhood;639-off-by-one.diff;https://issues.apache.org/jira/secure/attachment/12428342/639-off-by-one.diff,17/Dec/09 20:09;gdusbabek;stacktrave.txt;https://issues.apache.org/jira/secure/attachment/12428341/stacktrave.txt,17/Dec/09 18:48;gdusbabek;storage-conf.xml;https://issues.apache.org/jira/secure/attachment/12428330/storage-conf.xml,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,50:31.0,,,no_permission,,,,,,,,,,,,19794,,,Fri Dec 18 15:10:30 UTC 2009,,,,,,0|i0g067:,91458,,,,,,,,,,,17/Dec/09 18:48;gdusbabek;I should also point out that the insertions are coming from another node running in client-only mode:,17/Dec/09 18:50;stuhood;What version of Cassandra are you running?,17/Dec/09 19:01;gdusbabek;trunk.,17/Dec/09 20:09;gdusbabek;Here is a stack trace.,17/Dec/09 20:23;stuhood;Hey Gary: I think this is just an off-by-one error. Can you confirm with the attached patch?,"17/Dec/09 20:35;gdusbabek;+1 
That solved it. 

",17/Dec/09 20:40;jbellis;Is this something that we can add a test for?,"17/Dec/09 20:48;stuhood;Add assertion to constructor, to catch the problem immediately.",18/Dec/09 15:10;jbellis;committed to 0.5 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper misses first updates when restarting a node,CASSANDRA-515,12439113,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,10/26/2009 21:36,3/12/2019 14:02,3/13/2019 22:24,10/27/2009 15:29,0.5,,,,0,,,,,,"Easy way to reproduce:

Start node A.
Start node B, with autobootstrap=false.
Kill B, wipe data dir, and restart (still w/ autobootstrap=false).

A will show B as down, with its old token.  (B will see both nodes correctly.)

This appears to be because when you wipe data dir, generation restarts at 1.  (This is not just operator error; besides during testing, this could arise if a node dies completely and has to be replaced.)  Then gossip state is ignored until the new heartbeat is larger than the one previously reached.

It appears that initializing the generation to seconds-since-epoch would fix this.",,,,,,,,,,,,,,,,,,,,26/Oct/09 22:23;jbellis;515.patch;https://issues.apache.org/jira/secure/attachment/12423258/515.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,37:26.5,,,no_permission,,,,,,,,,,,,19731,,,Tue Oct 27 15:29:40 UTC 2009,,,,,,0|i0fzen:,91334,,,,,,,,,,,26/Oct/09 22:23;jbellis;as described.,26/Oct/09 22:37;urandom;Looks good. +1,"27/Oct/09 12:34;hudson;Integrated in Cassandra #240 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/240/])
    initialize generation to seconds-since-epoch, a value much more likely to be unique than ""1""
patch by jbellis; reviewed by eevans for 
",27/Oct/09 15:29;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix few minor problems in nodeprobe cfstats,CASSANDRA-646,12443829,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,rrabah,rrabah,12/19/2009 1:52,3/12/2019 14:02,3/13/2019 22:24,1/13/2010 16:46,0.5,,Legacy/Tools,,0,,,,,,"nodeprobe cfstats reports that readlatency/writelatency is NaN on the keyspace level although it obviously is not.

For example:
Keyspace: Keyspace1
        Read Count: 392
        Read Latency: NaN ms.
        Write Count: 262
        Write Latency: NaN ms.
        Pending Tasks: 0

                Column Family: MyCF
                Memtable Columns Count: 143
                Memtable Data Size: 123433
                Memtable Switch Count: 2
                Read Count: 392
                Read Latency: 0.533 ms.
                Write Count: 262
                Write Latency: 0.000 ms.
                Pending Tasks: 0

                Column Family: Standard2
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 0
                Read Latency: NaN ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0

The problem here is that there is more than one cf, and one of them has read latency/writelatency NaN. This causes the keyspace readlatency/writelatency to be NaN instead of the average across all cfs. 

Another problem with cfstats is that it does not account for the delays when a read/write times out, so it does not accurately reflect the health of the system under too much stress. 
",,,,,,,,,,,,,,,,,,,,12/Jan/10 20:45;jbellis;646-05.patch;https://issues.apache.org/jira/secure/attachment/12430042/646-05.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,57:55.3,,,no_permission,,,,,,,,,,,,19798,,,Wed Jan 13 16:46:48 UTC 2010,,,,,,0|i0g07r:,91465,,,,,,,,,,,"19/Dec/09 01:57;jbellis;If it times out, there is no data.  Making something up would be nonsensical.

Remember, StorageProxy is the core of the fat client, as well as the server routing.","19/Dec/09 02:44;jbellis;sorry, cfstats is not the StorageProxy stats.  cfstats _does_ include full time, even for operations that another node gives up on.","19/Dec/09 19:52;rrabah;If we want readLatency to mean only read from disk time, but not include network delay time can we at least log the exception on the server when a timedout exception happens (like we used to in version 0.4). The reason I mention this is that we had some major TimedOutExceptions being thrown, and the system was suffering badly, but the server logs and cfstats showed everything to be perfectly running fine. It's only when we dug into the client logs that we started noticing that. It makes monitoring the health of the system harder, when you have many connected clients to the cassandra servers, and you need to look at each of their logs separately. ",19/Dec/09 22:00;jbellis;You could do that from the SP side.,"20/Dec/09 16:16;rrabah;Fair enough. Should I log a separate enhancement to log the TimedOutException in the SP side, and leave this bug to fix NaN for a Keyspace in the presence of multiple cfs?",09/Jan/10 19:02;jbellis;Sure.,"12/Jan/10 20:45;jbellis;patch to fix NaNs.  applies to 0.5 (does not apply to trunk, I will fix that on merge)",13/Jan/10 16:40;gdusbabek;+1,13/Jan/10 16:46;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception during batch_mutate,CASSANDRA-834,12457340,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,brandon.williams,brandon.williams,2/24/2010 20:34,3/12/2019 14:02,3/13/2019 22:24,3/23/2010 15:10,0.6,,,,0,,,,,,"If a batch mutation is sent with deletions referring to a SCF but no SC is specified in the Deletion object, the following traceback is generated:

ERROR 15:28:16,746 Fatal exception in thread Thread[ROW-MUTATION-STAGE:22,5,main]
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.Column cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.Column cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:300)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:284)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:87)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:73)
        at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:329)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:341)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:314)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:270)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:200)
        at org.apache.cassandra.service.StorageProxy$3.runMayThrow(StorageProxy.java:282)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
","debian lenny amd64 OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",,,,,,,,,,,,,,,,,,,24/Feb/10 23:20;brandon.williams;834-test.patch;https://issues.apache.org/jira/secure/attachment/12436924/834-test.patch,22/Mar/10 22:08;jbellis;834-v2.txt;https://issues.apache.org/jira/secure/attachment/12439515/834-v2.txt,01/Mar/10 21:19;brandon.williams;834.patch;https://issues.apache.org/jira/secure/attachment/12437520/834.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,24:00.3,,,no_permission,,,,,,,,,,,,19885,,,Tue Mar 23 15:10:12 UTC 2010,,,,,,0|i0g1d3:,91651,,,,,,,,,,,24/Feb/10 23:20;brandon.williams;System test to reproduce.,01/Mar/10 21:19;brandon.williams;Patch to validate column paths in a slice predicate.,"01/Mar/10 22:24;jbellis;hmm...

shouldn't supercolumn==null mean ""apply this predicate to top level supercolumns?""  that is what we do in get_slice for instance.

this will be important when we add deletion of ranges, since there's no other way to specify ""a range of supercolumns"" than in the predicate.","22/Mar/10 22:08;jbellis;Patch taking the 2nd approach, of promoting columns to supercolumns when SC is null in a super CF.",23/Mar/10 13:59;brandon.williams;+1,23/Mar/10 15:10;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BRAF read can loop infinitely instead of detecting EOF,CASSANDRA-2241,12499622,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,2/24/2011 18:53,3/12/2019 14:02,3/13/2019 22:24,2/25/2011 15:00,0.7.3,,,,0,,,,,,"(marking this Minor since normally we never try to read past the end of an SSTable, but CASSANDRA-2240 is running into it.)",,,21600,21600,,0%,21600,21600,,,,,,,,,,,,24/Feb/11 18:59;jbellis;2241.txt;https://issues.apache.org/jira/secure/attachment/12471862/2241.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,32:53.4,,,no_permission,,,,,,,,,,,,19345,,,Fri Feb 25 15:15:13 UTC 2011,,,,,,0|i0ga73:,93082,tjake,tjake,,,,,,,,,"24/Feb/11 18:59;jbellis;Adds tests for this problem and similar ones around EOF behavior.

Fixes bug and makes some changes to make BRAF more straightforward:

- ByteBuffer buffer -> byte[] (no more confusion between ""current"" and buffer's internal markers)

- Instead of bufferEnd (easy to confuse with bufferOffset + buffer.length), use validBufferBytes

- removes hitEOF

- gives read() the behavior of readAtMost(), which was the behavior read() is *supposed* to have

- avoid allocating new byte[1] for each call to write(int)

- adds asserts for expected internal state

- fixes length() when we seek to an earlier position that is still inside the current buffer",24/Feb/11 19:00;jbellis;(At least some of these are regressions introduced by CASSANDRA-1470.),"24/Feb/11 20:32;tjake;I don't see anything here that is a problem, but I'll run it through some workloads...","25/Feb/11 01:27;tjake;Ok, I ran this through some more tests and it looks good to me +1",25/Feb/11 15:00;jbellis;committed,"25/Feb/11 15:15;hudson;Integrated in Cassandra-0.7 #321 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/321/])
    fix BufferedRandomAccessFile bugs
patch by jbellis; reviewed by tjake for CASSANDRA-2241
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
typo in conf/storage-conf.xml: BinaryMemtableSizeInMB is defined twice,CASSANDRA-557,12440738,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,tuxracer69,tuxracer69,11/15/2009 10:19,3/12/2019 14:02,3/13/2019 22:24,11/15/2009 17:46,0.5,,,,0,,,,,,"current code at revision   836346


<!--
   ~ The threshold size in megabytes the binary memtable must grow to,
   ~ before it's submitted for flushing to disk.
  -->
  <BinaryMemtableSizeInMB>256</BinaryMemtableSizeInMB>

  <!--
   ~ The threshold size in megabytes the binary memtable must grow to, before it's submitted for flushing to disk.
  -->
  <BinaryMemtableSizeInMB>256</BinaryMemtableSizeInMB>
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,46:37.2,,,no_permission,,,,,,,,,,,,19755,,,Mon Nov 16 12:34:28 UTC 2009,,,,,,0|i0fznz:,91376,,,,,,,,,,,15/Nov/09 17:46;urandom;Fixed in trunk. Thanks for the report!,"16/Nov/09 12:34;hudson;Integrated in Cassandra #260 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/260/])
    remove duplicated configuration directive

Patch by eevans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include bootstrap targets in consistencylevel computations,CASSANDRA-497,12438347,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,10/16/2009 22:40,3/12/2019 14:02,3/13/2019 22:24,10/21/2009 14:18,0.5,,,,0,,,,,,"We want to preserve ConsistencyLevel semantics during bootstrap, and if a CL.quorum/all write is sent to a bootstrap target, it would have to wait for the forwarded write to complete to report in turn that the write is good.

Leaving write propagation to be done by the write originator means we don't have this extra layer of latency.
",,,,,,,,,,,,,,,,,,,,17/Oct/09 02:01;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-497-rename-nodePicker-replicationStrategy.txt;https://issues.apache.org/jira/secure/attachment/12422423/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-497-rename-nodePicker-replicationStrategy.txt,17/Oct/09 02:01;jbellis;ASF.LICENSE.NOT.GRANTED--0002-make-write-targets-computable-independent-of-replicati.txt;https://issues.apache.org/jira/secure/attachment/12422424/ASF.LICENSE.NOT.GRANTED--0002-make-write-targets-computable-independent-of-replicati.txt,17/Oct/09 02:01;jbellis;ASF.LICENSE.NOT.GRANTED--0003-Make-EndPoint-objects-immmutable-so-hashcode-can-t-ch.txt;https://issues.apache.org/jira/secure/attachment/12422425/ASF.LICENSE.NOT.GRANTED--0003-Make-EndPoint-objects-immmutable-so-hashcode-can-t-ch.txt,20/Oct/09 21:58;sandeep_tata;imports.patch;https://issues.apache.org/jira/secure/attachment/12422731/imports.patch,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,55:13.5,,,no_permission,,,,,,,,,,,,19722,,,Thu Oct 22 12:34:39 UTC 2009,,,,,,0|i0fzan:,91316,,,,,,,,,,,"16/Oct/09 22:43;jbellis;patch:

make write targets computable independent of replication strategy (i.e., make getReadStorageEndPoints the only method a Strategy needs to implement).  we do this by computing the token/endpoint -> Range[] map from the read endpoints, then using that to determine if a bootstrapping node needs to receive a write (if the token being written falls in any of its ranges).  Also, make the StorageProxy insert methods aware that bootstrap can entail having extra write targets temporarily, and include those in its consistencylevel calculations.","20/Oct/09 21:55;sandeep_tata;1. Making getReadStorageEndPoints the only method a replicationstrategy needs to implement is a nice move. Looks good.

2. I see the value in including bootstrapping nodes in the blockFor calculations. Reads are never directed to bootstrapping nodes, so the app should be insulated from having to reason about bootstrap for its consistency guarantees. The current logic doesn't guarantee that: determineBlockFor will need to be fixed.

You need to block for (#bootstrapping nodes + #determined as if there were no bootstrapping nodes).

For instance, if there's one node that's being bootstrapped in the range of interest, ConsistencyLevel.ONE should block for *2* not *1* so that a read is guaranteed to hit at least one copy.

ALL will work correctly. QUORUM will work correctly for cases where only 1 node is being bootstrapped for this range, 

",20/Oct/09 21:58;sandeep_tata;Had to add Set and HashSet to imports to get it to build .. probably because patch missed a hunk?,"20/Oct/09 22:06;jbellis;Thanks for taking a look!

> Reads are never directed to bootstrapping nodes, so the app should be insulated from having to reason about bootstrap for its consistency guarantees. The current logic doesn't guarantee that

Yes it does, because reads just use getReadStorageEndPoints, which correctly ignores bootstrap-in-progress.  

determineBlockFor just says ""given a set of nodes we're writing [or reading, but actually read just hardcodes N / 2 + 1 right now] to, how many do we need to form a quorum.""  So you'd control it by giving it the right set of nodes to reason about.

> Had to add Set and HashSet to imports to get it to build .. probably because patch missed a hunk? 

Sorry about that -- trunk has moved a bunch since that was generated.","20/Oct/09 22:35;sandeep_tata;> Yes it does, because reads just use getReadStorageEndPoints, which correctly ignores bootstrap-in-progress. 

Which is exactly why it might violate consistency guarantees. Here's an example:

Key k lives on A, B, C. Node  N is being bootstrapped for this range.

write(k, ConsistencyLevel.ONE) : blocks for 1 response, say this was from Node N.

read(k, ConsistencyLevel.ONE) --> gets data from one replica, say A. Node A might not yet have gotten the previous write (only N did): this violates the consistency semantics.","20/Oct/09 22:42;jbellis;Ah, I see what you mean now.  You're right.

So we need to block on write for quorum(normal) + bootstrap, not quorum(normal + bootstrap)","20/Oct/09 22:42;jbellis;or rather, blockfor(quorum) + bootstrap","20/Oct/09 22:54;sandeep_tata;
>or rather, blockfor(quorum) + bootstrap 

Exactly. blockFor(readEndPoints) + bootstrap instead of blockFor(writeEndPoints)",21/Oct/09 14:18;jbellis;committed w/ above modification,"22/Oct/09 12:34;hudson;Integrated in Cassandra #235 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/235/])
    Make EndPoint objects immmutable, so hashcode can't change (making the EndPoint potentially disappear from a map)
patch by jbellis; reviewed by Sandeep Tata for 
make write targets computable independent of replication strategy (i.e., make getReadStorageEndPoints the only method a Strategy needs to implement).  we do this by computing the token/endpoint -> Range[] map from the read endpoints, then using that to determine if a bootstrapping node needs to receive a write (if the token being written falls in any of its ranges).  Also, make the StorageProxy insert methods aware that bootstrap can entail having extra write targets temporarily, and include those in its consistencylevel calculations.
patch by jbellis; reviewed by Sandeep Tata for 
rename nodePicker -> replicationStrategy.
patch by jbellis; reviewed by Sandeep Tata for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avronateSubcolumns was assuming an avro array.,CASSANDRA-1454,12473158,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,9/2/2010 16:39,3/12/2019 14:02,3/13/2019 22:24,9/2/2010 16:47,0.7 beta 2,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,02/Sep/10 16:39;gdusbabek;ASF.LICENSE.NOT.GRANTED--v0-0001-avronateSubcolumns-was-assuming-avro-array.txt;https://issues.apache.org/jira/secure/attachment/12453693/ASF.LICENSE.NOT.GRANTED--v0-0001-avronateSubcolumns-was-assuming-avro-array.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,41:52.5,,,no_permission,,,,,,,,,,,,20147,,,Thu Sep 02 16:41:52 UTC 2010,,,,,,0|i0g55r:,92266,,,,,,,,,,,02/Sep/10 16:41;urandom;lgtm; +1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Old package name in Cql.g,CASSANDRA-42,12421818,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,permellqvist,permellqvist,permellqvist,4/1/2009 20:59,3/12/2019 14:02,3/13/2019 22:24,4/1/2009 22:52,,,,,0,,,,,,ANTLR file Cql.g contains old (com.facebook) package name,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,01/Apr/09 21:01;permellqvist;diff.txt;https://issues.apache.org/jira/secure/attachment/12404379/diff.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,52:25.1,,,no_permission,,,,,,,,,,,,19526,,,Wed Apr 01 22:52:25 UTC 2009,,,,,,0|i0fwin:,90866,,,,,,,,,,,"01/Apr/09 21:01;permellqvist;Patch (generated by 'svn diff build.xml' on revision 760988) 
Fixes package names in Cql.g",01/Apr/09 22:52;jbellis;applied!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant clean target should really clean,CASSANDRA-19,12419676,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,3/27/2009 20:04,3/12/2019 14:02,3/13/2019 22:24,3/27/2009 20:53,,,,,0,,,,,,"The ant clean target leaves build/cassandra.jar in place, and shouldn't. Technically, it should delete the entire build/ directory. ",,,,,,,,,,,,,,,,,,,,27/Mar/09 20:06;urandom;really-clean.patch;https://issues.apache.org/jira/secure/attachment/12403838/really-clean.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,41:56.3,,,no_permission,,,,,,,,,,,,19518,,,Fri Mar 27 20:41:56 UTC 2009,,,,,,0|i0fwe7:,90846,,,,,,,,,,,"27/Mar/09 20:06;urandom;This trivial patch will do, (although it would probably be better to <delete dir=""${build.dir}""/>).",27/Mar/09 20:41;jbellis;changed to removing entire dir as suggested,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Example session in README is missing semicolons,CASSANDRA-1782,12481001,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,justinazoff,justinazoff,justinazoff,11/26/2010 23:01,3/12/2019 14:02,3/13/2019 22:24,11/27/2010 2:18,0.7.0 rc 2,,Legacy/Documentation and Website,,0,,,,,,The example session in the README is missing the semicolons at the end of each command.,,,,,,,,,,,,,,,,,,,,26/Nov/10 23:03;justinazoff;cassandra-1782.patch;https://issues.apache.org/jira/secure/attachment/12460521/cassandra-1782.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,18:41.3,,,no_permission,,,,,,,,,,,,20311,,,Sat Nov 27 02:18:41 UTC 2010,,,,,,0|i0g7d3:,92623,jbellis,jbellis,,,,,,,,,26/Nov/10 23:03;justinazoff;Patch that adds the missing semicolons.,"27/Nov/10 02:18;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Insert, Delete and Insert into a column family doesnt work... ",CASSANDRA-703,12445593,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,vijay2win@yahoo.com,vijay2win@yahoo.com,1/15/2010 4:35,3/12/2019 14:02,3/13/2019 22:24,1/29/2010 23:47,0.5,,,,0,,,,,,"Here is the code to reproduce the issue...

        ColumnPath colpath = new ColumnPath().setColumn_family(""VERSIONS"").setSuper_column(""123"".getBytes()).setColumn(""1234"".getBytes());
        con.insert(""WBXCDOCUMENT"", ""vijay"", colpath, ""test"".getBytes(), System.currentTimeMillis(), 2);

        ColumnPath path = new ColumnPath().setColumn_family(""VERSIONS"").setSuper_column(""123"".getBytes());
        con.remove(""WBXCDOCUMENT"", ""vijay"", path, System.currentTimeMillis(), 2);

        con.insert(""WBXCDOCUMENT"", ""vijay"", colpath, ""test"".getBytes(), System.currentTimeMillis(), 2);

        ColumnOrSuperColumn col = con.get(""WBXCDOCUMENT"", ""vijay"", path, 2);
        assertEquals(col.getSuper_column().getColumns() != null, true);

Expected result, get the column family..... but it throws notfound exception which is wrong.","Linux, Cassandra .5",,,,,,,,,,,,,,,,,,,26/Jan/10 19:46;jbellis;703-05.txt;https://issues.apache.org/jira/secure/attachment/12431453/703-05.txt,26/Jan/10 19:31;jbellis;703-trunk.txt;https://issues.apache.org/jira/secure/attachment/12431451/703-trunk.txt,15/Jan/10 21:53;vijay2win@yahoo.com;bug-fix-703.txt;https://issues.apache.org/jira/secure/attachment/12430444/bug-fix-703.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,12:09.2,,,no_permission,,,,,,,,,,,,19826,,,Fri Jan 29 23:47:47 UTC 2010,,,,,,0|i0g0k7:,91521,,,,,,,,,,,"15/Jan/10 21:53;vijay2win@yahoo.com;Suggesting this changes....  This might need the users to flush the data to disk.... and then use it.... is this incompatable because the seralizer, deserializer is changed... we might need to add one more field timestamp.... not sure if it is the right way to do it.

1) Added the timestamp to the constructor for the supercolumn.
2) Added the timestamp to serializer deserializer
3) change the classes dependent on it to fix the initialization
4) When new col is added the recent chang timestamp is updated.

Thanks
Vijay","25/Jan/10 21:12;jbellis;just to rule out the obvious, did you test using timestamps 0, 1, 2 instead of System.currentTimeMillis()?  if the remove takes less than 1ms, the following insert will have the same timestamp and the remove will take precedence (ties go to tombstone).","25/Jan/10 21:28;vijay2win@yahoo.com;Hi Jonathan, yes actually i did the following...

first insert
System.currentTimeMillis() 

first delete
System.currentTimeMillis()  + 5

secound insert
System.currentTimeMillis()  + 10

        String supcol = ""12356"";
        Client con = ConnectionCacheUtils.getinstance().getCassandraConnection().getclient();
        ColumnPath colpath = new ColumnPath().setColumn_family(""VERSIONS"").setSuper_column(supcol.getBytes()).setColumn(""1234"".getBytes());
        con.insert(""WBXCDOCUMENT"", ""vijay"", colpath, ""test"".getBytes(), System.currentTimeMillis(), 2);
        
        ColumnPath path = new ColumnPath().setColumn_family(""VERSIONS"").setSuper_column(supcol.getBytes());
        ColumnOrSuperColumn col = con.get(""WBXCDOCUMENT"", ""vijay"", path, 2);
        assertEquals(col.getSuper_column().getColumns() != null, true);
        
        con.remove(""WBXCDOCUMENT"", ""vijay"", path, System.currentTimeMillis() + 5, 2);
        con.insert(""WBXCDOCUMENT"", ""vijay"", colpath, ""test"".getBytes(), System.currentTimeMillis() + 10, 2);
        
        ColumnOrSuperColumn col2 = con.get(""WBXCDOCUMENT"", ""vijay"", path, 2);
        assertEquals(col2.getSuper_column().getColumns() != null, true);

Regards
Vijay","26/Jan/10 16:33;jbellis;I can reproduce with this system test:

    def test_vijay(self):
        key = 'vijay'
        client.insert('Keyspace1', key, ColumnPath('Super1', 'sc1', _i64(4)), 'value4', 0, ConsistencyLevel.ONE)

        client.remove('Keyspace1', key, ColumnPath('Super1', 'sc1'), 1, ConsistencyLevel.ONE)

        client.insert('Keyspace1', key, ColumnPath('Super1', 'sc1', _i64(4)), 'value4', 2, ConsistencyLevel.ONE)

        result = client.get('Keyspace1', key, ColumnPath('Super1', 'sc1'), ConsistencyLevel.ONE)
        assert result.super_column.columns is not None, result.super_column
",26/Jan/10 19:31;jbellis;the internals are fine; the bug is in turning the data from the internal representation into Thrift objects.  patch attached.,26/Jan/10 19:46;jbellis;0.5 version of patch,"26/Jan/10 20:30;vijay2win@yahoo.com;+1, tested and works... thanks Jonathan..",29/Jan/10 23:47;jbellis;committed to 0.5 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error reading key until first use of the HTTP interface,CASSANDRA-156,12424927,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,markr,markr,5/8/2009 12:55,3/12/2019 14:02,3/13/2019 22:24,5/12/2009 20:47,0.3,,,,0,,,,,,"After startup, but before the first access to the HTTP interface, thrift command get_slice returns the following error:

./Cassandra-remote -h tst04o:9160 get_slice Messages 305 base 0 1
Traceback (most recent call last):
  File ""./Cassandra-remote"", line 96, in ?
    pp.pprint(client.get_slice(args[0],args[1],args[2],eval(args[3]),eval(args[4]),))
  File ""/opt/mailcontrol/gen-py/org/apache/cassandra/Cassandra.py"", line 213, in get_slice
    return self.recv_get_slice()
  File ""/opt/mailcontrol/gen-py/org/apache/cassandra/Cassandra.py"", line 233, in recv_get_slice
    raise x
thrift.Thrift.TApplicationException: Internal error processing get_slice

Error message on the log file:

ERROR [pool-1-thread-1] 2009-05-08 14:49:36,977 Cassandra.java (line 823) Internal error processing get_slice
java.lang.RuntimeException: error reading key 305
        at org.apache.cassandra.service.StorageProxy.weakReadRemote(StorageProxy.java:256)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:363)
        at org.apache.cassandra.service.CassandraServer.readColumnFamily(CassandraServer.java:112)
        at org.apache.cassandra.service.CassandraServer.get_slice(CassandraServer.java:191)
        at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:817)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:805)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:252)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.concurrent.TimeoutException: Operation timed out.
        at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:95)
        at org.apache.cassandra.service.StorageProxy.weakReadRemote(StorageProxy.java:252)
        ... 9 more

After first access to the HTTP interface, the get_slice method now succeeds.","java version ""1.6.0_13"" Linux tst04o 2.6.18-128.1.6.el5 #1 SMP Wed Apr 1 09:10:25 EDT 2009 x86_64 x86_64 x86_64 GNU/Linux
",,,,,,,,,,,,,,,,,,,08/May/09 17:19;jbellis;156.patch;https://issues.apache.org/jira/secure/attachment/12407642/156.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,57:08.3,,,no_permission,,,,,,,,,,,,19573,,,Wed May 13 09:26:38 UTC 2009,,,,,,0|i0fx7r:,90979,,,,,,,,,,,08/May/09 14:57;jbellis;how many nodes are you running?  can you reproduce this at will on a fresh start after wiping /var/cassandra/* ?,"08/May/09 16:38;markr;Thread dump as requested

Full thread dump Java HotSpot(TM) 64-Bit Server VM (11.3-b02 mixed mode):

""pool-1-thread-2"" prio=10 tid=0x00002aaafc0e6800 nid=0x33bb waiting on condition [0x0000000041804000..0x0000000041804b00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab24e2680> (a java.util.concurrent.SynchronousQueue$TransferStack)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:422)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:857)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""pool-1-thread-1"" prio=10 tid=0x00002aaafc0dc000 nid=0x33ba waiting on condition [0x0000000041703000..0x0000000041703a80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab24e2680> (a java.util.concurrent.SynchronousQueue$TransferStack)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:422)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:857)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""GMFD:1"" prio=10 tid=0x00002aaafc0da000 nid=0x33b9 waiting on condition [0x00000000457be000..0x00000000457bea00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2422968> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""Timer-1"" prio=10 tid=0x00002aaafc0d8800 nid=0x33b8 in Object.wait() [0x00000000456bd000..0x00000000456bdd80]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab23fcdf8> (a java.util.TaskQueue)
        at java.util.TimerThread.mainLoop(Timer.java:509)
        - locked <0x00002aaab23fcdf8> (a java.util.TaskQueue)
        at java.util.TimerThread.run(Timer.java:462)

""Timer thread for monitoring AnalyticsContext"" daemon prio=10 tid=0x00002aaafc0d7000 nid=0x33b7 in Object.wait() [0x00000000455bc000..0x00000000455bcd00]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00002aaab23b7480> (a java.util.TaskQueue)
        at java.util.TimerThread.mainLoop(Timer.java:509)
        - locked <0x00002aaab23b7480> (a java.util.TaskQueue)
        at java.util.TimerThread.run(Timer.java:462)

""UDP Selector Manager"" prio=10 tid=0x00002aaafc0d5800 nid=0x33b6 runnable [0x00000000454bb000..0x00000000454bbc80]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        - locked <0x00002aaab23a0398> (a sun.nio.ch.Util$1)
        - locked <0x00002aaab23a0380> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00002aaab23a0020> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        at org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:92)

""TCP Selector Manager"" prio=10 tid=0x00002aaafc0d0800 nid=0x33b5 runnable [0x0000000041f54000..0x0000000041f54c00]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        - locked <0x00002aaab2397380> (a sun.nio.ch.Util$1)
        - locked <0x00002aaab2397368> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00002aaab2396fe8> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        at org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:92)

""HINTED-HANDOFF-POOL:1"" prio=10 tid=0x00002aaafc0bf400 nid=0x33b4 waiting on condition [0x00000000453ba000..0x00000000453bab80]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab22f6748> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1963)
        at java.util.concurrent.DelayQueue.take(DelayQueue.java:164)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:583)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:576)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MINOR-COMPACTION-POOL:1"" prio=10 tid=0x00002aaafc0be400 nid=0x33b3 waiting on condition [0x00000000452b9000..0x00000000452b9b00]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab22dc660> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1963)
        at java.util.concurrent.DelayQueue.take(DelayQueue.java:164)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:583)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:576)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-HintsColumnFamily10:1"" prio=10 tid=0x00002aaafc0bb000 nid=0x33b2 waiting on condition [0x00000000451b8000..0x00000000451b8a80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab226f360> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-StandardByTime29:1"" prio=10 tid=0x00002aaafc0b9800 nid=0x33b1 waiting on condition [0x00000000450b7000..0x00000000450b7a00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab226ae38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-TableMetadata8:1"" prio=10 tid=0x00002aaafc0b8000 nid=0x33b0 waiting on condition [0x0000000044fb6000..0x0000000044fb6d80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2266918> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-StandardByTime17:1"" prio=10 tid=0x00002aaafc0b3000 nid=0x33af waiting on condition [0x0000000044eb5000..0x0000000044eb5d00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2262400> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-LocationInfo6:1"" prio=10 tid=0x00002aaafc0b1800 nid=0x33ae waiting on condition [0x0000000044db4000..0x0000000044db4c80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab225df20> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-base5:1"" prio=10 tid=0x00002aaafc0afc00 nid=0x33ad waiting on condition [0x0000000044cb3000..0x0000000044cb3c00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2259ae8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-Super24:1"" prio=10 tid=0x00002aaafc0ae000 nid=0x33ac waiting on condition [0x0000000044bb2000..0x0000000044bb2b80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2255680> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-extra3:1"" prio=10 tid=0x00002aaafc0ac800 nid=0x33ab waiting on condition [0x0000000044ab1000..0x0000000044ab1b00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2251188> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-Super12:1"" prio=10 tid=0x00002aaafc0abc00 nid=0x33aa waiting on condition [0x00000000449b0000..0x00000000449b0a80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab224cd20> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MEMTABLE-POOL-RecycleColumnFamily1:1"" prio=10 tid=0x00002aaafc0a8400 nid=0x33a9 waiting on condition [0x00000000448af000..0x00000000448afa00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab223e860> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""HTTP-REQUEST:1"" prio=10 tid=0x00002aaafc0a1400 nid=0x33a8 waiting on condition [0x00000000447ae000..0x00000000447aed80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2159ed8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MAP-REDUCE-STAGE:4"" prio=10 tid=0x00002aaafc09f800 nid=0x33a7 waiting on condition [0x00000000446ad000..0x00000000446add00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2157b70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MAP-REDUCE-STAGE:3"" prio=10 tid=0x00002aaafc09e000 nid=0x33a6 waiting on condition [0x00000000445ac000..0x00000000445acc80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2157b70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MAP-REDUCE-STAGE:2"" prio=10 tid=0x00002aaafc09c400 nid=0x33a5 waiting on condition [0x00000000444ab000..0x00000000444abc00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2157b70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""MAP-REDUCE-STAGE:1"" prio=10 tid=0x00002aaafc09a800 nid=0x33a4 waiting on condition [0x00000000443aa000..0x00000000443aab80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2157b70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:8"" prio=10 tid=0x00002aaafc098c00 nid=0x33a3 waiting on condition [0x00000000442a9000..0x00000000442a9b00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:7"" prio=10 tid=0x00002aaafc097400 nid=0x33a2 waiting on condition [0x00000000441a8000..0x00000000441a8a80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:6"" prio=10 tid=0x00002aaafc095800 nid=0x33a1 waiting on condition [0x00000000440a7000..0x00000000440a7a00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:5"" prio=10 tid=0x00002aaafc093c00 nid=0x33a0 waiting on condition [0x0000000043fa6000..0x0000000043fa6d80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:4"" prio=10 tid=0x00002aaafc092000 nid=0x339f waiting on condition [0x0000000043ea5000..0x0000000043ea5d00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:3"" prio=10 tid=0x00002aaafc090c00 nid=0x339e waiting on condition [0x0000000043da4000..0x0000000043da4c80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:2"" prio=10 tid=0x00002aaafc08f800 nid=0x339d waiting on condition [0x0000000043ca3000..0x0000000043ca3c00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-READ-STAGE:1"" prio=10 tid=0x00002aaafc08e400 nid=0x339c waiting on condition [0x0000000043ba2000..0x0000000043ba2b80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2154ca0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-MUTATION-STAGE:4"" prio=10 tid=0x00002aaafc08c800 nid=0x339b waiting on condition [0x0000000043aa1000..0x0000000043aa1b00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2152838> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-MUTATION-STAGE:3"" prio=10 tid=0x00002aaafc08ac00 nid=0x339a waiting on condition [0x00000000439a0000..0x00000000439a0a80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2152838> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-MUTATION-STAGE:2"" prio=10 tid=0x00002aaafc089400 nid=0x3399 waiting on condition [0x000000004389f000..0x000000004389fa00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2152838> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""ROW-MUTATION-STAGE:1"" prio=10 tid=0x00002aaafc087800 nid=0x3398 waiting on condition [0x000000004379e000..0x000000004379ed80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab2152838> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""CONSISTENCY-MANAGER:4"" prio=10 tid=0x00002aaafc085c00 nid=0x3397 waiting on condition [0x000000004369d000..0x000000004369dd00]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab21505d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""CONSISTENCY-MANAGER:3"" prio=10 tid=0x00002aaafc084400 nid=0x3396 waiting on condition [0x000000004359c000..0x000000004359cc80]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaab21505d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)

""CONSISTENCY-MANAGER:2"" prio=10 tid=0x00002aaafc082800 nid=0x3395 waiting on condition [0x000000004349b000..0x000000004349bc00]
   java.lang.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memtable Flush writers doesn't actually flush in parallel,CASSANDRA-2178,12498818,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,2/17/2011 1:45,3/12/2019 14:01,3/13/2019 22:24,2/17/2011 2:44,0.7.3,0.8 beta 1,,,0,,,,,,"The flushWriter JMXEnabledThreadPoolExecutor sets the core pool min to 1, and sets the LBQ to DatabaseDescriptor.getFlushWriters(). Increasing memtable_flush_writers should allow us to flush more in parallel. The pool will not grow until LBQ fills up to DatabaseDescriptor.getFlushWriters(). ",,,,,,,,,,,,,,,,,,,,17/Feb/11 02:35;lenn0x;0001-Set-the-core-min-pool-size-to-memtable_flush_writers-v2.patch;https://issues.apache.org/jira/secure/attachment/12471245/0001-Set-the-core-min-pool-size-to-memtable_flush_writers-v2.patch,17/Feb/11 01:47;lenn0x;0001-Set-the-core-min-pool-size-to-memtable_flush_writers.patch;https://issues.apache.org/jira/secure/attachment/12471239/0001-Set-the-core-min-pool-size-to-memtable_flush_writers.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,08:01.5,,,no_permission,,,,,,,,,,,,20482,,,Tue Mar 15 18:12:00 UTC 2011,,,,,,0|i0g9sn:,93017,,,,,,,,,,,"17/Feb/11 02:08;jbellis;Wouldn't setting {core=1, max=flush_writers, queue=SynchronousQueue) be better since it allows the pool size to adjust as needed?",17/Feb/11 02:35;lenn0x;Use a SynchronousQueue instead,17/Feb/11 02:41;jbellis;+1,17/Feb/11 02:44;lenn0x;commited to 0.7 and merged into trunk,"17/Feb/11 05:40;hudson;Integrated in Cassandra-0.7 #283 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/283/])
    Use a SynchronousQueue instead of LBQ so when memtable_flush_writers is > 1, it will allow actual parallel flushes. 
patch by goffinet reviewed by jbellis for CASSANDRA-2178
",15/Mar/11 18:12;jbellis;see CASSANDRA-2333 for related work.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Exception when run ""get Keyspace1.Standard1"" command in the CLI",CASSANDRA-1059,12463896,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,jamel.essoussi,jamel.essoussi,5/6/2010 16:26,3/12/2019 14:01,3/13/2019 22:24,5/24/2010 16:05,0.6.2,,Legacy/Tools,,0,bug,cassandra,cassandra-cli,cli,get,"Connected to: ""Test Cluster"" on 10.0.3.44/9160
cassandra> get Keyspace1.Standard1
line 0:-1 mismatched input '<EOF>' expecting '['
Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:331)
        at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:74)
        at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:213)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:270)",Linux,,,,,,,,,,,,,,,,,,,21/May/10 21:02;urandom;0001-input-errors-causes-cli-to-exit-w-AssertionErrors.txt;https://issues.apache.org/jira/secure/attachment/12445199/0001-input-errors-causes-cli-to-exit-w-AssertionErrors.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,32:32.5,,,no_permission,,,,,,,,,,,,19979,,,Mon May 24 16:05:08 UTC 2010,,,,,,0|i0g2qv:,91875,,,,,,,,,,,"06/May/10 16:32;jbellis;this means you gave it an invalid command (you can't ""get"" an entire CF at once) and it didn't know how to give you a human-readable error from that.  ","21/May/10 21:02;urandom;The attached patch replaces the assert for {{get}}, {{set}}, {{del}}, and {{count}} with a return so that fat fingering a command isn't fatal.",24/May/10 15:57;jbellis;+1,24/May/10 16:05;urandom;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SSTableExport can not accept -f , -k options",CASSANDRA-766,12455377,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,santal,santal,2/5/2010 1:43,3/12/2019 14:01,3/13/2019 22:24,2/10/2010 19:50,0.6,,Legacy/Tools,,0,,,,,,"the SSTableExport command can not accept -f , -k options correct, always said as bellow:

[root@hfdevcasda01 bin]# ./sstable2json -f out.json /opt/cassandra-wbx/data/CONTENT_HF/ChangeHistory-2-Data.db
You must supply exactly one sstable
Usage: org.apache.cassandra.tools.SSTableExport [-f outfile] <sstable> [-k key [-k key [...]]]",Linux,,60,60,,0%,60,60,,,,,,,,,,,,10/Feb/10 19:09;jbellis;766-v2.txt;https://issues.apache.org/jira/secure/attachment/12435475/766-v2.txt,10/Feb/10 17:45;jbellis;766.txt;https://issues.apache.org/jira/secure/attachment/12435471/766.txt,05/Feb/10 01:51;santal;issue766.patch;https://issues.apache.org/jira/secure/attachment/12434916/issue766.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,45:43.1,,,no_permission,,,,,,,,,,,,19855,,,Wed Feb 17 17:54:46 UTC 2010,,,,,,0|i0g0xz:,91583,,,,,,,,,,,05/Feb/10 01:44;santal;just remove one extra line will resolve this problem.,05/Feb/10 01:46;santal;patch submit,"05/Feb/10 01:51;santal;my bad, create a wrong patch file","10/Feb/10 17:45;jbellis;-f was supposed to be removed but I did it poorly.  this patch finishes the job and uses log4j of WARN,stderr for bin/ tools so that doesn't get in the way of redirecting stdout.",10/Feb/10 19:09;jbellis;v2 now with log4j-tools goodness,10/Feb/10 19:31;urandom;+1,10/Feb/10 19:50;jbellis;committed,"17/Feb/10 17:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing null check for READ-REPAIR header,CASSANDRA-109,12423783,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,permellqvist,permellqvist,permellqvist,4/25/2009 18:54,3/12/2019 14:01,3/13/2019 22:24,4/25/2009 19:16,0.3,,,,0,,,,,,"The READ-REPAIR header is assumed to always be present which is no longer the case. This leads to a non-critical  NullPointerException in ReadVerbHandler:109 

String repair = new String( message.getHeader(ReadCommand.DO_REPAIR) );

A null check is needed.",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,25/Apr/09 19:03;permellqvist;null_check.patch;https://issues.apache.org/jira/secure/attachment/12406449/null_check.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,13:56.8,,,no_permission,,,,,,,,,,,,19552,,,Sat Apr 25 19:13:56 UTC 2009,,,,,,0|i0fwxj:,90933,,,,,,,,,,,25/Apr/09 19:02;permellqvist;Patch adds null check to ReadVerbHandler when looking for READ-REPAIR header,25/Apr/09 19:03;permellqvist;null check patch,"25/Apr/09 19:13;jbellis;committed, w/ minor variation to avoid deserializing unnecessarily",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[multi_]get_count should take a SlicePredicate,CASSANDRA-744,12446705,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,jbellis,jbellis,1/26/2010 17:53,3/12/2019 14:01,3/13/2019 22:24,4/20/2010 14:53,0.7 beta 1,,,,0,,,,,,"both to make it more flexible, and to emphasize that counting ""everything"" is as bad as slicing it",,,,,,,,,,,,,,,,,,,,17/Apr/10 14:55;slebresne;ASF.LICENSE.NOT.GRANTED--0001-Add-SlicePredicate-to-get_count.patch;https://issues.apache.org/jira/secure/attachment/12442050/ASF.LICENSE.NOT.GRANTED--0001-Add-SlicePredicate-to-get_count.patch,17/Apr/10 14:55;slebresne;ASF.LICENSE.NOT.GRANTED--0002-Add-mutliget_count.patch;https://issues.apache.org/jira/secure/attachment/12442051/ASF.LICENSE.NOT.GRANTED--0002-Add-mutliget_count.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,55:05.4,,,no_permission,,,,,,,,,,,,19849,,,Tue May 11 10:25:57 UTC 2010,,,,,,0|i0g0t3:,91561,,,,,,,,,,,"17/Apr/10 14:55;slebresne;Got some time to kill, so I'm proposing two (quite trivial) patches.
First one add the SlicePredicate to get_count, second one add
a multiget_count.",20/Apr/10 14:49;gdusbabek;+1,20/Apr/10 14:53;gdusbabek;committed.  Thanks Sylvain!,"08/May/10 23:10;bendiken;Would it be possible to specify multiget_count in terms of i64 return values instead of i32, pretty please?","09/May/10 00:19;jbellis;that's a little silly, isn't it?  surely counting over 2B columns isn't a good idea in the first place...","10/May/10 20:43;bendiken;Is it? I apologize if I have too high expectations for Cassandra, but we've already used rows with up to one hundred million columns. Problems at that scale have been mostly GC churn and compaction related, and since it appears that such issues are being worked on (CASSANDRA-1014, CASSANDRA-16, and the like), a mere one order of magnitude more doesn't seem like *too* much of a stretch for Cassandra to eventually handle on sufficiently big iron.","11/May/10 08:56;slebresne;That's not really the problem of having row with more than MAX_INTEGER columns. What is silly
is counting those. Counting all the columns in a row is the same than reading the whole row. Excepted
that you don't send all those columns over the network. So a call to count will never need a i64 as any 
call that would need an i64 will very likely timeout. 
That's the ""raison d'être"" of this patch. You can count a huge row by paging (but it's probably a better
idea to not count those huge row at all if you can afford it as even with paging this is expensive).","11/May/10 09:54;bendiken;I guess it depends on your use case. We have one where each Cassandra row represents a very large set, each column name being a 20-byte SHA-1 binary hash identifying an object in that set and each such column's value being simply the empty string. As I mentioned, we've stored up to a hundred million columns per row in this manner. As each SHA-1 column takes 35.5 bytes of space in the SStables, that's a total of less than 4 gigs of disk storage for a row with 100 million columns. On the big iron we've run this on, these are not _inherently_ infeasible numbers. The limiting factor is Cassandra's implementation, not the hardware.

Counting the number of objects in a given set (i.e. the number of columns in a given row) is an important operation for us. It's fine for the count to take a while, as it is still vastly (many, many orders of magnitude) faster than the infeasible alternative of directly counting the source data (also stored in Cassandra, but apart) that the set data is derived from, which would (prior to your multi_get_count patch, which does alleviate it a little) involve performing an individual get_count operation for each of hundreds of millions (soon to be billions) of distinct source rows.

Now, given existing GC and SStable compaction issues that we've run into with Cassandra 0.6, we're in practice now manually sharding the larger sets into multiple rows of a size that Cassandra has less issues dealing with (on our hardware, up to 15-20 million columns per row is performing very well).

t expect that as Cassandra evolves and issues are fixed, we can keep upping this, and I don't see anything inherently ridiculous about rows of the size I've mentioned. It seems a little shortsighted to place incidental limits on the protocol, but then again I suppose the protocol will have broken backwards compatibility a couple of times by the time I get around to testing 2 billion columns with some future Cassandra 1.x version - so perhaps we can revisit this in a year or two ;-)
","11/May/10 10:25;slebresne;But then again, I do not even contest the fact that it could be useful to have row with billions of columns. As you mentioned, 
there is a few limitations to that today but they will hopefully be lifted soon enough. I still think that, despite these current limitations,  
the sharding  of the row you already do is useful at some point (but maybe this point is 1 billion columns in your case) if only for the 
sake of load distribution. But that's not my point at all.

My point is that the time it takes to perform one given individual get_count() operation that count n columns is as long as the time it takes 
to read those n columns (from server-side at least, the only advantage of get_count() over get_slice() is that you don't send those n columns 
over the network). So, if n > 2 billion, it will takes a bit of time to perform this one get_count() operation, even in a year or two, even with
super duper SSD drives and even if each column is quite small. I haven't tried (I don't have a super duper SSD drive and I don't live one 
or two year from now) and it's always dangerous to make assumption in the future, but I bet it will take far time than any reasonable 
timeout you would want to set for your Cassandra operations. 

Hence, the right way to count a row with 2 billions+ columns is to do multiple get_count() operations using a predicate to limit the number 
of counted columns by each individual get_count() operation and sum all those results client side. But then only the sum needs be a 64 bit 
integer, not the result of get_count().   ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect hostname parsing in cli,CASSANDRA-100,12423634,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,4/23/2009 22:46,3/12/2019 14:01,3/13/2019 22:24,4/24/2009 3:02,,,,,0,,,,,,"The command line client refuses to parse valid hostnames, (like those containing hyphens), while accepting some illegal ones, (like those with underscores).
",,,,,,,,,,,,,,,,,,,,23/Apr/09 22:54;urandom;0001-better-hostname-parsing-for-cli.txt;https://issues.apache.org/jira/secure/attachment/12406298/0001-better-hostname-parsing-for-cli.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,02:16.4,,,no_permission,,,,,,,,,,,,19550,,,Fri Apr 24 03:02:16 UTC 2009,,,,,,0|i0fwvj:,90924,,,,,,,,,,,"23/Apr/09 22:54;urandom;I have never worked with an antlr grammar file before (and I hope I never do again), so there might be a better way... but the attached patch Works For Me.",24/Apr/09 03:02;jbellis;applied,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
more missing svn properties,CASSANDRA-429,12435016,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,9/7/2009 15:27,3/12/2019 14:01,3/13/2019 22:24,9/7/2009 19:32,0.4,0.5,,,0,,,,,,"Per http://www.mail-archive.com/general@incubator.apache.org/msg22473.html

svn ps svn:eol-style CRLF bin/cassandra.bat
svn ps svn:mime-type application/octet-stream lib/libthrift-r808609.jar
svn ps svn:mime-type application/octet-stream lib/slf4j-api-1.5.8.jar
svn ps svn:mime-type application/octet-stream lib/slf4j-log4j12-1.5.8.jar
svn ps svn:eol-style native src/java/org/apache/cassandra/client/RingCache.java
svn ps svn:eol-style native
test/unit/org/apache/cassandra/client/TestRingCache.java
svn ps svn:eol-style native
test/unit/org/apache/cassandra/db/marshal/BytesTypeTest.java
svn ps svn:eol-style native
test/unit/org/apache/cassandra/dht/CollatingOrderPreservingPartitionerTest.java
svn ps svn:eol-style native
test/unit/org/apache/cassandra/dht/PartitionerTestCase.java

Likewise, I think this means updating http://wiki.apache.org/cassandra/HowToContribute to include configuring in auto-props:

*.jar = svn:mime-type application/octet-stream
*.bar = svn ps svn:eol-style CRLF bin/cassandra.bat


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,15:18.4,,,no_permission,,,,,,,,,,,,19685,,,Mon Sep 07 19:32:07 UTC 2009,,,,,,0|i0fyvj:,91248,,,,,,,,,,,"07/Sep/09 17:15;jbellis;my fault, I was missing enable-auto-props :-|",07/Sep/09 19:32;urandom;SVN properties and wiki documentation updated.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deleting and re-inserting row causes error in get_slice count parameter,CASSANDRA-920,12460296,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,bflorian,bflorian,3/25/2010 22:52,3/12/2019 14:01,3/13/2019 22:24,4/12/2010 21:14,0.6.1,,,,0,,,,,,"I've found that when I delete an entire row in a column family with super columns, and then re-insert values with the same row and super column keys, the count parameter to the get_slice call no longer works properly.  Its like it is still counting the deleted columns, but only returning the new columns.

The following example uses the Ruby Cassandra client (see link below), but I've seen the same behavior with the Java Thrift interface.

Test code:
--------------
require 'rubygems'
require 'cassandra'
cc = Cassandra.new('Keyspace1')
cc.insert(:Super1,'test-key1',{'bucket1' => {'1' => 'Item 1', '2' => 'Item 2', '5' => 'Item 5'}})
items = cc.get(:Super1,'test-key1','bucket1')
puts ""returned #{items.size} items, should be 3""
cc.remove(:Super1,'test-key1')
items = cc.get(:Super1,'test-key1','bucket1')
puts ""returned #{items.size} items, should be 0""
cc.insert(:Super1,'test-key1',{'bucket1' => {'3' => 'Item 3', '4' => 'Item 4', '6' => 'Item 6'}})
items = cc.get(:Super1,'test-key1','bucket1')
puts ""returned #{items.size} items, should be 3""
items = cc.get(:Super1,'test-key1','bucket1',:count => 3)
puts ""returned #{items.size} items, should be 3""
items = cc.get(:Super1,'test-key1','bucket1',:count => 4)
puts ""returned #{items.size} items, should be 3""
items = cc.get(:Super1,'test-key1','bucket1',:count => 5)
puts ""returned #{items.size} items, should be 3""
items = cc.get(:Super1,'test-key1','bucket1',:count => 6)
puts ""returned #{items.size} items, should be 3""

Output:
returned 3 items, should be 3
returned 0 items, should be 0
returned 3 items, should be 3
returned 1 items, should be 3
returned 2 items, should be 3
returned 2 items, should be 3
returned 3 items, should be 3

Ruby library link:
http://blog.evanweaver.com/files/doc/fauna/cassandra/files/README_rdoc.html",Mac OS/ Java 6,,,,,,,,,,,,,,,,,,,12/Apr/10 19:47;brandon.williams;ASF.LICENSE.NOT.GRANTED--0001_add_system_test.txt;https://issues.apache.org/jira/secure/attachment/12441537/ASF.LICENSE.NOT.GRANTED--0001_add_system_test.txt,12/Apr/10 20:58;jbellis;ASF.LICENSE.NOT.GRANTED--920.txt;https://issues.apache.org/jira/secure/attachment/12441545/ASF.LICENSE.NOT.GRANTED--920.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,38:12.1,,,no_permission,,,,,,,,,,,,19918,,,Mon Apr 12 21:14:32 UTC 2010,,,,,,0|i0g1w7:,91737,,,,,,,,,,,"07/Apr/10 20:38;brandon.williams;Patch to reproduce with a system test.  Note that there is a commented get_count in here.  If you uncomment it, strangely enough, the test passes.","08/Apr/10 00:04;brandon.williams;Oops, I had a bug in that patch.  Now with this one I'm unable to reproduce.","08/Apr/10 19:37;jbellis;Bob, closing since it appears to work fine in Brandon's test.  (Maybe it's a timestamping problem?  I have no idea what resolution the rb client uses by default.)",08/Apr/10 19:52;jbellis;reopening to commit test,"08/Apr/10 21:03;kingryan;I've rewritten this in terms of our cassandra.gem test suite and can't seem to find a bug in the client. It fails on the last line.

{code}
    @twitter.insert(:StatusRelationships, key,  {'user_timelines' => {@uuids[0] => 'Item 0', @uuids[1] => 'Item 1', @uuids[2] => 'Item 2'}})
    @twitter.remove(:StatusRelationships, key)

    items = @twitter.get(:StatusRelationships, key, 'user_timelines')
    assert_equal 0, items.size

    @twitter.insert(:StatusRelationships, key,
        {'user_timelines' => {@uuids[2] => 'Item 2', @uuids[3] => 'Item 3', @uuids[4] => 'Item 4'}})

    items = @twitter.get(:StatusRelationships, key, 'user_timelines')
    assert_equal 3, items.size

    items = @twitter.get(:StatusRelationships,key,'user_timelines',:count => 3)
    assert_equal 3, items.size 
{code}

And the server logs for this whole test:

{code}
DEBUG - batch_mutate
DEBUG - insert writing local key test_get_super_sub_keys_with_count_after_remove
DEBUG - remove
DEBUG - insert writing local key test_get_super_sub_keys_with_count_after_remove
DEBUG - multiget_slice
DEBUG - weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_count_after_remove', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@746a63d3', columnName='null')', start='', finish='', reversed=false, count=100)
DEBUG - batch_mutate
DEBUG - insert writing local key test_get_super_sub_keys_with_count_after_remove
DEBUG - multiget_slice
DEBUG - weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_count_after_remove', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@3bd840d9', columnName='null')', start='', finish='', reversed=false, count=100)
DEBUG - collecting 93814000-b668-11b2-8e6a-06dc05f11207:false:6@1270760311002397
DEBUG - collecting 13814000-4eff-11b3-88b0-351600af16aa:false:6@1270760311002397
DEBUG - collecting 13814000-802c-11b4-8782-bede78b81183:false:6@1270760311069304
DEBUG - collecting 13814000-e286-11b6-9ea9-48a9edd974b6:false:6@1270760311069304
DEBUG - collecting 13814000-a73a-11bb-8a14-b8d289de6d53:false:6@1270760311069304
DEBUG - multiget_slice
DEBUG - weakreadlocal reading SliceFromReadCommand(table='Twitter', key='test_get_super_sub_keys_with_count_after_remove', column_parent='QueryPath(columnFamilyName='StatusRelationships', superColumnName='[B@6e37d490', columnName='null')', start='', finish='', reversed=false, count=3)
DEBUG - collecting 93814000-b668-11b2-8e6a-06dc05f11207:false:6@1270760311002397
DEBUG - collecting 13814000-4eff-11b3-88b0-351600af16aa:false:6@1270760311002397
DEBUG - collecting 13814000-802c-11b4-8782-bede78b81183:false:6@1270760311069304
DEBUG - GC for ParNew: 7 ms, 20659648 reclaimed leaving 19819056 used; max is 1211826176
{code}

And the thrift structures for the failing multiget_slice

{code}
<CassandraThrift::ColumnParent column_family:""StatusRelationships"", super_column:""user_timelines"">
<CassandraThrift::SlicePredicate slice_range:<CassandraThrift::SliceRange start:"""", finish:"""", reversed:false, count:3>>
{code}
","08/Apr/10 22:38;brandon.williams;After much head scratching, Ryan and I figured out you have to remove the entire row, not just the subcolumns, to cause this.  Updated system test which does indeed fail.","08/Apr/10 23:17;brandon.williams;Oh, and just to come full circle... if you put a get_count in between the the remove and subsequent slice, it passes.  I guess I had it right in the first patch after all. ","12/Apr/10 18:44;jbellis;{code}
PYTHONPATH=test nosetests --tests=system.test_server:TestMutations.test_super_reinsert
.
----------------------------------------------------------------------
Ran 1 test in 2.432s

OK
{code}","12/Apr/10 19:47;brandon.williams;I must have attached the wrong patch, try this one.","12/Apr/10 20:58;jbellis;(Brief) fix attached.

SuperColumns suck.","12/Apr/10 21:03;brandon.williams;+1, both to the patch and SCs sucking.",12/Apr/10 21:14;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingService.StreamDestinations never empties,CASSANDRA-1076,12464261,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,gdusbabek,gdusbabek,5/11/2010 16:41,3/12/2019 14:01,3/13/2019 22:24,5/14/2010 17:58,0.6.2,0.7 beta 1,,,0,,,,,,"The problem is that StreamOutManager.streamManagers never has anything removed from it.  In order for StreamingService.getDestinations() to work properly, we either need to track hosts differently, or remove from StreamOutManager.streamManagers when we are no longer streaming to a node.

I lean towards the former, as any time we call StreamOutManager.get(), we're back in the same boat.",,,,,,,,,,,,,,,,CASSANDRA-956,,,,14/May/10 16:33;gdusbabek;0001-remove-from-streamManagers-when-finished.patch;https://issues.apache.org/jira/secure/attachment/12444509/0001-remove-from-streamManagers-when-finished.patch,14/May/10 16:33;gdusbabek;0002-a-better-StreamingServcice.getStatus.patch;https://issues.apache.org/jira/secure/attachment/12444507/0002-a-better-StreamingServcice.getStatus.patch,14/May/10 16:33;gdusbabek;0003-nix-StreamFile.patch;https://issues.apache.org/jira/secure/attachment/12444508/0003-nix-StreamFile.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,56:00.4,,,no_permission,,,,,,,,,,,,19984,,,Fri May 14 16:58:50 UTC 2010,,,,,,0|i0g2un:,91892,,,,,,,,,,,11/May/10 20:25;gdusbabek;Patch is against trunk.,"14/May/10 14:25;gdusbabek;Stu, I noticed you assigned this issue to yourself.  Do you still intend to review the patches?","14/May/10 15:56;stuhood;* The synchronized methods don't seem necessary: all structures are concurrent, and I don't think we need any operations to compose
* getPendingFiles has a race between 'streamManagers.containsKey(host)' and 'get(host)': perhaps just perform a single get, and check for null

Other than that, looks good to me.
",14/May/10 16:33;gdusbabek;fixed those.,"14/May/10 16:58;stuhood;+1 Looks good.
Thanks Gary!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
debian build dep on ant-optional is missing,CASSANDRA-2164,12498643,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,2/15/2011 14:46,3/12/2019 14:01,3/13/2019 22:24,6/21/2011 17:24,0.7.7,0.8.2,,,0,,,,,,"Without the ant-optional package installed in Debian, builds fail (on lenny) with:

    Could not create type regexpmapper due to No supported regular expression matcher found: java.lang.ClassNotFoundException: org.apache.tools.ant.util.regexp.Jdk14RegexpMatcher

The attached patch makes it build. Tested on lenny and squeeze.
",,,,,,,,,,,,,,,,,,,,15/Feb/11 14:47;scode;2164.txt;https://issues.apache.org/jira/secure/attachment/12471077/2164.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,15:25.1,,,no_permission,,,,,,,,,,,,20475,,,Wed Jun 22 07:47:05 UTC 2011,,,,,,0|i0g9pj:,93003,thepaul,thepaul,,,,,,,,,"15/Feb/11 14:48;scode;(Ok, I hope the patch got uploaded alright. Not yet used to the new JIRA. I attached it separately as a file, as the 'submit patch' action didn't seem to offer an actual file upload facility.)","21/Jun/11 17:15;thepaul;+1 this from me, fwiw","21/Jun/11 17:24;jbellis;committed, thanks!","21/Jun/11 17:28;thepaul;+1 again, but this time With Authority!  The extra dependency is both necessary and satisfiable on all debian/ubuntu releases since 2008 (Lenny/Hardy or later), and it's doubtful that Cassandra builds, runs correctly, and is properly supportable on older releases anyway.","22/Jun/11 07:47;hudson;Integrated in Cassandra-0.7 #507 (See [https://builds.apache.org/job/Cassandra-0.7/507/])
    add ant-optional debian Build-Depends
patch by Peter Schuller; reviewed by Paul Cannon for CASSANDRA-2164

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1138102
Files : 
* /cassandra/branches/cassandra-0.7/debian/control
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUGS.txt cites bugs which are now fixed,CASSANDRA-354,12432536,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,markr,markr,8/7/2009 15:57,3/12/2019 14:01,3/13/2019 22:24,8/11/2009 17:13,0.4,,Legacy/Documentation and Website,,0,,,,,,"BUGS.txt should only list bugs which are still bugs, not those which are fixed.

It lists CASSANDRA-208 which is now fixed (I think)",,,,,,,,,,,,,,,,,,,,07/Aug/09 17:02;jbellis;354.patch;https://issues.apache.org/jira/secure/attachment/12415867/354.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,02:07.9,,,no_permission,,,,,,,,,,,,19648,,,Wed Aug 12 13:08:27 UTC 2009,,,,,,0|i0fyfb:,91175,,,,,,,,,,,07/Aug/09 17:02;jbellis;patch to update BUGS.txt attached,11/Aug/09 16:39;urandom;+1,11/Aug/09 16:54;euphoria;lgtm,11/Aug/09 17:13;jbellis;committed,"12/Aug/09 13:08;hudson;Integrated in Cassandra #165 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/165/])
    update BUGS.txt for 0.4
patch by jbellis; reviewed by Eric Evans and Michael Greene for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QuorumResponseHandler sets timeout incorrectly,CASSANDRA-911,12459899,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,3/22/2010 19:01,3/12/2019 14:01,3/13/2019 22:24,3/26/2010 1:16,0.6,,,,0,,,,,,"We noticed that the timeout calculation seems wrong:

long timeout = System.currentTimeMillis() - startTime + DatabaseDescriptor.getRpcTimeout();

Lets propose that 3 seconds elapse (currentTime - startTime). It will take that value and add the default RpcTimeout (5 seconds) making the timeout 8 seconds.",,,,,,,,,,,,,,,,,,,,22/Mar/10 19:02;lenn0x;0001-CASSANDRA-911-fixed-incorrect-timeout-calculation.patch;https://issues.apache.org/jira/secure/attachment/12439491/0001-CASSANDRA-911-fixed-incorrect-timeout-calculation.patch,25/Mar/10 19:17;rschildmeijer;CASSANDRA-911.patch;https://issues.apache.org/jira/secure/attachment/12439812/CASSANDRA-911.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,18:24.2,,,no_permission,,,,,,,,,,,,19915,,,Fri Mar 26 17:40:51 UTC 2010,,,,,,0|i0g1u7:,91728,,,,,,,,,,,"22/Mar/10 19:18;jbellis;+1, can you apply to WRH and AsyncResult too?","25/Mar/10 19:17;rschildmeijer;Fixed timeout calculation for QRH, WRH and AsyncResult",25/Mar/10 22:17;lenn0x;+1. Ill commit,26/Mar/10 17:40;jbellis;updated CHANGES,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid host/port parameters to cassandra-cli leaves system in unrecoverable state,CASSANDRA-867,12458579,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mwn,mwn,mwn,3/9/2010 20:44,3/12/2019 14:01,3/13/2019 22:24,3/9/2010 21:03,0.6,,Legacy/Tools,,0,,,,,,"bin\cassandra-cl.bati -host localhost -port 8880  (cassandra not running localhost/8880 ;) ) 

Starting Cassandra Client
Exception connecting to localhost/8880 - java.net.ConnectException: Connection refused: connect
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
cassandra> j
Exception null
cassandra> quit
Exception null

Problem is that main does not ensure that cliClient_ is set to an instanse of CliClient.",WinXP / java 1.6 / svn @ 921110 trunk,,,,,,,,,,,,,,,,,,,09/Mar/10 20:47;mwn;CASSANDRA-867.patch;https://issues.apache.org/jira/secure/attachment/12438326/CASSANDRA-867.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,03:58.3,,,no_permission,,,,,,,,,,,,19897,,,Tue Mar 09 21:03:58 UTC 2010,,,,,,0|i0g1kf:,91684,,,,,,,,,,,09/Mar/10 20:47;mwn;Small patch to solve above issue.,"09/Mar/10 21:03;jbellis;committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
move confluence wiki to moin,CASSANDRA-145,12424756,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,5/6/2009 21:04,3/12/2019 14:01,3/13/2019 22:24,5/13/2009 2:30,,,,,0,,,,,,"
I believe the project would be better served by Apache's moinmoin wiki. Here's why:

 * more consistent with other apache projects
 * better/easier to remember url, http://wiki.apache.org/cassandra vs. http://cwiki.apache.org/confluence/display/CSDR/Index
 * uses a more mainstream markup that more people will be familiar with.
 * looks better (although I admit this is subjective)
 * there is at least one person willing to spend some time keeping it updated and accurate, and that person doesn't like confluence. ;)

I've copied all of the content over. If there are no serious objections, I'd like to have the home page link to http://wiki.apache.org/cassandra, and the old wiki disabled.
",,,,,,,,,,,,,,,,,,,,07/May/09 15:16;urandom;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-145-point-wiki-links-at-moin.txt;https://issues.apache.org/jira/secure/attachment/12407541/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-145-point-wiki-links-at-moin.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,28:45.4,,,no_permission,,,,,,,,,,,,19566,,,Wed May 13 02:30:34 UTC 2009,,,,,,0|i0fx5b:,90968,,,,,,,,,,,07/May/09 08:28;johanoskarsson;+1,"07/May/09 14:19;urandom;In addition to Johan's, the following seconds were also made on the mailing list:

http://mail-archives.apache.org/mod_mbox/incubator-cassandra-dev/200905.mbox/%3Ce06563880905061417g79dc42a0rbe98271aa95b616d@mail.gmail.com%3E

http://mail-archives.apache.org/mod_mbox/incubator-cassandra-dev/200905.mbox/<f4d6a21a0905061420g388059c9x53f7915b75475611@mail.gmail.com>

http://mail-archives.apache.org/mod_mbox/incubator-cassandra-dev/200905.mbox/<a073486d0905061700x2d4af7a9v7cbc8ae9bc6a1029@mail.gmail.com>
",07/May/09 15:18;urandom;The attached patch fixes links to the wiki on the home page.,13/May/09 02:30;jbellis;all moved.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in Range.contains(Token),CASSANDRA-236,12428082,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,6/17/2009 8:37,3/12/2019 14:01,3/13/2019 22:24,6/17/2009 16:09,0.4,,,,0,,,,,,"There is an off-by-one error in Range.contains(Token): the endpoint (right) of the range is always supposed to be exclusive, but it was inclusive in the non-wrapping case.",,,,,,,,,,,,,,,,,,,,17/Jun/09 08:37;stuhood;range.patch;https://issues.apache.org/jira/secure/attachment/12410910/range.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,09:27.0,,,no_permission,,,,,,,,,,,,19604,,,Thu Jun 18 12:35:21 UTC 2009,,,,,,0|i0fxpb:,91058,,,,,,,,,,,17/Jun/09 08:37;stuhood;Fixes the issue and test.,"17/Jun/09 16:09;jbellis;committed, thanks!","18/Jun/09 12:35;hudson;Integrated in Cassandra #112 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/112/])
    fix off-by-one error in Range.contains(Token): the endpoint (right) of the range is always supposed to be exclusive, but it was inclusive in the non-wrapping case.  patch by Stu Hood; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
possible to have orphaned data files that never get deleted,CASSANDRA-432,12435133,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,jbellis,jbellis,9/8/2009 17:16,3/12/2019 14:01,3/13/2019 22:24,11/2/2009 19:37,0.5,,,,0,,,,,,"startup looks for data files, then tries to load them

delete removes starting w/ the data file (so that if there is a partial delete, we don't panic on the next start b/c of missing index or bloom filter)

but this means that if we have a partial delete, the index/BF files can persist forever",,,,,,,,,,,,,,,,,,,,29/Oct/09 20:17;gdusbabek;cassandra-432-v1.patch;https://issues.apache.org/jira/secure/attachment/12423612/cassandra-432-v1.patch,02/Nov/09 19:34;gdusbabek;cassandra-432-v2.patch;https://issues.apache.org/jira/secure/attachment/12423844/cassandra-432-v2.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,17:01.5,,,no_permission,,,,,,,,,,,,19688,,,Tue Nov 03 12:35:50 UTC 2009,,,,,,0|i0fyw7:,91251,,,,,,,,,,,21/Oct/09 19:54;jbellis;(refering to ColumnFamilyStore.onStart and SSTable.delete),29/Oct/09 20:17;gdusbabek;Removes stray -Index.db and -Filter.db files.,"02/Nov/09 19:20;jbellis;isn't the {1} in the pattern redundant?  or am i mis-remembering what it means?  (i think it means ""specify a count for the preceding item"")","02/Nov/09 19:33;gdusbabek;Yeah, not needed.  It was leftover from a more complicated regex.",02/Nov/09 19:34;gdusbabek;removes redundant {1} from regex.,02/Nov/09 19:37;jbellis;committed,"03/Nov/09 12:35;hudson;Integrated in Cassandra #247 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/247/])
    scan on startup for orphaned filter and index files and remove them.
patch by Gary Dusbabek; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json2sstable/sstable2json don't export/import correct column names when the column family is of BytesType ordering,CASSANDRA-618,12442906,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,rrabah,rrabah,12/9/2009 20:51,3/12/2019 14:01,3/13/2019 22:24,12/14/2009 21:10,0.5,,Legacy/Tools,,0,,,,,,"Easy to reproduce.
1- start with an empty node.
2- run: client.insert(""Keyspace1"",
                          key_user_id,
                          new ColumnPath(""Standard1"", null, ""name"".getBytes(""UTF-8"")),
                          ""Ramzi"".getBytes(""UTF-8""),
                          timestamp,
                          ConsistencyLevel.ONE)
3- flush to get sstable
4- sstable2json and export the sstable to a file
5- delete sstable
6- json2sstable and import the json into a new sstable.
7- sstable2json on new sstable, you will see that the name is different than the name in the original json file. 
Also do a get on the column and it will return no result. ",,,,,,,,,,,,,,,,,,,,14/Dec/09 19:59;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-618-unittest-that-demonstrates-bug.txt;https://issues.apache.org/jira/secure/attachment/12427952/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-618-unittest-that-demonstrates-bug.txt,14/Dec/09 19:59;urandom;ASF.LICENSE.NOT.GRANTED--v2-0002-de-serialize-columns-to-from-hex-regardless-of-compara.txt;https://issues.apache.org/jira/secure/attachment/12427953/ASF.LICENSE.NOT.GRANTED--v2-0002-de-serialize-columns-to-from-hex-regardless-of-compara.txt,14/Dec/09 19:59;urandom;ASF.LICENSE.NOT.GRANTED--v2-0003-do-trivial-arithmetic-correctly.txt;https://issues.apache.org/jira/secure/attachment/12427954/ASF.LICENSE.NOT.GRANTED--v2-0003-do-trivial-arithmetic-correctly.txt,10/Dec/09 01:54;rrabah;unittest.patch;https://issues.apache.org/jira/secure/attachment/12427554/unittest.patch,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,10:10.7,,,no_permission,,,,,,,,,,,,19784,,,Thu Dec 17 22:03:03 UTC 2009,,,,,,0|i0g01j:,91437,,,,,,,,,,,"09/Dec/09 21:10;jbellis;Ramzi, could you add a test to SSTableExportTest illustrating the problem?",10/Dec/09 01:54;rrabah;Attached is a unit test for exporting/importing to json that reproduces this problem,11/Dec/09 22:17;urandom;The attached patch(s) addresses this by serializing all column names to hex (instead of using the comparators toString()).,"12/Dec/09 01:05;rrabah;Applying the patch I am getting this error when I run ant clean build test:
 [junit] Testcase: testImportSuperCf(org.apache.cassandra.tools.SSTableImportTest):	Caused an ERROR
    [junit] Invalid localDeleteTime read: -2140491435
    [junit] java.io.IOException: Invalid localDeleteTime read: -2140491435
    [junit] 	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:368)
    [junit] 	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:325)
    [junit] 	at org.apache.cassandra.db.filter.SSTableNamesIterator.<init>(SSTableNamesIterator.java:103)
    [junit] 	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:69)
    [junit] 	at org.apache.cassandra.tools.SSTableImportTest.testImportSuperCf(SSTableImportTest.java:63)
","12/Dec/09 14:34;jbellis;I'm getting that error even w/o the patch, even rolling back to code from dec 9 when i know the tests were passing.  possibly a change in wall clock time is what is causing the test failure?","14/Dec/09 20:01;urandom;Apparently there is something on the order of 1,000 milliseconds in a second. Who knew.","14/Dec/09 20:34;rrabah;in SSTableExport.java  for super column name we still use asKey(comparator.getString(column.name()))
change to byteToHex() besides that, looks good","14/Dec/09 21:10;urandom;Right you are. Man, I'm batting a thousand with this ticket. Thanks Ramzi!","17/Dec/09 22:03;hudson;Integrated in Cassandra #291 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/291/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LegacySSTableTest breaks when run from a svn checkout,CASSANDRA-1309,12469940,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,jbellis,jbellis,7/22/2010 18:47,3/12/2019 14:01,3/13/2019 22:24,7/27/2010 13:25,0.7 beta 1,,,,0,,,,,,"Works fine under git where there is no .svn turd.

    [junit] ------------- Standard Error -----------------
    [junit] Failed to read .svn
    [junit] java.io.FileNotFoundException: /Users/jonathan/projects/cassandra/svn-trunk/test/data/legacy-sstables/.svn/Keyspace1/Standard1-.svn-0-Index.db (No such file or directory)
    [junit] 	at java.io.RandomAccessFile.open(Native Method)
    [junit] 	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    [junit] 	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    [junit] 	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    [junit] 	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:256)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:187)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:170)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:150)
    [junit] 	at org.apache.cassandra.io.sstable.LegacySSTableTest.testVersion(LegacySSTableTest.java:102)
    [junit] 	at org.apache.cassandra.io.sstable.LegacySSTableTest.testVersions(LegacySSTableTest.java:95)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)
    [junit] 	at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
",,,,,,,,,,,,,,,,,,,,22/Jul/10 21:26;stuhood;0001-Only-test-valid-version-strings.patch;https://issues.apache.org/jira/secure/attachment/12450221/0001-Only-test-valid-version-strings.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,26:16.9,,,no_permission,,,,,,,,,,,,20073,,,Tue Jul 27 13:25:08 UTC 2010,,,,,,0|i0g49z:,92123,,,,,,,,,,,"22/Jul/10 21:26;stuhood;Skips invalid version subdirectories.

Also, it looks like we're missing a few versions in test/data/legacy-sstables: would a committer mind following the instructions in LegacySSTableTest to generate sstables for versions 'c' and 'd'? In the past, I've posted them as git 'data diffs', but I think patch might have dropped them silently.",27/Jul/10 13:25;gdusbabek;+1 committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
binary artifacts need ivy for dependencies,CASSANDRA-796,12456366,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,2/15/2010 22:18,3/12/2019 14:01,3/13/2019 22:24,2/16/2010 18:59,0.6,,Legacy/Tools,,0,,,,,,"Currently, if you generate a release, the binary artifact is missing required dependencies and the means to obtain them. The patch that follows a) copies build.xml and ivy.xml into the bin artifact and b) makes build.xml smart enough to tell when it's being run from a binary artifact.

This would also require updating all installation and quickstart documentation as well (the patch updates README.txt).",,,,,,,,,,,,,,,,,,,,15/Feb/10 22:20;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-796-adapt-build.xml-to-work-from-binary-arti.txt;https://issues.apache.org/jira/secure/attachment/12435914/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-796-adapt-build.xml-to-work-from-binary-arti.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,03:40.8,,,no_permission,,,,,,,,,,,,19867,,,Wed Feb 17 17:54:42 UTC 2010,,,,,,0|i0g14n:,91613,,,,,,,,,,,"16/Feb/10 16:03;gdusbabek;tested, +1.",16/Feb/10 18:59;urandom;committed; thanks,"17/Feb/10 17:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data directories not being properly scrubbed,CASSANDRA-1542,12475000,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,stuhood,stuhood,9/24/2010 2:57,3/12/2019 14:01,3/13/2019 22:24,9/28/2010 5:43,0.7 beta 2,,,,0,,,,,,"AbstractCassandraDaemon is trying to scrub data directories once the server has already been initialized (CASSANDRA-1477, r997490).

To reproduce, delete a single component of an SSTable and restart.",,,,,,,,,,,,,,,,,,,,28/Sep/10 05:39;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0001-do-not-intialize-table-instances-when-loading-schema.-.txt;https://issues.apache.org/jira/secure/attachment/12455799/ASF.LICENSE.NOT.GRANTED--v2-0001-do-not-intialize-table-instances-when-loading-schema.-.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,50:50.4,,,no_permission,,,,,,,,,,,,20189,,,Tue Sep 28 13:31:31 UTC 2010,,,,,,0|i0g5vb:,92381,stuhood,stuhood,,,,,,,,,"27/Sep/10 06:50;gdusbabek;The only problem I saw is that we were calling Table.open() for each table as part of DD.loadSchemas().  This made it impossible to scrub the data directories *after* we find out about the tables but *before* they are initialized.  We were already calling Table.open() in AbstractCassandraDaemon, so I decided to remove the call from DD.

That being said, stack traces in logs are still alarming for users.  Since the reason behind FNFE is well understood, I decided to modify ACD.setup() to catch it specifically and emit the error sans trace.  FWIW, CFS.scrubDataDirectories() does seem to be doing its job correctly.

This patch addresses those two items.

One question I have is if we should detect missing bloom filters and row indexes and proactively rebuild them.  I can't think of a case where this would happen in real life, so this would end up being a courtesy for people manually moving files around.  We probably shouldn't encourage this.","27/Sep/10 15:55;jbellis;I don't know that ""corrupt sstable"" is really that much less scary than a stack trace. :)","27/Sep/10 18:45;stuhood;> One question I have is if we should detect missing bloom filters and row indexes and proactively rebuild them.
Might be a cool feature to add at some point: would probably be more useful for recovery of corrupted filters/indexes than for missing ones.

+1 Looks good.",28/Sep/10 05:43;gdusbabek;Committed with a less intimidating error message. ,"28/Sep/10 13:31;hudson;Integrated in Cassandra #549 (See [https://hudson.apache.org/hudson/job/Cassandra/549/])
    do not intialize table instances when loading schema. complain less loudly when there is a missing sstable component. patch by gdusbabek, reviewed by stuhood. CASSANDRA-1542
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EstimatedHistogram.max is buggy,CASSANDRA-1413,12472062,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,8/20/2010 6:56,3/12/2019 14:01,3/13/2019 22:24,8/25/2010 20:36,0.7 beta 2,,,,0,,,,,,"EH.max returns the largest bucket floor, which will will be LESS than the largest value added to the histogram, which is not the usual behavior expected of a method called max.",,,,,,,,,,,,,,,,,,,,23/Aug/10 22:13;brandon.williams;0002_adjust_EH_sizes.txt;https://issues.apache.org/jira/secure/attachment/12452866/0002_adjust_EH_sizes.txt,20/Aug/10 06:58;jbellis;1413.txt;https://issues.apache.org/jira/secure/attachment/12452610/1413.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,46:45.8,,,no_permission,,,,,,,,,,,,20126,,,Wed Aug 25 20:36:16 UTC 2010,,,,,,0|i0g4wn:,92225,brandon.williams,brandon.williams,,,,,,,,,20/Aug/10 18:46;brandon.williams;Isn't the ISE being thrown here going to get raised in CFS when cfstats is called?,"20/Aug/10 19:03;jbellis;Dunno, that's why I wanted you to review.  You sized those EHes. :)

Isn't the ""right"" solution just to make them large enough?

Open to other ideas.","23/Aug/10 22:13;brandon.williams;I think we need to adjust the sizes a bit.  Bump the column count EH to 114, giving us a max of about 2.4B columns (113 allows for 1996099046), and bump the row size EH to 150, allowing for almost a 1.7T row.  It's theoretically possible to make a row larger than that, but I think having cfstats throw an exception will be the least of the user's problems at that point.",25/Aug/10 20:36;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Warn operator when there is not enough disk space for compaction,CASSANDRA-804,12456535,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,2/17/2010 13:51,3/12/2019 14:01,3/13/2019 22:24,2/17/2010 16:59,0.6,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,17/Feb/10 16:25;jbellis;804-v3.txt;https://issues.apache.org/jira/secure/attachment/12436112/804-v3.txt,17/Feb/10 14:48;jbellis;804.txt;https://issues.apache.org/jira/secure/attachment/12436109/804.txt,17/Feb/10 14:51;gdusbabek;reset-compactionFileLocation.txt;https://issues.apache.org/jira/secure/attachment/12436110/reset-compactionFileLocation.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,35:47.0,,,no_permission,,,,,,,,,,,,19871,,,Wed Feb 17 17:54:44 UTC 2010,,,,,,0|i0g16f:,91621,,,,,,,,,,,"17/Feb/10 14:35;gdusbabek;I don't understand the purpose of the while loop.  If there isn't enough space (compactionFileLocation==null), shouldn't the error be reported and the function return 0?","17/Feb/10 14:41;jbellis;the purpose is, if we can't compact all N files, see if we can compact N - 1, N - 2, etc.  Merging rows will actually free space in the average case, so it's possible that by merging some files rather than giving up completely that we will actually succeed at all of them next time.",17/Feb/10 14:48;jbellis;better patch.,"17/Feb/10 14:51;gdusbabek;That makes sense, but if I'm reading the code correctly, the loop will always empty out smallerSSTables since compactionFileLocation is invariant.  Perhaps it should be reset in the while loop.  (Apply reset-compactionFileLocation.txt on top of your patch to see.)","17/Feb/10 15:30;jbellis;definitely, +1 your fix.",17/Feb/10 16:25;jbellis;combined patch attached,17/Feb/10 16:49;gdusbabek;+1,17/Feb/10 16:59;jbellis;committed,"17/Feb/10 17:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    use while loop instead of recursion when trimming sstables compaction list to avoid blowing stack in pathological cases.
patch by jbellis; reviewed by gdusbabe for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests log RTE to stderr even though tests still succeed.,CASSANDRA-806,12456582,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,2/17/2010 20:11,3/12/2019 14:01,3/13/2019 22:24,2/17/2010 21:17,0.6,,,,0,,,,,,"[junit] ------------- Standard Error -----------------
   [junit] ERROR 20:05:29,165 Error in executor futuretask
   [junit] java.util.concurrent.ExecutionException: java.lang.RuntimeException: No replica strategy configured for ltable
   [junit]     at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
   [junit]     at java.util.concurrent.FutureTask.get(FutureTask.java:83)
   [junit]     at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
   [junit]     at java.lang.Thread.run(Thread.java:619)
   [junit] Caused by: java.lang.RuntimeException: No replica strategy configured for ltable
   [junit]     at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:245)
   [junit]     at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1150)
   [junit]     at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:149)
   [junit]     at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:88)
   [junit]     at org.apache.cassandra.service.AntiEntropyService$Validator.call(AntiEntropyService.java:487)
   [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
   [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
   [junit]     ... 2 more
   [junit] ERROR 20:05:31,949 Error in executor futuretask
   [junit] java.util.concurrent.ExecutionException: java.lang.RuntimeException: No replica strategy configured for rtable
   [junit]     at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
   [junit]     at java.util.concurrent.FutureTask.get(FutureTask.java:83)
   [junit]     at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:65)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
   [junit]     at java.lang.Thread.run(Thread.java:619)
   [junit] Caused by: java.lang.RuntimeException: No replica strategy configured for rtable
   [junit]     at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:245)
   [junit]     at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1150)
   [junit]     at org.apache.cassandra.service.AntiEntropyService.getNeighbors(AntiEntropyService.java:149)
   [junit]     at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:88)
   [junit]     at org.apache.cassandra.service.AntiEntropyService$Validator.call(AntiEntropyService.java:487)
   [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
   [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
   [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
   [junit]     ... 2 more
   [junit] ------------- ---------------- ---------------",,,,,,,,,,,,,,,,,,,,17/Feb/10 20:50;gdusbabek;0001-use-real-keyspace-names-so-that-real-replication-str.patch;https://issues.apache.org/jira/secure/attachment/12436126/0001-use-real-keyspace-names-so-that-real-replication-str.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,06:29.1,,,no_permission,,,,,,,,,,,,19873,,,Wed Feb 17 21:17:15 UTC 2010,,,,,,0|i0g16v:,91623,,,,,,,,,,,"17/Feb/10 20:52;gdusbabek;This was introduced in 620 when each table was assigned a specific replication strategy, rather than relying on a global one.",17/Feb/10 21:06;stuhood;+1 Looks good to me.,17/Feb/10 21:17;gdusbabek;r911178,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli doesn't allow hyphens in hostnames,CASSANDRA-677,12444836,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,rschildmeijer,urandom,urandom,1/6/2010 20:40,3/12/2019 14:01,3/13/2019 22:24,3/24/2010 18:43,0.6,,Legacy/Tools,,2,,,,,,"It's not possible to use a hostname that contains a hyphen with the ""connect"" command interactively, (the parser does not accept hostnames that contain hyphens).

Note: It is still possible to connect to such hosts by passing it on the command line using -host.",Any,,,,,,,,,,,,,,,,,,,18/Jan/10 20:08;rschildmeijer;CASSANDRA-677.patch;https://issues.apache.org/jira/secure/attachment/12430667/CASSANDRA-677.patch,17/Jan/10 16:13;rschildmeijer;CASSANDRA-677.patch;https://issues.apache.org/jira/secure/attachment/12430560/CASSANDRA-677.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,07:26.8,,,no_permission,,,,,,,,,,,,19814,,,Wed Mar 24 18:43:17 UTC 2010,,,,,,0|i0g0ef:,91495,,,,,,,,,,,"18/Jan/10 17:00;urandom;Thanks Roger. But I'm afraid this won't work for a couple of reasons.

1. Because `Identifier' is used for column family names, it permits the use of hyphens there, which as things currently stand will cause problems (hint: on disk, CFs use hypen delimited filenames).

2. Something other than `Identifier' should be used since that would allow underscores in hostnames, which should not be permitted.",18/Jan/10 20:07;rschildmeijer;Improvements made after Eric's comment about the previous patch (Identifier is now left untouched).,"19/Jan/10 22:52;urandom;Sorry Roger, I did some quick testing and this patch seem to have issues as well. It does allow hyphens in hostnames, but only when there are no numbers involved.

cassandra> connect cass-1.lab/9160
line 1:13 mismatched input '1' expecting Identifier
Exception Cannot open null host.","24/Mar/10 18:43;urandom;This has been applied. See CASSANDRA-914 for some additional background.

Thanks Roger.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction bucketizing is computing average size incorrectly,CASSANDRA-814,12456955,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,2/21/2010 21:48,3/12/2019 14:01,3/13/2019 22:24,2/23/2010 22:47,0.6,,,,0,,,,,,"in the worst case (which doesn't seem to actually affect anyone) this could prevent sstables from being compacted, incorrectly",,,,,,,,,,,,,,,,,,,,23/Feb/10 22:15;jbellis;ASF.LICENSE.NOT.GRANTED--0001-rename-CM.getCompactionBuckets-CM.getBuckets.txt;https://issues.apache.org/jira/secure/attachment/12436772/ASF.LICENSE.NOT.GRANTED--0001-rename-CM.getCompactionBuckets-CM.getBuckets.txt,23/Feb/10 22:15;jbellis;ASF.LICENSE.NOT.GRANTED--0002-fix-average-size-calculation.txt;https://issues.apache.org/jira/secure/attachment/12436773/ASF.LICENSE.NOT.GRANTED--0002-fix-average-size-calculation.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,00:12.1,,,no_permission,,,,,,,,,,,,19877,,,Tue Feb 23 22:47:07 UTC 2010,,,,,,0|i0g18n:,91631,,,,,,,,,,,"23/Feb/10 22:00;stuhood;This doesn't look right...

 * We use < and > in the if statement for existing buckets, which means that an SSTable of exactly half the average or exactly 1.5 the average will create a new bucket.
 * We always divide by 2 to calculate the average, although there will typically be more than 2 sstables in a bucket.","23/Feb/10 22:16;jbellis;> We use < and > in the if statement

doesn't seem that inclusive comparisons is necessarily any more correct, really.

> We always divide by 2

good catch, fixed in new patchset.",23/Feb/10 22:30;stuhood;+1,23/Feb/10 22:47;jbellis;committed to 0.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HashingStrategy does not default to RANDOM if the conf file is missing this entry,CASSANDRA-6,12416988,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,sandeep_tata,sandeep_tata,3/16/2009 18:16,3/12/2019 14:01,3/13/2019 22:24,4/8/2009 20:49,,,,,0,,,,,,"When the conf file does not have a line for hashingstrategy, the value does not default to RANDOM, instead it goes to null because of:

DatabaseDescriptor.java:147

hashingStrategy_ = xmlUtils.getNodeValue(""/Storage/HashingStrategy"");

",all,,1800,1800,,0%,1800,1800,,,,,,,,,,,,08/Apr/09 19:23;jbellis;0001-don-t-manually-create-commitlog-directory-cassandra.patch;https://issues.apache.org/jira/secure/attachment/12404998/0001-don-t-manually-create-commitlog-directory-cassandra.patch,08/Apr/09 19:24;jbellis;0002-sanity-check-configuration-before-starting.patch;https://issues.apache.org/jira/secure/attachment/12404999/0002-sanity-check-configuration-before-starting.patch,16/Mar/09 18:24;sandeep_tata;default_values_patch.txt;https://issues.apache.org/jira/secure/attachment/12402302/default_values_patch.txt,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,30:14.8,,,no_permission,,,,,,,,,,,,19510,,,Wed Apr 08 20:49:58 UTC 2009,,,,,,0|i0fwbj:,90834,,,,,,,,,,,"16/Mar/09 18:24;sandeep_tata;Here's a very simple patch that fixes this issue.

Added XMLUtils.getNodeValue with a default value arg, and used this in DatabaseDescriptor.",29/Mar/09 16:30;jbellis;Avinash applied Sandeep's patch for this specific issue but more generally we need to add some checks to startup for configuration sanity and abort if things are too badly broken.  (Missing hash fn would fall into that category.),"08/Apr/09 19:26;jbellis;attached patches that refuse to start up when things are broken.  (this means we don't need the ""default"" value patch anymore.)","08/Apr/09 20:44;urandom;Patch 0002 adds an unused import (org.apache.cassandra.db.ColumnFamilyStore), and I'm assuming that the call to xmlUtils.getAttributeValue() should instead be XMLUtils.getAttributeValue().

Other than those two minor issues, it looks good to me.",08/Apr/09 20:49;jbellis;committed with suggested improvements,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""maven install"" does not find libthrift-r808609.jar",CASSANDRA-624,12443048,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,cpierret,cpierret,12/10/2009 23:56,3/12/2019 14:01,3/13/2019 22:24,12/30/2009 16:55,0.5,0.6,Legacy/Tools,,0,,,,,,"Do a fresh svn checkout of subversion trunk HEAD (at revision 89436 when error occurred)
run: maven -e install
It fails, not finding dependency libthrift.jar

pom.xml references ${basedir}/lib/libthrift-r808609.jar
It should be ${basedir}/lib/libthrift-r820831.jar

",svn trunk r889436,,,,,,,,,,,,,,,THRIFT-363,,,,11/Dec/09 00:21;cpierret;pom.xml.patch;https://issues.apache.org/jira/secure/attachment/12427663/pom.xml.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,55:52.1,,,no_permission,,,,,,,,,,,,19786,,,Mon Apr 25 13:59:09 UTC 2011,,,,,,0|i0g02v:,91443,,,,,,,,,,,"10/Dec/09 23:57;cpierret;Index: pom.xml
===================================================================
--- pom.xml	(revision 889422)
+++ pom.xml	(working copy)
@@ -187,7 +187,7 @@
       <artifactId>libthrift</artifactId>
       <version>UNKNOWN</version>
       <scope>system</scope>
-      <systemPath>${basedir}/lib/libthrift-r808609.jar</systemPath>
+      <systemPath>${basedir}/lib/libthrift-r820831.jar</systemPath>
     </dependency>
     <dependency>
       <groupId>reardencommerce</groupId>
",30/Dec/09 16:55;urandom;This was fixed in http://svn.apache.org/viewvc?revision=892351&view=revision (I wasn't aware there was an issue open on it).,"25/Apr/11 13:59;jfarrell;Libthrift and libfb303 now available in the apache repo (http://repo1.maven.org/maven2)

<dependency>
  <groupId>org.apache.thrift</groupId>
  <artifactId>libthrift</artifactId>
  <version>[0.6.1,)</version>
</dependency>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
range queries don't respect snitch for local replicas,CASSANDRA-2286,12500783,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,3/8/2011 16:37,3/12/2019 14:01,3/13/2019 22:24,3/11/2011 18:28,0.7.4,,,,0,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,08/Mar/11 16:39;jbellis;2286.txt;https://issues.apache.org/jira/secure/attachment/12472994/2286.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,20:18.4,,,no_permission,,,,,,,,,,,,20541,,,Fri Mar 11 23:07:02 UTC 2011,,,,,,0|i0gah3:,93127,slebresne,slebresne,,,,,,,,,11/Mar/11 18:20;slebresne;+1,"11/Mar/11 18:28;jbellis;committed [w/ addition of ""&& !liveEndpoints.isEmpty()"" to local condition]","11/Mar/11 23:07;hudson;Integrated in Cassandra-0.7 #375 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/375/])
    makerange queries respect snitch for local replicas
patch by jbellis; reviewed by slebresne for CASSANDRA-2286
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogTest and RecoveryManager2Test is failing in trunk,CASSANDRA-1318,12470134,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mdennis,mdennis,mdennis,7/26/2010 6:00,3/12/2019 14:01,3/13/2019 22:24,7/27/2010 4:03,0.7 beta 1,,,,0,,,,,,"{code}
test:
     [echo] running unit tests
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/home/mdennis/mdev/trunkclean/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.db.CommitLogTest
    [junit] Tests run: 12, Failures: 1, Errors: 0, Time elapsed: 0.937 sec
    [junit] 
    [junit] Testcase: testCleanup(org.apache.cassandra.db.CommitLogTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.CommitLogTest.testCleanup(CommitLogTest.java:66)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.CommitLogTest FAILED

BUILD FAILED
{code}",,,,,,,,,,,,,,,,,,,,27/Jul/10 03:59;mdennis;1318-trunk.patch;https://issues.apache.org/jira/secure/attachment/12450556/1318-trunk.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,08:00.2,,,no_permission,,,,,,,,,,,,20079,,,Tue Jul 27 13:41:07 UTC 2010,,,,,,0|i0g4br:,92131,,,,,,,,,,,26/Jul/10 06:04;mdennis;It appears that when we started keeping persistent stats (CASSANDRA-1155) we ended up writing more than the half the segment size chosen by the test to the commit log causing it to fail.,26/Jul/10 13:08;jbellis;wouldn't adding a flush to the system keyspace be a less fragile fix?,"26/Jul/10 13:47;jbellis;RecoveryManager2Test is also failing from the same 1155 commit, may be something similar",26/Jul/10 17:42;mdennis;RecoveryManager2Test is nonde,27/Jul/10 03:38;mdennis;Adding a flush to CommitLogTest doesn't help because forceBlockingFlush() eventually calls discardCompletedSegmentsInternal() which re-dirties the column family in case a write happens concurrently with the flush (e.g. CommitLog:411).,"27/Jul/10 04:03;jbellis;committed, minus the CLH commented-out code","27/Jul/10 13:41;hudson;Integrated in Cassandra #501 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/501/])
    fix commitlog tests post-1135.  patch by mdennis; reviewed by jbellis for CASSANDRA-1318
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bug with test,CASSANDRA-2063,12496928,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,amit71,amit71,1/27/2011 11:34,3/12/2019 14:01,3/13/2019 22:24,1/27/2011 22:44,0.7.1,,,,0,,,,,,"when executing nosetests (e.g: nosetests test/system/test_avro_system.py), you get the following error:

    mod = load_module(part_fqname, fh, filename, desc)
  File ""/tmp/apache-cassandra-0.7.0-src/test/system/test_avro_system.py"", line 19
    from . import AvroTester
         ^
SyntaxError: invalid syntax

All *.py scripts should be changed to be ""from __init__ import (AvroTester)""    instead of ""from . import AvroTester""


",RHL. Python 2.4.3,,600,600,,0%,600,600,,,,,,,,,,,,27/Jan/11 21:27;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2063-Python2.4-friendly-imports.txt;https://issues.apache.org/jira/secure/attachment/12469599/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2063-Python2.4-friendly-imports.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,06:38.2,,,no_permission,,,,,,,,,,,,20422,,,Fri Jan 28 08:49:46 UTC 2011,,,,,,0|i0g93j:,92904,,,,,,,,,,,"27/Jan/11 22:06;jbellis;i'm kind of surprised that import is the only thing keeping 2.4 from running, but +1","27/Jan/11 22:44;urandom;Yeah, same here.  Maybe OP didn't make it past this (I don't have 2.4 handy to test with).  Anyway, committed.","27/Jan/11 23:04;hudson;Integrated in Cassandra-0.7 #222 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/222/])
    CASSANDRA-2063 Python2.4-friendly imports

Patch by eevans for CASSANDRA-2063
","28/Jan/11 08:49;amit71;why is it surprising you?
when you do the test, you will face one more issue. i dont remember where
exactly, but in one of the parameter declaration there is an extra 'b'
outside the value (instad of ""a='value'"" its written ""a= b'value' "".
anyway, i suggest also to add script that will be executed in the beginning
of each and every test that will do the following:
1)  delete the DB.
2) kill the existed instance of Cassandra in case there is a file of
existing Casandra in root diretory.
3) delete the existing file that prevent the test from being ran.

I can tell u that for me it was really confusing, especially that not much
documentation is written on it.
Cheers,
Amit


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rejecting keyspace creation when RF > N is incorrect,CASSANDRA-1428,12472428,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,moonpolysoft,moonpolysoft,8/24/2010 23:29,3/12/2019 14:01,3/13/2019 22:24,10/6/2010 19:36,0.7 beta 3,,,,0,,,,,,"The behavior introduced in this patch http://www.mail-archive.com/commits@cassandra.apache.org/msg05913.html is incorrect.

Disallowing keyspace creation when RF > N is semantically incorrect and makes both scaling a cluster up and down more difficult than it should be.  This is compounded by the current lack of any API methods to change the replication factor.  Most dynamo style systems allow RF to be set > N for smaller clusters.  The cluster will behave as if RF = N until enough nodes are added such that RF < N.",,,,,,,,,,,,,,,,,,,,05/Oct/10 11:15;jbellis;1428.txt;https://issues.apache.org/jira/secure/attachment/12456378/1428.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,08:28.8,,,no_permission,,,,,,,,,,,,20130,,,Tue Oct 12 14:03:55 UTC 2010,,,,,,0|i0g4zz:,92240,gdusbabek,gdusbabek,,,,,,,,,"25/Aug/10 13:08;gdusbabek;>Most dynamo style systems allow RF to be set > N for smaller clusters.
Implementing this would add substantial complexity to the replication and bootstrap code.  It's a trade-off not currently worth the flexibility imo.

I am in favor of allowing the creation of keyspaces with RF > N though (letting high CL reads/writes fail).  This worked fine until r959726 changed RackUnawareStrategy (now SimpleStrategy) to throw IllegalStateException when N < RF.  I argued (unsuccessfully) that the throw should happen elsewhere, or at least it shouldn't care when there was no data to bootstrap.

fwiw, modifying the replication factor (and other CF and KS properties) is on its way.  See CASSANDRA-1285.  It will be ready when 0.7 ships.","21/Sep/10 15:41;jbellis;Wait, how is ""allowing the creation of keyspaces with RF > N"" that you are in favor of different from ""allow[ing] RF to be set > N"" that you are not?","21/Sep/10 15:42;jbellis;incidently, in r999469 I changed this from using live node count as N to using total known cluster members as N.","21/Sep/10 16:04;gdusbabek;I interpreted ""RF to be set > N for smaller clusters"" as meaning, ""let high-CL writes succeed even though we don't have enough nodes to support the RF when N is small.""  I don't think we should allow that.

From an syspop pov, I think we should allow the creation of keyspaces where RF > N.  There is no harm in this for new keyspaces where there is nothing to stream.
","21/Sep/10 16:28;jbellis;I see, you want to allow creating the KS, but not writing to it.  (Is this what you want too, Cliff?)

My reasoning for wanting to prevent creation was, if we're not going to allow writing to it, better to fail early rather than surprise people later on.  But I am okay with taking that check out if the cure is worse than the disease.

I definitely think the semantics of actually trying to allow writes in a RF > N situation are unclear and would rather avoid that.","21/Sep/10 16:36;moonpolysoft;This ticket was specifically in reference to creation of a keyspace with RF > N.  Since all of the schema stuff now is the responsibility of an api client instead of a config, it would require clients to make different schemas say if they were in a testing environment with fewer machines than production.

","30/Sep/10 02:24;jbellis;So we all agree that creation is fine, so let's allow that.  (Less sure that Cliff agrees that we should reject writes that can't satisfy the RF, but I won't argue that point unless there's actually something to disagree about. :)",05/Oct/10 12:56;gdusbabek;How about a system test that verifies that a keyspace with RF > N can be created?  With this patch SS.loadSchemaFromYAML() doesn't work when RF > N.,06/Oct/10 19:36;jbellis;added test and fixed loadSchemaFromYAML which was doing a separate check.,"12/Oct/10 14:03;hudson;Integrated in Cassandra #563 (See [https://hudson.apache.org/hudson/job/Cassandra/563/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
overflow in NodeCmd,CASSANDRA-2057,12496751,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,kingryan,kingryan,kingryan,1/26/2011 0:54,3/12/2019 14:01,3/13/2019 22:24,1/26/2011 1:52,0.7.1,,Legacy/Tools,,0,,,,,,We aggregate the long read/write counts across CFs into an int.,,,,,,,,,,,,,,,,,,,,26/Jan/11 00:56;kingryan;nodetool_overflow.patch;https://issues.apache.org/jira/secure/attachment/12469365/nodetool_overflow.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,52:21.2,,,no_permission,,,,,,,,,,,,20418,,,Wed Jan 26 02:28:41 UTC 2011,,,,,,0|i0g927:,92898,jbellis,jbellis,,,,,,,,,"26/Jan/11 01:52;jbellis;committed, thanks!","26/Jan/11 02:28;hudson;Integrated in Cassandra-0.7 #215 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/215/])
    fix potential overflow in nodetool cfstats
patch by Ryan King; reviewed by jbellis for CASSANDRA-2057
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Equality problem in schema updates,CASSANDRA-1962,12495222,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,1/11/2011 9:53,3/12/2019 14:01,3/13/2019 22:24,1/11/2011 14:24,0.7.1,,,,0,,,,,,"CFMetaData.apply uses equals to compare objects that are not the same class: this may work now, but we shouldn't rely on that behaviour.",,,,,,,,,,,,,,,,,,,,11/Jan/11 09:54;stuhood;0001-Strings-never-equal-UTFs.txt;https://issues.apache.org/jira/secure/attachment/12467990/0001-Strings-never-equal-UTFs.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:34.9,,,no_permission,,,,,,,,,,,,20384,,,Tue Jan 11 17:45:36 UTC 2011,,,,,,0|i0g8gv:,92802,slebresne,slebresne,,,,,,,,,11/Jan/11 10:21;slebresne;+1,11/Jan/11 14:24;jbellis;committed,"11/Jan/11 17:45;hudson;Integrated in Cassandra-0.7 #151 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/151/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableWriter.writeStatistics is serializing incorrect data.,CASSANDRA-1976,12495423,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,gdusbabek,gdusbabek,1/12/2011 21:45,3/12/2019 14:01,3/13/2019 22:24,1/18/2011 22:58,0.7.1,,,,0,,,,,,it is serializing rowSizes twice instead of serializing rowSizes and columnCounts.,,;24/Jan/11 17:18;jbellis;600,,0,600,,,0,600,,,,,,,,,,,18/Jan/11 22:45;jbellis;1976.txt;https://issues.apache.org/jira/secure/attachment/12468695/1976.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:09.1,,,no_permission,,,,,,,,,,,,20388,,,Wed Jan 19 00:28:14 UTC 2011,,,,,,0|i0g8jz:,92816,gdusbabek,gdusbabek,,,,,,,,,18/Jan/11 22:48;gdusbabek;+1,18/Jan/11 22:58;jbellis;committed,"19/Jan/11 00:28;hudson;Integrated in Cassandra-0.7 #174 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/174/])
    fix writing SSTable column count statistics
patch by jbellis; reviewed by gdusbabek for CASSANDRA-1976
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table.open has a broken lock in it,CASSANDRA-734,12446409,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jmhodges,jmhodges,1/23/2010 3:52,3/12/2019 14:01,3/13/2019 22:24,1/25/2010 16:55,0.5,,,,0,,,,,,Table.open's lock is used around the Map#put method call but not the #get. This makes it a source of spurious bugs. The attached patch synchronizes the entire Table.open method and removes the unused createLock static.,,,,,,,,,,,,,,,,,,,,25/Jan/10 03:12;jbellis;734-nbhm.txt;https://issues.apache.org/jira/secure/attachment/12431272/734-nbhm.txt,23/Jan/10 03:53;jmhodges;broken_lock_in_table_open.patch;https://issues.apache.org/jira/secure/attachment/12431190/broken_lock_in_table_open.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,38:21.9,,,no_permission,,,,,,,,,,,,19843,,,Mon Jan 25 16:55:41 UTC 2010,,,,,,0|i0g0qv:,91551,,,,,,,,,,,"23/Jan/10 04:38;jbellis;I don't think introducing full synchronized lock into a method called for basically every operation is a good idea.  We could use nonblockinghashmap for the container instead, which would take care of the get problem.  (we could just use NBHM with putIfAbsent if Table creation weren't something we want to avoid doing twice for the same keyspace in case of a race.)  what do Real Java Programmers do for ""singleton cache?""","24/Jan/10 10:50;jmhodges;To start with, synchronization being slow is mostly a scary story left around from the bad old days of Java 1.3 and lower. http://www.ibm.com/developerworks/java/library/j-jtp04223.html

Second, any thing we build instead of using synchronized will be nearly exactly duplicating synchronized's behavior except broken and slower. There seems to be nothing that compiles down to just ""lock around this get and, if that's null, create this other thing and then put it in there"" in Java bytecode. The only other way to make this work is load all the tables at boot time. Which, of course, is a non-starter. However, synchronized does say ""all this work has to be done together"" which fixes our bug and has years of JVM hackers behind it making it as fast as possible. 

Third, we should be aiming as much for correctness as we can. A Cassandra node is eventually consistent, but its codebase is not. Fixing a bug that will, eventually, kick Twitter and Digg and Rackspace's ass now is better than holding off until a ""faster"" way can be found in some possible future where unicorns live and candy mountains are not just scary things in creepy guys basements.

synchronized does a bang up job of fixing this bug now and doing so in a way that is more performant than other ""correct"" ways.

After this patch goes in, we should be re-evaluate all of these calls to Table.open(), though. I'm going to bet that in most cases it would make more sense for the client object to hold on to a reference to the Table if they need it and not let go every time like they do currently after the Table.open() call goes out of scope.

Edited for a friggin' ""it's"" grammar problem.","24/Jan/10 12:54;gdusbabek;> The only other way to make this work is load all the tables at boot time.
See CassandraDaemon.setup().

Why not this:
1.  Create a synchronized initTables() method that is called from CD, that sets an initDone flag and blows up it is ever called again.
2.  Take the locking  and synchronicity out of Table.open() (there really is no point as long as the backing collection is unmodifiable), and turn it into a purely 'getter'-type method.","24/Jan/10 19:00;jmhodges;Sweet. I hadn't seen that call to  Table.getAllTableNames() in CD.setup(). If the laziness constraint can be relaxed, that's fine by me! I'll write up a patch.","24/Jan/10 19:11;jbellis;> synchronization being slow is mostly a scary story left around from the bad old days of Java 1.3 and lower

That's true for _uncontested_ syncs but that is not what we have here.  The JVM isn't going to be able to optimize those away, and it's going to be several orders of magnitude slower than w/o the sync.

> any thing we build instead of using synchronized will be nearly exactly duplicating synchronized's behavior except broken and slower

NBHM is lock-free (which actually means it uses lower-level CAS which is much cheaper).

> we should be aiming as much for correctness as we can

I never said otherwise.  But let's do it without causing unnecessary performance regressions.
","24/Jan/10 19:14;jbellis;> Take the locking and synchronicity out of Table.open() 

wouldn't we just have to undo that for CASSANDRA-44?","24/Jan/10 20:17;jmhodges;bq. NBHM is lock-free (which actually means it uses lower-level CAS which is much cheaper). 

How does a NBHM solve the problem get-and-then-put-but-only-instantiate-the-object-at-all-if-get-is-null? I haven't seen any docs on get with conditional set and conditional instantiation.


bq. wouldn't we just have to undo that for CASSANDRA-44?

Not if we do the initTable work, and then later turn it in a NBHM. With initTable in place, and we go to update a Table, we would only have to do a put without the conditional instantiation.","24/Jan/10 20:43;jmhodges;bq. Not if we do the initTable work, and then later turn it in a NBHM. With initTable in place, and we go to update a Table, we would only have to do a put without the conditional instantiation.

(And it seems to me that if it's possible to construct a Table in more than one thread in our solution to CASSANDRA-44, we're very likely solving CASSANDRA-44 wrong.)","25/Jan/10 01:15;jbellis;> How does a NBHM solve the problem get-and-then-put-but-only-instantiate-the-object-at-all-if-get-is-null?

It doesn't: it solves the problem of doing get() on a thread-unsafe object while remaining high performance.  I'm saying, we can use Table.open in close to its current form by replacing the current HashMap w/ a NBHM, and continuing to use a synchronized block for if the get() is null.

> Not if we do the initTable work, and then later turn it in a NBHM.

True enough, but is that then really simpler than just fixing Table.open?","25/Jan/10 02:54;jmhodges;bq. It doesn't: it solves the problem of doing get() on a thread-unsafe object while remaining high performance. I'm saying, we can use Table.open in close to its current form by replacing the current HashMap w/ a NBHM, and continuing to use a synchronized block for if the get() is null. 

You've forgotten about instantiating the Table twice. One thread notices that the get is null and in another thread the same happens before the first thread manages to do a put.","25/Jan/10 03:12;jbellis;that's why you have to do the second check once you synchronize.  it's a double-checked locking variant, using NBHM to provide thread safety on the initial get() [like you would with volatile, in standard non-broken DCL]

patch attached since i'm clearly not explaining this very well :)",25/Jan/10 04:49;jmhodges;Works for me. Must have missed the last of your sentence.,25/Jan/10 16:55;jbellis;committed to 0.5 branch (for 0.5.1) and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
README.txt inaccuracies,CASSANDRA-448,12435782,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,pquerna,urandom,urandom,9/16/2009 4:41,3/12/2019 14:01,3/13/2019 22:24,9/16/2009 14:49,0.4,0.5,Legacy/Documentation and Website,,0,,,,,,"There are a couple of issues:

 * Argument ordering of a chown statement is incorrect for *BSD systems (see:http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg00903.html)
 * Some paths are out of sync with current examples/recommendations (i.e /var/cassandra/...).",,,,,,,,,,,,,,,,,,,,16/Sep/09 14:21;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-448-README.txt-inaccuracies.txt;https://issues.apache.org/jira/secure/attachment/12419772/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-448-README.txt-inaccuracies.txt,16/Sep/09 04:42;urandom;chown-readme.txt;https://issues.apache.org/jira/secure/attachment/12419725/chown-readme.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,29:01.0,,,no_permission,,,,,,,,,,,,19692,,,Wed Sep 16 14:49:42 UTC 2009,,,,,,0|i0fyzr:,91267,,,,,,,,,,,16/Sep/09 12:29;jbellis;+1,16/Sep/09 14:49;urandom;committed; thanks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NOTICE should not reference pom.xml for dev list,CASSANDRA-415,12434666,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,9/2/2009 20:02,3/12/2019 14:01,3/13/2019 22:24,9/8/2009 13:43,0.4,0.5,Legacy/Documentation and Website,,0,,,,,,This should be avoided for the same reasons as author tags in source files (see also: http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg00728.html),,,,,,,,,,,,,,,,,,,,07/Sep/09 19:44;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-415-remove-developer-info-from-pom.xml.txt;https://issues.apache.org/jira/secure/attachment/12418843/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-415-remove-developer-info-from-pom.xml.txt,07/Sep/09 16:07;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-415-remove-ref-to-dev-list-in-pom.xml.txt;https://issues.apache.org/jira/secure/attachment/12418827/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-415-remove-ref-to-dev-list-in-pom.xml.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,16:58.7,,,no_permission,,,,,,,,,,,,19678,,,Tue Sep 08 13:43:22 UTC 2009,,,,,,0|i0fysf:,91234,,,,,,,,,,,07/Sep/09 16:07;urandom;v1-0001-CASSANDRA-415-remove-ref-to-dev-list-in-pom.xml.txt should be applied to the 0.4 branch,"07/Sep/09 17:16;jbellis;should we r/m the developers section from pom.xml too?  I don't want pom to be cannonical for anything, and I don't want to have to maintain it in addition to the cannonical place either.","07/Sep/09 19:48;urandom;I'm game. 

The attached v1-0001-CASSANDRA-415-remove-developer-info-from-pom.xml.txt patch removes contributor details from pom.xml. As far as I know, this tests out OK, (at least as far as can be expected, see also CASSANDRA-430).",08/Sep/09 00:57;jbellis;+1,08/Sep/09 13:43;urandom;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default configuration broken with UUID change,CASSANDRA-340,12432120,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,euphoria,euphoria,euphoria,8/4/2009 4:15,3/12/2019 14:01,3/13/2019 22:24,8/4/2009 13:05,0.4,,,,0,,,,,,The change to LexicalUUIDType and TimeUUIDType didn't get propagated to the default storage-conf.  This wasn't caught my nosetests since there are no UUID types in the system tests.,,,,,,,,,,,,,,,,,,,,04/Aug/09 04:16;euphoria;240.diff;https://issues.apache.org/jira/secure/attachment/12415450/240.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,05:43.4,,,no_permission,,,,,,,,,,,,19642,,,Tue Aug 04 13:05:43 UTC 2009,,,,,,0|i0fyc7:,91161,,,,,,,,,,,04/Aug/09 04:16;euphoria;Simple fix: UUIDType -> TimeUUIDType,"04/Aug/09 13:05;jbellis;committed, thanks for the catch!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Binary Memtable Example work with getReadStorageEndPoints,CASSANDRA-449,12435789,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,9/16/2009 7:02,3/12/2019 14:01,3/13/2019 22:24,9/21/2009 14:22,0.5,,,,0,,,,,,The method getNSEndPoint() seems to have been renamed or removed. Using getReadStorageEndPoints will work. In another ticket I'll work towards making the example use a RingCache instead.,,,,,,,,,,,,,,,,,,,,21/Sep/09 03:43;lenn0x;0001-Updated-BMT-example-to-use-getReadStorageEndPoints.patch;https://issues.apache.org/jira/secure/attachment/12420157/0001-Updated-BMT-example-to-use-getReadStorageEndPoints.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,22:31.3,,,no_permission,,,,,,,,,,,,19693,,,Tue Sep 22 14:37:51 UTC 2009,,,,,,0|i0fyzz:,91268,,,,,,,,,,,21/Sep/09 14:22;jbellis;committed,"22/Sep/09 14:37;hudson;Integrated in Cassandra #205 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/205/])
    update contrib/CassandraBulkLoader to use getReadStorageEndPoints.  patch by Chris Goffinet; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flexjson.jar -> flexjson-1.7.jar,CASSANDRA-410,12434557,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,9/1/2009 21:08,3/12/2019 14:01,3/13/2019 22:24,9/1/2009 22:09,0.4,0.5,,,0,,,,,,The version of flexjson in our repo is 1.7 (md5sum 2f80efaa1963a8123335c3f79dc31943). It would be useful to rename jar file to make this more obvious.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,21:50.2,,,no_permission,,,,,,,,,,,,19675,,,Tue Sep 01 22:09:05 UTC 2009,,,,,,0|i0a2uv:,56736,,,,,,,,,,,01/Sep/09 21:21;jbellis;+1,01/Sep/09 22:09;urandom;Done.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
long commitlog syncs can cause write pauses,CASSANDRA-668,12444649,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,1/5/2010 2:29,3/12/2019 14:01,3/13/2019 22:24,1/5/2010 17:32,0.5,,,,0,,,,,,"on a heavily loaded system (deliberately exacerbated by running the bonnie++ i/o benchmarking tool at the same time as cassandra -- which might not be too far off from the environment you would see on some VPS hosts), we're seeing CL sync times of 1-5s, causing write pauses.",,,,,,,,,,,,,,,,,,,,05/Jan/10 02:37;jbellis;668.patch;https://issues.apache.org/jira/secure/attachment/12429410/668.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,15:24.6,,,no_permission,,,,,,,,,,,,19810,,,Tue Jan 05 17:32:49 UTC 2010,,,,,,0|i0g0cn:,91487,,,,,,,,,,,"05/Jan/10 02:37;jbellis;increase default sync period, and wait for last sync to finish before submitting another","05/Jan/10 17:15;brandon.williams;+1, no more write pauses with bonnie++ running.","05/Jan/10 17:18;jbellis;There's still some mystery here, because the commitlog was on a separate device + xfs filesystem, so syncing that shouldn't have to wait for IO from the data filesystem.  But short term this patch is better than nothing.",05/Jan/10 17:19;jbellis;(committed patch to 0.5 and trunk),05/Jan/10 17:29;jbellis;(un-tagging 0.5 since any further work almost certainly won't be backported there),"05/Jan/10 17:32;jbellis;(actually I think the best thing to do is tag this 0.5 and resolve it, and create a new issue for further investigation)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy ignores snitch when determining whether to do a local read,CASSANDRA-1317,12470122,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,7/25/2010 22:38,3/12/2019 14:01,3/13/2019 22:24,7/26/2010 21:23,0.6.4,,,,1,,,,,,this primarily affects CASSANDRA-1314 and to a lesser degree CASSANDRA-981,,,,,,,,,,,,,,,,,,,,26/Jul/10 02:34;jbellis;1317.txt;https://issues.apache.org/jira/secure/attachment/12450438/1317.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,53:30.5,,,no_permission,,,,,,,,,,,,20078,,,Thu Jul 29 13:15:18 UTC 2010,,,,,,0|i0g4bj:,92130,,,,,,,,,,,26/Jul/10 19:53;tupshin;In combination with SimpleSnitch from https://issues.apache.org/jira/browse/CASSANDRA-1314 this patch allows for much better cache utilization resulting in a full order of magnitude decrease in disk reads across my entire cluster.,"26/Jul/10 21:23;jbellis;committed.

as a side benefit, it was simplest to refactor things so that local and remote reads in a multiget are done in parallel now (before, we would do all local reads, then all remote ones).","29/Jul/10 13:15;hudson;Integrated in Cassandra #503 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/503/])
    update client_only example for CASSANDRA-1317 changes

Patch by eevans
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrade to antlr 3,CASSANDRA-46,12421910,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,permellqvist,jbellis,jbellis,4/2/2009 19:21,3/12/2019 14:01,3/13/2019 22:24,4/3/2009 21:28,,,,,0,,,,,,"Currently we require both antlr 3 and antlr 2 to build.  This seems broken.

(Also, a newer version of antlr is out, which might be useful.  As of this writing the latest is 3.1.3.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,47:18.5,,,no_permission,,,,,,,,,,,,19527,,,Fri Apr 03 21:28:14 UTC 2009,,,,,,0|i0fwjj:,90870,,,,,,,,,,,"02/Apr/09 21:47;permellqvist;Solution tested locally (upgrade antlr)

- Put antlr-3.1.3 in lib/ 
http://www.antlr.org/download/antlr-3.1.3.jar

- Remove all other antlr-jars and stringtemplate-3.0.jar from lib/

- Update build xml with correct classpaths and new argument for output dir


      <java classname=""org.antlr.Tool""

            classpath=""${build.lib}/antlr-3.1.3.jar""

            fork=""true"">

         <arg value=""${build.src}/org/apache/cassandra/cli/Cli.g"" />

         <arg value=""-o"" />
         <arg value=""${build.src}/org/apache/cassandra/cli/"" />

      </java>

      <java classname=""org.antlr.Tool""

            classpath=""${build.lib}/antlr-3.1.3.jar""

            fork=""true"">

         <arg value=""${build.src}/org/apache/cassandra/cql/compiler/parse/Cql.g"" />

         <arg value=""-o"" />
         <arg value=""${build.src}/org/apache/cassandra/cql/compiler/parse/"" />

      </java>

- Change src/org/apache/cassandra/cql/compiler/common/CqlCompiler.java so that signature of recoverFromMismatchedSet method returns Object

        public Object recoverFromMismatchedSet(IntStream input,
                                             RecognitionException re,
                                             BitSet follow) throws RecognitionException
        {
            throw re;
        }",03/Apr/09 17:00;jbellis;do we need the antlr-runtime jar that's currently in lib/ ?,"03/Apr/09 20:39;permellqvist;Works just fine for me without the antlr-runtime jar. Only antlr related jar needed is antlr-3.1.3.jar
I guess if there were separate lib-directories for build-time and run-time we could save a few bytes in the run-time distribution by using antlr-runtime there.
Probably not worth the effort?",03/Apr/09 21:28;jbellis;applied.  please use svn diff in the future; patching manually is a pain.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli is not executable,CASSANDRA-229,12427841,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,appodictic,appodictic,6/14/2009 18:37,3/12/2019 14:01,3/13/2019 22:24,6/16/2009 19:33,0.4,,Legacy/Tools,,0,,,,,,After producing a Cassandra jar bin/cassandra-cli is not executable,,,,,,,,,,,,,,,,,,,,16/Jun/09 19:24;urandom;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-229-install-shell-scripts-executable.txt;https://issues.apache.org/jira/secure/attachment/12410845/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-229-install-shell-scripts-executable.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,48:51.3,,,no_permission,,,,,,,,,,,,19602,,,Wed Jun 17 12:34:49 UTC 2009,,,,,,0|i0fxnr:,91051,,,,,,,,,,,"15/Jun/09 21:48;urandom;I'm not seeing this. On my machine bin/cassandra-cli is executable both after fresh checkout and after running ""ant jar"".","15/Jun/09 21:58;appodictic;Excuse me, I was not running ant jar.

I did 'ant release'. apache-cassandra-incubating-0.4.0-dev-bin.tar.gz when extracted did not have execute on the cassandra-cli. 

[ecapriolo@laptop1 cassandra-trunk]$ ant release
[ecapriolo@laptop1 bin]$ ls -lag
total 24
drwxr-xr-x 2 ecapriolo 4096 Jun 15 17:55 .
drwxrwxr-x 7 ecapriolo 4096 Jun 15 17:55 ..
-rwxr-xr-x 1 ecapriolo 3127 Jun 14 13:59 cassandra
-rw-r--r-- 1 ecapriolo 1129 Jun 14 13:59 cassandra-cli
-rw-r--r-- 1 ecapriolo 1981 Jun 14 13:59 cassandra.in.sh
-rw-r--r-- 1 ecapriolo 1172 Jun 14 13:59 stop-server
",16/Jun/09 14:28;urandom;You are right. Thanks for the additional information.,16/Jun/09 19:26;urandom;The attached patch installs the scripts (not the include(s)) into the tarball with the execute bit set.,16/Jun/09 19:33;jbellis;committed,"17/Jun/09 12:34;hudson;Integrated in Cassandra #111 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/111/])
    install bin/ scripts as executable when building release.  patch by Eric Evans; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hinted handoff reads all hints for a single keyspace into memory,CASSANDRA-680,12444868,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,1/7/2010 5:12,3/12/2019 14:01,3/13/2019 22:24,1/9/2010 19:03,0.6,,,,0,,,,,,Need to add paging to HHOM.deliverAllHints,,,,,,,,,,,,,,,,,,,,07/Jan/10 20:37;jbellis;ASF.LICENSE.NOT.GRANTED--0001-add-hint-delivery-paging.txt;https://issues.apache.org/jira/secure/attachment/12429671/ASF.LICENSE.NOT.GRANTED--0001-add-hint-delivery-paging.txt,07/Jan/10 20:37;jbellis;ASF.LICENSE.NOT.GRANTED--0002-cleanup-of-HHOM.txt;https://issues.apache.org/jira/secure/attachment/12429672/ASF.LICENSE.NOT.GRANTED--0002-cleanup-of-HHOM.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,03:24.4,,,no_permission,,,,,,,,,,,,19815,,,Sat Jan 09 19:03:04 UTC 2010,,,,,,0|i0g0f3:,91498,,,,,,,,,,,"08/Jan/10 02:03;brandon.williams;+1, this works.  The nodes keep going, but eventually run into CASSANDRA-16 when the hints family is compacted.","08/Jan/10 18:25;stuhood;Jonathan: I'm not exactly sure what the rules are, but I think when you attach patches that you intend to commit, you should check the 'Grant license to ASF' radio button to make everything kosher legally.","08/Jan/10 18:35;jbellis;Even the ASF understands that when I commit a patch I wrote myself, I'm giving them permission to use the code even if I didn't check a box in Jira.  (Seriously.)","09/Jan/10 12:35;hudson;Integrated in Cassandra #318 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/318/])
    cleanup of HHOM.  patch by jbellis; tested by Brandon Williams for .
add hint delivery paging.  patch by jbellis; tested by Brandon Williams for 
",09/Jan/10 19:03;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
multiple seeds (only when seed count = node count?) can cause cluster partition,CASSANDRA-150,12424825,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jaakko,jbellis,jbellis,5/7/2009 16:13,3/12/2019 14:01,3/13/2019 22:24,11/26/2009 2:16,0.5,,,,0,,,,,,happens fairly frequently on my test cluster of 5 nodes.  (i normally restart all nodes at once when updating the code.  haven't tested w/ restarting one machine at a time.),,,,,,,,,,,,,,,,,,,,25/Nov/09 12:58;jaakko;150.patch;https://issues.apache.org/jira/secure/attachment/12426096/150.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,59:46.9,,,no_permission,,,,,,,,,,,,19569,,,Thu Nov 26 12:34:02 UTC 2009,,,,,,0|i0fx6f:,90973,,,,,,,,,,,"07/May/09 16:21;jbellis;daishi's ""Unable to find a live Endpoint we might be out of live nodes""  bug was almost certainly caused by the same thing.  (he has a 3-node cluster.)

for both of us switching to a single seed has fixed the issue for now.  (but ultimately we do want to support multiple seeds for redundancy.)","24/Nov/09 13:59;jaakko;Network partition may happen if (1) cluster size is at least four nodes, (2) all nodes are seeds and (3) at least two nodes boot ""simultaneously"".

Gossiping cycle works as follows:
(i) gossip to random live node
(ii) gossip to random unreachable node
(iii) if the node gossiped to at (i) was not seed, gossip to random seed

Suppose there are four nodes in the cluster: nodeA, nodeB, nodeC and nodeD, all of them seeds. Suppose they are all brought online at the same time. Following event sequence leads to partition:

(1) nodeA comes online. No live nodes (and no unreachable either, of course), so gossip to random seed. Let's suppose nodeA chooses nodeB. It sends nodeB gossip.
(2) nodeB gets nodeA's gossip and marks it live. It sends its own gossip, and since it has a live node (nodeA), it sends gossip according to gossip's first rule. nodeA is seed, so no gossip is sent to random seed at (iii).
(3) nodeC comes online. It has not seen other live nodes yet, so it will gossip to random seed. Let's suppose it chooses nodeD.
(4) nodeD comes online and sees nodeC's gossip. Since it now has a live node, it will send nodeC gossip according to the first rule. Since nodeC is seed, again no gossip is sent to random seed.

(there are other sequences as well, but basic idea is the same)

Now all nodes know of one live node, so they will always send gossip according to the first rule. Since this node is seed, they will never send gossip to random seed according to rule three. This will prevent them from finding rest of the cluster. One non-seed node will break this loop, as gossip sent to it will trigger gossip to random seed.

While investigating this, I noticed we might have caused some harm to scalability of gossip mechanism when we added two new application states for node movement. I'll fix this bug tomorrow when checking if there is a problem.
",24/Nov/09 14:32;jbellis;That makes total sense.  Nice work!,"25/Nov/09 12:58;jaakko;Added extra condition to send gossip to random seed also if liveEndpoints.size + unreachableEndpoints.size is less than seeds.size. This will cause us to send same gossip to same seed twice occasionally (when the seed is live, but we have not yet seen enough nodes), but I think this is OK, as this is quite special case and will go away as soon as we've seen enough nodes.

Another option would be to add extra parameter excludeThisNode to sendGossip and not send gossip if random returns that address, but IMHO this option is messy and gains very little.
","25/Nov/09 23:25;jbellis;I don't think this quite works -- e.g. the guy on the mailing list with 3 seeds in a 4 node cluster.  I think we have to make the check ""have we seen all the seeds yet"" rather than ""have we seen as many nodes as there are seeds""","26/Nov/09 01:33;jaakko;This kind of partition cannot happen if there are less than four seeds or there is even a single non-seed node. There must be enough seeds to form at least two separate network closures of at least two seeds each. If there has been a problem with 3/4 cluster, it must be different from this as there are two preconditions that are not met.

Gossip rule #1 sends gossip to a live node and rule #3 sends to a random seed if the node in #1 was not seed. If there is even a single non-seed node, it will trigger gossip to a random seed every time gossip is sent to it. Eventually this will break the network closures. What the patch basically does is it aggressively searches for seeds as long as it has found at least as many nodes as there are seeds. It does not matter even if this does not include all seeds, as that means there are non-seeds in liveEndpoints, which triggers search for random seed every time gossip is sent to it. So basically this is just to help Gossiper to get started, not to find all seeds. Whether it finds all seeds or at least one non-seed does not matter, it can continue from there.

Now of course the ""correct"" checks for this condition would be to on each gossip round check (1) whether liveEndpoints and unreachableEndpoints include all seeds or (2) if liveEndpoints includes at least one non-seed. However, putting these checks on the normal execution path only for the sake of one special case does not appeal to me, so decided to add this simple check instead.

Now that I think of it, there is one extremely special case that still could cause a partition: cluster of 4 seeds and 2 non-seeds. First 2 seeds and 2 non-seeds come online -> everybody is happy as cluster size is the same as number of seeds. Now both seeds go down, and then the other two seeds come up. Again everybody is happy. Now suppose the two non-seeds go down, and after that the two original seeds come up simultaneously, and happen to choose each other from the list of random seeds. In this case all seeds will send gossip only to the other seed, as they have 2 nodes in unreachableEndpoint, which makes the total number of seen nodes equal number of seeds. To avoid this, we might relax the condition a bit and send gossip to a seed if number of liveEndpoints is less than seeds (that is, ignore unreachableEndpoints). This modification would take care of the scenario above, but don't know if it is worth the trouble. If either of the non-seeds recovers (or one of the seeds goes down), this deadlock will be broken.
","26/Nov/09 01:50;jaakko;It might indeed be better to check only if liveEndpoints.size < seeds.size (do not count unreachableEndpoints). This will cause a bit more unnecessary gossip to seeds in some special cases, but is perhaps better approach. Have to think about this a bit still.","26/Nov/09 02:12;jbellis;I see, so to make the less-strong check be enough when seeds.size <= live node count we reason that:

either all the live nodes are seeds, in which case non-seeds that come online will introduce themselves to a member of the ring by definition, and become known in turn,

or there is at least one non-seed node in the list, in which case eventually someone will gossip to it, and then do a gossip to a random seed from the existing clause in the if statement.

> It might indeed be better to check only if liveEndpoints.size < seeds.size

yes, let's go with this.  better to do a little extra gossiping in corner cases than risk indefinite partitions.",26/Nov/09 02:16;jbellis;committed as described above,"26/Nov/09 12:34;hudson;Integrated in Cassandra #269 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/269/])
    send extra gossip to random seed as long as there are less nodes alive than seed nodes configured
patch by Jaakko Laine; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pom.xml out of date,CASSANDRA-430,12435036,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,niallp,urandom,urandom,9/7/2009 19:42,3/12/2019 14:01,3/13/2019 22:24,10/13/2009 22:55,0.4,0.5,Legacy/Tools,,0,,,,,,The pom.xml file is out of date (at least )with respect to current dependencies.,,,,,,,,,,,,,,,,,,,,13/Oct/09 21:02;niallp;CASSANDRA-430-pom-dependencies.patch;https://issues.apache.org/jira/secure/attachment/12422021/CASSANDRA-430-pom-dependencies.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,02:23.9,,,no_permission,,,,,,,,,,,,19686,,,Tue Oct 13 22:55:04 UTC 2009,,,,,,0|i0fyvr:,91249,,,,,,,,,,,"13/Oct/09 18:19;urandom;Tag, you're it.",13/Oct/09 21:02;niallp;Attaching a patch for the pom.xml that fixes the dependencies and excludes TestRingCache from the tests (fails because it contains no tests),"13/Oct/09 22:29;euphoria;Thanks for the patch Niall, +1",13/Oct/09 22:55;urandom;committed; thanks for the patch!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
enable/disable HH via JMX,CASSANDRA-1550,12475318,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,9/28/2010 17:20,3/12/2019 14:01,3/13/2019 22:24,9/30/2010 16:59,0.6.6,0.7 beta 3,Legacy/Tools,,0,,,,,,,,,,,,,,,,,,,,,,,,,,30/Sep/10 16:16;jbellis;1550.txt;https://issues.apache.org/jira/secure/attachment/12456023/1550.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,23:41.9,,,no_permission,,,,,,,,,,,,20196,,,Wed Mar 02 21:42:10 UTC 2011,,,,,,0|i0g5x3:,92389,brandon.williams,brandon.williams,,,,,,,,,30/Sep/10 16:17;jbellis;(patch vs 0.6),30/Sep/10 16:23;brandon.williams;+1,30/Sep/10 16:59;jbellis;committed,"25/Feb/11 07:33;nar3ndra;Cassandra 0.7.2/trunk. The default constructor in StorageProxy is private. As a result the Operations cannot be executed from JMX. Hence, this change is not working.",25/Feb/11 15:54;jbellis;the StorageProxy mbean is created once requests start arriving,"02/Mar/11 21:42;nar3ndra;Even though the load is on and read/writes happening, I don't see ""operations"" component on Jconsole. To clarify further, I see only Jconsole->MBeans->org.apache.cassandra.db.StorageProxy.Attributes. I don't see Jconsole->MBeans->org.apache.cassandra.db.StorageProxy.Operations. As a result I cannot operation like enable/disable HH. And all this while read/writes are happening on the node.

Am I missing something here?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FBUtilities.hexToBytes() doesn't accommodate odd-length strings.,CASSANDRA-1411,12472038,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,8/19/2010 20:49,3/12/2019 14:01,3/13/2019 22:24,8/19/2010 21:02,0.7 beta 2,,,,0,,,,,,"This is a problem when a user specifies ByteOrderedPartitioner with an odd-length initial token (like ""0"").",,,,,,,,,,,,,,,,,,,,19/Aug/10 20:53;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-FBUtilities.hexToBytes-doesn-t-handle-odd-length-strin.txt;https://issues.apache.org/jira/secure/attachment/12452572/ASF.LICENSE.NOT.GRANTED--v1-0001-FBUtilities.hexToBytes-doesn-t-handle-odd-length-strin.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,56:23.8,,,no_permission,,,,,,,,,,,,20124,,,Sat Aug 21 11:14:34 UTC 2010,,,,,,0|i0g4w7:,92223,,,,,,,,,,,19/Aug/10 20:56;jhermes;+1 anti-annoyance patch.,20/Aug/10 15:56;messi;Good ol' well-thought pragmatism? Why sanitize user input and adjust the token when you can hide errors by making core functions more flexible.,"20/Aug/10 16:02;gdusbabek;Erroring on odd-lengthed, yet still valid hex values is neither pragmatic or very useful.  hex 0 is dec 0, hex 101 is dec 257, so is hex 0101; let's just handle them.","21/Aug/10 11:14;hudson;Integrated in Cassandra #518 (See [https://hudson.apache.org/hudson/job/Cassandra/518/])
    FBUtilities.hexToBytes doesn't handle odd-length strings. patch by Gary Dusbabek, reviewed by Jon Hermes. CASSANDRA-1411
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wordcount contrib does not work in Hadoop distributed mode,CASSANDRA-817,12457030,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,johanoskarsson,johanoskarsson,johanoskarsson,2/22/2010 16:41,3/12/2019 14:01,3/13/2019 22:24,2/22/2010 17:23,0.6,,,,0,,,,,,The column name is set in a static variable in the job setup. That variable will be empty when the job has been distributed to the tasktrackers. The variable must be set via the setup method in the mapper.,,,,,,,,,,,,,,,,,,,,22/Feb/10 16:46;johanoskarsson;CASSANDRA-817.patch;https://issues.apache.org/jira/secure/attachment/12436594/CASSANDRA-817.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,51:09.9,,,no_permission,,,,,,,,,,,,19879,,,Wed Feb 24 13:07:31 UTC 2010,,,,,,0|i0g19b:,91634,,,,,,,,,,,22/Feb/10 16:46;johanoskarsson;Sets the column name using the configuration and the setup method.,22/Feb/10 16:51;jbellis;+1,22/Feb/10 17:23;johanoskarsson;Committed to trunk and the 0.6 branch.,"24/Feb/10 13:07;hudson;Integrated in Cassandra #364 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/364/])
    Fix bug to allow distributed Hadoop jobs. Patch by johan, review by jbellis. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming code relies on sockets being bound to the correct address (InetAddress.anyLocalAddress() is bad),CASSANDRA-737,12446594,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,1/25/2010 19:46,3/12/2019 14:01,3/13/2019 22:24,1/25/2010 21:26,0.6,,,,0,,,,,,"I came across this while testing streaming locally.  The new streaming code makes use of the remote socket address supplied by the socket.  This means that it will return whatever address the socket is bound to, which is not necessarily the address configured for cassandra.  This confuses StreamContextManager when data comes streaming in from addresses that it doesn't recognize.

Two solutions will work.
1. bind outgoing sockets to the correct interface.
2. Include the local address in StreamContexts that get sent.

I opted for 1 since it required less code.  2 was easy enough but would have required changing the format of the message to make the source address more easily accessible (the constructor for IncomingStreamReader wants to know the source host to create the stream context at the destination).",Mac OS X.,,,,,,,,,,,,,,,CASSANDRA-620,,,,25/Jan/10 20:52;gdusbabek;0001-bind-sockets-locally-to-cassandra-specified-address.patch;https://issues.apache.org/jira/secure/attachment/12431355/0001-bind-sockets-locally-to-cassandra-specified-address.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,55:33.0,,,no_permission,,,,,,,,,,,,19845,,,Mon Jan 25 21:26:59 UTC 2010,,,,,,0|i0g0rj:,91554,,,,,,,,,,,25/Jan/10 19:54;gdusbabek;I think I caught all the socket creation places.,25/Jan/10 19:55;jbellis;does this affect 0.5 too?,"25/Jan/10 20:01;gdusbabek;No. This came in at the same time as IncomingStreamReader.  Before that, I don't think we relied on the remote address of a socket being the address specified in the config.  It hadn't mattered.",25/Jan/10 20:45;jbellis;is your working copy up to date?  the patch to OutboundTcpConnection.java fails to apply for me,25/Jan/10 20:52;gdusbabek;The last patch was Intellij generated.  Here is a git-generated patch.,"25/Jan/10 20:59;jbellis;+1 (damn intellij, it's not like patch format hasn't been standard for years)",25/Jan/10 21:26;gdusbabek;r902981,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
junit and antlr jars should not be in binary dist,CASSANDRA-417,12434668,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,9/2/2009 20:28,3/12/2019 14:01,3/13/2019 22:24,5/6/2010 18:09,,,,,0,,,,,,"The only third-party jars that should be shipped in the binary distribution, are those that are needed at runtime (see also: http://www.mail-archive.com/cassandra-dev@incubator.apache.org/msg00728.html)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,19679,,,Thu May 06 18:09:32 UTC 2010,,,,,,0|i0fysv:,91236,,,,,,,,,,,"06/May/10 18:09;urandom;As of 0.6.0, Ivy is being used for build-time dependency management and JUnit (among others), are no longer being shipped in the binary artifacts. Antlr _is_, but the components needed for build, also require the runtime components, making it more complex than it is worth to separate them.

Closing this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up after failed (repair) streaming operation,CASSANDRA-2088,12497286,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,stuhood,stuhood,2/1/2011 5:49,3/12/2019 14:01,3/13/2019 22:24,4/13/2011 21:06,0.7.5,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,,12/Apr/11 17:45;slebresne;0001-Better-detect-failures-from-the-other-side-in-Incomi.patch;https://issues.apache.org/jira/secure/attachment/12476140/0001-Better-detect-failures-from-the-other-side-in-Incomi.patch,11/Apr/11 05:50;amorton;0001-detect-streaming-failures-and-cleanup-temp-files.patch;https://issues.apache.org/jira/secure/attachment/12475965/0001-detect-streaming-failures-and-cleanup-temp-files.patch,11/Apr/11 05:50;amorton;0002-delete-partial-sstable-if-compaction-error.patch;https://issues.apache.org/jira/secure/attachment/12475966/0002-delete-partial-sstable-if-compaction-error.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,23:40.9,,,no_permission,,,,,,,,,,,,20437,,,Wed Apr 13 21:06:38 UTC 2011,,,,,,0|i0g98n:,92927,jbellis,jbellis,,,,,,,,,"01/Feb/11 05:53;stuhood;Regarding repair: http://www.mail-archive.com/user@cassandra.apache.org/msg09259.html
And compaction: CASSANDRA-2084","07/Mar/11 05:23;amorton;I'm keen to try this ticket (to learn more about compaction and repair) if it's not already been worked on. Also if it's ok for me to take a couple of days while I dig into this.

For compaction I'm looking in
- CompactionManager.doCompaction where it creates a new SSTableWriter via cfs.createCompactionWriter() 
- CompactionManager.doCleanupCompaction() also uses an SSTableWriter

Are the sorts of failures we're considering for compaction ones that come from the CompactionIterator or SSTableScanner ?

For repair I'm looking in:
- IncomingStreamReader appears to clean up the temporary pending file in some error situations. Do we have any more info on the sorts of failures here? e.g. If there is an IOException sending the re-stream message, or a non checked exception it will fail to cleaup the file. 
- I'm looking into what happens in StreamInSession.finished() closeIfFinished()
- Are we considering failures during the streaming or when processing the data after the stream has finished?

Any guidance welcome. ","07/Mar/11 16:59;jbellis;bq. Are the sorts of failures we're considering for compaction ones that come from the CompactionIterator or SSTableScanner ?

Both. Also I suppose it's possible for the writer to error out from lack of disk space since it only checks at the beginning for space and doesn't ""reserve"" it vs flushes.

bq. Are we considering failures during the streaming or when processing the data after the stream has finished?

The former is much more common (I've never seen the latter reported), so I'd start with that.","11/Apr/11 05:50;amorton;patch 0001 tracks failures during AES streaming, files for failed Stream sessions are cleaned up and repair is allowed to continue. Failed files are logged at the StreamSession, TreeRequest, and RepairSession level. 

patch 0002 handle exceptions when doing a (normal) compaction and deletes the temp SSTable. The SSTableWriter components are closed before deletion so that windows will delete correctly. ","12/Apr/11 17:45;slebresne;I think there is a few different things here and I think we should separate them somehow.

Fixing the fact that streaming leave tmp files around when it fails is a 2 lines fix and I think this is simple enough that it could go to 0.7. I'm attaching a patch against 0.7. It's extracted from Aaron first patch, although rebased on 0.7 (and fix a bug).

Making repair aware that there has been some failures is actually more complicated so that should go in 0.8.1 or something (and should go to CASSANDRA-2433 or another ticket that describe the problem better). ","12/Apr/11 17:51;jbellis;bq. I'm attaching a patch against 0.7

Is that 0001-Better-detect-failures-from-the-other-side-in-Incomi.patch?  I don't see the connection to .tmp files.  (Also: have you verified that the channel will actually infinite-loop returning 0?  Kind of odd behavior, although I guess it's technically within-spec.)","12/Apr/11 18:21;slebresne;bq. Is that 0001-Better-detect-failures-from-the-other-side-in-Incomi.patch? I don't see the connection to .tmp files. (Also: have you verified that the channel will actually infinite-loop returning 0? Kind of odd behavior, although I guess it's technically within-spec.)

Yes. IncomingStreamReader does clean the tmp file when there is an expection (there's an enclosing 'try catch'). The problem is that no exception is raised if the other side of the connection dies. What will happen then is the read will infinitely read 0 bytes. So this actually avoid the infinite loop returning 0 (and so I think answered your second question, so it wasn't very clear).

Note that without this patch, there is an infinite loop that will hold a socket open forever (and consume cpu, though very few probably in that case). So this is not just merely a fix of deleting the tmp files. But it does as a consequence of correctly raising an exception when should be.","12/Apr/11 18:25;jbellis;+1, and can you move some of that explanation inline as a comment?","12/Apr/11 19:39;slebresne;Committed that first part. I think we should keep that open to fix the tmp files for failed compaction and move the rest to another ticket (like CASSANDRA-2433 for instance).

About the attached patch on cleaning up failed compaction:
  * We should also handle cleanup and scrub
  * We should handle SSTableWriter.Builder as it is yet another place where we could miss to cleanup a tmp file on error.
  * In theory a failed flush could leave a tmp file behind. If that happens having a tmp file would be the least of your problem but for completeness sake we could handle it.
  * The logging when failing to close iwriter and dataFile in SSTableWriter could probably go at error (we should not be failing there, if we do something is wrong)
  * That's nitpick but I'm not a huge fan of catching RuntimeException in this case as this pollute the code for something that would be a programming error (that's probably debatable though). Maybe another solution would be to have this in the final block. It means making sure closeAndDelete() is ok with the file being already closed and/or deleted and having this final block *after* the closeAndOpenReader call.
","13/Apr/11 03:08;amorton;Thanks will take another look at the cleanup for compaction. 
",13/Apr/11 21:06;jbellis;Created CASSANDRA-2468 for compaction cleanup. Will close this one for streaming.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadResponseResolver might miss an inconsistency,CASSANDRA-1830,12492543,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tilgovi,jbellis,jbellis,12/6/2010 21:52,3/12/2019 14:01,3/13/2019 22:24,12/15/2010 16:10,0.6.9,0.7.0 rc 3,,,0,,,,,,"Rather than comparing the digests of all the digest requests to one another, the last one seen ""wins"" and is compared to the digest of each version seen from a data request.",,,,,,,,,,,,,,,,,,,,06/Dec/10 21:53;jbellis;1830.txt;https://issues.apache.org/jira/secure/attachment/12465620/1830.txt,15/Dec/10 15:47;slebresne;1830_0.7.txt;https://issues.apache.org/jira/secure/attachment/12466326/1830_0.7.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,47:58.0,,,no_permission,,,,,,,,,,,,20330,,,Tue Dec 21 21:48:55 UTC 2010,,,,,,0|i0g7o7:,92673,slebresne,slebresne,,,,,,,,,06/Dec/10 21:53;jbellis;Patch based on Randall Leeds's from CASSANDRA-982.,"15/Dec/10 15:47;slebresne;+1 on v6 version (very minor comment: the import of ColumnFamily in DigestMismatchException.java is unnecessary)

Also attaching a port to 0.7.","15/Dec/10 16:10;jbellis;committed, thanks!","15/Dec/10 16:43;hudson;Integrated in Cassandra-0.6 #23 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/23/])
    ReadResponseResolver check digests against each other
patch by Randall Leeds and jbellis; reviewed by slebrense for CASSANDRA-1830
","21/Dec/10 21:48;hudson;Integrated in Cassandra-0.6 #32 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/32/])
    fix NPE regression caused by CASSANDRA-1830
patch by jbellis; reviewed by mdennis and tjake
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gossip throws IllegalStateException,CASSANDRA-1343,12470543,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,7/30/2010 19:34,3/12/2019 14:01,3/13/2019 22:24,8/3/2010 18:37,0.7 beta 1,,,,0,,,,,,"when starting a second node, gossip throws IllegalStateException when KS with RF>1 defined on an existing 1-node cluster.

we should be able to define keyspaces on a cluster of any size.",,,,,,,,,,,,,,,,CASSANDRA-1428,,,,03/Aug/10 18:23;gdusbabek;0001-complain-if-there-aren-t-enough-nodes-to-support-req.patch;https://issues.apache.org/jira/secure/attachment/12451143/0001-complain-if-there-aren-t-enough-nodes-to-support-req.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,33:31.7,,,no_permission,,,,,,,,,,,,20090,,,Fri Sep 03 23:28:09 UTC 2010,,,,,,0|i0g4h3:,92155,,,,,,,,,,,"30/Jul/10 19:43;gdusbabek;/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:54219,suspend=y,server=n -ea -Xms128M -Xmx1G -XX:TargetSurvivorRatio=90 -XX:+AggressiveOpts -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+HeapDumpOnOutOfMemoryError -XX:SurvivorRatio=128 -XX:MaxTenuringThreshold=0 -Dcom.sun.management.jmxremote.port=8081 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcassandra -Dcassandra-foreground=yes -Dlog4j.configuration=log4j-server.properties -Dmx4jport=9081 -Dfile.encoding=MacRoman -classpath /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/deploy.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/dt.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/javaws.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/jce.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/management-agent.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/plugin.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/sa-jdi.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/alt-rt.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/charsets.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/classes.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/dt.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/jce.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/jconsole.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/jsse.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/laf.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/management-agent.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/../Classes/ui.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/apple_provider.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/dnsns.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/localedata.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/sunjce_provider.jar:/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/lib/ext/sunpkcs11.jar:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/conf1:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/core:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jackson-mapper-asl-1.4.0.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/log4j-1.2.14.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/antlr-3.1.3.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/clhm-production.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-cli-1.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jetty-6.1.21.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jackson-core-asl-1.4.0.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/slf4j-log4j12-1.5.8.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/hadoop-core-0.20.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jug-2.0.0.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/slf4j-api-1.5.8.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jetty-util-6.1.21.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-collections-3.2.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/jline-0.9.94.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/servlet-api-2.5-20081211.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/high-scale-lib.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-codec-1.2.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/commons-lang-2.4.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/libthrift-r959516.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/json-simple-1.1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/snakeyaml-1.6.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/avro-1.3.3~cust2.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/guava-r05.jar:/Users/gary.dusbabek/codes/apache/git-trunk/lib/avro-1.3.3-sources~cust1.jar:/Users/gary.dusbabek/codes/apache/git-trunk/build/lib/jars/junit-4.6.jar:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/thrift:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/avro:/Users/gary.dusbabek/codes/apache/git-trunk/out/production/ext:/Applications/IntelliJ IDEA 9.0.1.app/lib/idea_rt.jar org.apache.cassandra.thrift.CassandraDaemon
Connected to the target VM, address: '127.0.0.1:54219', transport: 'socket'
 INFO 13:42:52,352 [main] Loading settings from /Users/gary.dusbabek/codes/apache/git-trunk/out/production/conf1/cassandra.yaml
DEBUG 13:42:52,613 [main] Syncing log with a period of 10000
 INFO 13:42:52,613 [main] DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
DEBUG 13:42:52,628 [main] setting auto_bootstrap to true
DEBUG 13:42:52,768 [main] Starting CFS Statistics
DEBUG 13:42:52,786 [main] Starting CFS Schema
DEBUG 13:42:52,788 [main] Starting CFS Migrations
DEBUG 13:42:52,800 [main] Starting CFS LocationInfo
DEBUG 13:42:52,844 [main] Starting CFS HintsColumnFamily
 INFO 13:42:52,891 [main] Couldn't detect any schema definitions in local storage.
 INFO 13:42:52,891 [main] Found table data in data directories. Consider using JMX to call org.apache.cassandra.service.StorageService.loadSchemaFromYaml().
 INFO 13:42:52,900 [main] Replaying /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515326767.log
DEBUG 13:42:52,902 [main] Replaying /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515326767.log starting at 276
DEBUG 13:42:52,903 [main] Reading mutation at 276
DEBUG 13:42:52,910 [main] replaying mutation for system.[B@5f9299f5: {ColumnFamily(LocationInfo [B:false:1@1280515327074,])}
 INFO 13:42:52,945 [main] Finished reading /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515326767.log
DEBUG 13:42:52,946 [main] Finished waiting on mutations from recovery
 INFO 13:42:52,947 [main] Creating new commitlog segment /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log
 INFO 13:42:52,953 [main] switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=0)
 INFO 13:42:52,966 [main] Enqueuing flush of Memtable-LocationInfo@828432489(17 bytes, 1 operations)
 INFO 13:42:52,968 [FLUSH-WRITER-POOL:1] Writing Memtable-LocationInfo@828432489(17 bytes, 1 operations)
 INFO 13:42:53,050 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-2-Data.db
DEBUG 13:42:53,095 [CompactionExecutor:1] Checking to see if compaction of LocationInfo would be useful
DEBUG 13:42:53,096 [MEMTABLE-POST-FLUSHER:1] Discarding 1000
DEBUG 13:42:53,098 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=0), column family 1000.
DEBUG 13:42:53,098 [COMMIT-LOG-WRITER] Marking replay position 0 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log)
 INFO 13:42:53,100 [main] Recovery complete
DEBUG 13:42:53,101 [main] Deleting CommitLog-1280515326767.log
 INFO 13:42:53,102 [main] Log replay complete
DEBUG 13:42:53,116 [CompactionExecutor:1] Estimating compactions for HintsColumnFamily
DEBUG 13:42:53,117 [CompactionExecutor:1] Estimating compactions for LocationInfo
DEBUG 13:42:53,117 [CompactionExecutor:1] Estimating compactions for Schema
DEBUG 13:42:53,117 [CompactionExecutor:1] Estimating compactions for Migrations
DEBUG 13:42:53,117 [CompactionExecutor:1] Estimating compactions for Statistics
DEBUG 13:42:53,117 [CompactionExecutor:1] Checking to see if compaction of HintsColumnFamily would be useful
 INFO 13:42:53,117 [main] Cassandra version: 0.7.0-SNAPSHOT
 INFO 13:42:53,117 [main] Thrift API version: 9.0.0
DEBUG 13:42:53,117 [CompactionExecutor:1] Checking to see if compaction of LocationInfo would be useful
DEBUG 13:42:53,118 [CompactionExecutor:1] Checking to see if compaction of Schema would be useful
DEBUG 13:42:53,118 [CompactionExecutor:1] Checking to see if compaction of Migrations would be useful
DEBUG 13:42:53,118 [CompactionExecutor:1] Checking to see if compaction of Statistics would be useful
 INFO 13:42:53,118 [main] Saved Token found: 127492708246848026497487109173721015738
 INFO 13:42:53,119 [main] Saved ClusterName found: Test Cluster
 INFO 13:42:53,119 [main] Saved partitioner not found. Using org.apache.cassandra.dht.RandomPartitioner
 INFO 13:42:53,120 [main] switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=276)
 INFO 13:42:53,120 [main] Enqueuing flush of Memtable-LocationInfo@1780804346(95 bytes, 2 operations)
 INFO 13:42:53,120 [FLUSH-WRITER-POOL:1] Writing Memtable-LocationInfo@1780804346(95 bytes, 2 operations)
 INFO 13:42:53,282 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-3-Data.db
DEBUG 13:42:53,283 [MEMTABLE-POST-FLUSHER:1] Discarding 1000
DEBUG 13:42:53,306 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=276), column family 1000.
DEBUG 13:42:53,306 [COMMIT-LOG-WRITER] Marking replay position 276 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log)
DEBUG 13:42:53,307 [CompactionExecutor:1] Checking to see if compaction of LocationInfo would be useful
 INFO 13:42:53,335 [main] Starting up server gossip
DEBUG 13:42:53,381 [main] clearing cached endpoints
DEBUG 13:42:53,590 [main] Will try to load mx4j now, if it's in the classpath
 INFO 13:42:53,591 [main] Will not load MX4J, mx4j-tools.jar is not in the classpath
DEBUG 13:42:54,344 [GC inspection] GC for ParNew: 13 ms, 224224 reclaimed leaving 92832088 used; max is 1211826176
DEBUG 13:42:54,345 [GC inspection] GC for ConcurrentMarkSweep: 71 ms, 67203872 reclaimed leaving 25628216 used; max is 1211826176
DEBUG 13:42:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:43:20,282 [MIGRATION-STAGE:1] Applying migration 5300795b-9c0a-11df-8af1-e700f669bcfc
 INFO 13:43:20,284 [MIGRATION-STAGE:1] switching in a fresh Memtable for Migrations at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=11790)
 INFO 13:43:20,284 [MIGRATION-STAGE:1] Enqueuing flush of Memtable-Migrations@19614086(5585 bytes, 1 operations)
 INFO 13:43:20,284 [FLUSH-WRITER-POOL:1] Writing Memtable-Migrations@19614086(5585 bytes, 1 operations)
 INFO 13:43:20,284 [MIGRATION-STAGE:1] switching in a fresh Memtable for Schema at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=11790)
 INFO 13:43:20,285 [MIGRATION-STAGE:1] Enqueuing flush of Memtable-Schema@1088945411(2768 bytes, 3 operations)
DEBUG 13:43:20,419 [GC inspection] GC for ParNew: 13 ms, 147440 reclaimed leaving 105343544 used; max is 1211826176
DEBUG 13:43:20,427 [GC inspection] GC for ConcurrentMarkSweep: 72 ms, 75573104 reclaimed leaving 29770440 used; max is 1211826176
 INFO 13:43:20,432 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/Migrations-e-1-Data.db
DEBUG 13:43:20,433 [CompactionExecutor:1] Checking to see if compaction of Migrations would be useful
 INFO 13:43:20,433 [FLUSH-WRITER-POOL:1] Writing Memtable-Schema@1088945411(2768 bytes, 3 operations)
DEBUG 13:43:20,433 [MEMTABLE-POST-FLUSHER:1] Discarding 1002
DEBUG 13:43:20,433 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=11790), column family 1002.
DEBUG 13:43:20,433 [COMMIT-LOG-WRITER] Marking replay position 11790 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log)
 INFO 13:43:20,595 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/Schema-e-1-Data.db
DEBUG 13:43:20,602 [MEMTABLE-POST-FLUSHER:1] Discarding 1003
DEBUG 13:43:20,602 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log', position=11790), column family 1003.
DEBUG 13:43:20,602 [COMMIT-LOG-WRITER] Marking replay position 11790 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log)
DEBUG 13:43:20,608 [MIGRATION-STAGE:1] Starting CFS Super1
DEBUG 13:43:20,609 [MIGRATION-STAGE:1] Starting CFS Standard2
DEBUG 13:43:20,610 [MIGRATION-STAGE:1] Starting CFS Super2
DEBUG 13:43:20,611 [MIGRATION-STAGE:1] Starting CFS Standard1
DEBUG 13:43:20,612 [MIGRATION-STAGE:1] Starting CFS Super3
DEBUG 13:43:20,612 [MIGRATION-STAGE:1] Starting CFS StandardByUUID1
DEBUG 13:43:20,613 [CompactionExecutor:1] Checking to see if compaction of Schema would be useful
 INFO 13:43:20,626 [COMMIT-LOG-WRITER] Creating new commitlog segment /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log
DEBUG 13:43:20,657 [MIGRATION-STAGE:1] Applying migration 53580f3c-9c0a-11df-8af1-e700f669bcfc
 INFO 13:43:20,660 [MIGRATION-STAGE:1] switching in a fresh Memtable for Migrations at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log', position=12779)
 INFO 13:43:20,660 [MIGRATION-STAGE:1] Enqueuing flush of Memtable-Migrations@193189276(6998 bytes, 1 operations)
 INFO 13:43:20,660 [MIGRATION-STAGE:1] switching in a fresh Memtable for Schema at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log', position=12779)
 INFO 13:43:20,661 [MIGRATION-STAGE:1] Enqueuing flush of Memtable-Schema@2084371115(4181 bytes, 4 operations)
 INFO 13:43:20,661 [FLUSH-WRITER-POOL:1] Writing Memtable-Migrations@193189276(6998 bytes, 1 operations)
 INFO 13:43:20,815 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/Migrations-e-2-Data.db
 INFO 13:43:20,816 [FLUSH-WRITER-POOL:1] Writing Memtable-Schema@2084371115(4181 bytes, 4 operations)
DEBUG 13:43:20,855 [CompactionExecutor:1] Checking to see if compaction of Migrations would be useful
DEBUG 13:43:20,856 [MEMTABLE-POST-FLUSHER:1] Discarding 1002
DEBUG 13:43:20,864 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log', position=12779), column family 1002.
DEBUG 13:43:20,864 [COMMIT-LOG-WRITER] Not safe to delete commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log); dirty is 1000, 1003, 
DEBUG 13:43:20,864 [COMMIT-LOG-WRITER] Marking replay position 12779 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log)
 INFO 13:43:21,040 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/Schema-e-2-Data.db
DEBUG 13:43:21,041 [CompactionExecutor:1] Checking to see if compaction of Schema would be useful
DEBUG 13:43:21,041 [MEMTABLE-POST-FLUSHER:1] Discarding 1003
DEBUG 13:43:21,041 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log', position=12779), column family 1003.
DEBUG 13:43:21,041 [COMMIT-LOG-WRITER] Not safe to delete commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log); dirty is 1000, 
DEBUG 13:43:21,042 [COMMIT-LOG-WRITER] Marking replay position 12779 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log)
DEBUG 13:43:21,044 [MIGRATION-STAGE:1] Starting CFS Super1
DEBUG 13:43:21,050 [MIGRATION-STAGE:1] Starting CFS Standard2
DEBUG 13:43:21,051 [MIGRATION-STAGE:1] Starting CFS Super2
DEBUG 13:43:21,052 [MIGRATION-STAGE:1] Starting CFS Standard1
DEBUG 13:43:21,053 [MIGRATION-STAGE:1] Starting CFS Super3
DEBUG 13:43:21,058 [MIGRATION-STAGE:1] Starting CFS StandardByUUID1
 INFO 13:43:21,061 [COMMIT-LOG-WRITER] Creating new commitlog segment /Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log
 INFO 13:43:21,073 [RMI TCP Connection(2)-10.6.34.56] switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log', position=5708)
 INFO 13:43:21,081 [RMI TCP Connection(2)-10.6.34.56] Enqueuing flush of Memtable-LocationInfo@300139206(17 bytes, 1 operations)
 INFO 13:43:21,082 [FLUSH-WRITER-POOL:1] Writing Memtable-LocationInfo@300139206(17 bytes, 1 operations)
 INFO 13:43:21,093 [RMI TCP Connection(2)-10.6.34.56] switching in a fresh Memtable for Schema at CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log', position=5708)
 INFO 13:43:21,100 [RMI TCP Connection(2)-10.6.34.56] Enqueuing flush of Memtable-Schema@972791731(4181 bytes, 4 operations)
 INFO 13:43:21,235 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-4-Data.db
 INFO 13:43:21,235 [FLUSH-WRITER-POOL:1] Writing Memtable-Schema@972791731(4181 bytes, 4 operations)
DEBUG 13:43:21,239 [MEMTABLE-POST-FLUSHER:1] Discarding 1000
DEBUG 13:43:21,240 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log', position=5708), column family 1000.
 INFO 13:43:21,240 [COMMIT-LOG-WRITER] Discarding obsolete commit log:CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515372947.log)
DEBUG 13:43:21,243 [CompactionExecutor:1] Checking to see if compaction of LocationInfo would be useful
DEBUG 13:43:21,439 [GC inspection] GC for ParNew: 7 ms, 350704 reclaimed leaving 105776848 used; max is 1211826176
DEBUG 13:43:21,439 [GC inspection] GC for ConcurrentMarkSweep: 114 ms, 75637088 reclaimed leaving 30139760 used; max is 1211826176
 INFO 13:43:21,450 [CompactionExecutor:1] Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='/Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-1-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-2-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-3-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-e-4-Data.db')]
DEBUG 13:43:21,465 [CompactionExecutor:1] Expected bloom filter size : 1024
DEBUG 13:43:21,466 [COMMIT-LOG-WRITER] Not safe to delete commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log); dirty is 1003, 1002, 
DEBUG 13:43:21,469 [COMMIT-LOG-WRITER] Marking replay position 5708 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log)
DEBUG 13:43:21,471 [FILEUTILS-DELETE-POOL:1] Deleting CommitLog-1280515372947.log.header
DEBUG 13:43:21,473 [FILEUTILS-DELETE-POOL:1] Deleting CommitLog-1280515372947.log
 INFO 13:43:21,484 [FLUSH-WRITER-POOL:1] Completed flushing /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/Schema-e-3-Data.db
DEBUG 13:43:21,486 [MEMTABLE-POST-FLUSHER:1] Discarding 1003
DEBUG 13:43:21,489 [COMMIT-LOG-WRITER] discard completed log segments for CommitLogContext(file='/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log', position=5708), column family 1003.
DEBUG 13:43:21,495 [COMMIT-LOG-WRITER] Not safe to delete commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515400626.log); dirty is 1002, 
DEBUG 13:43:21,496 [COMMIT-LOG-WRITER] Marking replay position 5708 on commit log CommitLogSegment(/Users/gary.dusbabek/cass-configs/trunk/node1/commitlog/CommitLog-1280515401061.log)
 INFO 13:43:22,016 [CompactionExecutor:1] Compacted to /Users/gary.dusbabek/cass-configs/trunk/node1/data/system/LocationInfo-tmp-e-5-Data.db.  913 to 492 (~53% of original) bytes for 2 keys.  Time: 554ms.
DEBUG 13:43:22,016 [CompactionExecutor:1] Checking to see if compaction of Schema would be useful
DEBUG 13:43:22,016 [CompactionExecutor:1] Checking to see if compaction of LocationInfo would be useful
DEBUG 13:43:22,439 [GC inspection] GC for ParNew: 11 ms, 1461000 reclaimed leaving 109998456 used; max is 1211826176
DEBUG 13:43:22,439 [GC inspection] GC for ConcurrentMarkSweep: 87 ms, 75576552 reclaimed leaving 34421904 used; max is 1211826176
DEBUG 13:43:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:44:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:45:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:46:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:47:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:48:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:49:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:50:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:51:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:52:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:53:55,374 [Timer-1] Disseminating load info ...
DEBUG 13:54:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:55:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:56:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:57:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:58:55,373 [Timer-1] Disseminating load info ...
DEBUG 13:59:55,374 [Timer-1] Disseminating load info ...
DEBUG 14:00:55,379 [Timer-1] Disseminating load info ...
DEBUG 14:01:55,380 [Timer-1] Disseminating load info ...
DEBUG 14:02:55,381 [Timer-1] Disseminating load info ...
DEBUG 14:03:55,382 [Timer-1] Disseminating load info ...
DEBUG 14:04:55,383 [Timer-1] Disseminating load info ...
DEBUG 14:05:55,383 [Timer-1] Disseminating load info ...
DEBUG 14:06:55,383 [Timer-1] Disseminating load info ...
DEBUG 14:07:55,383 [Timer-1] Disseminating load info ...
DEBUG 14:08:55,383 [Timer-1] Disseminating load info ...
DEBUG 14:09:36,781 [GC inspection] GC for ParNew: 1 ms, 21480096 reclaimed leaving 22127704 used; max is 1211826176
DEBUG 14:09:55,383 [Timer-1] Disseminating load info ...
DEBUG 14:10:55,384 [Timer-1] Disseminating load info ...
DEBUG 14:11:55,384 [Timer-1] Disseminating load info ...
DEBUG 14:12:55,385 [Timer-1] Disseminating load info ...
DEBUG 14:13:55,385 [Timer-1] Disseminating load info ...
DEBUG 14:14:55,385 [Timer-1] Disseminating load info ...
DEBUG 14:15:55,384 [Timer-1] Disseminating load info ...
DEBUG 14:16:55,384 [Timer-1] Disseminating load info ...
DEBUG 14:17:55,384 [Timer-1] Disseminating load info ...
DEBUG 14:18:55,384 [Timer-1] Disseminating load info ...
DEBUG 14:19:55,385 [Timer-1] Disseminating load info ...
DEBUG 14:20:55,385 [Timer-1] Disseminating load info ...
DEBUG 14:21:55,386 [Timer-1] Disseminating load info ...
DEBUG 14:22:06,144 [ROW-READ-STAGE:3] Their data definitions are old. Sending updates since 00000000-0000-1000-0000-000000000000
DEBUG 14:22:06,166 [ROW-READ-STAGE:3] collecting 0 of 1000: 5300795b-9c0a-11df-8af1-e700f669bcfc:false:5554@1280515400281
DEBUG 14:22:06,166 [ROW-READ-STAGE:3] collecting 1 of 1000: 53580f3c-9c0a-11df-8af1-e700f669bcfc:false:6967@1280515400638
DEBUG 14:22:06,169 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
DEBUG 14:22:06,992 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
 INFO 14:22:07,349 [GOSSIP_STAGE:1] Node /127.0.0.2 is now part of the cluster
DEBUG 14:22:07,349 [GOSSIP_STAGE:1] Resetting pool for /127.0.0.2
DEBUG 14:22:07,893 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
 INFO 14:22:07,980 [HINTED-HANDOFF-POOL:1] Started hinted handoff for endpoint /127.0.0.2
 INFO 14:22:07,980 [GOSSIP_STAGE:1] InetAddress /127.0.0.2 is now UP
 INFO 14:22:07,985 [HINTED-HANDOFF-POOL:1] Finished hinted handoff of 0 rows to endpoint /127.0.0.2
DEBUG 14:22:55,386 [Timer-1] Disseminating load info ...
DEBUG 14:23:35,990 [MESSAGE-DESERIALIZER-POOL:1] Running  on default stage
DEBUG 14:23:36,913 [GOSSIP_STAGE:1] Node /127.0.0.2 state bootstrapping, token 61078635599166706937511052402724559481
ERROR 14:23:36,915 [GOSSIP_STAGE:1] Error in ThreadPoolExecutor
java.lang.IllegalStateException: replication factor (3) exceeds number of endpoints (1)
	at org.apache.cassandra.locator.RackUnawareStrategy.calculateNaturalEndpoints(RackUnawareStrategy.java:61)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:180)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:207)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:786)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:767)
	at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:607)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:569)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:721)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:686)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:640)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:61)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
ERROR 14:23:36,916 [GOSSIP_STAGE:1] Uncaught exception in thread Thread[GOSSIP_STAGE:1,5,main]
java.lang.IllegalStateException: replication factor (3) exceeds number of endpoints (1)
	at org.apache.cassandra.locator.RackUnawareStrategy.calculateNaturalEndpoints(RackUnawareStrategy.java:61)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:180)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:207)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:786)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:767)
	at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:607)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:569)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:721)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:686)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:640)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:61)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
DEBUG 14:23:55,387 [Timer-1] Disseminating load info ...
 INFO 14:24:07,913 [WRITE-/127.0.0.2] error writing to /127.0.0.2
DEBUG 14:24:08,912 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
 INFO 14:24:12,913 [Timer-0] InetAddress /127.0.0.2 is now dead.
DEBUG 14:24:12,914 [Timer-0] Resetting pool for /127.0.0.2
DEBUG 14:24:19,913 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
DEBUG 14:24:30,913 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
DEBUG 14:24:41,925 [WRITE-/127.0.0.2] attempting to connect to /127.0.0.2
Disconnected from the target VM, address: '127.0.0.1:54219', transport: 'socket'

Process finished with exit code 255
","30/Jul/10 22:48;gdusbabek;solution: bring all nodes up, then load schema.","30/Jul/10 22:50;gdusbabek;On second thought, I think the right solution is to disallow KS creation when the number of live nodes cannot support the replication factor.",03/Aug/10 18:33;jbellis;+1,"04/Aug/10 13:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    complain if there aren't enough nodes to support requested RF. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1343
","03/Sep/10 23:28;blanquer;the same or something similar is still happening in beta1
I've opened a ticket that results in the same error message, however I believe it is different since it doesn't happen when trying to create a KS but when bringing up a new node:
https://issues.apache.org/jira/browse/CASSANDRA-1467",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy throws an InvalidRequestException in readProtocol during bootstrap,CASSANDRA-1862,12493250,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,zznate,zznate,zznate,12/14/2010 22:40,3/12/2019 14:01,3/13/2019 22:24,12/16/2010 19:51,0.7.0 rc 3,,,,0,,,,,,"Though the error message provides details, IRE is supposed to signify poorly formed API requests. In the context of a client request, an UnavailableException is more appropriate. This would allow the client to take action like removing the node from its host list. ",,,,,,,,,,,,,,,,,,,,14/Dec/10 22:42;zznate;1862.txt;https://issues.apache.org/jira/secure/attachment/12466266/1862.txt,15/Dec/10 16:32;zznate;1862_0.6.txt;https://issues.apache.org/jira/secure/attachment/12466327/1862_0.6.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,10:37.4,,,no_permission,,,,,,,,,,,,20348,,,Wed Apr 20 15:40:47 UTC 2011,,,,,,0|i0g7v3:,92704,jbellis,jbellis,,,,,,,,,14/Dec/10 22:42;zznate;Changes exception type to UnavailalbeException,"15/Dec/10 15:10;jbellis;committed, thanks!","15/Dec/10 15:24;hudson;Integrated in Cassandra-0.7 #83 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/83/])
    change exceptionfor readrequests duringbootstrap from InvalidRequest to Unavailable
patch by Nate McCall; reviewed by jbellis for CASSANDRA-1862
",15/Dec/10 16:32;zznate;Backport to 0.6,15/Dec/10 16:50;jbellis;I'm a little leery of breaking 0.6 compatibility for something fairly minor.  Thoughts?,"15/Dec/10 17:38;zznate;I think this is extremely minor (the exception declaration of the method did not change) and brings us in line with the API as specified in cassandra.thrift: 

IRE:
""Invalid request could mean keyspace or column family does not exist, required parameters are missing, or a parameter is malformed...""

vs. UE:
""Not all the replicas required could be created and/or read.""","15/Dec/10 17:43;jbellis;bq. the exception declaration of the method did not change

i'll buy that.  committed","15/Dec/10 18:14;hudson;Integrated in Cassandra-0.6 #24 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/24/])
    backport CASSANDRA-1862 from 0.7
","20/Apr/11 15:40;doubleday;Don't know wether commenting a closed jira makes sense but I think that this made matters worse because now bootstrapping is not distinguishable anymore.
I thought that UnavailableException would signal that a read cannot be served due to CL. And a bootstrapping coordinator does not imply this right? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make cache sizes in CfDef Strings again so we can use %s of rows in the CF as in 0.6,CASSANDRA-1313,12470053,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,jbellis,jbellis,7/23/2010 22:21,3/12/2019 14:01,3/13/2019 22:24,8/13/2010 15:56,0.7 beta 2,,,,0,,,,,,"(note, this was reverted for 0.7 in CASSANDRA-1394)",,,,,,,,,,,,,,,,,,,,05/Aug/10 20:56;jhermes;1313-SY.txt;https://issues.apache.org/jira/secure/attachment/12451370/1313-SY.txt,05/Aug/10 20:58;jhermes;1313-YB.txt;https://issues.apache.org/jira/secure/attachment/12451373/1313-YB.txt,05/Aug/10 20:56;jhermes;snakeyaml-percentToFloat.txt;https://issues.apache.org/jira/secure/attachment/12451371/snakeyaml-percentToFloat.txt,05/Aug/10 20:56;jhermes;snakeyaml-r1131-dev.jar;https://issues.apache.org/jira/secure/attachment/12451372/snakeyaml-r1131-dev.jar,05/Aug/10 20:47;jhermes;yamlbeans-1.04.jar;https://issues.apache.org/jira/secure/attachment/12451368/yamlbeans-1.04.jar,,,,,,,,,5,,,,,,,,,,,,,,,,,,,44:40.1,,,no_permission,,,,,,,,,,,,20075,,,Sat Aug 14 12:48:35 UTC 2010,,,,,,0|i0g4an:,92126,,,,,,,,,,,"28/Jul/10 21:44;jhermes;Simply making \{rows,keys\}_cached strings instead of doubles is problematic. The current code looks for 'double \{rows,keys\}_cached', and changing every lookup is a non-trivial chunk of code. The lookup would also take extra cycles (or just two extra cycles if the first lookups cache the String->Double operation).

Loading a String into a Double from the YAML using snakeyaml directly has proven difficult:
* Getters and setters on the Config/RawKS/RawCF beans are never called, so there is no immediate hook.
* Telling the constructor in DD that ""50%"" should be a double does not help the constructor parse the string into a double -- snakeyaml explodes.
* Pushing the values into a String on the bean, then populating the double from the string leaves extra variables around. Also, these extra variables have to be named something different than \{rows,keys\}_cached, which means extra work in our 0.6->0.7 Converter.
* Pushing the values into a String on a FooConfig, then generating Config from FooConfig works, but means two passes on the load and hard to maintain later. 
* *Finally,* change the snakeyaml code directly. Whenever it sees a number followed by a percent, it should correctly construct a FloatType (be it Float, Double, or BigDecimal according to destination).

I decided the best fix is to effect a change in snakeyaml as outlined above (and tracked at: http://code.google.com/p/snakeyaml/issues/detail?id=75 ).

Attached are 3 things:
- The patch to snakeyaml trunk, named snakeyaml-percentToFloat.txt,
- The lib built from trunk after patch applied, named snakeyaml-r1131-dev1.jar,
- and a patch for C trunk (changes conf/cassandra.yaml and test/conf/cassandra.yaml), named 1313.txt.

With this, the values legal for \{rows,keys\}_cached is a literal (1000, 0.5, 0) or a string (""1000"", ""50%"", ""0%""). As before, a value between [0,1] is a fraction of the total, and all larger values are absolute. Because this is a change to the parser, you can also give percents anywhere else a double is expected (read_repair_chance: '72.25%' if you want).

Should this change not be accepted, one of the above methods can be employed in our code, or I can override-in all the functionality of the patch (though the code will look like a trainwreck).","28/Jul/10 21:46;jhermes;Attached debugging patch to print the values of \{rows,keys\}_cached to STDOUT when the config is first loaded.
Named snakeyaml-output.patch.",28/Jul/10 23:18;mdennis;+1 on the approach.  I'd like to see snakeyaml adopt this patch.,"29/Jul/10 02:53;jbellis;I like this, too.  Nice work.","30/Jul/10 15:14;py4fun;You do not need to patch SnakeYAML to achieve the goal. SnakeYAML has the following feature: any scalar which matches a given regular expression can be constructed with some custom code.
Examples can be found in the tests.

We shall think twice before we patch SnakeYAML. Appending '%' to a number is not a common pattern. It is not part of the YAML specification. Other users may be confused when they get Float when they expect String

-
Andrey 
","05/Aug/10 20:47;jhermes;Replacing snakeyaml with yamlbeans.

Status:
M       conf/cassandra.yaml
D       src/java/org/apache/cassandra/utils/SkipNullRepresenter.java
M       src/java/org/apache/cassandra/tools/NodeProbe.java
M       src/java/org/apache/cassandra/tools/SchemaTool.java
M       src/java/org/apache/cassandra/service/StorageService.java
M       src/java/org/apache/cassandra/service/StorageServiceMBean.java
M       src/java/org/apache/cassandra/config/DatabaseDescriptor.java
M       src/java/org/apache/cassandra/config/Config.java
M       src/java/org/apache/cassandra/config/Converter.java
M       src/java/org/apache/cassandra/config/RawColumnFamily.java
M       NOTICE.txt
D       lib/snakeyaml-1.6.jar
A       lib/yamlbeans-1.04.jar

Reading the config and loading keyspaces works properly.
Exporting keyspaces (using schematool) works properly given a target output file.
Converting from 0.6 to 0.7 has no diff from using snakeyaml.",05/Aug/10 20:56;jhermes;Re-attaching snakeyaml approach.,05/Aug/10 20:58;jhermes;And lastly fixing 1313-YB.txt to remove debug statements.,"06/Aug/10 13:46;py4fun;Jon, may I kindly ask you to clarify your decision ?

1)  you requested to change SnakeYAML without clear message what other users (not only Cassandra) will gain from such a change
2) I have provided you with a solution (http://code.google.com/p/snakeyaml/source/browse/src/test/java/examples/CustomImplicitResolverTest.java) which you can copy-and-paste to your code with minimum effort. You did not give any feedback back to the SnakeYAML community (http://code.google.com/p/snakeyaml/issues/detail?id=75). Why the solution did not work ?
3) using YamlBeans does not actually solve your problem. As far as I can see you introduced ""These getters/setters allow us to read X% in as a double"". The same can be done with SnakeYAML
4) Be aware that SnakeYAML implements complete 1.1 YAML specification and it has many other unique features not available in YamlBeans.


I strongly believe that communication is the best way to quickly solve the issues. SnakeYAML is open for suggestions and we are glad to help others to use the library. But we also expect that the community explains the decision and help us to improve SnakeYAML where it needs to be improved.","06/Aug/10 16:24;jhermes;1) I've made it painfully clear why I offered the patch to the community.
2) I have many good solutions now. This has been a solved problem for a while. There is a reason why snakeyaml-75 is marked as an Enhancement, not a Bug.
3,4) Switching to YamlBeans was not because I couldn't solve the problem without it, but because the code is much easier to read, smaller, and removed many lines of our code.

If you still desire clarification, feel free to e-mail me. Unless there are complications on this defect, I'm considering it finished and moving on to other things. As for snakeyaml-75, I'm leaving it open to be closed as Invalid at your discretion.","13/Aug/10 15:56;jbellis;committed, w/ removal of commented-out snakeyaml code and printStackTrace calls","14/Aug/10 12:48;hudson;Integrated in Cassandra #514 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/514/])
    add back percentage option for cache size configuration, and replace SnakeYAML with YamlBeans.  patch by Jon Hermes; reviewed by jbellis for CASSANDRA-1313
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove ""cluster"" command in nodeprobe usage",CASSANDRA-480,12437627,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,sammy.yu,sammy.yu,sammy.yu,10/8/2009 18:08,3/12/2019 14:01,3/13/2019 22:24,10/9/2009 14:28,0.5,,Legacy/Tools,,0,,,,,,"Looks like the cluster command has been merged with the ring command, we should remove it from usage.",,,,,,,,,,,,,,,,,,,,08/Oct/09 18:10;sammy.yu;0001--CASSANDRA-480-Updated-usage-so-that-cluster-comman.patch;https://issues.apache.org/jira/secure/attachment/12421644/0001--CASSANDRA-480-Updated-usage-so-that-cluster-comman.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,28:41.7,,,no_permission,,,,,,,,,,,,19713,,,Fri Oct 09 14:28:41 UTC 2009,,,,,,0|i0fz6v:,91299,,,,,,,,,,,09/Oct/09 14:28;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ensure compaction thresholds are sane,CASSANDRA-1527,12474746,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,jbellis,jbellis,9/21/2010 19:45,3/12/2019 14:01,3/13/2019 22:24,9/27/2010 22:10,0.7 beta 2,,Legacy/CQL,,0,,,,,,"make sure min <= max and neither is negative.

also make sure that min=max=0 works (this is ""no compaction"")",,,,,,,,,,,,,,,,,,,,27/Sep/10 21:07;jhermes;1527-unoopsed.txt;https://issues.apache.org/jira/secure/attachment/12455751/1527-unoopsed.txt,27/Sep/10 20:29;jhermes;1527.txt;https://issues.apache.org/jira/secure/attachment/12455748/1527.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,33:57.4,,,no_permission,,,,,,,,,,,,20179,,,Tue Sep 28 13:31:31 UTC 2010,,,,,,0|i0g5rb:,92363,,,,,,,,,,,"22/Sep/10 22:33;jhermes;Because there are now too many ways to invalidate a metadata, I'm going to enumerate them.

1) Creation:
- First and foremost, catching these boundaries on the constructor fixes the case where the config is invalid. The config is read and DD.readTablesFromYaml() creates a CFMD for each CF.
- This also catches 'schematool import', which hits SS.loadSchemasFromYaml() which hits DD.readTablesFromYaml() in the same way.
- This also catches creation of an avro CfDef and loading it (generally, this is because we serialized it incorrectly). Calling CFMD.inflate(avro cfdef) will hit the constructor as well.

2) Modification:
- Throwing a runtime exception on the JMX CFS.setMin/MaxCompactionThreshold() methods will suffice to stop this invalidation point. This could be done a bit more kindly.
- Catching these boundaries on CFMD.apply(avro/thrift def) limits the case where someone makes a def and then pushes it in. This is ALSO caught by the constructor.

Now, bad things. A constructor throwing a ConfigurationException is not the nicest thing in the world. These would be internal errors, as thrift/avro aren't always involved.","24/Sep/10 21:19;jhermes;Can't deal with the constructor throwing an exception, it's just too ugly.

Current patch:
# Adding validation to readTablesFromYaml()/loadSchemas() is pretty, and catches the erroneous config. This is necessarily different than the same validation for add/update API calls, as this is parsed and deflated immediately.
# Explode on invalid JMX poking. To counter this, made disableAutoCompaction() pokeable (previously internal). This should help avoid the case where one goes to disable via JMX, sets max = 0 < min, and kills the node.
# Leave inflate alone. If we deflate it incorrectly, then 1) was wrong.
# CFMetaData has a validateMinMaxCTs() for both avro and thrift. When updating a CF, the method is called in CFMetaData.apply(). When adding a CF, the method is called in CassandraServer.convertToCFMetaData(). (This is duplicated based on protocol because there is no interface for ""a CfDef"" to deal with both avro and thrift at the same time, much in same way all the other methods in CFMetaData are duplicated.)
# Check that apply validates correctly in DefsTest.","24/Sep/10 21:33;jhermes;Now, for min=max=0, estimating the compactions _will currently_ explode if min == max.

Estimated compactions are the sum of all (1 + size_i / thresholdrange) for i in SSTables with size > min. What do we want the estimated compactions to be when min == max (or thresholdrange=0)?
Furthermore, do we want to be estimating compactions when minor compactions are disabled?","27/Sep/10 20:06;jbellis;bq. Estimated compactions are the sum of all (1 + size_i / thresholdrange) for i in SSTables with size > min

that looks buggy to me.  doesn't estimate of

                n += Math.ceil((double)sstables.size() / maxct);

make more sense?

bq. do we want to be estimating compactions when minor compactions are disabled

no","27/Sep/10 20:29;jhermes;min=max is fine, min=0 || max=0 is fine.",27/Sep/10 21:07;jhermes;Patch error caused loss of code. That was unfortunate.,27/Sep/10 21:09;jbellis;can you add unit or system tests demonstrating rejection of invalid configurations?,"27/Sep/10 21:12;jbellis;nvm, I see it in DefsTest.

what is the commented-out code for in DD?  is that a todo that should be cleaned up?","27/Sep/10 21:24;jhermes;Oh, missed one!
The comments should be removed, the rest is valid.",27/Sep/10 22:10;jbellis;committed,"28/Sep/10 13:31;hudson;Integrated in Cassandra #549 (See [https://hudson.apache.org/hudson/job/Cassandra/549/])
    sanity checks for compaction thresholds.  
patch by jhermes; reviewed by jbellis for CASSANDRA-1527
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool cfstats broken on TRUNK,CASSANDRA-1433,12472528,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jjordan,jjordan,8/25/2010 21:58,3/12/2019 14:01,3/13/2019 22:24,9/2/2010 22:22,0.7 beta 2,,Tool/nodetool,,0,,,,,,"""nodetool -h localhost cfstats"" doesn't print anything.  Other commands work fine.
",windows and linux apache-cassandra-2010-08-23_13-57-40-bin.tar.gz,,,,,,,,,,,,,,,,,,,26/Aug/10 21:25;jbellis;1433.txt;https://issues.apache.org/jira/secure/attachment/12453171/1433.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,00:38.0,,,no_permission,,,,,,,,,,,,20135,,,Sun Sep 12 19:39:04 UTC 2010,,,,,,0|i0g513:,92245,gdusbabek,gdusbabek,,,,,,,,,"25/Aug/10 22:00;jhermes;Confirmed this does not work on head right now either.
Ubuntu 10.04, sun-jdk-1.6


For cfstats to print -NOTHING-, then the cfstoreMap in NodeCmd must be empty, which means the iterator must also come back empty.",26/Aug/10 21:25;jbellis;patch fixes ColumnFamilyStoreMBeanIterator to use new type name,02/Sep/10 17:41;gdusbabek;+1,02/Sep/10 22:22;jbellis;committed,"12/Sep/10 19:39;hudson;Integrated in Cassandra #533 (See [https://hudson.apache.org/hudson/job/Cassandra/533/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add locking around row cache accesses,CASSANDRA-1293,12469514,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,7/17/2010 2:16,3/12/2019 14:01,3/13/2019 22:24,10/1/2010 21:22,0.7 beta 3,,,,0,,,,,,"CASSANDRA-1267 means we need to lock around removeDeleted on the row cache entry and the write path where we merge in new columns (otherwise there can be a race where we incorrectly continue to remove a column, that has been updated by the writer thread to be newly relevant)",,,,,,,,,,,,,,,,,,,,01/Oct/10 20:44;jbellis;1293.txt;https://issues.apache.org/jira/secure/attachment/12456158/1293.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,16:22.8,,,no_permission,,,,,,,,,,,,20064,,,Sat Oct 02 12:56:27 UTC 2010,,,,,,0|i0g46f:,92107,brandon.williams,brandon.williams,,,,,,,,,"30/Aug/10 22:16;stuhood;How much of a benefit do we get from the row cache being write-through? It would be really nice not having to lock it... or maybe making the locking and write-throughness an option, via CASSANDRA-1158. It would also be easier to manage cache sizes in terms of bytes if we didn't need to deal with mutations to the row cache.","01/Oct/10 20:44;jbellis;We expect to save [cache hit rate] of a read per write with write-through, since the alternative is to invalidate the cached row at write time and reload it when it's requested again.

Attached patch moves the cache udpate into CFS.apply.",01/Oct/10 21:02;brandon.williams;+1,01/Oct/10 21:17;brandon.williams;No quantifiable difference in write speed with or without this patch.,01/Oct/10 21:22;jbellis;committed,"02/Oct/10 12:56;hudson;Integrated in Cassandra #553 (See [https://hudson.apache.org/hudson/job/Cassandra/553/])
    lock row cache updates to prevent race condition
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-1293
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json spews because it uses DatabaseDescriptor before loadSchemas() is called,CASSANDRA-1128,12465390,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mdennis,mdennis,mdennis,5/25/2010 17:14,3/12/2019 14:01,3/13/2019 22:24,5/28/2010 13:06,0.7 beta 1,,Legacy/Tools,,0,,,,,,sstable2json depends on DatabaseDescriptor for ColumnFamily meta data.  DD requires loadSchemas() is called before the CFMD can be accesed.  nothing in the code path in sstable2json calls loadSchemas().,,,,,,,,,,,,,,,,,,,,25/May/10 17:16;mdennis;CASSANDRA-1128.patch;https://issues.apache.org/jira/secure/attachment/12445473/CASSANDRA-1128.patch,27/May/10 20:34;mdennis;CASSANDRA-1128.patch2;https://issues.apache.org/jira/secure/attachment/12445700/CASSANDRA-1128.patch2,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,48:46.2,,,no_permission,,,,,,,,,,,,20000,,,Sat May 29 12:45:43 UTC 2010,,,,,,0|i0g35z:,91943,,,,,,,,,,,25/May/10 17:16;mdennis;patch against -r948111,"25/May/10 19:48;gdusbabek;+1

DD.loadSchemas() requires that system tables are present.  I wish there were a better way to do this, but I can't think of one.","25/May/10 19:50;gdusbabek;I'm going to hold off on committing this while I investigate some more.  There's got to be a better way.  Also, I want to make sure that sstableimport isn't broken in the same fundamental way.",25/May/10 21:29;mdennis;let me know if there is anything I can do to help...,"26/May/10 14:23;gdusbabek;Matthew, can you add this same fix to SSTableImport as well as a check immediately after the DD.loadSchemas() to verify that >0 non-system tables are defined?  The relevant call is DD.getNonSystemTables().",27/May/10 20:36;mdennis;patch2 against r948964,28/May/10 13:06;gdusbabek;committed with minor revisions.,"29/May/10 12:45;hudson;Integrated in Cassandra #449 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/449/])
    have sstable import/export load schema from local storage. Patch by Matthew Dennis, reviewed by Gary Dusbabek. CASSANDRA-1128
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable2Json tool returns a different key when a querying for a specific key in an SSTable that does not exist,CASSANDRA-2168,12498678,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bcoverston,bcoverston,bcoverston,2/15/2011 19:19,3/12/2019 14:01,3/13/2019 22:24,3/5/2011 5:34,0.7.4,,,,0,,,,,,"bin/sstable2json storage/core/data/Foo/BAR-1-Data.db -k NonExistantKey

returns

{ ""ExistantKey"" } ",,,,,,,,,,,,,,,,,,,,05/Mar/11 05:05;bcoverston;2168.txt;https://issues.apache.org/jira/secure/attachment/12472736/2168.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,00:02.2,,,no_permission,,,,,,,,,,,,20477,,,Sat Mar 05 05:34:45 UTC 2011,,,,,,0|i0g9qf:,93007,jbellis,jbellis,,,,,,,,,"18/Feb/11 22:00;jbellis;I think what we want is in the export overload that loops through the toExport list, we need to add a SSTableScanner.seekTo method that calls getPosition with Operator.EQ instead of GE (maybe we want to move the Operator into a parameter of SeekTo? there is only one other caller).","05/Mar/11 05:02;bcoverston;I made a simpler change. All I did was make a sanity check to see if after seeking the key matched the key we were looking for. If seek reached the end of the list it would only deserialize the row if it is the one we are looking for, otherwise it will continue to the next key in the for loop.",05/Mar/11 05:05;bcoverston;Patch 2168 attached for review.,"05/Mar/11 05:34;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update contrib/bmt_example to work post-CASSANDRA-44,CASSANDRA-1062,12463992,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,jbellis,jbellis,5/7/2010 14:39,3/12/2019 14:01,3/13/2019 22:24,5/19/2010 12:46,0.7 beta 1,,,,0,,,,,,the main problem is we need to pass an id to new ColumnFamily(...).  Not sure what the right fix is here.,,,,,,,,,,,,,,,,,,,,18/May/10 13:32;gdusbabek;0001-make-CassandraBulkLoader-compile.patch;https://issues.apache.org/jira/secure/attachment/12444790/0001-make-CassandraBulkLoader-compile.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,13:33.2,,,no_permission,,,,,,,,,,,,19980,,,Thu May 20 12:42:37 UTC 2010,,,,,,0|i0g2rj:,91878,,,,,,,,,,,"17/May/10 18:13;gdusbabek;I don't see anything that needs to be done here, except maybe some documentation along the lines of ""make sure your schema is set up.""

CBL only accesses the cluster via fat client, which collects schema info via gossip.",17/May/10 19:03;jbellis;the call to the CF constructor no longer compiles,18/May/10 23:58;jbellis;+1,"20/May/10 12:42;hudson;Integrated in Cassandra #441 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/441/])
    make CassandraBulkLoader compile. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1062
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DC Quorum broken @ trunk,CASSANDRA-952,12461108,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,4/4/2010 5:43,3/12/2019 14:01,3/13/2019 22:24,5/18/2010 17:16,0.7 beta 1,,,,0,,,,,,"Currently DCQuorum is broken in trunk, Suggesting the following fix... 

Write to DC's
1) Move determineBlockFor(int expandedTargets, ConsistencyLevel consistency_level) to AbstractEndpointSnitch
2) Add the same to support DC Quorum in DatacenterShardStategy

Read to DC's
1) find suitable nodes was a list which was returning a list of local DC's earlier but now it is just one node and MD is been sent by other nodes. Need to have an option to even avoid MD from other DC's?","Linux, Cassandra",,,,,,,,,,,,,,,,,,,14/May/10 23:55;vijay2win@yahoo.com;0002-dcquorum-fixes-v2.txt;https://issues.apache.org/jira/secure/attachment/12444546/0002-dcquorum-fixes-v2.txt,09/Apr/10 05:31;vijay2win@yahoo.com;952-Canges_To_Stategy_V002.txt;https://issues.apache.org/jira/secure/attachment/12441264/952-Canges_To_Stategy_V002.txt,05/Apr/10 17:07;vijay2win@yahoo.com;952-Change_BlockFor.txt;https://issues.apache.org/jira/secure/attachment/12440777/952-Change_BlockFor.txt,09/Apr/10 05:31;vijay2win@yahoo.com;952-Changes_To_ResponseHandler_v002.txt;https://issues.apache.org/jira/secure/attachment/12441265/952-Changes_To_ResponseHandler_v002.txt,06/Apr/10 01:16;vijay2win@yahoo.com;952-Fix_Refactor_DCStatergy.txt;https://issues.apache.org/jira/secure/attachment/12440820/952-Fix_Refactor_DCStatergy.txt,17/May/10 14:29;jbellis;952-v3.txt;https://issues.apache.org/jira/secure/attachment/12444683/952-v3.txt,28/Apr/10 06:31;vijay2win@yahoo.com;952-v3.txt;https://issues.apache.org/jira/secure/attachment/12443042/952-v3.txt,17/May/10 18:49;vijay2win@yahoo.com;952-v4.txt;https://issues.apache.org/jira/secure/attachment/12444724/952-v4.txt,29/Apr/10 17:42;vijay2win@yahoo.com;952-v4.txt;https://issues.apache.org/jira/secure/attachment/12443208/952-v4.txt,01/May/10 20:55;vijay2win@yahoo.com;952-v5.txt;https://issues.apache.org/jira/secure/attachment/12443389/952-v5.txt,12/May/10 21:48;jbellis;ASF.LICENSE.NOT.GRANTED--0001-clean-out-callback-purging-from-truncate-writerh.txt;https://issues.apache.org/jira/secure/attachment/12444346/ASF.LICENSE.NOT.GRANTED--0001-clean-out-callback-purging-from-truncate-writerh.txt,12/May/10 21:48;jbellis;ASF.LICENSE.NOT.GRANTED--0002-dcquorum-fixes.txt;https://issues.apache.org/jira/secure/attachment/12444347/ASF.LICENSE.NOT.GRANTED--0002-dcquorum-fixes.txt,01/May/10 20:55;vijay2win@yahoo.com;DC-Config.xml;https://issues.apache.org/jira/secure/attachment/12443390/DC-Config.xml,13,,,,,,,,,,,,,,,,,,,47:30.4,,,no_permission,,,,,,,,,,,,19935,,,Tue May 18 17:16:20 UTC 2010,,,,,,0|i0g23b:,91769,,,,,,,,,,,"06/Apr/10 14:47;jbellis;The fundamental problem here is that an int count of replicas to block for is not sufficient to represent the DC strategy wants: we want to SEND to all replicas, but only count replicas in the CURRENT DC towards success.  Allowing other DC replicas will mostly work, since they will typically be slower to arrive than local ones, but if there are failure conditions locally then it could succeed where it should not, which would violate its contract to readers.

I think you will need to push the concept of what to block for into the WriteResponseHandler, and similarly for reads.

",09/Apr/10 05:31;vijay2win@yahoo.com;Made Changes as suggested,"27/Apr/10 18:49;jbellis;Finally ready to get this in (now that CASSANDRA-994 is done).  Can you rebase?  Sorry, it's going to be a bit messy.","28/Apr/10 06:31;vijay2win@yahoo.com;this patch works in my environment, and i think we need couple of methods to be added to RackInferringSnitch which is getReplicationFactor and getDatacenters to make it work with the DCSS. Let me know if you dont agree with the changes, i have renamed the DCEPS to XMLFileSnitch. there is a lot of changes in these classes.... ","28/Apr/10 13:50;jbellis;We're not adding DCEPS/XMLFileSnitch back, and we shouldn't need to make any changes to the Snitch classes.  It should be able to run against any RackAwareSnitch, since it gets the per-DC settings from DCSS/datacenters.properties now.","29/Apr/10 17:42;vijay2win@yahoo.com;Attached is the modified version, please note that XMLFileSnitch included in this patch, uses a different logic than PFS to read the hosts, if you choose not to include that thats fine too.","30/Apr/10 17:11;jbellis;It looks like this is mixing code from CASSANDRA-967 in.  Let's do these one at a time, or we're more likely to introduce bugs.","01/May/10 20:55;vijay2win@yahoo.com;I have attached the changes, Please note that we have to change a bit of logic in rangeslice in order to remove blockFor for from the SP, IF you think it is unnecessary, i put blockFor for now and add a depreciated annotation on it (by doing this i can leave the rangeslice untouched).","12/May/10 21:55;jbellis;patch 01 cleans out callback purging from WriteResponseHandler (leaving ExpiringMap to clean it out automatically).

02 adds the dcquorum changes, extracting AbstractWRH so the DQSyncRH doesn't have to subclass WRH which is a poor fit, and cleans up the WRH heirarchy.","13/May/10 16:49;jeromatron;Reviewing part of it and I know this wasn't part of the change - but in RackAwareStrategy couldn't we save some instruction execution by moving the check for whether we already have found another datacenter/rack outside of the checks?  Like so:

{code}
Token t = iter.next();

// First try to find one in a different data center
// If we have already found something in a diff datacenter no need to find another
if (!bDataCenter)
{
    if (!snitch.getDatacenter(metadata.getEndpoint(primaryToken)).equals(snitch.getDatacenter(metadata.getEndpoint(t))))
    {
        endpoints.add(metadata.getEndpoint(t));
        bDataCenter = true;
        continue;
    }
}

// Now  try to find one on a different rack
// If we have already found something in a diff rack no need to find another
if (!bOtherRack)
{
    if (!snitch.getRack(metadata.getEndpoint(primaryToken)).equals(snitch.getRack(metadata.getEndpoint(t))) &&
        snitch.getDatacenter(metadata.getEndpoint(primaryToken)).equals(snitch.getDatacenter(metadata.getEndpoint(t))))
    {
        endpoints.add(metadata.getEndpoint(t));
        bOtherRack = true;
        continue;
    }
}
{code}

not that big of a deal, but since it's done on every write, might be helpful.  Maybe I'm missing something there though...","13/May/10 17:01;vijay2win@yahoo.com;Jeremy, Thanks... different datacenter is physically a different rack.... I dont think we need additional check for that....","13/May/10 17:36;jeromatron;Vijay - okay - I wasn't thinking of doing an additional check, I was just thinking of bringing the check for bDatacenter and bOtherRack outside of the block they were in, in RackAwareStrategy.","13/May/10 17:38;jeromatron;Jonathan - in your 0002 patch, the DatacenterShardStrategy changes - you refactored forLoopReturn into endpoints.  I'm not sure that you want to do that.  forLoopReturn is re-initialized on every loop iteration to include all of the replicas needed for that particular datacenter.  Endpoints includes all replicas regardless of datacenter.","13/May/10 20:05;jeromatron;Jonathan - Also it looks like you're doing the datacenter replication factor per keyspace now, which is what we're hoping to have for multi-tenants.  However, in StorageService, I was under the impression that the replication strategy is handled as though it was per keyspace but in reality it is a quasi singleton.  So all keyspaces will share the same RS of that type.  Is that not true now?  I didn't see any change to that in your patches.  Just wanted to make sure that doesn't cause odd problems - if all of the keyspaces are sharing the same DatacenterShardStrategy instance which is only meant for one of those keyspaces.","14/May/10 17:59;vijay2win@yahoo.com;Jeremy, 
For 002 i am submitting the update for the patch... Thanks!
replication strategy - Yes currently thats the case, but DSS can handle all the required levels it will just forward the requests to AbstractReplicationStrategy if other than DCQuorum or DCQuorumSync.","14/May/10 19:59;jeromatron;Cool - yeah - looking at it again, DSS contains DC RF information for all keyspaces, so it really wouldn't matter if it was a singleton for all keyspaces.

Btw, I was mostly just reviewing the getNaturalEndpointsInternal method of DSS - Jonathan had asked me to take a look at it in IRC.  Just so you know I wasn't reviewing the entire patch.  I don't know if that matters too much, but thought I would clarify.","14/May/10 23:55;vijay2win@yahoo.com;Attached is the fix and along with it i have a testcase, so we dont miss this in future.

Thanks
Vijay","17/May/10 14:29;jbellis;v3 attached is the same as v2 w/ (unintentional?) breakages to TruncateResponseHandler reverted and formatting fixed.

the new test needs to work with normal propertyfilesnitch, XMLFS is not going back in.",17/May/10 18:49;vijay2win@yahoo.com;Yes that was unintentional... This has updates to the test and also fix to the PEPS. And Hope it works... ,"18/May/10 13:31;hudson;Integrated in Cassandra #439 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/439/])
    make DCQUORUM CLs actually wait for the nodes per DC rather than attempting a purely count-based approach that didn't really work as advertised.  patch by Vijay Parthasarathy and jbellis; reviewed by Jeremy Hanna for CASSANDRA-952
","18/May/10 17:16;jbellis;committed v4.

there is still a minor issue w/ replica placement but I have opened CASSANDRA-1103 instead of round-tripping this again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
skipBytes in SSTable*Iterator is in an assert,CASSANDRA-899,12459125,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,3/15/2010 10:19,3/12/2019 14:01,3/13/2019 22:24,3/15/2010 15:50,0.6,,,,0,,,,,,"In SSTable Name and Slice iterator, the seek to the indexed offset is in an assert. 
As a consequence, if Cassandra is run without assertions, reads (specially for large 
rows) can be really inefficient.",,,,,,,,,,,,,,,,,,,,15/Mar/10 10:19;slebresne;899-codeWithSideEffectInAssert.diff;https://issues.apache.org/jira/secure/attachment/12438809/899-codeWithSideEffectInAssert.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,50:02.5,,,no_permission,,,,,,,,,,,,19910,,,Mon Mar 15 15:50:02 UTC 2010,,,,,,0|i0g1rj:,91716,,,,,,,,,,,"15/Mar/10 15:50;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hadoop recordreader hardcodes row count,CASSANDRA-837,12457743,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,3/1/2010 14:28,3/12/2019 14:01,3/13/2019 22:24,3/5/2010 20:32,0.6,,,,0,,,,,,We need to use the split size instead.,,,,,,,,,,,,,,,,,,,,03/Mar/10 02:53;jbellis;ASF.LICENSE.NOT.GRANTED--0001-move-configuration-static-methods-into-ConfigHelper.txt;https://issues.apache.org/jira/secure/attachment/12437667/ASF.LICENSE.NOT.GRANTED--0001-move-configuration-static-methods-into-ConfigHelper.txt,03/Mar/10 02:53;jbellis;ASF.LICENSE.NOT.GRANTED--0002-r-m-fields-from-CFSplit-that-are-redundant-to-informat.txt;https://issues.apache.org/jira/secure/attachment/12437668/ASF.LICENSE.NOT.GRANTED--0002-r-m-fields-from-CFSplit-that-are-redundant-to-informat.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,19:38.7,,,no_permission,,,,,,,,,,,,19887,,,Fri Mar 05 20:32:20 UTC 2010,,,,,,0|i0g1dr:,91654,,,,,,,,,,,"01/Mar/10 15:19;johanoskarsson;It's worth noting that when I tried using the default split size of 16k pretty much all my tasks timed out and died, failing the job. This was before the timeout was raised to 10s though, so might work better now. But before we have improved performance of the slice operation we should probably lower the 16k limit.","01/Mar/10 16:01;jbellis;In that case we should probably reduce the default to 8k, but we're testing 10k-20k rows read per second here via get_range_slice.  how big are your rows, and are you running on a VM or real hardware?","01/Mar/10 18:08;johanoskarsson;The rows only contain the text from a tweet, so not very big. This was running on EC2 instances, granted it's not the best real world test but shows the margin is not very big. Hopefully it will improve after CASSANDRA-821 is resolved.","03/Mar/10 02:54;jbellis;02
    r/m fields from CFSplit that are redundant to information in configuration; use split size for row count

01
    move configuration static methods into ConfigHelper
","05/Mar/10 09:21;johanoskarsson;+1. 
My only concern is that having the static configuration methods in the ConfigHelper might make them harder to find for the users, most input formats I have worked with have them in the input format class itself. A class level javadoc in the input format with a short user guide might be a good complement.","05/Mar/10 13:24;jbellis;Would it be better to just move CH.* to the InputFormat?

I split it out since it seemed weird to have the record reader call a bunch of static methods on the IF, but if that's the customary place then that's fine.",05/Mar/10 20:32;jbellis;committed w/ extra javadoc to 0.6 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spelling correction: rename DatacenterShardStategy to DatacenterShardStrategy,CASSANDRA-943,12460982,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,esigler,rodrigoap,rodrigoap,4/1/2010 23:47,3/12/2019 14:01,3/13/2019 22:24,4/20/2010 18:44,0.7 beta 1,,,,0,,,,,,Missing 'r' Stategy.,,,,,,,,,,,,,,,,,,,,20/Apr/10 17:22;esigler;CASSANDRA-943.patch;https://issues.apache.org/jira/secure/attachment/12442327/CASSANDRA-943.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,22:20.5,,,no_permission,,,,,,,,,,,,19931,,,Tue Apr 20 18:44:56 UTC 2010,,,,,,0|i0g21b:,91760,,,,,,,,,,,"20/Apr/10 17:22;esigler;This is a quick rename of the file and class. The base set of tests appears to run successfully after the rename, but I didn't find any other references to it, and there doesn't seem to be any unit testing around this Strategy, so I'm not certain I've done all that is needed.",20/Apr/10 18:44;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clean up SSTR.open/get and r/m unnecessary synchronization,CASSANDRA-413,12434654,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,9/2/2009 16:20,3/12/2019 14:01,3/13/2019 22:24,9/2/2009 20:35,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,02/Sep/09 16:21;jbellis;413.patch;https://issues.apache.org/jira/secure/attachment/12418398/413.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,25:13.4,,,no_permission,,,,,,,,,,,,19677,,,Wed Sep 02 20:30:55 UTC 2009,,,,,,0|i0fyrz:,91232,,,,,,,,,,,"02/Sep/09 18:25;junrao;I am confused by some of the comments. The comment at the head of SSTableReader says ""Do not use open() on existing SSTable files; use get() instead."". However, get() is marked as deprecated. Other than that, the patch looks fine.
","02/Sep/09 19:55;jbellis;You're right, that's needlessly confusing.

I'd like to move to a world where open() gets called by Table.onStart and after that we just use the existing SSTR references.  but, in the interim, get() is better than calling open() again.

I'll try to make that more clear.",02/Sep/09 20:30;jbellis;committed w/ clarification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update pom.xml for Maven usage,CASSANDRA-660,12444444,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ryandaum,ryandaum,ryandaum,12/31/2009 19:23,3/12/2019 14:01,3/13/2019 22:24,1/18/2010 21:37,0.5,,Legacy/Tools,,0,,,,,,"The pom.xml is out of date in the following ways:

  * Version # is incorrect (currently set to 0.4.1 release)
  * Dependencies are out of date (has flexjson and is missing clhm)

",,,,,,,,,,,,,,,,,,,,31/Dec/09 19:43;ryandaum;0001-Update-pom.xml-for-0.5-release-change-version-and.patch;https://issues.apache.org/jira/secure/attachment/12429215/0001-Update-pom.xml-for-0.5-release-change-version-and.patch,31/Dec/09 19:24;ryandaum;0001-Updated-pom.xml-to-build-trunk-updated-version-to-0.patch;https://issues.apache.org/jira/secure/attachment/12429213/0001-Updated-pom.xml-to-build-trunk-updated-version-to-0.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,37:42.5,,,no_permission,,,,,,,,,,,,19807,,,Tue Jan 19 13:10:10 UTC 2010,,,,,,0|i0g0av:,91479,,,,,,,,,,,"31/Dec/09 19:24;ryandaum;Attaching patches for trunk (0.9-SNAPSHOT).  Forthcoming for 0.5 branch.

Remember to remove -SNAPSHOT for final release.",31/Dec/09 19:43;ryandaum;Patch for 0.5 final release,"18/Jan/10 21:37;urandom;This was applied to the 0.5 branch (and merged to trunk), some time ago. Apologies for not getting the ticket closed out before.","19/Jan/10 13:10;hudson;Integrated in Cassandra #328 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/328/])
    update pom.xml for maven usage

Patch by Ryan Daum; reviewed by eevans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomPartitioner convertFromDiskFormat is slow,CASSANDRA-581,12441634,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,11/25/2009 2:00,3/12/2019 14:01,3/13/2019 22:24,12/5/2009 1:45,0.5,,,,1,,,,,,"convertFromDiskFormat in RandomPartitioner is slow. It uses split. We were testing with 1000+ keys using multi-get on a local node. We saw on average 200ms~, with the applied patch it went down to 76ms~.
",,,,,,,,,,,,,,,,,,,,25/Nov/09 02:12;lenn0x;0001-Make-convertFromDiskFormat-use-substring-over-split-.patch;https://issues.apache.org/jira/secure/attachment/12426053/0001-Make-convertFromDiskFormat-use-substring-over-split-.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,07:54.5,,,no_permission,,,,,,,,,,,,19764,,,Sat Dec 05 12:34:26 UTC 2009,,,,,,0|i0fztb:,91400,,,,,,,,,,,25/Nov/09 02:07;stuhood;+1 Looks good to me.,25/Nov/09 02:14;lenn0x;Forgot the +1 on the splitPoint for second.,"25/Nov/09 02:49;jbellis;Can you add a test that catches the +1 bug?  This is definitely ""should be covered by the test suite"" area.","05/Dec/09 01:45;lenn0x;Jonathan,

The test already exist, just my fault on that patch change. I verified the test under RandomPartitionerTest validates the +1 change I did.

Commited.","05/Dec/09 12:34;hudson;Integrated in Cassandra #278 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/278/])
    Change convertFromDiskFormat to use substring splitting vs using split operation (slow). patch by goffinet; reviewed by stuhood for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some Thrift Exceptions not passed down to Client,CASSANDRA-711,12445854,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,lenn0x,lenn0x,1/18/2010 20:16,3/12/2019 14:01,3/13/2019 22:24,3/1/2010 16:22,0.6,,,,0,,,,,,"We still don't pass all exceptions down to client via Thrift. We have seen a few of these when working on our client library:

org.apache.thrift.protocol.TProtocolException: Required field 'start' was not present! Struct: SliceRange(start:null, finish:null, reversed:false, count:100)

Would be good if those exceptions were passed down, instead of 'TSocket Read 0 Bytes'.
",,,,,,,,,,,,,,,,,,,,10/Feb/10 03:28;jbellis;711-test.txt;https://issues.apache.org/jira/secure/attachment/12435405/711-test.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,18:59.3,,,no_permission,,,,,,,,,,,,19828,,,Mon Mar 01 16:22:39 UTC 2010,,,,,,0|i0g0lr:,91528,,,,,,,,,,,"18/Jan/10 20:18;jbellis;can you include full ST?

it sounds like a thrift bug, not ours.","18/Jan/10 21:00;lenn0x;You might be right... Hmm...

ERROR [pool-1-thread-34] 2010-01-18 12:13:35,252 TThreadPoolServer.java (line 257) Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Required field 'start' was not present! Struct: SliceRange(start:null, finish:null, reversed:false, count:100)
        at org.apache.cassandra.service.SliceRange.validate(SliceRange.java:587)
        at org.apache.cassandra.service.SliceRange.read(SliceRange.java:515)
        at org.apache.cassandra.service.SlicePredicate.read(SlicePredicate.java:366)
        at org.apache.cassandra.service.Cassandra$get_slice_args.read(Cassandra.java:3063)
        at org.apache.cassandra.service.Cassandra$Processor$get_slice.process(Cassandra.java:937)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:895)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)",25/Jan/10 20:35;jbellis;fix attached to THRIFT-689.,10/Feb/10 03:28;jbellis;test to demonstrate problem.  may need rebasing.,"25/Feb/10 15:31;nicktelford;Another example of this:

ERROR - Thrift error occurred during processing of message.
org.apache.thrift.protocol.TProtocolException: Required field 'timestamp' was not found in serialized data! Struct: Column(name:null, value:null, timestamp:0)
	at org.apache.cassandra.service.Column.read(Column.java:382)
	at org.apache.cassandra.service.SuperColumn.read(SuperColumn.java:317)
	at org.apache.cassandra.service.ColumnOrSuperColumn.read(ColumnOrSuperColumn.java:295)
	at org.apache.cassandra.service.Cassandra$batch_insert_args.read(Cassandra.java:10447)
	at org.apache.cassandra.service.Cassandra$Processor$batch_insert.process(Cassandra.java:1084)
	at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

Seems to arise when you don't pass a field that was marked as ""required"" in the structs Thrift interface spec. Thrift appears to do the checks on the server-side and not properly handle the exception.","25/Feb/10 15:56;jbellis;yes, that is why I submitted a fix to thrift and linked the thrift ticket two comments above yours",01/Mar/10 06:12;stuhood;The patch for THRIFT-689 was committed in Thrift SVN r916825. What are the next steps here? Updating Cassandra's thrift to that exact revision?,"01/Mar/10 13:24;jbellis;yeah, update and test for regressions :)

I'll do that today.","01/Mar/10 16:22;jbellis;Done, thanks for the Thrift help Stu.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
need serialVersionUID for Token,CASSANDRA-655,12444224,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,12/27/2009 18:43,3/12/2019 14:01,3/13/2019 22:24,12/29/2009 21:45,0.5,0.6,,,0,,,,,,"Similar to CASSANDRA-595, Token needs a serialVersionUID or JMX client applications will be required to use the same build in order to work (and this is highly inconvenient).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,02:03.6,,,no_permission,,,,,,,,,,,,19804,,,Wed Dec 30 13:00:11 UTC 2009,,,,,,0|i0g09r:,91474,,,,,,,,,,,29/Dec/09 08:02;todd;Every class that implements Serializable should declare serialVersionUID values. Are there specific reasons why we are not doing this? Any objections to explicitly defining these for every serializable class?,"29/Dec/09 14:55;urandom;Casandra uses its own serialization for many of these classes, including Token. Some of these implement Serializable as well only to accommodate JMX. For these, we left serialVerionUID unset to discourage people from treating it as stable and using it over the hand-rolled serialization. Obviously this needs to be rethought now.","29/Dec/09 16:42;jbellis;we still have a lot of classes that implement Serializable, that don't need to at all, even for JMX.

of the remaining ones (the ones used in JMX that Eric mentions), yes, we should declare the UID for each.
","29/Dec/09 21:45;urandom;This is fixed in:

* r894486 (trunk) which cleans up Serializable implementing classes that shouldn't
* r894476 (0.5) which adds a serialVersionUID to o.a.c.dht.Token","30/Dec/09 13:00;hudson;Integrated in Cassandra #308 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/308/])
    audit classes implementing Serializable

Patch by eevans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition leads to FileNotFoundException on startup,CASSANDRA-1382,12471371,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,rzotter,rzotter,8/12/2010 4:08,3/12/2019 14:01,3/13/2019 22:24,8/17/2010 18:12,0.7 beta 2,,,,0,,,,,,"On startup LocationInfo file is deleted then attempted to be read from.

Steps to reproduce: Kill then quickly restart

Switching to ParallelGC to avoid CMS/CompressedOops incompatibility
INFO 17:05:08,680 DiskAccessMode isstandard, indexAccessMode is mmap
 INFO 17:05:08,786 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,797 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,807 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,833 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,834 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,839 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,862 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,864 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,876 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,885 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,892 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,893 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,897 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,901 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,906 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,909 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,918 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,922 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,928 Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:08,933 Deleted /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db
 INFO 17:05:08,936 Deleted /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db
 INFO 17:05:08,936 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,937 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,947 Deleted /var/lib/cassandra/data/system/LocationInfo-e-14-Data.db
 INFO 17:05:08,947 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,948 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-15-<>
ERROR 17:05:08,948 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-15-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,950 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,951 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,970 Deleted /var/lib/cassandra/data/system/LocationInfo-e-13-Data.db
 INFO 17:05:08,971 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-14-<>
ERROR 17:05:08,971 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-14-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-14-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,972 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,973 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-15-<>
ERROR 17:05:08,973 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-15-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,974 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,974 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,996 Loading schema version acc5646a-a59d-11df-83fb-e700f669bcfc
 WARN 17:05:09,158 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
 INFO 17:05:09,164 Replaying /var/lib/cassandra/commitlog/CommitLog-1281571453475.log, /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:09,172 Finished reading /var/lib/cassandra/commitlog/CommitLog-1281571453475.log
 INFO 17:05:09,172 Finished reading /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:09,173 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1281571508928.log', position=592)
 INFO 17:05:09,183 Enqueuing flush of Memtable-LocationInfo@137493297(17 bytes, 1 operations)
 INFO 17:05:09,183 Writing Memtable-LocationInfo@137493297(17 bytes, 1 operations)
 INFO 17:05:09,184 switching in a fresh Memtable for Statistics at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1281571508928.log', position=592)
 INFO 17:05:09,184 Enqueuing flush of Memtable-Statistics@86823325(0 bytes, 0 operations)
 INFO 17:05:09,265 Completed flushing /var/lib/cassandra/data/system/LocationInfo-e-18-Data.db
 INFO 17:05:09,273 Writing Memtable-Statistics@86823325(0 bytes, 0 operations)
 INFO 17:05:09,352 Completed flushing /var/lib/cassandra/data/system/Statistics-e-1-Data.db
 INFO 17:05:09,353 Recovery complete ",,,,,,,,,,,,,,,,,,,,12/Aug/10 20:48;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-separate-CFS-dir-cleanup-from-CFS-instantiation.txt;https://issues.apache.org/jira/secure/attachment/12451948/ASF.LICENSE.NOT.GRANTED--v1-0001-separate-CFS-dir-cleanup-from-CFS-instantiation.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,13:38.8,,,no_permission,,,,,,,,,,,,20112,,,Wed Aug 18 13:13:49 UTC 2010,,,,,,0|i0g4pr:,92194,jbellis,jbellis,,,,,,,,,"12/Aug/10 18:13;gdusbabek;When there are compacted files to delete, initializing the system table tries to delete its statistics, which results in trying to initialize the system table.  The statistics deletion needs to happen outside of the CFS initialization.",12/Aug/10 20:49;gdusbabek;Another approach to this fix would have been us have an initialization stage and make sure that CFS creation and statistics clean up happen on it. The stats cleanup would be submitted at the end of the stage while a CFS was being initialized.,"13/Aug/10 16:51;jbellis;it looks like this patch says we only delete statistics for sstables that had .compacted files left hanging around on the next restart, which won't be the case if they got cleaned up by the GC hook",13/Aug/10 17:52;gdusbabek;It was the same way before.  SSTable.deleteIfCompacted() is the only method that calls StatisticsTable.deleteSSTableStatistics().  The only place SST.deleteIfCompacted() gets called is from the CFS constructor which is called during init.,17/Aug/10 16:34;jbellis;+1,"18/Aug/10 13:13;hudson;Integrated in Cassandra #517 (See [https://hudson.apache.org/hudson/job/Cassandra/517/])
    missed CHANGES.txt for CASSANDRA-1382
separate CFS dir cleanup from CFS instantiation. fixes race condition. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1382
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incorrect package name for property_snitch class files,CASSANDRA-1259,12468770,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,7/7/2010 22:44,3/12/2019 14:01,3/13/2019 22:24,7/9/2010 4:43,0.6.4,,,,0,,,,,,"The classes in contrib/property_snitch both have ""src.java"" prepended to them (probably a bug introduced by an IDE).

The attached trivial patch fixes this.",,,,,,,,,,,,,,,,,,,,07/Jul/10 22:45;urandom;0001-remove-erroneous-src.java-from-package-name.patch;https://issues.apache.org/jira/secure/attachment/12448931/0001-remove-erroneous-src.java-from-package-name.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,59:29.0,,,no_permission,,,,,,,,,,,,20052,,,Fri Jul 09 04:43:09 UTC 2010,,,,,,0|i0g3yv:,92073,,,,,,,,,,,07/Jul/10 22:59;jbellis;+1,09/Jul/10 04:43;urandom;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMX RowCache requests also include writes,CASSANDRA-770,12455490,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,brandon.williams,brandon.williams,2/5/2010 20:30,3/12/2019 14:01,3/13/2019 22:24,2/5/2010 20:59,0.6,,,,0,,,,,,"I have a CF that I only write to, unless I manually query it.  I have observed the RowCache request count increasing on this CF in line with my writes, but have never actually queried it.","debian lenny OpenJDK 64-Bit Server VM (build 1.6.0_0-b11, mixed mode)
",,,,,,,,,,,,,,,,,,,05/Feb/10 20:38;jbellis;770.txt;https://issues.apache.org/jira/secure/attachment/12435007/770.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,59:42.8,,,no_permission,,,,,,,,,,,,19856,,,Wed Feb 17 17:54:42 UTC 2010,,,,,,0|i0g0yv:,91587,,,,,,,,,,,"05/Feb/10 20:55;brandon.williams;+1, no longer increases due to writes.",05/Feb/10 20:59;jbellis;committed,"17/Feb/10 17:54;hudson;Integrated in Cassandra #357 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/357/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable Import/Export doesn't support ExpiringColumns,CASSANDRA-1754,12480308,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,11/18/2010 8:34,3/12/2019 14:01,3/13/2019 22:24,11/29/2010 19:37,0.7.0 rc 2,,Legacy/Tools,,0,,,,,,,,,,,,,,,,,,,,,,,,,,29/Nov/10 16:07;slebresne;0001-Support-Expiring-Columns-in-SSTableImport-Export.patch;https://issues.apache.org/jira/secure/attachment/12464872/0001-Support-Expiring-Columns-in-SSTableImport-Export.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,37:46.9,,,no_permission,,,,,,,,,,,,20293,,,Sat Dec 11 07:35:11 UTC 2010,,,,,,0|i0g76n:,92594,jbellis,jbellis,,,,,,,,,29/Nov/10 19:37;jbellis;committed,"11/Dec/10 07:35;hudson;Integrated in Cassandra-0.7 #70 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/70/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
digest mismatches are processed serially,CASSANDRA-1323,12470224,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,7/27/2010 2:32,3/12/2019 14:01,3/13/2019 22:24,7/27/2010 3:45,0.6.4,,,,0,,,,,,for multiget situations this can dramatically increase latency.  need to parallelize these.,,,,,,,,,,,,,,,,,,,,27/Jul/10 02:33;jbellis;1323.txt;https://issues.apache.org/jira/secure/attachment/12450550/1323.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,52:34.4,,,no_permission,,,,,,,,,,,,20080,,,Tue Jul 27 03:45:36 UTC 2010,,,,,,0|i0g4cv:,92136,,,,,,,,,,,27/Jul/10 02:52;brandon.williams;+1,27/Jul/10 03:45;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in DatacenterShardStatergy,CASSANDRA-787,12456082,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,2/11/2010 23:31,3/12/2019 14:01,3/13/2019 22:24,2/18/2010 20:06,0.6,,,2/11/2010 0:00,0,,,,,,There is a long pending fix to contribute back... Plz find the patch.,Linux cassandra 0.5,,,,,,,,,,,,,,,,,,,11/Feb/10 23:32;vijay2win@yahoo.com;patch.txt;https://issues.apache.org/jira/secure/attachment/12435630/patch.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,06:54.7,,,no_permission,,,,,,,,,,,,19862,,,Thu Feb 18 20:06:54 UTC 2010,,,,,,0|i0g12n:,91604,,,,,,,,,,,18/Feb/10 20:06;jbellis;committed to 0.6.  (would have been faster if marked Patch Submitted :),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming broken on windows (FileStreamTask.CHUNK_SIZE is too big).,CASSANDRA-795,12456333,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,2/15/2010 15:32,3/12/2019 14:01,3/13/2019 22:24,2/16/2010 22:57,0.5,,,,0,,,,,,Setting chunk size smaller addresses the problem.  We should explore setting SO_SNDBUF higher to see if that fixes the problem.,,,,,,,,,,,,,,,,,,,,15/Feb/10 19:26;gdusbabek;0001-set-SO_SNDBUF-bigger.patch;https://issues.apache.org/jira/secure/attachment/12435896/0001-set-SO_SNDBUF-bigger.patch,15/Feb/10 19:26;gdusbabek;0002-unit-test-for-streaming-a-big-file.patch;https://issues.apache.org/jira/secure/attachment/12435895/0002-unit-test-for-streaming-a-big-file.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,36:01.7,,,no_permission,,,,,,,,,,,,19866,,,Tue Feb 16 22:57:18 UTC 2010,,,,,,0|i0g14f:,91612,,,,,,,,,,,15/Feb/10 15:36;jbellis;(Closed CASSANDRA-793 as dupe of this.),15/Feb/10 15:38;jbellis;If you add a unit test w/ a 64MB file I can test on windows.  Writing a row w/ a single column of 64MB ought to do it.,15/Feb/10 19:26;gdusbabek;Test case attached (it takes a while to complete).,"15/Feb/10 19:56;jbellis;it would probably be a lot faster if you put the bulk of the volume in a column value[] so it doesn't have to do a whole bunch of String serialization in the setup.

Test passes for me (64 bit windows 7) w/o patch 01 so I don't think I can usefully report on that. :)","16/Feb/10 12:25;tantra;On WinXp this patch doesn't work. Help only reduce CHUNK_SIZE

java.lang.RuntimeException: java.io.IOException: Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена
	at sun.nio.ch.SocketDispatcher.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:33)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:104)
	at sun.nio.ch.IOUtil.write(IOUtil.java:60)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:334)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:449)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:520)
	at org.apache.cassandra.net.FileStreamTask.stream(FileStreamTask.java:96)
	at org.apache.cassandra.net.FileStreamTask.runMayThrow(FileStreamTask.java:64)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
ERROR [MESSAGE-STREAMING-POOL:1] 2010-02-16 15:21:34,234 CassandraDaemon.java (line 78) Fatal exception in thread Thread[MESSAGE-STREAMING-POOL:1,5,main]
java.lang.RuntimeException: java.io.IOException: Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Невозможно выполнить операцию на сокете, т.к. буфер слишком мал или очередь переполнена
	at sun.nio.ch.SocketDispatcher.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:33)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:104)
	at sun.nio.ch.IOUtil.write(IOUtil.java:60)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:334)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:449)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:520)
	at org.apache.cassandra.net.FileStreamTask.stream(FileStreamTask.java:96)
	at org.apache.cassandra.net.FileStreamTask.runMayThrow(FileStreamTask.java:64)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
","16/Feb/10 22:09;gdusbabek;I was able to duplicate this in windows XP by throwing roughly 200MB at a socket in short order. SO_RCVBUF and SO_SNDBUF both default to 8k and setting them up to 256k didn't address the problem. Since the default values were so low, I didn't think that setting them in the multi-MB range was going to prove fruitful.

Google wasn't very helpful explaining the underlying cause of the error (stems from winsock err code WSAENOBUFS) except for stating the obvious (there isn't enough allocated memory somewhere). Taking it all in, I get the sense that when you chunk data up to send over a socket, those chunks end up in kernel memory (paged), not user memory, and if you use too much of that (varies across windows versions), a WSAENOBUFS ensues.

Setting FileStreamTask.CHUNK_SIZE to 32MB solves the problem though. Unless there are any objections, I think we'll stick with that solution.",16/Feb/10 22:15;jbellis;+1 set to 32MB,"16/Feb/10 22:57;gdusbabek;r910738 (0.5), 910744 (trunk)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
better default logging paths,CASSANDRA-391,12433874,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,8/24/2009 18:14,3/12/2019 14:01,3/13/2019 22:24,8/25/2009 15:58,0.4,,Legacy/Documentation and Website,,0,,,,,,"Pursuant to the changes made to path names in storage-conf.xml (CASSANDRA-373), the sample log4j.properties should be updated for saner log paths.

Patch to follow.",,,,,,,,,,,,,,,,,,,,24/Aug/09 18:17;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-391-updated-log-file-location.txt;https://issues.apache.org/jira/secure/attachment/12417500/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-391-updated-log-file-location.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:14.5,,,no_permission,,,,,,,,,,,,19666,,,Tue Aug 25 14:21:27 UTC 2009,,,,,,0|i0fynb:,91211,,,,,,,,,,,24/Aug/09 18:21;jbellis;+1,"25/Aug/09 14:21;hudson;Integrated in Cassandra #177 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/177/])
    updated log file location

Patch by eevans; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0.7 on Windows,CASSANDRA-1538,12474921,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vloncar,jbellis,jbellis,9/23/2010 11:03,3/12/2019 14:01,3/13/2019 22:24,10/11/2010 14:20,0.7 beta 3,,Packaging,,0,,,,,,"bat file needs to be told to look for log4j-server.properties

anything else?",,,,,,,,,,,,,,,,,,,,28/Sep/10 09:52;vloncar;add-log4j-path-to-bat.patch;https://issues.apache.org/jira/secure/attachment/12455824/add-log4j-path-to-bat.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,52:59.0,,,no_permission,,,,,,,,,,,,20185,,,Wed Sep 29 12:47:30 UTC 2010,,,,,,0|i0g5uf:,92377,jbellis,jbellis,,,,,,,,,28/Sep/10 09:52;vloncar;Here is a trivial patch to add log4j configuration to cassandra.bat. Any chance of this getting in before 0.7.0-beta2 so the few of us on windows don't have to think about it anymore?,28/Sep/10 14:54;jbellis;committed,"29/Sep/10 12:47;hudson;Integrated in Cassandra #550 (See [https://hudson.apache.org/hudson/job/Cassandra/550/])
    point log4j to log4j-server.properties in cassandra.bat.  patch by Vladimir Loncar; reviewed by jbellis for CASSANDRA-1538
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Running on Windows XP, get ""Incorrect number of parameters: and""",CASSANDRA-307,12430946,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,euphoria,jcarver,jcarver,7/20/2009 16:46,3/12/2019 14:01,3/13/2019 22:24,7/21/2009 15:23,0.4,,Legacy/Tools,,0,,,,,,"My eclipse workspace is in ""C:\Documents and Settings\Jason\My Documents""

A couple lines in cassandra.bat missed using double-quotes to protect against folders with spaces in the name.",Windows XP,,300,300,,0%,300,300,,,,,,,,,,,,20/Jul/09 19:38;euphoria;307_v2.diff;https://issues.apache.org/jira/secure/attachment/12414029/307_v2.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,49:35.0,,,no_permission,,,,,,,,,,,,19627,,,Tue Jul 21 15:23:23 UTC 2009,,,,,,0|i0fy4v:,91128,,,,,,,,,,,"20/Jul/09 16:47;jcarver;This fixes two lines that need double quotes on Windows XP:

from: svn diff bin\cassandra.bat


Index: bin/cassandra.bat
===================================================================
--- bin/cassandra.bat   (revision 795691)
+++ bin/cassandra.bat   (working copy)
@@ -45,7 +45,7 @@
 REM ***** CLASSPATH library setting *****

 REM Shorten lib path for old platforms
-subst P: %CASSANDRA_HOME%\lib
+subst P: ""%CASSANDRA_HOME%\lib""
 P:
 set CLASSPATH=P:\

@@ -58,7 +58,7 @@

 :okClasspath
 set CASSANDRA_CLASSPATH=%CASSANDRA_HOME%;%CASSANDRA_CONF%;%CLASSPATH%;%CASSANDR
A_HOME%\build\classes
-set CASSANDRA_PARAMS=-Dcassandra -Dstorage-config=%CASSANDRA_CONF%
+set CASSANDRA_PARAMS=-Dcassandra -Dstorage-config=""%CASSANDRA_CONF%""
 goto runDaemon

 :runDaemon
","20/Jul/09 17:49;euphoria;With this patch applied, I still get errors if I put spaces in the Cassandra path, on the loading of the storage conf.  See this issue: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6506304

Still tracking this down, but for now I'd recommend avoiding spaces in Windows development.","20/Jul/09 18:35;jcarver;Interesting, my JDK was old (1.6.0_05) so I couldn't reproduce the problem.  Perhaps the workaround on the link you sent me:

=====
Submitted On 09-OCT-2008
David.F

I had a line with same exception :
		
          document = builder.parse(translationFileName);
		
I succeed to get rid of error with 'new File(...)':
		
          document = builder.parse(new File(translationFileName));
=====


Applied like:

Index: src/java/org/apache/cassandra/utils/XMLUtils.java
===================================================================
--- src/java/org/apache/cassandra/utils/XMLUtils.java   (revision 795691)
+++ src/java/org/apache/cassandra/utils/XMLUtils.java   (working copy)
@@ -43,8 +43,13 @@
     {
         DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();
         DocumentBuilder db = dbf.newDocumentBuilder();
-        document_ = db.parse(xmlSrc);

+        //Prevent issue where spaces break xml source path on JDK 6/Windows
+        //     by wrapping xml path in a File obj
+        File xmlFile = new File(xmlSrc);
+
+        document_ = db.parse(xmlFile);
+
         XPathFactory xpathFactory = XPathFactory.newInstance();
         xpath_ = xpathFactory.newXPath();
     }","20/Jul/09 19:17;euphoria;This was the path I was going down.  I asked this on IRC but haven't received a response yet.

<michaelgreene> Currently we parse the storage file location as a URI
<michaelgreene> but parse(File) is another option and will fix 307
<michaelgreene> anyone need the URI version for some reason?","20/Jul/09 19:38;euphoria;Looks like we can use a File just fine.

Attached patch works from the following path and is a consolidation of the code posted so far in this issue: E:\space path\cassandra-307>bin\cassandra.bat","21/Jul/09 15:23;urandom;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Script in debian/cassandra.in.sh is significantly out of date,CASSANDRA-1771,12480756,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,zznate,zznate,11/24/2010 0:06,3/12/2019 14:01,3/13/2019 22:24,11/24/2010 15:19,0.7.0 rc 2,,,,0,,,,,,Looks like the correct version is in TRUNK given the 0.7 environment property changes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,19:30.1,,,no_permission,,,,,,,,,,,,20304,,,Wed Nov 24 15:19:30 UTC 2010,,,,,,0|i0g7an:,92612,,,,,,,,,,,"24/Nov/10 15:19;urandom;I should have made the change in 0.7 and merged it forward to trunk/.  Thanks for the catch, this is fixed in r1038639.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in sstable2json,CASSANDRA-934,12460731,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,3/30/2010 19:13,3/12/2019 14:01,3/13/2019 22:24,3/30/2010 19:44,0.6.1,,Legacy/Tools,,0,,,,,,"When sstable2json is not passed any excluded keys via -x, an NPE is raised.",,,,,,,,,,,,,,,,,,,,30/Mar/10 19:14;brandon.williams;934.patch;https://issues.apache.org/jira/secure/attachment/12440266/934.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,44:27.0,,,no_permission,,,,,,,,,,,,19924,,,Tue Mar 30 19:44:27 UTC 2010,,,,,,0|i0g1zb:,91751,,,,,,,,,,,30/Mar/10 19:44;jbellis;committed to 0.6 branch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_range_slice() returns removed columns,CASSANDRA-647,12443844,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,vomjom,vomjom,12/19/2009 18:22,3/12/2019 14:01,3/13/2019 22:24,12/25/2009 3:03,0.5,,,,0,,,,,,"Here's an example (using my new python library at http://github.com/vomjom/pycassa ):

>>> import pycassa
>>> test = pycassa.ColumnFamily(pycassa.connect(), 'Test Keyspace', 'Test UTF8')
>>> list(test.get_range())
[]
>>> test.insert('key', {'column': 'value'})
1261512409
>>> list(test.get_range())
[('key', {'column': 'value'})]
>>> test.remove('key', 'column')
1261512421
>>> list(test.get_range())
[('key', {'column': 'K0\xd2\x85'})]
",Linux,,,,,,,,,,,,,,,,,,,24/Dec/09 22:39;jbellis;647.patch;https://issues.apache.org/jira/secure/attachment/12428932/647.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,39:12.2,,,no_permission,,,,,,,,,,,,19799,,,Fri Dec 25 03:03:11 UTC 2009,,,,,,0|i0g07z:,91466,,,,,,,,,,,24/Dec/09 22:39;jbellis;adds thrift test and fixes bug,24/Dec/09 23:49;lenn0x;+1,25/Dec/09 03:03;jbellis;committed to 0.5 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
need serialVersionUID for Range,CASSANDRA-595,12442231,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,urandom,urandom,12/2/2009 16:08,3/12/2019 14:01,3/13/2019 22:24,12/9/2009 2:01,,,,,0,,,,,,"Range implements Serializable solely to accommodate JMX (Cassandra serializes Ranges differently), and the serialVersionUID was left unset to communicate that we aren't providing any guarantees about the serialization format. This is very inconvenient however, because it means that JMX client applications must be from the same build to work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,01:00.1,,,no_permission,,,,,,,,,,,,19771,,,Wed Dec 09 02:01:00 UTC 2009,,,,,,0|i0fzwf:,91414,,,,,,,,,,,09/Dec/09 02:01;jbellis;stu snuck this into CASSANDRA-608,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping doesn't work on new clusters,CASSANDRA-696,12445444,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,1/13/2010 22:19,3/12/2019 14:01,3/13/2019 22:24,1/14/2010 16:48,0.5,,,,0,,,,,,"This is an edge case.

1. start a clean 3 node cluster with autobootstrap on.
2. load some data.
3. bootstrap in a 4th node.

the logs in the 4th node will indicate that data was not received.  If you restart the cluster in between steps 1 and 2, or 2 and 3, boot strapping works fine.  

I find that waiting on the table flush when making the streaming request solves the problem (see patch).",,,,,,,,,,,,,,,CASSANDRA-682,,,,,13/Jan/10 22:20;gdusbabek;wait_for_memtable_flush.patch;https://issues.apache.org/jira/secure/attachment/12430180/wait_for_memtable_flush.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,29:53.9,,,no_permission,,,,,,,,,,,,19825,,,Thu Jan 14 16:48:42 UTC 2010,,,,,,0|i0g0in:,91514,,,,,,,,,,,13/Jan/10 22:20;gdusbabek;block on the table flush.,13/Jan/10 22:29;jbellis;is this a 0.5 bug?,"13/Jan/10 23:12;gdusbabek;Just tested on 0.5... yes, the bug is there too.","13/Jan/10 23:19;gdusbabek;To be precise, it looks like anything still in the memtables will not be streamed to the bootstrapping node since we're not blocking on the table.flush() call.  I haven't researched it enough, but I suspect the same thing will happen in an established cluster: everything already committed to SSTables gets streamed, and anything still left in the memtable is left behind.  It's just that in a brand new cluster, there never are any SSTables in the first place--it makes the bug more obvious.","13/Jan/10 23:22;jbellis;+1.  can you commit to 0.5 and merge to trunk?

",13/Jan/10 23:22;jbellis;(minor tweak: can you change the re-throw from Interrupted to AssertionError?),"14/Jan/10 16:48;gdusbabek;r899280 (0.5)
r899290 (trunk)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow bootstrapping to a token that is already owned by a live node,CASSANDRA-1561,12475517,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,9/30/2010 16:54,3/12/2019 14:01,3/13/2019 22:24,10/7/2010 15:41,0.6.6,0.7 beta 3,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,30/Sep/10 16:54;jbellis;1561.txt;https://issues.apache.org/jira/secure/attachment/12456027/1561.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,11:55.9,,,no_permission,,,,,,,,,,,,20200,,,Thu Oct 07 15:41:11 UTC 2010,,,,,,0|i0g5zj:,92400,gdusbabek,gdusbabek,,,,,,,,,"30/Sep/10 16:57;jbellis;btw, we're already checking for this on node move/loadbalance:

{code}
        if (token != null && tokenMetadata_.sortedTokens().contains(token))
            throw new IOException(""target token "" + token + "" is already owned by another node"");
{code}",07/Oct/10 15:11;gdusbabek;+1,07/Oct/10 15:41;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect parameter names in Thrift interface,CASSANDRA-214,12427125,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,tve,tve,6/4/2009 15:33,3/12/2019 14:01,3/13/2019 22:24,6/5/2009 19:44,0.4,,,,0,,,,,,"The Thrift interface is incorrect as follows. This does not affect correctnes, it just makes it hard to understand the operations.

The Thrift interface for get_superColumn is incorrect. It seems to me that ""3:string columnFamily"" should really be ""3:string columnFamily_superColumnName"" (I know this doesn't have any functional impact, just makes it hard to understand what the operation does)

The Thrift interface for get_slice_super is incorrect. It seems to me that ""3:string columnFamily_superColumnName"" should really be ""3:string columnFamily""
",,,,,,,,,,,,,,,,,,,,05/Jun/09 14:25;jbellis;214-v2.patch;https://issues.apache.org/jira/secure/attachment/12409988/214-v2.patch,04/Jun/09 16:13;jbellis;214.patch;https://issues.apache.org/jira/secure/attachment/12409887/214.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,38:38.4,,,no_permission,,,,,,,,,,,,19597,,,Fri Jun 05 19:44:09 UTC 2009,,,,,,0|i0fxkf:,91036,,,,,,,,,,,"04/Jun/09 17:38;junrao;The patch looks fine.

Now that we are on Thrift api. 
3:string columnFamily_column in get_slice is also confusing. In a regular CF, the parameter should be just CF. In a super CF, the parameter could also be CF_superColumn. Neither matches the current parameter name. Since most users are likely on regular CF, may be we should rename 3 to just columnFamily? 

Similarly, I propose changing 3:string columnFamily_column in get_column_count to just columnFamily.
 ","04/Jun/09 18:00;jbellis;i don't know that being wrong X% of the time is that much better than 100 - X% :)

what if we name it something more generic?  `columnFamily_params`, or `column_path`?","04/Jun/09 18:16;junrao;How about columnFamily__OR__columnFamily_superColumn ?
","04/Jun/09 18:25;jbellis;better, but a little unwieldy :)

columnParent?  columnOwner?  columnContainer?",04/Jun/09 20:58;tve;Maybe instead of focusing on the name it would be good to first focus on the actual spec? I.e. in which operations can which forms of column / super column spec be used? IMHO it would be more important to add this in the form of comments in the interface spec than trying to come up with names that capture all this in_one_word...,"04/Jun/09 21:16;sandeep_tata;There are a bunch of places in the code that have the same problem -- it is not clear if a variable should contain ""CF:col"" or ""CF"" ... we should slowly clean that up using whatever naming convention we pick here.  (In addition to writing better comments.)",04/Jun/09 21:19;jbellis;we ought to be able to do better than that in the code w/ overloaded methods that each have a real signature where a string is a string not two connected with a colon :),"05/Jun/09 14:25;jbellis;Okay, second stab here at the interface file.  I'll update CassandraServer if these are reasonable.

for convenience, here is the terminology used in the patch:

# CF = ColumnFamily name
# SC = SuperColumn name
# C = Column name
# columnParent: the parent of the columns you are specifying.  ""CF"" or ""CF:SC"".
# columnPath: full path to a column.  ""CF:C"" or ""CF:SC:C"".
# superColumnPath: full path to a supercolumn.  ""CF:SC"" only.
# columnPathOrParent: remove will wipe out any layer.  ""CF"" or ""CF:C"" or ""CF:SC"" or ""CF:SC:C"".
","05/Jun/09 16:46;junrao;Looks fine to me in general. For get_columns_since, 3:string columnPath should be columnFamily.",05/Jun/09 17:03;jbellis;can't you do a _since on a supercolumn?  the subcolumns are sorted by time.,05/Jun/09 17:43;junrao;It seems get_slice_since can be used on a SCF. So the parameter should be columnParent instead.,"05/Jun/09 19:44;jbellis;You're right, should be columnParent.

Committed w/ that change.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE heisenbug when starting entire cluster (for first time) with autobootstrap=true,CASSANDRA-522,12439416,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,10/29/2009 15:24,3/12/2019 14:01,3/13/2019 22:24,11/3/2009 2:24,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,03/Nov/09 02:23;jbellis;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-522.txt;https://issues.apache.org/jira/secure/attachment/12423880/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-522.txt,03/Nov/09 02:23;jbellis;ASF.LICENSE.NOT.GRANTED--0002-brute-force-replacement-of-throwing-maps-around-with-e.txt;https://issues.apache.org/jira/secure/attachment/12423881/ASF.LICENSE.NOT.GRANTED--0002-brute-force-replacement-of-throwing-maps-around-with-e.txt,03/Nov/09 02:23;jbellis;ASF.LICENSE.NOT.GRANTED--0003-fix-NPE-caused-by-getToken-on-endpoint-that-isn-t-memb.txt;https://issues.apache.org/jira/secure/attachment/12423882/ASF.LICENSE.NOT.GRANTED--0003-fix-NPE-caused-by-getToken-on-endpoint-that-isn-t-memb.txt,03/Nov/09 02:23;jbellis;ASF.LICENSE.NOT.GRANTED--0004-add-logging-assert.txt;https://issues.apache.org/jira/secure/attachment/12423883/ASF.LICENSE.NOT.GRANTED--0004-add-logging-assert.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,35:51.2,,,no_permission,,,,,,,,,,,,19735,,,Tue Nov 03 12:35:51 UTC 2009,,,,,,0|i0fzg7:,91341,,,,,,,,,,,"03/Nov/09 02:24;jbellis;04
    add logging, assert

03
    fix NPE caused by getToken on endpoint that isn't member of ring yet.  add assert to prevent in the future.

02
    brute-force replacement of throwing maps around with encapsulated TokenMetadata

01
    convert replication strategy methods to multimap

committing because a ton of stuff is blocking on it","03/Nov/09 12:35;hudson;Integrated in Cassandra #247 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/247/])
    fix NPE caused by getToken on endpoint that isn't member of ring yet.  add assert to prevent in the future.
patch by jbellis for 
brute-force replacement of throwing maps around with encapsulated TokenMetadata
patch by jbellis for 

convert replication strategy methods to multimap
patch by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blocking insert may have fewer responses than replication factor,CASSANDRA-180,12425493,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,junrao,junrao,junrao,5/14/2009 18:42,3/12/2019 14:01,3/13/2019 22:24,5/15/2009 21:37,,,,,0,,,,,,"Currently, block_insert always assumes the number of responses equals the replication factor. However, for a small cluster (e,g, 1 node) and/or when failure occurs, the number of responses could be fewer than the replication factor.",,,,,,,,,,,,,,,,,,,,15/May/09 15:17;jbellis;180-v2.patch;https://issues.apache.org/jira/secure/attachment/12408253/180-v2.patch,15/May/09 15:20;jbellis;180-v3.patch;https://issues.apache.org/jira/secure/attachment/12408254/180-v3.patch,14/May/09 18:45;junrao;issue180.patchv1;https://issues.apache.org/jira/secure/attachment/12408161/issue180.patchv1,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,17:43.6,,,no_permission,,,,,,,,,,,,19585,,,Sat May 16 13:04:11 UTC 2009,,,,,,0|i0fxcv:,91002,,,,,,,,,,,14/May/09 18:45;junrao;Attach a fix. The expected number of responses is set based on the number of live nodes identified.,"15/May/09 15:17;jbellis;Reducing the responseCount is going to break things, since that's used for determining when a successful quorum has been reached.

Say you have a 5 node cluster and a replication factor of 3.  But there is a network split and the node a client is talking to can only see itself.  With your patch it would start up a QRH with a RC of 1, get the ack, and report that the write was successful.  But we've just sliently violated our promise of quorum consistency (at least 2 nodes).

The existing code is optimal for when a write succeeeds -- as soon as a quorum is reached it returns, w/o waiting for any more responses that may or may not come.  The only problem is that it will wait for timeout when it is impossible for a write to reach quorum b/c there are not enough nodes.  I've attached a patch that addresses that problem.  What do you think?

(Note that we don't need to try to solve the problem of ""what if at the beginning of a write there are enough nodes to reach quorum, but partway through we get a nack from a node making it impossible"" b/c nodes only ack success, they don't nack failure.  And making them do so adds more complication than it is worth for such an uncommon case.)",15/May/09 15:20;jbellis;v3 checks for the right quorum count.,"15/May/09 15:31;sandeep_tata;+1 for v3

We can now get rid of ""// TODO: throw a thrift exception if we do not have N nodes""","15/May/09 21:20;junrao;v3 looks good to me too.
","15/May/09 21:37;jbellis;committed v3, and r/m'd the TODO line.","16/May/09 13:04;hudson;Integrated in Cassandra #78 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/78/])
    check for enough endpoints before starting a quorum wait.
patch by jbellis; reviewed by Jun Rao and Sandeep Tata for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_slice declares NotFoundException but does not throw it,CASSANDRA-518,12439206,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,10/27/2009 18:50,3/12/2019 14:01,3/13/2019 22:24,10/28/2009 4:44,0.4,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,27/Oct/09 18:58;jbellis;518.patch;https://issues.apache.org/jira/secure/attachment/12423345/518.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,05:30.7,,,no_permission,,,,,,,,,,,,19732,,,Wed Oct 28 04:44:26 UTC 2009,,,,,,0|i0fzfb:,91337,,,,,,,,,,,27/Oct/09 19:00;jbellis;(the actual change is trivial; most of the lines of diff are for the thrift-generated interface/gen-java/org/apache/cassandra/service/Cassandra.java),27/Oct/09 19:05;stuhood;Looks good to me.,28/Oct/09 04:44;jbellis;committed to 0.4 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
don't count temporary to-stream files towards system load,CASSANDRA-554,12440666,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,11/13/2009 21:01,3/12/2019 14:01,3/13/2019 22:24,12/1/2009 23:32,0.5,,,,0,,,,,,"default load metric is ""space used in data directories"" which will include to-stream files.",,,,,,,,,,,,,,,,,,,,01/Dec/09 23:20;jbellis;554.patch;https://issues.apache.org/jira/secure/attachment/12426597/554.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:46.2,,,no_permission,,,,,,,,,,,,19753,,,Sat Dec 05 12:34:26 UTC 2009,,,,,,0|i0fznb:,91373,,,,,,,,,,,"01/Dec/09 23:20;jbellis;use size of sstables as load, instead of naively counting everything in the data directories (which would include things like snapshots as well as streaming data)",01/Dec/09 23:27;stuhood;+1 Looks good to me.,01/Dec/09 23:32;jbellis;committed,"05/Dec/09 12:34;hudson;Integrated in Cassandra #278 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/278/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avoid replaying fully-flushed commitlog segments,CASSANDRA-1298,12469599,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,7/19/2010 12:45,3/12/2019 14:00,3/13/2019 22:24,7/20/2010 16:28,0.7 beta 1,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,19/Jul/10 12:48;jbellis;1298.txt;https://issues.apache.org/jira/secure/attachment/12449836/1298.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,58:14.6,,,no_permission,,,,,,,,,,,,20067,,,Wed Jul 21 12:50:28 UTC 2010,,,,,,0|i0g47j:,92112,,,,,,,,,,,20/Jul/10 02:58;mdennis;+1,20/Jul/10 16:28;jbellis;committed,"21/Jul/10 12:50;hudson;Integrated in Cassandra #496 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/496/])
    avoid replaying fully-flushed commitlog segments.  patch by jbellis; reviewed by mdennis for CASSANDRA-1298
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra will replay the last mutation in a commitlog when it shouldn't,CASSANDRA-1512,12474406,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,9/17/2010 17:15,3/12/2019 14:00,3/13/2019 22:24,9/20/2010 22:17,0.6.6,0.7 beta 2,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,17/Sep/10 17:17;jbellis;1512.txt;https://issues.apache.org/jira/secure/attachment/12454868/1512.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,02:53.6,,,no_permission,,,,,,,,,,,,20174,,,Mon Sep 20 22:17:34 UTC 2010,,,,,,0|i0g5in:,92324,gdusbabek,gdusbabek,,,,,,,,,17/Sep/10 17:17;jbellis;patch against trunk,17/Sep/10 17:19;jbellis;(also removes CommitLogTest.testCleanup b/c it's useless),"18/Sep/10 15:52;jbellis;Re testCleanup, it's useless because counting the commitlog segments is a very fragile thing to test.  Every time we start doing more with system tables it breaks.  RecoveryManager[23]Test are more robust.",20/Sep/10 13:02;gdusbabek;+1. ,20/Sep/10 22:17;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra holds a socket in CLOSE_WAIT on the storage port,CASSANDRA-1528,12474752,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,9/21/2010 20:18,3/12/2019 14:00,3/13/2019 22:24,9/21/2010 21:17,0.6.6,0.7 beta 2,,,0,,,,,,"To repro: telnet to 7000, disconnect.  You have a socket in CLOSE_WAIT that will stay there until the server is restarted.",,,,,,,,,,,,,,,,,,,,21/Sep/10 20:54;brandon.williams;1528.txt;https://issues.apache.org/jira/secure/attachment/12455190/1528.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,12:31.8,,,no_permission,,,,,,,,,,,,20180,,,Tue Sep 21 21:17:27 UTC 2010,,,,,,0|i0g5s7:,92367,,,,,,,,,,,"21/Sep/10 20:51;brandon.williams;Apparently, we should actually close sockets when we say we do.  Patch to do so.",21/Sep/10 21:12;zznate;Verified before and after via telnet. Patch applies clean. ,21/Sep/10 21:17;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove deprecated get_string*_property() thrift methods,CASSANDRA-965,12461539,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,4/8/2010 15:51,3/12/2019 14:00,3/13/2019 22:24,4/8/2010 19:28,0.7 beta 1,,,,0,,,,,,"The get_string_property() and get_string_list_property() methods were deprecated in 0.6, they can now be removed in preparation for 0.7.
",,,,,,,,,,,,,,,,,,,,08/Apr/10 18:59;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-965-remove-deprecated-methods-update-depende.txt;https://issues.apache.org/jira/secure/attachment/12441196/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-965-remove-deprecated-methods-update-depende.txt,08/Apr/10 18:59;urandom;ASF.LICENSE.NOT.GRANTED--v1-0002-RingCache-fixups-in-the-wake-of-CASSANDRA-44.txt;https://issues.apache.org/jira/secure/attachment/12441197/ASF.LICENSE.NOT.GRANTED--v1-0002-RingCache-fixups-in-the-wake-of-CASSANDRA-44.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,16:04.4,,,no_permission,,,,,,,,,,,,19940,,,Fri Apr 09 13:43:11 UTC 2010,,,,,,0|i0g267:,91782,,,,,,,,,,,08/Apr/10 19:00;urandom;patches attached (see patch headers for explanations),08/Apr/10 19:16;jbellis;+1,08/Apr/10 19:28;urandom;committed.,"09/Apr/10 13:43;hudson;Integrated in Cassandra #402 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/402/])
    CASSANDRA-965 remove deprecated methods, update dependent code

 * removed method defs and regenerated thrift code
 * removed CassandraServer implementations
 * updated cassandra-cli to call new (describe_*) methods
 * updated RingCache to use describe_ring()
 * removed ""show config file"" from cli (no thrift access for this anymore).
 * updated release notes.

Patch by eevans for CASSANDRA-965
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disallow column family names containing hyphens,CASSANDRA-915,12460046,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,urandom,urandom,3/23/2010 21:33,3/12/2019 14:00,3/13/2019 22:24,4/19/2010 21:58,0.7 beta 1,,,,0,,,,,,"You cannot use use hyphens in column family names because hyphens are used as delimiters in sstable filenames (which are derived from the CF name). 

It should be an error to configure such a column family name.",,,,,,,,,,,,,,,,,,,,23/Mar/10 22:35;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-915-don-t-allow-hyphens-in-column-family-nam.txt;https://issues.apache.org/jira/secure/attachment/12439616/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-915-don-t-allow-hyphens-in-column-family-nam.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,45:35.8,,,no_permission,,,,,,,,,,,,19917,,,Wed Mar 24 12:40:18 UTC 2010,,,,,,0|i0g1v3:,91732,,,,,,,,,,,"23/Mar/10 22:37;urandom;The attached patch should take care of 0.6 (and trunk for the time being), but this ticket will need to be added as a dependency to CASSANDRA-44 in order to make sure that the new-shiny handles this case as well.",23/Mar/10 22:45;jbellis;+1,23/Mar/10 23:00;urandom;committed to 0.6 and trunk,"24/Mar/10 12:40;hudson;Integrated in Cassandra #389 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/389/])
    don't allow hyphens in column family names

Patch by eevans; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe_keyspace not returning reconciler info,CASSANDRA-1273,12469177,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,7/13/2010 17:08,3/12/2019 14:00,3/13/2019 22:24,7/13/2010 17:40,0.7 beta 1,,,,0,,,,,,describe_keyspace should return info on its reconciler.,,,,,,,,,,,,,,,,,,,,13/Jul/10 17:09;jeromatron;add-reconciler-data-patch.txt;https://issues.apache.org/jira/secure/attachment/12449368/add-reconciler-data-patch.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,40:32.1,,,no_permission,,,,,,,,,,,,620,,,Wed Jul 14 13:56:14 UTC 2010,,,,,,0|i0g41z:,92087,,,,,,,,,,,13/Jul/10 17:09;jeromatron;adding patch to return reconciler info to describe_keyspace rval map.,"13/Jul/10 17:40;johanoskarsson;Committed, thanks Jeremy!","14/Jul/10 13:56;hudson;Integrated in Cassandra #491 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/491/])
    Change cescribe_keyspace to return reconciler info. Patch by Jeremy Hanna, review by johan. CASSANDRA-1273
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avoid creating a new byte[] for each mutation replayed,CASSANDRA-1219,12467621,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,6/22/2010 21:41,3/12/2019 14:00,3/13/2019 22:24,6/23/2010 4:07,0.7 beta 1,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,22/Jun/10 21:42;jbellis;1219.txt;https://issues.apache.org/jira/secure/attachment/12447745/1219.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,02:37.5,,,no_permission,,,,,,,,,,,,20037,,,Wed Jun 23 15:14:38 UTC 2010,,,,,,0|i0g3q7:,92034,,,,,,,,,,,22/Jun/10 22:02;mdennis;+1 but we should move the alloc outside of the file loop too,23/Jun/10 04:07;jbellis;committed w/ suggested improvement,"23/Jun/10 15:14;hudson;Integrated in Cassandra #474 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/474/])
    avoid allocation-per-row in log replay
patch by jbellis; reviewed by mdenns for CASSANDRA-1219
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.py broken in trunk,CASSANDRA-1033,12463264,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,4/28/2010 22:05,3/12/2019 14:00,3/13/2019 22:24,4/30/2010 15:36,0.7 beta 1,,,,0,,,,,,"stress.py is broken on trunk, apparently due to the removal of the keyspace argument to thrift calls.",,,,,,,,,,,,,,,,,,,,29/Apr/10 20:10;brandon.williams;1033.txt;https://issues.apache.org/jira/secure/attachment/12443228/1033.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,36:45.1,,,no_permission,,,,,,,,,,,,19966,,,Fri Apr 30 15:41:42 UTC 2010,,,,,,0|i0g2lb:,91850,,,,,,,,,,,"29/Apr/10 20:08;brandon.williams;Patch to create keyspaces as needed, login, and remove the keyspace argument to calls.","30/Apr/10 15:36;jbellis;committed (would be nice to check for keyspace before attempting to create, though)","30/Apr/10 15:41;jbellis;changed ""print e"" to ""print e.why"" which just gives ""Keyspace already exists.""  good enough for contrib/ :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clhm licensing and attribution notices should match current practice,CASSANDRA-528,12439967,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,11/5/2009 20:02,3/12/2019 14:00,3/13/2019 22:24,11/5/2009 22:48,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,05/Nov/09 20:03;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-528-clhm-licensing-attribution-that-match-cu.txt;https://issues.apache.org/jira/secure/attachment/12424149/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-528-clhm-licensing-attribution-that-match-cu.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,25:47.4,,,no_permission,,,,,,,,,,,,19738,,,Fri Nov 06 12:34:56 UTC 2009,,,,,,0|i0fzhj:,91347,,,,,,,,,,,05/Nov/09 20:25;jbellis;+1,05/Nov/09 22:48;urandom;committed,"06/Nov/09 12:34;hudson;Integrated in Cassandra #250 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/250/])
    clhm licensing/attribution that match current practice

Patch by eevans; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Return standard/super designation to introspection and fix typo in DD,CASSANDRA-314,12431286,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,euphoria,euphoria,euphoria,7/23/2009 19:05,3/12/2019 14:00,3/13/2019 22:24,7/23/2009 20:12,,,,,0,,,,,,"As reported by Evan in IRC, DatabaseDescriptor is using ""ColumnIMESTAMP"" instead of ""COLUMN_TIMESTAMP"", and there is now no way to introspect for super-ness.  Type used to show Standard or Super but now shows the comparator type.",,,,,,,,,,,,,,,,,,,,23/Jul/09 19:21;euphoria;314_v1.diff;https://issues.apache.org/jira/secure/attachment/12414367/314_v1.diff,23/Jul/09 19:35;euphoria;314_v2.diff;https://issues.apache.org/jira/secure/attachment/12414369/314_v2.diff,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,12:11.7,,,no_permission,,,,,,,,,,,,19631,,,Thu Jul 23 20:12:11 UTC 2009,,,,,,0|i0fy6f:,91135,,,,,,,,,,,"23/Jul/09 19:21;euphoria;This should do it.  Adds the comparator as 'compareWith' as well as revert type to mean Super or Standard, and converts 'flushperiod' to 'flushPeriod' for consistency.

Output on default storage-conf:

$ ./Cassandra-remote -h 127.0.0.1:9160 describeTable ""Table1""
{ 'Standard1': { 'compareWith': 'org.apache.cassandra.db.marshal.UTF8Type',
                 'desc': 'Table1.Standard1(ROW_KEY, COLUMN_MAP(COLUMN_KEY, COLUMN_VALUE, COLUMN_TIMESTAMP))',
                 'flushPeriod': '60',
                 'type': 'Standard'},
  'Standard2': { 'compareWith': 'org.apache.cassandra.db.marshal.UTF8Type',
                 'desc': 'Table1.Standard2(ROW_KEY, COLUMN_MAP(COLUMN_KEY, COLUMN_VALUE, COLUMN_TIMESTAMP))',
                 'flushPeriod': '0',
                 'type': 'Standard'},
  'StandardByUUID1': { 'compareWith': 'org.apache.cassandra.db.marshal.UUIDType',
                       'desc': 'Table1.StandardByUUID1(ROW_KEY, COLUMN_MAP(COLUMN_KEY, COLUMN_VALUE, COLUMN_TIMESTAMP))',
                       'flushPeriod': '0',
                       'type': 'Standard'},
  'Super1': { 'compareWith': 'org.apache.cassandra.db.marshal.UTF8Type',
              'desc': 'Table1.Super1(ROW_KEY, SUPER_COLUMN_MAP(SUPER_COLUMN_KEY, COLUMN_MAP(COLUMN_KEY, COLUMN_VALUE, COLUMN_TIMESTAMP)))',
              'flushPeriod': '0',
              'type': 'Super'}}
",23/Jul/09 19:35;euphoria;Incorporates Evan's suggestion to mirror storage-conf naming instead of arbitrary names.  Follows CamelCase standard set there.,23/Jul/09 20:12;jbellis;committed v2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
source for (unreleased )commons javaflow jar,CASSANDRA-428,12435007,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,9/7/2009 14:31,3/12/2019 14:00,3/13/2019 22:24,9/8/2009 21:23,0.4,0.5,,,0,,,,,,"Commons javaflow has never had a release, so our 1.0-SNAPSHOT jar is considered bad practice by some, (see: http://www.mail-archive.com/general@incubator.apache.org/msg22471.html).

This issue has been raised twice during cassandra release votes.",,,,,,,,,,,,,,,,,,,,07/Sep/09 16:32;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-428-remove-unused-continuation-code.txt;https://issues.apache.org/jira/secure/attachment/12418828/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-428-remove-unused-continuation-code.txt,07/Sep/09 15:20;urandom;commons-javaflow-r812153-sources.jar;https://issues.apache.org/jira/secure/attachment/12418816/commons-javaflow-r812153-sources.jar,07/Sep/09 15:20;urandom;commons-javaflow-r812153.jar;https://issues.apache.org/jira/secure/attachment/12418815/commons-javaflow-r812153.jar,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,54:29.4,,,no_permission,,,,,,,,,,,,19684,,,Tue Sep 08 21:23:31 UTC 2009,,,,,,0|i0fyvb:,91247,,,,,,,,,,,"07/Sep/09 14:54;jbellis;if simply removing the code that depends on it is an option, that would be my preferred solution.","07/Sep/09 15:20;urandom;I propose committing the attached jars, and removing lib/commons-javaflow-1.0-SNAPSHOT.jar.

These jars were built from a fresh checkout of commons-javaflow using r812153 (HEAD as of today). They pass functional and unit tests.",07/Sep/09 15:22;urandom;I'd be +1 with removing it as well (though I haven't looked very closely at our usage of it).,"07/Sep/09 16:36;urandom;v1-0001-CASSANDRA-428-remove-unused-continuation-code.txt removes the code that created the commons-javaflow dependency, and to the best of my knowledge, isn't being used for anything.

Of the two solutions I prefer this one be applied to the 0.4 branch.

P.S. I really like removing code. It might be a sickness.",08/Sep/09 20:40;jbellis;+1 for the r/m patch from me,08/Sep/09 21:23;urandom;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable import tool,CASSANDRA-499,12438587,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,10/20/2009 16:18,3/12/2019 14:00,3/13/2019 22:24,12/2/2009 20:15,0.5,,Legacy/Tools,,0,,,,,,Create a tool for converting json to SSTables,,,,,,,,,,,,,,,,,,,,22/Oct/09 21:01;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-499-JSON-to-SSTable-converter-importer.txt;https://issues.apache.org/jira/secure/attachment/12422951/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-499-JSON-to-SSTable-converter-importer.txt,20/Oct/09 21:01;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-499-null-out-row-member-on-seekTo.txt;https://issues.apache.org/jira/secure/attachment/12422717/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-499-null-out-row-member-on-seekTo.txt,22/Oct/09 21:01;urandom;ASF.LICENSE.NOT.GRANTED--v1-0002-add-new-JSON-parsing-library.txt;https://issues.apache.org/jira/secure/attachment/12422952/ASF.LICENSE.NOT.GRANTED--v1-0002-add-new-JSON-parsing-library.txt,20/Oct/09 21:01;urandom;ASF.LICENSE.NOT.GRANTED--v1-0002-export-specific-keys-passed-on-command-line.txt;https://issues.apache.org/jira/secure/attachment/12422718/ASF.LICENSE.NOT.GRANTED--v1-0002-export-specific-keys-passed-on-command-line.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,05:31.0,,,no_permission,,,,,,,,,,,,19723,,,Sat Dec 05 12:34:25 UTC 2009,,,,,,0|i0fzb3:,91318,,,,,,,,,,,20/Oct/09 21:01;urandom;export by-key,20/Oct/09 21:05;jbellis;+1,"21/Oct/09 12:34;hudson;Integrated in Cassandra #234 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/234/])
     null-out row member on seekTo()

Otherwise a later call to IteratingRow.skipRemaining() (from next())
will undermine the seek.
","22/Oct/09 21:03;urandom;I made a mess out of the attachment names, but:

v1-0001-CASSANDRA-499-JSON-to-SSTable-converter-importer.txt -- importer and wrapper script
v1-0002-add-new-JSON-parsing-library.txt -- adds the jar and documents in NOTICE.txt

Are for the JSON importer.","27/Oct/09 15:06;jbellis;looks good overall

needs entry to licenses/ (are we still doing that?)

can you add a test to round-trip CF -> json -> CF (super and normal versions)?

","27/Oct/09 15:59;urandom;> needs entry to licenses/ (are we still doing that?)

Nope, that information is tacked onto the end of LICENSE.txt (less controversial with IPMC members), but I will make sure that it gets added.

> can you add a test to round-trip CF -> json -> CF (super and normal versions)? 

Yup.",04/Nov/09 02:51;jbellis;test should probably include deleted data (and check that it's not exported),"09/Nov/09 17:12;jbellis;... actually we should have an option to include/exclude the tombstones i think, for a single machine you will not want to bother but in a dump/reload scenario you do want them.

sorry to feature creep :)","25/Nov/09 06:05;jbellis;For the record, I wrote a little code turning the column into a three-tuple (to include timestamp) instead of a dict for CASSANDRA-582, but didn't commit since it didn't seem worth it on its own.","25/Nov/09 16:14;jbellis;So after the debugging session last night I think there are two uses for import / export: debugging, and actual data migration.

For debugging we want the raw view: unmerged sstables, the deletion times on CF and SC objects, and C.isdeleted boolean.

For migration we want removeDeleted to be called, and we want to be able to merge sstables into a single json stream in a sort of compaction.  Deletion information could be elided since anything we show will be undeleted.

I no longer think we want raw data for cluster dump/load; if you're round-tripping through json for that, You're Doing It Wrong.

So for this ticket I think the minimum useful feature set is to support dump/load of raw sstables.  Later, we can add merge and cleaned-up-format for post-removeDeleted export.

How does that sound?","30/Nov/09 03:36;urandom;I went ahead and committed what I have so far, it spans the following changesets:

https://svn.apache.org/viewvc?view=revision&revision=885322
https://svn.apache.org/viewvc?view=revision&revision=885323
https://svn.apache.org/viewvc?view=revision&revision=885324
https://svn.apache.org/viewvc?view=revision&revision=885325

This includes an import utility w/ wrapper script (r885322) and its json dependency (r885323). The timestamp and isdeleted boolean are now preserved for columns, along with a deletedAt for super columns (r885324). There are also tests for both import and export (r885325).

I'd appreciate it if someone could give this a once over, and I'll leave this ticket open for the time being in case any feedback needs to be incorporated.",02/Dec/09 17:57;gdusbabek;I took a look at the changes.  +1.,"05/Dec/09 12:34;hudson;Integrated in Cassandra #278 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/278/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli parsing error,CASSANDRA-738,12446624,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,johanhil,johanhil,1/26/2010 1:21,3/12/2019 14:00,3/13/2019 22:24,2/24/2010 16:39,0.6,,Legacy/Tools,,1,,,,,,"Steps to reproduce:
1. Download the 0.5 release
2. Start Cassandra
3. Start cassandra-cli
4. Execute ""set foo.bar['toot']='balls'""

Expected output:
An error message telling me I'm not doing it right.

Actual output:
cassandra> set foo.bar['toot']='balls'
Exception in thread ""main"" java.lang.AssertionError: serious parsing error (this is a bug).
	at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:367)
	at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:63)
	at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:131)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:172)

Perhaps this is related to https://issues.apache.org/jira/browse/CASSANDRA-615 in a non-direct way.",,,,,,,,,,,,,,,,,,,,24/Feb/10 01:36;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-738-cli-friendlier-error-messages.txt;https://issues.apache.org/jira/secure/attachment/12436796/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-738-cli-friendlier-error-messages.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,32:22.1,,,no_permission,,,,,,,,,,,,19846,,,Wed Feb 24 16:39:00 UTC 2010,,,,,,0|i0g0rr:,91555,,,,,,,,,,,20/Feb/10 18:32;jab_doa;I experienced exactly the same problem. Setup as described in http://wiki.apache.org/cassandra/GettingStarted.,24/Feb/10 03:28;jbellis;+1 Eric's fixes,24/Feb/10 16:39;urandom;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodeprobe outputs incorrectly ordered ring,CASSANDRA-421,12434859,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,9/4/2009 15:54,3/12/2019 14:00,3/13/2019 22:24,9/7/2009 14:55,0.4,0.5,Legacy/Tools,,0,,,,,,"Ring data is returned as a Map and since ranges are never sorted, the ring output can contain nodes that are out of order.",,,,,,,,,,,,,,,,,,,,04/Sep/09 15:55;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-421-properly-order-ranges-in-nodeprobe-outpu.txt;https://issues.apache.org/jira/secure/attachment/12418638/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-421-properly-order-ranges-in-nodeprobe-outpu.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,01:36.9,,,no_permission,,,,,,,,,,,,19681,,,Mon Sep 07 14:55:45 UTC 2009,,,,,,0|i0fytr:,91240,,,,,,,,,,,"04/Sep/09 15:58;urandom;If we roll an rc2, it would be nice to include this patch (it should be safe, it is out of the critical path).",04/Sep/09 16:01;jbellis;+1,07/Sep/09 14:55;urandom;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Path not found under Windows 7,CASSANDRA-1270,12469062,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,szogun1987,szogun1987,7/12/2010 12:57,3/12/2019 14:00,3/13/2019 22:24,12/28/2010 15:41,,,,,0,,,,,,"I'm not sure that this is bug maybe it is my fault but when I try to run Cassandra using bin\cassandra -f my system returns ""Path not found message"". When i comment ECHO OFF from cassandra.bat I have seen that last line of output contains "".8.jar"";""D:\Cassandra\bin\..\lib\slf4j-log4j12-1.5.8.jar"";""D:\Cassandra\bin\..\build\classes"" ""org.apache.cassandra.thrift.CassandraDaemon""""
D:\Cassandra is my cassandra root directory. Directory ""D:\Casandra\build\classes\org\apache\cassandra\thrift"" contains CassandraDaemon.class, CassandraDaemon$1.class, CassandraDaemon$2.class files.

Apologize for my Vocabulary and Grammar.
","Windows 7 Professional, JRE 1.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,42:09.0,,,no_permission,,,,,,,,,,,,20054,,,Tue Dec 28 15:42:08 UTC 2010,,,,,,0|i0g41b:,92084,,,,,,,,,,,28/Dec/10 15:42;jbellis;I believe this is fixed in recent 0.6 releases,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clean up better after streaming,CASSANDRA-550,12440628,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,11/13/2009 16:28,3/12/2019 14:00,3/13/2019 22:24,11/13/2009 21:00,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,13/Nov/09 20:46;jbellis;550-2.patch;https://issues.apache.org/jira/secure/attachment/12424894/550-2.patch,13/Nov/09 16:45;jbellis;550.patch;https://issues.apache.org/jira/secure/attachment/12424862/550.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,11:58.9,,,no_permission,,,,,,,,,,,,19750,,,Sat Nov 14 12:33:59 UTC 2009,,,,,,0|i0fzmf:,91369,,,,,,,,,,,"13/Nov/09 18:11;rays;Getting an error after restarting with this patch applied.

ERROR [main] 2009-11-13 14:41:07,289 CassandraDaemon.java (line 184) Exception encountered during startup.
java.lang.NullPointerException
	at org.apache.cassandra.db.SystemTable.initMetadata(SystemTable.java:150)
	at org.apache.cassandra.service.StorageService.start(StorageService.java:259)
	at org.apache.cassandra.service.CassandraServer.start(CassandraServer.java:72)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:94)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:166)

full log: http://pastebin.com/m25468765",13/Nov/09 19:32;jbellis;looks like a separate bug...  this should fix it,"13/Nov/09 20:13;rays;should I file as new?

ERROR - Exception encountered during startup.
java.lang.ClassCastException: [B cannot be cast to java.lang.Comparable
at java.util.concurrent.ConcurrentSkipListMap.comparable(ConcurrentSkipListMap.java:621)
at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:862)
at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
at java.util.concurrent.ConcurrentSkipListSet.add(ConcurrentSkipListSet.java:202)
at org.apache.cassandra.db.SystemTable.initMetadata(SystemTable.java:122)
at org.apache.cassandra.service.StorageService.start(StorageService.java:259)
at org.apache.cassandra.service.CassandraServer.start(CassandraServer.java:72)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:94)
at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:166)
Exception encountered during startup.",13/Nov/09 20:53;rays;Its working now after 500-2.patch was applied and successfully cleaned up some old junk on my nodes.,13/Nov/09 21:00;jbellis;committed,"14/Nov/09 12:33;hudson;Integrated in Cassandra #258 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/258/])
    specifically look for TOKEN and GENERATION columns
patch by jbellis; tested by Ray Slakinski for 
clean up temporary for-streaming files when done
patch by jbellis; tested by Ray Slakinski for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool ring crashes when no schema is loaded,CASSANDRA-1286,12469470,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,7/16/2010 17:04,3/12/2019 14:00,3/13/2019 22:24,8/2/2010 20:22,0.7 beta 1,,Tool/nodetool,,1,,,,,,"Nodetool ring uses SP.getRangeToEndpointMap(null) that tries to retrieve the first non system keyspace.
Hences it crashes (with a IndexOutOfBoundsException) if no schema is loaded.

Should we return a nice little error message or make it work even no schema is loaded ?",,,,,,,,,,,,,,,,,,,,02/Aug/10 15:07;slebresne;1286-Don-t-use-ranges-to-print-the-ring-with-nodetool.patch;https://issues.apache.org/jira/secure/attachment/12451044/1286-Don-t-use-ranges-to-print-the-ring-with-nodetool.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,18:02.7,,,no_permission,,,,,,,,,,,,20058,,,Wed Aug 04 13:25:35 UTC 2010,,,,,,0|i0g44v:,92100,,,,,,,,,,,31/Jul/10 20:18;brandon.williams;FYI: CASSANDRA-1291 is also suffering from a problem with SP.getRangeToEndpointMap(null) when the RF is greater than 1.,"01/Aug/10 02:12;dopsun;This becomes a real problem for Cassandra 0.7, because by default, Cassandra 0.7 does not load any key space, and means that cannot use the nodetool to check the newly setup cluster.",02/Aug/10 12:25;gdusbabek;We should be able to get ring data by directly consulting TokenMetadata (no need to collect range information which is what the keyspace argument gives us).,"02/Aug/10 15:07;slebresne;Attaching a patch that follows Gary suggestion. It simply exposes the
map of tokens -> endpoint (including the boostrapping ones).

This fixes the ring when no schema is loaded at least.",02/Aug/10 19:30;gdusbabek;Should we deprecate StorageServiceMBean.getPendingRangeToEndpointMap()?,"02/Aug/10 19:47;jbellis;No, that's there because people want to use it to connect their clients to the ""right"" machines directly.",02/Aug/10 20:21;gdusbabek;+1 committed.,"04/Aug/10 13:25;hudson;Integrated in Cassandra #509 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/509/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool cfhistograms write/read latency columns are reversed,CASSANDRA-2123,12497861,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,cburroughs,cburroughs,2/7/2011 15:53,3/12/2019 14:00,3/13/2019 22:24,2/9/2011 16:48,0.7.1,,Tool/nodetool,,0,,,,,,"As first reported by Oleg Proudnikov in the thread http://www.mail-archive.com/user@cassandra.apache.org/msg09607.html the columns for read and write latency are reversed in the output of cfhistograms.  The Mbean values are correct.

Example output during stress.java insert test.
{noformat}
Keyspace1/Standard1 histograms
Offset      SSTables     Write Latency      Read Latency          Row Size      Column Count
1                  0                 0                 0                 0                 0
2                  0                 0                 1                 0                 0
3                  0                 0               998                 0                 0
4                  0                 0              7729                 0                 0
5                  0                 0             22844                 0                 0
6                  0                 0             44439                 0           6524792
7                  0                 0             64576                 0                 0
8                  0                 0             79000                 0                 0
10                 0                 0            139338                 0                 0
12                 0                 0             84675                 0                 0
14                 0                 0             36928                 0                 0
17                 0                 0             16547                 0                 0
20                 0                 0              3926                 0                 0
24                 0                 0              1681                 0                 0
29                 0                 0               776                 0                 0
35                 0                 0               357                 0                 0
42                 0                 0               172                 0                 0
50                 0                 0                51                 0                 0
60                 0                 0                15                 0                 0
72                 0                 0                10                 0                 0
86                 0                 0                 4                 0                 0
103                0                 0                 6                 0                 0
124                0                 0                 3                 0                 0
149                0                 0                 1                 0                 0
179                0                 0                 0                 0                 0
215                0                 0                 1                 0                 0
258                0                 0                 1                 0                 0
310                0                 0                 0                 0                 0
372                0                 0                 1           6524792                 0
446                0                 0                 2                 0                 0
535                0                 0                 0                 0                 0
642                0                 0                 0                 0                 0
770                0                 0                 0                 0                 0
924                0                 0                 0                 0                 0
1109               0                 0                 1                 0                 0
1331               0                 0                 0                 0                 0
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,48:50.9,,,no_permission,,,,,,,,,,,,20454,,,Wed Feb 09 18:01:03 UTC 2011,,,,,,0|i0g9g7:,92961,,,,,,,,,,,09/Feb/11 16:48;jbellis;fixed in r1068967,"09/Feb/11 18:01;hudson;Integrated in Cassandra-0.7 #267 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/267/])
    put read/write latencies in the right columns for nodetool cfhistograms
patch by jbellis for CASSANDRA-2123
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows batch files use incorrect paths,CASSANDRA-1713,12479195,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vloncar,vloncar,vloncar,11/5/2010 12:42,3/12/2019 14:00,3/13/2019 22:24,11/9/2010 16:05,0.6.8,0.7.0 rc 1,Packaging,,0,,,,,,"Windows .bat files (with the exception of cassandra.bat) use %CD% to set CASSANDRA_HOME, and since that is incorrect, they fail to start with ClassNotFoundException.",Windows,,,,,,,,,,,,,,,,,,,05/Nov/10 12:43;vloncar;windows-batch-fix.patch;https://issues.apache.org/jira/secure/attachment/12458902/windows-batch-fix.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,02:04.7,,,no_permission,,,,,,,,,,,,20268,,,Tue Nov 09 16:05:10 UTC 2010,,,,,,0|i0g6xj:,92553,bcoverston,bcoverston,,,,,,,,,08/Nov/10 22:02;bcoverston;+1 this looks good. Thanks!,08/Nov/10 23:38;jbellis;what is the trick to apply?  fails for me against 0.7 and 0.6,"09/Nov/10 09:10;vloncar;I created the patch with diff, and tested it with patch tool, both from GnuWin32 package. It works on both 0.6 and 0.7, although on 0.6 there is one line of offset (it still applies successfully). I also noticed that cassandra.bat in 0.7 sets CASSANDRA_CONF, and i dont see it being used (conf dir gets on classpath a few lines later, so this seems redundant). I tested without it, and it appears to work normally. Should i also include that in the patch?

On a related note, what is needed to bring cassandra up to par on windows? I know there are some JNA enhancements, and now direct IO which are Linux-only, but there are probably more I am not aware of. Maybe opening a ticket to track these wouldn't be a bad idea?","09/Nov/10 16:05;jbellis;committed.

bq. what is the trick to apply

for the record: dos2unix bin/*.bat; patch; unix2dos bin/*.bat.  If there is a way to get patch to deal with targets with CLRF line endings, I couldn't find it.  (It deals w/ CLRF in the _patch_ just fine.)

bq. I also noticed that cassandra.bat in 0.7 sets CASSANDRA_CONF, and i dont see it being used

removed in the merge to 0.7

bq. what is needed to bring cassandra up to par on windows? I know there are some JNA enhancements, and now direct IO which are Linux-only

I think that's about it really (besides CASSANDRA-292).  The JNA stuff is all in CLibrary -- extracting the higher-level code to another class, and using CLibrary/WindowsLibrary (what is the equivalent of libc?) from that depending on platform is probably the way to go.

bq. Maybe opening a ticket to track these

I'd say open a ticket if you're going to work on it. :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RP.describeOwnership() does some bad math,CASSANDRA-2071,12496999,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,1/27/2011 23:23,3/12/2019 14:00,3/13/2019 22:24,1/27/2011 23:36,0.7.1,,,,0,,,,,,"If the input isn't sorted correctly for some reason, then describeOwnership() fails to calculate the ownership %ages correctly.

Repro is 2 nodes with these tokens, you get these fractions:
49000620740128447720217646403197156812 : 0.7615167
770141183460469231731687303715884105727 : 4.2384834

423% ownership is obviously broken.",,,7200,7200,,0%,7200,7200,,,,,,,,,,,,27/Jan/11 23:29;jhermes;2071.txt;https://issues.apache.org/jira/secure/attachment/12469610/2071.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,36:37.1,,,no_permission,,,,,,,,,,,,20426,,,Thu Jan 27 23:54:35 UTC 2011,,,,,,0|i0g95b:,92912,,,,,,,,,,,"27/Jan/11 23:26;jhermes;Fixes the math by using a closed form in all cases.

Now returns:
49000620740128447720217646403197156812 : 0.7615167
770141183460469231731687303715884105727 : 0.2384833
","27/Jan/11 23:29;jhermes;Whoops, wrong file.",27/Jan/11 23:36;brandon.williams;Committed.,"27/Jan/11 23:54;hudson;Integrated in Cassandra-0.7 #223 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/223/])
    Fix RP.describeOwnership()
Patch by Jon Hermes, reviewed by brandonwilliams for CASSANDRA-2071
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"pig contrib module not building, other errors",CASSANDRA-1150,12465974,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,6/2/2010 15:53,3/12/2019 14:00,3/13/2019 22:24,6/12/2010 13:09,0.6.3,,,,0,,,,,,"Currently, the pig contrib module fails to build because of dependency issues - it looks like dependencies like hadoop that were at one time in the main cassandra dependency list.

Also, once the dependencies are resolved, there are still errors when running the example pig query in the README.txt in the module.

This ticket would address both of those issues and getting it working both on 0.6.x as well as mainline trunk.",,,,,,,,,,,,,,,,CASSANDRA-1162,,,,02/Jun/10 17:32;jeromatron;1150-build-0_6.txt;https://issues.apache.org/jira/secure/attachment/12446160/1150-build-0_6.txt,02/Jun/10 17:35;jeromatron;1150-build-trunk.txt;https://issues.apache.org/jira/secure/attachment/12446162/1150-build-trunk.txt,08/Jun/10 20:09;jeromatron;1150-runtime-0_6-patch.txt;https://issues.apache.org/jira/secure/attachment/12446622/1150-runtime-0_6-patch.txt,08/Jun/10 22:06;jeromatron;1150-runtime-trunk.txt;https://issues.apache.org/jira/secure/attachment/12446631/1150-runtime-trunk.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,29:11.6,,,no_permission,,,,,,,,,,,,20010,,,Sun Jun 13 12:45:40 UTC 2010,,,,,,0|i0g3av:,91965,,,,,,,,,,,02/Jun/10 16:37;jeromatron;This patch is just for the build portion of the bug.,"02/Jun/10 17:29;jbellis;I think I'd rather ""fix"" this by adding the instruction ""rename or symlink your pig jar, to pig.jar"" so we're not pinned to a specific pig version.","02/Jun/10 17:32;jeromatron;Updated to use a wildcard for the pig version, also attaching build fixes for both 0.6 branch and trunk.",02/Jun/10 17:35;jeromatron;Bah - updated the trunk build fix - wishing IDEA wouldn't be buggy about what it included in patches...,04/Jun/10 02:53;jbellis;committed,04/Jun/10 17:20;jeromatron;It is now building at least - resolving this issue as CASSANDRA-1162 will address the other concerns - making it work properly within core and using a demo usage of it in the contrib section.,07/Jun/10 22:51;jeromatron;Reopening since integrating with core will take a while (no mvn repo with pig).,07/Jun/10 23:29;jeromatron;Adding patches for 0.6 branch and trunk.,"07/Jun/10 23:31;jeromatron;Adds a default pig script to run based on the README suggestion.  Adds more to the README indicating how to run in local mode since that's the least volatile mode to run in.

Also took out a couple of references to 0.7.0-dev -> 0.7.0.","08/Jun/10 20:09;jeromatron;For both runtime patches:
Fixed the runtime problems with pig. Updated the README.txt for pig and added an example-script.pig. Fixed a few places where it referred to pig 0.7.0-dev.

For trunk:
It was calling the comparator's newInstance where it should use the singleton version.  Fixed that in a few other places as well.  Fixed a couple of comments in WordCountSetup.  Used cassandra.yaml instead of storage-conf.xml.  
Also enabled the executable bit on the word_count, word_count_setup, and pig_cassandra scripts.",08/Jun/10 22:06;jeromatron;Added a bit to the NEWS.txt to let people know about the singleton model for AbstractType extensions.,12/Jun/10 13:09;johanoskarsson;Committed to 0.6 branch and trunk.,"13/Jun/10 12:45;hudson;Integrated in Cassandra #464 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/464/])
    Remove references to -dev version of pig, add example script, use comparators singletons. Patch by Jeremy Hanna, review by johan. CASSANDRA-1150
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ant build script in contrib/stress fails.,CASSANDRA-2291,12500802,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jahangir,jahangir,3/8/2011 19:12,3/12/2019 14:00,3/13/2019 22:24,3/8/2011 19:17,0.8 beta 1,,,,0,,,,,,"Build fails in contrib/stress with following message:
/mnt/hgfs/workspace/cassandra-source/contrib/stress/build.xml:38: javac doesn't support the nested ""path"" element.

Fix:
Needs to move <path refid=""cassandra.classes"" /> inside path element.

SVN copy:
            <path refid=""cassandra.classes"" />
            <classpath>
                <path>
                    <fileset dir=""${cassandra.lib}"">
                        <include name=""**/*.jar"" />
                    </fileset>
                </path>
            </classpath>

To be changed to:
            <classpath>
                <path>
                    <path refid=""cassandra.classes"" />
                    <fileset dir=""${cassandra.lib}"">
                        <include name=""**/*.jar"" />
                    </fileset>
                </path>
            </classpath>",Apache Ant version 1.8.0.,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,17:56.7,,,no_permission,,,,,,,,,,,,20544,,,Tue Mar 08 19:17:56 UTC 2011,,,,,,0|i0gai7:,93132,,,,,,,,,,,08/Mar/11 19:17;jbellis;fixed in r1079494.  thanks for the report!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java doesn't read more unique rows than 2x the number of threads,CASSANDRA-2147,12498161,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,2/9/2011 19:02,3/12/2019 14:00,3/13/2019 22:24,2/18/2011 22:48,0.7.3,,,,0,,,,,,"This can be observed by watching how much the row/key cache grows on each run.  I'm not sure when this started or if it was always the case, but it's actually useful behavior when you want to benchmark just the cache, so it'd be nice to preserve as an option.",,,7200,7200,,0%,7200,7200,,,,,,,,,,,,18/Feb/11 19:58;xedin;CASSANDRA-2147.patch;https://issues.apache.org/jira/secure/attachment/12471437/CASSANDRA-2147.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,55:44.8,,,no_permission,,,,,,,,,,,,20464,,,Sat Feb 19 01:42:19 UTC 2011,,,,,,0|i0g9lr:,92986,brandon.williams,brandon.williams,,,,,,,,,10/Feb/11 02:55;jbellis;can't we use stdev instead of adding another option?,"18/Feb/11 19:58;xedin;there was a small mistake in the guassian distribution algorithm which is fixed now (also -r flag wasn't parsed correctly, which is fixed too).",18/Feb/11 22:48;brandon.williams;Committed.,"19/Feb/11 01:42;hudson;Integrated in Cassandra-0.7 #297 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/297/])
    Fix key distribution in stress.java.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2147
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
detect incomplete commitlogheader,CASSANDRA-1119,12465237,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mdennis,jbellis,jbellis,5/24/2010 2:28,3/12/2019 13:59,3/13/2019 22:24,6/10/2010 17:07,0.6.3,0.7 beta 1,,,0,,,,,,"Kelvin reported:

I just came across a corrupted CL file.  Here's the stacktrace when starting the server:
Listening for transport dt_socket at address: 8888
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
	at java.io.RandomAccessFile.readLong(RandomAccessFile.java:758)
	at org.apache.cassandra.db.commitlog.CommitLogHeader.readCommitLogHeader(CommitLogHeader.java:145)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:181)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:167)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:95)
	at org.apache.cassandra.thrift.CassandraDaemon.init(CassandraDaemon.java:142)
	... 5 more

He added that the segment is only 6 bytes long, indicating that the header was never completely written.  CLH should catch that EOF and skip the segment when replay is attempted.",,,,,,,,,,,,,,,,,,,,08/Jun/10 23:29;mdennis;0001-cassandra-0.6-1119.patch;https://issues.apache.org/jira/secure/attachment/12446639/0001-cassandra-0.6-1119.patch,08/Jun/10 23:29;mdennis;0001-trunk-1119.patch;https://issues.apache.org/jira/secure/attachment/12446640/0001-trunk-1119.patch,10/Jun/10 15:52;mdennis;0002-cassandra-0.6-1119.patch;https://issues.apache.org/jira/secure/attachment/12446769/0002-cassandra-0.6-1119.patch,10/Jun/10 15:52;mdennis;0002-trunk-1119.patch;https://issues.apache.org/jira/secure/attachment/12446770/0002-trunk-1119.patch,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,35:19.3,,,no_permission,,,,,,,,,,,,19998,,,Thu Jun 10 17:18:35 UTC 2010,,,,,,0|i0g347:,91935,,,,,,,,,,,10/Jun/10 13:35;gdusbabek;Let's log a WARN when this happens so there is some kind of indication that Bad Stuff exists.,"10/Jun/10 14:04;jbellis;well, it's expected behavior if you happen to kill the server at the right time.  maybe INFO.",10/Jun/10 15:52;mdennis;I had not logged anything since it is expected behavior,10/Jun/10 15:53;mdennis;0002 patches log at INFO,"10/Jun/10 16:32;jbellis;Is it possible that we could end up with garbage in the commitlog header, if we lose power during the header update post flush?  (i.e., not a brand new segment?)

if so we should probably split the header out into a separate file, and if we can't read the header, start replay of the mutations file from the beginning.  (This could be a new ticket.)",10/Jun/10 17:07;gdusbabek;committed.,"10/Jun/10 17:18;gdusbabek;Yes, it is possible that the header could be corrupted while writing mutations to the CL (existing CL mutations would be intact).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"changing row cache save interval is reflected in 'describe keyspace' on node it was submitted to, but not nodes it was propagated to",CASSANDRA-1853,12493092,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,scode,scode,12/13/2010 17:19,3/12/2019 13:59,3/13/2019 22:24,12/21/2010 22:31,0.7.0 rc 3,,,,0,,,,,,"This is minor unless it indicates a bigger issue. On our test cluster (running cassandra 0.7 branch from today) we noticed that on submission of a new row cache save period, such as:

   update column family KeyValue with row_cache_save_period=3600;

The change would be reflected in describe_keyspace() (describe keyspace ... in cassandra-cli) on the node to which the schema migration was submitted, but not on other nodes in the cluster.

This in spite of the schema migration having propagated, judging by Schema['Last Migration'] being identical on all nodes. It is not n and of itself is not a big problem, but it does give the impression that the migrations have trouble propagating throughout the cluster even though they do.

(I had a quick (only) look in the code paths of migration application and did not find any obvious special casing of the node that happens to be local. Filing bug instead, hoping someone knows off hand what the reason is.)


",,,,,,,,,,,,,,,,,,,,17/Dec/10 17:47;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-apply-CF-metadata-updates-on-replicas.txt;https://issues.apache.org/jira/secure/attachment/12466475/ASF.LICENSE.NOT.GRANTED--v1-0001-apply-CF-metadata-updates-on-replicas.txt,20/Dec/10 14:26;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0001-apply-CF-metadata-updates-only-at-apply-time.txt;https://issues.apache.org/jira/secure/attachment/12466635/ASF.LICENSE.NOT.GRANTED--v2-0001-apply-CF-metadata-updates-only-at-apply-time.txt,21/Dec/10 12:15;gdusbabek;ASF.LICENSE.NOT.GRANTED--v3-0001-apply-CF-metadata-updates-only-at-apply-time.txt;https://issues.apache.org/jira/secure/attachment/12466704/ASF.LICENSE.NOT.GRANTED--v3-0001-apply-CF-metadata-updates-only-at-apply-time.txt,21/Dec/10 17:37;gdusbabek;ASF.LICENSE.NOT.GRANTED--v4-0001-apply-CF-metadata-updates-only-at-apply-time.txt;https://issues.apache.org/jira/secure/attachment/12466742/ASF.LICENSE.NOT.GRANTED--v4-0001-apply-CF-metadata-updates-only-at-apply-time.txt,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,49:27.6,,,no_permission,,,,,,,,,,,,682,,,Tue Dec 21 23:14:44 UTC 2010,,,,,,0|i0g7tb:,92696,,,,,,,,,,,13/Dec/10 17:30;scode;Forgot to say that restarting the nodes in question (that do not show the updated value) causes it to catch up in describe keyspace output.,17/Dec/10 19:49;jbellis;shouldn't applyModels be the One True Place to call CFMetaData.apply?  I'd like to r/m the apply calls in the constructor and in CassandraServer to avoid confusion.,"20/Dec/10 20:58;jbellis;- still have the apply in CassandraServer, is that redundant?
- iirc (and maybe i do not) the original explanation for this patch was that the other nodes don't go through the constructor that calls apply.  how else are they getting a UCF object?  intellij says the no-op constructor is unused.  i feel like i'm missing something important here.
- brace placement
","21/Dec/10 12:15;gdusbabek;bq. still have the apply in CassandraServer
That apply() is on the Mutation, not a CFMetaData.

bq. how else are they getting a UCF object?
They deserialize the one they are sent by inflate()ing it.  It uses the no-arg constructor.

fixed the braces in v3.","21/Dec/10 14:34;jbellis;bq. That apply() is on the Mutation, not a CFMetaData.

I'm talking about this one:
{code}
        CFMetaData oldCfm = DatabaseDescriptor.getCFMetaData(...);
...
            oldCfm.apply(cf_def);
{code}

bq. They deserialize the one they are sent by inflate()ing it. It uses the no-arg constructor.

inflate calls the many-arg constructor.  I removed the no-arg constructor to experiment and ""ant clean test"" passes.","21/Dec/10 14:36;jbellis;bq. I'm talking about this one

... that's in the _avro_ CassandraServer, btw.","21/Dec/10 17:37;gdusbabek;fixed the avro.CassandraServer problem.

bq. inflate calls the many-arg constructor. 
I think we're talking about different things. I referred to UCF.inflate().  Either way, CFM.apply() is only called on the active metadata in one place.

v4 attached.","21/Dec/10 18:32;jbellis;I still don't see anything calling a no-arg UCF constructor, including UCF.subinflate (there is no UCF.inflate).

The reason I am stuck on this is, if everyone was calling the arg-full UCF constructor, then apply must have been getting called, which means our bug diagnosis is wrong.","21/Dec/10 22:12;jbellis;{code}
 * Each class that extends Migration is required to implement a no arg constructor, which will be used to inflate the
 * object from it's serialized form.
{code}

reflection ftw.

+1 after fixing formatting of
{code}
        } catch (ConfigurationException ex) 
{code}

:)",21/Dec/10 22:31;gdusbabek;committed.,"21/Dec/10 22:43;hudson;Integrated in Cassandra-0.7 #106 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/106/])
    apply CF metadata updates only at apply time. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1853
","21/Dec/10 23:14;hudson;Integrated in Cassandra #637 (See [https://hudson.apache.org/hudson/job/Cassandra/637/])
    apply CF metadata updates only at apply time. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1853
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update gc options for debian package,CASSANDRA-1172,12466399,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,jbellis,jbellis,6/7/2010 23:03,3/12/2019 13:59,3/13/2019 22:24,6/21/2010 14:44,0.6.3,,,,0,,,,,,"/etc/default/cassandra needs the new jvm options from cassandra.in.sh

also, it looks like heap size is also set-able in the init.d script, but it was ignored in favor of the values from /etc/default/cassandra, it would be less confusing to leave those out of the init.d script",,,,,,,,,,,,,,,,,,,,18/Jun/10 19:24;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1172-update-gc-options-for-debian-package.txt;https://issues.apache.org/jira/secure/attachment/12447487/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-1172-update-gc-options-for-debian-package.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,29:48.6,,,no_permission,,,,,,,,,,,,20016,,,Mon Jun 21 14:44:33 UTC 2010,,,,,,0|i0g3fr:,91987,,,,,,,,,,,"18/Jun/10 19:29;urandom;bq. /etc/default/cassandra needs the new jvm options from cassandra.in.sh

See attached patch.

bq. also, it looks like heap size is also set-able in the init.d script, but it was ignored in favor of the values from /etc/default/cassandra, it would be less confusing to leave those out of the init.d script

This is standard practice on Debian systems. The defaults file (/etc/default/cassandra) is optional, as are the variables set within. It's probably easier to think of it as a way of overriding the init script, rather than as a canonical source of configuration.",19/Jun/10 03:57;jbellis;+1,21/Jun/10 14:44;urandom;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replicate on write NPE for empty row,CASSANDRA-2289,12500798,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,3/8/2011 19:00,3/12/2019 13:59,3/13/2019 22:24,3/9/2011 15:24,0.8 beta 1,,,,0,,,,,,Replicate on write will throw a NPE for the first write to a row.,,,,,,,,,,,,,,,,,,,,09/Mar/11 01:24;stuhood;0001-Appendix-to-CASSANDRA-2289.txt;https://issues.apache.org/jira/secure/attachment/12473084/0001-Appendix-to-CASSANDRA-2289.txt,08/Mar/11 19:00;stuhood;0001-don-t-replicate-empty-row.txt;https://issues.apache.org/jira/secure/attachment/12473027/0001-don-t-replicate-empty-row.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,03:25.6,,,no_permission,,,,,,,,,,,,20542,,,Wed Mar 09 18:23:03 UTC 2011,,,,,,0|i0gahr:,93130,slebresne,slebresne,,,,,,,,,08/Mar/11 20:03;slebresne;+1,08/Mar/11 20:09;jbellis;committed,"08/Mar/11 21:26;hudson;Integrated in Cassandra #759 (See [https://hudson.apache.org/hudson/job/Cassandra/759/])
    don't replicate rows that don't exist yet
patch by Stu Hood; reviewed by slebresne for CASSANDRA-2289
",09/Mar/11 01:24;stuhood;Very sorry... the original patch was not enough to fix this. Attaching an appendix. EDIT: errata?,09/Mar/11 07:59;slebresne;+1 to the errata too. ,09/Mar/11 15:24;jbellis;committed,"09/Mar/11 18:23;hudson;Integrated in Cassandra #765 (See [https://hudson.apache.org/hudson/job/Cassandra/765/])
    fix #2 for counter replication NPE
patch by Stu Hood; reviewed by slebresne for CASSANDRA-2289
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Returning split length of 0 confuses Pig,CASSANDRA-2184,12498930,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,jbellis,jbellis,2/17/2011 18:35,3/12/2019 13:59,3/13/2019 22:24,2/18/2011 22:49,0.7.3,,,,1,,,,,,"Matt Kennedy reports on the user list,

bq. There is a new feature in Pig 0.8 that will try to reduce the number of splits used to speed up the whole job.  Since the ColumnFamilyInputFormat lists the input size as zero, this feature eliminates all of the splits except for one. 
bq. The workaround is to disable this feature for jobs that use CassandraStorage by setting -Dpig.splitCombination=false in the pig_cassandra script.
{noformat}

bq. However, we wanted to keep splitCombination on because it is a useful optimization for a lot of our use cases, so I went digging for the least intrusive way to keep the split combiner on, but also prevent it from combining splits that read from Cassandra.  My solution, which you are welcome to critique, is to change line 65 of http://svn.apache.org/viewvc/cassandra/trunk/src/java/org/apache/cassandra/hadoop/ColumnFamilySplit.java such that it returns Long.MAX_VALUE instead of zero.

I looked into actually returning the number of keys in the split but Hadoop javadoc says ""Get the size of the split, so that the input splits can be sorted by size"" so since our splits should be very very close in size this doesn't sound like it's worth doing an extra round trip to the host servers to get super accurate numbers on.  Returning MAX_VALUE seems like it's good enough.",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,21:00.8,,,no_permission,,,,,,,,,,,,20487,,,Fri Feb 18 22:51:53 UTC 2011,,,,,,0|i0g9tz:,93023,,,,,,,,,,,18/Feb/11 22:21;brandon.williams;Committed.,"18/Feb/11 22:51;hudson;Integrated in Cassandra-0.7 #296 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/296/])
    Change split length from 0 to Long.MAX_VALUE
Patch by Matt Kennedy, reviewed by brandonwilliams for CASSANDRA-2184
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MIsc license inclusion booboos with recent index commits,CASSANDRA-1294,12469517,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,zznate,zznate,zznate,7/17/2010 4:50,3/12/2019 13:59,3/13/2019 22:24,7/17/2010 12:47,0.7 beta 1,,,,0,,,,,,"Noticed some missing license headers, a duped header in one case when going over changes in CASSANDRA-1154",,,,,,,,,,,,,,,,,,,,17/Jul/10 04:52;zznate;trunk-1249-license-headers.txt;https://issues.apache.org/jira/secure/attachment/12449748/trunk-1249-license-headers.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,47:29.2,,,no_permission,,,,,,,,,,,,20065,,,Mon Jul 19 14:45:02 UTC 2010,,,,,,0|i0g46n:,92108,,,,,,,,,,,17/Jul/10 04:52;zznate;the radio button for attachment license is strangely ironic here.,17/Jul/10 12:47;jbellis;committed,17/Jul/10 13:54;messi;Don't add license headers as Javadoc comments.,"17/Jul/10 14:40;zznate;Agree with Folke on this in principal, however javadoc seemed to be the most prevalent style in the codebase. Another patch for someone more OCD than myself perhaps. ","17/Jul/10 15:35;jbellis;we do have a mix -- probably an older version of RAT used the javadoc style? new RAT generates non-javadoc'd license headers.  It would be nice to standardize on the latter, but in the meantime, either is fine.","19/Jul/10 14:45;hudson;Integrated in Cassandra #494 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/494/])
    clean up license headers.  patch by Nate McCall for CASSANDRA-1294
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read_repair_chance is missing from CfDef,CASSANDRA-1180,12466698,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,arya,arya,6/10/2010 23:02,3/12/2019 13:59,3/13/2019 22:24,6/15/2010 14:51,0.7 beta 1,,,,0,,,,,,CfDef is missing read_repair_chance if that is going to remain a configuration option in 0.7.,,,,,,,,,,,,,,,,,,,,15/Jun/10 14:22;gdusbabek;0001-add-read_repair_chance-to-CfDef.patch;https://issues.apache.org/jira/secure/attachment/12447133/0001-add-read_repair_chance-to-CfDef.patch,15/Jun/10 14:22;gdusbabek;0002-avoid-constructor-proliferation-in-CFM.patch;https://issues.apache.org/jira/secure/attachment/12447134/0002-avoid-constructor-proliferation-in-CFM.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,39:44.7,,,no_permission,,,,,,,,,,,,20024,,,Tue Jun 15 14:51:59 UTC 2010,,,,,,0|i0g3hj:,91995,,,,,,,,,,,11/Jun/10 19:39;gdusbabek;Looks like this was taken care of on 2 April as part of CASSANDRA-930,"14/Jun/10 20:56;arya;Hi Gary,

The patch in the bug you've mentioned above contains only changes to Java StorageProxy, CFMetaData, and DatabaseDescriptor. However it does not include changes to Interface/cassandra.thrift as of today's trunc. It reads:

/* describes a column family. */
struct CfDef {
    1: required string table,
    2: required string name,
    3: optional string column_type=""Standard"",
    4: optional string clock_type=""Timestamp"",
    5: optional string comparator_type=""BytesType"",
    6: optional string subcomparator_type="""",
    7: optional string reconciler="""",
    8: optional string comment="""",
    9: optional double row_cache_size=0,
    10: optional bool preload_row_cache=0,
    11: optional double key_cache_size=200000
}

Missing read_repair_chance which makes dynamic creation of CFs using the Thrift API to miss that configuration value. My apologies if my original description was not so clear.",15/Jun/10 13:27;jbellis;don't we need some logic to connect the CfDef field with CFMetaData?,15/Jun/10 14:24;gdusbabek;jbellis: oops.  fixed.  The second patch cleans up the constructors in CFMetadata.,15/Jun/10 14:46;jbellis;+1 latest,15/Jun/10 14:51;gdusbabek;Fixed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
batch_mutate Deletion with column family type mismatch causes RuntimeException,CASSANDRA-1139,12465613,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mdennis,tholzer,tholzer,5/28/2010 2:29,3/12/2019 13:59,3/13/2019 22:24,6/4/2010 2:58,0.6.3,,,,0,batch_mutate,Deletion,,,,"When specifying a super column family name inside a Deletion and a standard column family name in the mutations dictionary, we get a RuntimeException in the server and a TimedOutException on the client:

{noformat}
ERROR 14:22:29,757 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.UnsupportedOperationException: This operation is not supported for Super Columns.
        at org.apache.cassandra.db.SuperColumn.timestamp(SuperColumn.java:137)
        at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:65)
        at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:29)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:87)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:73)
        at org.apache.cassandra.db.RowMutationSerializer.freezeTheMaps(RowMutation.java:337)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:349)
        at org.apache.cassandra.db.RowMutationSerializer.serialize(RowMutation.java:322)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:275)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:200)
        at org.apache.cassandra.service.StorageProxy$3.runMayThrow(StorageProxy.java:310)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR 14:22:29,757 Fatal exception in thread Thread[ROW-MUTATION-STAGE:39,5,main]
{noformat}

{noformat}
Traceback (most recent call last):
  File ""./test.py"", line 15, in <module>
    client.batch_mutate(""Keyspace1"", mutations, ConsistencyLevel.QUORUM)
  File ""cassandra/Cassandra.py"", line 771, in batch_mutate
    self.recv_batch_mutate()
  File ""cassandra/Cassandra.py"", line 798, in recv_batch_mutate
    raise result.te
cassandra.ttypes.TimedOutException: TimedOutException()
{noformat}

To reproduce:

{noformat}
from thrift.transport.TSocket import TSocket
from thrift.protocol.TBinaryProtocol import TBinaryProtocol
from cassandra.Cassandra import Client
from cassandra.ttypes import Deletion, Mutation, ConsistencyLevel

if __name__ == ""__main__"":
    tsocket = TSocket('localhost', 9160)
    tsocket.open()
    tprotocol = TBinaryProtocol(tsocket)
    client = Client(tprotocol)
    deletion = Deletion(1, 'supercolumn', None)
    mutation = Mutation(deletion=deletion)
    mutations = { 'key' : { 'Standard1' : [ mutation ] } }
    client.batch_mutate(""Keyspace1"", mutations, ConsistencyLevel.QUORUM)
{noformat}

",Linux & Python 2.6,,,,,,,,,,,,,,,,,,,01/Jun/10 18:58;mdennis;CASSANDRA-0_6-1139.patch;https://issues.apache.org/jira/secure/attachment/12446043/CASSANDRA-0_6-1139.patch,01/Jun/10 18:40;mdennis;CASSANDRA-1139.patch;https://issues.apache.org/jira/secure/attachment/12446039/CASSANDRA-1139.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,13:53.7,,,no_permission,,,,,,,,,,,,20005,,,Fri Jun 04 02:58:27 UTC 2010,,,,,,0|i0g38f:,91954,,,,,,,,,,,28/May/10 17:13;jbellis;Looks like converting the given test to a system test (in test_bad_calls) and updating ThriftValidation to catch this is fairly straightforward.,01/Jun/10 18:41;mdennis;patch against trunk r950099,01/Jun/10 18:58;mdennis;0.6 patch against cassandra-0.6 r950198,04/Jun/10 02:58;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra doesn't startup on single core boxes.,CASSANDRA-2182,12498911,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,2/17/2011 16:13,3/12/2019 13:59,3/13/2019 22:24,2/17/2011 16:31,0.7.3,,,,0,,,,,,"I happened to run cassandra in a VM and got the following error, caused by the single core:

ERROR 10:47:30,304 Exception encountered during startup.
java.lang.AssertionError: multi-threaded stages must have at least 2 threads
        at org.apache.cassandra.concurrent.StageManager.multiThreadedStage(StageManager.java:60)
        at org.apache.cassandra.concurrent.StageManager.<clinit>(StageManager.java:53)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:303)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:159)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:175)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)",,,3600,3600,,0%,3600,3600,,,,,,,,,,,,17/Feb/11 16:15;tjake;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-2182-rr-pool-needs-at-least-two-threads.txt;https://issues.apache.org/jira/secure/attachment/12471285/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-2182-rr-pool-needs-at-least-two-threads.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,18:21.1,,,no_permission,,,,,,,,,,,,20485,,,Thu Feb 17 17:23:07 UTC 2011,,,,,,0|i0g9tj:,93021,,,,,,,,,,,17/Feb/11 16:18;jbellis;+1 (this was from CASSANDRA-2069 which was committed after 0.7.2),"17/Feb/11 17:23;hudson;Integrated in Cassandra-0.7 #287 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/287/])
    read repair stage requires a minimum of 2 threads
patch by tjake for CASSANDRA-2182
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
lost+found directories cause problems for cassandra,CASSANDRA-1547,12475245,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mdennis,mdennis,mdennis,9/27/2010 21:40,3/12/2019 13:59,3/13/2019 22:24,10/5/2010 14:06,0.6.6,,,,0,,,,,,ext3/4 make lost+found directories at the root of the file system.  if you then point C* at the root of the FS (e.g. you have a mount point of /cassandra_data and/or /cassandra_commitlog) C* thinks lost+found is a keyspace and spews.,,,,,,,,,,,,,,,,,,,,04/Oct/10 16:13;mdennis;1547-cassandra-0.6.txt;https://issues.apache.org/jira/secure/attachment/12456293/1547-cassandra-0.6.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,21:24.4,,,no_permission,,,,,,,,,,,,20193,,,Tue Oct 05 14:06:02 UTC 2010,,,,,,0|i0g5wf:,92386,gdusbabek,gdusbabek,,,,,,,,,28/Sep/10 20:21;jbellis;didn't we used to have code to ignore files starting with . (mostly for the benefit of OS X)?,"28/Sep/10 20:24;gdusbabek;We still do, but I believe it is only inside the KS directories.","04/Oct/10 14:53;gdusbabek;Matt, what error do you see.  To test, I created a lost+found dir inside my data dir and didn't have any problems.","04/Oct/10 16:12;mdennis;I wasn't able to get a copy of the stack trace at the time, but it looks like it's just the commitlog directly and not the data directory too.

{code}
 INFO 10:32:24,958 JNA not found. Native methods will be disabled.
 INFO 10:32:25,152 DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
 INFO 10:32:25,607 Sampling index for /var/lib/cassandra/data/system/LocationInfo-1-Data.db
 INFO 10:32:25,639 Sampling index for /var/lib/cassandra/data/system/LocationInfo-2-Data.db
 INFO 10:32:25,702 Sampling index for /var/lib/cassandra/data/Keyspace1/Standard1-1-Data.db
 INFO 10:32:25,889 Replaying /var/lib/cassandra/commitlog/CommitLog-1286206104383.log, /var/lib/cassandra/commitlog/lost+found
java.io.FileNotFoundException: /var/lib/cassandra/commitlog/lost+found (Is a directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:144)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:186)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:173)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:114)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:214)
 INFO 10:32:25,913 Finished reading /var/lib/cassandra/commitlog/CommitLog-1286206104383.log
ERROR 10:32:25,916 Exception encountered during startup.
java.io.FileNotFoundException: /var/lib/cassandra/commitlog/lost+found (Is a directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:144)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:186)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:173)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:114)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:214)
Exception encountered during startup.
{code}
",04/Oct/10 16:13;mdennis;it looks like 0.7 doesn't have this problem,"05/Oct/10 12:37;gdusbabek;We still want to avoid any hidden files.

EDIT: Duh.  And this patch does avoid them.",05/Oct/10 12:39;gdusbabek;+1,05/Oct/10 14:06;gdusbabek;+1 committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load balancing does not account for the load of the moving node,CASSANDRA-762,12455243,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,2/3/2010 23:20,3/12/2019 13:59,3/13/2019 22:24,2/10/2010 2:15,0.5,,,,1,,,,,,"Given a node A (with load 10 gb) and a node B (with load 20 gb), running the loadbalance command against node A will:
1. Remove node A from the ring
  * Recalculates pending ranges so that node B is responsible for the entire ring
2. Pick the most loaded node
  * node B is still reporting 20 gb load, because that is all it has locally
3. Choose a token that divides the range of the most loaded node in half

Since the token calculation doesn't take into account the load that node B is 'inheriting' from node A, the token will divide node B's load in half and swap the loads. Instead, the token calculation needs to pretend that B has already inherited the 10 gb from node A, for a total of 30 gb. The token that should be chosen falls at 15 gb of the total load, or 5 gb into node B's load.",,,,,,,,,,,,,,,,,,,,08/Feb/10 00:09;stuhood;0001-Wait-BROADCAST_INTERVAL-for-load-information-and-cal.patch;https://issues.apache.org/jira/secure/attachment/12435142/0001-Wait-BROADCAST_INTERVAL-for-load-information-and-cal.patch,08/Feb/10 00:58;stuhood;for-0.5-0001-Wait-BROADCAST_INTERVAL-for-load-information-and-cal.patch;https://issues.apache.org/jira/secure/attachment/12435144/for-0.5-0001-Wait-BROADCAST_INTERVAL-for-load-information-and-cal.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,26:00.5,,,no_permission,,,,,,,,,,,,18550,,,Wed Feb 10 02:15:24 UTC 2010,,,,,,0|i0g0x3:,91579,,,,,,,,,,,"04/Feb/10 19:26;jbellis;As I said in IRC, a loadbalancing node doesn't compute its destination token until after all its data has been transferred off.  (We *should* add a sleep until load is rebroadcast, but that will only affect which node gets picked to pull half the range from.)  So if there is a bug here it is in the code, not the algorithm.

Can you reproduce this?","08/Feb/10 00:09;stuhood;This patch waits for load information after leaving the ring, and lowers the BROADCAST_INTERVAL to .9 of RING_DELAY.","08/Feb/10 00:58;stuhood;Here is a rebased copy of the patch for 0.5. Because 0.5 is still stat'ing files, it doesn't lower the BROADCAST_INTERVAL, although we may want to.",08/Feb/10 16:12;jbellis;did you test on 0.5?,"08/Feb/10 16:49;stuhood;Yes, I tested on trunk and 0.5","08/Feb/10 18:02;jbellis;looking at 0.5 patch:

patch does not build.  (looks like the Gossiper diff made it in by mistake.)

the load wait should probably be for BROADCAST_INTERVAL + RING_DELAY.
","09/Feb/10 06:04;stuhood;Hmm... I just verified that 'for-0.5-0001...' applies cleanly to the cassandra-0.5 branch at git revision 4331781362a16e8ea444fdf478c8c63882af7bb3. There isn't anything related to gossip in that patch.

Also, the patch for trunk applies (with some offsets), and also doesn't contain anything related to gossip.","09/Feb/10 23:37;jbellis;committed to 0.5 w/ my suggested change to sleep duration.

working on committing to trunk but asf svn seems to be dead in the water atm.",10/Feb/10 02:15;jbellis;got the merge to trunk done.  chickened out a bit and set the broadcast period to 60s to be conservative.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
allow overriding existing token owner with a new IP,CASSANDRA-1118,12465236,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,jbellis,jbellis,5/24/2010 2:26,3/12/2019 13:59,3/13/2019 22:24,6/7/2010 20:26,0.6.3,0.7 beta 1,,,0,,,,,,"We'd like to support replacing one node with another at the same IP (e.g. when the data is on Amazon's EBS and can easily be mounted to a new host), as noted in CASSANDRA-872.  But in practice this is reported to not work w/o a cluster restart (can't find the ML thread now ... ?)",,,,,,,,,,,,,,,,,,,,02/Jun/10 13:47;gdusbabek;0001-use-start-time-to-resolve-node-token-reassignment-di.patch;https://issues.apache.org/jira/secure/attachment/12446144/0001-use-start-time-to-resolve-node-token-reassignment-di.patch,05/Jun/10 05:10;gdusbabek;v2-0001-use-generation-time-to-resolve-node-token-reassignme.patch;https://issues.apache.org/jira/secure/attachment/12446400/v2-0001-use-generation-time-to-resolve-node-token-reassignme.patch,07/Jun/10 15:02;gdusbabek;v3-0001-use-generation-time-to-resolve-node-token-reassignme.patch;https://issues.apache.org/jira/secure/attachment/12446489/v3-0001-use-generation-time-to-resolve-node-token-reassignme.patch,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,38:56.0,,,no_permission,,,,,,,,,,,,19997,,,Tue Jun 08 12:45:21 UTC 2010,,,,,,0|i0g33z:,91934,,,,,,,,,,,"28/May/10 19:38;gdusbabek;I'm guessing the description should read ""We'd like to support replacing one node with another at *a different* IP...""","28/May/10 20:00;jbellis;Yes, that's right.  (oops.)","01/Jun/10 22:15;gdusbabek;Basically for my notes so I don't forget this...

This feature is already implemented, but I see two bugs.  First, there is a flaw in the state logic that assumes any time a node is seen for the first time that is the canonical host for a given token.  It is manifest when NodeB(tokenX) comes online intending to replace NodeA(tokenX).  when B sees A for the first time during the normal course of gossip, B incorrectly assumes that A is replacing it, which messes up B's view of the ring.  Second, partitions are not addressed (Node C, returning from a partition will not see that B has replaced A) and will have an incorrect view of the ring.",02/Jun/10 13:47;gdusbabek;Use start time to resolve node token reassignment disagreement. ,02/Jun/10 13:48;gdusbabek;Patch is against trunk.,"04/Jun/10 04:23;jbellis;would it be simpler to use the gossip ""generation"" to detect the more recently started host?",04/Jun/10 04:32;gdusbabek;That would be better.,05/Jun/10 05:10;gdusbabek;Modified patch to use generation time.,06/Jun/10 02:19;jbellis;you can get the generation from endpointstate.heartbeatstate without needing to add an extra applicationstate,07/Jun/10 15:02;gdusbabek;Use heatbeat generation time instead of something different.,07/Jun/10 18:43;jbellis;+1,"08/Jun/10 12:45;hudson;Integrated in Cassandra #459 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/459/])
    use generation time to resolve node token reassignment disagreement. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1118
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift sockets leak in 0.6 hadoop interface,CASSANDRA-1081,12464359,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,johanoskarsson,riffraff,riffraff,5/12/2010 15:59,3/12/2019 13:59,3/13/2019 22:24,5/20/2010 8:20,0.6.2,,,,0,hadoop,leak,socket,thrift,,"Thrift connections appear not to be closed properly in 0.6 in ColumnFamilyRecordReader, which causes a file descriptor leak on the server and may eventually cause jobs to fail.

This appear to be fixed in 0.7 https://issues.apache.org/jira/browse/CASSANDRA-1017 so it may be worth backporting the patch or add a quick fix to close the Tsockets.",,,,,,,,,,,,,,,,,,,,19/May/10 15:16;johanoskarsson;CASSANDRA-1081.patch;https://issues.apache.org/jira/secure/attachment/12444944/CASSANDRA-1081.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,16:09.8,,,no_permission,,,,,,,,,,,,19987,,,Thu May 20 08:20:58 UTC 2010,,,,,,0|i0g2vr:,91897,,,,,,,,,,,19/May/10 15:16;johanoskarsson;Simple patch that makes sure the socket is closed properly.,19/May/10 19:11;jbellis;+1,20/May/10 08:20;johanoskarsson;Committed to 0.6 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"JOIN verb was removed, breaking serialization by ordinal.",CASSANDRA-1642,12477992,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,10/21/2010 14:09,3/12/2019 13:59,3/13/2019 22:24,10/21/2010 14:24,0.7 beta 3,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,21/Oct/10 14:12;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-put-SS.JOIN-verb-back-in-to-keep-ordinals-in-alignment.txt;https://issues.apache.org/jira/secure/attachment/12457755/ASF.LICENSE.NOT.GRANTED--v1-0001-put-SS.JOIN-verb-back-in-to-keep-ordinals-in-alignment.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,17:06.0,,,no_permission,,,,,,,,,,,,20233,,,Thu Oct 21 14:55:51 UTC 2010,,,,,,0|i0g6hj:,92481,,,,,,,,,,,21/Oct/10 14:17;jbellis;+1,"21/Oct/10 14:55;hudson;Integrated in Cassandra #572 (See [https://hudson.apache.org/hudson/job/Cassandra/572/])
    put JOIN verb back. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1642
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system_drop_column_family() and system_rename_column_family() should not take keyspace args,CASSANDRA-1168,12466308,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mdennis,benjaminblack,benjaminblack,6/7/2010 2:25,3/12/2019 13:59,3/13/2019 22:24,6/8/2010 14:17,0.7 beta 1,,,,0,,,,,,"With the addition of set_keyspace(), things that are scoped by keyspace should no longer take keyspace args.  system_add_column_family() is correct in only taking a cf_def.  system_drop_column_family() and system_rename_column_family() should be changed to match.
",,,,,,,,,,,,,,,,,,,,07/Jun/10 22:37;mdennis;0001-trunk-1168.patch;https://issues.apache.org/jira/secure/attachment/12446539/0001-trunk-1168.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,37:15.0,,,no_permission,,,,,,,,,,,,20013,,,Wed Jun 09 13:38:59 UTC 2010,,,,,,0|i0g3ev:,91983,,,,,,,,,,,"07/Jun/10 22:37;mdennis;in addition to system_drop_column_family and system_rename_column_family, the keyspace param was also removed from truncate

Once committed, I'll update the wiki at http://wiki.apache.org/cassandra/API - are there other places that require doc updates? ",08/Jun/10 14:17;gdusbabek;+1 committed!,08/Jun/10 23:42;mdennis;wiki updated,"09/Jun/10 13:38;hudson;Integrated in Cassandra #460 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/460/])
    remove keyspace args from some system calls. patch by Matthew Dennis, reviewed by Gary Dusbabek. CASSANDRA-1168
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception auto-bootstrapping two nodes nodes at the same time,CASSANDRA-1011,12462730,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,erickt,erickt,4/22/2010 5:09,3/12/2019 13:59,3/13/2019 22:24,7/14/2010 12:11,0.7 beta 1,,,,0,,,,,,"I've got a small cluster of 3 machines, and after starting the first node (which is the seed), I brought up the other two nodes at the same time. This exception then gets raised on the seed node. Looks like the seed node is assigning the same token to the subnodes at the same time:

ERROR 21:46:49,417 Error in ThreadPoolExecutor
java.lang.RuntimeException: Bootstrap Token collision between /10.0.0.2 and /10.0.0.3 (token Token(bytes[4c617374204d6967726174696f6e])
	at org.apache.cassandra.locator.TokenMetadata.addBootstrapToken(TokenMetadata.java:130)
	at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:548)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:511)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:705)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:670)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:624)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAck2VerbHandler.doVerb(Gossiper.java:1016)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR 21:46:49,418 Fatal exception in thread Thread[GMFD:1,5,main]
java.lang.RuntimeException: Bootstrap Token collision between /10.0.0.2 and /10.0.0.3 (token Token(bytes[4c617374204d6967726174696f6e])
	at org.apache.cassandra.locator.TokenMetadata.addBootstrapToken(TokenMetadata.java:130)
	at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:548)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:511)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:705)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:670)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:624)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAck2VerbHandler.doVerb(Gossiper.java:1016)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
",,,,,,,,,,,,,,,,,,,,13/Jul/10 18:10;gdusbabek;0001-fail-bootstrap-if-all-nodes-are-bootstrapping.patch;https://issues.apache.org/jira/secure/attachment/12449372/0001-fail-bootstrap-if-all-nodes-are-bootstrapping.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,31:08.4,,,no_permission,,,,,,,,,,,,19955,,,Wed Jul 14 13:56:22 UTC 2010,,,,,,0|i0g2gf:,91828,,,,,,,,,,,"24/Jun/10 18:31;gdusbabek;There is still plenty of window for this to happen, but this should narrow it down some.","24/Jun/10 18:44;jbellis;i don't think the window between when the bootstrapping machine chooses a machine to ask for a token, and when it actually does so (sub ms under any scenario i can think of) is large enough to add the extra check in getBalancedToken.  either way the main danger is that some node has already chosen the same token but we haven't heard about it yet over gossip.

it would be worth checking though in getBootstrapSource that if sorted node zero has a bootstrap target already (i.e., _every_ node already has one), then we should fail the bootstrap (and tell the user to specify an initialtoken manually, or wait for one of the current bootstraps to finish).","12/Jul/10 21:12;gdusbabek;> it would be worth checking though in getBootstrapSource that if sorted node zero has a bootstrap target already (i.e., every node already has one), then we should fail the bootstrap (and tell the user to specify an initialtoken manually, or wait for one of the current bootstraps to finish).

I think I'm missing something.  We don't keep track of whether or not node zero has a bootstrap target, do we?  Nothing I can see in TMD indicates this.","12/Jul/10 22:25;jbellis;metadata.pendingRangeChanges = bootstrap targets, more or less",14/Jul/10 03:45;jbellis;+1,"14/Jul/10 13:56;hudson;Integrated in Cassandra #491 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/491/])
    fail bootstrap if all nodes are bootstrapping. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1011
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"renaming a keyspace, then trying to use original name again makes errorations",CASSANDRA-1548,12475250,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,thepaul,thepaul,9/27/2010 22:41,3/12/2019 13:59,3/13/2019 22:24,9/28/2010 14:58,0.7 beta 2,,,,0,,,,,,"My test case does the following:

* Create a keyspace with at least one CF in it.
* Rename that keyspace
* Create a new keyspace with the same original name, containing a CF with the same name as earlier.

The second keyspace creation receives an error (although the keyspace does get created):

{{javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=keyspacename,columnfamily=cfname}}

After that point, trying to do almost anything with the new keyspace will generate the same error- even trying to drop it. This persists until cassandra itself is restarted.

One supposes that some JMX thing is lacking reregistration upon keyspace rename.","encountered on Debian squeeze, with cassandra from HEAD (r1001931).  effects can be seen with both Telephus and Pycassa as clients; probably any.",,,,,,,,,,,,,,,,,,,28/Sep/10 07:08;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-ensure-that-a-table-unloads-CFS-instances-when-they-ar.txt;https://issues.apache.org/jira/secure/attachment/12455809/ASF.LICENSE.NOT.GRANTED--v1-0001-ensure-that-a-table-unloads-CFS-instances-when-they-ar.txt,27/Sep/10 22:45;thepaul;test_case_1548.py;https://issues.apache.org/jira/secure/attachment/12455770/test_case_1548.py,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,48:22.7,,,no_permission,,,,,,,,,,,,20194,,,Wed Sep 29 12:47:31 UTC 2010,,,,,,0|i0g5wn:,92387,,,,,,,,,,,"27/Sep/10 22:45;thepaul;a test case which tickles this bug, using pycassa",28/Sep/10 13:48;jbellis;+1,28/Sep/10 14:58;gdusbabek;committed.,"29/Sep/10 12:47;hudson;Integrated in Cassandra #550 (See [https://hudson.apache.org/hudson/job/Cassandra/550/])
    ensure that a table unloads CFS instances when they are cleared. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1548
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe_keyspace() should include strategy_options,CASSANDRA-1560,12475456,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,thobbs,thobbs,9/29/2010 23:19,3/12/2019 13:59,3/13/2019 22:24,9/30/2010 1:17,0.7 beta 3,,,,0,,,,,,describe_keyspace() does not include the strategy_options in the returned KsDef,,,,,,,,,,,,,,,,,,,,30/Sep/10 00:31;jhermes;1560.txt;https://issues.apache.org/jira/secure/attachment/12455955/1560.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,31:17.1,,,no_permission,,,,,,,,,,,,633,,,Thu Sep 30 12:48:05 UTC 2010,,,,,,0|i0g5zb:,92399,jbellis,jbellis,,,,,,,,,"30/Sep/10 00:31;jhermes;If more things get added to KSMetaData (which shouldn't happen), then it would be worthwhile to abstract this to KSMetaData.convertToThrift().",30/Sep/10 01:17;jbellis;committed,"30/Sep/10 12:48;hudson;Integrated in Cassandra #551 (See [https://hudson.apache.org/hudson/job/Cassandra/551/])
    add strategy options to describe_keyspace output.  patch by jhermes; reviewed by jbellis for CASSANDRA-1560
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Login information stored in threads may be reused.,CASSANDRA-1057,12463842,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,kenmacd,kenmacd,kenmacd,5/6/2010 2:25,3/12/2019 13:59,3/13/2019 22:24,6/6/2010 4:53,0.6.3,,,,0,thrift,,,,,"CassandraServer stores the login information in a ThreadLocal<AccessLevel>.

CassandraDaemon starts the server with 64 threads. When the first 64 clients connect they should get their own thread, but after that threads will be reused.

In a quick test I created a Server with 5 threads, and a ThreadLocal<Integer>, and the value is seen by new clients connecting.

Thrift doesn't destroy the threads when a client disconnects. Maybe an option in Thrift would make more sense to make this method usable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,37:48.8,,,no_permission,,,,,,,,,,,,19978,,,Sun Jun 06 04:53:08 UTC 2010,,,,,,0|i0g2qf:,91873,,,,,,,,,,,"07/May/10 03:52;kenmacd;One solution would be to create your own TThreadPoolServer so you can replace the ExecutorService with one that overrides the afterExecute() method and clears out the ThreadLocal data.

I'm trying to figure out the best way to accomplish this, but TThreadPoolServer is all private, so the only way to replace the ExecutorService is to copy/paste it, or use reflection.

If you create the ExecutorService in the CassandraDaemon and pass it into the new TThreadPoolServer you'll have access to the CassandraServer to call the logout() method on it (once you write the simple logout to clean the ThreadLocal data).

If I find a good solution I'll post a patch. How against using reflection are you?",07/May/10 04:37;jbellis;I don't see any reason why Thrift wouldn't accept a patch adding a TTPS constructor that takes an ES as an argument.  Want to submit one and ask Bryan for review?  (He's the main java committer.),"28/May/10 17:06;jbellis;Any progress on the Thrift front, Kenny?","28/May/10 17:26;kenmacd;Not really. I sent a patch to the thrift-user list, but didn't receive a reply on it. I also cloned the thrift repo on github and was working on this patch:

http://github.com/KenMacD/thrift/commit/0ac9a2dc17326411212a02b201bc7ef969cea9c7

I haven't had a lot of time to look at it lately though. I was going to look at if it made sense to add builder to the other servers. 
Thrift 0.3 is about to come out, so maybe I'll see if I can get them to include a fix before 0.4. 

As a workaround in another project I work on I copied the TThreadPoolServer and was using my own version. You can see my workaround @:

http://github.com/ted/ted/commit/f4784c7adee2ff02bab51f529b190a4b16066e7c

This TThreadPoolServer takes an ExecutorService in which I override the afterExecute() to call my logout() on the thread. Seems to work well.
",28/May/10 18:10;jbellis;It would be nice to have a workaround in Cassandra 0.6.3.  Can you submit a patch with you TTPS modification?,"28/May/10 20:06;kenmacd;Sure, I should be able to attach a patch tomorrow morning.","29/May/10 13:16;kenmacd;Patch is available on my GitHub:

http://github.com/KenMacD/cassandra/commit/83e6917b23ca31f354c0baf4aa3dc014268626b2

I only tested it by running on the unit tests, so you'll probably want to give it a more real test.","04/Jun/10 02:59;jbellis;I confess: I couldn't figure out how to get a patch from that link, to apply to apache svn.","04/Jun/10 13:02;kenmacd;:)

Please try this link instead:

http://github.com/KenMacD/cassandra/commit/83e6917b23ca31f354c0baf4aa3dc014268626b2.diff

If that still doesn't work let me know and I'll send you a copy in email.","06/Jun/10 04:53;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception while recovering commitlog when debug logging enabled,CASSANDRA-1274,12469178,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mdennis,johanoskarsson,johanoskarsson,7/13/2010 17:10,3/12/2019 13:59,3/13/2019 22:24,7/27/2010 12:46,0.6.4,,,,0,,,,,,"On a cluster with debug logging enabled the commit log fails to recover on start. An UTF8 exception is thrown when trying to toString a column from the system column family LocationInfo. That CF is using UTF8Type but I suspect the column name in this specific case is a byte representation of an ip address, and as such not a valid UTF8 string. That column is most perhaps created in SystemTable line 74.

Full exception stack trace:
ERROR [main] 2010-07-13 11:03:17,050 AbstractCassandraDaemon.java (line 107) Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: invalid UTF8 bytes [10, -48, 40, -124]
        at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:43)
        at org.apache.cassandra.db.Column.getString(Column.java:200)
        at org.apache.cassandra.db.marshal.AbstractType.getColumnsString(AbstractType.java:85)
        at org.apache.cassandra.db.ColumnFamily.toString(ColumnFamily.java:393)
        at org.apache.commons.lang.ObjectUtils.toString(ObjectUtils.java:241)
        at org.apache.commons.lang.StringUtils.join(StringUtils.java:3073)
        at org.apache.commons.lang.StringUtils.join(StringUtils.java:3133)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:250)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:171)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:120)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:221)
",,,,,,,,,,,,,,,,,,,,27/Jul/10 05:38;mdennis;1274-0.6.patch;https://issues.apache.org/jira/secure/attachment/12450563/1274-0.6.patch,27/Jul/10 05:38;mdennis;1274-trunk.patch;https://issues.apache.org/jira/secure/attachment/12450564/1274-trunk.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,24:15.5,,,no_permission,,,,,,,,,,,,20055,,,Tue Aug 03 20:46:20 UTC 2010,,,,,,0|i0g427:,92088,,,,,,,,,,,13/Jul/10 17:24;jbellis;in 0.6?,"13/Jul/10 17:41;johanoskarsson;This was in trunk from yesterday. It's a modified version, but none of that code should have been touched.","24/Jul/10 01:38;brandon.williams;Jon Hermes presented a theory that you have to be using an IP that has at least one quad over 128, which won't be met by a localhost cluster.  I can reliably reproduce on a real cluster, even in 0.6.","27/Jul/10 05:35;mdennis;Jon Hermes is correct, it has to be invalid UTF8 bytes to trigger this","27/Jul/10 05:38;mdennis;one line patches to change it to BytesType

Once the value is in the log in .6 the node will need to be started witout DEBUG logging to clear out the hints after which the log level can be turned back to DEBUG.

",27/Jul/10 12:46;jbellis;committed,"03/Aug/10 16:21;brandon.williams;Just clearing the CL won't help, because the HH cf already exists, so it's not recreated.  Then the problem continues to occur.","03/Aug/10 16:38;jbellis;Isn't the HH CF ""created"" on server startup in DD?  Or did that change in 0.7 too?",03/Aug/10 20:46;mdennis;The meta data is created statically in CfMetaData,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinaryMemtable interface silently dropping data.,CASSANDRA-1093,12464558,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,tjungen,tjungen,5/14/2010 20:13,3/12/2019 13:59,3/13/2019 22:24,7/26/2010 14:47,0.6.4,,,,0,,,,,,"I've been attempting to use the Binary Memtable (BMT) interface to load a large number of rows. During my testing, I discovered that on larger loads (~1 million rows), occasionally some of the data never appears in the database. This happens in a non-deterministic manner, as sometimes all the data loads fine, and other times a significant chunk goes missing. No errors are ever logged to indicate a problem. I'm attaching some sample code that approximates my application's usage of Cassandra and explains this bug in more detail.","Linux Centos5, Fedora Core 4. Java HotSpot Server 1.6.0_14. See readme for more details.",,,,,,,,,,,,,,,,,,,14/Jul/10 13:42;jbellis;1093.txt;https://issues.apache.org/jira/secure/attachment/12449452/1093.txt,14/May/10 20:14;tjungen;cassandra_bmt_test.tar.gz;https://issues.apache.org/jira/secure/attachment/12444527/cassandra_bmt_test.tar.gz,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,56:50.5,,,no_permission,,,,,,,,,,,,19989,,,Mon Jul 26 14:47:20 UTC 2010,,,,,,0|i0g2yf:,91909,,,,,,,,,,,14/May/10 20:14;tjungen;Sample code and instructions for how to run. See readme.txt in the archive.,"15/May/10 23:56;lenn0x;I've never seen this happen and I've done many imports. At the very end of the import, are you calling nodetool flush <Keyspace> ? ",16/May/10 05:10;lenn0x;Tonight I imported 1M rows and verified all rows existed.,"16/May/10 05:16;tjungen;Yes, I'm flushing each node after the import. I've also tried flushing the system keyspace (no effect). As noted in the readme, I would not be surprised if this problem is unique to my hardware/software configuration and isn't an inherent problem with Cassandra's BMT interface.

For what it's worth, I've hacked together a ""workaround"" for this problem by writing SSTables directly (using o.a.c.io.SSTableWriter), copying the generated files to appropriate directories on the nodes, and then restarting the nodes. This solution is bound to result in other bugs, but for now I've verified that there is no lost data with this method.","28/May/10 16:33;jbellis;can you reproduce using Toby's code, Brandon?","28/May/10 18:46;brandon.williams;I can't get it past the generate step without an OOM:


cassandra_bmt_test# java -jar -Xmx4096m build/cassandra-bmt-test.jar generate foo 1000000
Generating data...
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
        at java.util.ArrayList.<init>(ArrayList.java:132)
        at java.util.ArrayList.<init>(ArrayList.java:139)
        at CassandraBMTTest.generateData(Unknown Source)
        at CassandraBMTTest.main(Unknown Source)

I'm going to try with < 1M and see if that works.","28/May/10 19:03;tjungen;I've been able to observe the error with a generate parameter of 25,000. Note that the generate step creates the entire randomized data set in memory before writing it to disk, so this test is limited by memory. With a parameter of 25,000 I ran fine with 512MB of heap space, at 100,000 I'd expect you to need around 2GB of heap space. 

The parameter for the generate step corresponds to a ""document"", and each document results in roughly 100 rows.","28/May/10 20:17;brandon.williams;I used 100K ""documents"":

Processed 4547169 values.
Done.
        Missing documents: 0
        Mismatched documents: 0
        Missing index entries: 0
        Wrong-sized index entries: 0
        Mismatched index entries: 0","28/May/10 20:24;tjungen;Looks like everything passed. You may want to re-run from start to finish one or two more times (my error didn't occur consistently), but if it still passes at that point then close this issue as CannotReproduce and I'll attribute the problem to my hardware setup. As mentioned I've found somewhat of a workaround. I'll gladly donate my test code as a possible unit test for BMT if needed. :)","28/May/10 21:38;brandon.williams;I did another 100K run and it passed:


Processed 4547169 values.
Done.
        Missing documents: 0
        Mismatched documents: 0
        Missing index entries: 0
        Wrong-sized index entries: 0
        Mismatched index entries: 0

Is it possible you aren't waiting long enough for the flush to complete? (nodetool doesn't block on the flush command, you have to watch the system.log)","28/May/10 22:55;tjungen;Yep, I'm waiting until I see the flush message in the log. It reads something like ""BinaryMemtable@7a82b flushed to disk"".

One thing I'm thinking may be causing problems is my nodes being out of sync time-wise. I'll have to verify their clocks, but is it possible that if the clocks differ significantly that values get lost?","04/Jun/10 18:42;brandon.williams;No, that shouldn't happen since the timestamps for columns are supplied by the client.","13/Jul/10 16:33;jbellis;BMT is a very fire-and-forget api, so any failure condition will cause messages to be dropped with no way of knowing.

Probably the most likely one is, under heavy load (network and/or cpu) it's reasonably common for one node in the cluster to be marked ""down"" incorrectly by other nodes in the cluster.  This causes any messages on the MessagingService queue to that node to be dropped summarily, and the pool connection to be re-attempted when the failure detector believes it is ""up"" again.  (See OutboundTcpConnectionPool.reset)","13/Jul/10 20:19;tjungen;Thanks for the insight Jonathan. That was my intuition as well, and I observed my cluster periodically marking nodes as down for a second or two. I figured it was random network hiccups, since our network hardware is rather old. It would make sense that these periodic interruptions caused the BMT to lose data.

While looking through the code, I did try to see if I could use BMT with the blocking MessagingService API (in the way the Thrift API works unless ConsistencyLevel.ZERO is specified), but it looks like BMT is hardcoded to be asynchronous. It might be nice for that option to be there, but since this issue appears to only affect me (and I no longer need to use BMT for my purposes), it's a super-low priority suggestion.","13/Jul/10 20:35;brandon.williams;If it is a node is being errantly marked down, in 0.6.3 or later you can try increasing the PhiConvictThreshold configuration directive and see if that helps.  EC2 users are setting it to 10 or 11, 8 is the default.","13/Jul/10 22:10;jbellis;As it happens, Riptano has a client that is running into this too, so I'll take a stab at fixing it. :)","14/Jul/10 13:42;jbellis;patch to add response from BinaryVerbHandler, and updates bmt_example to use sendRR","24/Jul/10 02:18;tjungen;Applied and tested the patch, appears to solve the problem. Haven't run multiple tests yet to make sure, but looks good so far. Obviously, this slows down the write, but that's an acceptable loss. It's likely still faster and more efficient than using the thrift API.

I'll be out of my office for the next three weeks, but I'll try to test more when I get back. Feel free to mark as resolved in the mean time.","26/Jul/10 14:47;jbellis;committed, with additional note that wait-for-acks can reduce throughput",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Attempting to mutate a non-existant CF does not propagate an error to the client,CASSANDRA-1036,12463365,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,brandon.williams,brandon.williams,4/29/2010 19:33,3/12/2019 13:59,3/13/2019 22:24,5/7/2010 13:58,0.7 beta 1,,,,0,,,,,,"An error gets logged on the server:

ERROR 15:23:21,035 Attempting to mutate non-existant column family Standard1

But nothing is raised on the client side, so it appears the request succeeded.",,,,,,,,,,,,,,,,,,,,07/May/10 12:35;slebresne;1036-Validate-CF-for-deletion-in-mutation-map.patch;https://issues.apache.org/jira/secure/attachment/12443958/1036-Validate-CF-for-deletion-in-mutation-map.patch,07/May/10 12:47;slebresne;1036-v2-Validate-CF-for-deletion-in-mutation-map.patch;https://issues.apache.org/jira/secure/attachment/12443960/1036-v2-Validate-CF-for-deletion-in-mutation-map.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,35:51.2,,,no_permission,,,,,,,,,,,,19967,,,Fri May 07 13:58:04 UTC 2010,,,,,,0|i0g2lz:,91853,,,,,,,,,,,07/May/10 12:35;slebresne;Attached patch should correct the problem.,07/May/10 12:47;slebresne;Oups. Figured that I could add the validation to Avro too. Patch v2 attached.,07/May/10 12:51;gdusbabek;Should we include a system test to verify?,"07/May/10 13:00;slebresne;You mean, for the avro part ? I'll have to admit that I'm not up to date on the avro part, 
but the avro system test for batch_mutate has a 
        # FIXME: still need to apply a mutation that deletes
so I was figuring that maybe that wasn't working yet. But I can have a look.","07/May/10 13:14;gdusbabek;No.  I'm referring to test/system/test_thrift_server.py.  There are a few examples in there where a call is made and we expect an exception.  We should have a call like that where we attempt to mutate a non-existent CF and check to make sure an exception is received by the client.

See the usages of ""_expect_exception"".",07/May/10 13:18;slebresne;The patch includes system tests for thrift. Or are those included not the ones you had in mind ?,07/May/10 13:30;gdusbabek;My apologies.  For some reason I was expecting a git-style patch with a  listing of the modified files at the top.  I see the system test now.,07/May/10 13:56;gdusbabek;+1.,07/May/10 13:58;gdusbabek;Thanks for the patch!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli doesn't work with system allowed column family names,CASSANDRA-1005,12462487,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,kingjamm,kingjamm,4/19/2010 20:34,3/12/2019 13:59,3/13/2019 22:24,5/24/2010 16:04,0.6.2,,,,0,cassandra-cli,,,,,"Given the following definitions for columns:

<Keyspaces>

<Keyspace Name=""NGram"">

<KeysCachedFraction>0.01</KeysCachedFraction>

<ColumnFamily CompareWith=""UTF8Type"" Name=""1GramR""/>

<ColumnFamily CompareWith=""UTF8Type"" Name=""1GramL""/>

</Keyspaces>

The appropriate keyspaces are created an persisteted on startup. When executing a query or a set operation in the cassandra-cli, you end up with the following error:

******************************************************

cassandra> get NGram.1GramR['hte']

line 1:10 extraneous input '1' expecting Identifier

No such column family: GramR

******************************************************


Following the syntax of the grammer we can see the following:

setStmt
: K_SET columnFamilyExpr '=' value -> ^(NODE_THRIFT_SET columnFamilyExpr value)
;

...

columnFamilyExpr
: table DOT columnFamily '[' rowKey ']'
( '[' a+=columnOrSuperColumn ']'
('[' a+=columnOrSuperColumn ']')?
)?
-> ^(NODE_COLUMN_ACCESS table columnFamily rowKey ($a+)?)
;
...

// syntactic Elements
Identifier
: Letter ( Alnum | '_' )*
;

There is a mismatch on what is appropriate values for this in the system. So either the restriction needs to be lifted in the cli, or the system must have a way of honoring the names.",Windows XP 32 bit,,,,,,,,,,,,,,,,,,,21/May/10 21:50;urandom;0001-support-all-legal-keyspace-and-column-names-in-cli.patch;https://issues.apache.org/jira/secure/attachment/12445206/0001-support-all-legal-keyspace-and-column-names-in-cli.patch,21/May/10 21:50;urandom;cli.sh;https://issues.apache.org/jira/secure/attachment/12445207/cli.sh,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,21:33.2,,,no_permission,,,,,,,,,,,,19950,,,Mon May 24 16:04:41 UTC 2010,,,,,,0|i0g2f3:,91822,,,,,,,,,,,19/Apr/10 20:36;kingjamm;changed from major to minor as we have a current workaround.,"06/May/10 03:21;jbellis;is this another ""abandon all hope"" antlr thing?","21/May/10 21:50;urandom;The attached patch seems to do it.

Also attached is the cassandra-cli script I used to test. To run it, first create keyspaces named {{1Space}} and {{0000}}, with column families named {{2Family}} and {{1111}} respectively, then run:

{noformat}
$ cassandra-cli < cli.sh
{noformat}",24/May/10 15:57;jbellis;+1,24/May/10 16:04;urandom;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion failure loadbalance-ing a ByteOrderedPartitioner cluster,CASSANDRA-1008,12462713,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,erickt,erickt,erickt,4/21/2010 22:42,3/12/2019 13:59,3/13/2019 22:24,5/24/2010 17:51,0.7 beta 1,,,,1,,,,,,"This seems to be a similar problem to CASSANDRA-1006:

ERROR [GMFD:4] 2010-04-21 15:37:56,942 CassandraDaemon.java (line 77) Fatal exception in thread Thread[GMFD:4,5,main]
java.lang.NumberFormatException: For input string: ""To""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:481)
	at org.apache.cassandra.utils.FBUtilities.hexToBytes(FBUtilities.java:361)
	at org.apache.cassandra.dht.AbstractByteOrderedPartitioner$1.fromString(AbstractByteOrderedPartitioner.java:133)
	at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:622)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:517)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:705)
	at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:695)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:624)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAckVerbHandler.doVerb(Gossiper.java:966)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:41)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
",,,,,,,,,,,,,,,,,,,,21/Apr/10 23:07;erickt;0001-Use-the-token-factory-to-convert-tokens-to-strings.-.patch;https://issues.apache.org/jira/secure/attachment/12442489/0001-Use-the-token-factory-to-convert-tokens-to-strings.-.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,42:30.7,,,no_permission,,,,,,,,,,,,19952,,,Tue May 25 12:56:37 UTC 2010,,,,,,0|i0g2fr:,91825,,,,,,,,,,,21/Apr/10 23:07;erickt;Potential patch to fix this bug.,"24/May/10 16:42;stuhood;+1
Thanks for catching this Erick... sorry it fell through the cracks for such a long time there.","24/May/10 17:51;jbellis;committed, thanks","25/May/10 12:56;hudson;Integrated in Cassandra #445 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/445/])
    convert byte tokens to strings correctly.  patch by Erick Tryzelaar; reviewed by Stu Hood for CASSANDRA-1008
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion error on quorum write,CASSANDRA-593,12442183,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dispalt,dispalt,dispalt,12/2/2009 5:33,3/12/2019 13:59,3/13/2019 22:24,12/2/2009 22:41,0.5,,,,0,,,,,,"In the middle of a quorum write I encountered this error.


2009-12-02_04:59:59.79647 java.lang.AssertionError: invalid response count 3
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.WriteResponseHandler.<init>(WriteResponseHandler.java:47)
2009-12-02_04:59:59.79647       at org.apache.cassandra.locator.AbstractReplicationStrategy.getWriteResponseHandler(AbstractReplicationStrategy.java:61)
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.StorageService.getWriteResponseHandler(StorageService.java:1081)
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:193)
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.CassandraServer.batch_insert(CassandraServer.java:445)
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.Cassandra$Processor$batch_insert.process(Cassandra.java:983)
2009-12-02_04:59:59.79647       at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:712)
2009-12-02_04:59:59.79647       at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
2009-12-02_04:59:59.79647       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2009-12-02_04:59:59.79647       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2009-12-02_04:59:59.79647       at java.lang.Thread.run(Thread.java:636)",using trunk (r885572) while in the middle of a loadbalance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,41:46.0,,,no_permission,,,,,,,,,,,,19770,,,Sat Dec 05 12:34:26 UTC 2009,,,,,,0|i0fzvz:,91412,,,,,,,,,,,02/Dec/09 22:41;jbellis;one-line fix in r886332,"05/Dec/09 12:34;hudson;Integrated in Cassandra #278 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/278/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
possible NPE in StorageService,CASSANDRA-828,12457237,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,riffraff,riffraff,riffraff,2/23/2010 23:06,3/12/2019 13:59,3/13/2019 22:24,2/24/2010 22:28,0.6,,,,0,,,,,,"the code
 {{{

     if (endPointThatLeft.equals(FBUtilities.getLocalAddress()))
            {
                logger_.info(""Received removeToken gossip about myself. Is this node a replacement for a removed one?"");
                return;
            }
            if (logger_.isDebugEnabled())
                logger_.debug(""Token "" + token + "" removed manually (endpoint was "" + ((endPointThatLeft == null) ? ""unknown"" : endPointThatLeft) + "")"");
            if (endPointThatLeft != null)
            {
                removeEndPointLocally(endPointThatLeft);
            }
}}}

appears wrong: if it is possible for the leaving endpoint to be unknown then the first ""if"" has a possible null dereference, which can be eliminated by swapping the arguments or reordering the code.

As a side note, I believe FBUtilities.getLocalAddress should probably be synchronized (or localInetAddress made volatile) per the usual ""the java MM does not guarantee any change will ever be visible""  mantra which may or may not be considered relevant :)",,,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,16:50.0,,,no_permission,,,,,,,,,,,,19881,,,Wed Feb 24 22:34:25 UTC 2010,,,,,,0|i0g1br:,91645,,,,,,,,,,,"23/Feb/10 23:16;jbellis;> if it is possible for the leaving endpoint to be null

it's not

> getLocalAddress should probably be synchronized, or localInetAddress made volatile

made localInetAddress volatile in r915580, thanks.","23/Feb/10 23:23;riffraff;wow, this was fast, thanks.
 
But then aren't the following two checks for nullness unnecessary?","23/Feb/10 23:27;jbellis;you're right, something is fishy here.  reopened.",24/Feb/10 22:28;jbellis;the != null check is definitely redundant.  removed in r916006.,"24/Feb/10 22:34;jbellis;my mistake: endpoint, the argument to the method, can't be null, but endpointThatLeft can be.  Made suggested fix of inverting if condition in r916008.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When you omit keyspace in the ""show keyspace"" command in the CLI your connection gets terminated",CASSANDRA-551,12440652,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,hsbis,hsbis,hsbis,11/13/2009 19:18,3/12/2019 13:59,3/13/2019 22:24,11/13/2009 21:16,0.5,,,,0,,,,,,"$ bin/cassandra-cli --host localhost --port 9160

cassandra> describe keyspace          
line 0:-1 mismatched input '<EOF>' expecting Identifier
Exception Required field 'keyspace' was not present! Struct: describe_keyspace_args(keyspace:null)
org.apache.thrift.protocol.TProtocolException: Required field 'keyspace' was not present! Struct: describe_keyspace_args(keyspace:null)
	at org.apache.cassandra.service.Cassandra$describe_keyspace_args.validate(Cassandra.java:10723)
	at org.apache.cassandra.service.Cassandra$describe_keyspace_args.write(Cassandra.java:10692)
	at org.apache.cassandra.service.Cassandra$Client.send_describe_keyspace(Cassandra.java:558)
	at org.apache.cassandra.service.Cassandra$Client.describe_keyspace(Cassandra.java:549)
	at org.apache.cassandra.cli.CliClient.executeDescribeTable(CliClient.java:259)
	at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:75)
	at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:108)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:148)
cassandra> describe keyspace Keyspace1
Exception Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes.
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes.
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:314)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:262)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:192)
	at org.apache.cassandra.service.Cassandra$Client.recv_describe_keyspace(Cassandra.java:565)
	at org.apache.cassandra.service.Cassandra$Client.describe_keyspace(Cassandra.java:550)
	at org.apache.cassandra.cli.CliClient.executeDescribeTable(CliClient.java:259)
	at org.apache.cassandra.cli.CliClient.executeCLIStmt(CliClient.java:75)
	at org.apache.cassandra.cli.CliMain.processCLIStmt(CliMain.java:108)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:148)
","Ubuntu, java 6",,600,600,,0%,600,600,,,,,,,,,,,,13/Nov/09 20:14;hsbis;CASSANDRA-551.patch;https://issues.apache.org/jira/secure/attachment/12424889/CASSANDRA-551.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,16:15.6,,,no_permission,,,,,,,,,,,,19751,,,Sat Nov 14 12:33:58 UTC 2009,,,,,,0|i0fzmn:,91370,,,,,,,,,,,13/Nov/09 20:14;hsbis;Now the users gets an error message and the connection isn't terminated.,13/Nov/09 21:16;urandom;committed; thanks,"14/Nov/09 12:33;hudson;Integrated in Cassandra #258 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/258/])
    gracefully handle missing keyspace argument (cli)

Patch by Hafsteinn Baldvinsson; reviewed by eevans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avoid setting up completion handler for no-op stream in non-bootstrap mode,CASSANDRA-750,12447060,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,1/29/2010 23:05,3/12/2019 13:59,3/13/2019 22:24,1/29/2010 23:47,0.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,29/Jan/10 23:06;jbellis;750.txt;https://issues.apache.org/jira/secure/attachment/12431835/750.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,08:42.5,,,no_permission,,,,,,,,,,,,19850,,,Fri Jan 29 23:47:33 UTC 2010,,,,,,0|i0g0uf:,91567,,,,,,,,,,,29/Jan/10 23:08;gdusbabek;+1,29/Jan/10 23:47;jbellis;committed to 0.5 and trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Server fails to join cluster if IPv6 only,CASSANDRA-969,12461722,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,cody.lerum,cody.lerum,4/10/2010 18:16,3/12/2019 13:59,3/13/2019 22:24,4/14/2010 13:37,0.6.1,,,,0,,,,,,"When configuring Cassandra for IPv6 connectivity on the server to server side the addition of a second node causes the both servers to loop on ArrayIndexOutOfBoundsExection for 5 minutes

The first server has 

Caused by: java.lang.ArrayIndexOutOfBoundsException: 65536
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:155)

While the second has

Caused by: java.lang.ArrayIndexOutOfBoundsException: 131072
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:155)

the index is double.

These servers work find in a cluster together if they are configured IPv4

server1 in the output is 2607:f3d0:0:2::16
server2 is 2607:f3d0:0:1::f","Ubuntu 9.10 x64
java: Java(TM) SE Runtime Environment (build 1.6.0_15-b03)
cassandra 0.6.0-rc1",,,,,,,,,,,,,,,,,,,10/Apr/10 18:25;cody.lerum;ASF.LICENSE.NOT.GRANTED--cassandra-v6-2-filtered.pcap;https://issues.apache.org/jira/secure/attachment/12441347/ASF.LICENSE.NOT.GRANTED--cassandra-v6-2-filtered.pcap,10/Apr/10 18:18;cody.lerum;ASF.LICENSE.NOT.GRANTED--cassandra-v6.pcap;https://issues.apache.org/jira/secure/attachment/12441346/ASF.LICENSE.NOT.GRANTED--cassandra-v6.pcap,12/Apr/10 13:01;gdusbabek;ASF.LICENSE.NOT.GRANTED--encode_decode_ipv6_addresses_safely.txt;https://issues.apache.org/jira/secure/attachment/12441480/ASF.LICENSE.NOT.GRANTED--encode_decode_ipv6_addresses_safely.txt,10/Apr/10 18:16;cody.lerum;ASF.LICENSE.NOT.GRANTED--server1.log;https://issues.apache.org/jira/secure/attachment/12441344/ASF.LICENSE.NOT.GRANTED--server1.log,10/Apr/10 18:16;cody.lerum;ASF.LICENSE.NOT.GRANTED--server2.log;https://issues.apache.org/jira/secure/attachment/12441345/ASF.LICENSE.NOT.GRANTED--server2.log,,,,,,,,,5,,,,,,,,,,,,,,,,,,,59:09.4,,,no_permission,,,,,,,,,,,,19942,,,Wed Apr 14 04:40:26 UTC 2010,,,,,,0|i0g273:,91786,,,,,,,,,,,10/Apr/10 18:16;cody.lerum;system.log files for both servers,10/Apr/10 18:18;cody.lerum;wireshark capture of the networking traffic on server1,"10/Apr/10 18:25;cody.lerum;this file shows the initial server startup networking traffic as well.

second server starts up at about 6 seconds in ","12/Apr/10 12:59;gdusbabek;Looks like CompactEndPointSerializationHelper is assuming a 4 byte address during deserialization, but actually writes a full 16 byte IPv6 address during serialization.",12/Apr/10 13:01;gdusbabek;This patch should address your specific problem.  What we really need to do is audit the code for this problem.  There are quite a few places where we send addresses over the wire.,12/Apr/10 13:10;jbellis;+1,12/Apr/10 13:30;cody.lerum;I'll try and recompile later today and test.,"12/Apr/10 21:43;cody.lerum;I checked out the .6 branch and built off that.

Still getting errors

ERROR [MESSAGE-DESERIALIZER-POOL:1] 2010-04-12 15:37:57,020 DebuggableThreadPoolExecutor.java (line 94) Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException: 65536
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 65536
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:155)
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:113)
        at org.apache.cassandra.net.MessageSerializer.deserialize(Message.java:136)
        at org.apache.cassandra.net.MessageDeserializationTask.run(MessageDeserializationTask.java:45)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
","12/Apr/10 21:45;cody.lerum;actually I take that back

-rw-r--r-- 1 root root 1275022 2010-03-28 09:25 apache-cassandra-0.6.0-rc1.jar

The lib in my new build is still old.","12/Apr/10 22:19;cody.lerum;ok got it built..


The joining server shows

ERROR 16:17:52,106 Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.EOFException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.io.EOFException
        at org.apache.cassandra.net.MessageDeserializationTask.run(MessageDeserializationTask.java:49)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:180)
        at java.io.DataInputStream.readUTF(DataInputStream.java:592)
        at java.io.DataInputStream.readUTF(DataInputStream.java:547)
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:140)
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:113)
        at org.apache.cassandra.net.MessageSerializer.deserialize(Message.java:136)
        at org.apache.cassandra.net.MessageDeserializationTask.run(MessageDeserializationTask.java:45)
        ... 6 more

and existing

ERROR 16:17:42,991 Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
        at org.apache.cassandra.net.MessageDeserializationTask.run(MessageDeserializationTask.java:49)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.net.UnknownHostException: addr is of illegal length
        at java.net.InetAddress.getByAddress(InetAddress.java:935)
        at java.net.InetAddress.getByAddress(InetAddress.java:1311)
        at org.apache.cassandra.net.CompactEndPointSerializationHelper.deserialize(CompactEndPointSerializationHelper.java:37)
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:139)
        at org.apache.cassandra.net.HeaderSerializer.deserialize(Header.java:113)
        at org.apache.cassandra.net.MessageSerializer.deserialize(Message.java:136)
        at org.apache.cassandra.net.MessageDeserializationTask.run(MessageDeserializationTask.java:45)
","13/Apr/10 01:18;gdusbabek;As I suspected: we have more code that is not IPv6 friendly.

Cody, can you give me the low-down on how I can turn off IPv4 and test in an environment that is reasonably similar to yours?  Assume I have an ubuntu VM at my disposal.","13/Apr/10 01:50;cody.lerum;Gary,

I'm running on ubuntu with dual stack (both v4 and v6) I am merely only using ipv6 addresses in the storage and seed portions of the storage-conf.xml.

in your simply set

/etc/network/interfaces

iface eth0 inet6 static
        address 2607:f3d0:0:2::A
        netmask 64
        gateway 2607:f3d0:0:2::1

and on the other server

iface eth0 inet6 static
        address 2607:f3d0:0:2::B
        netmask 64
        gateway 2607:f3d0:0:2::1


Then in your storage-conf.xml

<Seeds>
      <Seed>2607:f3d0:0:1::b</Seed>
 </Seeds>
 <ListenAddress>2607:f3d0:0:2::a</ListenAddress>
  <!-- internal communications port -->
  <StoragePort>7000</StoragePort>

As long as both the vm's are on the same network (non-routed) you should be able to test just fine.




","13/Apr/10 17:52;gdusbabek;I am not able to reproduce the latest problem.  I went as far as creating a unit test to test CompactEndPointSerializationHelper for all manner of IPv4 and IPv6 addresses.  It seems to do the job.

The EOF in the new stack trace makes me think that one of the nodes might not be up on the same code.  Cody, can you verify?",13/Apr/10 18:47;cody.lerum;I may have screwed up the build. I will try the latest from Hudson ,"14/Apr/10 04:40;cody.lerum;Gary, I tested off http://hudson.zones.apache.org/hudson/job/Cassandra/405/ and it all looks good.

root@cassandra:/opt/cassandra# bin/cassandra -f
 INFO 22:32:35,577 Auto DiskAccessMode determined to be mmap
 WARN 22:32:35,840 Couldn't detect any schema definitions in local storage. I hope you've got a plan.
 INFO 22:32:35,851 Replaying /var/lib/cassandra/commitlog/CommitLog-1271219308493.log
 INFO 22:32:35,899 Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1271219555899.log
 INFO 22:32:35,910 LocationInfo has reached its threshold; switching in a fresh Memtable at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1271219555899.log', position=163)
 INFO 22:32:35,911 Enqueuing flush of Memtable(LocationInfo)@1971599989
 INFO 22:32:35,913 Writing Memtable(LocationInfo)@1971599989
 INFO 22:32:36,012 Completed flushing /var/lib/cassandra/data/system/LocationInfo-b-1-Data.db
 INFO 22:32:36,025 Log replay complete
 INFO 22:32:36,051 Saved Token found: 149994310325493222650165912864788358013
 INFO 22:32:36,052 Saved ClusterName found: CLEARFLY-1
 INFO 22:32:36,062 Starting up server gossip
 INFO 22:32:36,110 Binding thrift service to /2607:f3d0:0:2:0:0:0:16:9160
 INFO 22:32:36,115 Cassandra starting up...
 INFO 22:33:35,564 Node /2607:f3d0:0:1:0:0:0:f is now part of the cluster
 INFO 22:33:36,553 InetAddress /2607:f3d0:0:1:0:0:0:f is now UP


You can close this out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in count columns.,CASSANDRA-729,12446189,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,gasolwu,gasolwu,1/21/2010 3:14,3/12/2019 13:59,3/13/2019 22:24,2/5/2010 19:03,0.5,,,,0,,,,,,"same as thrift api (get_count).

Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
cassandra> connect localhost/9160
Connected to localhost/9160
cassandra> del Keyspace1.Standard1['1']
row removed.
cassandra> set Keyspace1.Standard1['1']['foo'] = 'foo value'
Value inserted.
cassandra> set Keyspace1.Standard1['1']['bar'] = 'bar value'
Value inserted.
cassandra> get Keyspace1.Standard1['1']
=> (column=foo, value=foo value, timestamp=1264043095206)
=> (column=bar, value=bar value, timestamp=1264043106184)
Returned 2 results.
cassandra> count Keyspace1.Standard1['1']
2 columns
cassandra> del Keyspace1.Standard1['1']['foo']
column removed.
cassandra> get Keyspace1.Standard1['1']       
=> (column=bar, value=bar value, timestamp=1264043106184)
Returned 1 results.
cassandra> count Keyspace1.Standard1['1']     
2 columns
cassandra>","debian lenny, sun jdk 1.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,20:10.4,,,no_permission,,,,,,,,,,,,19840,,,Fri Feb 05 19:03:23 UTC 2010,,,,,,0|i0g0pr:,91546,,,,,,,,,,,"21/Jan/10 03:20;jbellis;this sounds like CASSANDRA-647, are you testing 0.5.0 final or an earlier release?","21/Jan/10 03:25;gasolwu;yes, testing in 0.5.0-rc3 and 0.5.0 final.","22/Jan/10 09:23;gasolwu;ColumnFamily.getSortedColumns() (return this.columns_) contains deleted column, it's odd.
i don't know how to fix.

CassandraServer.java
382:     Map<String, Collection<IColumn>> columnsMap = multigetColumns(commands, consistency_level);

        for (ReadCommand command: commands)
        {
            Collection<IColumn> columns = columnsMap.get(command.key);
            if(columns == null)
            {
               columnFamiliesMap.put(command.key, 0);
            }
            else
            {
394:            columnFamiliesMap.put(command.key, columns.size()); // contains removed column,
            }
        }
        return columnFamiliesMap;",04/Feb/10 17:41;jbellis;Looks to me like this was fixed by the patch for CASSANDRA-703; can you verify that it works for you in the 0.5 branch?,05/Feb/10 19:03;jbellis;lmorchard tested the 0.5 branch and reports that this bug is fixed now.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
multiget returns empty ColumnOrSuperColumn instead of null,CASSANDRA-739,12446636,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,1/26/2010 5:03,3/12/2019 13:59,3/13/2019 22:24,4/27/2010 22:13,0.7 beta 1,,,,0,,,,,,"the later is more intuitive, and the former violates the rule that COSC should have exactly one of {column, super_column} set.",,,,,,,,,,,,,,,,,,,,27/Apr/10 21:26;jbellis;739-rebased.txt;https://issues.apache.org/jira/secure/attachment/12443004/739-rebased.txt,26/Apr/10 15:42;jbellis;739.txt;https://issues.apache.org/jira/secure/attachment/12442861/739.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,49:25.7,,,no_permission,,,,,,,,,,,,19847,,,Tue Apr 27 22:13:01 UTC 2010,,,,,,0|i0g0rz:,91556,,,,,,,,,,,"26/Apr/10 15:42;jbellis;simplest solution: remove the deprecated multiget method.  (as opposed to multiget_slices, which doesn't have this problem.)

bumped thrift major version to 5.0.0.",27/Apr/10 21:49;brandon.williams;+1,27/Apr/10 22:13;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
multiget_count() should not take a keyspace arg,CASSANDRA-1422,12472286,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,cgist,cgist,8/23/2010 19:09,3/12/2019 13:59,3/13/2019 22:24,8/24/2010 14:44,0.7 beta 2,,,,0,,,,,,"With the addition of set_keyspace(), things that are scoped by keyspace should no longer take keyspace args.",,,,,,,,,,,,,,,,,,,,23/Aug/10 21:49;jhermes;1422.txt;https://issues.apache.org/jira/secure/attachment/12452861/1422.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,49:35.3,,,no_permission,,,,,,,,,,,,20128,,,Tue Aug 24 14:44:22 UTC 2010,,,,,,0|i0g4yn:,92234,jbellis,jbellis,,,,,,,,,"23/Aug/10 21:49;jhermes;Remove one line from cassandra.thrift.
Change keyspace arg to keySpace.get() in CassandraServer#multiget_count.
Change test to not pass in a keyspace arg.

Thrift API version needs to be bumped.",24/Aug/10 14:44;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
better error handling for CommitLogSyncDelay,CASSANDRA-349,12432405,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,urandom,urandom,8/6/2009 15:03,3/12/2019 13:59,3/13/2019 22:24,8/7/2009 17:03,0.4,,,,0,,,,,,"When CommitLogSyncDelay is missing from the config, cassandra throws a NumberFormatException. Since this required directive is new for 0.4, it should probably have better handling, (like CommitLogSync does for example).",,,,,,,,,,,,,,,,,,,,06/Aug/09 16:45;jbellis;349.patch;https://issues.apache.org/jira/secure/attachment/12415759/349.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,45:36.1,,,no_permission,,,,,,,,,,,,19646,,,Sat Aug 08 12:35:18 UTC 2009,,,,,,0|i0fye7:,91170,,,,,,,,,,,06/Aug/09 16:45;jbellis;patch,07/Aug/09 16:26;urandom;+1,07/Aug/09 17:03;jbellis;committed,"08/Aug/09 12:35;hudson;Integrated in Cassandra #161 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/161/])
    human-readable error for bad CommitLogSyncDelay.
patch by jbellis; reviewed by Eric Evans for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.util.NoSuchElementException when returning a node to the cluster,CASSANDRA-1432,12472526,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,amorton,amorton,8/25/2010 21:40,3/12/2019 13:59,3/13/2019 22:24,8/26/2010 22:14,0.6.6,0.7 beta 2,,,0,,,,,,"I'm running the v0.7-beta1 in a 4 nodes cluster and just doing some simple testing. One of the nodes had been down (machine off, unclean shutdown) for an hour or so not sure how many writes were going on, when I bought it back up this message appears in the other 3 nodes...


INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,199 Gossiper.java (line 584) Node /192.168.34.27 has restarted, now UP again
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,200 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.27
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,201 StorageService.java (line 636) Node /192.168.34.27 state jump to normal
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,201 StorageService.java (line 643) Will not change my token ownership to /192.168.34.27
ERROR [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,640 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[HINTED-HANDOFF-POOL:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.NoSuchElementException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.util.NoSuchElementException
        at orgapache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.util.NoSuchElementException
        at java.util.concurrent.ConcurrentSkipListMap.lastKey(ConcurrentSkipListMap.java:1981)
        at java.util.concurrent.ConcurrentSkipListMap$KeySet.last(ConcurrentSkipListMap.java:2331)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:121)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:218)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:78)
        at org.apache.cassandra.db.HintedHandOffManager$1.runMayThrow(HintedHandOffManager.java:296) not sure how many writes were going on
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more

On the machine that was off (34.27) there are no errors in the logs, and here are the entries for around the same time...

 INFO [main] 2010-08-25 19:29:50,679 CommitLog.java (line 340) Recovery complete
 INFO [main] 2010-08-25 19:29:50,769 CommitLog.java (line 180) Log replay complete
 INFO [main] 2010-08-25 19:29:50,797 StorageService.java (line 342) Cassandra version: 0.7.0-beta1-SNAPSHOT
 INFO [main] 2010-08-25 19:29:50,797 StorageService.java (line 343) Thrift API version: 10.0.0
 INFO [main] 2010-08-25 19:29:50,813 SystemTable.java (line 240) Saved Token found: 85070591730234615865843651857942052864
 INFO [main] 2010-08-25 19:29:50,813 SystemTable.java (line 257) Saved ClusterName found: FOO
 INFO [main] 2010-08-25 19:29:50,813 SystemTable.java (line 272) Saved partitioner not found. Using org.apache.cassandra.dht.RandomPartitioner
 INFO [main] 2010-08-25 19:29:50,814 ColumnFamilyStore.java (line 422) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/local1/junkbox/cassandra/commitlog/CommitLog-12827213897
70.log', position=41336)
 INFO [main] 2010-08-25 19:29:50,814 ColumnFamilyStore.java (line 706) Enqueuing flush of Memtable-LocationInfo@916236367(95 bytes, 2 operations)
 INFO [FLUSH-WRITER-POOL:1] 2010-08-25 19:29:50,815 Memtable.java (line 150) Writing Memtable-LocationInfo@916236367(95 bytes, 2 operations)
 INFO [FLUSH-WRITER-POOL:1] 2010-08-25 19:29:50,873 Memtable.java (line 157) Completed flushing /local1/junkbox/cassandra/data/system/LocationInfo-e-6-Data.db
 INFO [main] 2010-08-25 19:29:50,917 StorageService.java (line 374) Starting up server gossip
 INFO [main] 2010-08-25 19:29:51,093 ColumnFamilyStore.java (line 1239) Loaded 0 rows into the Super2 cache
 INFO [main] 2010-08-25 19:29:51,170 CassandraDaemon.java (line 153) Binding thrift service to /0.0.0.0:9160
 INFO [main] 2010-08-25 19:29:51,174 CassandraDaemon.java (line 167) Using TFramedTransport with a max frame size of 15728640 bytes.
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,198 Gossiper.java (line 578) Node /192.168.34.28 is now part of the cluster
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,199 Gossiper.java (line 578) Node /192.168.34.29 is now part of the cluster
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,199 Gossiper.java (line 578) Node /192.168.34.26 is now part of the cluster
 INFO [main] 2010-08-25 19:29:51,204 CassandraDaemon.java (line 208) Listening for thrift clients...
 INFO [main] 2010-08-25 19:29:51,210 Mx4jTool.java (line 73) Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,417 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.28
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,417 Gossiper.java (line 570) InetAddress /192.168.34.28 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,418 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /192.168.34.28
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,855 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.29
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,855 Gossiper.java (line 570) InetAddress /192.168.34.29 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,860 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /192.168.34.29
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:52,930 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.26
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:52,930 Gossiper.java (line 570) InetAddress /192.168.34.26 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:52,930 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /192.168.34.26

I ran a repair on all the nodes and this was all that they each logged 
 INFO [manual-repair-fe7c5abb-bb0a-4415-aa75-0d72ba4e7f1b] 2010-08-25 19:49:24,194 AntiEntropyService.java (line 803) Waiting for repair requests to: []

The cluster seemed OK and kept on working.",,,,,,,,,,,,,,,,,,,,26/Aug/10 21:36;jbellis;1432-06.txt;https://issues.apache.org/jira/secure/attachment/12453173/1432-06.txt,26/Aug/10 21:37;jbellis;1432-trunk.txt;https://issues.apache.org/jira/secure/attachment/12453174/1432-trunk.txt,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,52:45.3,,,no_permission,,,,,,,,,,,,20134,,,Thu Aug 26 22:14:56 UTC 2010,,,,,,0|i0g50v:,92244,brandon.williams,brandon.williams,,,,,,,,,"26/Aug/10 06:41;amorton;I think this is stopping the HH and AE from running. 

I did the following:
- stopped a node
- deleted commit log and data dir for the keyspace 
- turned the node on 
- ran repair 

Then saw similar messages in the logs, with the repairing node logging the Waiting for repair requests to: [] message. No data was sent to the node. 

So my last comment about ""cluster seemed ok"" is probably wrong. 
","26/Aug/10 07:52;stuhood;> Waiting for repair requests to: [] message.
This would imply that that node thinks the replication factor is 1, and that it therefore doesn't need to request trees.","26/Aug/10 21:39;jbellis;the HH paging code doesn't handle zero columns found for the row, which could happen if cleanup removes the rows before they are handed off or if they are tombstoned and aged out during compaction.

patches attached for 0.6 and trunk",26/Aug/10 22:05;brandon.williams;+1,26/Aug/10 22:14;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyStore masking IOException from FileUtils as IOError,CASSANDRA-1557,12475385,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,amorton,amorton,9/29/2010 8:02,3/12/2019 13:59,3/13/2019 22:24,10/7/2010 20:59,0.7 beta 3,,Legacy/CQL,,0,,,,,,"The code in ColumnFamilyStore.snapshot() line 1368 is catching an IOException from the call to FileUtils.createHardLink() and wrapping it in an IOError. However the code in TruncateVerbHandler:56 is looking for the IOException. This can result  in the client not getting a response to a truncate() API call. 

When running on a machine with very low memory I attempted to truncate a CF with few rows, the following error occurred in the logs.

ERROR [MUTATION_STAGE:25] 2010-09-29 16:44:39,341 AbstractCassandraDaemon.java (line 88) Fatal exception in thread Thread[MUTATION_STAGE:25,5,main]
java.io.IOError: java.io.IOException: Cannot run program ""ln"": java.io.IOException: error=12, Cannot allocate memory
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1368)
        at org.apache.cassandra.db.ColumnFamilyStore.truncate(ColumnFamilyStore.java:1511)
        at org.apache.cassandra.db.Table.truncate(Table.java:633)
        at org.apache.cassandra.db.TruncateVerbHandler.doVerb(TruncateVerbHandler.java:54)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at javautil.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Cannot run program ""ln"": java.io.IOException: error=12, Cannot allocate memory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:460)
        at org.apache.cassandra.io.util.FileUtils.createHardLinkWithExec(FileUtils.java:263)
        at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:229)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1360)
        ... 7 more
Caused by: java.io.IOException: java.io.IOException: error=12, Cannot allocate memory
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:453)
        ... 10 more

On the client I got this:

  File ""/tech/home//git_home/trojan/trojan/cassandra/Cassandrapy"", line 846, in truncate
    self.recv_truncate()
  File ""/tech/home//git_home/trojan/trojan/cassandra/Cassandra.py"", line 857, in recv_truncate
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
  File ""/tech/home//git_home/trojan/trojan/thrift/protocol/TBinaryProtocol.py"", line 126, in readMessageBegin
    sz = self.readI32()
<snip>
    chunk = self.read(sz-have)
  File ""/tech/home//git_home/trojan/trojan/thrift/transport/TSocket.py"", line 92, in read
    buff = self.handle.recv(sz)
timeout: timed out",,,,,,,,,,,,,,,,,,,,01/Oct/10 16:38;jbellis;1557.txt;https://issues.apache.org/jira/secure/attachment/12456133/1557.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,38:49.1,,,no_permission,,,,,,,,,,,,20199,,,Thu Oct 07 20:59:42 UTC 2010,,,,,,0|i0g5yn:,92396,amorton,amorton,,,,,,,,,01/Oct/10 16:38;jbellis;patch allows snapshot IOException to propagate up to TruncateVerbHandler,"07/Oct/10 20:59;amorton;Looks good to me.

Tested by throwing an IOException from CFS.snapshot(). TruncateVerbHandler caught it and logged the error, the client returned immediately without an error being raised. 

One small thing I noticed was that ColumnFamilyStore.truncate() wraps all exceptions from forceBlockingFlush() in an IOException..

        try
        {
            forceBlockingFlush();
        }
        catch (Exception e)
        {
            throw new IOException(e);
        }

but all other functions that call forceBlockingFlush (e..g snapshot or addIndex) catch the two exceptions it throws and wrap them differently
            try
            {
                forceBlockingFlush();
            }
            catch (ExecutionException e)
            {
                throw new RuntimeException(e);
            }
            catch (InterruptedException e)
            {
                throw new AssertionError(e);
            }
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Twisted driver for CQL,CASSANDRA-2022,12496185,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,urandom,urandom,1/20/2011 21:58,3/12/2019 13:59,3/13/2019 22:24,3/16/2011 3:06,0.8 beta 1,,Legacy/CQL,1/20/2011 0:00,0,,,,,,"In-tree CQL drivers should be reasonably consistent with one another (wherever possible/practical), and implement a minimum of:

• Query compression
• Keyspace assignment on connection
• Connection pooling / load-balancing

The goal is not to supplant the idiomatic libraries, but to provide a consistent, stable base for them to build upon.",,,,,,,,,,,,,,,,,,,,15/Mar/11 22:27;brandon.williams;txcql.txt;https://issues.apache.org/jira/secure/attachment/12473742/txcql.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,27:25.8,,,no_permission,,,,,,,,,,,,20403,,,Thu Mar 17 16:23:25 UTC 2011,,,,,,0|i0g8uf:,92863,,,,,,,,,,,"14/Mar/11 19:55;urandom;You need to change {{default_validation_class}} to {{default_validation}} to make {{example.py}} work, but otherwise, LGTM.","15/Mar/11 22:27;brandon.williams;Update patch fixes validation_class, adds support for SchemaDisagreementException, allows a decoder to be passed to the conn pool, adds a setup.py and README.",16/Mar/11 03:06;brandon.williams;Committed,"17/Mar/11 16:23;hudson;Integrated in Cassandra #789 (See [https://hudson.apache.org/hudson/job/Cassandra/789/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM in the Thrift thread doesn't shut down server,CASSANDRA-2269,12500381,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,3/3/2011 23:46,3/12/2019 13:59,3/13/2019 22:24,3/15/2011 19:48,0.6.13,0.7.5,Legacy/CQL,,0,,,,,,"Example:

{noformat}
java.lang.OutOfMemoryError: Java heap space
        at org.cliffc.high_scale_lib.NonBlockingHashMap$CHM.resize(NonBlockingHashMap.java:849)
        at org.cliffc.high_scale_lib.NonBlockingHashMap$CHM.access$200(NonBlockingHashMap.java:699)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:634)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:339)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.put(NonBlockingHashMap.java:302)
        at org.apache.cassandra.utils.ExpiringMap.put(ExpiringMap.java:112)
        at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:237)
        at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:305)
        at org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:386)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:347)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:92)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:175)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:254)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:215)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:1272)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1166)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{noformat}",,,14400,14400,,0%,14400,14400,,,,,,,,,,,,04/Mar/11 00:15;jbellis;2269-0.6.txt;https://issues.apache.org/jira/secure/attachment/12472634/2269-0.6.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,24:09.7,,,no_permission,,,,,,,,,,,,19344,,,Fri Mar 18 19:14:11 UTC 2011,,,,,,0|i0gadb:,93110,tjake,tjake,,,,,,,,,"04/Mar/11 00:15;jbellis;patch to extract exception logging from DTPE and call from the Thrift executor.

(the actual shutdown is done by the default exception hook we set up -- it's not normally called on executor threads because both Future and executor afterExecute machinery swallow exceptions.)",04/Mar/11 04:24;tjake;Makes sense... Is there any known way to reproduce it to verify the fix infact works?,04/Mar/11 04:32;jbellis;Adding ByteBuffer.allocateDirect(256 * 1024 * 1024) to any of the CassandraServer methods should do it,15/Mar/11 19:21;tjake;+1,15/Mar/11 19:48;jbellis;committed,"18/Mar/11 19:14;hudson;Integrated in Cassandra-0.6 #63 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/63/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql driver jar,CASSANDRA-2263,12500226,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,3/2/2011 20:59,3/12/2019 13:59,3/13/2019 22:24,5/30/2011 17:55,1.0.0,,Legacy/CQL,,1,cql,,,,,"Work was done in CASSANDRA-1848  to create a jar for the CQL Java driver.  The generated Thrfit code was broken out into it's own jar as well, since that is a dependency for both servers and clients. However, based on the work currently happening in CASSANDRA-2262 and CASSANDRA-2124, it seems that additional dependencies will exist, and new jar(s) will need to be created.

The easiest way to fix this will probably be to put copies of all of {{o.a.c.db.marshal}} and {{o.a.c.utils}}, and a copy of {{o.a.c.config.ConfigurationException}} into the CQL driver jar (a split along those lines to create another jar doesn't make sense IMO).",,,0,0,,0%,0,0,,,,,,,,,,,,30/May/11 15:53;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2263-don-t-ship-jdbc-in-main-jar.txt;https://issues.apache.org/jira/secure/attachment/12480856/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2263-don-t-ship-jdbc-in-main-jar.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,45:44.9,,,no_permission,,,,,,,,,,,,20533,,,Tue May 31 22:43:04 UTC 2011,,,,,,0|i0gabz:,93104,,,,,,,,,,,"28/May/11 19:45;larswunderlich;I don't know whether this issue should be documented in a separate bug for 0.8.0-rc1, but since this one deals with separation of jar file, I add the comment here.

The following code fails on my machine:

Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
Connection c = DriverManager.getConnection(""jdbc:cassandra:/@localhost:9160/testspace"");

As of apache-cassandra-0.8.0-rc1 together with apache-cassandra-cql-1.0.2.jar the connection to local host couldn't be established, even though it was running and the keyspace was fine, because of the following reasons:

1.) cql jar requires direct classpath relationship to apache-cassandra-0.8.0-rc1.jar, without it cannot run at all, what contradicts server implementation encapsulation/secret in my mind to attach core jar file: 

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/db/marshal/AbstractType
	at org.apache.cassandra.cql.jdbc.Connection.execute(Connection.java:142)
	at org.apache.cassandra.cql.jdbc.Connection.execute(Connection.java:124)
	at org.apache.cassandra.cql.jdbc.CassandraConnection.<init>(CassandraConnection.java:83)
	at org.apache.cassandra.cql.jdbc.CassandraDriver.connect(CassandraDriver.java:86)
	at java.sql.DriverManager.getConnection(Unknown Source)
	at java.sql.DriverManager.getConnection(Unknown Source)
	at TestConnection.main(TestConnection.java:18)

2.) apache-cassandra-0.8.0-rc1.jar internally contains org.apache.cassandra.cql.jdbc package a second time, which might conflict with the cql standalone jar version of the driver in terms of class compatibility.

3.) Using CassandraDriver from core apache-cassandra-0.8.0-rc1.jar results in an error message that cassandra.yaml couldn't be found, but cassandra.yaml is not required from my point of view for clients anyway.

Cannot locate cassandra.yaml
Fatal configuration error; unable to start server.  See log for stacktrace.","29/May/11 18:56;larswunderlich;Connection problem could be solved by passing parameter -Dcassandra.config=cassandra.yaml when starting test case and adding cassandra.yaml file as copy to classpath. However, cql driver couldn't be started out of the box and I haven't found documentation including code examples how to successfully setup a working JDBC connection as of now.  ","30/May/11 15:55;urandom;Ultimately, we don't want to have to depend on the main jar, but the fact that this is the case now, is known.  I think the problem here (as you already suggested), is that there is also a copy in the cassandra jar.  The attached patch remedies that.",30/May/11 16:16;jbellis;+1,30/May/11 17:55;urandom;committed,"31/May/11 03:56;jbellis;I think this broke the jdbc tests post-clean:

{noformat}
build-test:
    [javac] /Users/jonathan/projects/cassandra/svn-0.8.0/build.xml:973: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 124 source files to /Users/jonathan/projects/cassandra/svn-0.8.0/build/test/classes
    [javac] /Users/jonathan/projects/cassandra/svn-0.8.0/drivers/java/test/org/apache/cassandra/cql/jdbc/PreparedStatementTest.java:357: cannot find symbol
    [javac] symbol  : class CassandraPreparedStatement
    [javac] location: class org.apache.cassandra.cql.jdbc.PreparedStatementTest
    [javac]         CassandraPreparedStatement stmt = (CassandraPreparedStatement)con.prepareStatement(q);
    [javac]         ^
{noformat}","31/May/11 22:43;urandom;Gah, I'd fixed this locally but forgot to commit.  It's there now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe_schema_versions does not list downed hosts,CASSANDRA-1678,12478609,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,appodictic,appodictic,10/28/2010 18:15,3/12/2019 13:59,3/13/2019 22:24,11/2/2010 14:05,0.7.0 rc 1,,Legacy/CQL,,0,,,,,,"According to the description unreachable hosts should be listed. It does not seem like they are.
{noformat}
 map<string, list<string>> describe_schema_versions()

  [java] key:c3f38ebc-e1c5-11df-95a0-e700f669bcfc
     [java] 	127.0.0.2
     [java] 	127.0.0.3
     [java] 	127.0.0.4
     [java] 	127.0.0.1

Address         Status State   Load            Token                                       
                                       105444142448428656124184491892431731479    
127.0.0.3       Up     Normal  56.53 KB        43021486531749787992103274496183765897      
127.0.0.1       Up     Normal  56.24 KB        49910048177093876350019363877113991186      
127.0.0.5       Down   Normal  52.49 KB        64377498999076014343862177049497951437      
127.0.0.2       Up     Normal  65.27 KB        84713069031498515281943177906254878023      
127.0.0.4       Up     Normal  55.95 KB        105444142448428656124184491892431731479
{noformat}

The code looks like this:
{noformat}
 Cassandra.Client client = fcw.getClient();
    Map<String,List<String>> sv =client.describe_schema_versions();
    for (Map.Entry<String,List<String>> mapEntry: sv.entrySet()){
      System.out.println(""key:""+mapEntry.getKey());
      for (String listForKey : mapEntry.getValue()){
        System.out.println(""\t""+listForKey);
      }
    }
{noformat}",,,,,,,,,,,,,,,,,,,,02/Nov/10 13:15;gdusbabek;ASF.LICENSE.NOT.GRANTED--v4-0001-include-dead-hosts-in-unreachable.txt;https://issues.apache.org/jira/secure/attachment/12458631/ASF.LICENSE.NOT.GRANTED--v4-0001-include-dead-hosts-in-unreachable.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,30:14.2,,,no_permission,,,,,,,,,,,,699,,,Wed Nov 03 12:49:55 UTC 2010,,,,,,0|i0g6pr:,92518,,,,,,,,,,,"28/Oct/10 18:30;jbellis;Odd, I see the code trying to add the missing hosts in describeSchemaVersions:

{code}
            results.put(DatabaseDescriptor.INITIAL_VERSION.toString(), missingHostNames);
{code}

But IMO the right fix is to avoid that entirely and do the simple thing instead:

{code}
  /**
   * for each schema version present in the cluster, returns a list of nodes at that version.
   * hosts that do not respond will not be included.
   * the cluster is all on the same version if the size of the map is 1 and the
   * length of the list	in that map value is the number of nodes in the cluster.
   */
{code}","29/Oct/10 04:53;appodictic;Yes changing the description would ""Fix"" the problem. At minimum the description should match what the method actually does. I imagine this is used by applications that want to ensure the schema is in place before doing writes. Ignoring downed hosts could be misleading, as users would have to ensure the list size matches the size it should be. You could view that as a separate problem but I think users would want both in one method.","29/Oct/10 19:16;gdusbabek;the 'missing hosts' Jonathan referred to were the live hosts that did not respond to the version query.  There could be other missing hosts that getLiveMembers() didn't return because they were down.

Would it suit everybody if we included the unreachable members at version=DD.INITIAL_VERSION?","29/Oct/10 19:28;jbellis;wfm, although not having a distinction between ""responded that he was at INITIAL_VERSION"" and ""didn't respond"" feels a little unsavory.","29/Oct/10 19:33;gdusbabek;Since they're just strings I could just give them the value ""UNREACHABLE""?",29/Oct/10 19:41;jbellis;+1,29/Oct/10 20:01;appodictic;+1,29/Oct/10 21:31;jbellis;seems to me it would be simpler to just loop through all hosts after versions are received; if host is not in ackedhosts then add it to the unreachable set.  this avoids problems caused by FD moving a node from live to dead or vice versa between requests being sent and processed.,"01/Nov/10 22:57;jbellis;minor point: allHosts can be replaced with Iterables.concat(live, unreachable)

is the ""check for version disagreement"" loop still useful now?","02/Nov/10 13:17;gdusbabek;The check is still useful for logging the disagreeing hosts.  When the feature was first created I seem to recall someone wanting it to be logged (didn't make sense to me).

v4 uses the Iterables call.",02/Nov/10 14:00;jbellis;+1,"03/Nov/10 12:49;hudson;Integrated in Cassandra #585 (See [https://hudson.apache.org/hudson/job/Cassandra/585/])
    include dead hosts in unreachable. patch by gdusbabek, reviewed by jbellis. CASSANDRA-1678
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
API Version Mismatch,CASSANDRA-1484,12473633,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,arya,arya,9/8/2010 23:02,3/12/2019 13:59,3/13/2019 22:24,9/8/2010 23:12,0.7 beta 2,,Legacy/CQL,,0,,,,,,"Updated to Cassandra Trunk and Thrift Trunk. API versions mismatch when describe_version() is called. 

AssertionError: Thrift API version mismatch. (Client: 14, Server: 13)

This causes some API clients that do version validation like pycassa to fail. 
","CentOS 5.2
Trunk Wed. Sep 8th",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,12:29.1,,,no_permission,,,,,,,,,,,,20162,,,Wed Sep 08 23:12:29 UTC 2010,,,,,,0|i0g5cf:,92296,,,,,,,,,,,08/Sep/10 23:12;jbellis;fixed in r995280,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimeUUID comparator identify different UUID as long as they have same timestamp,CASSANDRA-907,12459667,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,3/19/2010 15:57,3/12/2019 13:59,3/13/2019 22:24,3/19/2010 19:51,0.6,,,,0,,,,,,Everything's in the title. As such TimeUUID behave as simple timestamp which is weird at best.,,,,,,,,,,,,,,,,,,,,19/Mar/10 15:58;slebresne;TimeUUID_compareSameTimestamp.diff;https://issues.apache.org/jira/secure/attachment/12439283/TimeUUID_compareSameTimestamp.diff,19/Mar/10 16:12;jalessi;test case.txt;https://issues.apache.org/jira/secure/attachment/12439287/test+case.txt,19/Mar/10 16:41;slebresne;testDifferentTimeUUIDSameTimestamp.diff;https://issues.apache.org/jira/secure/attachment/12439292/testDifferentTimeUUIDSameTimestamp.diff,,,,,,,,,,,3,,,,,,,,,,,,,,,,,,,03:51.1,,,no_permission,,,,,,,,,,,,19913,,,Fri Mar 19 19:51:11 UTC 2010,,,,,,0|i0g1tb:,91724,,,,,,,,,,,19/Mar/10 16:03;jbellis;can you add a Test case that catches the original bug?,19/Mar/10 16:12;jalessi;The attached file has a test case in it that reproduces the bug,"19/Mar/10 16:18;jbellis;to clarify: a junit test case for ""ant test"" :)","19/Mar/10 16:41;slebresne;Posting a test for the test_server.py script.
I can come up with a test for the junit test if you prefer but there 
doesn't seems to be junit tests for TimeUUID right now.

As a side note, the system tests (test_server) seems a bit 
broken. The 'test_bad_calls' fails when insert is called with 
null argument because the thrift exception sent must have 
changed somehow. But you have to regenerate the thrift 
java binding (and recompile) to see the failing tests.","19/Mar/10 19:51;jbellis;added junit test and committed to 0.6 and trunk, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wordcount contrib is not including all required jars,CASSANDRA-816,12457009,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,johanoskarsson,johanoskarsson,johanoskarsson,2/22/2010 14:45,3/12/2019 13:59,3/13/2019 22:24,2/22/2010 17:50,0.6,,,,0,,,,,,The wordcount contrib build process is not including the libraries downloaded using ivy in the final jar file.,,,,,,,,,,,,,,,,,,,,22/Feb/10 16:16;johanoskarsson;CASSANDRA-816.patch;https://issues.apache.org/jira/secure/attachment/12436585/CASSANDRA-816.patch,22/Feb/10 14:46;johanoskarsson;CASSANDRA-816.patch;https://issues.apache.org/jira/secure/attachment/12436574/CASSANDRA-816.patch,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,28:22.9,,,no_permission,,,,,,,,,,,,19878,,,Wed Feb 24 13:07:31 UTC 2010,,,,,,0|i0g193:,91633,,,,,,,,,,,22/Feb/10 14:46;johanoskarsson;Adds the required files to the jar.,"22/Feb/10 16:16;johanoskarsson;Created the patch in the wrong dir, this is the corrected one",22/Feb/10 16:28;jbellis;do we need this in 0.6 or does this only affect the post-0.6 ivy changes?,"22/Feb/10 17:03;johanoskarsson;According to CASSANDRA-802 the ivy changes were in 0.6, so this one should also go into both 0.6 and trunk.","22/Feb/10 17:11;jbellis;+1

please commit first to 0.6 then merge to trunk with repo merge (that is, not old-style cherry picking).  thanks!",22/Feb/10 17:50;johanoskarsson;Committed to trunk and the 0.6 branch.,"24/Feb/10 13:07;hudson;Integrated in Cassandra #364 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/364/])
    Fix bug where ivy downloaded jar files were not included. Patch by johan, review by jbellis. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consisteny Level of ZERO blocks for ack on Commit Log,CASSANDRA-399,12434368,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,8/29/2009 22:23,3/12/2019 13:59,3/13/2019 22:24,8/29/2009 22:47,0.4,,,,0,,,,,,"If consistency level is set to ZERO and endpoint is local, clients must wait for a write to the commit log. We need to remove this special case, and just send through MessagingService.getMessagingInstance().sendOneWay.",,,,,,,,,,,,,,,,,,,,29/Aug/09 22:31;lenn0x;0001-CASSANDRA-399-If-consistency-level-is-set-to-ZERO-an.patch;https://issues.apache.org/jira/secure/attachment/12418076/0001-CASSANDRA-399-If-consistency-level-is-set-to-ZERO-an.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,33:42.5,,,no_permission,,,,,,,,,,,,19671,,,Sun Aug 30 12:35:36 UTC 2009,,,,,,0|i0fyp3:,91219,,,,,,,,,,,"29/Aug/09 22:33;sandeep_tata;I remember we put this in to get limited read-your-writes consistency for a session. (Log + rm.apply()  before ack.)
See CASSANDRA-132.

There are cases when this is not the right thing to do (eg skip logging during a load operation), but for a normal client, read-your-writes is way easier to deal with than pure eventual consistency. 





","29/Aug/09 22:47;jbellis;Committed before I saw Sandeep's objection.  But insert -- the method that handles ConsistencyLevel.ZERO -- is the wrong place to do this.  If you want any blocking you need to use ONE or higher, that's how it's supposed to work.",29/Aug/09 22:47;jbellis;(which goes to the separate insertBlocking method.),"29/Aug/09 22:49;jbellis;to clarify: I am +1 on adding a CASSANDRA-132 special case for insertBlocking, but -1 on having it in insert.  My bad for not catching that in review originally.  Should we re-open 132?",29/Aug/09 22:51;lenn0x;I agree that I think this special case should live in insertBlocking. ,"29/Aug/09 23:05;sandeep_tata;I'm +1 on the patch, but we should re-open 132. 
Current insertBlocking=1 is not the same as session level read-your-writes.
I guess we could just special-case this write for insertBlocking=1. I'd rather not introduce a new ConsistencyLevel.


ConsistencyLevel.ZERO should help stress test MessagingService... I remember with the old codepath writes would get queued up in the MessagingService and we ended up with a few lost writes. Should be interesting to see if those problems pop up again.



","30/Aug/09 12:35;hudson;Integrated in Cassandra #182 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/182/])
    r/m special case of local destination when writing with ConsistencyLevel.ZERO, since it causes blocking for commitlog.  (MessagingService still optimizes out the network write/read.)  Patch by Chris Goffinet; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use get_range_slices in stress.py,CASSANDRA-1094,12464579,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,5/15/2010 1:42,3/12/2019 13:59,3/13/2019 22:24,5/17/2010 14:35,0.7 beta 1,,Legacy/Tools,,0,,,,,,,,,,,,,,,,,,,,,,,,,,15/May/10 01:43;stuhood;get_range_slices.diff;https://issues.apache.org/jira/secure/attachment/12444556/get_range_slices.diff,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,35:41.9,,,no_permission,,,,,,,,,,,,19990,,,Tue May 18 13:31:39 UTC 2010,,,,,,0|i0g2yn:,91910,,,,,,,,,,,17/May/10 14:35;jbellis;committed,"18/May/10 13:31;hudson;Integrated in Cassandra #439 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/439/])
    use get_range_slices in stress.py.  patch by Stu Hood; reviewed by jbellis for CASSANDRA-1094
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java doesn't insert the correct amount of rows,CASSANDRA-2200,12499092,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,2/18/2011 22:29,3/12/2019 13:59,3/13/2019 22:24,2/23/2011 19:34,0.7.3,,,,0,,,,,,"For example, if you pass -n 2000000 you only get 1999800 (with 300 threads at least, didn't check if it was related)",,,28800,28800,,0%,28800,28800,,,,,,,,,,CASSANDRA-2020,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,36:29.3,,,no_permission,,,,,,,,,,,,20500,,,Wed Feb 23 19:34:23 UTC 2011,,,,,,0|i0g9xj:,93039,,,,,,,,,,,18/Feb/11 22:36;xedin;this is related to gaussian function and (when -r is used) pseudo-random generator which generate the same keys in some circumstances. We either should acknowledge this is an known issue or retry key generation (which could take infinite time).,18/Feb/11 22:38;jbellis;_generation_ should be deterministic.  key distribution functions should only be applied to reads.,"18/Feb/11 22:52;xedin;oh sorry for misleading comment, of course this is about reads. for write however - each thread was a portion of range of keys to generate and that range is calculated in the following way:

(keysPerThread * (idx + keysToSkip), keysPerThread * (idx + 1)), where _idx_ - index of the current thread, _keysPerThread_ - totalKeys / threadCount, _keysToSkip_ - determined by user in params.

This is ported from python code (line 203). For some numbers of the total keys and threadCount it won't generate precise ranges. Seems like we'll need to range one more thread at the end to generate those missing rows sometimes.",18/Feb/11 23:03;brandon.williams;Maybe we should just move forward with CASSANDRA-2020 then.,"18/Feb/11 23:16;xedin;I concur. It seems to me that porting from py_stress wasn't such a good idea, stress.java needs re-design.",23/Feb/11 19:34;brandon.williams;Solved by CASSANDRA-2020,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove analytics package,CASSANDRA-297,12430544,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,7/15/2009 20:46,3/12/2019 13:59,3/13/2019 22:24,7/16/2009 21:42,0.4,,,,0,,,,,,The long-term strategy is for JMX instrumentation; the Ganglia-based analytics code should be removed.,,,,,,,,,,,,,,,,,,,,16/Jul/09 16:50;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-297-remove-ganglia-analytics-package.txt;https://issues.apache.org/jira/secure/attachment/12413704/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-297-remove-ganglia-analytics-package.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,12:38.8,,,no_permission,,,,,,,,,,,,19625,,,Fri Jul 17 13:01:50 UTC 2009,,,,,,0|i0fy2n:,91118,,,,,,,,,,,"16/Jul/09 19:12;euphoria;The code removal looks fine, still reading through the specifics just to be careful.

However, are you aware of specific metrics being removed through this patch?  I've been tallying things up and cross-checking against nodeprobe, but if you already know it'd be easier than me hunting them down.","16/Jul/09 19:31;urandom;The only data-point removed by this patch is the write stats in Table.load, which is already covered by the CFSMBean WriteCount and WriteLatency attributes.","16/Jul/09 20:31;euphoria;Unit tests pass, system tests pass, bin/nodeprobe statistics all look good.

The only code changes are deletes, and they're all scoped to this ticket.

+1",16/Jul/09 21:42;urandom;committed,"17/Jul/09 13:01;hudson;Integrated in Cassandra #140 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/140/])
    remove ganglia analytics package

Patch by eevans; reviewed by Michael Greene for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractCassandraDaemon blows up when log4j config is specified using a physical file.,CASSANDRA-1907,12494138,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,12/27/2010 20:56,3/12/2019 13:59,3/13/2019 22:24,12/27/2010 22:19,0.7.1,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,27/Dec/10 21:06;gdusbabek;0001-1907.patch;https://issues.apache.org/jira/secure/attachment/12467015/0001-1907.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,31:38.5,,,no_permission,,,,,,,,,,,,19357,,,Mon Dec 27 22:34:38 UTC 2010,,,,,,0|i0g85b:,92750,tjake,tjake,,,,,,,,,27/Dec/10 21:31;tjake;+1,27/Dec/10 22:19;gdusbabek;committed.,"27/Dec/10 22:34;hudson;Integrated in Cassandra-0.7 #120 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/120/])
    log4j configuration wasn't handling configurations specified by URL. patch by Gary Dusbabek, reviewed by Jake Luciani. CASSANDRA-1907
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove unused setters from DatabaseDescriptor,CASSANDRA-591,12442134,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,12/1/2009 19:27,3/12/2019 13:59,3/13/2019 22:24,12/1/2009 20:48,0.5,,,,0,,,,,,"setLogFileLocation() and setTableNames() are unused, make no sense, and should be removed.",,,,,,,,,,,,,,,,,,,,01/Dec/09 19:28;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-591-remove-unused-setters.txt;https://issues.apache.org/jira/secure/attachment/12426568/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-591-remove-unused-setters.txt,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,40:39.3,,,no_permission,,,,,,,,,,,,19769,,,Sat Dec 05 12:34:26 UTC 2009,,,,,,0|i0fzvj:,91410,,,,,,,,,,,01/Dec/09 19:40;jbellis;+1,01/Dec/09 20:48;urandom;committed; thanks,"05/Dec/09 12:34;hudson;Integrated in Cassandra #278 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/278/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra Cli hangs forever if schema does not settle within timeout window,CASSANDRA-2187,12498988,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,2/18/2011 2:24,3/12/2019 13:59,3/13/2019 22:24,3/22/2011 22:18,0.7.3,,,,0,,,,,,"validateSchemaIsSettled will hang in the while loop since we never update start if migrations never settle.
",,,,,,,,,,,,,,,,,,,,18/Feb/11 02:30;lenn0x;0001-Fix-Cassandra-cli-to-respect-timeout-if-schema-does-.patch;https://issues.apache.org/jira/secure/attachment/12471359/0001-Fix-Cassandra-cli-to-respect-timeout-if-schema-does-.patch,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,40:58.9,,,no_permission,,,,,,,,,,,,20489,,,Wed Feb 23 15:18:09 UTC 2011,,,,,,0|i0g9un:,93026,jbellis,jbellis,,,,,,,,,18/Feb/11 02:40;jbellis;kind of confusing flow to have a start variable that we keep mutating.  simpler to just eliminate it and use System.cTM in the loop condition?,"18/Feb/11 02:56;hudson;Integrated in Cassandra-0.7 #291 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/291/])
    Fix Cassandra cli to respect timeout if schema does not settle patch by goffinet; reviewed by jbellis for CASSANDRA-2187
","23/Feb/11 15:18;hudson;Integrated in Cassandra-0.7 #309 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/309/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodeprobe setcompactionthreshold <Max> does not allow zero,CASSANDRA-465,12436888,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,9/30/2009 4:46,3/12/2019 13:58,3/13/2019 22:24,10/7/2009 21:52,0.5,,,,0,,,,,,We should allow setting maxthreshold to zero.,,,,,,,,,,,,,,,,,,,,06/Oct/09 16:44;lenn0x;0001-Allow-maxthreshold-to-be-greater-than-zero-v2.patch;https://issues.apache.org/jira/secure/attachment/12421439/0001-Allow-maxthreshold-to-be-greater-than-zero-v2.patch,06/Oct/09 16:50;lenn0x;0001-Allow-maxthreshold-to-be-greater-than-zero-v3.patch;https://issues.apache.org/jira/secure/attachment/12421441/0001-Allow-maxthreshold-to-be-greater-than-zero-v3.patch,07/Oct/09 21:30;lenn0x;0001-Allow-maxthreshold-to-be-greater-than-zero-v4.patch;https://issues.apache.org/jira/secure/attachment/12421583/0001-Allow-maxthreshold-to-be-greater-than-zero-v4.patch,30/Sep/09 04:52;lenn0x;0001-CASSANDRA-465.-Allow-maxthreshold-to-be-set-to-zero.patch;https://issues.apache.org/jira/secure/attachment/12420876/0001-CASSANDRA-465.-Allow-maxthreshold-to-be-set-to-zero.patch,,,,,,,,,,4,,,,,,,,,,,,,,,,,,,45:05.8,,,no_permission,,,,,,,,,,,,19702,,,Thu Oct 08 12:35:15 UTC 2009,,,,,,0|i0fz3r:,91285,,,,,,,,,,,"30/Sep/09 15:45;jbellis;Let's be a little more rigorous here.

We should require that min <= max.

min == max == 0 should disable compaction.

otherwise, min should be >= 2.","06/Oct/09 16:56;jbellis;maxthreshold default should be a constant in CompactionManager (that CM also defaults to of course)

currently max=0 disables compaction sort of by accident (from the sublist call in CFS.doCompaction), which generates a Warning log message.  we should add a special case to doCompaction that doesn't call doFileCompaction at all in that case.","07/Oct/09 21:35;jbellis;imo the info line you added should be debug level for consistency.

other than that +1, go ahead and commit w/o an extra patch cycle","08/Oct/09 12:35;hudson;Integrated in Cassandra #221 (See [http://hudson.zones.apache.org/hudson/job/Cassandra/221/])
    Added the ability to set maxthreshold to zero in nodeprobe, and added a special case to CF.doCompaction to not run compaction when its disabled. patch by goffinet; reviewed by jbellis for 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
