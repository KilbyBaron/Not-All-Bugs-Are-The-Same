Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Reviewer),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
streaming from all (not one) neighbors during rebuild/bootstrap,CASSANDRA-3922,12542825,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,scode,scode,scode,16/Feb/12 06:17,12/Mar/19 14:20,13/Mar/19 22:25,16/Feb/12 08:40,1.1.0,,,,,,0,,,,,"The last round of changes that happened in CASSANDRA-3483 before it went in actually changed behavior - we now stream from *ALL* neighbors that have a range, rather than just one. This leads to data size explosion.

Attaching patch to revert to intended behavior.",,,,,,,,,,,,,,,,16/Feb/12 06:18;scode;CASSANDRA-3922-1.1.txt;https://issues.apache.org/jira/secure/attachment/12514762/CASSANDRA-3922-1.1.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-16 08:40:57.031,,,no_permission,,,,,,,,,,,,228111,,,Thu Feb 16 08:40:57 UTC 2012,,,,,,0|i0gpuf:,95617,slebresne,slebresne,,,,,,,,,"16/Feb/12 08:40;slebresne;+1, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gossip stage backed up due to migration manager future de-ref,CASSANDRA-3832,12540842,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,scode,scode,scode,01/Feb/12 23:45,12/Mar/19 14:20,13/Mar/19 22:26,09/Feb/12 10:14,1.1.0,,,,,,0,,,,,"This is just bootstrapping a ~ 180 trunk cluster. After a while, a
node I was on was stuck with thinking all nodes are down, because
gossip stage was backed up, because it was spending a long time
(multiple seconds or more, I suppose RPC timeout maybe) doing the
following. Cluster-wide restart -> back to normal. I have not
investigated further.

{code}
""GossipStage:1"" daemon prio=10 tid=0x00007f9d5847a800 nid=0xa6fc waiting on condition [0x000000004345f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000005029ad1c0> (a java.util.concurrent.FutureTask$Sync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:218)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:364)
	at org.apache.cassandra.service.MigrationManager.rectifySchema(MigrationManager.java:132)
	at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:75)
	at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:802)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:918)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:68)
{code}
",,,,,,,,,,,,,,,,05/Feb/12 23:14;scode;CASSANDRA-3832-trunk-dontwaitonfuture.txt;https://issues.apache.org/jira/secure/attachment/12513377/CASSANDRA-3832-trunk-dontwaitonfuture.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-05 23:12:15.392,,,no_permission,,,,,,,,,,,,226196,,,Thu Feb 09 10:37:50 UTC 2012,,,,,,0|i0gor3:,95440,jbellis,jbellis,,,,,,,,,"05/Feb/12 22:46;scode;This happens w/o bootstrap too. I just took the same ~ 180 node cluster on 0.8 and deployed trunk on it. All nodes restarted almost at the same time, but no one is in any state but Normal. It's sitting there not really coming up (probably is, but slowly) despite this just being a full cluster restart effectively.","05/Feb/12 23:00;scode;Meanwhile, MigrationStage is stuck like this:

{code}
""MigrationStage:1"" daemon prio=10 tid=0x00007fb5b450e800 nid=0x3395 waiting on condition [0x0000000043479000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000005032ed688> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2116)
	at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:61)
	at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:119)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
{code}

The GossipStage submits the job on the migration state on the local node and waits for the result. The migration stage in turn sends a message and waits for the response synchronously.

The migration request runs on the migration stage on the remote node, which is presumably stuck with it's own task on the migration stage.

In effect, we are causing a distributed deadlock (or almost deadlock, I'm not sure - I suppose we might get unstuck eventually since things do time out after rpc timeout).
","05/Feb/12 23:02;scode;Though in this case so far as I was looking at code, the cluster is not showing signs of recovering. Essentially, the cluster is in a ""fallen and can't get up"" state.",05/Feb/12 23:12;jbellis;It looks like the intent of the wait is to avoid sending more than one request to the same target for the same version.  One possible fix would be to replace the wait with a Set of in-progress requests a la HHOM.,"05/Feb/12 23:14;scode;Attaching simple patch to just not wait on the future. Given that we have no special code path to handle timeouts anyway, this does not introduce any actual lack of failure handling beyond what is already there, so as far as I can tell it should not cause any failure to reach schema agreement that we would not already be vulnerable to.

Also upping priority since this bug causes clusters to refuse to start up even with full cluster re-starts by the operator.","05/Feb/12 23:15;scode;(Sorry about the e-mail spam I just generated, there are race conditions in the JIRA UI and the 'submit patch' button exhcnages places with start/stop progress...)","05/Feb/12 23:17;scode;jbellis: Ok, that seems plausible (about the intent). I wonder if it is worth it though? What are the negative consequences of multiple requests beyond the CPU usage?",05/Feb/12 23:18;scode;I will submit a patch that does it and you can decide.,05/Feb/12 23:22;jbellis;committed,"05/Feb/12 23:27;scode;A concern here though is that in order to make this scale on very large clusters, you probably want to limit the amount of schema migration attempts that are in progress for a given schema version, and not just limit the amount of outstanding for a single node.

But doing that requires complicating the code so that we don't fail to migrate to a new schema just because one node happened to go down just after we were notified of 500 nodes being e.g. alive and having a schema we don't recognize.

For now, I will proceed with avoiding duplicate (endpoint, schema) pairs rather than global throttling.
","05/Feb/12 23:28;scode;My last comment was made before reading ""committed"", sorry.","05/Feb/12 23:28;scode;If you still want the augmented patch, let me know.","05/Feb/12 23:58;jbellis;I'm happy to wait and see if it's actually a problem, before fixing it. :)","06/Feb/12 03:35;scode;Something is still fishy. A node that I did a decommission+bootstrap on is not finishing bootstrap and perpetually claiming to wait for schema, with migration stage backed up like below. Investigating.

{code}
""MigrationStage:1"" daemon prio=10 tid=0x00007f63ac0bb000 nid=0xc8a waiting on condition [0x000000004355b000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000508776fa8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2116)
	at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:61)
	at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:122)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}
","06/Feb/12 03:37;scode;And the test is exactly to see whether there is someone active in migration stage:

{code}
    public static boolean isReadyForBootstrap()
    {
        return StageManager.getStage(Stage.MIGRATION).getActiveCount() == 0;
    }
{code}

","06/Feb/12 03:38;scode;It eventually got unstuck, but it took quite some time. I'm not sure why it was having to wait for those responses.","06/Feb/12 05:23;scode;I added logging to see which node it's waiting on a response from, and quickly logged into that node to catch it red handed - it was sitting in the exact same place in the migration manager on the migration stage:

{code}
""MigrationStage:1"" daemon prio=10 tid=0x00007f18ec4dc800 nid=0x1d64 waiting on condition [0x0000000043391000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000050157fdd0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2116)
	at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:61)
	at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:124)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

I guess we're triggering distributed deadlock ""internally"" within the migration stage even though we fixed it so that the gossip stage wouldn't be backed up. If my understanding is correct, this is because all nodes, when a node is marked alive, just know that it has a different schema - not who has the ""newer"" schema. So when a node joins it gets migration messages from others while it also tries to send migration messages to others and waiting on the response. Whenever it sends a migration message to someone whose migration stage is busy waiting on a response from the node in question - deadlock (until timeout).
","06/Feb/12 05:32;scode;What I'd like in this case is to send a request and register a callback with the response that just puts a task on the migration stage that does the schema merge. That, and keep track of outstanding requests and alter the logic for ""is ready to bootstrap"".

I'm still fuzzy on the overall design of messaging; is it me or is there no way to send a request and wait for a response without hogging a thread for it? I want an on-delivery callback, not be waiting for a synchronous get(). A hacky work-around would be to just keep track of all outstanding and get() them in order - it would work, but would be a bit unclean to have one slow request block others from being de-ref:ed.
","09/Feb/12 10:14;slebresne;So if I read this correctly, the initial did fix the initial issue of bootstrap being stuck. I understand it's still not perfect but for the sake of keeping track of changes (in particular what went into 1.1.0), I'm closing this one. Let's fix remaining problems in a separate ticket.","09/Feb/12 10:37;scode;The remaining issue is still causing problems for bootstrap, though not quite as sever as the original problem. Follow-up work filed in CASSANDRA-3882.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assertion error in RowRepairResolver,CASSANDRA-3156,12521845,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,brandon.williams,brandon.williams,08/Sep/11 03:18,12/Mar/19 14:20,13/Mar/19 22:26,08/Sep/11 19:26,1.0.0,,,,,,0,,,,,"Only seems to happen on a coordinator who does not have a copy of the data:

DEBUG 03:15:59,866 Processing response on a callback from 3840@/10.179.64.227
DEBUG 03:15:59,866 Preprocessed data response
DEBUG 03:15:59,866 Processing response on a callback from 3841@/10.179.111.137
DEBUG 03:15:59,866 Preprocessed digest response
DEBUG 03:15:59,865 Processing response on a callback from 3837@/10.179.111.137
DEBUG 03:15:59,865 Preprocessed data response
DEBUG 03:15:59,865 Preprocessed data response
DEBUG 03:15:59,867 Preprocessed digest response
DEBUG 03:15:59,867 resolving 2 responses
ERROR 03:15:59,866 Fatal exception in thread Thread[ReadRepairStage:526,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:77)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR 03:15:59,866 Fatal exception in thread Thread[ReadRepairStage:525,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:77)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR 03:15:59,867 Fatal exception in thread Thread[ReadRepairStage:528,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:77)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
DEBUG 03:15:59,867 resolving 2 responses
DEBUG 03:15:59,867 resolving 2 responses
DEBUG 03:15:59,867 resolving 2 responses",,,,,,,,,,,,,,,,08/Sep/11 19:18;jbellis;3156.txt;https://issues.apache.org/jira/secure/attachment/12493647/3156.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-08 03:51:29.216,,,no_permission,,,,,,,,,,,,4058,,,Thu Sep 08 20:22:17 UTC 2011,,,,,,0|i0gghb:,94100,brandon.williams,brandon.williams,,,,,,,,,"08/Sep/11 03:22;brandon.williams;Also some spurious digest mismatches mixed in, even though I have no reason to suspect there is actually a mismatch in my dev env (3 nodes, rf=2):


DEBUG 03:15:59,823 Digest mismatch:
org.apache.cassandra.service.DigestMismatchException: Mismatch for key DecoratedKey(20580074455139572311737153648595094740, 30363933) (fb3f10b793298382b554737490bc78b5 vs db8d74ec919be7c1a1dda15c85754eb0)
        at org.apache.cassandra.service.RowDigestResolver.resolve(RowDigestResolver.java:105)
        at org.apache.cassandra.service.RowDigestResolver.resolve(RowDigestResolver.java:30)
        at org.apache.cassandra.service.ReadCallback$AsyncRepairRunner.runMayThrow(ReadCallback.java:229)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

But this seems to happen when the coordinator _does_ have a copy of the data.
","08/Sep/11 03:51;jbellis;So, how RR is supposed to work is like this:

Optimistic phase:
Coordinator sends data read to closest replica, digest to others

If there is a mismatch (optimism fail), we go to the repair phase:
Coordinator sends data reads to all replicas to merge + repair

The failing assert is saying ""I got a digest reply, during the repair phase--i.e. we sent a data request but got a digest back.""  

No idea how this is happening.",08/Sep/11 18:20;brandon.williams;Appears to be caused by http://svn.apache.org/viewvc?revision=1151304&view=revision,08/Sep/11 19:18;jbellis;The new threadlocal code was omitting to reset the buffer before each use.  Patch attached.,08/Sep/11 19:23;brandon.williams;+1,08/Sep/11 19:26;jbellis;committed,"08/Sep/11 20:22;hudson;Integrated in Cassandra #1092 (See [https://builds.apache.org/job/Cassandra/1092/])
    reset ReadVerbHandler buffer between uses
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3156

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166865
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/db/ReadVerbHandler.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"trunk is unable to participate with an 0.8 ring, again",CASSANDRA-3144,12521471,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,brandon.williams,brandon.williams,brandon.williams,06/Sep/11 17:04,12/Mar/19 14:20,13/Mar/19 22:26,07/Sep/11 01:57,1.0.0,,,,,,0,,,,,"Title pretty much says it all, looks like a rehash of CASSANDRA-2818 to some degree.",,,,,,,,,,,,,,,,06/Sep/11 22:59;jbellis;3144.txt;https://issues.apache.org/jira/secure/attachment/12493243/3144.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-06 22:59:10.562,,,no_permission,,,,,,,,,,,,4067,,,Wed Sep 07 02:15:39 UTC 2011,,,,,,0|i0ggbz:,94076,jbellis,jbellis,,,,,,,,,"06/Sep/11 22:59;jbellis;As Brandon determined, this is a regression caused by CASSANDRA-1788.",06/Sep/11 23:09;brandon.williams;+1.  I hope this ticket dies in a fire.,07/Sep/11 01:57;jbellis;committed,"07/Sep/11 02:15;hudson;Integrated in Cassandra #1081 (See [https://builds.apache.org/job/Cassandra/1081/])
    use message.version in outbound header instead of MS.version
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3144

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1165957
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HintedHandoff fails to deliver any hints,CASSANDRA-3972,12544412,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,brandon.williams,brandon.williams,brandon.williams,28/Feb/12 12:35,12/Mar/19 14:20,13/Mar/19 22:26,28/Feb/12 16:35,1.1.0,,,,,,0,hintedhandoff,,,,"Summary says it all.  Whether in a memtable or sstable, no hints are delivered.",,,,,,,,,,,,,,,,28/Feb/12 16:29;brandon.williams;3972.txt;https://issues.apache.org/jira/secure/attachment/12516349/3972.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-28 14:35:33.328,,,no_permission,,,,,,,,,,,,229649,,,Tue Feb 28 16:35:36 UTC 2012,,,,,,0|i05h0f:,29853,jbellis,jbellis,,,,,,,,,"28/Feb/12 13:12;brandon.williams;At least one problem here is that CFS.removeDeleted is removing everything here (though listing the hints family in the cli works correctly):

{code}
ColumnFamily hintsPage = ColumnFamilyStore.removeDeleted(hintStore.getColumnFamily(filter), Integer.MAX_VALUE);
{code}

At DEBUG you can see the columns collected, but then hintsPage is null.  If we simply change this to:

{code}
ColumnFamily hintsPage = hintStore.getColumnFamily(filter);
{code}

Then at least some of the hints get sent.",28/Feb/12 14:35;jbellis;removeDeleted should just drop tombstones.,28/Feb/12 14:58;brandon.williams;It's not too surprising then that a very long and arduous bisect points toward CASSANDRA-3716.,"28/Feb/12 15:45;slebresne;CASSANDRA-3716 made it so that purge don't use the current time internally and is only based on the column local deletion time and the value of gcBefore. So indeed, ColumnFamilyStore.removeDeleted(c, Integer.MAX_VALUE) will remove every expiring column since it basically means 'remove everything that will be expired at the end of time'. The current time should be used instead as gcBefore. Not sure what is the remaining problem though if it doesn't fix it fully.",28/Feb/12 16:29;brandon.williams;That does indeed fix it.  Patch to use the current time when filtering tombstones.,28/Feb/12 16:31;jbellis;+1,28/Feb/12 16:35;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unit tests failure,CASSANDRA-3727,12538033,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,,slebresne,slebresne,11/Jan/12 09:39,12/Mar/19 14:20,13/Mar/19 22:26,12/Jan/12 09:25,1.0.7,,,Legacy/Testing,,,0,,,,,"On current 1.0 branch (and on my machine: Linux), I have the following unit test failures:
* CliTest and EmbeddedCassandraTest: they both first kind of pass (JUnit first prints a message with no failures in it), then hang until JUnit timeout and fails with a 'Timeout occurred'. In other word, the tests themselves are passing, but something they do prevents the process to exit cleanly leading to a JUnit timeout. I don't want to discard that as not a problem, because if something can make the process not exit cleanly, this can be a pain for restarts (and in particular upgrades) and hence would be basically a regression. I'm marking the ticket as blocker (for the release of 1.0.7) mostly because of this one.
* SystemTableTest: throws an assertionError. I haven't checked yet, so that could be an easy one to fix.
* RemoveTest: it fails, saying that '/127.0.0.1:7010 is in use by another process' (consistently). But I have no other process running on port 7010. It's likely just of problem of the test, but it's new and in the meantime removes are not tested.
* I also see a bunch of stack trace with errors like:
{noformat}
    [junit] ERROR 10:01:59,007 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1-hc-1-Index.db to /home/mcmanus/Git/cassandra/build/test/cassandra/data/Keyspace1/backups/Indexed1-hc-1-Index.db (errno 17)
{noformat}
(with SSTableReaderTest). This does not make the tests fail, but it is still worth investigating. It may be due to CASSANDRA-3101.",,,,,,,,,,,,,,,,11/Jan/12 17:54;jbellis;3727.txt;https://issues.apache.org/jira/secure/attachment/12510212/3727.txt,11/Jan/12 10:24;xedin;CASSANDRA-3727-CliTest-timeout-fix.patch;https://issues.apache.org/jira/secure/attachment/12510171/CASSANDRA-3727-CliTest-timeout-fix.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-01-11 09:56:16.104,,,no_permission,,,,,,,,,,,,223539,,,Thu Jan 19 15:01:37 UTC 2012,,,,,,0|i0gnhz:,95237,,,,,,,,,,,11/Jan/12 09:56;xedin;CliTest and others should be timeouting because of newly added shutdown hook.,"11/Jan/12 10:07;slebresne;Are you saying that they should be timeouting as in 'having them timeouting is a feature' or are you just pointing out the likely source of the problem? In the latter, do  you remember the ticket that introduced those (or better, have a fix for it)?","11/Jan/12 10:11;xedin;I'm saying that it's likely source of the problem and for CliTest fix would be pretty straightforward, make CliMain to disconnect properly after tests are done (I'm going to attach a patch fixing patch in a few minutes to this ticket), I don't know about other tests tho. ",11/Jan/12 10:24;xedin;fixed CliTest timeout by correctly closing transport connection which allows Cassandra shutdown hook to proceed without waiting for RPC connections to close.,11/Jan/12 14:20;jbellis;Hmm.  Can we fix the thrift shutdown instead to not wait for sockets to be closed nicely?,"11/Jan/12 14:22;xedin;I guess we can, but I thought that it's kind of feature that it does wait...","11/Jan/12 15:58;jbellis;Definitely not, otherwise there is no way to shut down if clients stay connected",11/Jan/12 15:59;jbellis;(The thrift shutdown was introduced for CASSANDRA-3335),"11/Jan/12 16:03;jbellis;bq. Can we fix the thrift shutdown instead to not wait for sockets to be closed nicely

I'm not sure why this isn't how it already works.  From CustomTThreadPoolServer.WorkerProcess:

{code}
.               while (!stopped_ && processor.process(inputProtocol, outputProtocol))
                {
                    inputProtocol = inputProtocolFactory_.getProtocol(inputTransport);
                    outputProtocol = outputProtocolFactory_.getProtocol(outputTransport);
                }
{code}

In other words, as soon as stopped_ is set (by the stop() method), each thread should finish the current request but not accept more.",11/Jan/12 16:09;xedin;It's easy to test - run cassandra and in other terminal session connect to it using CLI and try Ctrl-C Cassandra server without closing CLI.,"11/Jan/12 17:54;jbellis;So, I was over-optimistic in CASSANDRA-3335 when I thought I could get by without a MessagingService shutdown method.  The problem is that although my changes there do work to prevent accepting new connections, and to stop work on existing connections *after the first command post-shutdown*, that's not good enough in this case since the client is just sitting on its connection and never sends another command.

So, this patch renames MS.waitForCallbacks() back to shutdown(), and refuses to add new callbacks after that.  However, the analysis on 3335 that there's no good way to deal with an exception here, so we do this instead in ExpiringMap:

{code}
.   public V put(K key, V value, long timeout)
    {
        if (shutdown)
        {
            // StorageProxy isn't equipped to deal with ""I'm nominally alive, but I can't send any messages out.""
            // So we'll just sit on this thread for until the rest of the server shutdown completes.
            //
            // See comments in CustomTThreadPoolServer.serve, CASSANDRA-3335, and CASSANDRA-3727.
            try
            {
                Thread.sleep(Long.MAX_VALUE);
            }
            catch (InterruptedException e)
            {
                throw new AssertionError(e);
            }
        }
        CacheableObject<V> previous = cache.put(key, new CacheableObject<V>(value, timeout));
        return (previous == null) ? null : previous.getValue();
    }
{code}

Then, we switch the Thrift executor (and all DTPE instances) to use daemon threads, and remove the wait-for-WorkerProcess threads code from CustomTThreadPoolServer.serve:

{code}
.       // Thrift's default shutdown waits for the WorkerProcess threads to complete.  We do not,
        // because doing that allows a client to hold our shutdown ""hostage"" by simply not sending
        // another message after stop is called (since process will block indefinitely trying to read
        // the next meessage header).
        //
        // The ""right"" fix would be to update thrift to set a socket timeout on client connections
        // (and tolerate unintentional timeouts until stopped_ is set).  But this requires deep
        // changes to the code generator, so simply setting these threads to daemon (in our custom
        // CleaningThreadPool) and ignoring them after shutdown is good enough.
        //
        // Remember, our goal on shutdown is not necessarily that each client request we receive
        // gets answered first [to do that, you should redirect clients to a different coordinator
        // first], but rather (1) to make sure that for each update we ack as successful, we generate
        // hints for any non-responsive replicas, and (2) to make sure that we quickly stop
        // accepting client connections so shutdown can continue.  Not waiting for the WorkerProcess
        // threads here accomplishes (2); MessagingService's shutdown method takes care of (1).
        //
        // See CASSANDRA-3335 and CASSANDRA-3727.
{code}

Finally, this patch also updates Memtable's memorymeter thread to use the newly daemonized DTPE for good measure, since there's no reason to ever block shutdown for that either.",11/Jan/12 18:12;brandon.williams;+1,11/Jan/12 18:50;jbellis;committed; leaving open for other test failures,11/Jan/12 19:54;jbellis;SystemTableTest failure taken care of on CASSANDRA-3579,"11/Jan/12 20:01;jbellis;bq. RemoveTest: it fails, saying that '/127.0.0.1:7010 is in use by another process' (consistently). 

FWIW, I get a timeout instead (on Windows).",11/Jan/12 20:05;xedin;I get the same thing Sylvain does in RemoveTest : '/127.0.0.1:7010 is in use by another process' on Mac OS X.,11/Jan/12 20:31;brandon.williams;Same 'in use by another process' under linux.  There is definitely no other process.,"11/Jan/12 22:04;jbellis;RemoveTest is trying to stop/start MessagingService multiple times in the same suite (see setup/tearDown methods).  My guess is that worked well enough pre-CASSANDRA-3335.  I'll see about making it happy again, although this feels fragile.",11/Jan/12 23:01;jbellis;Pushed 452ddf63c530fc573551e6fc9c79c1a876f11dd0 with the socket teardown code added back.  Now it's hitting {{java.io.IOException: Failed to delete c:\Users\Jonathan\projects\cassandra\git\build\test\cassandra\commitlog\CommitLog-1326319330071.log}}.  Progress?,"12/Jan/12 02:46;jbellis;User error...  it was my paused Cassandra instance in my IDE holding that CL segment open.  With that figured out, I also pushed 1e750138177e9cd9cbd6537451a4b5cd301dab3a which allows MS to be restarted by RemoveTest.

All the tests now pass for me, except RecoveryManagerTruncateTest which has failed on Windows for 0.8+.","12/Jan/12 09:25;slebresne;I still had intermittent failure of columnFamilyStoreTest, but because SystemTable.isIndexBuilt() was suffering from the same 'I forgot to expunge tombstones' problem than SystemTable.loadTokens(). I took on myself to commit the same fix for that instance directly (and check no other method had this problem).

So closing this as all tests are now passing.

However I'd be interested to know if anyone else is seeing the 'unable to create link' stack trace during tests, because if so we should probably open another ticket to investigate.","19/Jan/12 14:56;butlermh;Building Cassandra 1.0.7 I get one test failure:

{code}
Class org.apache.cassandra.db.compaction.CompactionsTest
Name	Tests	Errors	Failures	Time(s)	Time Stamp	Host
CompactionsTest	1	1	0	0.000	2012-01-19T12:52:18	mbutler-OptiPlex-990
Tests
Name	Status	Type	Time(s)
testSuperColumnCompactions	Error	Timeout occurred. Please note the time in the report does not reflect the time until the timeout.

junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
	0.001
{code}

and also see exceptions being thrown saying it was unable to create a hard link
{code}

  [junit]  WARN 13:53:33,357 Overriding RING_DELAY to 1000ms
    [junit] ERROR 13:53:39,794 Unable to create hard link
    [junit] com.sun.jna.LastErrorException: errno was 17
    [junit] 	at org.apache.cassandra.utils.CLibrary.link(Native Method)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:146)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 13:53:39,796 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Standard1-hc-1-Digest.sha1 to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Standard1-hc-1-Digest.sha1 (errno 17)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Standard1-hc-1-Digest.sha1 to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Standard1-hc-1-Digest.sha1 (errno 17)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:160)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 8 more
    [junit] ERROR 13:53:39,797 Unable to create hard link
    [junit] com.sun.jna.LastErrorException: errno was 17
    [junit] 	at org.apache.cassandra.utils.CLibrary.link(Native Method)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:146)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 13:53:39,798 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Standard1-hc-2-Filter.db to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Standard1-hc-2-Filter.db (errno 17)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Standard1-hc-2-Filter.db to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Standard1-hc-2-Filter.db (errno 17)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:160)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 8 more
    [junit] ERROR 13:53:41,501 Unable to create hard link
    [junit] com.sun.jna.LastErrorException: errno was 17
    [junit] 	at org.apache.cassandra.utils.CLibrary.link(Native Method)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:146)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 13:53:41,502 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1.626972746864617465-hc-1-Filter.db to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1.626972746864617465-hc-1-Filter.db (errno 17)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1.626972746864617465-hc-1-Filter.db to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1.626972746864617465-hc-1-Filter.db (errno 17)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:160)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 8 more
    [junit] ERROR 13:53:41,504 Unable to create hard link
    [junit] com.sun.jna.LastErrorException: errno was 17
    [junit] 	at org.apache.cassandra.utils.CLibrary.link(Native Method)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:146)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 13:53:41,505 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1-hc-1-Digest.sha1 to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1-hc-1-Digest.sha1 (errno 17)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1-hc-1-Digest.sha1 to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1-hc-1-Digest.sha1 (errno 17)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:160)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 8 more
    [junit] ERROR 13:53:41,802 Unable to create hard link
    [junit] com.sun.jna.LastErrorException: errno was 17
    [junit] 	at org.apache.cassandra.utils.CLibrary.link(Native Method)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:146)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 13:53:41,802 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1.626972746864617465-hc-2-Digest.sha1 to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1.626972746864617465-hc-2-Digest.sha1 (errno 17)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1.626972746864617465-hc-2-Digest.sha1 to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1.626972746864617465-hc-2-Digest.sha1 (errno 17)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:160)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 8 more
    [junit] ERROR 13:53:41,804 Unable to create hard link
    [junit] com.sun.jna.LastErrorException: errno was 17
    [junit] 	at org.apache.cassandra.utils.CLibrary.link(Native Method)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:146)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 13:53:41,805 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1-hc-1-Filter.db to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1-hc-1-Filter.db (errno 17)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1-hc-1-Filter.db to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1-hc-1-Filter.db (errno 17)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:160)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 8 more
    [junit] ------------- ---------------- ---------------
{code}

Any suggestions on a fix?",19/Jan/12 15:01;jbellis;See CASSANDRA-3735 for the hard link fix,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush Assertion Error - CF size changed during serialization,CASSANDRA-3482,12531051,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,dhendry,dhendry,10/Nov/11 21:05,12/Mar/19 14:07,13/Mar/19 22:26,11/Nov/11 17:56,1.0.3,,,,,,0,,,,,"I have seen the following assert in the logs - there are no other suspicious or unexpected log messages.

INFO [FlushWriter:9] 2011-11-10 13:08:58,882 Memtable.java (line 237) Writing Memtable-UserData@1388955390(25676955/430716097 serialized/live bytes, 478913 ops)
ERROR [FlushWriter:9] 2011-11-10 13:08:59,513 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[FlushWriter:9,5,main]
java.lang.AssertionError: CF size changed during serialization: was 4 initially but 3 written
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:94)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeWithIndexes(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:177)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:264)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:47)
        at org.apache.cassandra.db.Memtable$4.runMayThrow(Memtable.java:289)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Once the error occurs, further MemtablePostFlusher tasks are blocked:

nodetool tpstats:
  Pool Name                    Active   Pending      Completed   Blocked  All time blocked
  MemtablePostFlusher               1        18             16         0                 0

It *seems* that all further flushed for the particular CF (in this case UserData) will also result in the same assertion error. Restarting the node fixes the problem.","RHEL 6
java version ""1.6.0_26""
6 node cluster",,,,,,,,,,,,,,,11/Nov/11 16:38;jbellis;3482.txt;https://issues.apache.org/jira/secure/attachment/12503383/3482.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-11 16:38:09.576,,,no_permission,,,,,,,,,,,,216789,,,Fri Nov 11 17:56:43 UTC 2011,,,,,,0|i0gkg7:,94743,slebresne,slebresne,,,,,,,,,"10/Nov/11 21:13;dhendry;I should also point out that the number of columns being logged in the error, 3/4, is MUCH smaller than the expected column count based on the flushed bytes.

Just based on the usage pattern for this CF and reading the description for CASSANDRA-2503, I suspect (without any real evidence) they may be related.","10/Nov/11 21:22;dhendry;Oh, and the problem crops up again with 1h of restarting the node so. ""Restarting the node fixes the problem"" isnt exactly true.","11/Nov/11 15:40;dhendry;Pretty sure CASSANDRA-2503 introduced this problem. 

My experimental evidence at this point (since I have not traced through the code) is that 2 out of 2 times the node was started using 1.0.2 (release), the assert cropped up within 1 hour of going live (first time was after 59 mins, second was after 23 mins). I reverted the changes introduced in CASSANDRA-2503 (still running 1.0.2), and the node has now been up for 17 hours with no problems.",11/Nov/11 16:38;jbellis;I bet you're right.  Patch attached to fix the race introduced by 2503.,"11/Nov/11 17:56;slebresne;+1, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression option chunk_length is not converted into KB as it should,CASSANDRA-3492,12531368,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,14/Nov/11 20:58,12/Mar/19 14:07,13/Mar/19 22:26,15/Nov/11 07:41,1.0.3,,,,,,0,compression,,,,,,,,,,,,,,,,,,,,14/Nov/11 20:58;slebresne;0001-Fix-chunk-length-option.patch;https://issues.apache.org/jira/secure/attachment/12503674/0001-Fix-chunk-length-option.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-14 21:11:58.937,,,no_permission,,,,,,,,,,,,217104,,,Tue Nov 15 09:08:38 UTC 2011,,,,,,0|i0gkkn:,94763,xedin,xedin,,,,,,,,,14/Nov/11 20:58;slebresne;Oups,14/Nov/11 21:11;xedin;+1,"14/Nov/11 21:32;michaelsembwever;Does this involve a `nodetool compact/scrub` to repair existing sstables?
(the answer is yes).",15/Nov/11 07:41;slebresne;Committed,15/Nov/11 09:08;michaelsembwever;Looks good. Back to running w/ Xmx8g :-),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrap is broken in 1.0.0-rc1,CASSANDRA-3285,12525291,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,30/Sep/11 10:53,12/Mar/19 14:07,13/Mar/19 22:26,30/Sep/11 15:58,1.0.0,,,,,,0,bootstrap,,,,"The commit of #3219 introduced two bugs: the condition to bootstrap is that there *are* non-system tables instead, a _not_ is missing, and the setToken() was wrongly push up into the ""I'm not bootstrapping"" block so a boostrapping node was left in the joining state.",,,,,,,,,,,,,,,,30/Sep/11 10:53;slebresne;fix-boostrap.patch;https://issues.apache.org/jira/secure/attachment/12497138/fix-boostrap.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-30 15:17:02.325,,,no_permission,,,,,,,,,,,,40997,,,Fri Sep 30 15:58:18 UTC 2011,,,,,,0|i0gi1j:,94353,jbellis,jbellis,,,,,,,,,30/Sep/11 15:17;jbellis;+1,"30/Sep/11 15:58;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CfDef can default to an invalid id and fail during system_add_column_family,CASSANDRA-3288,12525372,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,zznate,zznate,30/Sep/11 22:12,12/Mar/19 14:07,13/Mar/19 22:26,01/Oct/11 05:37,1.0.0,,,Legacy/CQL,,,0,,,,,"The line from this commit:
https://github.com/apache/cassandra/commit/38e3e85b121ba6308ba3ceb26312d12ed0d609ec#L1R683

Introduced an issue in that some clients, particularly Hector, will send a CfDef with an ID having been set to 0. Done via the CfDef#setId, the isSetId bit is flipped to true, causing error if schemaId of 0 already exists, which given the use case, is likely. 

Since we know the context of a system_create_column_family, this can be sidestepped by just stepping on whatever ID is there (irrelevant on a create anyway) with the value returned from: Schema.instance.nextCFId()",,,,,,,,,,,,,,,,30/Sep/11 22:16;jbellis;3288.txt;https://issues.apache.org/jira/secure/attachment/12497231/3288.txt,30/Sep/11 23:18;zznate;3822v2.txt;https://issues.apache.org/jira/secure/attachment/12497243/3822v2.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-30 22:16:44.417,,,no_permission,,,,,,,,,,,,41673,,,Sat Oct 01 05:37:42 UTC 2011,,,,,,0|i0gi33:,94360,zznate,zznate,,,,,,,,,30/Sep/11 22:16;jbellis;patch to explicitly ignore client-set ids on create,30/Sep/11 22:20;jbellis;first reported on http://www.mail-archive.com/user@cassandra.apache.org/msg17649.html,30/Sep/11 23:18;zznate;v2 adds same ignore for system_add_keyspace,01/Oct/11 05:37;jbellis;committed v2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assert err on ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:126),CASSANDRA-3289,12525378,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,cywjackson,cywjackson,30/Sep/11 23:06,12/Mar/19 14:07,13/Mar/19 22:26,01/Oct/11 15:04,1.0.0,,,,,,0,,,,,"I have the following in trunk:

RowKey: b
=> (column=a, value=38383838383838383838, timestamp=1317421952793000)
=> (column=d, value=617364646661736466, timestamp=1317420968944000)
=> (column=e, value=38383838383838383838, timestamp=1317421096152000)
=> (column=f, value=33343334333433343334, timestamp=1317422838818000)
=> (column=g, value=33343334333433343334, timestamp=1317422565130000)
=> (column=i, value=33343334333433343334, timestamp=1317422879258000)
=> (column=j, value=33343334333433343334, timestamp=1317422814873000)
=> (column=o, value=33343334333433343334, timestamp=1317422867106000)
=> (column=x, value=33343334333433343334, timestamp=1317422394097000)
=> (column=z, value=38383838383838383838, timestamp=1317421982057000)

Keyspace: testks:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [168:1]
  Column Families:
    ColumnFamily: testcf
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy

every thing is flushed to the sstables, but not in the same sstables, and the columns are in some what 'random' form:

/var/lib/cassandra/data/testks/testcf-h-10-Data.db
{
""61"": [[""0c"",""76"",1317405903119000], [""0d"",""76"",1317405977002000], [""7a"",""38383838383838383838"",1317422276322000]],
""62"": [[""61"",""38383838383838383838"",1317421952793000], [""63"",""4e864303"",1317421827329000,""d""], [""64"",""617364646661736466"",1317420968944000], [""65"",""38383838383838383838"",1317421096152000], [""67"",""33343334333433343334"",1317422565130000], [""78"",""33343334333433343334"",1317422394097000], [""7a"",""38383838383838383838"",1317421982057000]]
}
/var/lib/cassandra/data/testks/testcf-h-12-Data.db
{
""62"": [[""6a"",""33343334333433343334"",1317422814873000]]
}
/var/lib/cassandra/data/testks/testcf-h-13-Data.db
{
""62"": [[""66"",""33343334333433343334"",1317422838818000]]
}
/var/lib/cassandra/data/testks/testcf-h-14-Data.db
{
""62"": [[""6f"",""33343334333433343334"",1317422867106000]]
}
/var/lib/cassandra/data/testks/testcf-h-15-Data.db
{
""62"": [[""69"",""33343334333433343334"",1317422879258000]]
}


then i basically make a call to get key=b with all the column names (yes included column names that didn't exist to save time):

ColumnFamilyResult<String, String> queryColumns = template.queryColumns(""b"", Arrays.asList(""a"",""b"",""c"",""d"",""e"",""f"",""g"",""h"",""i"",""j"",""k"",""l"",""m"",""n"",""o"",""p"",""q"",""r"",""s"",""t"",""u"",""v"",""w"",""x"",""y"",""z""));

(let me know if it would be easier to just upload the sstables to the ticket)",,,,,,,,,,,,,,,,01/Oct/11 14:16;jbellis;3289-v2.txt;https://issues.apache.org/jira/secure/attachment/12497361/3289-v2.txt,01/Oct/11 02:14;jbellis;3289.txt;https://issues.apache.org/jira/secure/attachment/12497262/3289.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-01 02:01:19.441,,,no_permission,,,,,,,,,,,,41740,,,Sat Oct 01 15:04:25 UTC 2011,,,,,,0|i0gi3j:,94362,slebresne,slebresne,,,,,,,,,30/Sep/11 23:08;cywjackson;should mention a workaround is to do major compaction to stack everything back to a single sstable,"01/Oct/11 02:01;jbellis;The full trace:
{noformat}
java.lang.AssertionError: Added column does not sort as the last column
        at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:126)
        at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:123)
        at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:118)
        at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:116)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:61)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1297)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1147)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1114)
        at org.apache.cassandra.db.Table.getRow(Table.java:388)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:58)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:795)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}","01/Oct/11 02:14;jbellis;collectTimeOrderedData has to use a column store that allows random inserts.  Patch attached to use ThreadSafeSortedColumns there, and ArrayBacked for collectAllData.","01/Oct/11 13:33;slebresne;Unfortunately this doesn't work in the case of reading to put in the cache, because in that case, we need to always use ThreadSafeSortedColumns (in CFS, line 1128). I see a few solutions:
  * make ABSC work even if the input doesn't arrive in sorted order (i.e, remove the assertion since the code is already there). We had this debate already, I still think that it is a solution that make sense and note that in that case it would likely still be the more efficient solution. But I can understand that feeling is not shared.
  * make CollactionController only use non thread-safe CF (a little bit like the attached patch, but using TreeMapSortedColumns in the collectTimeOrderedData) *and* add a  cloneMe towards TSSC in the 'read for cache' case. But that involves a copy of the CF.
  * instead of passing the Factory around, pass a boolean indicating if the returned CF should be thread safe or not. In that way, if the boolean is true, both collectTimeOrderedData and collectAddData would use TSSC, otherwise, cTOD would use TMSC and cAD would use ABSC.

Again, I think the first one has my preference, but if we're afraid that by doing so we'll start using ABSC in places where it's clearly not the most efficient solution, then the last solution is probably the best one.

But I would agree that this passing of Factory around is not very easy to use correctly and we should probably come up with a cleaner solution.
",01/Oct/11 14:16;jbellis;v2 attached.  you will perhaps not be surprised that I prefer the boolean approach. :),"01/Oct/11 14:52;slebresne;bq. v2 attached

+1

bq. you will perhaps not be surprised that I prefer the boolean approach

nothing ventured, nothing gained :)",01/Oct/11 15:04;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in sstable reference counting,CASSANDRA-3085,12520293,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,26/Aug/11 19:22,12/Mar/19 14:07,13/Mar/19 22:26,29/Aug/11 20:29,1.0.0,,,,,,0,,,,,"DataTracker gives us an atomic View of memtable/sstables, but acquiring references is not atomic.  So it is possible to acquire references to an SSTableReader object that is no longer valid, as in this example:

View V contains sstables {A, B}.  We attempt a read in thread T using this View.
Meanwhile, A and B are compacted to {C}, yielding View W.  No references exist to A or B so they are cleaned up.
Back in thread T we acquire references to A and B.  This does not cause an error, but it will when we attempt to read from them next.",,,,,,,,,,,,,,,,28/Aug/11 21:58;jbellis;3085-v2.txt;https://issues.apache.org/jira/secure/attachment/12492015/3085-v2.txt,26/Aug/11 20:59;jbellis;3085.txt;https://issues.apache.org/jira/secure/attachment/12491836/3085.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-29 15:55:07.287,,,no_permission,,,,,,,,,,,,20962,,,Sat Sep 17 00:26:54 UTC 2011,,,,,,0|i0gfhr:,93940,slebresne,slebresne,,,,,,,,,26/Aug/11 20:59;jbellis;Turns out this is only a problem for where we're operating on sub-View granularity post-CASSANDRA-1608: CFS.markCurrentViewReferenced was correctly retrying until it got a consistent set of references.,"28/Aug/11 21:56;jbellis;v2 encapsulates the lockless atomic acquisition in CFS.markReferenced(Interval).

Not 100% sure how important the changes to the getRangeSlice tokens were, that I took out. :)

If we need those, we might need to make getRangeSlice loop manually w/o the encapsulation, since we need the view to compute the Interval, but we need the Interval to search for sstables.","29/Aug/11 15:55;slebresne;* The try of the try...finally block should start just after the markReferenced to be safe in CollationControler.
* In CFS.getRangeSlice, I am not confortable using the same try...finally block for both the view and the iterator. RowIteratorFactory.getIterator() does make seeks and could throw an error that would leave a bunch of sstable referenced.
* Nit: Maybe we could use a plain old DataTracker.View instead of introducing ViewFragment, since the View constructor is accessible to CFS anyway ?","29/Aug/11 16:02;jbellis;It seems weird to me to overload the purpose of a full View for this: it's not meant to be updated, nor does it contain the full set of sstables.  I think I'd rather use a different class so that View's purpose remains clear.","29/Aug/11 16:16;slebresne;I would argue that even now a View is not meant to be updated either (we pick new Views but a View itself is fixed), nor does it really ensure that it contains the full set of sstables in a way since that set is a moving target. But I suppose this is just a matter of perspective on what View is, and I'm fine with ViewFragment. ","29/Aug/11 20:29;jbellis;committed with requested fixes, fix for interval computation involving stopAt.isEmpty(), and javadoc for CFS.markReferenced","29/Aug/11 21:17;hudson;Integrated in Cassandra #1055 (See [https://builds.apache.org/job/Cassandra/1055/])
    fix race condition in sstable reference counting
patch by jbellis; reviewed by slebresne for CASSANDRA-3085

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1162988
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/CollationController.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowIteratorFactory.java
","14/Sep/11 23:17;yangyangyyy;bq. but it will when we attempt to read from them next.

Jonathan: since the sstable deletion code is wrapped inside SSTableReader.releaseReference(), I thought as long as anyone is holding a reference to the SSTableReader, the file would not be deleted? could you please explain a bit?

Thanks
Yang","14/Sep/11 23:58;jbellis;Right.  A sstable starts with one reference.  When we do a read, we acquire a reference, then release when we're done.  When we compact it, we release a reference.  

So we can do arbitrary numbers of reads w/o reference count getting to zero, but once we compact, either the compact release or a read release will drop it to zero. This last release will unmap and delete it, whether from compaction or a read.","15/Sep/11 08:37;yangyangyyy;Thanks Jonathan.

but I still can't see why the old code would cause errors, could you please see if the following reasoning makes sense?



if you look at the operations of +1 and -1 by read paths and the compaction path, either the read path or the compaction can be seen as the following sequence

+1

//access the SSTableReader

-1


where for the compaction the ""+1"" happens at creation of the SSTableReader; for read paths the ""+1"" happens at acquireReference()

since every path (either compaction or reader) does one +1 and one -1, by the time a path finishes, the ref count will be equal to the number of live code paths

if the file is removed, the ref count must be 0, hence live paths count at that moment must be 0. if there are no future paths to run, it's all good. if there are , the path would access a file already removed and we have a problem. but this is impossible because: if  the +1 comes after compaction.release(), because *compaction.release() comes after the view change in DataTracker.replace()*, then reader path +1 comes after DataTracker.replace(),  but this is impossible because the read path can not see that SSTableReader in its view.

","15/Sep/11 19:59;jbellis;bq. since every path (either compaction or reader) does one +1 and one -1

Not so -- you're missing the ""this sstable is compacted and is obsolete"" release done by DataTracker.","16/Sep/11 07:35;yangyangyyy;
bq.  Not so – you're missing the ""this sstable is compacted and is obsolete"" release done by DataTracker.


do you mean this part in DataTracker ? (line 262)

    private void replace(Collection<SSTableReader> oldSSTables, Iterable<SSTableReader> replacements)
    {
        View currentView, newView;
        do
        {
            currentView = view.get();
            newView = currentView.replace(oldSSTables, replacements);
        }
        while (!view.compareAndSet(currentView, newView));

        addNewSSTablesSize(replacements);
        removeOldSSTablesSize(oldSSTables); //<==== this calls releaseReference() 

        notifySSTablesChanged(replacements, oldSSTables);
        cfstore.updateCacheSizes();
    }

the SSTableReaders which do a releaseReference() are all in the ""oldSSTables"" set, the newView takes out all the oldSSTables in the newView computation. Since the view change is done atomically in view.compareAndSet(), each SSTableReader can invoke the releaseReference() only once in this path, since once it's called releaseReference(), it must have been removed from the view.

so, for this path, each SSTableReader can invoke the releaseReference() once in its life time. that's what I   mean: you can ""view"" the compaction path as  doing  a ""+"" at the construction of SSTableReader, and doing a corresponding ""-"" at the code above.


also 
StreamingOutSession.close() calls releaseReference() without acquireReference(), would that cause a problem ? similar calls are StreamingOut.createPendingFiles() , StreamingOutSession.startNext()


Thanks a lot for your patience, please pardon my numerous questions, I just want to make sure that I thoroughly understand it and there are no hidden issues.
","16/Sep/11 08:39;slebresne;bq. but I still can't see why the old code would cause errors

I don't think it was, the report of this issue is not totally correct, but the code was fairly ugly: badly encapsulated and potentially more inefficient that it needs to.

bq. StreamingOutSession.close() calls releaseReference() without acquireReference(), would that cause a problem ? similar calls are StreamingOut.createPendingFiles() , StreamingOutSession.startNext()

It's not a problem because those releaseReference calls do are paired with acquireReference calls (or rather they are supposed to be and last time I checked all seemed ok). The only thing is that in that case, the acquireReference calls are unfortunately not near (from a code point of view) to their paired releaseReference. But we have no choice with how the code of streaming is structured right now. Typically for a streamOutSession, references are acquired in either StreamingRepairTask or StreamOut.transferRanges and released as soon as the sstable is not needed, that is either in createPendingFiles() if it happens the sstable has none of the ranges we want to stream or in next()/close() when the sstable has been fully streamed (and thus won't been accessed by this streamOutSession).

The only small annoying thing with Streaming is that since acquire/release is not enclosed within a try ... finally block, we could leave a sstable marked forever if an error occurs during streaming. CASSANDRA-3112 includes code that would basically allow to deal with this. 
","17/Sep/11 00:26;yangyangyyy;thanks guys, I'm clear now",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trunk single-pass streaming doesn't handle large row correctly,CASSANDRA-3003,12518240,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,yukim,slebresne,slebresne,08/Aug/11 19:52,12/Mar/19 14:07,13/Mar/19 22:26,05/Sep/11 14:57,1.0.0,,,,,,0,streaming,,,,"For normal column family, trunk streaming always buffer the whole row into memory. In uses
{noformat}
  ColumnFamily.serializer().deserializeColumns(in, cf, true, true);
{noformat}
on the input bytes.
We must avoid this for rows that don't fit in the inMemoryLimit.

Note that for regular column families, for a given row, there is actually no need to even recreate the bloom filter of column index, nor to deserialize the columns. It is enough to filter the key and row size to feed the index writer, but then simply dump the rest on disk directly. This would make streaming more efficient, avoid a lot of object creation and avoid the pitfall of big rows.

Counters column family are unfortunately trickier, because each column needs to be deserialized (to mark them as 'fromRemote'). However, we don't need to do the double pass of LazilyCompactedRow for that. We can simply use a SSTableIdentityIterator and deserialize/reserialize input as it comes.",,,,,,,,,,,,,,,,30/Aug/11 15:42;yukim;3003-v3.txt;https://issues.apache.org/jira/secure/attachment/12492255/3003-v3.txt,04/Sep/11 15:25;yukim;3003-v5.txt;https://issues.apache.org/jira/secure/attachment/12492974/3003-v5.txt,21/Aug/11 13:05;yukim;ASF.LICENSE.NOT.GRANTED--3003-v1.txt;https://issues.apache.org/jira/secure/attachment/12491091/ASF.LICENSE.NOT.GRANTED--3003-v1.txt,26/Aug/11 15:52;yukim;ASF.LICENSE.NOT.GRANTED--3003-v2.txt;https://issues.apache.org/jira/secure/attachment/12491798/ASF.LICENSE.NOT.GRANTED--3003-v2.txt,31/Aug/11 16:45;yukim;v3003-v4.txt;https://issues.apache.org/jira/secure/attachment/12492476/v3003-v4.txt,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-08-10 07:06:27.294,,,no_permission,,,,,,,,,,,,20932,,,Mon Sep 05 15:23:41 UTC 2011,,,,,,0|i0ges7:,93825,slebresne,slebresne,,,,,,,,,"08/Aug/11 19:55;slebresne;Marking critical, because at least for counter column family, when the row is larger than the inMemoryLimit, the code will actually crash because it will use lazilyCompactedRow which will try to do it's 2 passes.","10/Aug/11 07:06;stuhood;Oof... I don't know how I missed this one in review: very, very sorry Yuki/Sylvain.

Perhaps we can use this as an opportunity to switch to using only PrecompactedRow (for narrow rows which might go to cache) or EchoedRow (for wide rows, which go directly to disk)?

In order to use EchoedRow, we'd have to move where we do CounterContext cleanup: I've suggested in the past that it could be done at read time if we added ""fromRemote"" as a field in the metadata of an SSTable. Every SSTable*Iterator would be affected, because they'd need to respect the fromRemote field.

Alternatively, we could revert 2920 and 2677 (which I would hate: this has been a huge cleanup).

EDIT: Oops, apparently I didn't review this one. Anyway!","10/Aug/11 09:28;slebresne;bq. In order to use EchoedRow, we'd have to move where we do CounterContext cleanup

I really think it is not very hard to do 'inline'. We really just want to deserialize, cleanup, reserialize. It should be super easy to add some ""CounterCleanedRow"" that does that.

bq. at it could be done at read time if we added ""fromRemote"" as a field in the metadata of an SSTable

Yes, but it does sound a bit complicated to me compared to doing the cleanup right away during streaming. It would also be less efficient, because until we have compacted the streamed sstable, each read will have to call the cleanup over and over, while we really only care to have it done twice (unless we completely change where we do cleanup).

bq. Perhaps we can use this as an opportunity to switch to using only PrecompactedRow (for narrow rows which might go to cache) or EchoedRow (for wide rows, which go directly to disk)?

I agree in that there is no point in doing manual deserialization there. About the PrecompactedRow for narrow rows which might go to cache, I'll just precise that it is worth using PrecompactedRow only if 1) we are doing AES streaming and 2) the row is in cache in the first place (which we can know since we always at least deserialize the row key).","11/Aug/11 00:16;stuhood;bq. I really think it is not very hard to do 'inline'. We really just want to deserialize, cleanup, reserialize. It should be super easy to add some ""CounterCleanedRow"" that does that.
I'm probably missing something, but isn't the problem that this can't be done without two passes for rows that are too large to fit in memory? And you can't perform two passes without buffering data somewhere? I suggested removing the cleanup step out of streaming because then the row could be echoed to disk without modification.

bq. It would also be less efficient, because until we have compacted the streamed sstable, each read will have to call the cleanup over and over
This is true, but compaction is fairly likely to trigger soon after a big batch of streamed files arrives, since they will trigger compaction thresholds.","11/Aug/11 01:51;yukim;Stu, Sylvan,

Let me try to fix this by using EchoedRow to serialize directly to disk, and creating new ""CounterCleanedRow"" suggested by Sylvain above.","11/Aug/11 07:41;slebresne;bq. I'm probably missing something, but isn't the problem that this can't be done without two passes for rows that are too large to fit in memory?

Hum true. What we need to do is deserialize each row with the 'fromRemote' flag on so that the delta are cleaned up, and them reserialize the result. But that will potentially reduce the column serialized size (and thus modify the row total size and the column index). Now we could imagine to remember the offset of the beginning of the row, to load the column index in memory and update it during the first pass (it would likely be ok to simply update the index offsets without changing the index structure itself), and to seek back at the end to write the updated data size and column index. However, this unfortunately won't be doable with the current SequentialWriter (and CompressedSequentialWriter) since we cannot seek back (without truncating). Retrospectively, it would have been nicer to have the cleaning of a counter context not change its size :(

So yeah, it sucks. I'm still mildly fan of moving the cleanup because it ""feels wrong"" somehow. It feels it would be better to have that delta cleaning done sooner than latter. But this may end up being the simplest/more efficient solution.","11/Aug/11 13:33;jbellis;bq. it would have been nicer to have the cleaning of a counter context not change its size

Can we pad it somehow?","11/Aug/11 14:06;slebresne;bq. Can we pad it somehow?

It's doable. Basically a context is an array of shards, with a header that is a (variable) list of which of those shards are a delta. When we cleanup the delta we remove the header basically. We could have a specific cleanup for streaming that just set all the header to -1. But we probably want to do that only for the cleanup during streaming, and have compaction clean those afterwards, otherwise it is ugly. I don't know how much easier it is than cleaning during reads, though it avoids having to add a new info for sstable metadata.","19/Aug/11 20:24;jbellis;How is this looking, Yuki?","21/Aug/11 13:05;yukim;Instead of creating CounterCleanedRow, I added appendFromStream method to SSTW, which handles both normal and counter column.

I still need to work on SSTII because attached patch causes problem when iterating over cleaned up CounterColumns with 0-padding added during streaming.
That also causes StreamingTransferTest fail.

Will post update version soon.","26/Aug/11 15:52;yukim;V2 attached and ready for the review.
For Counter columns, instead of padding in place of removed delta, v2 just ""mark"" the counter column to clear delta later, by multiplying #elt by -1 in order to keep the header size for later removal. Marking only occur when deserialize ""fromRemote"", and actual removal of delta is done when reading again from disk after the streaming.","26/Aug/11 16:52;jbellis;Does CounterColumn.create work for both ""normal,"" non-streamed counter updates, as well as streaming?  Or do we need two distinct paths there?",27/Aug/11 01:19;yukim;Looks like we need distinct paths. Counter reads from remote in the read path also get marked (have negative #elt) and may cause problem. I'll take a look.,"30/Aug/11 15:42;yukim;v3 attached. It marks counter column to delete delta after deserializing it from stream without clearing all delta. In this way, marking does not affect regular counter update.","30/Aug/11 23:04;yukim;In v3, I forgot to handle the case where counter columns inside super column. I'll update soon.",31/Aug/11 16:45;yukim;Added handling of counters inside SuperColumn.,"31/Aug/11 18:26;slebresne;I think this is a little bit sad to deserialize all the columns in the non-counter case. We do need to do it right now because of the computation of the max timestamp, but maybe we could have the other side send use the max timestamp as part of the stream header (but I agree, it's a bit more complicated).

For the record, the handling of counter columns amounts to the initial proposition of Stu of moving the cleanup to the reads (though the solution is slightly different). So the ""we'll cleanup on each read before the sstable is compacted"" remark does hold here, but I don't see a better solution right now and the ""those sstables will likely be compacted quickly"" argument probably make this ok anyway.

Other comments:
* we need to use Integer.MIN_VALUE as the value for expireBefore when deserializing the columns, otherwise the expired columns will be converted to DeletedColumns, which will change there serialized size (and thus screw up the data size and column index)
* for markDeltaAsDeleted, we must check if the length is already negative and leave it so if it is, otherwise if a streamed sstable get re-streamed to another node before it was compacted, we could end up not cleaning the delta correctly.
* it would be nice in SSTW.appendFromStream() to assert the sanity of our little deserialize-reserialize dance and assert what we did write the number of bytes that we wrote in the header.
* the patch change a clearAllDelta to a markDeltaAsDeleted in CounterColumnTest which is bogus (and the test does fail with that change).
* I would markDeltaAsDeleted to markForClearingDelta as this describe what the function does better
* nitpick: there is a few space at end of lines in some comments (I know I know, I'm picky).
","04/Sep/11 15:37;yukim;Sylvain,

Thank you for the review.
For now, I leave the max timestamp calculation part as it is done during streaming.

bq. we need to use Integer.MIN_VALUE as the value for expireBefore when deserializing the columns, otherwise the expired columns will be converted to DeletedColumns, which will change there serialized size (and thus screw up the data size and column index)

Fixed.

bq. for markDeltaAsDeleted, we must check if the length is already negative and leave it so if it is, otherwise if a streamed sstable get re-streamed to another node before it was compacted, we could end up not cleaning the delta correctly.

bq. it would be nice in SSTW.appendFromStream() to assert the sanity of our little deserialize-reserialize dance and assert what we did write the number of bytes that we wrote in the header.

Nice point. I added the same assertion as other append() does.

bq. the patch change a clearAllDelta to a markDeltaAsDeleted in CounterColumnTest which is bogus (and the test does fail with that change).

I forgot to revert this one. I should have run test before submitting...

bq. I would markDeltaAsDeleted to markForClearingDelta as this describe what the function does better

Fixed.

bq. nitpick: there is a few space at end of lines in some comments (I know I know, I'm picky).

Fixed this one too, I guess.","05/Sep/11 14:57;slebresne;lgtm, +1.

Committed with a tiny change to use a cheaper array backed column family in appendToStream, since we deserialize in order (and in a single thread).","05/Sep/11 15:23;hudson;Integrated in Cassandra #1074 (See [https://builds.apache.org/job/Cassandra/1074/])
    Handle large rows with single-pass streaming
patch by yukim; reviewed by slebresne for CASSANDRA-3003

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1165306
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/CounterColumn.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/context/CounterContext.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Odd flush behavior,CASSANDRA-3203,12522964,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,brandon.williams,brandon.williams,13/Sep/11 21:49,12/Mar/19 14:07,13/Mar/19 22:26,14/Sep/11 23:09,1.0.0,,,,,,0,,,,,"Given the same workload against 0.8, trunk is creating more than twice the amount of sstables.  Even though a uniform stress workload is being generated, flush size degrades quickly:

{noformat}
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:22,878 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@2058235391(7741
035/110172631 serialized/live bytes, 151785 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:24,888 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1520390052(3887
220/72403158 serialized/live bytes, 76220 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:26,890 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1868496516(4097
085/76255481 serialized/live bytes, 80335 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:28,893 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@498232521(43513
20/80922269 serialized/live bytes, 85320 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:29,895 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1592308290(2310
810/44514839 serialized/live bytes, 45310 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:30,897 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@775439677(22684
80/64984390 serialized/live bytes, 44480 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:31,899 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@928217914(26741
85/76231422 serialized/live bytes, 52435 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:32,901 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@158103119(27511
95/77317732 serialized/live bytes, 53945 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:33,903 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@2035169258(3132
420/88934701 serialized/live bytes, 61420 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:34,905 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1097314626(2979
675/83651699 serialized/live bytes, 58425 ops)
{noformat}

The serialized to live size ratio appears completely out of whack.",,,,,,,,,,,,,,,,14/Sep/11 04:32;jbellis;3203-prelim.txt;https://issues.apache.org/jira/secure/attachment/12494378/3203-prelim.txt,14/Sep/11 20:34;jbellis;3203-v2.txt;https://issues.apache.org/jira/secure/attachment/12494507/3203-v2.txt,15/Sep/11 01:21;brandon.williams;3203.png;https://issues.apache.org/jira/secure/attachment/12494549/3203.png,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-09-14 04:32:14.662,,,no_permission,,,,,,,,,,,,4024,,,Fri Sep 16 05:09:20 UTC 2011,,,,,,0|i0gh1z:,94193,brandon.williams,brandon.williams,,,,,,,,,"14/Sep/11 00:56;brandon.williams;Here is what I've noticed:

Things proceed normally for a while:
{noformat}
 INFO 00:50:16,801 Enqueuing flush of Memtable-Standard1@304913075(36443580/45554475 serialized/live bytes, 714580 ops)
{noformat}

Eventually, something bad happens:
{noformat}
 INFO 00:51:00,001 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='Counter1') (estimated 0 bytes)
 INFO 00:51:00,002 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='Super1') (estimated 0 bytes)
 INFO 00:51:00,002 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='SuperCounter1') (estimated 0 bytes)
 INFO 00:51:00,002 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='Standard1') (estimated 88291581 bytes)
 INFO 00:51:00,004 Enqueuing flush of Memtable-Standard1@741697653(2646900/88291581 serialized/live bytes, 51900 ops)
 INFO 00:51:00,004 flushing high-traffic column family ColumnFamilyStore(table='system', columnFamily='NodeIdInfo') (estimated 0 bytes)
 INFO 00:51:00,004 flushing high-traffic column family ColumnFamilyStore(table='system', columnFamily='IndexInfo') (estimated 0 bytes)
 INFO 00:51:00,004 flushing high-traffic column family ColumnFamilyStore(table='system', columnFamily='LocationInfo') (estimated 87 bytes)
 INFO 00:51:00,005 Enqueuing flush of Memtable-LocationInfo@1706933590(70/87 serialized/live bytes, 2 ops)
 INFO 00:51:00,005 flushing high-traffic column family ColumnFamilyStore(table='system', columnFamily='Migrations') (estimated 0 bytes)
 INFO 00:51:00,006 flushing high-traffic column family ColumnFamilyStore(table='system', columnFamily='HintsColumnFamily') (estimated 0 bytes)
 INFO 00:51:00,006 flushing high-traffic column family ColumnFamilyStore(table='system', columnFamily='Schema') (estimated 0 bytes)
 INFO 00:51:00,006 estimated 0 bytes used by all memtables pre-flush
 INFO 00:51:00,008 flushing ColumnFamilyStore(table='Keyspace1', columnFamily='Standard1') to free up 459320 bytes
 INFO 00:51:00,009 Enqueuing flush of Memtable-Standard1@456936648(14280/476332 serialized/live bytes, 280 ops)
 INFO 00:51:00,010 flushing ColumnFamilyStore(table='system', columnFamily='Schema') to free up 0 bytes
 INFO 00:51:00,010 flushing ColumnFamilyStore(table='system', columnFamily='HintsColumnFamily') to free up 0 bytes
 INFO 00:51:00,011 flushing ColumnFamilyStore(table='system', columnFamily='Migrations') to free up 0 bytes
 INFO 00:51:00,011 flushing ColumnFamilyStore(table='system', columnFamily='LocationInfo') to free up 0 bytes
 INFO 00:51:00,012 flushing ColumnFamilyStore(table='system', columnFamily='IndexInfo') to free up 0 bytes
 INFO 00:51:00,013 flushing ColumnFamilyStore(table='system', columnFamily='NodeIdInfo') to free up 0 bytes
 INFO 00:51:00,013 flushing ColumnFamilyStore(table='Keyspace1', columnFamily='SuperCounter1') to free up 0 bytes
 INFO 00:51:00,014 flushing ColumnFamilyStore(table='Keyspace1', columnFamily='Super1') to free up 0 bytes
 INFO 00:51:00,014 flushing ColumnFamilyStore(table='Keyspace1', columnFamily='Counter1') to free up 0 bytes
 INFO 00:51:01,016 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='Counter1') (estimated 0 bytes)
 INFO 00:51:01,016 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='Super1') (estimated 0 bytes)
 INFO 00:51:01,016 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='SuperCounter1') (estimated 0 bytes)
 INFO 00:51:01,016 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='Standard1') (estimated 83808954 bytes)
 INFO 00:51:01,017 Enqueuing flush of Memtable-Standard1@911886300(2512770/83817460 serialized/live bytes, 49270 ops)
{noformat}

After this point, it stays bad:

{noformat}
 INFO 00:51:01,879 Enqueuing flush of Memtable-Standard1@937412989(2670870/96531021 serialized/live bytes, 52370 ops)
 INFO 00:51:02,029 Enqueuing flush of Memtable-Standard1@282282499(2668065/88997573 serialized/live bytes, 52315 ops)
{noformat}","14/Sep/11 04:32;jbellis;patch that fixes two problems:

- removes double-counting of ByteBuffer overhead in jamm 0.2.4
- removes 25% fudge factor that is no longer useful with Slab allocation

That still leaves us with at least one other problem, though.","14/Sep/11 05:19;brandon.williams;Something strange is going on here, as this patch actually exacerbated the problem slightly.","14/Sep/11 20:30;jbellis;v2 attached:

- fixed-better version of jamm 0.2.5 (now just includes buffer.remaining() + shallow overhead)
- pre-adds the CFMetadata object to the ""seen"" set for MemoryMeter so we don't re-count it for each row [this is the big one]

I'm getting liveRatio of 4-6 now which makes me wonder if we need to add a higher fudge factor to make it not OOM again. :)","14/Sep/11 21:17;brandon.williams;bq. I'm getting liveRatio of 4-6 now which makes me wonder if we need to add a higher fudge factor to make it not OOM again. 

I couldn't OOM it, and I get much more consistent flush behavior now, even better than before CASSANDRA-1610 which inadvertently caused the re-counting.

+1",14/Sep/11 23:09;jbellis;committed,"15/Sep/11 01:21;brandon.williams;Here's what jconsole looks like with with a 1G heap receiving 100M inserts from stress.  Analysis of the heap indicates the memory decrease is due to bloom filters and index sampling, so I think we're safe without a fudge factor, especially since the fudge factor predated the SlabAllocator.  The points where the heap did hit ~75%, the GCI pressure valve did a good job in combination with CMS of dropping the usage back down before there was any danger.","16/Sep/11 05:09;jbellis;If MeteredFlusher + getLiveSize were really doing their job right though, the pressure valve wouldn't be getting involved.  (However, I'm not sure it's entirely fair to tell it ""you can have 1/3 of the heap"" and then fill up more than 2/3 w/ BF + index.)

I think I'd like to hold off on further tweaking until after 1.0; the current fudge factor means we're erring on the side of extra safety, and I'm okay with that (especially since, as you noted, leveled compaction isn't very sensitive to initial flush size).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem SliceByNamesReadCommand on super column family after flush operation,CASSANDRA-3446,12529885,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,rheostat,rheostat,02/Nov/11 16:15,12/Mar/19 14:07,13/Mar/19 22:26,08/Nov/11 17:55,1.0.3,,,,,,0,supercolumns,,,,"I'm having a problem with doing a multiget_slice on a super column family
after its first flush. Updates to the column values work properly, but
trying to retrieve the updated values using a multiget_slice operation fail
to get the updated values. Instead they return the values from before the
flush. The problem is not apparent with standard column families.

I've seen this problem in Cassandra v1.0.0 and v1.0.1. The problem
is not present in Cassandra v0.7.6.

Steps to reproduce:

   1. Create one or more super column entries
   2. Verify the sub column values can be updated and that you can retrieve
   the new values
   3. Use nodetool to flush the column family or restart cassandra
   4. Update the sub column values
   5. Verify they have been updated using cassandra-cli
   6. Verify you *DO NOT* get the updated values when doing a
   multiget_slice; instead you get the old values from before the flush

You can get the most recent value by doing a flush followed by a major
compaction. However, future updates are not retrieved properly either.

With debug turned on, it looks like the multiget_slice query uses the
following command/consistency level:
SliceByNamesReadCommand(table='test_cassandra', key=666f6f,
columnParent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', columns=[foo,])/QUORUM.

Cassandra-cli uses the following command/consistency level for a get_slice:
SliceFromReadCommand(table='test_cassandra', key='666f6f',
column_parent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', start='', finish='', reversed=false,
count=1000000)/QUORUM

Notice the test program gets 'bar2' for the column values and cassandra-cli
gets 'bar3' for the column values:

tcpdump from test program using hector-core:1.0-1

16:46:07.424562 IP iam.47158 > iam.9160: Flags [P.], seq 55:138, ack 30,
win 257, options [nop,nop,TS val 27474096 ecr 27474095], length 83
E....#@.@.PK.........6#.....].8......{.....
..8...8.........multiget_slice................foo..........test................foo.........
16:46:07.424575 IP iam.9160 > iam.47158: Flags [.], ack 138, win 256,
options [nop,nop,TS val 27474096 ecr 27474096], length 0
E..4..@.@.<.........#..6].8..........(.....
..8...8.
16:46:07.428771 IP iam.9160 > iam.47158: Flags [P.], seq 30:173, ack 138,
win 256, options [nop,nop,TS val 27474097 ecr 27474096], length 143
@.@.<&........#..6].8................
............foo...............foo...............foo1.......bar2
........6h........foo2.......bar2
........I.....


tcpdump of cassandra-cli:

16:30:55.945123 IP iam.47134 > iam.9160: Flags [P.], seq 370:479, ack 5310,
win 387, options [nop,nop,TS val 27246226 ecr 27241207], length 109
E.....@.@.9q..........#..n.X\
.............
................get_range_slices..............test.........................................................d.........
16:30:55.945152 IP iam.9160 > iam.47134: Flags [.], ack 479, win 256,
options [nop,nop,TS val 27246226 ecr 27246226], length 0
E..4..@.@."".........#...\
...n.......(.....
........
16:30:55.949245 IP iam.9160 > iam.47134: Flags [P.], seq 5310:5461, ack
479, win 256, options [nop,nop,TS val 27246227 ecr 27246226], length 151
E.....@.@.""V........#...\
...n.............
....................get_range_slices...................foo..................foo...............foo1.......bar3
........&.........foo2.......bar3
........: .....","Linux iam 3.0.0-12-generic #20-Ubuntu SMP Fri Oct 7 14:56:25 UTC 2011 x86_64 x86_64 x86_64 GNU/Linux
java version ""1.6.0_23""
OpenJDK Runtime Environment (IcedTea6 1.11pre) (6b23~pre10-0ubuntu5)
OpenJDK 64-Bit Server VM (build 20.0-b11, mixed mode)
hector-core; 1.0-1
",,,,,,,,,,,,,,,07/Nov/11 21:39;jbellis;0001-fix-addColumn-and-collation-supercolumn-bugs.patch;https://issues.apache.org/jira/secure/attachment/12502817/0001-fix-addColumn-and-collation-supercolumn-bugs.patch,07/Nov/11 21:39;jbellis;0002-r-m-minTimestamp-method.patch;https://issues.apache.org/jira/secure/attachment/12502818/0002-r-m-minTimestamp-method.patch,04/Nov/11 19:19;rbranson;3446-test.patch;https://issues.apache.org/jira/secure/attachment/12502506/3446-test.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-11-02 18:40:17.098,,,no_permission,,,,,,,,,,,,215744,,,Thu Nov 10 14:28:49 UTC 2011,,,,,,0|i0gk07:,94671,slebresne,slebresne,,,,,,,,,02/Nov/11 18:40;jbellis;Can you post your columnfamily create statement?,"02/Nov/11 19:59;rheostat;create keyspace test_cassandra;
use test_cassandra;
create column family test1 with column_type = 'Super' and comparator =
'AsciiType';
create column family test2 with column_type = 'Standard' and comparator =
'AsciiType';
",03/Nov/11 04:42;jbellis;Is this just a single-node cluster?,"03/Nov/11 04:49;rheostat;Yes, single node cluster.


On Wed, Nov 2, 2011 at 9:43 PM, Jonathan Ellis (Commented) (JIRA) <

",04/Nov/11 19:19;rbranson;Adds a unit test to ColumnFamilyStoreTest to reproduce the issue.,"07/Nov/11 18:37;norru;I had a similar problem and spent a fair amount of time tracking it down. It appears related to a problem in collating data from Memtables and SStables, but only when the query involves SuperColumns

I may have found a fix. It did solve the problem for me but I haven't tested extensively for regressions or concurrency issues.

{code}
package org.apache.cassandra.db;

public class TreeMapBackedSortedColumns extends TreeMap<ByteBuffer, IColumn> implements ISortedColumns



    /*
     * If we find an old column that has the same name
     * the ask it to resolve itself else add the new column
    */
    public void addColumn(IColumn column, Allocator allocator)
    {
        ByteBuffer name = column.name();
        IColumn oldColumn = put(name, column);
        if (oldColumn != null)
        {
            if (oldColumn instanceof SuperColumn)
            {
                assert column instanceof SuperColumn;
                ((SuperColumn) oldColumn).putColumn((SuperColumn)column, allocator);
                // we need to restore the old value here or things won't work! --norru@scee.net
                put(name, oldColumn); // <--- here it is
            }
            else
            {
                // calculate reconciled col from old (existing) col and new col
                IColumn reconciledColumn = column.reconcile(oldColumn, allocator);
                put(name, reconciledColumn);
            }
        }
    }
{code}

Let me know if you need a proper patch. It's an one liner so it might be easier for you to add the line yourself.

Also let me know if you need more details. Cheers!","07/Nov/11 18:57;jbellis;There's at least two problems here.  One is the one Nicola describes.  The other is that the name-based path in CollationController stops as soon as it finds one subcolumn in a given supercolumn.

Working on a patch for both.","07/Nov/11 21:38;jbellis;There's a third bug involved, that can hide the second: SuperColumn.minTimestamp is calculated incorrectly.

Patch 01 adds tests for the two ""main"" bugs and fixes them.  Patch 02 removes SC.minTimestamp since part of the 01 fix is recognizing that we shouldn't short-circuit a SuperColumn read, since we don't know how many potential subcolumns there are without an exhaustive search.",07/Nov/11 22:02;rheostat;I applied the patches to 1.0.2 and all looks good. Thanks a lot for the quick turn around and keep up the good work!!,"08/Nov/11 09:27;slebresne;+1, good catches.",08/Nov/11 17:55;jbellis;committed,"10/Nov/11 14:28;jborgstrom;I'm not sure it's related at all but CASSANDRA-3466 is about the HintsColumnFamily mysteriously losing or gaining some subcolumns after a hints delivery and corresponding flush.

First assumption was that this was related to this ticket. But after some further testing with a patched 1.0.2 I was still able to reproduce the AssertionError.
So either these two tickets are not related at all or the proposed patches does not fix all cases.

If anyone's interested CASSANDRA-3466 contains an attachment with DEBUG-logs and data files from when I reproduced the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction of hints can get stuck in a loop,CASSANDRA-4022,12545687,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,yukim,brandon.williams,brandon.williams,08/Mar/12 17:46,12/Mar/19 14:07,13/Mar/19 22:26,31/Mar/12 04:45,1.2.0 beta 1,,,,,,0,,,,,"Not exactly sure how I caused this as I was working on something else in trunk, but:

{noformat}
 INFO 17:41:35,682 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-339-Data.db')]
 INFO 17:41:36,430 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-340-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 5.912220MB/s.  Time: 748ms.
 INFO 17:41:36,431 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-340-Data.db')]
 INFO 17:41:37,238 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-341-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 5.479976MB/s.  Time: 807ms.
 INFO 17:41:37,239 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-341-Data.db')]
 INFO 17:41:38,163 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-342-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 4.786083MB/s.  Time: 924ms.
 INFO 17:41:38,164 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-342-Data.db')]
 INFO 17:41:39,014 GC for ParNew: 274 ms for 1 collections, 541261288 used; max is 1024458752
 INFO 17:41:39,151 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-343-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 4.485132MB/s.  Time: 986ms.
 INFO 17:41:39,151 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-343-Data.db')]
 INFO 17:41:40,016 GC for ParNew: 308 ms for 1 collections, 585582200 used; max is 1024458752
 INFO 17:41:40,200 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-344-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 4.223821MB/s.  Time: 1,047ms.
 INFO 17:41:40,201 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-344-Data.db')]
 INFO 17:41:41,017 GC for ParNew: 252 ms for 1 collections, 617877904 used; max is 1024458752
 INFO 17:41:41,178 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-345-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 4.526449MB/s.  Time: 977ms.
 INFO 17:41:41,179 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-345-Data.db')]
 INFO 17:41:41,885 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-346-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 6.263938MB/s.  Time: 706ms.
 INFO 17:41:41,887 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-346-Data.db')]
 INFO 17:41:42,617 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-347-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes for 1 keys at 6.066311MB/s.  Time: 729ms.
 INFO 17:41:42,618 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-347-Data.db')]
 INFO 17:41:43,376 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-348-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes for 1 keys at 5.834222MB/s.  Time: 758ms.
 INFO 17:41:43,377 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-348-Data.db')]
 INFO 17:41:44,307 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-349-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes for 1 keys at 4.760323MB/s.  Time: 929ms.
 INFO 17:41:44,308 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-349-Data.db')]
 INFO 17:41:45,021 GC for ParNew: 245 ms for 1 collections, 731287832 used; max is 1024458752
 INFO 17:41:45,316 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-350-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes for 1 keys at 4.395965MB/s.  Time: 1,006ms.
 INFO 17:41:45,316 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-350-Data.db')]
 INFO 17:41:46,022 GC for ParNew: 353 ms for 1 collections, 757476872 used; max is 1024458752
 INFO 17:41:46,451 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-351-Data.db,].  4,637,160 to 4,637
{noformat}

I suspect we broke something subtle in CASSANDRA-3955.",,,,,,,,,,,,,,,,19/Mar/12 19:19;yukim;4022-v2.txt;https://issues.apache.org/jira/secure/attachment/12518928/4022-v2.txt,09/Mar/12 21:39;yukim;4022.txt;https://issues.apache.org/jira/secure/attachment/12517793/4022.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-03-08 23:40:50.382,,,no_permission,,,,,,,,,,,,230870,,,Sat Mar 31 04:45:37 UTC 2012,,,,,,0|i0gr1r:,95812,jbellis,jbellis,,,,,,,,,"08/Mar/12 18:01;brandon.williams;It seems part of the problem is it doesn't know about one of the sstables it needs to compact:

{noformat}

cassandra-1:/srv/cassandra# ls -l /var/lib/cassandra/data/system/HintsColumnFamily/
total 66804
-rw-r--r-- 1 root root 63642821 Mar  8 17:36 system-HintsColumnFamily-hd-13-Data.db
-rw-r--r-- 1 root root       80 Mar  8 17:36 system-HintsColumnFamily-hd-13-Digest.sha1
-rw-r--r-- 1 root root      976 Mar  8 17:36 system-HintsColumnFamily-hd-13-Filter.db
-rw-r--r-- 1 root root       26 Mar  8 17:36 system-HintsColumnFamily-hd-13-Index.db
-rw-r--r-- 1 root root     4344 Mar  8 17:36 system-HintsColumnFamily-hd-13-Statistics.db
-rw-r--r-- 1 root root  4637160 Mar  8 17:59 system-HintsColumnFamily-hd-639-Data.db
-rw-r--r-- 1 root root       81 Mar  8 17:59 system-HintsColumnFamily-hd-639-Digest.sha1
-rw-r--r-- 1 root root      496 Mar  8 17:59 system-HintsColumnFamily-hd-639-Filter.db
-rw-r--r-- 1 root root       26 Mar  8 17:59 system-HintsColumnFamily-hd-639-Index.db
-rw-r--r-- 1 root root     5944 Mar  8 17:59 system-HintsColumnFamily-hd-639-Statistics.db
-rw-r--r-- 1 root root        0 Mar  8 17:59 system-HintsColumnFamily-tmp-hd-640-Data.db
-rw-r--r-- 1 root root        0 Mar  8 17:59 system-HintsColumnFamily-tmp-hd-640-Index.db
{noformat}",08/Mar/12 18:11;brandon.williams;Yuki mentions that it may be caused by CASSANDRA-3442 too.,"08/Mar/12 18:12;brandon.williams;I should note that the machine does not hand anything off, so everything in these sstables must be tombstones.","08/Mar/12 20:24;brandon.williams;What is happening very reproducibly now is that I started the node, and 5 minutes later the forced compaction check in ACS kicks off, and then I have looping compaction on the hints but it's only compacting the last sstable over and over.",08/Mar/12 20:36;brandon.williams;Confirmed that this only happens with CASSANDRA-3442 applied.,"08/Mar/12 23:40;yukim;The basic idea behind CASSANDRA-3442 is to perform single sstable compaction if its droppable tombstone ratio is above threshold.
This works because single sstable compaction drops tombstones and lowers droppable tombstone ratio, which prevents recursive compaction on that sstable.

But there is a situation which compaction does not drop tombstones. That happens when the key in compacting sstable appears in another sstables.

Let me find the way to handle above case...","08/Mar/12 23:58;jbellis;bq. But there is a situation which compaction does not drop tombstones. That happens when the key in compacting sstable appears in another sstables.

What if we checked the most-recent-timestamp of the other sstables, and avoid compaction only if there is potentially data old enough that we'd need the tombstones to suppress it?","09/Mar/12 21:39;yukim;One possible solution is to add check for overwrap of key range stored in sstable.
Patch attached with minor fix for test.",11/Mar/12 23:55;jbellis;I think we should check for overlaps *and* timestamp is old enough to have data we care about suppressing.  The former alone will be common in size-tiered compaction.,"14/Mar/12 19:41;yukim;I tried to use timestamp to determine whether sstable should be compacted, but it does not guarantee to suppress tombstones. Tombstones only get dropped when those keys don't appear in other sstables besides compacting ones. Currently I think the only way to stop compaction loop is to make sure interested sstable does not have overlap so its tombstones actually drop.","14/Mar/12 19:57;jbellis;We don't need to suppress tombstones -- we just need to make sure that any data that the tombstones we're compacting, is new enough that we don't need the tombstones to suppress them.  In other words, that throwing away our tombstones won't make deleted data start showing up again.","15/Mar/12 15:22;yukim;I understand the situation, but isn't it covered by just checking key overlap?
If there is no overlap, then tombstones in target sstable are guaranteed to be the only and the newest ones?","15/Mar/12 16:38;jbellis;Right, if there's no overlap we're free to compact -- but I'm worried that with SizeTiered compaction we'll have overlap in a lot of cases where we could still compact if we looked closer.","15/Mar/12 21:26;jbellis;Suppose for example we have two sstables:

SSTable A has a tombstone for row K, column foo, at time=100.

SSTable B has data for row K, column bar, at time=200.

We would like to allow A to be compacted by itself to get rid of tombstones, since even though B has overlapping data it is new enough that removing the TS is safe.","19/Mar/12 19:19;yukim;Dropping tombstone is only done when the key tombstone belongs to does not appear in other sstables that are not compacting(ConpactionController#shouldPurge).
We may be able to tweak the above to look through timestamp of columns, but it will cost too much.

Instead, I come up with ""guessing"" how many tombstones there are outside of overlapping keys among sstables. When sstable has droppable tombstone ratio > threshold but overlaps keys with others, then calculate:

{code}
(# of keys outside of overlap) remainingKeys = sstable.estimatedKeys - sstable.estimatedKeysForRanges(overlapped range)
(# of columns outside of overlap) remainingColumns = sstable.estimatedColumnCount.percentile(remaingingKeys / total keys) * remainingKeys
{code}

and if (remainingColumns / total columns) * (droppable tombstone ratio) is greater than threshold, compact that sstable itself.

I think in this way, the chance of single sstable compaction increases, while avoiding recursive sstable compaction.","23/Mar/12 17:41;jbellis;bq. Dropping tombstone is only done when the key tombstone belongs to does not appear in other sstables that are not compacting(ConpactionController#shouldPurge).

Right. I'm saying we can make that more sophisticated, e.g. by changing shouldPurge signature to {{(key, maxTombstoneTimestamp)}} which we could then compare to the min timestamp from the overlapping-but-not-compaction-participant sstables.

But, thinking about that more, it's unlikely to help much since both STCS and LCS mix data of different ages together routinely.  So I'll let that drop now. :)",23/Mar/12 18:25;jbellis;v2 lgtm but I'm going to rebase on top of CASSANDRA-4080 before committing.,31/Mar/12 04:45;jbellis;rebased + committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL test failures,CASSANDRA-2613,12506419,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,xedin,jbellis,jbellis,06/May/11 14:53,12/Mar/19 14:07,13/Mar/19 22:26,06/May/11 16:31,0.8.0,,,Legacy/CQL,,,0,,,,,"{noformat}
FAIL: delete columns from a row
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 360, in test_delete_columns
    assert ['kd', None, None] == r, r
AssertionError: [u'kd']

======================================================================
FAIL: delete columns from multiple rows
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 379, in test_delete_columns_multi_rows
    assert ['kc', None] == r, r
AssertionError: [u'kc']

======================================================================
FAIL: delete entire rows
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 397, in test_delete_rows
    assert ['kd', None, None] == r, r
AssertionError: [u'kd']

======================================================================
FAIL: retrieve multiple columns
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 149, in test_select_columns
    assert ['Row Key', 'ca1', 'col', 'cd1'] == [col_dscptn[0] for col_dscptn in d], d
AssertionError: [('Row Key', 'org.apache.cassandra.db.marshal.UTF8Type', None, None, None, None, None, False), ('col', 'org.apache.cassandra.db.marshal.AsciiType', None, None, None, None, True), ('cd1', 'org.apache.cassandra.db.marshal.AsciiType', None, None, None, None, True)]

======================================================================
FAIL: range should not fail when keys were not set
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 252, in test_select_range_with_single_column_results
    assert len(r) == 2
AssertionError
{noformat}",,,,,,,,,,,,,,,,06/May/11 16:20;xedin;CASSANDRA-2613.patch;https://issues.apache.org/jira/secure/attachment/12478447/CASSANDRA-2613.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-06 15:02:16.365,,,no_permission,,,,,,,,,,,,20729,,,Tue May 10 22:30:30 UTC 2011,,,,,,0|i0gcev:,93441,jbellis,jbellis,,,,,,,,,"06/May/11 15:02;xedin;I have just fetched a fresh cassandra-0.8 branch and there is no support for BATCH with multiple statements (and no test for it, of course) it should be only in trunk right now CASSANDRA-2537, what branch have you been using to run tests?",06/May/11 15:13;jbellis;(original discription was tests on trunk; updated for 0.8),"06/May/11 15:51;xedin;I see that the problem exists, I just didn't reinstall my cql driver, will fix ASAP, we don't return a ""null"" columns any more for a key so need to fix tests, sorry...","06/May/11 15:59;jbellis;bq. we don't return a ""null"" columns any more for a key so need to fix tests

We need to keep returning the nulls as discussed in CASSANDRA-2593.","06/May/11 16:31;jbellis;committed, thanks!","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error starting up a cassandra cluster after creating a table in the system keyspace: Attempt to assign id to existing column family.,CASSANDRA-2563,12505230,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,cdaw,cdaw,26/Apr/11 18:50,12/Mar/19 14:07,13/Mar/19 22:26,27/Apr/11 15:55,0.8.0 beta 2,,,,,,0,,,,,"*Repro Steps*
* rm -rf /var/lib/cassandra/*
* rm -rf /var/log/cassandra/*
* Start Cassandra
* In cqlsh, create a column family and insert data
{noformat}
cqlsh> CREATE COLUMNFAMILY users (
   ...   KEY varchar PRIMARY KEY,
   ...   password varchar,
   ...   gender varchar,
   ...   session_token varchar,
   ...   state varchar,
   ...   birth_year bigint);

cqlsh> INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');
cqlsh> INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');
cqlsh> INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');
{noformat}

* Quit cqlsh
* Kill Cassandra
* Startup Cassandra and get error

{noformat}
 INFO 18:38:24,509 Loading schema version 087af100-7034-11e0-0000-242d50cf1fde
ERROR 18:38:24,774 Exception encountered during startup.
java.io.IOError: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:489)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.CFMetaData.map(CFMetaData.java:126)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:485)
	... 3 more
Exception encountered during startup.
java.io.IOError: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:489)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.CFMetaData.map(CFMetaData.java:126)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:485)
	... 3 more
{noformat}



*UPDATE:  This issue happens if I create the CF in the default keyspace.*

*Workaround*
{noformat}
cqlsh> CREATE KEYSPACE cqldb with 
   ...   strategy_class =  
   ...     'org.apache.cassandra.locator.SimpleStrategy' 
   ...   and strategy_options:replication_factor=1;
cqlsh> use cqldb;

The create the table and insert data.
{noformat}
","Branch: cassandra-0.8; git pull @ 11:30amPST on 4/26
Server: RHEL5.5 single node",,,,,,,,,,,,,,,26/Apr/11 21:31;jbellis;2563.txt;https://issues.apache.org/jira/secure/attachment/12477442/2563.txt,26/Apr/11 19:01;cdaw;cassandra.2563.tar;https://issues.apache.org/jira/secure/attachment/12477431/cassandra.2563.tar,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-26 19:49:56.178,,,no_permission,,,,,,,,,,,,20698,,,Wed Apr 27 17:18:12 UTC 2011,,,,,,0|i0gc3z:,93392,cdaw,cdaw,,,,,,,,,26/Apr/11 19:01;cdaw;The /var/lib/cassandra directory tarball.,26/Apr/11 19:06;cdaw;I was able to reproduce this again ...  cleared out /var/lib/cassandra/* and then started up server and repeated steps.,26/Apr/11 19:49;jbellis;is default keyspace = system?,"26/Apr/11 19:56;cdaw;The CF was created in the system keyspace:

{noformat}
Keyspace: system:
  Replication Strategy: org.apache.cassandra.locator.LocalStrategy
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.140625/30/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Column Metadata:
        Column Name: session_token
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: state
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: birth_year
          Validation Class: org.apache.cassandra.db.marshal.LongType
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: gender
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
{noformat}
",26/Apr/11 21:31;jbellis;patch to disallow screwing w/ system keyspace,"26/Apr/11 21:59;cdaw;The patch works good and gives this error when creating a new column family:
Bad Request: system keyspace is not user-modifiable

The only thing to think about is whether or not this fix limits the ability of support to fix corruption issues.
","27/Apr/11 15:55;jbellis;committed.

note that this only disallows schema changes; modifying system *data* is still allowed (and still dangerous :)","27/Apr/11 17:18;hudson;Integrated in Cassandra-0.8 #46 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/46/])
    disallow making schema changes to system keyspace
patch by jbellis; tested by cdaw for CASSANDRA-2563
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: create keyspace does not the replication factor argument and allows invalid sql to pass thru,CASSANDRA-2525,12504840,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,cdaw,cdaw,21/Apr/11 01:35,12/Mar/19 14:07,13/Mar/19 22:26,22/Apr/11 20:50,0.8.0 beta 2,,,,,,0,cql,,,,"I ran the following SQL in cqlsh and immediately received a socket closed error.  After that point, I couldn't run nodetool, and then got an exception starting up the cluster.

Please Note:  The following syntax is valid in 0.74 but invalid in 0.8.
The 0.8 cassandra-cli errors on the following statement, so the resolution of the bug is to have cqlsh block this incorrect syntax.

{code}
create keyspace testcli with replication_factor=1
and placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy';
{code}

{code}
CREATE KEYSPACE testcql with replication_factor = 1 and strategy_class = 'org.apache.cassandra.locator.SimpleStrategy';	
{code}


{code}
ERROR 01:29:26,989 Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 4 more
Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 4 more
{code}","Cluster: 3 node Rackspace cluster running Centos 5.5.

Build: https://builds.apache.org/hudson/job/Cassandra/859/artifact/cassandra/build/apache-cassandra-2011-04-20_10-01-29-bin.tar.gz
",,,,,,,,,,,,,,,21/Apr/11 05:06;jbellis;2525.txt;https://issues.apache.org/jira/secure/attachment/12476960/2525.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-21 02:27:32.589,,,no_permission,,,,,,,,,,,,20675,,,Tue Apr 26 22:14:18 UTC 2011,,,,,,0|i0gbvj:,93354,urandom,urandom,,,,,,,,,"21/Apr/11 02:27;yukim;I got the same problem, and after making change in the statement like following

{code}
CREATE KEYSPACE CqlTest WITH strategy_class = SimpleStrategy AND strategy_options:replication_factor = 1;
{code}

everything works fine.
(I'm using apache-cassandra-2011-04-12_18-15-17 build version.)

I think CQL doc should also be updated, because it states that ""replication_factor"" is required.","21/Apr/11 05:06;jbellis;patch to reject malformed keyspaces from cql.

(also patches CQL.textile docs source, but I have not yet figured out how to generate the html from that.)",21/Apr/11 14:24;jbellis;opened CASSANDRA-2526 for doc generation,22/Apr/11 20:28;urandom;+1,22/Apr/11 20:50;jbellis;committed,"23/Apr/11 01:26;hudson;Integrated in Cassandra-0.8 #36 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/36/])
    validate CQL create keyspace options
patch by jbellis; reviewed by eevans for CASSANDRA-2525
",26/Apr/11 22:14;cdaw;retested and verified,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running cleanup on a node with join_ring=false removes all data,CASSANDRA-2428,12503560,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,lenn0x,lenn0x,06/Apr/11 21:58,12/Mar/19 14:07,13/Mar/19 22:26,07/Apr/11 21:00,0.7.5,,,,,,0,,,,,"If you need to bring up a node with join_ring=false for operator maintenance, and this node already has data, it will end up removing the data on the node. We noticed this when we were calling cleanup on a specific CF.
",,,,,,,,,,,,,,,,07/Apr/11 20:01;slebresne;0001-Don-t-allow-cleanup-when-node-hasn-t-join-the-ring.patch;https://issues.apache.org/jira/secure/attachment/12475742/0001-Don-t-allow-cleanup-when-node-hasn-t-join-the-ring.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-07 20:01:45.635,,,no_permission,,,,,,,,,,,,20619,,,Thu Apr 07 23:44:17 UTC 2011,,,,,,0|i0gbbj:,93264,jbellis,jbellis,,,,,,,,,"06/Apr/11 21:58;lenn0x;If curious why we were running join_ring = false after the node had data, it was because we had ran out of space on a node, and many SSTables had been created, but unable to compact. When we brought the node back online after moving some data off, the number of SSTables being read w/ live traffic caused us to run out of file descriptors. So we decided to run join_ring=false on bootup, and let compaction run without taking on traffic. This worked really well, and we wanted to finalize our cleanup process.
","07/Apr/11 20:01;slebresne;The ability of not joining on startup is a convenience feature but is honestly a bit half backed. In particular the token ranges are not set correctly. That's the reason why cleanup gets an empty list of ranges when querying the local ranges and thus clean everything.

Attached patch band-aid this by refusing to do a cleanup compaction. I don't think we want to do more for 0.7. We could make join_ring=false better so that it sets the ranges based and what's in the system tables, but let's fix the dangerous behavior first and let that for another ticket.","07/Apr/11 20:38;kingryan;Sylvain-

That seems like the right plan.",07/Apr/11 20:40;jbellis;+1,07/Apr/11 21:00;slebresne;Committed as r1090007,"07/Apr/11 23:44;hudson;Integrated in Cassandra-0.7 #426 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/426/])
    Fix unit tests for CASSANDRA-2428
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
auto bootstrap happened on already bootstrapped nodes,CASSANDRA-2435,12503656,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,scode,scode,07/Apr/11 17:29,12/Mar/19 14:07,13/Mar/19 22:26,11/Apr/11 03:10,0.7.5,,,,,,1,,,,,"I believe the following was observed on 0.7.2. I meant to dig deeper, but never had the time, and now I want to at least file this even if I don't have extremely helpful information.

A piece of background is that we consciously made the decision to have the default configuration on nodes have auto_bootstrap set to true. The logic was that if one accidentally were to start a new node, we'd rather have it join with data than join *without* data and cause bogus read results in the cluster.

We executed this policy (by way of having the puppet managed config have auto_bootstrap set to true).

On one of our clusters with 5 nodes, we did some moves. All looked well; the moves completed. For unrelated reasons, we wanted to restart nodes after they had been moved. When we did, three of the 5, specifically those 3 that were *NOT* seed nodes, initiated a bootstrap procedure! Before the moves the cluster had been running for several days at least.

The logs indicated the automatic token selection, and they joined the ring under a new automatically selected token.

Presumably, this violated consistency but at the time there was no live traffic to the cluster and we didn't confirm (put traffic on it after repair+cleanup).

I did look a little bit at the code in light of this but didn't see anything obvious, so I don't really know what the likely culprit is.

A potential complication was that seed nodes were moved without using the correct procedure of de-seeding them first. This was clearly wrong, but it is not obvious to me that it would cause other nodes to incorrectly bootstrap since a node should *never* bootstrap more than once if the local system tables say it's been bootstrapped.
",,,,,,,,,,,,,,,,08/Apr/11 21:15;jbellis;2435.txt;https://issues.apache.org/jira/secure/attachment/12475840/2435.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-08 21:14:38.201,,,no_permission,,,,,,,,,,,,20624,,,Mon Apr 11 03:26:05 UTC 2011,,,,,,0|i0gbcv:,93270,nickmbailey,nickmbailey,,,,,,,,,"08/Apr/11 21:14;jbellis;recall that move (until 0.8) consists of

- unbootstrap
- bootstrap to new location

unbootstrap calls storageservice.leavering (same as decommission), which marks the node as not-bootstrapped with setBootstrapped(false).  

in one of the refactorings during 0.7 development we removed the call to setBootstrapped(true) from finishBootstrapping.  So next restart it will indeed autobootstrap if that is enabled in the config file.",08/Apr/11 21:15;jbellis;patch to fix.,"10/Apr/11 12:13;scode;FWIW, looks good to me (but I only did visual inspection and some code jumping in the 0.7 branch; haven't tested it).
",10/Apr/11 19:04;nickmbailey;+1,11/Apr/11 03:10;jbellis;committed,"11/Apr/11 03:26;hudson;Integrated in Cassandra-0.7 #430 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/430/])
    re-set bootstrapped flag after move finishes
patch by jbellis; reviewed by Peter Schuller and Nick Bailey for CASSANDRA-2435
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra crashes with segmentation fault on Debian 5.0 and Ubuntu 10.10,CASSANDRA-2441,12503787,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,xedin,xedin,08/Apr/11 21:35,12/Mar/19 14:07,13/Mar/19 22:26,12/Apr/11 15:09,0.8 beta 1,,,,,,0,,,,,"Last working commit is c8d1984bf17cab58f40069e522d074c7b0077bc1 (merge from 0.7), branch: trunk.

What I did is cloned git://git.apache.org/cassandra.git and did git reset each commit with `ant clean && ant && ./bin/cassandra -f` until I got cassandra started","Both servers have identical hardware configuration: Quad-Core AMD Opteron(tm) Processor 2374 HE, 4 GB RAM (rackspace servers)

Java version ""1.6.0_20""
OpenJDK Runtime Environment (IcedTea6 1.9.7) (6b20-1.9.7-0ubuntu1)
OpenJDK 64-Bit Server VM (build 19.0-b09, mixed mode)",,,,,,,,,,,,,,,11/Apr/11 20:51;jbellis;2441.txt;https://issues.apache.org/jira/secure/attachment/12476056/2441.txt,11/Apr/11 20:21;jbellis;2441.txt;https://issues.apache.org/jira/secure/attachment/12476053/2441.txt,11/Apr/11 19:28;jbellis;jamm-0.2.1.jar;https://issues.apache.org/jira/secure/attachment/12476042/jamm-0.2.1.jar,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-04-08 22:12:13.884,,,no_permission,,,,,,,,,,,,20627,,,Mon Oct 15 06:18:30 UTC 2012,,,,,,0|i0839r:,45129,xedin,xedin,,,,,,,,,08/Apr/11 22:12;jbellis;are you using sun jdk?,"08/Apr/11 22:17;xedin;no, I installed OpenJDK using apt-get `apt-get install openjdk-6-jdk openjdk-6-jre`",08/Apr/11 22:24;jbellis;give sun jdk a try.,"08/Apr/11 22:26;xedin;Ok, I will give it a try and comment back!","08/Apr/11 23:14;xedin;Works with Sun JDK

{noformat}
java version ""1.6.0_24""
Java(TM) SE Runtime Environment (build 1.6.0_24-b07)
Java HotSpot(TM) 64-Bit Server VM (build 19.1-b02, mixed mode)
{noformat}

But this could be a problem if, for example, Whirr sets up openjdk instead of one from the Sun, I will check that ASAP and comment...","08/Apr/11 23:23;jbellis;Looks like this isn't the only time someone has seen segfaults due to using javaagent with OpenJDK: http://liteforums.appdynamics.com/discussion/143/appagent-causing-segfault/p1

I would prefer to fix by changing our packaging to explicitly use Sun JDK instead of ""whatever.""

Would also be useful to try w/ the IBM JDK: http://www.ibm.com/developerworks/java/jdk/linux/download.html","08/Apr/11 23:26;xedin;Thats a good idea, I agree! Let me test IBM JDK tomorrow...",10/Apr/11 15:23;jbellis;Also jrockit: http://www.oracle.com/technetwork/middleware/jrockit/downloads/index.html,10/Apr/11 17:44;xedin;Latest code (branch trunk) works with JRockit (jrockit-jdk1.6.0_22-R28.1.1-4.0.1) but note that JVM does not support following options: -Xmn<size> -XX:ThreadPriorityPolicy -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8  -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly,"10/Apr/11 18:23;xedin;using IBM JDK started without any changes to the conf/cassandra-env.sh and everything worked fine.

{noformat}
java version ""1.6.0""
Java(TM) SE Runtime Environment (build pxi3260sr9fp1-20110208_03(SR9 FP1))
IBM J9 VM (build 2.4, JRE 1.6.0 IBM J9 2.4 Linux x86-32 jvmxi3260sr9-20110203_74623 (JIT enabled, AOT enabled)
J9VM - 20110203_074623
JIT  - r9_20101028_17488ifx3
GC   - 20101027_AA)
JCL  - 20110203_01
{noformat}
","11/Apr/11 03:11;jbellis;So, definitely an OpenJDK-only bug.

I'll see if I can give it a no-java-agent mode so we can limp along without it for OpenJDK.",11/Apr/11 10:23;xedin;+1,"11/Apr/11 19:28;jbellis;attached.  (requires jamm 0.2.1 in lib/, also attached.)

(most of the patch is svn deleting the 0.2 jar.  silly svn.)","11/Apr/11 20:21;jbellis;On Brandon's advice I moved the entire warning into the log4j call, even though this makes it unwieldy and we don't have perfect information as to what the cause is at that point.",11/Apr/11 20:32;xedin;Can you please re-attach git apply and patch both say that patch is corrupted at line 90?..,11/Apr/11 20:51;jbellis;manually ripped the binary portion out of the patch.,11/Apr/11 21:00;xedin;+1,12/Apr/11 15:09;jbellis;committed,"12/Apr/11 16:42;hudson;Integrated in Cassandra-0.8 #2 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/2/])
    hack to allow OpenJDK to run w/o javaagent (otherwise it segfaults)
patch by jbellis and brandonwilliams; reviewed by Pavel Yaskevich for CASSANDRA-2441
","12/Apr/12 09:42;richardlow;Tracked this down to the stack size setting.  In cassandra-env.sh, the stack size is set to 128k.  From running gdb, the segfault is tracked down to a stack overflow in parse_manifest.c:234 within openjdk.  It's clear what's going on: there's a huge statically allocated variable.

Setting the stack size to 256k means Cassandra can start up with javaagent.  So we could reenable this for openjdk if people are prepared to take the memory hit on stack size.",02/Oct/12 15:22;amram99;Thanks Richard! Changing the stack size from 180k to 256k worked for me on Ubuntu 12.04 with Cassandra 1.1.5 and OpenJDK 64-Bit Server VM/1.6.0_24,10/Oct/12 22:07;alexiswilke;I can confirm Bubba Gump comment. I have the same setup and increasing the stack solved the crash issue with OpenJDK.,15/Oct/12 06:18;mauzhang;180k works fine with OpenJDK 1.7.0_07. So update OpenJDK could be an option,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expring columns can expire between the two phase of LazilyCompactedRow.,CASSANDRA-2349,12501688,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,17/Mar/11 14:37,12/Mar/19 14:07,13/Mar/19 22:26,18/Mar/11 04:31,0.7.5,,,,,,0,,,,,"LazilyCompactedRow reads the columns to compact twice. First to create the index, bloom filter and calculate the data size, and then another phase to actually write the columns. But a column can expire between those two phase, which will result in a bad data size in the sstable (and a possibly corrupted row index).",,7200,7200,,0%,7200,7200,,,,,,,,,17/Mar/11 17:25;slebresne;0001-Introduce-expireBefore-and-keep-it-constant-all-thro.patch;https://issues.apache.org/jira/secure/attachment/12473924/0001-Introduce-expireBefore-and-keep-it-constant-all-thro.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-17 16:11:23.767,,,no_permission,,,,,,,,,,,,20572,,,Mon Mar 21 22:15:38 UTC 2011,,,,,,0|i0gav3:,93190,jbellis,jbellis,,,,,,,,,"17/Mar/11 14:40;slebresne;This could be fixed by making LazilyCompactedRow do only one phase. We could write a place holder header, write the column, and seek back at the end to write the header. The tricky part is that this won't fit well with AbstractCompactedRow, since it is assume that the column count is known before write is called.","17/Mar/11 15:07;slebresne;Actually, this won't work because we cannot anticipate the index size.","17/Mar/11 15:56;slebresne;Attaching simpler solution consisting in not transforming expired columns into tombstones, so that the second phase of LazilyCompactedRow sees the same columns than the first phase.

I would be happy to come up with a unit test for this, but this is a subtle race condition and I'm really not sure how to write a test that capture it.","17/Mar/11 16:11;jbellis;don't we need forceKeepExpired for both passes or we have the opposite problem?  If so, would prefer fKE a constructor arg for the iterator instead of setter.",17/Mar/11 16:14;jbellis;Or: should we have an expireBefore parameter like gcBefore so it can stay constant during the compaction?,"17/Mar/11 16:27;slebresne;bq. don't we need forceKeepExpired for both passes or we have the opposite problem? If so, would prefer fKE a constructor arg for the iterator instead of setter.

There is no opposite problem, since you can't have a column expired for the first phase but not for the second one.

Note that the point of transforming expired columns to tombstone is to re-gain quickly the disk space used by the column value (without having to wait for gcGrace). So using fKE in phase 1 would prevent that for LazilyCompactedRow, which would be a pity.","17/Mar/11 16:47;jbellis;bq. There is no opposite problem, since you can't have a column expired for the first phase but not for the second one.

But you can have an expired column counted as a tombstone for the first but not in the second since you have enabled forceKE.  No?

bq. the point of transforming expired columns to tombstone is to re-gain quickly the disk space used by the column value

Right, which is why using a constant expireBefore value that we pass to the serializer instead of computing it locally each time seems like a better solution. ","17/Mar/11 16:58;slebresne;bq. But you can have an expired column counted as a tombstone for the first but not in the second since you have enabled forceKE. No?

You are right. Oups.

You are right, having an expireBefore is a better solution. I'll work out the patch.",17/Mar/11 17:25;slebresne;Attached patch using the expireBefore idea.,18/Mar/11 04:31;jbellis;committed,"21/Mar/11 22:15;hudson;Integrated in Cassandra-0.7 #397 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/397/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keys get lost in bootstrap,CASSANDRA-2633,12506829,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,richardlow,richardlow,richardlow,11/May/11 10:36,12/Mar/19 14:07,13/Mar/19 22:26,11/May/11 20:55,0.7.6,0.8.1,,,,,0,,,,,"When bootstrapping a new node, the key at the upper end of the new node's range can get lost.  To reproduce:

* Set up one cassandra node, create a keyspace and column family and perform some inserts
* Read every row back
* Bootstrap a second node
* Read every row back

You find one row is missing, whose row key is exactly equal to the token the new node gets (for OPP - for RP it's the key whose hash is equal to the token).  If you don't do the reads after the inserts, the key is not lost.  I tracked the problem down to o.a.c.io.sstable.SSTableReader in getPosition.  The problem is that the cached position is used if it is there (so only if the reads were performed).  But this is incorrect because the cached position is the start of the row, not the end.  This means the end row itself is not transferred.  This causes the last key in the range to get lost.

Although I haven't seen it, this may occur during antientropy repairs too.

The attached patch (against the 0.7 branch) fixes it by not using the cache for Operator.GT.  I haven't tested with 0.8 but from looking at the code I think the problem is present.

This might be related to CASSANDRA-1992",,0,0,,0%,0,0,,,,,,,,,11/May/11 10:38;richardlow;0.7-2633.txt;https://issues.apache.org/jira/secure/attachment/12478797/0.7-2633.txt,11/May/11 15:32;slebresne;0001-2633-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12478832/0001-2633-unit-test.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-11 14:43:52.817,,,no_permission,,,,,,,,,,,,20743,,,Wed May 11 19:44:05 UTC 2011,,,,,,0|i0gcj3:,93460,slebresne,slebresne,,,,,,,,,"11/May/11 14:43;jbellis;So say we have a node A with rows A B C D on it.

We bootstrap a node C.

C requests (A, C] from A.

A will do a GT scan starting with A.  So a cache hit will result in [A, C] being transferred instead. That is a bug, I'll see if I can create a unit test that demonstrates that separately.

But I don't see how this affects the C row?
","11/May/11 15:07;richardlow;A cache hit results in [A, C) being returned.  All GT scans with cache hits give positions at the start of the row rather than the end.  The above patch fixes both ends - skip over A, but include C.","11/May/11 15:22;jbellis;getPosition only affects start of scan, not end.",11/May/11 15:32;slebresne;Good catch. Attaching a unit test to catch the bug.,"11/May/11 15:36;richardlow;It looks to me that for client reads, getPosition is just used for the start of an iterator, as you say.  But for streaming, getPosition is used for the end position too in SSTableReader.getPositionsForRanges.  Or have I misunderstood what's going on?","11/May/11 15:39;slebresne;bq. But I don't see how this affects the C row?

This affects the C row because it will use the position of C found as the position where to stop scanning. But the position of C is the start of C, so when used as an end position, it excludes it. That is, getPositionForRanges will return (start of A, start of C), which results in scanning [A, C) as Richard says.

So +1 on this.","11/May/11 16:03;jbellis;got it.  committed, thanks!","11/May/11 19:44;hudson;Integrated in Cassandra-0.7 #480 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/480/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool move fails to stream out data from moved node to new endpoint.,CASSANDRA-2948,12515417,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,soverton,soverton,soverton,26/Jul/11 15:42,12/Mar/19 14:07,13/Mar/19 22:26,27/Jul/11 22:47,0.8.3,,,,,,0,move,streaming,,,"When moving a node in the ring with nodetool move, that node streams its data to itself instead of to the new endpoint responsible for its old range.

Steps to reproduce:
* Create a cluster (A,B,C,D) with tokens (0,4,8,C) using ByteOrderedPartitioner
* Create a keyspace and CF with RF=1
* Insert keys (2,6,A,E). This should put one key on each node.
* Move node A to token 7. This should cause:
  + node C streams key 6 to node A
  + node A streams key E to node B
  However instead, node A streams key E to itself.

Selected log messages from node A:
 INFO [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:17,075 StorageService.java (line 1878) Moving miles/10.2.129.41 from Token(bytes[00]) to Token(bytes[07]).
DEBUG [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:17,080 StorageService.java (line 1941) Table ks: work map {/10.2.129.16=[(Token(bytes[04]),Token(bytes[07])]]}.
 INFO [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:17,080 StorageService.java (line 1946) Sleeping 30000 ms before start streaming/fetching ranges.
...
 INFO [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:46,728 StorageService.java (line 522) Moving: fetching new ranges and streaming old ranges
DEBUG [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:46,728 StorageService.java (line 1960) [Move->STREAMING] Work Map: {ks={(Token(bytes[0c]),Token(bytes[00])]=[miles/10.2.129.41]}}
DEBUG [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:46,729 StorageService.java (line 1965) [Move->FETCHING] Work Map: {ks={/10.2.129.16=[(Token(bytes[04]),Token(bytes[07])]]}}
DEBUG [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:46,730 StorageService.java (line 2411) Requesting from /10.2.129.16 ranges (Token(bytes[04]),Token(bytes[07])]
...
 INFO [StreamStage:1] 2011-07-26 16:29:46,737 StreamOut.java (line 90) Beginning transfer to miles/10.2.129.41
DEBUG [StreamStage:1] 2011-07-26 16:29:46,737 StreamOut.java (line 91) Ranges are (Token(bytes[0c]),Token(bytes[00])]

This appears to be caused because in StorageService.move we call
    Gossiper.instance.addLocalApplicationState(ApplicationState.STATUS, valueFactory.moving(newToken));
and then get the new token metadata in order to calculate where the new endpoint is that we should stream to
    TokenMetadata tokenMetaClone = tokenMetadata_.cloneAfterAllSettled();
however, in addLocalApplicationState there is no notification broadcast for the change in local state, so tokenMetadata_ never updates the list of moving nodes, and the tokenMetaClone is still the state of the ring from before the move.

",,,,,,,,,,,,,,,,26/Jul/11 15:48;soverton;2948.patch;https://issues.apache.org/jira/secure/attachment/12487854/2948.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-27 22:47:15.838,,,no_permission,,,,,,,,,,,,20905,,,Wed Jul 27 23:21:20 UTC 2011,,,,,,0|i0gefz:,93770,brandon.williams,brandon.williams,,,,,,,,,"26/Jul/11 15:48;soverton;Attached patch which will cause Gossiper to broadcast notifications when the local application state is updated.

It also fixes the calculation of the endpoints to stream from, which should use the old state of the ring, not the new one.","27/Jul/11 22:47;brandon.williams;Committed, thanks!","27/Jul/11 23:21;hudson;Integrated in Cassandra-0.8 #242 (See [https://builds.apache.org/job/Cassandra-0.8/242/])
    Gossiper notifies of local state changes.
Patch by Sam Overton, reviewed by brandonwilliams for CASSANDRA-2948

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1151659
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch mutation of counters in multiple supercolumns throws an exception during replication.,CASSANDRA-2949,12515424,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,soverton,soverton,26/Jul/11 16:19,12/Mar/19 14:07,13/Mar/19 22:26,01/Aug/11 15:13,0.8.3,,,,,,0,batch_mutate,counters,supercolumns,,"Steps to reproduce:
* Perform a batch mutation of more than one counter in more than one super-column in the same column-family.
* The following exception is thrown during replication:

DEBUG [MutationStage:63] 2011-07-26 17:05:12,653 CounterMutationVerbHandler.java (line 52) Applying forwarded CounterMutation(RowMutation(keyspace='ks1', key='4ae71336e44bf9bf', modifications=[ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:8@1311696312648,]),SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:8@1311696312648,]),])]), QUORUM)
DEBUG [MutationStage:63] 2011-07-26 17:05:12,653 StorageProxy.java (line 432) insert writing local & replicate CounterMutation(RowMutation(keyspace='ks1', key='4ae71336e44bf9bf', modifications=[cf1]), QUORUM)
DEBUG [MutationStage:63] 2011-07-26 17:05:12,654 Table.java (line 398) applying mutation of row 4ae71336e44bf9bf
ERROR [ReplicateOnWriteStage:125] 2011-07-26 17:05:12,655 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[ReplicateOnWriteStage:125,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: ColumnFamily ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),]) already has modifications in this mutation: ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),])
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.IllegalArgumentException: ColumnFamily ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),]) already has modifications in this mutation: ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),])
        at org.apache.cassandra.db.RowMutation.add(RowMutation.java:123)
        at org.apache.cassandra.db.CounterMutation.makeReplicationMutation(CounterMutation.java:120)
        at org.apache.cassandra.service.StorageProxy$5$1.runMayThrow(StorageProxy.java:455)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
",,,,,,,,,,,,,,,,26/Jul/11 16:21;soverton;2949.patch;https://issues.apache.org/jira/secure/attachment/12487861/2949.patch,27/Jul/11 10:39;slebresne;2949_v2.patch;https://issues.apache.org/jira/secure/attachment/12487959/2949_v2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-27 10:39:09.826,,,no_permission,,,,,,,,,,,,20906,,,Mon Aug 01 15:22:36 UTC 2011,,,,,,0|i0geg7:,93771,soverton,soverton,,,,,,,,,"26/Jul/11 16:21;soverton;Attached patch which fixes this issue by adding an ""addAll"" method to RowMutation which allows the same CF to be explicitly added to a mutation more than once.","27/Jul/11 10:39;slebresne;The patch does fix it, but I think there is a better fix. The reason the assertion is triggered is because when we read to replicate and there is super columns, we end up adding one read command for each super column (and thus get back multiple time the same CF, which triggers the assertion). The rational being that we only query the subcolumns that are in the original mutation.

However, since we deserialize full super columns anyway, it will be more efficient to generate only one read command for all the super columns (reading full super columns) and to filter afterwards the subcolumns we don't want to bother sending to the other nodes.

Attaching a ""v2"" patch that does this. It also ship with a unit test.
","27/Jul/11 12:22;soverton;Confirmed v2 patch fixes the issue and test passes.

If it's likely that supercolumns may get indexes (CASSANDRA-598) or the serialization format changes to make direct sub-column access possible (CASSANDRA-674) then this should probably be revisited to limit the scope of the read.","27/Jul/11 15:12;slebresne;bq. If it's likely that supercolumns may get indexes (CASSANDRA-598) or the serialization format changes to make direct sub-column access possible (CASSANDRA-674) then this should probably be revisited to limit the scope of the read.

Regarding super columns, the more likely move will be to replace them internally by columns with composite names, in which case this issue will be moot anyway.","01/Aug/11 15:13;slebresne;Committed, thanks","01/Aug/11 15:22;hudson;Integrated in Cassandra-0.8 #248 (See [https://builds.apache.org/job/Cassandra-0.8/248/])
    don't throw exception on batch of counter super columns
patch by slebresne; reviewed by soverton for CASSANDRA-2949

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152795
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/RowMutation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SuperColumn.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamily.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/CounterMutationTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/CounterMutation.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair fails with java.io.EOFException,CASSANDRA-2752,12509699,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,terjem,terjem,09/Jun/11 11:24,12/Mar/19 14:07,13/Mar/19 22:26,14/Jun/11 07:56,0.8.1,,,,,,0,,,,,"Issuing repair on node 1  (1.10.42.81) in a cluster quickly fails with
INFO [AntiEntropyStage:1] 2011-06-09 19:02:47,999 AntiEntropyService.java (line 234) Queueing comparison #<Differencer #<TreeRequest manual-repair-0c17c5f9-583f-4a31-a6d4-a9e7306fb46e, /1
.10.42.82, (JP,XXX), (Token(bytes[6e]),Token(bytes[313039])]>>
 INFO [AntiEntropyStage:1] 2011-06-09 19:02:48,026 AntiEntropyService.java (line 468) Endpoints somewhere/1.10.42.81 and /1.10.42.82 have 2 range(s) out of sync for (JP,XXX) on (Token(bytes[6e]),Token(bytes[313039])]
 INFO [AntiEntropyStage:1] 2011-06-09 19:02:48,026 AntiEntropyService.java (line 485) Performing streaming repair of 2 ranges for #<TreeRequest manual-repair-0c17c5f9-583f-4a31-a6d4-a9e7306
fb46e, /1.10.42.82, (JP,XXX), (Token(bytes[6e]),Token(bytes[313039])]>
 INFO [AntiEntropyStage:1] 2011-06-09 19:02:48,030 StreamOut.java (line 173) Stream context metadata [/data/cassandra/node0/data/JP/XXX-g-3-Data.db sections=1 progress=0/36592 - 0%], 1 sstables.
 INFO [AntiEntropyStage:1] 2011-06-09 19:02:48,031 StreamOutSession.java (line 174) Streaming to /1.10.42.82
ERROR [CompactionExecutor:9] 2011-06-09 19:02:48,970 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[CompactionExecutor:9,1,main]
java.io.EOFException
        at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.doIndexing(SSTableWriter.java:457)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1099)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1090)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


On .82
ERROR [CompactionExecutor:12] 2011-06-09 19:02:48,051 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[CompactionExecutor:12,1,main]
java.io.EOFException
        at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.doIndexing(SSTableWriter.java:457)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1099)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1090)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [Thread-132] 2011-06-09 19:02:48,051 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-132,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.EOFException
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:152)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
Caused by: java.util.concurrent.ExecutionException: java.io.EOFException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:136)
        ... 3 more
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.doIndexing(SSTableWriter.java:457)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1099)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1090)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Looks to me like the receiving side fails first.

",,,,,,,,,,,,,,,,13/Jun/11 19:26;jbellis;2752.txt;https://issues.apache.org/jira/secure/attachment/12482364/2752.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-09 15:17:43.532,,,no_permission,,,,,,,,,,,,20806,,,Tue Jun 14 17:02:36 UTC 2011,,,,,,0|i0gd8n:,93575,slebresne,slebresne,,,,,,,,,"09/Jun/11 12:49;terjem;Need to test a bit more, but quite likely this is related to repair on CFs with secondary indexes.","09/Jun/11 14:45;terjem;Seems confirmed, only able to reproduce this on a CF with secondary indexes.",09/Jun/11 15:17;jbellis;Is this data from 0.7 that got upgraded?,"09/Jun/11 15:45;terjem;Fresh 0.8 data.

Just as a side test since the ""compactionexecutor"" is involved, we issued a full compaction of that CF and it completed without any error so the source SSTables seems good.

I was trying to reproduce this locally on my desktop before leaving office to get it in a debugger. Quickly generated 10k random inserts into a CF with secondary index, but then I experienced that repair got stuck eating 100% on both nodes instead...

I did not have time to figure out if it was due to some config issue or related to the same issue though.

",09/Jun/11 22:57;lenn0x;I am trying to track down a similar issue. Instead was bootstrapping a new node in my case.,"10/Jun/11 03:50;muga_nishizawa;It seems tmp files (e.g. XXX-tmp-XXX-Data.db) that receiver node creates during repair process are broken.  EOFException occurs while RowIndexer is reading the broken files.  

According to result of scrub command, data files on sender nodes are not broken.  ","10/Jun/11 14:18;terjem;Chris: Same as the ""compactionexecutor"" or the infinite loop? (made CASSANDRA-2758 for that just now).

Seems like the sstables here has been truncated for some reason.
Rowindexer iterates through a bunch of rows just fine.
Then it reaches the problem row.

For this row, it can get the key, it jumps pass the bloomfilter etc and when it is about to read the column count, it fails and is trying to read at an offset which equals the length of the file..
So far, all the stacktraces I have seen are all are on the column count read. 

A wild guess may be that it has failed to write the actual content (columns) of the last row that was stream?

Unfortunately it does not to happen all the time, but it does only happen on the CF with secondary indexes.



","10/Jun/11 16:06;terjem;Also struck me on the way home from work today that the CF with secondary indexes also happen to be the only CF in this system which, I think, on a regular basis may actually update all columns for a key.

That is, sstables will on a regular basis have keys where no columns is valid anymore.

Not sure if that could for instance trigger something odd in the streaming?


",13/Jun/11 16:33;muga_nishizawa;It seems like timing issue.  The exception doesn't always occur even with use of same data.  ,"13/Jun/11 17:00;terjem;SSTableWriter.java:
                ColumnFamily.serializer().deserializeFromSSTableNoColumns(ColumnFamily.create(cfs.metadata), dfile);

                // don't move that statement around, it expects the dfile to be before the columns
                updateCache(key, dataSize, null);

                rowSizes.add(dataSize);
                columnCounts.add(dfile.readInt());


I believe the problem is in updateCache.
If rowcache is enabled (and it is in this case) and the row needs to be updated in cache, this will read (deserialize) the row.

However, after all the columns is read, the offset in the file is not reset back to the location where the  column count is stored and things go bad.

I haven't actually tried to change the code to test, but I tried to disable the row cache, and so far, repair seems to work fine when it is disabled.",13/Jun/11 19:26;jbellis;patch to seek back after deserializing a row to update cache with,"14/Jun/11 07:56;slebresne;Good catch. +1 (committed).
Thanks Terje.","14/Jun/11 17:02;hudson;Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Index manager cannot support deleting and inserting into a row in the same mutation""",CASSANDRA-2773,12510393,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,yulinyen,yulinyen,15/Jun/11 03:10,12/Mar/19 14:07,13/Mar/19 22:26,30/Jun/11 01:02,0.7.7,0.8.2,,,,,2,,,,,"I use hector 0.8.0-1 and cassandra 0.8.

1. create mutator by using hector api, 
2. Insert a few columns into the mutator for key ""key1"", cf ""standard"". 
3. add a deletion to the mutator to delete the record of ""key1"", cf ""standard"".
4. repeat 2 and 3
5. execute the mutator.

the result: the connection seems to be held by the sever forever, it never returns. when I tried to restart the cassandra I saw unsupportedexception : ""Index manager cannot support deleting and inserting into a row in the same mutation"". and the cassandra is dead forever, unless I delete the commitlog. 

I would expect to get an exception when I execute the mutator, not after I restart the cassandra.",,,,,,,,,,,,,,,,20/Jun/11 16:49;jbellis;2773-v2.txt;https://issues.apache.org/jira/secure/attachment/12483172/2773-v2.txt,17/Jun/11 17:17;jbellis;2773.txt;https://issues.apache.org/jira/secure/attachment/12482957/2773.txt,27/Jun/11 20:00;jancona;ASF.LICENSE.NOT.GRANTED--v1-0001-allow-deleting-a-rowand-updating-indexed-columns-init-.txt;https://issues.apache.org/jira/secure/attachment/12483997/ASF.LICENSE.NOT.GRANTED--v1-0001-allow-deleting-a-rowand-updating-indexed-columns-init-.txt,27/Jun/11 20:00;jancona;ASF.LICENSE.NOT.GRANTED--v1-0002-CASSANDRA-2773-Add-unit-tests-to-verfy-fix-cherry-pick.txt;https://issues.apache.org/jira/secure/attachment/12483998/ASF.LICENSE.NOT.GRANTED--v1-0002-CASSANDRA-2773-Add-unit-tests-to-verfy-fix-cherry-pick.txt,23/Jun/11 21:34;jancona;cassandra.log;https://issues.apache.org/jira/secure/attachment/12483644/cassandra.log,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-06-15 03:33:29.608,,,no_permission,,,,,,,,,,,,20820,,,Thu Jun 30 02:18:15 UTC 2011,,,,,,0|i0gddb:,93596,slebresne,slebresne,,,,,,,,,"15/Jun/11 03:33;jbellis;Like it says, you will have to send your deletes and mutations in different mutations.","15/Jun/11 03:45;yulinyen;sure, I know what you mean.

however, when the user uses it the wrong way, the server should at least response an error or exception... not just hold the connection and do nothing...

In addition, the server should not be dead forever, when this kind of exception occurs. ","15/Jun/11 03:51;jbellis;You're right, there's a validation problem here.",17/Jun/11 17:17;jbellis;we could add some valdation logic but it looks like it's almost as easy to just remove this limitation.  patch to do this attached.,"20/Jun/11 09:19;slebresne;Hum, we cannot remove the column from cf in ignoreObsoleteMutations() because cf is the original column family from the row mutation and that's racy with commit log write (à la CASSANDRA-2604). We should clone the column family, but maybe it's simpler to add validation logic after all ? In any case, it could be worth it adding some comment in Table.apply() or Table.ignoreObsoleteMutations(). ","20/Jun/11 16:49;jbellis;bq. we cannot remove the column from cf in ignoreObsoleteMutations()

You're right.  Fortunately I don't think that's actually necessary.  v2 attached.","20/Jun/11 16:53;slebresne;Right, +1. I still think we should add a comment somewhere saying we shouldn't change cf (and that there is no reason to change it anyway).","23/Jun/11 16:00;jbellis;Committed, with comment.  (I wish CF objects could be immutable AND efficient...)","23/Jun/11 17:27;hudson;Integrated in Cassandra-0.8 #187 (See [https://builds.apache.org/job/Cassandra-0.8/187/])
    allow deleting a rowand updating indexed columns init in the same mutation
patch by jbellis; reviewed by slebresne for CASSANDRA-2773

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1138959
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/Table.java
","23/Jun/11 21:32;jancona;We are experiencing this issue on a cluster running 0.7.6-2. Once this occurs on a server, the same message is repeated over and over again in system.log. When we attempted to restart the box it failed. I will attach the log to this issue.",23/Jun/11 21:34;jancona;Log of failed restart attempt,"24/Jun/11 17:04;jancona;Marked as critical, because when this occurs the server will not restart without removing commitlog and attending loss of data.","24/Jun/11 17:55;jbellis;I explained on-list that I'm not comfortable committing this to 0.7.  The critical thing we need to deliver in 0.7 at this stage is stability.  

The rarity of the problem combined with the complexity of the code involved leads me to conclude that it's better to live with a bug we know how to avoid, than risk introducing new ones.  Remember, the risk of new bugs affects _everyone_, while the fix can only benefit those who were generating the unusual mutation pattern here.  It's our responsibility to take a balanced view.

For the rare people who do find themselves affected here, your options include
- stay on < 0.7.6
- drain the commitlog with an earlier version before upgrading, then re-upgrade to 0.7.6 after fixing your code to not generate problematic mutations
- run an unofficial build with this patch included
- upgrade to 0.8.2 when released","24/Jun/11 19:03;jancona;I disagree. This is a bug which allows a client to make a cluster unresponsive by performing a seemingly innocuous series of operations. If that happens, the cluster is un-restartable without loss of data. I wouldn't call a release where this can occur ""stable"". So if the goal for 0.7 is stability...

WRT ""fixing your code to not generate problematic mutations,"" this may be difficult to do. I have so far identified code that does deletes followed by updates in the same mutation, but I haven't yet found any updates followed by deletes. Are we sure that only the update-followed-by-delete scenario is problematic?

In any case, even after reviewing all our code for the relevant scenarios, I would not feel comfortable deploying an 0.7 release with this vulnerability to production. The risk of a catastrophic failure is too great.
","27/Jun/11 15:54;slebresne;Actually, there isn't really much risk for data loss given that as Jonathan said, if you hit that, it's fairly easy to go back to 0.7.5, fix client code and upgrade again. Granted this is not user friendly and not something you should expect from a minor upgrade, but let at least set the record straight on the data loss part.

That being said, I don't think the patch on this ticket could screw up indexes more that we use to prior to 0.7.6, so maybe we can commit to 0.7.7 on that ground.

I'd still suggest fixing client code in the meantime. ","27/Jun/11 16:02;jbellis;bq. I don't think the patch on this ticket could screw up indexes more that we use to prior to 0.7.6

That's a valid way to frame the issue.

I'm good to commit for 0.7.7 if Jim can test the patch first, since he's the only one we've heard of hitting this in 0.7.x.  (Specifically, we want to make sure that if we query ""WHERE foo = X"" we don't get results back where foo is something other than X.  Ideally you'd start with an empty database, or at least drop + recreate indexes first to make sure the results aren't contaminated w/ corrupt entries from pre-0.7.6.)",27/Jun/11 20:03;jancona;I applied the 0.8 patch and added a couple of tests to ColumnFamilyStoreTest. The tests trigger the UnsupportedOperationException in 0.7.6 and return the correct values with the patch applied. Would you like me to test the same scenario against an actual patched server?,"27/Jun/11 21:27;jbellis;Thanks for the test case, Jim.

If by the same scenario, you mean the workload that left your commitlog throwing exceptions, then yes please.","29/Jun/11 15:35;jancona;We have deployed and tested 0.7.6 plus this patch to the affected cluster. The cluster restarted successfully and the tests that caused the original failure ran successfully. In addition, functional tests of our applications show no regressions. I also reviewed the Cassandra system logs after the testing and saw no errors or obvious problems.
","30/Jun/11 01:02;jbellis;committed.  Thanks, Jim!","30/Jun/11 02:18;hudson;Integrated in Cassandra-0.7 #519 (See [https://builds.apache.org/job/Cassandra-0.7/519/])
    add additional tests for #2773
patch by Jim Ancona; reviewed by jbellis for CASSANDRA-2773
allow deleting and inserting into an indexed row in the same mutation
patch by jbellis; reviewed by slebresne and tested by Jim Ancona for CASSANDRA-2773

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1141354
Files : 
* /cassandra/branches/cassandra-0.7/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1141353
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/Table.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
possible early deletion of commit logs,CASSANDRA-3269,12525098,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,yangyangyyy,yangyangyyy,28/Sep/11 20:27,12/Mar/19 14:07,13/Mar/19 22:26,04/Oct/11 16:59,1.0.0,,,,,,0,,,,,"I ran my cluster for about 2 days. the cluster has 2 nodes. I restarted one box several times, and the other one was always running. the one always running ended up accumulating 100GB of commit logs.


this is 1.0.0 code from about Sept 15 in github. I kept the original setting for 
#commitlog_total_space_in_mb: 4096
i.e. commented out


here is some sample of the output:

-rw-r--r-- 1 yyang yyang 134217857 2011-09-28 03:51 CommitLog-1317181834810.log
-rw-r--r-- 1 yyang yyang 134217869 2011-09-28 03:50 CommitLog-1317181764105.log
-rw-r--r-- 1 yyang yyang 134217783 2011-09-28 03:49 CommitLog-1317181694633.log
-rw-r--r-- 1 yyang yyang 134217750 2011-09-28 02:39 CommitLog-1317176955102.log
yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ ls -lt /mnt/cass/lib//cassandra/commitlog/|wc -l
727
yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ du -s /mnt/cass/lib/cassandra/commitlog/ 
95095316        /mnt/cass/lib/cassandra/commitlog/
",,,,,,,,,,,,,,,,03/Oct/11 15:14;jbellis;3269-v2.txt;https://issues.apache.org/jira/secure/attachment/12497474/3269-v2.txt,03/Oct/11 15:23;jbellis;3269-v3.txt;https://issues.apache.org/jira/secure/attachment/12497475/3269-v3.txt,03/Oct/11 11:33;slebresne;3269.patch;https://issues.apache.org/jira/secure/attachment/12497443/3269.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-09-28 20:46:19.507,,,no_permission,,,,,,,,,,,,37551,,,Tue Oct 04 16:58:45 UTC 2011,,,,,,0|i0ghv3:,94324,jbellis,jbellis,,,,,,,,,"28/Sep/11 20:46;jbellis;Hmm.  Would be useful to have a debug log -- it would say this

{noformat}
                logger.debug(""Not safe to delete commit log "" + segment + ""; dirty is "" + segment.dirtyString() + ""; hasNext: "" + iter.hasNext());
{noformat}

whenever a CF was flushed but it can't remove a segment.

For bonus points, you could add a log message in CommitLog before it creates the forceFlush runnable in createNewSegment when the size cap is exceeded.","28/Sep/11 22:07;yangyangyyy;Thanks Jonathan. 
unfortunately I removed the logs. I'll keep the load testing, if this appears again, I'll update this bug","29/Sep/11 17:01;yangyangyyy;I ran the code for a while, it created a lot of commit logs (not yet over the 4G limit ), I shut it down, ran with the code with extra debugging. 

then it showed the following messages, saying that not safe to remove. but in the end, the old commit logs were indeed removed, and I have only a new one left. (see the ls output at the end)



 INFO [FlushWriter:2] 2011-09-29 16:50:28,410 Memtable.java (line 236) Writing Memtable-multi_click_filter@1810361645(106184307/1327303837 serialized/live bytes, 1538903 ops)
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:28,410 CommitLog.java (line 501) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317314880183.log); dirty is ; hasNext: false
 INFO [FlushWriter:2] 2011-09-29 16:50:31,610 Memtable.java (line 272) Completed flushing /mnt/cass/lib/cassandra/data/testBudget_items/multi_click_filter-h-33-Data.db (118127883 bytes)
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:31,611 CommitLog.java (line 461) discard completed log segments for ReplayPosition(segmentId=1317314880183, position=0), column family 1010.
 INFO [FlushWriter:2] 2011-09-29 16:50:31,611 Memtable.java (line 236) Writing Memtable-IpFilter@1199856819(109065609/1328000737 serialized/live bytes, 1580661 ops)
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:31,612 CommitLog.java (line 501) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317314880183.log); dirty is ; hasNext: false
 INFO [FlushWriter:2] 2011-09-29 16:50:34,038 Memtable.java (line 272) Completed flushing /mnt/cass/lib/cassandra/data/testBudget_items/IpFilter-h-33-Data.db (107340235 bytes)
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:34,040 CommitLog.java (line 461) discard completed log segments for ReplayPosition(segmentId=1317314880183, position=0), column family 1013.
 INFO [FlushWriter:2] 2011-09-29 16:50:34,040 Memtable.java (line 236) Writing Memtable-opsMetrics@1355039451(1297090/77250 serialized/live bytes, 15090 ops)
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:34,040 CommitLog.java (line 501) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317314880183.log); dirty is ; hasNext: false
 INFO [FlushWriter:2] 2011-09-29 16:50:34,129 Memtable.java (line 272) Completed flushing /mnt/cass/lib/cassandra/data/testBudget_counters/opsMetrics-h-19-Data.db (6282 bytes)
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:34,130 CommitLog.java (line 461) discard completed log segments for ReplayPosition(segmentId=1317314880183, position=0), column family 1001.
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:34,130 CommitLog.java (line 501) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317314880183.log); dirty is ; hasNext: false






yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ ls -lt /mnt/cass/lib/cassandra/commitlog/
total 2474280
-rw-r--r-- 1 yyang yyang         0 2011-09-29 16:48 CommitLog-1317314880183.log
-rw-r--r-- 1 yyang yyang         0 2011-09-29 16:43 CommitLog-1317314635458.log
-rw-r--r-- 1 yyang yyang         0 2011-09-29 16:40 CommitLog-1317314406536.log
-rw-r--r-- 1 yyang yyang         0 2011-09-29 16:36 CommitLog-1317314186503.log
-rw-r--r-- 1 yyang yyang         0 2011-09-29 16:21 CommitLog-1317313281629.log
-rw-r--r-- 1 yyang yyang 115115322 2011-09-29 16:02 CommitLog-1317312032419.log
-rw-r--r-- 1 yyang yyang 134217870 2011-09-29 16:00 CommitLog-1317311854994.log
-rw-r--r-- 1 yyang yyang 134217761 2011-09-29 15:57 CommitLog-1317311685452.log
-rw-r--r-- 1 yyang yyang 134217843 2011-09-29 15:54 CommitLog-1317311520485.log
-rw-r--r-- 1 yyang yyang 134217795 2011-09-29 15:52 CommitLog-1317311347744.log
-rw-r--r-- 1 yyang yyang 134217777 2011-09-29 15:49 CommitLog-1317311185371.log
-rw-r--r-- 1 yyang yyang 134217819 2011-09-29 15:46 CommitLog-1317311021414.log
-rw-r--r-- 1 yyang yyang 134217766 2011-09-29 15:43 CommitLog-1317310849097.log
-rw-r--r-- 1 yyang yyang 134217800 2011-09-29 15:40 CommitLog-1317310682639.log
-rw-r--r-- 1 yyang yyang 134217867 2011-09-29 15:38 CommitLog-1317310520489.log
-rw-r--r-- 1 yyang yyang 134217882 2011-09-29 15:35 CommitLog-1317310339007.log
-rw-r--r-- 1 yyang yyang 134217857 2011-09-29 15:32 CommitLog-1317310162460.log
-rw-r--r-- 1 yyang yyang 134217876 2011-09-29 15:29 CommitLog-1317310000804.log
-rw-r--r-- 1 yyang yyang 134217883 2011-09-29 15:26 CommitLog-1317309831513.log
-rw-r--r-- 1 yyang yyang 134217886 2011-09-29 15:23 CommitLog-1317309658187.log
-rw-r--r-- 1 yyang yyang 134217846 2011-09-29 15:20 CommitLog-1317309490763.log
-rw-r--r-- 1 yyang yyang 134217741 2011-09-29 15:18 CommitLog-1317309323730.log
-rw-r--r-- 1 yyang yyang 134217828 2011-09-29 15:15 CommitLog-1317309154140.log
-rw-r--r-- 1 yyang yyang 134217738 2011-09-29 15:12 CommitLog-1317308989678.log
yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ ls -lt /mnt/cass/lib/cassandra/commitlog/
total 4
-rw-r--r-- 1 yyang yyang 270 2011-09-29 16:50 CommitLog-1317314880183.log
",29/Sep/11 17:58;jbellis;Are you simply writing faster than it can flush?,"29/Sep/11 18:13;yangyangyyy;I don't think so, I have set the memtable_total_space to be very high
due to my larger memory available, and rely on the
commit_log_total_size to trigger flushing  (also I changed the
getLiveSize() ... as I said in the other JIRA I commented , since my
traffic contains a large portion of counter adds). so essentially the
only trigger for flushing should come from commit_log_total_size
threshold,  but I don't see these flushing requests coming out, it's
not that the flushing started and can't finish.


I'm still running it, let me set it to a smaller threshold and
hopefully get to the point soon.

On Thu, Sep 29, 2011 at 10:59 AM, Jonathan Ellis (Commented) (JIRA)
<jira@apache.org> wrote:
","02/Oct/11 23:40;yangyangyyy;the following shows that createNewSegment() sees that for  the oldest segment file xxxxx63739.log , it's forcing out a flush of LocationInfo CF. it does this multiple times (at least 1000 times ... ), but the actualy CFS.forceFlush() always says that the CF is clean. so no flushing happens.



DEBUG [COMMIT-LOG-WRITER] 2011-10-02 12:29:22,217 CommitLog.java (line 572) create new segment
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 12:29:22,217 CommitLog.java (line 523)  commit log total size now:23488116331
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 12:29:22,217 CommitLog.java (line 578) forcing a flush on segments : CommitLog-1317524063739.log out of 176
 WARN [COMMIT-LOG-WRITER] 2011-10-02 12:29:22,217 ColumnFamilyStore.java (line 718) forceFlush requested but everything is clean
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 12:29:22,218 CommitLog.java (line 584) forcing out CF:LocationInfo
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 12:30:15,357 CommitLog.java (line 572) create new segment
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 12:30:15,357 CommitLog.java (line 523)  commit log total size now:23622334131


but when flushing (triggered by other cf) happens, the discardSegment() says that the oldest one is still dirty on LocationInfo.



DEBUG [COMMIT-LOG-WRITER] 2011-10-02 05:11:14,182 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is opsMetrics (1001), LocationInfo (0), Budget (1000), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:21:26,481 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), Budget (1000), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:21:26,868 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:32:15,341 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:32:20,905 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:32:39,725 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:32:54,023 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:33:01,934 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:33:11,167 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 08:16:28,250 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 08:16:36,208 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 08:16:56,037 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 08:17:18,003 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true



since the isClean() of CFS is determined by columnFamilies.size() and isClean() of segment is determined by lastCFWrite(), it seems that these 2 data structures somehow got out of sync.


also an easy suspect is that the lastCFWrite is not concurrent, but I can't show an exact scenario of how this would lead to the symptom I see



","03/Oct/11 06:51;yangyangyyy;this piece of logging is particularly interesting.  the LocationInfo CF is flushed, right after that, it's trying to discard oldest segment, but the discard fails saying that locationInfo bit is dirty. but supposedly the bit should have been turned off




 INFO [main] 2011-10-02 04:22:55,033 CommitLog.java (line 174) Log replay complete, 15600762 replayed mutations
 INFO [main] 2011-10-02 04:22:58,560 ColumnFamilyStore.java (line 651) flush position is ReplayPosition(segmentId=1317524063739,
 position=0)

 INFO [main] 2011-10-02 04:22:59,244 ColumnFamilyStore.java (line 665) Enqueuing flush of Memtable-LocationInfo@2115195389(29/36
 serialized/live bytes, 1 ops)
 INFO [FlushWriter:2] 2011-10-02 04:22:59,244 Memtable.java (line 283) flushing memtable LocationInfo
 INFO [FlushWriter:2] 2011-10-02 04:22:59,244 Memtable.java (line 287) entered lock flushing memtable LocationInfo
 INFO [FlushWriter:2] 2011-10-02 04:22:59,244 Memtable.java (line 290) really start flushing memtable LocationInfo
 INFO [FlushWriter:2] 2011-10-02 04:22:59,245 Memtable.java (line 236) Writing Memtable-LocationInfo@2115195389(29/36 serialized
/live bytes, 1 ops)
 INFO [FlushWriter:2] 2011-10-02 04:22:59,396 Memtable.java (line 272) Completed flushing /mnt/cass/lib/cassandra/data/system/Lo
cationInfo-h-4-Data.db (80 bytes)
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,397 CommitLog.java (line 461) discard completed log segments for ReplayPosition(segmentId=1317524063739, position=0), column family 0.
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,397 CommitLog.java (line 473) trying to discard segment CommitLog-1317524063739.log
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,398 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: false
 INFO [main] 2011-10-02 04:22:59,447 ColumnFamilyStore.java (line 651) flush position is ReplayPosition(segmentId=1317524063739, position=174)
 INFO [main] 2011-10-02 04:22:59,447 ColumnFamilyStore.java (line 665) Enqueuing flush of Memtable-LocationInfo@2010157886(53/66 serialized/live bytes, 2 ops)
 INFO [FlushWriter:2] 2011-10-02 04:22:59,448 Memtable.java (line 283) flushing memtable LocationInfo
 INFO [FlushWriter:2] 2011-10-02 04:22:59,448 Memtable.java (line 287) entered lock flushing memtable LocationInfo
 INFO [FlushWriter:2] 2011-10-02 04:22:59,448 Memtable.java (line 290) really start flushing memtable LocationInfo
 INFO [FlushWriter:2] 2011-10-02 04:22:59,448 Memtable.java (line 236) Writing Memtable-LocationInfo@2010157886(53/66 serialized/live bytes, 2 ops)
 INFO [FlushWriter:2] 2011-10-02 04:22:59,572 Memtable.java (line 272) Completed flushing /mnt/cass/lib/cassandra/data/system/LocationInfo-h-6-Data.db (163 bytes)
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,573 CommitLog.java (line 461) discard completed log segments for ReplayPosition(segmentId=1317524063739, position=174), column family 0.
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,573 CommitLog.java (line 473) trying to discard segment CommitLog-1317524063739.log
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,573 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: false
 WARN [MutationStage:23] 2011-10-02 04:23:21,015 Memtable.java (line 136) MemoryMeter uninitialized (jamm not specified as java :
","03/Oct/11 07:19;yangyangyyy;I think I found why:



CommitLogSegment.turnOffIfNotWritten() :
    void turnOffIfNotWritten(Integer cfId, Integer flushPosition)
    {
        Integer lastWritten = cfLastWrite.get(cfId);
        if (lastWritten == null || lastWritten < flushPosition)
            cfLastWrite.remove(cfId);
    }

the comparison ""<"" probably should be ""<=""

let's say we flush a Memtable, and we have only 1 commitlogsegment in system now. after the flush, no write has come in, now the post-flush job tries to discard segments, 
the context passed in points to the current pointer, which is the same as cfLastWrite, 
then the comparison fails here, and the bit in this logsegment is not removed.

later, either the cf is never flushed, or when it is being forcefully flushed, we find that there is no new writes to the CF, so the CF isclean(), so the flush is never submitted, and the post-flush can never be run, so the dirty bit can never be cleared.....

sounds right?","03/Oct/11 09:31;slebresne;Actually no. The cfLastWrite position is the last position we started writing at for the given column family, not the last position we finished writing to. More precisely, we populate cfLastWrite in CLS.turnOn() which itself is called in CLS.write(RowMutation) just before the actual write with the position prior to the write. In other words, if lastWritten == flushPosition, it can only mean that the flush grabbed the commit log position *before* the last write (and thus we don't want to turn the segment off).","03/Oct/11 11:33;slebresne;But there is something very wrong in the commit log code. The commit r1171248 broke it (we ended up always setting the position of the LastWrite at -1). That commit is from September 15th, so that could match with what you're seeing, even though it should have resulted in commit log segment being deleted sooner than is safe rather than later. Attaching patch to fix anyway.","03/Oct/11 15:14;jbellis;not to bikeshed this too much, but v2 moves context and currentPosition out of the try/catch to make it clear that cP will always be valid in the catch section.  also merging declaration and assignment of cP prevents bogus refactors like the original regression.",03/Oct/11 15:23;jbellis;v3 inlines cP entirely.,03/Oct/11 15:25;slebresne;+1 on v3,03/Oct/11 15:29;jbellis;committed v3,"03/Oct/11 16:22;slebresne;To be clear, the committed patch is probably not the source of the problem Yang is seeing, so let's keep that open for now.

Yang, I don't see anything yet from the logs you've pasted that seem wrong per se. Can you try to lower the commit_log_total_size
to something small (like 300MB) and see if you can reproduce easily ? (you should hopefully never have more than 3 commit logs (2 even) since when the third one is created, we should be over the limit and the older one should be deleted).","03/Oct/11 16:31;yangyangyyy;Sylvain:


I did try to move down the threshold to 300MB, but in that case, it does not seem to occur easily (I haven't seen the issue appear after running under a 300MB threshold for 10 hours).


but if you look at  comment https://issues.apache.org/jira/browse/CASSANDRA-3269?focusedCommentId=13119163&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13119163


you see that LocationInfo being flushed already, twice, once at position 0, once at 174, then right after that, it tries to discard the segment, but complains that it's dirty, and the only dirty bit is locationInfo itself. that is the part that doesn't sound right.

","03/Oct/11 17:05;yangyangyyy;I'm a bit unclear about the logic in cfLastWrites vs position, it seems that cflastWrites take value from the logwriter position, which always increases, so cfLastWrite can be only smaller or equal than the position, which is the value obtained through maybeSwitchMemtable()---->getContext() .


but in log I see


 WARN [COMMIT-LOG-WRITER] 2011-10-03 15:48:38,038 CommitLogSegment.java (line 205) turnOffIfNotWritten:ad_ip_agent 50020888/3959
3935


the above logging is produced in 



    void turnOffIfNotWritten(Integer cfId, Integer flushPosition)
    {   
        Integer lastWritten = cfLastWrite.get(cfId);
                String keypace = Schema.instance.getCF(cfId).left;

                final ColumnFamilyStore cfs = Table.open(keypace).getColumnFamilyStore(cfId);
                logger.warn(""turnOffIfNotWritten:"" + cfs.columnFamily + "" "" + lastWritten + ""/"" + flushPosition);

        if (lastWritten == null || lastWritten < flushPosition)
            cfLastWrite.remove(cfId);
    }
","03/Oct/11 17:34;yangyangyyy;I changed the commitlog rotation size to 5MB and total to 200MB, see if the symptoms come out more easily","03/Oct/11 21:37;yangyangyyy;I added a warn() to CLS.turnOffIfNotWritten() :

    void turnOffIfNotWritten(Integer cfId, Integer flushPosition)
    {  
        Integer lastWritten = cfLastWrite.get(cfId);
                String keypace = Schema.instance.getCF(cfId).left;

                final ColumnFamilyStore cfs = Table.open(keypace).getColumnFamilyStore(cfId);
                logger.warn(""turnOffIfNotWritten:"" + cfs.columnFamily + "" "" + lastWritten + ""/"" + flushPosition);

        if (lastWritten == null || lastWritten < flushPosition)
            cfLastWrite.remove(cfId);
    }



I saw many cases where lastWritten is > than the flushPosition, this does not make sense. 
I think this is definitely going to lead to the un-purged commit logs symptoms, IF no more writes are added onto the dirty CFs in the oldest segment. so the analysis I gave before still holds, but we just need to find out why  lastWritten > flushPosition.

 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:19,552 CommitLogSegment.java (line 205) turnOffIfNotWritten:ad_impression_session 96030/46355
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:22,688 CommitLogSegment.java (line 205) turnOffIfNotWritten:ad_ip_agent 3070922/46355
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:24,397 CommitLogSegment.java (line 205) turnOffIfNotWritten:multi_click_filter 4996295/46522
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:25,576 CommitLogSegment.java (line 205) turnOffIfNotWritten:session_limit_filter 5242578/59906
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:27,013 CommitLogSegment.java (line 205) turnOffIfNotWritten:measuredSession 5241697/80685
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:28,740 CommitLogSegment.java (line 205) turnOffIfNotWritten:IpFilter 5242052/96877
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:28,919 CommitLogSegment.java (line 205) turnOffIfNotWritten:session_limit_filter 5242475/870755
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:29,187 CommitLogSegment.java (line 205) turnOffIfNotWritten:measuredSession 5242807/890241
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:29,471 CommitLogSegment.java (line 205) turnOffIfNotWritten:IpFilter 5242120/890241
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:29,690 CommitLogSegment.java (line 205) turnOffIfNotWritten:IpFilter 3422607/768691
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:45:36,674 CommitLogSegment.java (line 205) turnOffIfNotWritten:ad_impression_session 3552787/165636
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:45:39,484 CommitLogSegment.java (line 205) turnOffIfNotWritten:ad_ip_agent 5242441/165636
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:45:40,832 CommitLogSegment.java (line 205) turnOffIfNotWritten:multi_click_filter 5241748/165636
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:45:41,670 CommitLogSegment.java (line 205) turnOffIfNotWritten:ad_ip_agent 5242691/1387810
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:45:41,863 CommitLogSegment.java (line 205) turnOffIfNotWritten:multi_click_filter 5242353/1409533


","03/Oct/11 21:52;yangyangyyy;ok, this looks to be what could lead to the symptoms:


let's say the commitlog position is 0 now.

Table.apply() :

  switchlock.readlock.lock();
          commitlog.instance.add(mutation) ====> executor.add(new LogRecordAdder())
                             // but the Adder is not really executed yet, just submitted.
         
         /// add to memtable
  switchlock.readlock.unlock();



then we have a flush.
  CFS.maybeSwitchMemtable()
 
  switchlock.writelock.lock();
    ctx = Commitlog.instance.getContext();   // returns 0 

  

/// now the logRecordAdder is executed , and advances the position to 199;
    
/// blahblah
   postflusher.executes(
      discardCompletedSegments
                 
             turnOffIfNotWritten() ====> check fails, so the CF written by the last Adder
                                         is not cleaned
  )
  unlock


as  a result, IF the CF is never written again, it will forever remain dirty in the segment.



then we need to maintain the order between the adder and the getContext() call in maybeSwitchMemtable()


","03/Oct/11 22:38;yangyangyyy;per  https://issues.apache.org/jira/browse/CASSANDRA-3269?focusedCommentId=13119382&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13119382


","04/Oct/11 07:37;slebresne;{noformat}
switchlock.writelock.lock();
ctx = Commitlog.instance.getContext(); // returns 0

/// now the logRecordAdder is executed , and advances the position to 199;
{noformat}

That could (should) not happen. The commit log is mono-threaded by mean of its executor. And every action on the commit log happens in a task on that executor. In particular Commitlog.instance.getContext() push a task on the commit log executor. This means that a logRecordAdder that has been pushed before the switchlock is grabbed for the flush cannot return a position that is after the position return by getContext() at the beginning of the flush (i.e, it either return a greater position in the same segment or a position in a newer segment). So thanks to the switchlock (that stops writes momentarily), we know that that the ctx position for the flush is after every write that has been done in the memtable we are flushing.
Now we call discardCompletedSegments later, so what can happen is that there has been writes to the commit log between the time we had grabbed that flush position and the time discardCompletedSegments is called. That is the goal of CL.lastWrite. If when we discard segments, there has been no write on this segment after the flush position we are considering, then the segment can be turnOff, otherwise there is still ""active"" write for the column family so we don't turn it off. But if that happens (i.e, if lastWrite >= flushPosition), it means that the writes have been done in a newer memtable than the one we just flushed. So the segment will be turnOff when the flush for that newer memtable happens.

{quote}
you see that LocationInfo being flushed already, twice, once at position 0, once at 174, then right after that, it tries to discard the segment, but complains that it's dirty, and the only dirty bit is locationInfo itself. that is the part that doesn't sound right.
{quote}

That log is not fully conclusive, because it is entirely possible that there has been some write to that commit log after the second flush position. In which case it's ok to not unmark LocationInfo.

In particular, the last line of the log:
{noformat}
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,573 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: false
{noformat}
shows that the segment is the active one (hasNext == false), so it's perfectly reasonable to think there has been some write since position 174. The log is missing the value of lastWrite in the message to be able to say if it's the case.","04/Oct/11 16:12;yangyangyyy;bq. That could (should) not happen. The commit log is mono-threaded by mean of its executor.



this is no longer the case since it was changed (particularly getContext() ) in https://issues.apache.org/jira/browse/CASSANDRA-3253 due to possibility of deadlock","04/Oct/11 16:18;slebresne;Hum, #3253 moved a flush to an unrelated executor, so that a task on the commit log executor don't wait for another task on the commit log executor *because* that commit log executor is mono-threaded and so this would deadlock. But the commit log executor is still mono-thread (all hell would break loose if it wasn't) and CL.getContext() does push a task on that executor.","04/Oct/11 16:42;slebresne;{quote}
there is hard evidence in that link that cfWrite has appeared to be > flushPosition.
this would indeed lead to the turnOffIfNotWritten() not turning off the dirty bit, this point is right, no???
{quote}

Yes, it can and will happen that cfWrite > flushPosition. And then the dirty bit will not be turning off. That is right. However, if that happen, it means that there is a write for the column family in a memtable that is not flushed yet. And thus the flush of this memtable would clean the dirty bit off. The invariant that the code enforces  (unless there is a bug of course) is that 'for the flush of a given memtable, every write in that memtable will have been written to the commit log at a position that is < the flushPosition for *that* flush'. If follows that if cfWrite > flushPosition, there a write in a unflushed memtable.","04/Oct/11 16:58;yangyangyyy;thanks Sylvain.

now I see that cfWrite> flushPosition is  fine .
after I updated to the git HEAD  , the run in the last day has not produced excessive commitlogs. let me keep running and see if it's gone now


Yang",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read repair/reconciliation breaks slice based iteration at QUORUM,CASSANDRA-2643,12507010,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,byronclark,scode,scode,12/May/11 16:27,12/Mar/19 14:07,13/Mar/19 22:26,02/Aug/11 13:27,1.0.0,,,,,,0,,,,,"In short, I believe iterating over columns is impossible to do reliably with QUORUM due to the way reconciliation works.

The problem is that the SliceQueryFilter is executing locally when reading on a node, but no attempts seem to be made to consider limits when doing reconciliation and/or read-repair (RowRepairResolver.resolveSuperset() and ColumnFamily.resolve()).

If a node slices and comes up with 100 columns, and another node slices and comes up with 100 columns, some of which are unique to each side, reconciliation results in > 100 columns in the result set. In this case the effect is limited to ""client gets more than asked for"", but the columns still accurately represent the range. This is easily triggered by my test-case.

In addition to the client receiving ""too many"" columns, I believe some of them will not be satisfying the QUORUM consistency level for the same reasons as with deletions (see discussion below).

Now, there *should* be a problem for tombstones as well, but it's more subtle. Suppose A has:

  1
  2
  3
  4
  5
  6

and B has:

  1
  del 2
  del 3
  del 4
  5
  6 

If you now slice 1-6 with count=3 the tombstones from B will reconcile with those from A - fine. So you end up getting 1,5,6 back. This made it a bit difficult to trigger in a test case until I realized what was going on. At first I was ""hoping"" to see a ""short"" iteration result, which would mean that the process of iterating until you get a short result will cause spurious ""end of columns"" and thus make it impossible to iterate correctly.

So; due to 5-6 existing (and if they didn't, you legitimately reached end-of-columns) we do indeed get a result of size 3 which contains 1,5 and 6. However, only node B would have contributed columns 5 and 6; so there is actually no QUORUM consistency on the co-ordinating node with respect to these columns. If node A and C also had 5 and 6, they would not have been considered.

Am I wrong?

In any case; using script I'm about to attach, you can trigger the over-delivery case very easily:

(0) disable hinted hand-off to avoid that interacting with the test
(1) start three nodes
(2) create ks 'test' with rf=3 and cf 'slicetest'
(3) ./slicetest.py hostname_of_node_C insert # let it run for a few seconds, then ctrl-c
(4) stop node A
(5) ./slicetest.py hostname_of_node_C insert # let it run for a few seconds, then ctrl-c
(6) start node A, wait for B and C to consider it up
(7) ./slicetest.py hostname_of_node_A slice # make A co-ordinator though it doesn't necessarily matter

You can also pass 'delete' (random deletion of 50% of contents) or 'deleterange' (delete all in [0.2,0.8]) to slicetest, but you don't trigger a short read by doing that (see discussion above).
",,,,,,,,,,,,,,,,28/Jul/11 03:58;byronclark;ASF.LICENSE.NOT.GRANTED--short_read_0.8.sh;https://issues.apache.org/jira/secure/attachment/12488060/ASF.LICENSE.NOT.GRANTED--short_read_0.8.sh,29/Jul/11 06:25;byronclark;CASSANDRA-2643-poc.patch;https://issues.apache.org/jira/secure/attachment/12488173/CASSANDRA-2643-poc.patch,30/Jul/11 02:45;byronclark;CASSANDRA-2643-v2.patch;https://issues.apache.org/jira/secure/attachment/12488280/CASSANDRA-2643-v2.patch,02/Aug/11 04:11;byronclark;CASSANDRA-2643-v3.patch;https://issues.apache.org/jira/secure/attachment/12488855/CASSANDRA-2643-v3.patch,29/Jul/11 03:39;byronclark;reliable_short_read_0.8.sh;https://issues.apache.org/jira/secure/attachment/12488169/reliable_short_read_0.8.sh,13/May/11 15:01;slebresne;short_read.sh;https://issues.apache.org/jira/secure/attachment/12479111/short_read.sh,12/May/11 16:28;scode;slicetest.py;https://issues.apache.org/jira/secure/attachment/12478987/slicetest.py,,,,,7.0,,,,,,,,,,,,,,,,,,,2011-05-13 15:01:43.628,,,no_permission,,,,,,,,,,,,20748,,,Tue Aug 02 14:22:27 UTC 2011,,,,,,0|i0gclb:,93470,jbellis,jbellis,,,,,,,,,"12/May/11 19:04;scode;Unless I'm off, the problem with deleted columns is fairly similar to that of deleted rows. Like with range ghosts, an option would be to propagate tombstones to clients.

Another option is to make read responses include the range for which they are authoritative, and then only consider the intersection of all responses' authoritative ranges when reconciling results. The authoritative range of the response would have to be communicated back to the client, such that it can continue iterating from the appropriate column name even without actually receiving a column for that name.

Other than failing requests with a new kind of error, I don't see a good way to fix the tombstone case (the over-shoot case can be fixed by just capping results) without changing the protocol. For obvious reason's we don't want the co-originating node to go into a potentially unbounded re-try spin until sufficient results are obtained from all nodes participating.

FWIW, returning iteration state feels pretty clean to me (it's what our layer implements on top to enable easier iteration). It is also compatible with future improvements to e.g. cap the size of a response by bytes for safely slicing over columns whose size you do not know. By removing the strict requirement to deliver exactly the number of asked-for columns else it be interpreted as ""out of columns"", significant flexibility is gained. So if the only option for a clean fix is truly to change the protocol, at least other positive benefits may be had.","13/May/11 08:13;scode;I realized I failed to make the possibly most important point: That you can indeed get short reads such that iteration will stop early. Consider 3 nodes, RF=3, QUORUM.

* Node A has [10,40] of columns
* Node B has [10,40] of columns
* Node C has [10,20] of column deletions for the columns that A/B has,
but does NOT have any for [21,40] because it was down when those were written

Now a client slices [10,1000] with count=11. The co-ordinating node will reconcile that; C's tombstones will override A/B (I'm assuming tombstones are later than A+B's columns), but since C is lacking the ""remainder"" of columns you don't just get some columns at lowered consistency level - you actually get a ""short"" result, and the application or high-level client will believe that the iteration is complete.

This was the primary reason why I said in the OP that I believed ""iterating over columns is impossible to do reliably with QUORUM"". I somehow lost this when re-phrasing the JIRA post a couple of times.

Note: The short read case is not something I have tested and triggered, so is based on extrapolation from my understanding of the code.

","13/May/11 15:01;slebresne;You are right, there is a problem here.

I'll just note that you example is not a good example for QUORUM, because the fact that only C ""has [10,20] of column deletions"" means this situation did not happen with QUORUM writes (and the consistency guarantee for QUORUM involves read and write being at QUORUM).

However, this still show that there is a problem for ONE (writes) + ALL (reads). And it's not hard to come up with an example for QUORUM (reads and writes). Just consider the case where you insert like 10 columns and then delete the 3 first ones but with each time 1 node down, but never the same one.

To make this concrete, I'm attaching a script that produce this ""short read"" effect. Disclaimer: it uses https://github.com/pcmanus/ccm and require the patch I've attached to CASSANDRA-2646 (to be able to do a bounded slice with the cli).

The simplest way to fix that I see (which doesn't imply simple per se) would be to requests more columns if we're short after a resolve on the coordinator.  Yes in theory it means we may have to do a unknown number of such re-request, but in practice I strongly doubt this will be a problem. The problem has very little chance to happen in real life to start with (for QUORUM, my script is simple but implements something that has very very little change to actually happen in real life -- especially with HH, read repair and repair), but the chances that if that happens we need more that 1 re-request are ridiculously small.
","17/May/11 20:34;scode;You're right of course - my example was bogus. I'll also agree about re-try being reasonable under the circumstances, though perhaps not optimal.

With regards to the fix. Let me just make sure I understand you correctly. So given a read command with a limit N that yields <N columns (post-reconciliation), we may need to re-request from one or more nodes. But how do we distinguish between a legitimate short read and a spurious short read? The criteria seems to me to be, that a read is potentially spuriously short if ""one or more of the nodes involved returned a NON-short read"". If all of them returned short reads, it's fine; only if we have results from a node that we cannot prove did indeed exhaust its list of available columns do we need to check.

That is my understanding of your proposed solution, and that does seem doable on the co-ordinator side without protocol changes since we obviously know what we actually got from each node; it's just a matter of coding acrobatics (not sure how much work).

However, would you agree with this claim: This would fix the spurious short read problem specifically, but does not address the more general problem of consistency - i.e., one might receive columns that have not gone through reconciliation by QUORUM?

If we are to solve that, while still not implying protocol changes, I believe we need to do re-tries whenever a more general condition is true: That we do not have confirmed QUORUM for the full range implied by the start+limit range that we are being asked for. In other words, if one or more of the nodes participating in the read returned a response that satisfies:

  (1) The response was *not* short.
    AND
  (2) The response ""last"" column was < than the ""last"" column that we are to return post-reconciliation.

Lacking a protocol change to communicate authoritative ranges of responses, and given that the premise is that we *must* deliver start+limit unless there are < limit number of columns available, we necessarily can only consider the full range (first-to-last column) of a response as authoritative (except in the case of a short read, in which case it's authoritative to infinity).

Without revisiting the code to try to figure out what the easiest way to implement it is, one thought is that if you agree that a clean long-term fix would be to communicate authoritativeness in responses, perhaps one can at least make the logic to handle this compatible with that way of thinking. It's just that until protocol changes can happen, we'd (1) infer authoritativeness from columns/tombstones in the result instead of from explicit indicators in a response, and (2) since we cannot propagate short ranges to clients, we must re-request instead of cleanly return a short-but-not-eof-indicating range to the client.

Thoughts?",28/Jul/11 03:58;byronclark;[^short_read_0.8.sh] is an update to [^short_read.sh] that works with 0.8.x.,"29/Jul/11 02:18;byronclark;I'm having a really hard time reproducing this issue consistently on trunk. That's not to say that it doesn't happen, but I'm only seeing it one time out of fifteen using short_read.sh.

I'm running this on Linux and, from the logs, it looks like the downed node isn't being detected as down, even when I sleep 10 seconds before doing the set.

Any hints on getting this failure to happen more reliably?",29/Jul/11 03:39;byronclark;[^reliable_short_read_0.8.sh] reproduces the issue for me every time.  This script requires the following commit to ccm: https://github.com/byronclark/ccm/commit/974a5773228e783d4a91d7ba46d744e5a1216377,"29/Jul/11 06:25;byronclark;The attached [^CASSANDRA-2643-poc.patch], while extremely ugly, serves as a proof of concept that all the data is available and the short read problem can be corrected.","30/Jul/11 02:45;byronclark;[^CASSANDRA-2643-v2.patch] makes the following improvements on [^CASSANDRA-2643-poc.patch]:
* Cleans up duplicated code for counting live columns.
* Removes the need to store the ReadCommand in the RepairCallback.

Remaining issue to be dealt with:
* Storing maxLiveColumns in the resolver feels wrong, but that's where the data is generated.  We need to know how many live rows the biggest return had to detect if this is actually a short read. I'm open to suggestions a better place for that count to live.
","01/Aug/11 22:52;jbellis;Looks good on the whole.  One point to clear up:

{code}
if ((maxLiveColumns >= sliceCommand.count) && (liveColumnsInRow < sliceCommand.count))
{code}

maxLiveColumns is the max from a single response, so how can it be greater than sliceCommand.count?  Would this be a valid reformulation?

{code}
assert maxLiveColumns <= sliceCommand.count;
if ((maxLiveColumns == sliceCommand.count) && (liveColumnsInRow < sliceCommand.count))
{code}

Minor things I'd like to clean up:

- is maxLiveColumns valid on any AbstractRR subclass other than RRR? If not I'd rather move it in there and throw an UnsupportedOperation in ARR.
- Would prefer initializing commandsToRetry to Collections.emptyList, to avoid allocating that list in the common case that no retries are needed.  (Then clear of course needs to become allocate.)
",02/Aug/11 04:11;byronclark;[^CASSANDRA-2643-v3.patch] incorporates Jonathan's suggestions.,"02/Aug/11 13:27;jbellis;added a skeleton getMaxLiveColumns to RangeSliceResponseResolver (and created CASSANDRA-2986 to follow up on that) and committed.

thanks!","02/Aug/11 14:22;hudson;Integrated in Cassandra #992 (See [https://builds.apache.org/job/Cassandra/992/])
    fix ""short reads"" in [multi]get
patch by Byron Clark; reviewed by jbellis for CASSANDRA-2643

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1153115
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/service/IResponseResolver.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RepairCallback.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/AbstractRowResolver.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RowRepairResolver.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/AbstractColumnContainer.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RangeSliceResponseResolver.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CounterColumn Increments lost after restart,CASSANDRA-2642,12506986,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,uctopcu,uctopcu,12/May/11 13:47,12/Mar/19 14:07,13/Mar/19 22:26,12/May/11 15:22,0.8.0,,,,,,0,,,,,"While testing the 0.8.0-rc1; 

I've come across this problem. In order to reproduce please follow the steps:

- create a ColumnFamily named Counters
- do a few increments on a column
- get column value
- kill cassandra
- start cassandra
- get the column value

please see the cli-history.txt or pastebin http://pastebin.com/9jYdDiRY",,,,,,,,,,,,,,,,12/May/11 14:07;slebresne;0001-Don-t-consider-CL-mutation-from-remote-host.patch;https://issues.apache.org/jira/secure/attachment/12478966/0001-Don-t-consider-CL-mutation-from-remote-host.patch,12/May/11 13:48;uctopcu;cli-history.txt;https://issues.apache.org/jira/secure/attachment/12478963/cli-history.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-12 14:07:54.236,,,no_permission,,,,,,,,,,,,20747,,,Fri May 13 00:52:38 UTC 2011,,,,,,0|i0gcl3:,93469,jbellis,jbellis,,,,,,,,,12/May/11 14:07;slebresne;Patch attach with a unit test. We were considering the mutation from the CL as coming from remote host and thus not considering them as new increment (and thus we were keeping the max instead of summing),"12/May/11 14:45;jbellis;comment needs update:
{noformat}
                 // This is coming from a remote host
{noformat}

otherwise, +1",12/May/11 15:22;slebresne;Committed with comment removed,"13/May/11 00:52;hudson;Integrated in Cassandra-0.8 #103 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/103/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timeout exception for quorum reads after upgrade from 1.0.2 to 1.0.5,CASSANDRA-3551,12533530,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,apache.zli,apache.zli,01/Dec/11 22:58,12/Mar/19 14:07,13/Mar/19 22:26,07/Dec/11 17:35,1.0.6,,,,,,1,,,,,"I upgraded from 1.0.2 to 1.0.5. For some column families always got TimeoutException. I turned on debug and increase rpc_timeout to 1 minute, but still got timeout. I believe it is bug on 1.0.5.

ConsistencyLevel is QUORUM, replicate factor is 3. 

Here are partial logs. 


DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,717 StorageProxy.java (line 813) RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=SlicePre
dicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 00 0
2 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 
00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/999/99999], max_keys=1024}
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,718 StorageProxy.java (line 1012) restricted ranges for query [PROD/US/000/0,PROD/US/999/99999] are [[PROD/US/000/0,PROD/US/300/~], (PROD/US/300/~,PROD/
US/600/~], (PROD/US/600/~,PROD/US/999/99999]]
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,720 VoxeoStrategy.java (line 157) ReplicationFactor 3
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,720 VoxeoStrategy.java (line 33) PROD/US/300/~
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,721 VoxeoStrategy.java (line 96) End region for token PROD/US/300/~ PROD/US/300/~ 10.92.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,721 VoxeoStrategy.java (line 96) End region for token PROD/US/300/~ PROD/US/600/~ 10.72.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,721 VoxeoStrategy.java (line 96) End region for token PROD/US/300/~ PROD/US/999/~ 10.8.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,723 VoxeoStrategy.java (line 157) ReplicationFactor 3
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,724 ReadCallback.java (line 77) Blockfor/repair is 2/false; setting up requests to /10.92.208.103,/10.72.208.103
DEBUG [WRITE-/10.92.208.103] 2011-12-01 22:25:39,725 OutboundTcpConnection.java (line 206) attempting to connect to /10.92.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,726 StorageProxy.java (line 859) reading RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=
SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00
 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 7
2 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/300/~], max_keys=1024} from /10.92.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,726 StorageProxy.java (line 859) reading RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=
SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00
 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 7
2 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/300/~], max_keys=1024} from /10.72.208.103
DEBUG [WRITE-/10.8.208.103] 2011-12-01 22:25:39,727 OutboundTcpConnection.java (line 206) attempting to connect to /10.8.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,727 StorageProxy.java (line 859) reading RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=
SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00
 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 7
2 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/300/~], max_keys=1024} from /10.8.208.103
DEBUG [ReadStage:1] 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 0 of 1024: active:false:1@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 1 of 1024: name:false:4@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 2 of 1024: providerData:false:2283@1321549067179000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 3 of 1024: providerID:false:1@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,732 SliceQueryFilter.java (line 123) collecting 4 of 1024: timestamp:false:13@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,732 SliceQueryFilter.java (line 123) collecting 5 of 1024: vendorData:false:2364@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,733 ColumnFamilyStore.java (line 1331) scanned DecoratedKey(PROD/US/001/1, 50524f442f55532f3030312f31)
DEBUG [ReadStage:1] 2011-12-01 22:25:39,733 RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=Row(key=DecoratedKey(PROD/US/001/1, 50524f442f55532f3030312f31), cf=ColumnFamily(dataP
rovider [active:false:1@1322777621601000,name:false:4@1322777621601000,providerData:false:2283@1321549067179000,providerID:false:1@1322777621601000,timestamp:false:13@1322777621601000,vendorData:f
alse:2364@1322777621601000,]))} to 72@/10.72.208.103
DEBUG [RequestResponseStage:1] 2011-12-01 22:25:39,734 ResponseVerbHandler.java (line 44) Processing response on a callback from 72@/10.72.208.103
DEBUG [RequestResponseStage:2] 2011-12-01 22:25:39,887 ResponseVerbHandler.java (line 44) Processing response on a callback from 71@/10.92.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,889 SliceQueryFilter.java (line 123) collecting 0 of 2147483647: active:false:1@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 1 of 2147483647: name:false:4@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 2 of 2147483647: providerData:false:2283@1321549067179000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 3 of 2147483647: providerID:false:1@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 4 of 2147483647: timestamp:false:13@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,891 SliceQueryFilter.java (line 123) collecting 5 of 2147483647: vendorData:false:2364@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,892 StorageProxy.java (line 867) range slices read DecoratedKey(PROD/US/001/1, 50524f442f55532f3030312f31)
DEBUG [RequestResponseStage:3] 2011-12-01 22:25:39,936 ResponseVerbHandler.java (line 44) Processing response on a callback from 73@/10.8.208.103
DEBUG [ScheduledTasks:1] 2011-12-01 22:26:19,788 LoadBroadcaster.java (line 86) Disseminating load info ...
DEBUG [pool-2-thread-1] 2011-12-01 22:26:39,904 StorageProxy.java (line 874) Range slice timeout: java.util.concurrent.TimeoutException: Operation timed out.
","Linux, Cassandra 1.0.5",,,,,,,,,,,,,,,07/Dec/11 17:07;slebresne;3551.patch;https://issues.apache.org/jira/secure/attachment/12506485/3551.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-01 23:08:51.803,,,no_permission,,,,,,,,,,,,219258,,,Wed Dec 14 19:54:21 UTC 2011,,,,,,0|i0glav:,94881,jbellis,jbellis,,,,,,,,,01/Dec/11 23:08;jbellis;any exceptions on the other nodes?,"02/Dec/11 09:57;jalkanen;I am seeing this too; switching to ConsistencyLevel.ONE helps, but does not solve the problem completely, i.e. queries fail less often.","02/Dec/11 10:23;slebresne;More infos on you respective setups could help. For instance:
* you said, 'For some column families'. Is there something specific to those column families ? Are they using compression? leveled compaction?
* Janne: you're seeing it too, but on which version exactly did you definitively not see this problem and on which are you definitively seeing it? Is it 1.0.2 and 1.0.5 respectively as for Zhong?
* As Jonathan said, are you seeing any error in any node logs?","02/Dec/11 11:28;jalkanen;1.0.5, RF 3, 3 node cluster on EC2.  I upgraded just recently directly from 0.6.13, so I have not been on any earlier 1.0.x version.  No compression, just a straightforward upgrade with minimal tuning to the cassandra.yaml file.  2GB heap, maybe ~1GB in use.  Happens with column families which have 20 rows, CFs which have 10000 rows and more.  Happens when trying to read 100 rows at a time, happens when trying to read 10k rows at a time.  The only factor that I've noticed while trying to tune that has any effect is changing the CL.

No errors in node logs, no anomalies in system monitoring (like suddenly increased disk latency).  Only cassandra's storageproxy latency goes way up (hundreds of milliseconds), before failure.

Here is the exception from hector:

Caused by: me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
	at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:42)
	at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:163)
	at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:145)
	at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:233)
	at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:131)
	at me.prettyprint.cassandra.service.KeyspaceServiceImpl.getRangeSlices(KeyspaceServiceImpl.java:167)
	at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:67)
	at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:63)
	at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
	at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
	at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery.execute(ThriftRangeSlicesQuery.java:62)

Here's the CF definition:

    ColumnFamily: XXXX
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Row Cache Provider: org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
","02/Dec/11 18:31;apache.zli;There is no exceptions on other nodes. 

I might be wrong about 'For some column families'. I saw another column family failed with Range Slice too. It works for insert. It might work for others retrieve command. I need test more when I have time. There is no compression, one row data only.



 ColumnFamily: dataProvider
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds / keys to save : 1024.0/0/all
      Row Cache Provider: org.apache.cassandra.cache.SerializingCacheProvider
      Key cache size / save period in seconds: 1024.0/14400
      GC grace seconds: 432000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Column Metadata:
        Column Name: active
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Name: dataProvider_active_idx
          Index Type: KEYS
        Column Name: object
          Validation Class: org.apache.cassandra.db.marshal.BytesType
        Column Name: providerData
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Name: dataProvider_providerData_idx
          Index Type: KEYS
        Column Name: providerID
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Name: dataProvider_providerID_idx
          Index Type: KEYS
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
 ","06/Dec/11 18:06;dccwilliams;I hit a version of this problem...

I upgraded a production cluster from 1.0.3 (from a non-official version patched for CASSANDRA-3510) to 1.0.5. The aim was to pass CASSANDRA-3440.

This generated a timeout storm on range slices and I have reverted. 

Notes:

1/ The 1.0.5 node CPUs all showed tiny load - in fact, they seemed to be substantially less loaded than the 1.0.3 nodes were/are again

2/ The system.log files on the 1.0.5 nodes didn't record any errors

3/ range_slice timeout storm experienced in application layer. Example log trace below

org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out
        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_slice(Cassandra.java:560) ~[cassandra-thrift-1.0.1.jar:1.0.1]
        at org.apache.cassandra.thrift.Cassandra$Client.get_slice(Cassandra.java:542) ~[cassandra-thrift-1.0.1.jar:1.0.1]
        at org.scale7.cassandra.pelops.Selector$3.execute(Selector.java:683) ~[scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Selector$3.execute(Selector.java:680) ~[scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Operand.tryOperation(Operand.java:86) [scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Operand.tryOperation(Operand.java:66) [scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Selector.getColumnOrSuperColumnsFromRow(Selector.java:680) [scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Selector.getColumnsFromRow(Selector.java:689) [scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Selector.getColumnsFromRow(Selector.java:676) [scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Selector.getColumnsFromRow(Selector.java:562) [scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at com.fightmymonster.game.Monsters.getMonster(Monsters.java:92) [fmmServer.jar:na]
        at com.fightmymonster.rmi.monsters.GetMonster.doWork(GetMonster.java:25) [fmmServer.jar:na]
        at org.wyki.networking.starburst.SyncRmiOperation.run(SyncRmiOperation.java:50) [fmmServer.jar:na]
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_22]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_22]
        at java.lang.Thread.run(Thread.java:662) [na:1.6.0_22]
Caused by: java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.6.0_22]
        at java.net.SocketInputStream.read(SocketInputStream.java:129) ~[na:1.6.0_22]
        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127) ~[libthrift-0.6.1.jar:0.6.1]
        ... 23 common frames omitted","07/Dec/11 17:07;slebresne;This is due to CASSANDRA-3440. More precisely, the fact that in RowRepairResolver it has changed the message from the mutation verb to the read_repair one. The problem is that ReadRepairVerbHandler does not respond anything, but the RowRepairResolver is waiting for a response.

After looking, I haven't found any part of the code using the read_repair verb handler except for the RowRepairResolver (which would mean that before CASSANDRA-3440 it wasn't used at all, so it's worth having someone else double checking I didn't missed anything), so a simple fix is to make the ReadRepairVerbHandler return an acknowledgment. Attaching a patch for that.","07/Dec/11 17:29;jbellis;To be explicit: this only affects queries at CL > ONE.

bq. I haven't found any part of the code using the read_repair verb handler except for the RowRepairResolver, which would mean that before CASSANDRA-3440 it wasn't used at all

Right, it was used for a while, then we switched to MUTATION Verb to get the reply for ""free"" when we changed StorageProxy to wait for repair acks, but we never cleared out the Verb or RRVH.

+1 on the fix.","07/Dec/11 17:35;slebresne;Committed, thanks",14/Dec/11 19:54;jalkanen;Confirmed fixed in 1.0.6. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memtable with no post-flush activity can leave commitlog permanently dirty,CASSANDRA-2829,12511589,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,amorton,amorton,27/Jun/11 01:54,12/Mar/19 14:07,13/Mar/19 22:26,01/Aug/11 15:12,0.8.3,,,,,,0,,,,,"Only dirty Memtables are flushed, and so only dirty memtables are used to discard obsolete commit log segments. This can result it log segments not been deleted even though the data has been flushed.  

Was using a 3 node 0.7.6-2 AWS cluster (DataStax AMI's) with pre 0.7 data loaded and a running application working against the cluster. Did a rolling restart and then kicked off a repair, one node filled up the commit log volume with 7GB+ of log data, there was about 20 hours of log files. 

{noformat}
$ sudo ls -lah commitlog/
total 6.9G
drwx------ 2 cassandra cassandra  12K 2011-06-24 20:38 .
drwxr-xr-x 3 cassandra cassandra 4.0K 2011-06-25 01:47 ..
-rw------- 1 cassandra cassandra 129M 2011-06-24 01:08 CommitLog-1308876643288.log
-rw------- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308876643288.log.header
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 01:36 CommitLog-1308877711517.log
-rw-r--r-- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308877711517.log.header
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 02:20 CommitLog-1308879395824.log
-rw-r--r-- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308879395824.log.header
...
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 20:38 CommitLog-1308946745380.log
-rw-r--r-- 1 cassandra cassandra   36 2011-06-24 20:47 CommitLog-1308946745380.log.header
-rw-r--r-- 1 cassandra cassandra 112M 2011-06-24 20:54 CommitLog-1308947888397.log
-rw-r--r-- 1 cassandra cassandra   44 2011-06-24 20:47 CommitLog-1308947888397.log.header
{noformat}

The user KS has 2 CF's with 60 minute flush times. System KS had the default settings which is 24 hours. Will create another ticket see if these can be reduced or if it's something users should do, in this case it would not have mattered. 

I grabbed the log headers and used the tool in CASSANDRA-2828 and most of the segments had the system CF's marked as dirty.

{noformat}
$ bin/logtool dirty /tmp/logs/commitlog/

Not connected to a server, Keyspace and Column Family names are not available.

/tmp/logs/commitlog/CommitLog-1308876643288.log.header
Keyspace Unknown:
	Cf id 0: 444
/tmp/logs/commitlog/CommitLog-1308877711517.log.header
Keyspace Unknown:
	Cf id 1: 68848763
...
/tmp/logs/commitlog/CommitLog-1308944451460.log.header
Keyspace Unknown:
	Cf id 1: 61074
/tmp/logs/commitlog/CommitLog-1308945597471.log.header
Keyspace Unknown:
	Cf id 1000: 43175492
	Cf id 1: 108483
/tmp/logs/commitlog/CommitLog-1308946745380.log.header
Keyspace Unknown:
	Cf id 1000: 239223
	Cf id 1: 172211

/tmp/logs/commitlog/CommitLog-1308947888397.log.header
Keyspace Unknown:
	Cf id 1001: 57595560
	Cf id 1: 816960
	Cf id 1000: 0
{noformat}

CF 0 is the Status / LocationInfo CF and 1 is the HintedHandof CF. I dont have it now, but IIRC CFStats showed the LocationInfo CF with dirty ops. 

I was able to repo a case where flushing the CF's did not mark the log segments as obsolete (attached unit-test patch). Steps are:

1. Write to cf1 and flush.
2. Current log segment is marked as dirty at the CL position when the flush started, CommitLog.discardCompletedSegmentsInternal()
3. Do not write to cf1 again.
4. Roll the log, my test does this manually. 
5. Write to CF2 and flush.
6. Only CF2 is flushed because it is the only dirty CF. cfs.maybeSwitchMemtable() is not called for cf1 and so log segment 1 is still marked as dirty from cf1.

Step 5 is not essential, just matched what I thought was happening. I thought SystemTable.updateToken() was called which does not flush, and this was the last thing that happened.  

The expired memtable thread created by Table uses the same cfs.forceFlush() which is a no-op if the cf or it's secondary indexes are clean. 
    
I think the same problem would exist in 0.8. ",,,,,,,,,,,,CASSANDRA-2828,,,,21/Jul/11 13:01;amorton;0001-2829-unit-test-v08.patch;https://issues.apache.org/jira/secure/attachment/12487299/0001-2829-unit-test-v08.patch,27/Jun/11 02:00;amorton;0001-2829-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12483893/0001-2829-unit-test.patch,21/Jul/11 13:01;amorton;0002-2829-v08.patch;https://issues.apache.org/jira/secure/attachment/12487300/0002-2829-v08.patch,27/Jun/11 02:00;amorton;0002-2829.patch;https://issues.apache.org/jira/secure/attachment/12483892/0002-2829.patch,01/Aug/11 11:20;slebresne;2829.patch;https://issues.apache.org/jira/secure/attachment/12488395/2829.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-07-09 23:04:26.466,,,no_permission,,,,,,,,,,,,20854,,,Mon Aug 01 15:22:36 UTC 2011,,,,,,0|i0gdpr:,93652,jbellis,jbellis,,,,,,,,,27/Jun/11 02:00;amorton;2829-unit-test contains the unit test for the problem. 2829 is the fix. ,"09/Jul/11 23:04;jbellis;Good detective work finding this!

I'm not sure about the proposed fix, though -- I think this reasoning still applies:
{noformat}
                // we can't just mark the segment where the flush happened clean,
                // since there may have been writes to it between when the flush
                // started and when it finished.
{noformat}

... the memtable may have been clean when the flush started, but we don't block writes until flush finishes, so some may have finished in between (so the CL may have writes for this segment now).

(I don't have a better fix yet, this is a tough one.)","13/Jul/11 15:48;jbellis;Thinking out loud here.

CLHeader has a structure to tell us ""do we need to replay, and if so, from where?""

{code}
    private Map<Integer, Integer> cfDirtiedAt; // position at which each CF was last flushed

    boolean isDirty(Integer cfId)
    {
        return cfDirtiedAt.containsKey(cfId);
    } 

    boolean isDirty(Integer cfId)
    {
        return cfDirtiedAt.containsKey(cfId);
    } 
{code}

This is set in two places.  One is during a write:

{code}
                    if (!header.isDirty(id))
                    {
                        header.turnOn(id, logWriter.getFilePointer());
                        writeHeader();
                    }
{code}

The other is post-flush, as described above:

{code}
            if (segment.equals(context.getSegment()))
            {
                // we can't just mark the segment where the flush happened clean,
                // since there may have been writes to it between when the flush
                // started and when it finished. so mark the flush position as
                // the replay point for this CF, instead.
                if (logger.isDebugEnabled())
                    logger.debug(""Marking replay position "" + context.position + "" on commit log "" + segment);
                header.turnOn(id, context.position);
                segment.writeHeader();
                break;
            }
{code}

It feels like we need to add a ""most recent write at"" information as well as the ""oldest write/replay position at"" one.  This would not need to be persisted to disk.

(I thought that this is what 0.6 did, but looking at it that is not the case.  So this bug is present there as well, but I think at this point it just needs to be a known bug there.  Maybe even for 0.7.)","21/Jul/11 11:50;slebresne;bq. It feels like we need to add a ""most recent write at"" information as well as the ""oldest write/replay position at"" one. This would not need to be persisted to disk.

Agreed, I think this is the right fix too.","21/Jul/11 13:01;amorton;I got to take another look at this tonight on the 0.8 trunk and ported the unit test to 0.8. 

The 002-2829-v08 patch was my second attempt. It changes CFS.forceFlush() to always flush and trusts maybeSwitchMemtable() will only flush non clean CF's. 

There are no changes to  CommitLog.discardCompletedSegmentsInternal(). The CF will be turned off in any segment that is not the context segment. It will always be turned on in the current / context segment. I think this gives the correct behaviour, i.e. the cf can never have dirty changes in a segment that is not current AND the cf may have changes in a segment that is current. It is a bit sloppy though as clean CF's will mark segments as dirty which may delay them been cleaned. 


I also think there is a theoretical risk of a race condition with access to the segments Deque.  The iterator runs in the postFlushExecutor and the segments are added on the appropriate commit log executor service.

","21/Jul/11 14:55;jbellis;bq. I also think there is a theoretical risk of a race condition with access to the segments Deque. The iterator runs in the postFlushExecutor

discardCompletedSegments actually does the real work in a task on the CL executor. Unless that's not what you're thinking of, I think we're ok here.

bq. It changes CFS.forceFlush() to always flush and trusts maybeSwitchMemtable() will only flush non clean CF's

Hmm.  Interesting.

Part of me thinks it can't be that simple but I don't see a problem with it. :)

Sylvain?
","21/Jul/11 17:02;slebresne;I think this kind of work, in that we won't keep commit log forever, but it still keep commit logs for much longer than necessary because:
# it relies on forceFlush being called, which unless client triggered will only be after the memtable expires and quite a bunch of commit log could pile up during that time. Quite potentially enough to be a problem (if the commit logs fills up you hard drive, it doesn't matter much that ""it would have been deleted in 5 hours""). I think we can do much better with not too much effort.
# when we do flush the expired memtable, we'll call maybeSwitchMemtable() will potentially clean memtables. This doesn't sound like a good use of resource: we'll grab the write lock, create a latch, create a new memtable, increment the memtable switch number, push an almost no-op job on the flush executor.

I think we should fix the real problem. The problem is that we discard segment, we always keep the current segment dirty because we don't know if there was some write since we grabbed the context. Let's add that information and fix that. This would make commit log being deleted much quicker, even if we don't consider the corner case of column family that have suddenly no write anymore, because CFs like the system ones, that have very low update volume can retain the logs longer than it's really need.

As for the fix, because the CL executor is mono-threaded, this is fairly easy, let's have an in-memory map of cfId->lastPositionWritten, and compare that to the context position in discardCompletedSegmentInternal (we could probably even just use a set of cfid who would meant: dirty since last getContext).","30/Jul/11 13:56;hanzhu;{quote}Let's add that information and fix that. {quote}

Does it mean everytime an RowMutation is appended to the log, the log header should be fsynced again? It brings at least one extra disk seek at a critical path.",31/Jul/11 03:42;jbellis;This can be kept purely in-memory.  No need to sync anything.  (BTW there is no header per se post CASSANDRA-2419.),"31/Jul/11 04:30;hanzhu;{quote}This can be kept purely in-memory{quote}

OK. So these these log segments might no be ignored during log replay. Maybe not a problem at all.","01/Aug/11 11:20;slebresne;Houston, we have a problem.

In 0.8, we have a much bigger problem related to the commit log. Turns out we don't even turnOn the isDirty flag on writes. This means that typically if we fill a segment (with write of different cfs), starts a new one, and flush (one cf, say cf1), the previous segment will be removed even though it may be full of dirty writes for cf != cf1.

Attaching a patch that fix this issue as well as the original issue of this ticket (as it is not really more complicated). It adds two unit test, one for each issue (both fails in current 0.8). Bumping the priority of this too. ",01/Aug/11 13:54;jbellis;+1,"01/Aug/11 15:12;slebresne;Committed, thanks","01/Aug/11 15:22;hudson;Integrated in Cassandra-0.8 #248 (See [https://builds.apache.org/job/Cassandra-0.8/248/])
    fix bug where dirty commit logs were removed (and avoid keeping segment with no post-flush activity permanently dirty)
patch by slebresne; reviewed by jbellis for CASSANDRA-2829

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152793
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/CommitLogTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inherent deadlock situation in commitLog flush?,CASSANDRA-3253,12524448,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,jbellis,yangyangyyy,yangyangyyy,23/Sep/11 17:53,12/Mar/19 14:06,13/Mar/19 22:26,26/Sep/11 09:07,1.0.0,,,,,,0,commitlog,,,,"after my system ran for a while, it consitently goes into frozen state where all the mutations stage threads are waiting
on the switchlock,

the reason is that the switchlock is held by commit log, as shown by the following thread dump:



""COMMIT-LOG-WRITER"" prio=10 tid=0x00000000010df000 nid=0x32d3 waiting on condition [0x00007f2d81557000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f3579eec060> (a java.util.concurrent.FutureTask$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:248)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog.getContext(CommitLog.java:386)
        at org.apache.cassandra.db.ColumnFamilyStore.maybeSwitchMemtable(ColumnFamilyStore.java:650)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:722)
        at org.apache.cassandra.db.commitlog.CommitLog.createNewSegment(CommitLog.java:573)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:81)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:596)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:679)


we can clearly see that the COMMIT-LOG-WRITER thread is running the regular appender , but the appender itself calls getContext(), which again submits a new Callable to be executed, and waits on the Callable. but the new Callable is never going to be executed since the executor has only *one* thread.


I believe this is a deterministic bug.



",,,,,,,,,,,,,,,,25/Sep/11 01:24;jbellis;3253.txt;https://issues.apache.org/jira/secure/attachment/12496376/3253.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-25 01:24:25.957,,,no_permission,,,,,,,,,,,,3343,,,Mon Sep 26 09:07:35 UTC 2011,,,,,,0|i0ghnz:,94292,slebresne,slebresne,,,,,,,,,"23/Sep/11 18:05;yangyangyyy;it seems to be added in the recent feature to flush earliest segment with dirty CFs : 


https://github.com/apache/cassandra/commit/f599559221ad074d9af0a99d7ffdd482c2b6b10c#diff-3

CFS.forceFlush() was added to the commit log writing path",23/Sep/11 18:50;yangyangyyy;looks to be related to https://issues.apache.org/jira/browse/CASSANDRA-1991,"25/Sep/11 01:24;jbellis;excellent diagnosis of the problem, Yang.

patch attached to push the flush calls off of the CL executor.","26/Sep/11 09:07;slebresne;+1
Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong check of partitioner for secondary indexes,CASSANDRA-3540,12533155,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,yukim,slebresne,slebresne,29/Nov/11 17:57,12/Mar/19 14:06,13/Mar/19 22:26,08/Dec/11 12:06,1.0.6,,,Feature/2i Index,,,0,,,,,"CASSANDRA-3407 doesn't handle the fact that secondary indexes have a specific partitioner (LocalPartitioner). This result in the following error when starting nodes in 1.0.4:
{noformat}
java.lang.RuntimeException: Cannot open /var/lib/cassandra/data/Index/AttractionLocationCategoryDateIdx.AttractionLocationCategoryDateIdx_09partition_idx-h-1 because partitioner does not match org.apache.cassandra.dht.LocalPartitioner
{noformat}",,,,,,,,,,,,,,,,08/Dec/11 08:22;yukim;0001-Add-tests-for-opening-index-sstables.patch;https://issues.apache.org/jira/secure/attachment/12506586/0001-Add-tests-for-opening-index-sstables.patch,08/Dec/11 08:22;yukim;0002-Fix-SSTableMetadata-to-write-correct-partitioner.patch;https://issues.apache.org/jira/secure/attachment/12506587/0002-Fix-SSTableMetadata-to-write-correct-partitioner.patch,06/Dec/11 06:48;yukim;cassandra-1.0-3540-v2.txt;https://issues.apache.org/jira/secure/attachment/12506223/cassandra-1.0-3540-v2.txt,01/Dec/11 08:25;yukim;cassandra-1.0-3540.txt;https://issues.apache.org/jira/secure/attachment/12505740/cassandra-1.0-3540.txt,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-12-01 08:25:50.176,,,no_permission,,,,,,,,,,,,218883,,,Thu Dec 08 12:06:20 UTC 2011,,,,,,0|i0gl5z:,94859,slebresne,slebresne,,,,,,,,,"01/Dec/11 08:25;yukim;This bug is caused by SSTableMetadata providing node's partitioner when reading old version (prior hc) of sstable.

Attached patch let SSTableMetadata to use empty string("""") instead of DatabaseDescriptor.getPartitioner as default. When older version of sstable is read, SSTableMetadata provides empty string for partitioner, and check for partitioner is skipped.","01/Dec/11 10:03;slebresne;Some nits:
  * We can give then partitioner as an argument to Collector.finalizeMetadata() instead of adding a new field.
  * I think I slightly prefer using null when there is no partitioner available. I understand that """" make SSTableMetadata slightly simpler but in fact we shouldn't ever write a SSTableMetadata with no partitioner (we can only read one). So we could simply assert that partitioner != null in the serialize method.",01/Dec/11 16:44;jbellis;I'd also like to add a unit test (probably using clearUnsafe) to make sure we're exercising the logic and triggering the original 3407 bug.,"06/Dec/11 06:48;yukim;Updated patch to give partitioner an arg to Collector#finalizeMetadata().
I also added test to SSTableReaderTest which fails when run against current 1.0 branch and  succeeds after this patch.","06/Dec/11 13:17;slebresne;The patch lgtm. Regarding the tests, I think it would be worth testing if we're correctly able to load old sstable, those without a saved partitioner, which I don't think the attached test does, does it (but the current test is good, we should keep it) ? Typically we could have a similar test than in the patch, but that nukes the statistics component of the sstables before reloading them . ","08/Dec/11 08:22;yukim;Sylvain,

I separate the patch into two parts.

0001 just adds tests to reproduce CASSANDRA-3407, one test from previous patch (flush and open) and new test to simulate upgrading from previous version of SSTable. Patch generates Indexed1 SSTable which I created using v1.0.3 (SSTable version is ""hb"") under test/data/legacy-sstables/hb.
When testing, those sstables are copied into unit test data location just like ScrubTest does.

Note that it also generates Standard1 sstable in order to let LegacySSTableTest pass.

0002 contains fix same as previously submitted patch.

Only applying 0001 patch let SSTableReaderTest in current 1.0 branch fail, and you can see it success after applying 0002.","08/Dec/11 12:06;slebresne;I would have been fine with just nuking the metadata component :) but that's great like that.

+1, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect query results due to invalid SSTable.maxTimestamp,CASSANDRA-3510,12532118,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Urgent,Fixed,slebresne,amorton,amorton,21/Nov/11 07:15,12/Mar/19 14:06,13/Mar/19 22:26,22/Nov/11 09:40,1.0.4,,,,,,0,,,,,"related to CASSANDRA-3446

(sorry this is so long, took me a bit to work through it all and there is a lot of new code :) )
 
h1. Summary

SSTable.maxTimestamp for files created before 1.0 defaults to Long.MIN_VALUE, and this means the wrong data is returned from queries. 
 
h2. Details 

Noticed on a cluster that was upgraded from 0.8.X to 1.X, it then had trouble similar to CASSANDRA-3446. It was rolled back to 0.8 and the migrated to 1.0.3. 

4 Node cluster, all files upgraded to ""hb"" format. 

In a super CF there are situations where a get for a sub columns returns a different value than a get for the column. .e.g. 

{noformat}
[default@XXX] get Users[ascii('username')]['meta']['password'];
=> (column=password, value=3130323130343130, timestamp=1307352647576000)

[default@XX] get Users[ascii('username')]['meta'];     
(snip)       
=> (column=password, value=3034323131303034, timestamp=1319563673493000)
{noformat}

The correct value is the second one. 

I added logging after line 109 in o.a.c.db.CollectionController.collectTimeOrderedData() to log the sstable name and the file max timestamp, this is what I got:

{code:java}
for (SSTableReader sstable : view.sstables)
{
    long currentMaxTs = sstable.getMaxTimestamp();
    logger.debug(String.format(""Got sstable %s and max TS %d"", sstable, currentMaxTs));
    reduceNameFilter(reducedFilter, container, currentMaxTs);
{code}

{noformat}
DEBUG 14:08:46,012 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12348-Data.db') and max TS 1321824847534000
DEBUG 14:08:47,231 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12346-Data.db') and max TS 1321813380793000
DEBUG 14:08:49,879 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12330-Data.db') and max TS -9223372036854775808
DEBUG 14:08:49,880 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12325-Data.db') and max TS -9223372036854775808
{noformat}

The key I was reading is present in files 12330 and 12325, the first contains the *old / wrong* value with timestamp 1307352647576000 above. The second contains the *new / correct* value with timestamp 1319563673493000.

**Updated:** Incorrect, it was a later file that had the correct value, see the first comment. 

When CollectionController.collectTimeOrderedData() processes the 12325 file (after processing the 12330 file) while looping over the sstables the call to reduceNameFilter() removes the column  from the filter because the column read from the 12330 file has a time stamp of 1307352647576000 and the 12325 file incorrectly has a max time stamp of -9223372036854775808 .

SSTableMetadata is reading the max time stamp from the stats file, but it is Long.MIN_VALUE. I think this happens because scrub creates the SSTableWriter using cfs.createCompactionWriter() which sets the maxTimestamp in the meta data collector according to the maxTimestamp in the meta data for the file(s) that will be scrubbed / compacted. But for pre 1.0 format files the default in SSTableMetadata is Long.MIN_VALUE, (see SSTableMetaData.deserialize() and the ctor). So scrubbing a pre 1.0 file will write stats files that have maxTimestamp as Long.MIN_VALUE.

During scrubbing the SSTableWriter does not update the maxTimestamp because append(AbstractCompactedRow) is called which expects the that cfs.createCompactionWriter() was able to set the correct maxTimestamp on the meta data. Compaction also uses append(AbstractCompactedRow) so may create an SSTable with an incorrect maxTimestamp if one of the input files started life as a pre 1.0 file and has a bad maxTimestamp. 

It looks like the only time the maxTimestamp is calculated is when the SSTable is originally written. So the error from the old files will be carried along. 

e.g. If the files a,b and c have the maxTimestamps 10, 100 and Long.MIN_VALUE compaction will write a SSTable with maxTimestamp 100. However file c may actually contain columns with a timestamp > 100 which will be in the compacted file.

h1. Reproduce

1. Start a clean 0.8.7

2. Add a schema (details of the schema do not matter):
{noformat}
[default@unknown] create keyspace dev;   
5f834620-140b-11e1-0000-242d50cf1fdf
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] 
[default@unknown] use dev;
Authenticated to keyspace: dev
[default@dev] 
[default@dev] create column family super_dev with column_type = 'Super' 
...	and key_validation_class = 'AsciiType' and comparator = 'AsciiType' and 
...	subcomparator = 'AsciiType' and default_validation_class = 'AsciiType';
60490720-140b-11e1-0000-242d50cf1fdf
Waiting for schema agreement...
... schemas agree across the cluster
{noformat}

3. Shutdown 0.8.7

4. Start 1.0.3 using the same data. Check the schema version loaded, example below shows the wrong schema is loaded. I stepped the code and the wrong value was read from Migration.getLastMigrationId() due to this bug. 

{noformat}
 INFO [main] 2011-11-21 19:39:08,546 DatabaseDescriptor.java (line 501) Loading schema version 5f834620-140b-11e1-0000-242d50cf1fdf
{noformat}

5. Check the schema using the 1.0.3 CLI 

{noformat}
[default@unknown] use dev;
Authenticated to keyspace: dev
[default@dev] describe;
Keyspace: dev:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [datacenter1:1]
  Column Families:
[default@dev] 
{noformat}

6. I then did a 1.0.3 scrub and re-started. The correct schema version was read, but stepping the code both Schema SSTables had Long.MIN_VALUE as the maxTimestamp so I think it was only the random order of the files that made it work. 

{noformat}
DEBUG 19:52:30,744 Got sstable SSTableReader(path='/var/lib/cassandra/data/system/Schema-hb-4-Data.db') and max TS -9223372036854775808
DEBUG 19:52:30,744 Got sstable SSTableReader(path='/var/lib/cassandra/data/system/Schema-hb-3-Data.db') and max TS -9223372036854775808
{noformat}

h1. Fixes

Not sure, (wanted to get the ticket opened and find out if I was imagining things), guessing...

Use Long.MIN_VALUE as a magic maxTimestamp that means the value is not know. This would not fix issues where the incorrect maxTimestamp been included in compaction. 
 
Looking at making scrub re-calculate the maxTimestamp.

Also wondering if the maxTimestamp should default to Long.MAX_VALUE if read from a file format that does not support maxTimestamp ?
",,,,,,,,,,,,,,,,21/Nov/11 10:44;amorton;0001-3510-ignore-maxTimestamp-if-Long.MIN_VALUE.patch;https://issues.apache.org/jira/secure/attachment/12504489/0001-3510-ignore-maxTimestamp-if-Long.MIN_VALUE.patch,21/Nov/11 10:44;amorton;0002-3510-update-maxTimestamp-during-repair.patch;https://issues.apache.org/jira/secure/attachment/12504490/0002-3510-update-maxTimestamp-during-repair.patch,21/Nov/11 11:43;slebresne;3510.patch;https://issues.apache.org/jira/secure/attachment/12504493/3510.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-11-21 11:43:44.565,,,no_permission,,,,,,,,,,,,217854,,,Tue Nov 22 09:40:36 UTC 2011,,,,,,0|i0gksn:,94799,amorton,amorton,,,,,,,,,"21/Nov/11 09:16;amorton;To be clear, I do not think this is an issue with super CF's. The Schema CF is a standard, it was noticed on a super CF. 

I added some more logging and removed the call to cut the query short by checking maxTimestamp in CollationController.collectTimeOrderedData(). The query returned the result value (password col with timestamp 1319563673493000) and this was the output 

{code:java}
for (SSTableReader sstable : view.sstables)
{
    long currentMaxTs = sstable.getMaxTimestamp();
    logger.debug(String.format(""Got sstable %s and max TS %d"", sstable, currentMaxTs));
//                reduceNameFilter(reducedFilter, container, currentMaxTs);
//                if (((NamesQueryFilter) reducedFilter.filter).columns.isEmpty())
//                    break;

    IColumnIterator iter = reducedFilter.getSSTableColumnIterator(sstable);
    iterators.add(iter);
    if (iter.getColumnFamily() != null)
    {
        container.delete(iter.getColumnFamily());
        sstablesIterated++;
        while (iter.hasNext())
        {
            IColumn col = iter.next();
            if (col instanceof SuperColumn)
            {
              for (IColumn subcol : ((SuperColumn)col).columns)
              {
                if (subcol.name().equals(ByteBufferUtil.bytes(""password"")))
                  logger.debug(String.format(""Add Sub Column %s"", subcol.getString(cfs.metadata.subcolumnComparator)));
              }
            }
            else
            {
              logger.debug(String.format(""Add Column %s"", col.getString(cfs.metadata.comparator)));
            }
            container.addColumn(col);
        }
{code}


{noformat}
DEBUG 22:05:20,748 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12348-Data.db') and max TS 1321824847534000
DEBUG 22:05:20,749 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12346-Data.db') and max TS 1321813380793000
DEBUG 22:05:20,753 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12330-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,754 Add Sub Column password:false:8@1307352647576000
DEBUG 22:05:20,755 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12325-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,757 Add Sub Column password:false:8@1307352647576000
DEBUG 22:05:20,758 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12327-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,760 Add Sub Column password:false:8@1307352647576000
DEBUG 22:05:20,761 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12328-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,762 Add Sub Column password:false:8@1319563673493000
DEBUG 22:05:20,763 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12326-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,765 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12331-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,767 Add Sub Column password:false:8@1307352647576000
DEBUG 22:05:20,768 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12332-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,774 Read: 27 ms.
{noformat}

","21/Nov/11 10:44;amorton;patch 0001 is a proof of concept hack based on my first comment, it generated this output when using the extra logging 

{noformat}
DEBUG [ReadStage:1] 2011-11-21 23:12:28,578 CollationController.java (line 77) collectTimeOrderedData
DEBUG [ReadStage:1] 2011-11-21 23:12:28,578 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12348-Data.db') and max TS 1321824847534000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,578 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12346-Data.db') and max TS 1321813380793000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,583 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12330-Data.db') and max TS -9223372036854775808
DEBUG [ReadStage:1] 2011-11-21 23:12:28,584 CollationController.java (line 130) Add Sub Column password:false:8@1307352647576000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,585 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12325-Data.db') and max TS -9223372036854775808
DEBUG [ReadStage:1] 2011-11-21 23:12:28,587 CollationController.java (line 130) Add Sub Column password:false:8@1307352647576000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,588 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12327-Data.db') and max TS -9223372036854775808
DEBUG [ReadStage:1] 2011-11-21 23:12:28,590 CollationController.java (line 130) Add Sub Column password:false:8@1307352647576000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,591 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12328-Data.db') and max TS -9223372036854775808
DEBUG [ReadStage:1] 2011-11-21 23:12:28,592 CollationController.java (line 130) Add Sub Column password:false:8@1319563673493000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,593 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12326-Data.db') and max TS -9223372036854775808
DEBUG [ReadStage:1] 2011-11-21 23:12:28,595 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12331-Data.db') and max TS -9223372036854775808
DEBUG [ReadStage:1] 2011-11-21 23:12:28,596 CollationController.java (line 130) Add Sub Column password:false:8@1307352647576000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,597 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12332-Data.db') and max TS -9223372036854775808
DEBUG [pool-2-thread-1] 2011-11-21 23:12:28,604 StorageProxy.java (line 694) Read: 26 ms.
{noformat}


patch 0002 makes scrub update the maxTimestamp and when I ran my test afterwards it created this output:

**NOTE** patch 0002 has incorrect file name, it modifies scrub.

{noformat}

DEBUG [ReadStage:33] 2011-11-21 23:20:32,032 CollationController.java (line 77) collectTimeOrderedData
DEBUG [ReadStage:33] 2011-11-21 23:20:32,033 CollationController.java (line 111) Got sstable SSTableReader(path='/private/var/lib/cassandra/data/fmm/Users-hc-12357-Data.db') and max TS 1321824847534000
DEBUG [ReadStage:33] 2011-11-21 23:20:32,033 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hc-12352-Data.db') and max TS 1321813380793000
DEBUG [ReadStage:33] 2011-11-21 23:20:32,063 CollationController.java (line 111) Got sstable SSTableReader(path='/private/var/lib/cassandra/data/fmm/Users-hc-12356-Data.db') and max TS 1321560509938000
DEBUG [ReadStage:33] 2011-11-21 23:20:32,064 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hc-12349-Data.db') and max TS 1319813295567000
DEBUG [ReadStage:33] 2011-11-21 23:20:32,105 CollationController.java (line 130) Add Sub Column password:false:8@1319563673493000
DEBUG [ReadStage:33] 2011-11-21 23:20:32,105 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hc-12350-Data.db') and max TS 1318523190222000
DEBUG [pool-2-thread-2] 2011-11-21 23:20:32,106 StorageProxy.java (line 694) Read: 74 ms.
{noformat}","21/Nov/11 11:43;slebresne;The fix of the first patch looks. For the second patch, even though this is an option, I think I'd rather make this parts of compaction, because I don't like too much having potential subtle bug that needs scrub to be run (without really anything telling you that you potentially have a problem btw).

Attaching a patch that does this (make it work for compaction in general as long as we're not using an EchoedRow (which makes it work for scrub in particular)). The patch also adds a few comments and a unit test. It includes the fix of the first patch. ","21/Nov/11 15:08;jbellis;bq. 

{noformat}
+         * However, for old sstables without timestamp, we still want to update the timestamp (and we know
+         * that in this case we will not use EchoedRow).
{noformat}

Where is the ""don't use echoedrow for old sstables"" logic?","21/Nov/11 15:30;slebresne;bq. Where is the ""don't use echoedrow for old sstables"" logic?

In CompactionController.needDeserialize(), but I agree that the comment could be improved to recall it.",21/Nov/11 15:41;jbellis;Referring to the isLatestVersion check?,"21/Nov/11 16:57;slebresne;bq. Referring to the isLatestVersion check?

Yes.","21/Nov/11 19:14;amorton;Thanks, will test shortly. 
","21/Nov/11 19:57;amorton;Tested against the current 1.0 head, with the test case I had and the query works as expected. 

I agree doing it in compaction is safer, putting it in repair just got me there faster last night. 

Thanks for cleaning up the patch. 
+1",21/Nov/11 20:05;jbellis;+1,22/Nov/11 09:40;slebresne;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enormous counter,CASSANDRA-3006,12518299,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,yulinyen,yulinyen,09/Aug/11 09:41,12/Mar/19 14:17,13/Mar/19 22:26,10/Aug/11 15:31,0.8.4,,,,,,0,,,,,"I have two-node cluster with the following keyspace and column family settings.

Cluster Information:
   Snitch: org.apache.cassandra.locator.SimpleSnitch
   Partitioner: org.apache.cassandra.dht.RandomPartitioner
   Schema versions: 
	63fda700-c243-11e0-0000-2d03dcafebdf: [172.17.19.151, 172.17.19.152]

Keyspace: test:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [datacenter1:2]
  Column Families:
    ColumnFamily: testCounter (Super)
    ""APP status information.""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.CounterColumnType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType/org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 1.1578125/1440/247 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []

Then, I use a test program based on hector to add a counter column (testCounter[sc][column]) 1000 times. In the middle the adding process, I intentional shut down the node 172.17.19.152. In addition to that, the test program is smart enough to switch the consistency level from Quorum to One, so that the following adding actions would not fail. 

After all the adding actions are done, I start the cassandra on 172.17.19.152, and I use cassandra-cli to check if the counter is correct on both nodes, and I got a result 1001 which should be reasonable because hector will retry once. However, when I shut down 172.17.19.151 and after 172.17.19.152 is aware of 172.17.19.151 is down, I try to start the cassandra on 172.17.19.151 again. Then, I check the counter again, this time I got a result 481387 which is so wrong.

I use 0.8.3 to reproduce this bug, but I think this also happens on 0.8.2 or before also. ",ubuntu 10.04,,,,,,,,,,,,,,,10/Aug/11 13:24;slebresne;3006.patch;https://issues.apache.org/jira/secure/attachment/12489971/3006.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-09 14:28:19.145,,,no_permission,,,,,,,,,,,,20933,,,Fri Aug 12 06:40:29 UTC 2011,,,,,,0|i0gesv:,93828,jbellis,jbellis,,,,,,,,,"09/Aug/11 09:43;yulinyen;I forgot the mention that the counter is out of sync between these two nodes, one shows 481387 and the other one shows 20706.","09/Aug/11 14:28;slebresne;I've haven't had luck with reproducing so far. I've tried to stick with the description above but did not used hector (not saying it is hector fault though, maybe it is the way it does retry that I don't emulate well). If you are able to share a minimal hector script with which you reproduce this easily, that would be very helpful.","10/Aug/11 01:57;yulinyen;Here is the test program I am using now. the hector version is 0.8.0-2.
Hope this will be helpful.
------------------------------------------------

import java.util.Arrays;

import me.prettyprint.cassandra.model.AllOneConsistencyLevelPolicy;
import me.prettyprint.cassandra.serializers.StringSerializer;
import me.prettyprint.cassandra.service.CassandraHostConfigurator;
import me.prettyprint.cassandra.service.ThriftCluster;
import me.prettyprint.hector.api.Keyspace;
import me.prettyprint.hector.api.beans.HCounterColumn;
import me.prettyprint.hector.api.factory.HFactory;
import me.prettyprint.hector.api.mutation.Mutator;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


public class CounterTest {
	private Logger logger = LoggerFactory.getLogger(CounterTest.class) ;
	private static final Integer COUNTER_NUM = 1000 ;
	private static final StringSerializer ss = StringSerializer.get();
	private static final String HOST = ""172.17.19.151:9160"" ;
	private ThriftCluster cluster ;
	
	/**
	 * @param args
	 */
	public static void main(String[] args) {
		CounterTest tc = new CounterTest() ;

		try {
			tc.testAlarmCounter() ;
		} catch (InterruptedException e) {
			
		}
	}
	
	public CounterTest(){
		CassandraHostConfigurator chc = new CassandraHostConfigurator(HOST) ;
		chc.setMaxActive(100) ;
		chc.setMaxIdle(10) ;
		chc.setCassandraThriftSocketTimeout(60000) ;
		
		cluster = new ThriftCluster(""Test Cluster"", chc) ;
	}

	public void testAlarmCounter() throws InterruptedException{
		int successCounter = 0 ;
		int cl = 0;
		
		for(int i=0; i<COUNTER_NUM; i++){
			try{
				logger.info(""count: ""+i) ;
				
				Mutator<String> mutator = HFactory.createMutator(getKeyspace(cl), StringSerializer.get());
				
				HCounterColumn<String> column = HFactory.createCounterColumn(""testSC"", 1L) ;
				mutator.addCounter(""sc"", ""testCounter"", HFactory.createCounterSuperColumn(""testC"", Arrays.asList(column), ss, ss));
				mutator.execute() ;
				
				successCounter++ ;
			} catch(Exception e){
				logger.info(""Error! Change consistency level to 1."", e) ;
				cl=1 ;
			}
			
			Thread.sleep(50) ;
		}
		
		logger.info(""\nsuccess counter: ""+successCounter) ;
	}
	
	private Keyspace getKeyspace(int cl){
		if(cl == 1)
			return HFactory.createKeyspace(""test"", cluster, new AllOneConsistencyLevelPolicy()) ;
		else
			return HFactory.createKeyspace(""test"", cluster) ; // default consistency level is Quorum
	}
}","10/Aug/11 03:32;yulinyen;In order to make it easier to reproduce this issue, I document how I recreate this issue step by step.

1. clean any thing that is inside /var/lib/cassandra on node 172.17.19.151

2. start cassandra on node 172.17.19.151.

3. clean any thing that is inside /var/lib/cassnadra on node 172.17.19.152

4. modify the cassandra.yaml of 172.17.19.152 and add 172.17.19.151 as a seed.

5. start cassandra on node 172.17.19.152, I could see two node has formed a cluster, I also double check that using nodetool.

6. on node 172.17.19.151, I use cassandra-cli: to connect 172.17.19.151/9160, and execute commands -> 

create keyspace test
with placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy'
and strategy_options = [{datacenter1:2}];

create column family testCounter
    with column_type = Super
    and default_validation_class = CounterColumnType
    and replicate_on_write = true
    and comparator = BytesType
    and subcomparator = BytesType
    and comment = 'APP status information.';

7. use the test program to add the counter 1000 times. between each adding action the program will pause 50 millisecond.

8. in the middle of the adding process, shut down the cassandra on node 172.17.19.152, (let's say I shut down node 172.17.19.152 when count is 200.). Because the test program changes the consistency level to One when it encounters an exception (timeout exception to be exact), the following adding actions will still be success.

9. wait for the overall adding process to complete. I saw ""success counter: 999"" due to one exception. 

10. use the cassandra-cli to connect to 172.17.19.151 and 172.17.19.152 and check the counter value, the value is 1001 on both nodes. It shows 1001 because hector will retry when it encounters the timeout exception. 

11. shutdown the cassandra on 172.17.19.151, wait for a few seconds, I saw ""InetAddress /172.17.19.151 is now dead"" on node 172.17.19.152.

12. after seeing ""InetAddress /172.17.19.151 is now dead"", restart the cassandra on node 172.17.19.151.

13. check the counter again with cassandra-cli on both nodes, this time the counter should no longer be 1001, it should be other weird number.

Hope someone else could recreate it by these steps.","10/Aug/11 13:24;slebresne;Thanks, that helps a lot.

The problem is due to the RowMutation optimization that keeps the serialized data received on the wire to use in the commit log. This is wrong for counters because we use the deserialization of the RowMutation to clean the delta on the counter columns.

At least in the script to reproduce, this was a hint problem. The first node was creating a hint for the second one and was storing it himself (and the value of this hints was not cleared because of the bug above).

Patch attached to fix this. This basically disable the optimization for RowMutation that contains counter. I don't pretend this is particularly clean but I don't see any other simple solution.","10/Aug/11 15:02;jbellis;+1

(nit: we can break from the loop once we find a counter)",10/Aug/11 15:31;slebresne;Committed with breaking early change.,"10/Aug/11 16:21;hudson;Integrated in Cassandra-0.8 #267 (See [https://builds.apache.org/job/Cassandra-0.8/267/])
    Force deserialization of RowMutation for counters
patch by slebresne; reviewed by jbellis for CASSANDRA-3006

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156221
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/RowMutation.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
","12/Aug/11 06:40;yulinyen;Just tried 0.8.4. It seems much better now, but I still got value that is not what I expected.

Follow the same steps above, after step 11, I check the counter on .152, the counter values changes from 1001 to 200. And then I follow the steps 12 and 13. The counter value of .151 is still 1001, but the counter value of .152 is still 200. After  changing the consistencylevel to quorum and do the read again, this time the counter value of .152 became 1001.

I am kind of confused, because at step 10, I can get 1001 on both nodes, how come the value of .152 changes to 200 when .151 is down. Is this the right behavior of current design? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failures in cassandra long test: LongCompactionSpeedTest,CASSANDRA-3022,12518732,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,cdaw,cdaw,12/Aug/11 19:44,12/Mar/19 14:16,13/Mar/19 22:26,13/Aug/11 06:06,0.8.5,,,,,,0,,,,,"*The failing test case*
{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
{code}


*The following error is repeated in the console output*
{code}
    [junit] ERROR 04:02:20,654 Error in ThreadPoolExecutor
    [junit] java.util.MissingFormatArgumentException: Format specifier 's'
    [junit] 	at java.util.Formatter.format(Formatter.java:2432)
    [junit] 	at java.util.Formatter.format(Formatter.java:2367)
    [junit] 	at java.lang.String.format(String.java:2769)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:136)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:123)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:43)
    [junit] 	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
    [junit] 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    [junit] 	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    [junit] 	at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    [junit] 	at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager.doCompactionWithoutSizeEstimation(CompactionManager.java:559)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager.doCompaction(CompactionManager.java:506)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:141)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:107)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
{code}

*Cassandra Revision List at time of failure
{code}
Summary
* log ks and cf of large rows being compacted patch by Ryan King; reviewed by jbellis for CASSANDRA-3019
* revert r1156772
* cache invalidate removes saved cache files patch by Ed Capriolo; reviewed by jbellis for CASSANDRA-2325
* make sure truncate clears out the commitlog patch by jbellis; reviewed by slebresne for CASSANDRA-2950
* include column name in validation failure exceptions patch by jbellis; reviewed by David Allsopp for CASSANDRA-2849
* fix NPE when encryption_options is unspecified patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3007
* update CHANGES
* update CHANGES

Revision 1156830 by jbellis: 
log ks and cf of large rows being compacted
patch by Ryan King; reviewed by jbellis for CASSANDRA-3019
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java

Revision 1156791 by jbellis: 
revert r1156772
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java

Revision 1156772 by jbellis: 
cache invalidate removes saved cache files
patch by Ed Capriolo; reviewed by jbellis for CASSANDRA-2325
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java

Revision 1156763 by jbellis: 
make sure truncate clears out the commitlog
patch by jbellis; reviewed by slebresne for CASSANDRA-2950
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
	/cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java

Revision 1156753 by jbellis: 
include column name in validation failure exceptions
patch by jbellis; reviewed by David Allsopp for CASSANDRA-2849
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java

Revision 1156749 by jbellis: 
fix NPE when encryption_options is unspecified
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3007
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java

Revision 1156695 by jbellis: 
update CHANGES
	/cassandra/branches/cassandra-0.8/CHANGES.txt

Revision 1156694 by jbellis: 
update CHANGES
	/cassandra/branches/cassandra-0.8/CHANGES.txt
{code}

",,,,,,,,,,,,,,,,12/Aug/11 19:53;jbellis;3022.txt;https://issues.apache.org/jira/secure/attachment/12490282/3022.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-12 19:53:43.353,,,no_permission,,,,,,,,,,,,20940,,,Sat Aug 13 22:20:46 UTC 2011,,,,,,0|i0gew7:,93843,cdaw,cdaw,,,,,,,,,"12/Aug/11 19:53;jbellis;patch to fix, and provide human-readable keys where possible","12/Aug/11 23:27;cdaw;The patch fixed the problem.
{code}

long-test:
     [echo] running long tests
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/cathy/dev/cassandra-0.8/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.db.LongTableTest
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 15.548 sec
    [junit] 
    [junit] Testsuite: org.apache.cassandra.db.MeteredFlusherTest
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 25.154 sec
    [junit] 
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 6, Failures: 0, Errors: 0, Time elapsed: 82.062 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 988 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: 2751 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 1281 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=500000: 10034 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=500000 colsper=1: 13489 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=1000 colsper=5: 9982 ms
    [junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.cassandra.utils.LongBloomFilterTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 65.727 sec
    [junit] 
    [junit] Testsuite: org.apache.cassandra.utils.LongLegacyBloomFilterTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 42.892 sec
    [junit] 

BUILD SUCCESSFUL
{code}",13/Aug/11 06:06;jbellis;committed,"13/Aug/11 22:20;hudson;Integrated in Cassandra-0.8 #277 (See [https://builds.apache.org/job/Cassandra-0.8/277/])
    fix string formatting bug, and provide human-readable keys where possible
patch by jbellis; tested by Cathy Daw for CASSANDRA-3022

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1157333
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in describe_ring,CASSANDRA-3023,12518752,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,efalcao,efalcao,12/Aug/11 22:47,12/Mar/19 14:16,13/Mar/19 22:26,24/Aug/11 16:11,0.8.5,,,,,,0,,,,,"Not sure how much of the following is relevant besides the stack trace, but here I go:

I have a 2 DC, 2 node per DC cluster. DC1 had it's seed replaced but I hadn't restarted. I upgraded to 0.8.4 in the following fashion:

-edited seeds
-stopped both DC1 nodes
-upgraded jars
-started both nodes at the same time

The non-seed node came up first and showed the following error. Then when the seed node came up, the error went away on the non-seed node but started occurring on the seed node:

ERROR [pool-2-thread-15] 2011-08-12 22:32:27,438 Cassandra.java (line 3668) Internal error processing describe_ring
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.getRangeToRpcaddressMap(StorageService.java:623)
	at org.apache.cassandra.thrift.CassandraServer.describe_ring(CassandraServer.java:731)
	at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.process(Cassandra.java:3664)
	at org.apache.cassandra.thrift.Brisk$Processor.process(Brisk.java:464)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,24/Aug/11 16:04;brandon.williams;3023.txt;https://issues.apache.org/jira/secure/attachment/12491501/3023.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-15 20:32:06.221,,,no_permission,,,,,,,,,,,,617,,,Wed Aug 24 21:10:12 UTC 2011,,,,,,0|i0gewf:,93844,jbellis,jbellis,,,,,,,,,"15/Aug/11 20:32;brandon.williams;This is going to happen anytime a) the tokens are known from being persisted in the system table and (so the coordinator knows about the nodes) but b) it has not actually talked to them, so it has no state information yet.  There's no good way to fix this, because either we leave out tokens that we know about but do not have any other information for, or we just leave the NPE.  On the bright side, the window to trigger this should be very short, probably just a handful of seconds at most.","15/Aug/11 21:19;efalcao;FWIW, and I should have mentioned originally, the exception is thrown repeatedly every few seconds and doesn't seem to stop on the seed node",15/Aug/11 21:46;brandon.williams;Was the cluster still mixed between 0.8.3 and 0.8.4 at that point?  That would do it.  The easiest thing to do is not describe the ring until all machines are on the same version.,"15/Aug/11 21:56;efalcao;i was upgrading one DC first and that's when i noticed it (and never upgraded the 2nd DC). Not sure if that matters. If so, I'll try upgrading both DC's and see if the error persists after fully upgraded.","15/Aug/11 22:03;brandon.williams;This will happen in a mixed cluster because nodes prior to CASSANDRA-1777 will not have advertised the information that describe_ring now relies on, causing the NPE when the nodes that do have 1777 try to access it.

There's not a lot we can do here.  We can basically return half-broken data for nodes that haven't advertised this info yet, but that puts us back to before CASSANDRA-1777, so it's not much of a win.  I can't think of a reason why clients would need to call describe_ring while there's an upgrade in process; topology isn't changing.","24/Aug/11 02:08;jbellis;I think ""return half-broken data"" is the right solution, since pre-1777 clients aren't going to be looking for the new data anyway.","24/Aug/11 11:08;wmeler;Describe_ring is needed even if topology isn't changing. Smart clients need to describe ring all the time - especially at startup to know which nodes they should contact. I have such implementation in C based client app.
Also Hadoop is describing ring prior to job execution in org.apache.cassandra.hadoop.ColumnFamilyInputFormat.

Will problem disappear after full upgrade to 0.8.4? Now I have work-around - call describe_ring only on my old 0.8.1 nodes, but after full upgrade if it won't work I will have a big problem...","24/Aug/11 15:46;brandon.williams;bq. Will problem disappear after full upgrade to 0.8.4?

Yes.","24/Aug/11 16:04;brandon.williams;bq. I think ""return half-broken data"" is the right solution, since pre-1777 clients aren't going to be looking for the new data anyway.

You're right, but it sure bothers my OCD :)

Trivial patch attached.",24/Aug/11 16:07;jbellis;+1,24/Aug/11 16:11;brandon.williams;Committed.,"24/Aug/11 20:53;efalcao;Just another quick note: by the comments here, I thought I'd be able to do a rolling restart to 0.8.4

By the time I got to my last node and stopped it, my clients were under the impression that the whole cluster was down (it wasn't) but checking cassandra logs I saw the stacktrace above spewing out 5-10 times per second. When the last node was started, the stacktraces were still flowing rapidly. I downgraded my entire cluster to 0.8.3 to stop the error and bring my site back online

Any insight?",24/Aug/11 21:10;efalcao;I think my clients may have rapidly been calling describe_ring.........hrm......which should have stopped NPE'ing after the full (but rolling) upgrade.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError on nodetool cleanup,CASSANDRA-3039,12518911,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,rays,rays,15/Aug/11 19:18,12/Mar/19 14:16,13/Mar/19 22:26,16/Aug/11 19:23,0.8.5,,,,,,0,exception,nodetool,,,"While doing a cleanup I got the following AssertionError, I have tried a scrub and a major compaction before the cleanup which has not helped.

ST:

 INFO 18:49:58,540 Scrubbing SSTableReader(path='/vol/cassandra/data/system/LocationInfo-g-93-Data.db')
 INFO 18:49:58,834 Scrub of SSTableReader(path='/vol/cassandra/data/system/LocationInfo-g-93-Data.db') complete: 4 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:58,913 Scrubbing SSTableReader(path='/vol/cassandra/data/system/Migrations-g-56-Data.db')
 INFO 18:49:59,218 Scrub of SSTableReader(path='/vol/cassandra/data/system/Migrations-g-56-Data.db') complete: 1 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:59,256 Scrubbing SSTableReader(path='/vol/cassandra/data/system/Schema-g-58-Data.db')
 INFO 18:49:59,323 Scrub of SSTableReader(path='/vol/cassandra/data/system/Schema-g-58-Data.db') complete: 34 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:59,416 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5074-Data.db')
 INFO 18:50:50,137 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5074-Data.db') complete: 91735 rows in new sstable and 32 empty (tombstoned) rows dropped
 INFO 18:50:50,137 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5075-Data.db')
 INFO 18:50:53,075 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5075-Data.db') complete: 27940 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:50:53,089 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content-g-238-Data.db')

 INFO 18:51:10,302 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content-g-238-Data.db') complete: 70815 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:53:05,420 Cleaning up SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5078-Data.db')
 INFO 18:53:13,266 Cleaned up to /vol/cassandra/data/SpiderServices/Content2-tmp-g-5079-Data.db.  198,705,176 to 198,705,176 (~100% of original) bytes for 27,940 keys.  Time: 7,846ms.
 INFO 18:53:13,267 Cleaning up SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5077-Data.db')
ERROR 18:53:33,913 Fatal exception in thread Thread[CompactionExecutor:21,1,RMI Runtime]
java.lang.AssertionError
	at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:107)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:132)
	at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:866)
	at org.apache.cassandra.db.compaction.CompactionManager.access$500(CompactionManager.java:65)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:204)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)","Distributor ID:	Ubuntu
Description:	Ubuntu 10.10
Release:	10.10
Codename:	maverick

AWS: m2.xlarge instance

6 Node Cluster",,,,,,,,,,,,,,,15/Aug/11 19:42;jbellis;3039.txt;https://issues.apache.org/jira/secure/attachment/12490464/3039.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-15 19:42:28.504,,,no_permission,,,,,,,,,,,,20946,,,Tue Aug 16 21:22:28 UTC 2011,,,,,,0|i0gezz:,93860,xedin,xedin,,,,,,,,,"15/Aug/11 19:42;jbellis;Patch to make cleanup and normal compaction able to skip empty rows (rows containing nothing but expired tombstones).

Scrub can already handle these, so you can workaround by scrubbing frequently, but if your workload results in a lot of these rows you probably want to apply this patch sooner than later.",16/Aug/11 19:23;xedin;committed.,"16/Aug/11 21:22;hudson;Integrated in Cassandra-0.8 #281 (See [https://builds.apache.org/job/Cassandra-0.8/281/])
    Make cleanup and normal compaction able to skip empty rows (rows containing nothing but expired tombstones).
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3039

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1158425
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
On Disk Compression breaks SSL Encryption,CASSANDRA-3051,12519195,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,bcoverston,bcoverston,17/Aug/11 22:06,12/Mar/19 14:16,13/Mar/19 22:26,30/Aug/11 13:50,1.0.0,,,,,,0,,,,,"Encryption depends on FileStreamTask.write [1] protected member to be called because the SSLFileStreamTask.write overrides this to write back to the server.

When enabled, compression circumvents the call and the client does not communicate using an SSL socket back to the server.

[1]
protected long write(FileChannel fc, Pair<Long, Long> section, long length, long bytesTransferred) throws IOException

",Trunk,,,,,,,,,,,,,,,30/Aug/11 09:17;xedin;CASSANDRA-3051-v2.patch;https://issues.apache.org/jira/secure/attachment/12492216/CASSANDRA-3051-v2.patch,29/Aug/11 22:32;xedin;CASSANDRA-3051.patch;https://issues.apache.org/jira/secure/attachment/12492154/CASSANDRA-3051.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-29 15:05:38.324,,,no_permission,,,,,,,,,,,,20949,,,Tue Aug 30 14:24:23 UTC 2011,,,,,,0|i0gf3z:,93878,jbellis,jbellis,,,,,,,,,"29/Aug/11 15:05;xedin;Removed SSLFileStreamTask and added EncryptionOptions to the constructor of the FileStreamTask.

Rebased with latest trunk (last commit 0a4b1667bee674f7c0a22057cbdab97e368a20d1)","29/Aug/11 15:27;jbellis;lgtm, +1 if it actually works :)",29/Aug/11 15:34;xedin;I don't see anh reason why it won't :) but can't write a test because SSL is treacky with it's stores...,"29/Aug/11 15:43;gdusbabek;wait, you tested it locally first, right?  It's not difficult to set up a streaming environment locally.","29/Aug/11 15:48;jbellis;do we have a ""ssl howto"" somewhere?  I was hoping it would be in cassandra.yaml by encryption_options but no.  Or at least, not sufficiently ""for dummies"" for me.","29/Aug/11 15:55;xedin;No, unfortunately I haven't found any info about how to do that so you are welcome to test if you can...","29/Aug/11 15:58;gdusbabek;Not that I know of.  If someone wants to write one it would flesh out these basic steps:
# follow the steps for generating a keystore and a trust store here: http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore
# plug those files into encryption_options in cassandra.yaml
# make sure encryption_options.internode_encryption = all in the yaml.

I was mostly raising a voice of caution against committing code backed up by statements like ""I don't see anh reason why it won't...""  This is usually a prelude to something like ""we need to quickly release a new version because XYZ broke streaming.""  Just sayin'.","29/Aug/11 16:20;xedin;Following your logic - person who was working on ssl should've done that at first place, there is no guarantee that it works in the current state. I'm not pushing things forward just wondering why testing wasn't done before.","29/Aug/11 16:25;gdusbabek;Pavel, I tested SSL prior to committing the feature.

I was under the impression that this ticket exists because compression, when enabled, breaks SSL.  The implication is that it was working prior, else the ticket would be something more like ""SSL is broken.""","29/Aug/11 16:48;xedin;You misunderstood that, it is not breaking SSL it was just special cased in FileStreamTask so it wasn't using ssl socket. This patch removes special casing for SSL streaming by creating ssl socket directly in FileStreamTask if encryption options were set.","29/Aug/11 17:51;jbellis;Pavel, can you try to set up local SSL w/ a ccm cluster based on Gary's instructions to verify?",29/Aug/11 18:09;xedin;Sure,"29/Aug/11 21:38;xedin;I figured out the problem - SSLSocket always returns null on getChannel even on current code, I will refactor FileStreamingTask to support DataOutputStream instead of SocketChannel to unify normal and SSL transfers.","29/Aug/11 22:32;xedin;CompressedRandomAccessReader.transfer method was removed with special casing for compressed files, SocketChannel based transfer changed to DataOuputStream based to unify SSL and normal modes. SSLFileStreamTask removed as unused.","30/Aug/11 02:12;jbellis;- feels like we lose more than we gain by making writeHeader/write separate methods.  they aren't really self-contained so you have to keep the context they were called in, around mentally.  and if they were in-line, it would be obvious that you don't need to re-seek for each call to write().
- comments in write() don't really add much to what the code says, imo
- is flushing with each chunk necessary?  seems like that would harm performance
","30/Aug/11 09:17;xedin;removed writeHeader method, seek and flush are done once per section.",30/Aug/11 13:31;jbellis;+1,30/Aug/11 13:50;xedin;Committed.,"30/Aug/11 14:24;hudson;Integrated in Cassandra #1056 (See [https://builds.apache.org/job/Cassandra/1056/])
    Fix streaming over SSL when compressed SSTable involved
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3051

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163207
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java
* /cassandra/trunk/src/java/org/apache/cassandra/security/streaming/SSLFileStreamTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/FileStreamTask.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: ResultSet.next() gives NPE when run after an INSERT or CREATE statement,CASSANDRA-3052,12519225,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ardot,cdaw,cdaw,18/Aug/11 01:32,12/Mar/19 14:16,13/Mar/19 22:26,22/Aug/11 17:04,0.8.5,,,,,,0,cql,JDBC,,,"This test script used to work until I upgraded the jdbc driver to 1.0.4.

*CQL 1.0.4*: apache-cassandra-cql-1.0.4-SNAPSHOT.jar build at revision 1158979

*Repro Script*: 
* drop in test directory, change package declaration and run:  ant test -Dtest.name=resultSetNPE
* The script gives you a NullPointerException when you uncomment out the following lines after a CREATE or INSERT statement.
{code}
colCount = res.getMetaData().getColumnCount();

res.next();
{code}
* Please note that there is no need to comment out those lines if a SELECT statement was run prior.


{code}
package com.datastax.bugs;

import java.sql.DriverManager;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;

import org.junit.Test;

public class resultSetNPE {
    
    @Test
    public void createKS() throws Exception {   
        Connection initConn = null;
        Connection connection = null;

        ResultSet res;
        Statement stmt;
        int colCount = 0;
        
        Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
        
        // Check create keyspace
        initConn = DriverManager.getConnection(""jdbc:cassandra://127.0.0.1:9160/default"");     
        stmt = initConn.createStatement();

        try {
          System.out.println(""Running DROP KS Statement"");  
          res = stmt.executeQuery(""DROP KEYSPACE ks1"");  
          // res.next();
          
        } catch (SQLException e) {
            if (e.getMessage().startsWith(""Keyspace does not exist"")) 
            {
                // Do nothing - this just means you tried to drop something that was not there.
                // res = stmt.executeQuery(""CREATE KEYSPACE ks1 with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=1"");  
            } 
        }   
          
        System.out.println(""Running CREATE KS Statement"");
        res = stmt.executeQuery(""CREATE KEYSPACE ks1 with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=1"");  
        // res.next();

        initConn.close();    
    }  
 
    @Test
    public void createCF() throws Exception 
    {   

        Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
        int colCount = 0;

        Connection connection = DriverManager.getConnection(""jdbc:cassandra://127.0.0.1:9160/ks1"");     
        Statement stmt = connection.createStatement();

        System.out.print(""Running CREATE CF Statement"");
        ResultSet res = stmt.executeQuery(""CREATE COLUMNFAMILY users (KEY varchar PRIMARY KEY, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint)"");    
        
        //colCount = res.getMetaData().getColumnCount();
        System.out.println("" -- Column Count: "" + colCount); 
        //res.next();
        
        connection.close();               
    }  
    
    @Test
    public void simpleSelect() throws Exception 
    {   
        Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
        int colCount = 0;

        Connection connection = DriverManager.getConnection(""jdbc:cassandra://127.0.0.1:9160/ks1"");     
        Statement stmt = connection.createStatement();
        
        System.out.print(""Running INSERT Statement"");
        ResultSet res = stmt.executeQuery(""INSERT INTO users (KEY, password) VALUES ('user1', 'ch@nge')"");  
        //colCount = res.getMetaData().getColumnCount();
        System.out.println("" -- Column Count: "" + colCount); 
        //res.next();
        
        System.out.print(""Running SELECT Statement"");
        res = stmt.executeQuery(""SELECT KEY, gender, state FROM users"");  
        colCount = res.getMetaData().getColumnCount();
        System.out.println("" -- Column Count: "" + colCount); 
        res.getRow();
        res.next();
            
        connection.close(); 
    }  
}
{code}
",,,,,,,,,,,,,,,,21/Aug/11 17:09;ardot;statement-improper-result.txt;https://issues.apache.org/jira/secure/attachment/12491102/statement-improper-result.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-18 02:28:25.963,,,no_permission,,,,,,,,,,,,20950,,,Mon Aug 22 17:04:59 UTC 2011,,,,,,0|i0gf4f:,93880,jbellis,jbellis,,,,,,,,,"18/Aug/11 02:28;ardot;You are using {{executeQuery()}} on an {{INSERT}} (or {{UPDATE}}) statement which is not intended to return a {{ResultSet}}. You should be using an {{executeUpdate()}} which returns a count of the operations (Which CQL always returns as zero).

However NPE is a bad way of reporting the problem. I'll look into, and it provide an appropriate {{SQLException}} failure outcome.","18/Aug/11 16:59;cdaw;Hi Rick,

I am using a test harness which runs cql from a flat file, and up until this point all queries ran fine through executeQuery().

Thanks,
Cathy","18/Aug/11 17:06;jbellis;Unfortunate that improving our spec-compliance broke the test harness, but Rick is correct: the JDBC spec says that executeQuery may only be used for SQL returning a resultset, otherwise you must use executeUpdate.

Since CQL is relatively simple I don't think you need to get much more sophisticated than ""if statement.startswith(""select"") executeQuery, else executeUpdate.""",18/Aug/11 18:44;cdaw;I will make the updates to the test harness as recommended above.,"19/Aug/11 20:09;cdaw;* DROP/CREATE KEYSPACE does not work with Statement.executeUpdate()
* DROP/CREATE KEYSPACE do however work with Statement.execute()
{code}
    [junit] DROP KEYSPACE cqldb
    [junit] java.sql.SQLException: Not an update statement.

    [junit] CREATE KEYSPACE cqldb with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy'  and strategy_options:replication_factor=1
    [junit] java.sql.SQLException: Not an update statement.
{code}

I assumed this would work since since these are DDL statements
{panel}
Executes the given SQL statement, which may be an INSERT, UPDATE, or DELETE statement or an SQL statement that returns nothing, such as an SQL DDL statement.
{panel}


Do you want a new bug for this? Or is this as expected?",19/Aug/11 20:14;jbellis;That looks like a bug to me.  We can address it in this issue.,"19/Aug/11 21:37;ardot;The spec implies that {{executeUpdate()}} is the correct method for CQL/SQL commands like {{CREATE, INSERT,UPDATE,DELETE}}.

It also implies that the check is to be done on the returned result. In the case of {{executeQuery()}} an {{SQLException}} should be thrown if a {{ResultSet}} is not returned by the server. In the case of {{executeUpdate()}} return an error if a {{int}} update count is not returned.

We have an inconsistent solution in place and I'm currently working on a patch.
",21/Aug/11 17:09;ardot;Patch checks to make sure the intended result is returned from the various {{execute}} method variants. ,22/Aug/11 17:04;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ec2MultiRegionSnitch throws AssertionError on EC2,CASSANDRA-3000,12518136,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,initcron,initcron,07/Aug/11 14:47,12/Mar/19 14:16,13/Mar/19 22:26,10/Aug/11 16:28,,,,,,,0,ec2multiregionsnitch,ec2snitch,snitch,,"I found Ec2MultiRegionSnitch patch at https://issues.apache.org/jira/browse/CASSANDRA-2452 

However, I could not find any documentation on how to get it working, which address to use as seed, listen and thrift addresses. I used the following, 

seed_address     = Public DNS of the seed node 
listen_address   = Public DNS of the cluster node
rpc_address      = 0.0.0.0
endpoint_snitch: org.apache.cassandra.locator.Ec2MultiRegionSnitch


When I try to start cassandra, I get the following error: 

 INFO 14:44:19,822 Ec2Snitch adding ApplicationState ec2region=eu-west ec2zone=1c
 INFO 14:44:19,831 Starting Messaging Service on ec2-46-137-139-124.eu-west-1.compute.amazonaws.com/10.227.143.202:7000
 INFO 14:44:19,851 Using saved token 162732122844140653649170199706439942449
 INFO 14:44:19,852 Enqueuing flush of Memtable-LocationInfo@550579946(53/66 serialized/live bytes, 2 ops)
 INFO 14:44:19,852 Writing Memtable-LocationInfo@550579946(53/66 serialized/live bytes, 2 ops)
 INFO 14:44:19,908 Completed flushing /var/lib/cassandra/data/system/LocationInfo-h-20-Data.db (163 bytes)
 INFO 14:44:19,913 Compacting Major: [SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-20-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-19-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-17-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-18-Data.db')]
ERROR 14:44:19,922 Exception encountered during startup.
java.lang.AssertionError
        at org.apache.cassandra.gms.Gossiper.compareEndpointStartup(Gossiper.java:620)
        at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:803)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:706)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:839)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:986)
        at org.apache.cassandra.service.StorageService.setToken(StorageService.java:219)
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:520)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:434)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:213)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:335)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:91)
Exception encountered during startup.
java.lang.AssertionError
        at org.apache.cassandra.gms.Gossiper.compareEndpointStartup(Gossiper.java:620)
        at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:803)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:706)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:839)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:986)
        at org.apache.cassandra.service.StorageService.setToken(StorageService.java:219)
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:520)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:434)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:213)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:335)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:91)

","build version: apache-cassandra-2011-08-07_02-23-42

OS: Ubuntu 10.04.2 LTS \n \l

# uname -a
Linux ip-10-227-143-202 2.6.32-312-ec2 #24-Ubuntu SMP Fri Jan 7 18:30:50 UTC 2011 x86_64 GNU/Linux
",,,,,,,,,,,,,,,10/Aug/11 16:16;vijay2win@yahoo.com;0001-3000-fixing-localaddress-calls-insted-of-broadcastAd.patch;https://issues.apache.org/jira/secure/attachment/12489994/0001-3000-fixing-localaddress-calls-insted-of-broadcastAd.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-08 05:29:06.758,,,no_permission,,,,,,,,,,,,20931,,,Wed Aug 10 17:14:23 UTC 2011,,,,,,0|i0gerj:,93822,brandon.williams,brandon.williams,,,,,,,,,"08/Aug/11 05:29;vijay2win@yahoo.com;""listen_address = Public DNS of the cluster node""
the idea is to use broadcast address as the AWS public ip and Listen address to use AWS private IP. it is done automatically.... I will add additional checks to look for user error like this...","08/Aug/11 05:55;initcron;>""listen_address = Public DNS of the cluster node""
>the idea is to use broadcast address as the AWS public ip and Listen address to use AWS private IP. it is done >automatically.... I will add additional checks to look for user error like this...

Vijay, I still dont get this. What do you mean when you say ""it is done automatically"" ?  Does it mean that I do not need to configure listen address at all in cassandra.yaml and it will automatically be set?

Also, the reason I set listen_address to public dns on the cluster node is because it will automatically get resolved to private ip for the instances within the same region. Only the tnstances in other ec2 regions will route to its public IP. So it should do the right thing while setting up the listen address. 

","08/Aug/11 15:46;vijay2win@yahoo.com;Does it mean that I do not need to configure listen address at all in cassandra.yaml and it will automatically be set?

Yes. And the private and public ip's are quried @ amazon. broadcast ip will be set to public ip and the private ip will be used for inter node communication within the region. (https://issues.apache.org/jira/browse/CASSANDRA-2452)

I couldn't reproduce this issue, do you have an existing cluster which you are trying to update the snitch?","08/Aug/11 16:07;initcron;Vijay, 

Here is the cassandra.yaml config I am using  http://pastebin.com/7rBH45S8

I am not updating snitch on a existing cluster, but building one from scratch. 

Could you have a look at the config and see if I am doing anything wrong? ",10/Aug/11 16:16;vijay2win@yahoo.com;Issue seems to be caused by LocalAddress to broadcast address changes... but shows up when using broad cast address like in EC2MRS... Attached patch adds some comments and fixes.,10/Aug/11 16:28;brandon.williams;Committed.,"10/Aug/11 17:14;hudson;Integrated in Cassandra #1014 (See [https://builds.apache.org/job/Cassandra/1014/])
    Use broadcastAddress instead of localAddress.
Patch by Vijay, reviewed by brandonwilliams for CASSANDRA-3000

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156254
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java
* /cassandra/trunk/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
* /cassandra/trunk/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counter shard merging is not thread safe,CASSANDRA-3178,12522749,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,12/Sep/11 14:58,12/Mar/19 14:16,13/Mar/19 22:26,07/Nov/11 15:38,0.8.8,1.0.3,,,,,1,counters,,,,"The first part of the counter shard merging process is done during counter replication. This was done there because it requires that all replica are made aware of the merging (we could only rely on nodetool repair for that but that seems much too fragile, it's better as just a safety net). However this part isn't thread safe as multiple threads can do the merging for the same shard at the same time (which shouldn't really ""corrupt"" the counter value per se, but result in an incorrect context).

Synchronizing that part of the code would be very costly in term of performance, so instance I propose to move the part of the shard merging done during replication to compaction. It's a better place anyway. The only downside is that it means compaction will sometime send mutations to other node as a side effect, which doesn't feel very clean but is probably not a big deal either.",,,,,,,,,,,,,,,,03/Nov/11 11:49;slebresne;0001-Move-shard-merging-completely-to-compaction-1.0.patch;https://issues.apache.org/jira/secure/attachment/12502133/0001-Move-shard-merging-completely-to-compaction-1.0.patch,15/Sep/11 09:37;slebresne;0001-Move-shard-merging-completely-to-compaction-v2.patch;https://issues.apache.org/jira/secure/attachment/12494593/0001-Move-shard-merging-completely-to-compaction-v2.patch,12/Sep/11 15:04;slebresne;0001-Move-shard-merging-completely-to-compaction.patch;https://issues.apache.org/jira/secure/attachment/12494039/0001-Move-shard-merging-completely-to-compaction.patch,03/Nov/11 11:49;slebresne;0002-Simplify-improve-shard-merging-code-1.0.patch;https://issues.apache.org/jira/secure/attachment/12502134/0002-Simplify-improve-shard-merging-code-1.0.patch,15/Sep/11 09:37;slebresne;0002-Simplify-improve-shard-merging-code-v2.patch;https://issues.apache.org/jira/secure/attachment/12494594/0002-Simplify-improve-shard-merging-code-v2.patch,12/Sep/11 15:04;slebresne;0002-Simplify-improve-shard-merging-code.patch;https://issues.apache.org/jira/secure/attachment/12494040/0002-Simplify-improve-shard-merging-code.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2011-10-28 17:08:06.409,,,no_permission,,,,,,,,,,,,14977,,,Mon Nov 07 16:02:58 UTC 2011,,,,,,0|i0ggr3:,94144,yukim,yukim,,,,,,,,,12/Sep/11 15:04;slebresne;First patch move the code from counter replication to compaction (as described above). The second patch adapt the code to work correctly with the move to compaction but also simply and improve that code.,"13/Sep/11 15:51;slebresne;Note that the removal of flush_after_mins in 1.0 is a problem for this patch. The reason is that we want to remove a shard corresponding to a NodeId for which we know no increment has been made after time t. For that removal to be safe, we must make sure that compaction includes everything that has been issued before time t. For that, current patch check that the compaction has started after time t + 2 * flush_after_mins. I'll update the patch to use the memtables creationTime instead.  ","15/Sep/11 09:37;slebresne;Attaching v2, rebased and that remove the use of flush_after_mins.","28/Oct/11 17:08;jbellis;Yuki, can you review?","02/Nov/11 16:08;yukim;LGTM on 0.8 branch. I think it's safe to apply this on 1.0, but before that I want to make sure it works. Could you modify the patch so that I can test on 1.0?",03/Nov/11 11:49;slebresne;Attaching patches rebased against 1.0,04/Nov/11 09:14;yukim;+!. 1.0 patch also works fine.,"07/Nov/11 15:38;slebresne;Committed, thanks","07/Nov/11 16:02;hudson;Integrated in Cassandra-0.8 #394 (See [https://builds.apache.org/job/Cassandra-0.8/394/])
    patch by slebresne; reviewed by yukim for CASSANDRA-3178

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1198725
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/CounterColumn.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/CounterMutation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/Memtable.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/LazilyCompactedRow.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/PrecompactedRow.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/context/CounterContext.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/CounterMutationTest.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/context/CounterContextTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVM segfaults,CASSANDRA-3179,12522755,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,12/Sep/11 15:23,12/Mar/19 14:16,13/Mar/19 22:26,13/Sep/11 21:02,1.0.0,,,,,,0,,,,,Both with and without compressed OOPs enabled.  Seems to mostly happen during compaction+reads.  I'll attach some hs_err files shortly.,"java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)
",,,,,,,,,,,,,,,13/Sep/11 17:46;jbellis;3179-performance-test.txt;https://issues.apache.org/jira/secure/attachment/12494280/3179-performance-test.txt,12/Sep/11 16:58;jbellis;3179-v2.txt;https://issues.apache.org/jira/secure/attachment/12494065/3179-v2.txt,12/Sep/11 16:50;jbellis;3179.txt;https://issues.apache.org/jira/secure/attachment/12494061/3179.txt,12/Sep/11 15:26;brandon.williams;hs_err_pid12074.log;https://issues.apache.org/jira/secure/attachment/12494044/hs_err_pid12074.log,12/Sep/11 15:26;brandon.williams;hs_err_pid28971.log;https://issues.apache.org/jira/secure/attachment/12494045/hs_err_pid28971.log,13/Sep/11 22:07;yangyangyyy;hs_err_pid6461.log;https://issues.apache.org/jira/secure/attachment/12494335/hs_err_pid6461.log,12/Sep/11 15:26;brandon.williams;hs_err_pid7031.log;https://issues.apache.org/jira/secure/attachment/12494046/hs_err_pid7031.log,,,,,7.0,,,,,,,,,,,,,,,,,,,2011-09-12 16:28:00.391,,,no_permission,,,,,,,,,,,,4037,,,Wed Sep 14 16:34:15 UTC 2011,,,,,,0|i0ggrj:,94146,brandon.williams,brandon.williams,,,,,,,,,12/Sep/11 15:48;brandon.williams;I am unable to repro without compaction running.,"12/Sep/11 16:28;jbellis;The problem is that our whole ""zero copy"" mmap'd read path is broken by ""unmap sstables as soon as they are no longer referenced.""  Specifically, we release references during reads in CollationController or getRangeSlice as soon as we have generated CF objects from the sstables, but we will continue to reference the buffer contents later on when we send the results back to the user or the coordinator (or, as in the 7031 log, just use the results internally).",12/Sep/11 16:48;jbellis;patch to copy buffers read from an mmap'd source.,12/Sep/11 16:55;jbellis;v2 adds missing flip(),12/Sep/11 23:24;brandon.williams;+1,"13/Sep/11 04:16;jbellis;Also a patch that removes refcounting (and omits the copy above) to check what the performance hit is over pre-CASSANDRA-2521 sstable deletions.

Note that this is NOT intended as a ""real"" patch; it does not actually add back in GC-based sstable deletion.  No sstables will be removed w/o a server restart, but that should be adequate for testing read performance.","13/Sep/11 07:25;yangyangyyy;btw, as I was trying to going through the code to understand the problem, I found it seems that the MMapedSegmentedFile.cleanup() code could be skipped in some sequences of mixed read and compact:

read increases the refcount, then DataTracker.replaceCompactedSSTables()---->....---> SSTableReader.releaseReference() could see a refcount of 2, and not call the dfile.cleanup()


it's not a big problem ( code seems to run fine with the cleanup() lines commented out), but that kind of thwarts the purpose of adding them in ",13/Sep/11 17:34;jbellis;rebased performance-test,13/Sep/11 20:23;brandon.williams;I'm unable to discern any read performance difference between the performance test and v2 patches.,13/Sep/11 21:02;jbellis;committed v2,"13/Sep/11 21:05;jbellis;bq. SSTableReader.releaseReference() could see a refcount of 2, and not call the dfile.cleanup()

That means a read thread has a reference still, so cleanup will run when THAT thread releases it.","13/Sep/11 22:06;yangyangyyy;thanks.



unfortunately I got the SEGV again, after updating to the latest version. I tried to confirm so ran it again, so far seen 2 times.




 INFO 14:49:57,806 Compacting Minor: [SSTableReader(path='/usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/IpFilter-h
-76-Data.db'), SSTableReader(path='/usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/IpFilter-h-77-Data.db'), SSTableR
eader(path='/usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/IpFilter-h-79-Data.db'), SSTableReader(path='/usr/scratc
h/yyang/cass/lib/cassandra/data/testBudget_items/IpFilter-h-78-Data.db')]
9135.963: [GC 9135.963: [ParNew: 2550601K->133528K(2764800K), 1.2821340 secs] 5163440K->2756799K(5836800K) icms_dc=0 , 1.2824040
 secs] [Times: user=1.06 sys=0.04, real=1.28 secs]
 INFO 14:50:07,017 Completed flushing /usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/measuredSession-h-88-Data.db (
21435483 bytes)
 INFO 14:50:07,018 Writing Memtable-session_limit_filter@839347940(21205810/281849841 serialized/live bytes, 286565 ops)
 INFO 14:50:15,987 Completed flushing /usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/session_limit_filter-h-88-Data
.db (23326502 bytes)
 INFO 14:50:15,999 Writing Memtable-ad_impression_session@100750437(21205662/296528055 serialized/live bytes, 286563 ops)
9154.456: [GC 9154.456: [ParNew: 2591128K->125067K(2764800K), 0.2846910 secs] 5214399K->2756607K(5836800K) icms_dc=0 , 0.2849470 secs] [Times: user=0.71 sys=0.01, real=0.29 secs]
 INFO 14:50:26,220 Completed flushing /usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/ad_impression_session-h-89-Data.db (52154466 bytes)
 INFO 14:50:26,221 Writing Memtable-ad_ip_agent@1095480823(21205662/286042295 serialized/live bytes, 286563 ops)
 INFO 14:50:26,463 Compacted to [/usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/IpFilter-h-80-Data.db,].  94,782,508 to 83,577,108 (~88% of original) bytes for 6,368 keys at 2.783688MBPS.  Time: 28,633ms.
 INFO 14:50:26,464 CF Total Bytes Compacted: 3,494,202,400
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00002aaaab7b9c88, pid=6461, tid=1253022016
#
# JRE version: 7.0-b147
# Java VM: Java HotSpot(TM) 64-Bit Server VM (21.0-b17 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# J  org.apache.cassandra.io.util.AbstractDataInput.readByte()B
#
# Core dump written. Default location: /usr/scratch/yyang/whisky/core or core.6461



attaching the err file separately",13/Sep/11 22:07;yangyangyyy;failure log after using updated code,13/Sep/11 22:46;brandon.williams;Can you reproduce w/o compressed oops?,"13/Sep/11 23:02;yangyangyyy;let me try ...

On Tue, Sep 13, 2011 at 3:47 PM, Brandon Williams (JIRA)
","14/Sep/11 00:34;yangyangyyy;confirmed with the -XX:-UseCompressedOops arg,

has generated 1 SEGV so far


btw, it would be really helpful to add a simple dumb stress test testcase,
some bugs are not easily exposed with the unit tests.","14/Sep/11 00:40;brandon.williams;What process are you following to reproduce?  I can't do it with compaction+reads, but I'm also not using JRE 7.","14/Sep/11 00:52;yangyangyyy;I'm just writing + reading on the cassandra server, and doing this on the side:


while : ;do nodetool flush ; sleep 20;done


(disclaimer: the cassandra server is a modified one, where I have a
thread that takes in client request through a custom AVRO server, and
then call the storageProxy.batch_mutate() and StorageProxy.get()
directly. it should not be materially different from the pure
cassandra server as far as this bug is concerned, since it's basically
just swapping out the thrift server
)

On Tue, Sep 13, 2011 at 5:42 PM, Brandon Williams (JIRA)
",14/Sep/11 01:22;jbellis;You're sure you're on 1.0.0 >= r1170342?,"14/Sep/11 01:22;yangyangyyy;very interesting, I switched from mmap mode to ""standard"", and go the following errors on compaction,
I believe they follow the same path as the mmap route, but since it gave an exception here, instead of siliently SEGV, this could provide a useful hint to what caused the SEGV




Caused by: java.nio.channels.ClosedChannelException
        at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:268)
        at java.io.RandomAccessFile.readByte(RandomAccessFile.java:640)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:356)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:367)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:87)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:82)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:72)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:36)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
        ... 21 more
ERROR 01:19:22,415 Fatal exception in thread Thread[ReadStage:246,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: error reading 1 of 1
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1165)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.RuntimeException: error reading 1 of 1
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:83)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:107)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:194)
        at org.apache.cassandra.utils.MergeIterator.<init>(MergeIterator.java:47)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:142)
        at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:66)
        at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:96)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:249)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:61)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1276)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1171)

        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1127)
        at org.apache.cassandra.db.Table.getRow(Table.java:388)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:61)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:694)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1161)
        ... 3 more
Caused by: java.nio.channels.ClosedChannelException
        at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:268)
        at java.io.RandomAccessFile.readByte(RandomAccessFile.java:640)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:356)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:367)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:87)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:82)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:72)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:36)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
        ... 21 more
 INFO 01:19:23,738 Compacted to [/mnt/cass/lib/cassandra/data/testBudget_items/measuredSession-h-10-Data.db,].  16,840,264 to 16,840,036 (~99% of original) bytes for 22,531 keys at 3.247707MBPS.  Time: 4,945ms.
 INFO 01:19:23,738 CF Total Bytes Compacted: 157,135,410
","14/Sep/11 01:26;yangyangyyy;sorry for the misunderstanding. I applied the v2 patch on
4dbda612f49c97fd5e3f66e7875a20ec9a0dc829   (Aug 26 version)

I could test the latest one if you think that's going to be different.



On Tue, Sep 13, 2011 at 6:22 PM, Jonathan Ellis (JIRA) <jira@apache.org> wrote:
",14/Sep/11 01:30;jbellis;Please test 1.0.0 branch to make sure we are doing the same thing.,"14/Sep/11 07:56;yangyangyyy;I know now, it must be due to
https://issues.apache.org/jira/browse/CASSANDRA-3110

let me verify with the latest code


","14/Sep/11 16:34;yangyangyyy;with 1.0.0 HEAD, no more SEGV after a night of stress tests.

thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unable to start single node cluster when listen_address is not localhost,CASSANDRA-3191,12522827,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,tjake,tjake,13/Sep/11 00:10,12/Mar/19 14:16,13/Mar/19 22:26,13/Sep/11 17:30,1.0.0,,,,,,0,,,,,"Due to bootstrap default == true

{code}
INFO 20:07:51,839 Joining: waiting for ring and schema information
 INFO 20:08:21,839 Joining: getting bootstrap token
ERROR 20:08:21,843 Exception encountered during startup.
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:168)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:150)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapToken(BootStrapper.java:145)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:528)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:450)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:372)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:213)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:335)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Exception encountered during startup.
{code}",,,,,,,,,,,,,,,,13/Sep/11 17:07;jbellis;3191.txt;https://issues.apache.org/jira/secure/attachment/12494271/3191.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-13 00:13:36.536,,,no_permission,,,,,,,,,,,,4031,,,Tue Sep 13 17:30:08 UTC 2011,,,,,,0|i0ggwv:,94170,slebresne,slebresne,,,,,,,,,"13/Sep/11 00:13;brandon.williams;You can workaround this by explicitly disabling it in the config:

bq. auto_bootstrap: false

But imo, when a token is specified we should automatically assume there is no need to bootstrap.",13/Sep/11 00:27;tjake;The fact the option is missing from the config will make it hard for users to fix without pulling some hair out.,13/Sep/11 03:50;jbellis;Is your single node not a seed?,13/Sep/11 13:13;tjake;I left the listen address blank.  and the seed is 127.0.0.1,"13/Sep/11 16:27;slebresne;I could be wrong, but I think that was is happening here is that leaving it blank will resolve to the machine address (i.e, not the loopback ip), and thus the node doesn't recognize itself as a seed. In a way, it's a configuration error, listen_address should be 127.0.0.1. Not sure we can really improve that error message, but maybe we can improve the comments in the yaml.","13/Sep/11 17:07;jbellis;Patch to explain the problem and how to solve it in the exception message.
 ",13/Sep/11 17:14;slebresne;+1,13/Sep/11 17:30;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Arena allocation causes excessive flushing on small heaps,CASSANDRA-3168,12522427,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,09/Sep/11 13:52,12/Mar/19 14:16,13/Mar/19 22:26,12/Sep/11 15:50,1.0.0,,,,,,0,,,,,"adding allocator.size() to Memtable.getLiveSize has two problems:

1) it double-counts allocated parts of regions
2) it makes the size of an empty memtable the size of a single region

(2) is a particular problem because flushing a nearly-empty memtable will not actually free up much memory -- we just trade one almost-empty region, for another.  In testing, I even saw this happening to the low-traffic system tables like LocationInfo.",,,,,,,,,,,,,,,,09/Sep/11 18:33;stuhood;0002-3168.txt;https://issues.apache.org/jira/secure/attachment/12493837/0002-3168.txt,09/Sep/11 14:04;jbellis;3168.txt;https://issues.apache.org/jira/secure/attachment/12493777/3168.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-09 18:33:13.897,,,no_permission,,,,,,,,,,,,4046,,,Mon Sep 12 15:50:32 UTC 2011,,,,,,0|i0ggmv:,94125,stuhood,stuhood,,,,,,,,,09/Sep/11 14:04;jbellis;patch to remove allocator.size() from getLiveSize calculation.  also adds note about arena allocation to NEWS.,"09/Sep/11 18:33;stuhood;Using both currentThroughput and allocator.size is definitely wrong... don't know what I was thinking there.

Since this was the only code using SlabAllocator.size(), here's a patch to remove it.",09/Sep/11 18:35;stuhood;+1.,"12/Sep/11 15:50;jbellis;added debug logging to SlabAllocator for region count, and committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
debian packaging installation problem when installing for the first time,CASSANDRA-3198,12522884,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jsevellec,jsevellec,jsevellec,13/Sep/11 11:59,12/Mar/19 14:16,13/Mar/19 22:26,03/Oct/11 21:53,0.8.7,,,Packaging,,,0,,,,,"when installing cassandra through the debian packaging for the first time, there is permission problem when starting Cassandra.

Normally, the postinst script change owner of /var/log/cassandra and /var/lib/cassandra from root to cassandra user.

there is a problem with the test which verify if threre is a need to change the owner of these directory or not.

On a new install, the $2 parameter is not set and the the test is false and the owner is not changed.

(simply, i think replace ""&&"" with ""||"" might work)
",,,,,,,,,,,,,,,,28/Sep/11 17:15;slebresne;3198.patch;https://issues.apache.org/jira/secure/attachment/12496899/3198.patch,13/Sep/11 23:35;shyamal;debian-postinst-fixperms.patch;https://issues.apache.org/jira/secure/attachment/12494343/debian-postinst-fixperms.patch,13/Sep/11 14:15;jsevellec;trunk-3198-v1.patch;https://issues.apache.org/jira/secure/attachment/12494233/trunk-3198-v1.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-09-13 23:35:21.166,,,no_permission,,,,,,,,,,,,20971,,,Mon Oct 03 22:57:15 UTC 2011,,,,,,0|i0ggzz:,94184,urandom,urandom,,,,,,,,,13/Sep/11 14:15;jsevellec;here is the patch,"13/Sep/11 23:35;shyamal;I believe this is the correct patch. It sets permissions on first install, or failing that, when upgrading from earlier than 0.6.4-2 per CASSANDRA-1004","14/Sep/11 13:46;slebresne;That second patch looks good, but maybe we should change that 0.6.4-2 to 0.8.6. Eric, since you initially added that test, an opinion ?","14/Sep/11 15:43;urandom;The comparison to 0.6.4-2 can be safely removed now, so it should look something like:

{noformat}
if [ -z ""$2"" ]; then
    chown -R cassandra: /var/lib/cassandra
    chown -R cassandra: /var/log/cassandra
fi
{noformat}","14/Sep/11 16:02;slebresne;bq. The comparison to 0.6.4-2 can be safely removed now

Hum, the goal of this ticket is that anyone that have started using the deb package after 0.6.4-2 have /var/{lib,log}/cassandra owned by root.root, which was not intended. So we can decide that we want to leave all those people with those ""wrong"" permissions, but it's not safer to remove the comparison now that it was when the patch was initially committed.","14/Sep/11 19:02;urandom;{quote}
Hum, the goal of this ticket is that anyone that have started using the deb package after 0.6.4-2 have /var/{lib,log}/cassandra owned by root.root, which was not intended. So we can decide that we want to leave all those people with those ""wrong"" permissions, but it's not safer to remove the comparison now that it was when the patch was initially committed.
{quote}

I think that statement was always bugged.  What it _should_ do is to ensure that /var/{lib,log}/cassandra are owned by user cassandra on new installs only, not upgrades (allowing the user to override that ownership).

At this point, I'm not sure what the comparison to 0.6.4-2 was supposed to accomplish, I assume it was a (misguided )attempt at applying this policy to versions greater than or equal to 0.6.4-2 (we didn't always create the cassandra user and chown in postinst).  Either way, the current packaging will never be used to upgrade from an 0.6.x release directly, so there is no danger in removing it.

So, looking at that (untested )snippet again:

{noformat}
if [ -z ""$2"" ]; then
    chown -R cassandra: /var/lib/cassandra
    chown -R cassandra: /var/log/cassandra
fi
{noformat}

would only apply ownership changes if argument #2 is empty, that is to say, there is no previously configured version (i.e. not an upgrade).",14/Sep/11 19:07;jsevellec;I agree with that too,"15/Sep/11 00:59;shyamal;BTW, I tested the snippet in Eric's comment with the additional pre-0.6.4-2 check (my patch) pretty extensively: install, upgrade to package with new version, remove and reinstall etc. It certainly looks right, and I am sure it works because I tried it, several ways. I did not test the upgrade from 0.6.4-2 since I did not completely understand why it was important, but I left it in once I traced it down to CASSANDRA-1004. As I suspected  there's people out there that know better :-)

(BTW, in testing I found one minor irritation that /var/log/cassandra/output.log gets created owned by root after installation. I suspect this is because it is created before jsvc switches the user to after allowing privileged operations to complete...once I'm sure the problem is real I'll file it separately....but it was a good test case for the patch!)

","15/Sep/11 10:48;slebresne;bq. we didn't always create the cassandra user and chown in postinst

Yes, and to make it clear what my point is: while we do create the cassandra user since 0.6.4-2, we *never* chown in the postinst _excepted_ for people that upgraded *all the way* since a pre 0.6.4 release. That is, any current new installation (say of 0.8.5) *does not* chown in the postint and /var/lib/cassandra and /var/log/cassandra (the directories) are owned by root, not cassandra.

Now I'm perfectly fine saying that there is no point in changing things during upgrades and to only fix the chown in the postinst for new installs, but just wanted to make it clear that this has nothing to do with upgrades from some 0.6.x versions.


",28/Sep/11 15:53;shyamal;Any chance this can be fixed for the upcoming 1.0.0 release?,"28/Sep/11 17:15;slebresne;Yes, there is probably no reason for not fixing this. Attaching the simple patch that only chown the directories on new installs.

I still don't really understand how previous install have worked so far with /var/lib/cassandra being own by root but the cassandra process being run as the cassandra user, but it costs nothing to be conservative: let's fix it for new install and do nothing for upgrade for now.

We can always keep debating on the best way to ""fix"" old installs, but let's have clean new install right now. ","28/Sep/11 21:43;shyamal;Agree completely, Thanks!",03/Oct/11 21:53;urandom;committed,"03/Oct/11 22:57;hudson;Integrated in Cassandra-0.8 #358 (See [https://builds.apache.org/job/Cassandra-0.8/358/])
    set ownership on new installs

Patch by Sylvain Lebresne; reviewed by eevans for CASSANDRA-3198

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1178592
Files : 
* /cassandra/branches/cassandra-0.8/debian/cassandra.postinst
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rolling upgrades from 0.7 to 0.8 not possible,CASSANDRA-3166,12522418,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,krummas,krummas,09/Sep/11 12:45,12/Mar/19 14:16,13/Mar/19 22:26,12/Sep/11 16:50,0.8.6,,,,,,0,,,,,"We are in the progress of upgrading to 0.8 and we need to do a rolling upgrade, this fails miserably and it is reproducible;

1. set up a 3 node cluster with 0.7.9 and rf=3, read and write, QUORUM
2. upgrade one of the nodes (i upped a seednode, not sure if that is important)
3. continue reading/writing
4. see logs on the 0.7 node fill up with: INFO 12:36:08,240 Received connection from newer protocol version. Ignorning message.


it does work if i start the 0.7.9 nodes *after* the 0.8.4 node which makes me think that it matters if it is the 0.8 node connecting to the 0.7 nodes or the other way round.

Debug logging on the 0.8 node shows:
/var/log/cassandra/system.log.9:DEBUG [pool-2-thread-82] 2011-09-09 11:55:06,067 StorageProxy.java (line 178) Write timeout java.util.concurrent.TimeoutException for one (or more) of: 
/var/log/cassandra/system.log.9:DEBUG [pool-2-thread-76] 2011-09-09 11:55:06,067 StorageProxy.java (line 584) Read timeout: java.util.concurrent.TimeoutException: Operation timed out - received only 1 responses from /193.182.3.92,  .

nothing except for the ""newer protocol version..."" in the 0.7-logs

i will continue to look at this issue but if anyone has a quick patch, let me know

",,,,,,,,,,,,,,,,11/Sep/11 09:55;krummas;3166.txt;https://issues.apache.org/jira/secure/attachment/12493952/3166.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-09 13:42:57.385,,,no_permission,,,,,,,,,,,,4048,,,Mon Sep 12 17:48:01 UTC 2011,,,,,,0|i0gglz:,94121,brandon.williams,brandon.williams,,,,,,,,,"09/Sep/11 12:48;krummas;oh, note that it fails all the way to the client as well, timeouts in hector","09/Sep/11 13:42;scode;Are you seeing any ""Version for $IP is $VERSION"" output when running in debug mode? IncomingTcpConnection.run() should be logging that.","09/Sep/11 13:59;scode;Currently suspecting 6eb154ba5616e0df3ce4f11c88dbb1c92d317465 which adds the version reset on disconnection.
",09/Sep/11 14:09;scode;And that commit was due to CASSANDRA-2818 and CASSANDRA-2860.,"09/Sep/11 15:04;scode;Removing the resetVersion() did not help. I added some logging to IncomingTcpConnection and it seems that when the 0.8 node goes up first, the 0.7 node never tries to make an outgoing connection to it.

If my understanding is correct, from reading CASSANDRA-2818 and looking at the code, I think the intent is that we discover the version of the other guy whenever that guy connects to *us*; we can never find out that the other side has a mis-matched version based on activity on the outbound connection.

So, incoming connections would be a necessity in order for the 0.8 node to ever adjust it's lingo.","09/Sep/11 15:10;jbellis;That's right.  This is why when we do get a message from a newer-version host we make sure to add it to gossiper so we connect back to it.

Not sure if that fix got applied to 0.7 -- if not, making the 0.8 node a seed should work around it.","09/Sep/11 15:46;scode;Sorry, epic fail on my part. Removing the version reset does have an effect, and the 0.7 node is in fact connecting. I made a boo-boo involving running from the wrong working copy...
","09/Sep/11 16:51;scode;I'm having difficulty coming up with a clean yet simple fix here. Reverting CASSANDRA-2860 certainly fixes this problem, but re-introduces CASSANDRA-2860 instead.

I could imagine an environment variable/config option to disable the support for pretending you are older than you are, which could be used in a second round of rolling restarts after upgrading all nodes of a cluster to 0.8. A JMX tweakable setting would be nice, but upon changing it you'd want to tear down all the TCP connections to re-initiate versioning negotiation so maybe it's okay to leave it with an extra round of restarts required.

Alternatively, I think (not tested) things will tend to sort itself out incrementally every time you restart a 0.8 node since it will tend to initiate connections to other nodes immediately, but documenting for users that they need to restart nodes all over the place until everyone seems to have gotten it seems like a poor solution.

Adding some new kind of message that says ""i really am this other version"" or similar isn't clean.

Am I missing a much simpler and cleaner fix here?
","11/Sep/11 09:54;krummas;Minimal patch attached

Clear version in IncomingTcpConnection instead since that is the one setting it;
before we could end up in a state where the outgoing connections got closed, but the incoming one was still up, meaning the version was reset and it was never possible to get the version set again.

Now it is the IncomingTcpConnections responsibility to keep track of versions, if that one is closed, we are bound to get a new incoming connection and therefor set the version correctly

",11/Sep/11 09:55;krummas;Make IncomingTcpConnection responsible for version handling,"11/Sep/11 10:33;scode;+1 on my end. That's a very simple solution that I wasn't seeing. Can't figure out a way it will break anything.

* 0.7 <-> 0.7: No version mismatch ever, no reset ever happens. All is well.
* 0.8 <-> 0.8: Same.
* 0.7 <-> 0.8: 0.8 -> 0.7 will be killed (streaming) or retained but messages ignored (messaging). 0.7 -> 0.8 will work, and 0.8 will know the version of 0.7. Future outgoing will use correct version, and the pre-existing messaging connection starts sending messages at a version that isn't ignored.
* 0.7 node restarted and upgraded to 0.8 talking to 0.8: Both incoming/outgoing go down, so version reset, then equivalent of 0.8 <-> 0.8.
* 0.7 node restarted and upgraded to 0.8 talking to 0.7: Both incoming/outgoing go down, so version reset, then equivalent of 0.7 <-> 0.8.

","12/Sep/11 16:50;brandon.williams;Committed, with minor changes: logs pushed to debug, if logic around logging removed since interpolation of {} automatically does that.","12/Sep/11 17:48;hudson;Integrated in Cassandra-0.8 #322 (See [https://builds.apache.org/job/Cassandra-0.8/322/])
    Make IncomingTcpConnection responsible for version handling.
Patch by Marcus Erikkson, reviewed by Peter Schuller and brandonwilliams
for CASSANDRA-3166

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1169823
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/IncomingTcpConnection.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SlabAllocator OOMs much faster than HeapAllocator,CASSANDRA-3163,12522316,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,08/Sep/11 20:43,12/Mar/19 14:16,13/Mar/19 22:26,09/Sep/11 18:13,1.0.0,,,,,,0,,,,,"SlabAllocator will OOM with stress at around 5.5M rows, which in this configuration is roughly 3.3M rows per node.  Reverting to the HeapAllocator allowed all 10M rows to finish.  Examining the SA heap dump in MAT just shows ~98% of the heap is in 'remainder'","3 nodes, 1G heap, RF=2",,,,,,,,,,,,,,,09/Sep/11 14:17;jbellis;3163-v2.txt;https://issues.apache.org/jira/secure/attachment/12493778/3163-v2.txt,09/Sep/11 17:21;jbellis;3163-v3.txt;https://issues.apache.org/jira/secure/attachment/12493822/3163-v3.txt,09/Sep/11 18:01;jbellis;3163-v4.txt;https://issues.apache.org/jira/secure/attachment/12493828/3163-v4.txt,09/Sep/11 13:26;jbellis;3163.txt;https://issues.apache.org/jira/secure/attachment/12493775/3163.txt,08/Sep/11 21:22;brandon.williams;system.log.gz;https://issues.apache.org/jira/secure/attachment/12493670/system.log.gz,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-09-08 20:46:07.332,,,no_permission,,,,,,,,,,,,4051,,,Fri Sep 09 18:13:43 UTC 2011,,,,,,0|i0ggkf:,94114,brandon.williams,brandon.williams,,,,,,,,,08/Sep/11 20:46;jbellis;what liveRatio was it logging on the OOM run?,"08/Sep/11 21:22;brandon.williams;Here's a system.log with your extra debug statements and Memtable set to DEBUG.  It OOM'd many times, but never actually dumped.","09/Sep/11 13:26;jbellis;What is happening is, the slab-allocated row keys are being put into the IndexSummary of the new sstable on flush.  So the regions can only be GC's post-compaction.","09/Sep/11 14:17;jbellis;v2 re-clones into indexposition instead of trying to avoid cloning at all.  this should be a small penalty since we're only re-cloning sampled keys, not all of them.","09/Sep/11 17:18;jbellis;v3 also applies getMinimalKey to SSTR.first, last fields.",09/Sep/11 18:01;jbellis;v4 fixes NPE on sstable load,09/Sep/11 18:09;brandon.williams;+1,09/Sep/11 18:13;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in RowRepairResolver,CASSANDRA-3192,12522833,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,cdaw,cdaw,13/Sep/11 01:07,12/Mar/19 14:16,13/Mar/19 22:26,13/Sep/11 12:57,1.0.0,,,,,,0,,,,,"On a 3 node brisk cluster (running against C* 1.0 branch), I was running the java stress tool and the terasort concurrently in two sessions.  Eventually both jobs failed with TimedOutException.
  
From this point forward most additional activity will fail with a TimedOutException. 
* Java Stress Tool - 5 rows / 10 columns - Operation [0] retried 10 times - error inserting key 0 ((TimedOutException))
* Hive - show tables: FAILED: Error in metadata: com.datastax.bdp.hadoop.hive.metastore.CassandraHiveMetaStoreException: There was a problem with the Cassandra Hive MetaStore: Could not connect to Cassandra. Reason: Error connecting to node localhost

However, the Cassandra CLI appears to be happy
* Cassandra CLI: you can successfully insert and read using consistencylevel as ONE or ALL

The seed node has the following error repeatedly occurring in the logs.  The other two nodes have no errors.

{code}
ERROR [ReadRepairStage:15] 2011-09-13 00:44:25,971 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadRepairStage:15,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:82)
	at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{code}",,,,,,,,,,,,,,,,13/Sep/11 03:48;jbellis;3192.txt;https://issues.apache.org/jira/secure/attachment/12494171/3192.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-13 03:42:25.516,,,no_permission,,,,,,,,,,,,4030,,,Tue Sep 13 12:57:57 UTC 2011,,,,,,0|i0ggxb:,94172,slebresne,slebresne,,,,,,,,,"13/Sep/11 01:45;cdaw;I noticed that after starting up the server, this error message was in the log file of the second node (non-seed) after startup.

*node2*
{code}
ERROR 01:30:05,522 Fatal exception in thread Thread[HintedHandoff:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:282)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
	at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:333)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

",13/Sep/11 03:42;brandon.williams;Solved in CASSANDRA-3156,13/Sep/11 03:48;jbellis;patch to fix regression caused by CASSANDRA-2643,13/Sep/11 03:49;jbellis;(There's actually still a bug where we don't handle null CFs correctly in the resolve.  See patch.),13/Sep/11 12:16;slebresne;+1,13/Sep/11 12:57;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress cannot set the compaction strategy,CASSANDRA-3204,12522993,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,brandon.williams,brandon.williams,14/Sep/11 04:24,12/Mar/19 14:16,13/Mar/19 22:26,14/Sep/11 16:02,1.0.0,,,Legacy/Tools,,,0,,,,,"stress can't set the compaction strategy, so testing leveldb-style compaction is more difficult than it should be, especially with lots of cluster setup/teardown.",,,,,,,,,,,,,,,,14/Sep/11 12:22;xedin;CASSANDRA-3204.patch;https://issues.apache.org/jira/secure/attachment/12494435/CASSANDRA-3204.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-14 12:22:03.214,,,no_permission,,,,,,,,,,,,4023,,,Wed Sep 14 16:02:36 UTC 2011,,,,,,0|i0gh2f:,94195,brandon.williams,brandon.williams,,,,,,,,,"14/Sep/11 12:22;xedin;added -Z (--compaction-strategy=<strategy>) option.

rebased with latest cassandra-1.0.0 (last commit 9867a49581b88f2b53efe925290de81b6afc06f0)",14/Sep/11 15:50;brandon.williams;+1,14/Sep/11 16:02;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Cassandra compile under JDK 7,CASSANDRA-3275,12525211,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,satishbabu,xedin,xedin,29/Sep/11 16:59,12/Mar/19 14:16,13/Mar/19 22:26,30/Sep/11 15:36,1.0.1,,,,,,0,jdk7,,,,"Currently system won't compile under JDK 7 because of errors in CQL JDBC component.

{noformat}
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CResultSet.java:39: error: CResultSet is not abstract and does not override abstract method <T>getObject(String,Class<T>) in ResultSet
    [javac] class CResultSet extends AbstractResultSet implements CassandraResultSet
    [javac] ^
    [javac]   where T is a type-variable:
    [javac]     T extends Object declared in method <T>getObject(String,Class<T>)
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraConnection.java:81: error: CassandraConnection is not abstract and does not override abstract method getNetworkTimeout() in Connection
    [javac] class CassandraConnection extends AbstractCassandraConnection implements Connection
    [javac] ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDataSource.java:24: error: CassandraDataSource is not abstract and does not override abstract method getParentLogger() in CommonDataSource
    [javac] public class CassandraDataSource implements DataSource
    [javac]        ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDatabaseMetaData.java:32: error: CassandraDatabaseMetaData is not abstract and does not override abstract method generatedKeyAlwaysReturned() in DatabaseMetaData
    [javac] class CassandraDatabaseMetaData implements DatabaseMetaData
    [javac] ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDriver.java:40: error: CassandraDriver is not abstract and does not override abstract method getParentLogger() in Driver
    [javac] public class CassandraDriver implements Driver
    [javac]        ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraStatement.java:50: error: CassandraStatement is not abstract and does not override abstract method isCloseOnCompletion() in Statement
    [javac] class CassandraStatement extends AbstractStatement implements Statement
    [javac] ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraPreparedStatement.java:61: error: CassandraPreparedStatement is not abstract and does not override abstract method isCloseOnCompletion() in Statement
    [javac] class CassandraPreparedStatement extends CassandraStatement implements PreparedStatement
    [javac] ^
    [javac] Note: /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraPreparedStatement.java uses or overrides a deprecated API.
{noformat}",,,,,,,,,,,,,,,,30/Sep/11 07:01;satishbabu;3275-patch.diff;https://issues.apache.org/jira/secure/attachment/12497122/3275-patch.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-29 17:55:10.075,,,no_permission,,,,,,,,,,,,40144,,,Fri Sep 30 16:41:44 UTC 2011,,,,,,0|i0ghxr:,94336,ardot,ardot,,,,,,,,,"29/Sep/11 17:55;jbellis;Is that because JDK7 added some new ResultSet methods, or are we violating some generics rule that wasn't enforced before?","30/Sep/11 05:45;satishbabu;In JDK7 several new methods are added to support JDBC 4.1, Need to update drivers to fix these errors

Area: API: JDBC
Synopsis: New JDBC Methods, Including new Methods in Interfaces
Description: For the Java SE 7 release, there are new methods to support JDBC 4.1. This includes methods added to the java.sql.Connection, java.sql.Driver, javax.sql.CommonDatasource, and java.sql.Statement interfaces. Because all methods of an interface must be implemented, previous code that uses these interfaces will not compile on Java SE 7 unless you add the new methods. See the JDBC documentation for more information.
Nature of Incompatibility: source

http://www.oracle.com/technetwork/java/javase/compatibility-417013.html
",30/Sep/11 06:59;satishbabu;Added default implementation to newly added java.sql package,30/Sep/11 13:39;ardot;+1 LGTM.,30/Sep/11 13:54;xedin;Can you make this patch compatible with cassandra-1.0.0 branch? it seems like you use old directory structure - drivers/java...,"30/Sep/11 14:20;xedin;but I can confirm that this compiles so +1, I will check if we can move this to 1.1 and commit in that case.",30/Sep/11 15:14;jbellis;IMO we should put it in 1.0.1.,30/Sep/11 15:36;xedin;Committed.,"30/Sep/11 16:41;hudson;Integrated in Cassandra #1133 (See [https://builds.apache.org/job/Cassandra/1133/])
    fix JDBC driver to compile under JDK 7
patch by satishbabu; reviewed by xedin for CASSANDRA-3275

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177701
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/drivers/java/src/org/apache/cassandra/cql/jdbc/AbstractCassandraConnection.java
* /cassandra/trunk/drivers/java/src/org/apache/cassandra/cql/jdbc/AbstractResultSet.java
* /cassandra/trunk/drivers/java/src/org/apache/cassandra/cql/jdbc/AbstractStatement.java
* /cassandra/trunk/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDataSource.java
* /cassandra/trunk/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDatabaseMetaData.java
* /cassandra/trunk/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDriver.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FailureDetector can take a very long time to mark a host down,CASSANDRA-3273,12525134,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,29/Sep/11 04:21,12/Mar/19 14:16,13/Mar/19 22:26,03/Oct/11 20:37,0.8.7,1.0.0,,,,,0,,,,,"There are two ways to trigger this:

* Bring a node up very briefly in a mixed-version cluster and then terminate it
* Bring a node up, terminate it for a very long time, then bring it back up and take it down again

In the first case, what can happen is a very short interval arrival time is recorded by the versioning logic which requires reconnecting and can happen very quickly. This can easily be solved by rejecting any intervals within a reasonable bound, for instance the gossiper interval.

The second instance is harder to solve, because what is happening is that an extremely large interval is recorded, which is the time the node was left dead the first time.  This throws off the mean of the intervals and causes it to take a much longer time than it should to mark it down the second time.
",,,,,,,,,,,,,,,,29/Sep/11 21:04;brandon.williams;3273.txt;https://issues.apache.org/jira/secure/attachment/12497059/3273.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-29 04:36:42.937,,,no_permission,,,,,,,,,,,,38928,,,Mon Oct 03 21:20:41 UTC 2011,,,,,,0|i0ghwv:,94332,thepaul,thepaul,,,,,,,,,"29/Sep/11 04:36;jbellis;bq. a very short interval arrival time is recorded by the versioning logic which requires reconnecting and can happen very quickly

I thought only gossip heartbeats generate interval measurements, is that incorrect?

bq. an extremely large interval is recorded, which is the time the node was left dead the first time

What if we reset the intervals when we get a node back-from-the-dead?","29/Sep/11 15:53;slebresne;As a side note, this is a big pain in the ass for (distributed) testing, where you start/stop node quickly.","29/Sep/11 18:12;brandon.williams;bq. I thought only gossip heartbeats generate interval measurements, is that incorrect?

Heartbeats and generation changes.  I take back what I said though, it's not the versioning reconnection, and it's not a problem with regard to making the FD take a long time to mark a host down.

It is, however, possible to receive two intervals in a short amount of time, just due to timer skew between the two hosts, but it can only happen once since after that they will be in sync from the FD's perspective.

The net effect of this in the pathological case would be that the FD causes a host to be marked down if the host suddenly becomes silent for a period of 4-5s after the FD receives the initial (500ms) interval and then the short (1ms) one only.  

",29/Sep/11 18:39;jbellis;So it sounds like only case 2 is worth bothering with?,"29/Sep/11 21:04;brandon.williams;bq. What if we reset the intervals when we get a node back-from-the-dead?

That makes sense if we're observing a generation change, the node either rebooted or was taken over by a new machine, so relearning the network characteristics is a good idea.

In the case that there was only a heartbeat change, that indicates there was something bad (most likely in the network) and we should remember that for next time to avoid flapping.  However, in the case of a long partition where the generation won't change, we don't want to record the partition time as an interval since if the partition reoccurs soon, it will take us a very long time to mark the host down again.

This patch clears the intervals on a generation change, and handles the long partition case by defining a reasonable maximum to record, in this case the rpc timeout, since adapting beyond this rather than failing quickly doesn't make much sense that I can think of, but I'll entertain a higher hard set default if anyone disagrees.","30/Sep/11 00:20;cywjackson;smoke test only: so far so good

i have a node (6-node cluster) that was down for a LONG time (700 PHI), then start that node for about 30 sec before stopping it

ring shows that node is down in about 20-30secs, gives or takes

{noformat}
TRACE [GossipTasks:1] 2011-09-30 00:14:58,727 FailureDetector.java (line 156) PHI for /10.40.22.186 : 703.9568334429565
TRACE [GossipTasks:1] 2011-09-30 00:14:58,727 FailureDetector.java (line 160) notifying listeners that /10.40.22.186 is down
TRACE [GossipTasks:1] 2011-09-30 00:14:58,727 FailureDetector.java (line 161) intervals: 1027.0 1904.0 2153.0 951.0 215.0 1788.0 1002.0 1002.0 895.0 1133.0 1869.0 mean: 1267.1818181818182
DEBUG [GossipStage:1] 2011-09-30 00:14:58,728 Gossiper.java (line 661) Clearing interval times for /10.40.22.186 due to generation change
DEBUG [GossipStage:1] 2011-09-30 00:14:58,728 FailureDetector.java (line 242) Ignoring interval time of 2054002.0
TRACE [GossipTasks:1] 2011-09-30 00:14:59,729 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.0
TRACE [GossipTasks:1] 2011-09-30 00:15:00,730 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.43429448190325176
TRACE [GossipTasks:1] 2011-09-30 00:15:01,732 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.8690228244277856
TRACE [GossipTasks:1] 2011-09-30 00:15:02,733 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.2890479080886867
TRACE [GossipTasks:1] 2011-09-30 00:15:03,734 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.19662520906271305
TRACE [GossipTasks:1] 2011-09-30 00:15:04,735 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.20189636121957935
TRACE [GossipTasks:1] 2011-09-30 00:15:05,737 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.5977870734348798
TRACE [GossipTasks:1] 2011-09-30 00:15:06,738 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.20802340729819624
TRACE [GossipTasks:1] 2011-09-30 00:15:07,739 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.6139326289463335
TRACE [GossipTasks:1] 2011-09-30 00:15:08,740 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.21152308625862737
TRACE [GossipTasks:1] 2011-09-30 00:15:09,741 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.21261773854488178
TRACE [GossipTasks:1] 2011-09-30 00:15:10,743 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.6270982327510521
TRACE [GossipTasks:1] 2011-09-30 00:15:11,744 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.1968065146773795
TRACE [GossipTasks:1] 2011-09-30 00:15:12,745 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.579337235438655
TRACE [GossipTasks:1] 2011-09-30 00:15:13,746 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.37217274142982526
TRACE [GossipTasks:1] 2011-09-30 00:15:14,747 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.7443454828596505
TRACE [GossipTasks:1] 2011-09-30 00:15:15,757 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.3555955505071756
TRACE [GossipTasks:1] 2011-09-30 00:15:16,758 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.7083717111193488
TRACE [GossipTasks:1] 2011-09-30 00:15:17,759 FailureDetector.java (line 156) PHI for /10.40.22.186 : 1.061147871731522
TRACE [GossipTasks:1] 2011-09-30 00:15:18,760 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.3194684082936909
TRACE [GossipTasks:1] 2011-09-30 00:15:19,762 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.6395757534039692
TRACE [GossipTasks:1] 2011-09-30 00:15:20,763 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.9593636301059537
TRACE [GossipTasks:1] 2011-09-30 00:15:21,764 FailureDetector.java (line 156) PHI for /10.40.22.186 : 1.2791515068079384
TRACE [GossipTasks:1] 2011-09-30 00:15:22,765 FailureDetector.java (line 156) PHI for /10.40.22.186 : 1.598939383509923
TRACE [GossipTasks:1] 2011-09-30 00:15:23,767 FailureDetector.java (line 156) PHI for /10.40.22.186 : 1.919046728620201
TRACE [GossipTasks:1] 2011-09-30 00:15:24,768 FailureDetector.java (line 156) PHI for /10.40.22.186 : 2.238834605322186
TRACE [GossipTasks:1] 2011-09-30 00:15:25,769 FailureDetector.java (line 156) PHI for /10.40.22.186 : 2.5586224820241705
TRACE [GossipTasks:1] 2011-09-30 00:15:26,771 FailureDetector.java (line 156) PHI for /10.40.22.186 : 2.8787298271344484
TRACE [GossipTasks:1] 2011-09-30 00:15:27,772 FailureDetector.java (line 156) PHI for /10.40.22.186 : 3.198517703836433
TRACE [GossipTasks:1] 2011-09-30 00:15:28,773 FailureDetector.java (line 156) PHI for /10.40.22.186 : 3.518305580538418
TRACE [GossipTasks:1] 2011-09-30 00:15:29,774 FailureDetector.java (line 156) PHI for /10.40.22.186 : 3.838093457240402
TRACE [GossipTasks:1] 2011-09-30 00:15:30,776 FailureDetector.java (line 156) PHI for /10.40.22.186 : 4.158200802350681
TRACE [GossipTasks:1] 2011-09-30 00:15:31,777 FailureDetector.java (line 156) PHI for /10.40.22.186 : 4.4779886790526655
TRACE [GossipTasks:1] 2011-09-30 00:15:32,778 FailureDetector.java (line 156) PHI for /10.40.22.186 : 4.79777655575465
TRACE [GossipTasks:1] 2011-09-30 00:15:33,779 FailureDetector.java (line 156) PHI for /10.40.22.186 : 5.117564432456635
TRACE [GossipTasks:1] 2011-09-30 00:15:34,781 FailureDetector.java (line 156) PHI for /10.40.22.186 : 5.437671777566913
TRACE [GossipTasks:1] 2011-09-30 00:15:35,782 FailureDetector.java (line 156) PHI for /10.40.22.186 : 5.757459654268897
TRACE [GossipTasks:1] 2011-09-30 00:15:36,783 FailureDetector.java (line 156) PHI for /10.40.22.186 : 6.077247530970882
TRACE [GossipTasks:1] 2011-09-30 00:15:37,784 FailureDetector.java (line 156) PHI for /10.40.22.186 : 6.397035407672866
TRACE [GossipTasks:1] 2011-09-30 00:15:38,785 FailureDetector.java (line 156) PHI for /10.40.22.186 : 6.7168232843748505
TRACE [GossipTasks:1] 2011-09-30 00:15:39,786 FailureDetector.java (line 156) PHI for /10.40.22.186 : 7.036611161076836
TRACE [GossipTasks:1] 2011-09-30 00:15:40,788 FailureDetector.java (line 156) PHI for /10.40.22.186 : 7.356718506187114
TRACE [GossipTasks:1] 2011-09-30 00:15:41,789 FailureDetector.java (line 156) PHI for /10.40.22.186 : 7.676506382889099
TRACE [GossipTasks:1] 2011-09-30 00:15:42,790 FailureDetector.java (line 156) PHI for /10.40.22.186 : 7.996294259591083
TRACE [GossipTasks:1] 2011-09-30 00:15:43,791 FailureDetector.java (line 156) PHI for /10.40.22.186 : 8.316082136293067
TRACE [GossipTasks:1] 2011-09-30 00:15:43,792 FailureDetector.java (line 160) notifying listeners that /10.40.22.186 is down
TRACE [GossipTasks:1] 2011-09-30 00:15:43,792 FailureDetector.java (line 161) intervals: 1001.0 2004.0 1011.0 481.0 999.0 1514.0 487.0 1551.0 450.0 1001.0 2002.0 1516.0 2003.0 3012.0 mean: 1359.4285714285713
{noformat}",03/Oct/11 20:23;thepaul;+1,03/Oct/11 20:37;brandon.williams;Committed.,"03/Oct/11 21:20;hudson;Integrated in Cassandra-0.8 #357 (See [https://builds.apache.org/job/Cassandra-0.8/357/])
    Fix bug where the FailureDetector can take a very long time to mark a
host down.
Patch by brandonwilliams, reviewed by Paul Cannon for CASSANDRA-3273

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1178563
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/FailureDetector.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/IFailureDetector.java
* /cassandra/branches/cassandra-1.0.0/CHANGES.txt
* /cassandra/branches/cassandra-1.0.0/src/java/org/apache/cassandra/gms/FailureDetector.java
* /cassandra/branches/cassandra-1.0.0/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-1.0.0/src/java/org/apache/cassandra/gms/IFailureDetector.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractCompactionIterable uses a 1MB buffer for every sstable,CASSANDRA-3171,12522467,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,09/Sep/11 19:47,12/Mar/19 14:16,13/Mar/19 22:26,09/Sep/11 21:20,1.0.0,,,,,,0,,,,,"This causes an OOM for any compaction task that needs to open all the sstables when you have a lot of them (repair, major, etc)",,,,,,,,,,,,,,,,09/Sep/11 20:59;jbellis;3171-v2.txt;https://issues.apache.org/jira/secure/attachment/12493855/3171-v2.txt,09/Sep/11 19:57;brandon.williams;3171.txt;https://issues.apache.org/jira/secure/attachment/12493847/3171.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-09 20:03:34.009,,,no_permission,,,,,,,,,,,,4044,,,Fri Sep 09 21:20:01 UTC 2011,,,,,,0|i0ggo7:,94131,jbellis,jbellis,,,,,,,,,"09/Sep/11 19:57;brandon.williams;Patch to drop buffer size to 64k, as is the RandomAccessReader default.","09/Sep/11 20:03;jbellis;Is there anywhere we actually need non-default buffer sizes?  I think we should probably rip out that constructor parameter (on both RAR and SW) to force everyone to be sane whether they like it or not.

See also: the 10MB buffer in streaming and the 8MB one in IndexWriter.","09/Sep/11 20:59;jbellis;v2 removes unnecessary bufferSize parameters across the call heirarchy.  Main exception is CRAR, to which I added

{code}
// TODO refactor this to separate concept of ""buffer to avoid lots of read() syscalls"" and ""compression buffer""
{code}",09/Sep/11 21:14;brandon.williams;+1,09/Sep/11 21:20;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cassandra-cli use micro second timestamp, but CQL use milli second",CASSANDRA-3227,12523482,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,sabro,sabro,18/Sep/11 15:55,12/Mar/19 14:16,13/Mar/19 22:26,22/Sep/11 03:56,1.0.0,,,Legacy/CQL,,,1,cql,,,,"cassandra-cli set micro second timestamp by FBUtilities.timestampMicros. But CQL insert or update operation set milli second timestamp by AbstractModification.getTimestamp.

If you register data by cassandra-cli, you can't update data by CQL. Because CQL timestamp is judged as past time.",,,,,,,,,,,,,,,,19/Sep/11 02:18;jbellis;3227.txt;https://issues.apache.org/jira/secure/attachment/12495008/3227.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-19 02:18:32.335,,,no_permission,,,,,,,,,,,,4008,,,Thu Sep 22 03:56:13 UTC 2011,,,,,,0|i0ghcv:,94242,sabro,sabro,,,,,,,,,"19/Sep/11 02:18;jbellis;{noformat}
+    - CQL inserts/updates now generate microsecond resolution timestamps
+      by default, instead of millisecond.
{noformat}

Adds ClientState.getTimestamp to go the extra mile:
{noformat}
+    /**
+     * This clock guarantees that updates from a given client will be ordered in the sequence seen,
+     * even if multiple updates happen in the same millisecond.  This can be useful when a client
+     * wants to perform multiple updates to a single column.
+     */
{noformat}
","19/Sep/11 08:45;slebresne;I think this breaks rolling upgrade for anyone using CQL, since during the time where this is a mix a node pre and post this patch, columns will get either micro or milli seconds timestamps depending on the coordinator. ","19/Sep/11 13:56;jbellis;I'm happy to note this in NEWS.  If this is a problem for your workload, you can do a non-rolling restart.","19/Sep/11 15:25;slebresne;Works for me but more because ""we either do it now or never and leaving it to milliseconds would really sucks"" than out of sheer enthusiasm towards the idea.

As for the patch, I'm not too sure about this clientState.getTimestamp(). A clientState is a potentially a long leaving object, it lasts as long as the connection to a given client last. Since the patch only initialize to the current time but then update by +1 on every call disregarding of the actual time, it seems the timestamp will get completely desynchronized quickly. Or am I misunderstanding this ?","19/Sep/11 15:35;jbellis;{code}
+    public long getTimestamp()
+    {
+        long current = System.currentTimeMillis() * 1000;
+        clock = clock >= current ? clock + 1 : current;
+        return clock;
+    }
{code}

I may be missing something obvious, but my intention was to only use the +1 path if it needs to to avoid a conflict.  (And as soon as wall-clock time exceeds the +1 path, it switches back.)","19/Sep/11 15:47;slebresne;bq. I may be missing something obvious,

Nah, but I probably need to see a doctor for that dyslexia.

+1 with the upgrade warning in NEWS","21/Sep/11 21:40;sabro;Thank you. The patch fixed the problem.
""aa-bb"" is registered in unpatched cassandra.
""cc-dd"" is registered in patched cassandra.

{noformat}
[default@QuestWorld] list Globalization;
Using default limit of 100
-------------------
RowKey: 537570706f72746564
=> (column=aa-bb, value=cc, timestamp=1316640431837)
=> (column=cc-dd, value=ee, timestamp=1316640612801000)
=> (column=en-UK, value=en, timestamp=1316356279874416)
=> (column=en-US, value=en, timestamp=1312220986047000)
=> (column=fr-FR, value=fl, timestamp=1316356768334)
=> (column=ja-JP, value=ja, timestamp=1312220978873000)
=> (column=ko-kr, value=no, timestamp=1316356622692000)
=> (column=mm-nn, value=ss, timestamp=1316640580750399)
=> (column=yy-zz, value=xx, timestamp=1316640399778399)
-------------------
RowKey: ffffffffed
=> (column=it-CH, value=it, timestamp=1316356541245000)

2 Rows Returned.
{noformat}",22/Sep/11 03:56;jbellis;(committed),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes started at the same time end up with the same token,CASSANDRA-3219,12523364,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,tjake,tjake,16/Sep/11 17:39,12/Mar/19 14:16,13/Mar/19 22:26,19/Sep/11 16:59,1.0.0,,,,,,0,bootstrap,,,,"Since autoboostrap is defaulted to on when you start a cluster at once (http://screenr.com/5G6) you can end up with nodes being assigned the same token.

{code}
INFO 17:34:55,688 Node /67.23.43.14 is now part of the cluster
 INFO 17:34:55,698 InetAddress /67.23.43.14 is now UP
 INFO 17:34:55,698 Nodes /67.23.43.14 and tjake2/67.23.43.15 have the same token 8823900603000512634329811229926543166.  Ignoring /67.23.43.14
 INFO 17:34:55,698 Node /98.129.220.182 is now part of the cluster
 INFO 17:34:55,698 InetAddress /98.129.220.182 is now UP
 INFO 17:34:55,698 Nodes /98.129.220.182 and tjake2/67.23.43.15 have the same token 8823900603000512634329811229926543166.  Ignoring /98.129.220.182
{code}",,,,,,,,,,,,,,,,16/Sep/11 19:50;jbellis;3219.txt;https://issues.apache.org/jira/secure/attachment/12494854/3219.txt,19/Sep/11 11:46;slebresne;3219_v2.patch;https://issues.apache.org/jira/secure/attachment/12495049/3219_v2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-16 18:45:19.537,,,no_permission,,,,,,,,,,,,4013,,,Mon Sep 19 17:22:04 UTC 2011,,,,,,0|i0gh93:,94225,jbellis,jbellis,,,,,,,,,"16/Sep/11 18:45;jbellis;We should add special values ""auto"" and ""random"" to initial_token, so you can have random with bootstrap and auto-selected w/o.

Of course both of those are not recommended vs picking your own.","16/Sep/11 18:55;vijay2win@yahoo.com;Can we also have some thing like ""equal split"" which will try to split the token ranges into perfect halfs? this will work well for bootstrapping ring of sizes = 2^n","16/Sep/11 19:06;jbellis;That is what ""auto"" does, with the caveat that nodes need to be started 2 minutes apart so they don't race as in Jake's example here.",16/Sep/11 19:50;jbellis;auto and random initial_token modes added.,"19/Sep/11 11:46;slebresne;bq. auto and random initial_token modes added.

I hate it and I'm -1 on that idea.

Basically, I think that it's more complicated to explain/understand how to choose between those two options that it was to explain the ""old"" auto-bootstrap option while it's essentially the same option. The default to random would also make it more likely for people to leave it at that when bootstrapping new nodes, while random is really the worst possible algorithm you can use expect maybe for the 2-3 initial nodes of a cluster (and even then it's really only admissible because the balanced token algorithm don't work in that case and picking random token is the only simple choice we have).

I'd rather add back the auto-bootstrap option than setting the initial_token to random.

As for alternatives, I can propose one of:
  * Decide whether we are really bootstrapping (if we are, balanced token is the ""right"" automatic choice, otherwise we have no other choice than to fall back to random tokens) or not based on whether there is a keyspace defined already. That is the same test we use to decide whether we actually do some bootstrap streaming or not so this doesn't seem too far fetched (attaching a v2 patch for that to make it clear what I mean here).
  * Stop pretending we know how to pick up token automatically and just force user to set a token. We can default that token to 0 so that you can start a single node cluster with 0 configuration and we can ship a new small tiny script that compute the tokens for a n node initial cluster if we want people to be able to do without a calculator.
","19/Sep/11 13:53;jbellis;bq. I think that it's more complicated to explain/understand how to choose between those two options

Huh?  This is a HUGE simplification because initial_token behavior depends only on initial_token.  The old behavior (where initial_token=empty behavior does one thing with auto_bootstrap=true, and another with a_b=false) was ENORMOUSLY confusing: EVERY training class I taught was baffled by this.

Further, the old behavior doesn't let you specify bootstrap/random or nobootstrap/auto, both of which are valid things to do.

bq. based on whether there is a keyspace defined already

I don't see how this helps the situation Jake describes.

bq. just force user to set a token

This is a non-starter.","19/Sep/11 15:00;slebresne;
bq. Further, the old behavior doesn't let you specify bootstrap/random

That would depend on what is definition of valid. In my opinion, picking random token is *always* stupid, it will always result in crappy distribution (it just happens to be less stupid than ""auto"" in the noboostrap case, which imho is and should remain the only reason we ever generate random token). If you bootstrap, it means you have an existing cluster with data in it (I'm not saying you *have to*, I'm saying this is why bootstrap is for and so should be the case if you don't do something wrong). In such situation, I don't see why you would want to pick a random token. If some people like to live on the edge, they can write a random token generator and use that, but that we would want to expose an option, hence suggesting that this could be something useful ...

bq. or nobootstrap/auto, both of which are valid things to do

Again, really depends on the definition of ""valid"". First, if you start two nodes at the same time with that, you end up in this ticket situation. Sure the patch adds a ""don't do it"" comment but it doesn't really fix it more than that. Second, noboostrap (when token selection is involved, i.e, not a replace_token) is mainly useful to set up an initial cluster, that is when nodes don't have data at all (otherwise you want to bootstrap the node). In that situation, auto will likely don't do anything useful (it's a completely degenerated case for the algorithm). That the nobootstrap/auto pair doesn't work correctly is actually the only reason I can come up for us picking a random token in <= 0.8 versions.

Besides, when was the last time we had a user requesting to do one of boostrap/random or nobootstrap/auto, or us recommanding anything else than 'pick your token yourself'?

bq. I don't see how this helps the situation Jake describes

I'm willing to bet that when Jake encountered that problem he was trying to set up an initial cluster *before* having set up schemas and inserted some data. In that case, the second patch would pick a random token so there wouldn't be problem.

The thing is, there is not too many way to create a Cassandra cluster. First you create a cluster with n initial machines. For that you want to be in mode noboostrap/random (noboostrap/auto doesn't really work too well with no data; and by noboostrap I don't really speak of the auto-boostrap=false option, but more of not doing data streaming). Once you have data in the cluster, you want to bootstrap and then auto is always less stupid than random (IMHO). Hence the rational for the v2 patch.

bq. This is a non-starter

I find it weird to consider that a non-starter so rapidly when we all know that the very first advise we give is to hand pick token and that it's unreasonable to use auto (let's not talk about random) token in any real life situation (even the config file basically says it). But I'm willing to consider that it's not the right time to discuss that and to discard that solution, at least for now.
","19/Sep/11 15:11;jbellis;bq. when Jake encountered that problem he was trying to set up an initial cluster before having set up schemas and inserted some data

Maybe, but it sounded to me like the situation was ""I was adding new nodes to an existing cluster, and they picked the same token.""  Which as I pointed out in chat is NOT a new problem, but it's one we should address.

Another way of looking at my patch is, it's okay for defaults to give you something suboptimal (random tokens) but it's not okay for it to give you something broken (two nodes w/ same token).  If you want auto token picking and its potential downsides, you need to opt in.  (And hopefully read the comments and go with manual token assignment instead.)

bq. I find it weird to consider that a non-starter so rapidly

Because demo-ability matters.","19/Sep/11 15:14;vijay2win@yahoo.com;IMO... Instead of random we should actually try and balance the tokens when no keyspace is defined... By which I mean moving the nodes around as there is no data to stream and at that time it will be more predictive... This will give a better distribution...


","19/Sep/11 15:30;jbellis;Sure, in magic fairy land I'd love that too, but the question here is what can we improve for 1.0.","19/Sep/11 16:14;slebresne;bq. Which as I pointed out in chat is NOT a new problem, but it's one we should address.

Agreed, but I suspect this is due (in the not a new problem case) to races in Boostrapper.getBootstrapSource() detection of already bootstrapping node. We should fix that if possible, which the patch don't really since if you will still potentially have those race with auto. But note that this problem is present in 0.8 and I think is not a top priority because it's relatively rare to actually start bootstrapping 2 nodes at the same time in real life. Again, I'm not saying we shouldn't fix, but it's ok to say that as long as it's not worth than in 0.8, it can wait post 1.0.0 to get fixed.

Now there is a actual new problem with 1.0.0. That problem is that when you start an initial cluster, i.e, when in 0.8 you would start node with auto-boostrap=false, you do often end up starting nodes simultaneously. That is why older version were using random token when auto-bootstrap was false. This problem does need to be fix for 1.0.0 because that is a serious regression. However, my argument is that even though we now default to auto-boostrap=true, that doesn't mean that there is no difference between setting up the initial nodes of a cluster and the latter bootstrapping of nodes to add capacity to an existing cluster. Indeed, in 1.0.0 we decided to draw this line based on whether a schema had been created or not (we call the bootstrap() method based on that). Imho, this means that we have no boostrap option and the ""I have no schema"" is the old auto-boostrap=false. So we should use random token in that case and balanced one otherwise the same way we are doing it in 0.8.

And I'm saying that I would prefer we do that and report the fixing of Boostrapper.getBootstrapSource() rather than exposing (and making the default) the random choice of tokens, which is my opinion is a bad idea.","19/Sep/11 16:59;jbellis;I'm still not convinced this is intuitive behavior, but it's no more broken than what we've lived with for the last couple years, so I'll run with that on the grounds of ""we're already in freeze, we shouldn't mess with things when we can help it.""

rebased v2 + committed",19/Sep/11 17:22;jbellis;v2 actually breaks things because getNewToken doesn't repeat the test for seed-ness.  I reverted things and went with a simpler change to accomplish the same goal in r1172717.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeveledCompactionStrategy is too complacent,CASSANDRA-3224,12523442,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,17/Sep/11 18:10,12/Mar/19 14:16,13/Mar/19 22:26,20/Sep/11 13:13,1.0.0,,,,,,0,compaction,,,,"As the title says, it barely does anything.  I inserted 50G worth of data with 1G heap and 99% overwrite ratio, and it only compacted twice:

{noformat}
 INFO [CompactionExecutor:1] 2011-09-16 22:29:54,572 CompactionTask.java (line 118) Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-1-Data.db')]
 INFO [CompactionExecutor:1] 2011-09-16 22:29:58,606 CompactionTask.java (line 220) Compacted to [/var/lib/cassandra/data/Keyspace1/Standard1-h-2-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-4-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-5-Data.db,].  12,595,811 to 12,595,811 (~100% of original) bytes for 40,501 keys at 3.058122MBPS.  Time: 3,928ms.
 INFO [CompactionExecutor:1] 2011-09-16 22:29:58,607 CompactionTask.java (line 222) CF Total Bytes Compacted: 12,595,811
 INFO [CompactionExecutor:3] 2011-09-16 22:29:58,889 CompactionTask.java (line 118) Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-4-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-5-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-3-Data.db')]
 INFO [CompactionExecutor:3] 2011-09-16 22:30:06,900 CompactionTask.java (line 220) Compacted to [/var/lib/cassandra/data/Keyspace1/Standard1-h-7-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-9-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-11-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-12-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-14-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-15-Data.db,].  28,374,396 to 28,374,396 (~100% of original) bytes for 91,236 keys at 3.380379MBPS.  Time: 8,005ms.
 INFO [CompactionExecutor:3] 2011-09-16 22:30:06,901 CompactionTask.java (line 222) CF Total Bytes Compacted: 40,970,207
{noformat}

Resulting in the following levels:

{noformat}
L0: 4965
L1: 6
L2: 0
L3: 0
L4: 0
L5: 0
L6: 0
L7: 0
{noformat}

This is obviously going to result in extremely poor read performance.",,,,,,,,,,,,,,,,19/Sep/11 18:02;jbellis;3224-v2.txt;https://issues.apache.org/jira/secure/attachment/12495117/3224-v2.txt,19/Sep/11 19:34;jbellis;3224-v3.txt;https://issues.apache.org/jira/secure/attachment/12495131/3224-v3.txt,19/Sep/11 20:25;jbellis;3224-v4.txt;https://issues.apache.org/jira/secure/attachment/12495137/3224-v4.txt,19/Sep/11 23:07;jbellis;3224-v5.txt;https://issues.apache.org/jira/secure/attachment/12495159/3224-v5.txt,20/Sep/11 02:43;jbellis;3224-v6.txt;https://issues.apache.org/jira/secure/attachment/12495186/3224-v6.txt,18/Sep/11 03:08;jbellis;3224.txt;https://issues.apache.org/jira/secure/attachment/12494955/3224.txt,19/Sep/11 19:23;brandon.williams;system.log.bz2;https://issues.apache.org/jira/secure/attachment/12495127/system.log.bz2,,,,,7.0,,,,,,,,,,,,,,,,,,,2011-09-18 03:08:20.224,,,no_permission,,,,,,,,,,,,4009,,,Tue Sep 20 13:13:11 UTC 2011,,,,,,0|i0ghbj:,94236,brandon.williams,brandon.williams,,,,,,,,,"18/Sep/11 03:08;jbellis;This is a regression caused by CASSANDRA-3181.

The problem is that when CompactionTask calls submitBackground, the original Task is not yet technically done, so LeveledCompactionStrategy returns ""nothing to do"" to prevent multiple tasks running in parallel, which is not supported for LCS.  So after the first compaction runs five minutes in, that's all she wrote.

Fixing the Task/Executor mess is out of scope for 1.0 (created CASSANDRA-3225 to address in 1.1) s","19/Sep/11 17:43;brandon.williams;Better, but it looks like LCS starts miscounting at some point, and this causes compactions to stop:

{noformat}

DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,359 LeveledManifest.java (line 218) Level 0 contains 85 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,360 LeveledManifest.java (line 218) Level 1 contains 0 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,360 LeveledManifest.java (line 218) Level 2 contains 14 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,361 LeveledManifest.java (line 218) Level 3 contains 0 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,361 LeveledManifest.java (line 218) Level 4 contains 0 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,361 LeveledManifest.java (line 218) Level 5 contains 0 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,361 LeveledManifest.java (line 218) Level 6 contains 0 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,361 LeveledManifest.java (line 218) Level 7 contains 0 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,361 LeveledManifest.java (line 197) Compaction score for level 0 is 0.0
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,362 LeveledManifest.java (line 197) Compaction score for level 2 is 0.0
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,362 LeveledCompactionStrategy.java (line 112) CompactionManager candidates are 
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,362 LeveledManifest.java (line 218) Level 0 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,362 LeveledManifest.java (line 218) Level 1 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,362 LeveledManifest.java (line 218) Level 2 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,362 LeveledManifest.java (line 218) Level 3 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 4 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 5 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 6 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 7 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledCompactionStrategy.java (line 112) CompactionManager candidates are 
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 0 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 1 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 2 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,364 LeveledManifest.java (line 218) Level 3 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,364 LeveledManifest.java (line 218) Level 4 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,364 LeveledManifest.java (line 218) Level 5 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,364 LeveledManifest.java (line 218) Level 6 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,364 LeveledManifest.java (line 218) Level 7 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,364 LeveledCompactionStrategy.java (line 112) CompactionManager candidates are 
{noformat}

Actual levels are (according to json manifest):
{noformat}
L0: 85
L1: 0
L2: 14
L3: 0
L4: 0
L5: 0
L6: 0
L7: 0
{noformat}

Attempting to force a compaction via nodetool:
{noformat}

 INFO 17:42:28,933 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db'), SSTableReader(path='/var/lib
/cassandra/data/system/LocationInfo-h-1-Data.db')]
DEBUG 17:42:28,933 Expected bloom filter size : 512
 INFO 17:42:28,956 Compacted to [/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db,].  498 to 447 (~89% of original) bytes for 3 key
s at 0.019377MBPS.  Time: 22ms.
 INFO 17:42:28,956 CF Total Bytes Compacted: 225,632,757
 INFO 17:42:28,956 Nothing to compact in Migrations.Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO 17:42:28,957 Nothing to compact in Schema.Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
{noformat}",19/Sep/11 18:02;jbellis;v2 attached w/ one-line fix to score computation,"19/Sep/11 19:23;brandon.williams;Somehow, I ended up in the following situation, even though my workload is 100% unique and some compactions did succeed:

{noformat}
L0: 974
L1: 0
L2: 0
L3: 0
L4: 0
L5: 0
L6: 0
L7: 0
{noformat}

The box then died from OOM while trying to write the BF.  System log with db.compaction at DEBUG attached.",19/Sep/11 19:34;jbellis;v3 improves some logging and adds an assert,"19/Sep/11 20:25;jbellis;v4 adds more debug logging, which makes clear that the empty manifests being logged were from other CFS.

caps L0 compaction candidates at 32 to avoid OOMing from pessimisting BF sizing.

picks L0 initial victim based on age of data rather than randomly.","19/Sep/11 23:07;jbellis;v5 attached.  changes max-from-L0 to 10, and fixes manifest loading on restart.","20/Sep/11 02:43;jbellis;v6 changes scoring of levels to prioritize higher levels, reducing the duplicate work that gets done when lower levels are done in turn.

also logs what level each compacted sstable comes from.",20/Sep/11 12:43;brandon.williams;+1,20/Sep/11 13:13;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deflate Compression corrupts SSTables,CASSANDRA-3370,12527375,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,christianmovi,christianmovi,16/Oct/11 23:04,12/Mar/19 14:16,13/Mar/19 22:26,20/Oct/11 13:35,1.0.1,,,,,,0,compression,,,,"Hi,

it seems that the Deflate Compressor corrupts the SSTables. 3 out of 3 Installations were corrupt. Snappy works fine.

Here is what I did:

1. Start a single cassandra node (I was using ByteOrderedPartitioner)
2. Write data into cf that uses deflate compression - I think it has to be enough data so that the data folder contains some files.
3. When I now try to read (I did a range scan) from my application, it fails and the logs show corruptions:

Caused by: org.apache.cassandra.io.compress.CorruptedBlockException: (/home/cspriegel/Development/cassandra1/data/Test/Response-h-2-Data.db): corruption detected, chunk at 0 of length 65536.

regards,
Christian","Ubuntu Linux, amd64, Cassandra 1.0.0-rc2",,,,,,,,,,,,,,,20/Oct/11 10:57;slebresne;3370.patch;https://issues.apache.org/jira/secure/attachment/12499845/3370.patch,19/Oct/11 21:33;christianmovi;Test.zip;https://issues.apache.org/jira/secure/attachment/12499765/Test.zip,16/Oct/11 23:06;christianmovi;system.log;https://issues.apache.org/jira/secure/attachment/12499227/system.log,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-10-18 15:15:00.364,,,no_permission,,,,,,,,,,,,87538,,,Thu Oct 20 13:35:22 UTC 2011,,,,,,0|i0gj27:,94518,xedin,xedin,,,,,,,,,16/Oct/11 23:06;christianmovi;Attached system.log of broken installation,"18/Oct/11 15:15;slebresne;The exception is raised because there is a digest mismatch for the initial block of one of the sstable.

Haven't been able to reproduce so far using stress with deflate with the default 1M keys (which create a bunch of sstables, at least on my machine) using row slices and range scans (using both Random and ByteOrdered partitioners).

Would you be able to 1) try with 1.0.0 and 2) try with the stress tool that comes with Cassandra (it's in tools/stress of the source distribution, and you'll want to insert values with 'stress -I DeflateCompressor' and read with 'stress -I DeflateCompressor -o RANGE_SLICE' ) and see if you can reproduce? Another question is, did you used openJDK or Sun JDK?","18/Oct/11 15:21;christianmovi;My Java version is: 
java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)

Cassandra report during startup:
INFO 17:20:39,113 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26

I will test tonight ...","18/Oct/11 20:33;christianmovi;I tested again with 1.0.0. Unfortunetaly the problem still exists.

But I think I was able to narrow it down: It seems that the problem only occurs when I insert large byte-arrays.

It seems to work fine with 10kb arrays, no problem there. I was able to repeatedly insert and read.

With 100kb or 200kb arrays it crashes after about 1000-2000 insertions. (Insertions work, but range scan afterwards crashes)




",18/Oct/11 20:42;christianmovi;btw: I just tested the DeflaterOutputStream/DeflaterInputStream classes in a small testcase and there it works fine. I thought maybe Deflate in my jvm is broken.,"19/Oct/11 12:48;slebresne;Still cannot reproduce. I've tried multiple times inserting 5000 keys using the stress tool using values of 10KB, 20KB, 100KB and 200KB (using 'stress -I DeflateCompressor -S 200000 -n 5000'). I then try to reading both with 'stress -o RANGE_SLICE -n 5000' and by simply fetching the 100 first keys using the CLI (with a simple 'list Standard1;') and got no exceptions (the actual listing in the CLI took a while to be printed on screen because the columns are big but outside of that, no errors).

Would you mind trying the same experiment (with the same tools) or providing the test script you're using so we can check if it has to do with the specific insertions or with something in your environment.
","19/Oct/11 15:18;christianmovi;Ok, I will try the stress tool.

Just to be sure: You are talking about stress.py and not the java-based stress? Because I was trying the java stress and it did not accept the -I parameter.
","19/Oct/11 15:23;slebresne;No, I'm talking of the java one. The python one is old and won't support compression for instance. The java one in the 1.0.0 source does support compression through the -I parameter. Are you sure you're looking at the right version ? ","19/Oct/11 15:33;christianmovi;I see! I got the wrong version. Sorry, I did not know that it was included in the cassandra source. I thought I had to download it some place else.

I will try with that and let you know about the results...","19/Oct/11 20:27;christianmovi;I tried using stress from 1.0.0 and I got the same results as you. Stress for some reason works fine. 

One thing is strange about stress:
I let stress run for quite some time, but there is only 12 MB in the datafolder.
I let my tool run for 10 seconds, but there are 97MB in the data folder.

Is stress maybe not generating random data, so that it compresses really well? Might that be the difference?

Can I maybe share my application with you? Its a single source file with a pom.xml. 

If you have any idea what I can do, please let me know.

",19/Oct/11 21:33;christianmovi;Attached my client that causes the crash.,"20/Oct/11 10:57;slebresne;You test did help. Turns out that's because you're inserting random and thus basically uncompressible data, and the compressed data was bigger than the uncompressed one. The code is supposed to handle that but there is a bug in that part.

Patch attached to fix.",20/Oct/11 11:02;christianmovi;Great! This would also apply if some app would insert already compressed data.,20/Oct/11 13:20;xedin;+1,20/Oct/11 13:35;slebresne;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cqlsh: Error running ""select *"" vs ""select all columns""",CASSANDRA-3311,12525734,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,cdaw,cdaw,04/Oct/11 21:08,12/Mar/19 14:16,13/Mar/19 22:26,05/Oct/11 00:36,1.0.0,,,Legacy/CQL,,,0,,,,,"* Install cql 1.0.5 from [http://code.google.com/a/apache-extras.org/p/cassandra-dbapi2/downloads/detail?name=cql-1.0.5.tar.gz&can=2&q=]

* Query using ""select *""
{code}
cqlsh> select * from users;
Exception: 'utf8' codec can't decode byte 0xb4 in position 7: unexpected code byte
{code}

* Query selecting all columns
{code}
 select KEY, password, gender, session_token, state, birth_year from users;
   KEY |  password | gender | session_token | state | birth_year |
 user1 | ch@ngem3a |      f |          None |    TX |       1968 |
{code}

* Test Data
{code}
CREATE KEYSPACE ks1 with 
  strategy_class =  
    'org.apache.cassandra.locator.SimpleStrategy' 
  and strategy_options:replication_factor=1;
  
use ks1;

CREATE COLUMNFAMILY users (
  KEY varchar PRIMARY KEY, password varchar, gender varchar,
  session_token varchar, state varchar, birth_year bigint);
  
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3a', 'f', 'TX', '1968');    
{code}","Server: BDP/main
CQLSH: 1.0.5",,,,,,,,,,,,,,,04/Oct/11 21:50;xedin;CASSANDRA-3311.patch;https://issues.apache.org/jira/secure/attachment/12497713/CASSANDRA-3311.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-04 21:16:05.077,,,no_permission,,,,,,,,,,,,45384,,,Wed Oct 05 00:36:28 UTC 2011,,,,,,0|i0gidb:,94406,jbellis,jbellis,,,,,,,,,"04/Oct/11 21:16;jbellis;may be a bug in the schema part of the resultset.  can you take a look, Pavel?",05/Oct/11 00:36;jbellis;inlined declaration of cD with assignment + committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support TimeUUID in CassandraStorage,CASSANDRA-3327,12526104,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,140am,140am,06/Oct/11 19:41,12/Mar/19 14:16,13/Mar/19 22:26,19/Dec/11 20:27,0.8.10,1.0.7,,,,,1,pig,,,,"Cassandra CLI:

{code}
grunt> raw = LOAD 'cassandra://TEST/CF'
>>     USING CassandraStorage()
>>     AS (
>>         key:chararray,
>>         columns:bag {
>>             column:tuple(
>>                 name,
>>                 value
>>             )
>>         });

grunt> describe raw;
raw: {key: chararray,columns: {(name: bytearray,value: bytearray)}}

log_test =
    FOREACH raw
    GENERATE
        (CHARARRAY) key,
        flatten(columns);

grunt> DUMP log_test;
{code}

Returns:

{code}
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias log_test. Backend error : Unexpected data type java.util.UUID found in stream. Note only standard Pig type is supported when you output from UDF/LoadFunc
        at org.apache.pig.PigServer.openIterator(PigServer.java:890)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:655)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:303)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:67)
        at org.apache.pig.Main.run(Main.java:487)
        at org.apache.pig.Main.main(Main.java:108)
Caused by: java.lang.RuntimeException: Unexpected data type java.util.UUID found in stream. Note only standard Pig type is supported when you output from UDF/LoadFunc
        at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:478)
        at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:542)
        at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
        at org.apache.pig.impl.io.InterRecordWriter.write(InterRecordWriter.java:73)
        at org.apache.pig.impl.io.InterStorage.putNext(InterStorage.java:87)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:138)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:97)
        at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:498)
        at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.collect(PigMapOnly.java:48)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:263)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:256)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:58)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
{code}

According to driftx on IRC the setTupleValue function in CassandraStorage needs to handle the uuid case and cast it to a DataByteArray.",Cassandra 0.8.6 Build #348 (CASSANDRA-2777 + CASSANDRA-2810),,,,,,,,,,,,,,,19/Oct/11 18:28;brandon.williams;3327-v2.txt;https://issues.apache.org/jira/secure/attachment/12499727/3327-v2.txt,14/Oct/11 22:50;brandon.williams;3327.txt;https://issues.apache.org/jira/secure/attachment/12499107/3327.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-14 22:50:52.569,,,no_permission,,,,,,,,,,,,49685,,,Mon Dec 19 21:49:52 UTC 2011,,,,,,0|i0gik7:,94437,rbranson,rbranson,,,,,,,,,14/Oct/11 22:50;brandon.williams;Patch to do what that driftx guy suggested.,19/Oct/11 18:28;brandon.williams;v2 uses UUIDGen to decompose to a byte array.,"19/Dec/11 20:08;rbranson;Reviewed this, I am +1",19/Dec/11 20:27;brandon.williams;Committed.,"19/Dec/11 21:49;hudson;Integrated in Cassandra-0.8 #420 (See [https://builds.apache.org/job/Cassandra-0.8/420/])
    TimeUUID support in CassandraStorage.
Patch by brandonwilliams, reviewed by Rick Branson for CASSANDRA-3327

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1220926
Files : 
* /cassandra/branches/cassandra-0.8/contrib/pig/src/java/org/apache/cassandra/hadoop/pig/CassandraStorage.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache Daemon missing from the binary tarball,CASSANDRA-3331,12526225,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bcoverston,bcoverston,bcoverston,07/Oct/11 15:25,12/Mar/19 14:16,13/Mar/19 22:26,27/Oct/11 17:27,1.0.1,,,,,,0,windows,,,,"Apparently the tools used to run the binary release are missing from the binary tarball.

I will verify that they are in the 1.0 branch, then update the ticket so we can ensure that they are included.

Ben",,,,,,,,,,,,,,,,07/Oct/11 18:04;bcoverston;0001-add-files-to-src-and-bin-add-uninstall.patch;https://issues.apache.org/jira/secure/attachment/12498202/0001-add-files-to-src-and-bin-add-uninstall.patch,07/Oct/11 18:16;bcoverston;0002-adding-daemon-directory.patch;https://issues.apache.org/jira/secure/attachment/12498208/0002-adding-daemon-directory.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-07 19:02:51.969,,,no_permission,,,,,,,,,,,,50311,,,Thu Oct 27 16:54:51 UTC 2011,,,,,,0|i0gilj:,94443,jbellis,jbellis,,,,,,,,,07/Oct/11 18:04;bcoverston;Added INSTALL option to the batch file. Also included the bin/daemon directory along with the files and licenses.,07/Oct/11 19:02;jbellis;committed with some minor fixes,"13/Oct/11 22:18;urandom;I'd like to respectfully suggest that we _don't_ do this.

I know that we're aiming to run out-of-the-box wherever possible, but there has to be a line between software we supply, and what is expected to already be there.  I think this crosses that line, in the same way that jsvc (for unix hosts), or even the JVM itself, would.","13/Oct/11 22:33;jbellis;I'm fine with reverting this.  (IIRC though there were some quoting fixes to the .bat file, as well as the UNINSTALL option, so it should be a little more surgical than just doing a svn reverse-merge.)","14/Oct/11 07:41;slebresne;I'm also fine with reverting this, but I'd add a few remarks:
  # if we do revert, it would be nice to ""fix"" the .bat file so that it don't look for it in our specific repository that we would not ship (i.e, do a little bit more than reverting because the .bat before this patch was clearly suggesting we intended to ship it but did not). Ideally we would just expect it to be installed in whatever is supposed to be standard for windows and offer an helpful message if it is not found (since we can't do what we do for the debian package, i.e make it a dependency of the package)
  # it's been committed to 1.0.0 and I don't really want to re-roll the vote for that. That would mean we ship procrun for 1.0.0 but then never after that. A little weird imho.","14/Oct/11 12:50;jbellis;bq. Ideally we would just expect it to be installed in whatever is supposed to be standard for windows 

Is there a standard?  It just comes as a .zip, not a proper installer.

bq. That would mean we ship procrun for 1.0.0 but then never after that

The alternative is to keep it for all of 1.0 which will set the expectation that we'll keep doing it...  I'd rather bite the bullet early.  Everyone waits for 1.0.1 anyway right? :)",27/Oct/11 16:54;jbellis;removed and updated README,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE on malformed CQL,CASSANDRA-3349,12526757,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,thepaul,thepaul,11/Oct/11 21:17,12/Mar/19 14:16,13/Mar/19 22:26,12/Oct/11 14:53,0.8.8,1.0.1,,,,,0,lhf,,,,"It's not clear why, but the CQL grammar specification in Cql.g allows for an empty WHERE clause on DELETE, i.e.:

{noformat}
DELETE FROM someCF WHERE;
{noformat}

When this is used, with or without a column list, it causes an NPE on the node processing the CQL. Traceback on a recent 1.0.0 build:

{noformat}
ERROR [pool-2-thread-1] 2011-10-11 15:45:25,655 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.NullPointerException
        at org.apache.cassandra.cql.CqlParser.deleteStatement(CqlParser.java:1994)
        at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:292)
        at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:984)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:500)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1268)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{noformat}

The CQL client gets an error with the message, ""Internal application error"".

It might be better to allow leaving off the ""WHERE"" as well as the condition, to match SQL semantics, although fixing that probably won't solve this problem.",,,,,,,,,,,,,,,,12/Oct/11 10:24;xedin;CASSANDRA-3349.patch;https://issues.apache.org/jira/secure/attachment/12498727/CASSANDRA-3349.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-12 01:50:51.254,,,no_permission,,,,,,,,,,,,59409,,,Wed Oct 12 15:22:45 UTC 2011,,,,,,0|i0gitb:,94478,jbellis,jbellis,,,,,,,,,"12/Oct/11 01:50;jbellis;We don't want to allow leaving it off, because we don't support scan-and-delete server-side. (And truncate is far more performant.)

I'd rather fix by requiring the rest of the WHERE.",12/Oct/11 10:25;xedin;rebased with the latest 0.8 branch (last commit 9b9c4d32973ea4e586031775b3322180169135cd),12/Oct/11 13:07;jbellis;+1,12/Oct/11 14:53;jbellis;committed,"12/Oct/11 15:22;hudson;Integrated in Cassandra-0.8 #367 (See [https://builds.apache.org/job/Cassandra-0.8/367/])
    update CQL grammar to require key clause in delete statement
patch by pyaskevich; reviewed by jbellis for CASSANDRA-3349

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1182411
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/Cql.g
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli allows setting min_compaction_threshold to 1,CASSANDRA-3342,12526542,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,amnorvend,amnorvend,11/Oct/11 01:01,12/Mar/19 14:16,13/Mar/19 22:26,14/Oct/11 21:38,0.8.8,1.0.1,,Legacy/Tools,,,0,,,,,"{{
[root@Apture] update column family MagicLinks with min_compaction_threshold=1 and max_compaction_threshold=20;
b98e3b80-f3a3-11e0-0000-76abb4a6dbbf
Waiting for schema agreement...
... schemas agree across the cluster
}}

I'm told that a min_compaction_threshold of 1 is nonsensical.  I had a spell where my servers stopped doing compactions.  Once I upped the min_compaction_threshold, they started compacting again.  I'm unable to confirm for sure that this was the case.","Linux 2.6.32-131.6.1.el6.x86_64 #1 SMP Mon Jun 20 14:15:38 EDT 2011 x86_64 x86_64 x86_64 GNU/Linux (RHEL 6)
",,,,,,,,,,,,,,,14/Oct/11 21:23;xedin;CASSANDRA-3342.patch;https://issues.apache.org/jira/secure/attachment/12499100/CASSANDRA-3342.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-11 01:05:16.26,,,no_permission,,,,,,,,,,,,57014,,,Fri Oct 14 22:15:27 UTC 2011,,,,,,0|i0giqf:,94465,brandon.williams,brandon.williams,,,,,,,,,11/Oct/11 01:05;brandon.williams;ISTM the easiest thing to do here is actually use 2 when a user specifies 1.,"14/Oct/11 02:22;jbellis;I'd rather raise an error server-side, than silently not do what was requested",14/Oct/11 21:26;brandon.williams;+1 with minor typo fixes: s/then/than/ and s/greather/greater/,14/Oct/11 21:38;xedin;Committed.,"14/Oct/11 22:15;hudson;Integrated in Cassandra-0.8 #374 (See [https://builds.apache.org/job/Cassandra-0.8/374/])
    ColumnFamily min_compaction_threshold should be >= 2
patch by Pavel Yaskevich; reviewed by Brandon Williams for CASSANDRA-3342

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1183510
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed streaming may cause duplicate SSTable reference,CASSANDRA-3306,12525677,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,hsn,hsn,04/Oct/11 14:39,12/Mar/19 14:16,13/Mar/19 22:26,31/Oct/12 16:13,1.1.7,1.2.0 beta 2,,,,,1,,,,,"during stress testing, i always get this error making leveledcompaction strategy unusable. Should be easy to reproduce - just write fast.

ERROR [CompactionExecutor:6] 2011-10-04 15:48:52,179 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[CompactionExecutor:6,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.DataTracker$View.newSSTables(DataTracker.java:580)
	at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:546)
	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:268)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:232)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:960)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:199)
	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:47)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:131)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:114)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

and this is in json data for table:

{
  ""generations"" : [ {
    ""generation"" : 0,
    ""members"" : [ 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484 ]
  }, {
    ""generation"" : 1,
    ""members"" : [ ]
  }, {
    ""generation"" : 2,
    ""members"" : [ ]
  }, {
    ""generation"" : 3,
    ""members"" : [ ]
  }, {
    ""generation"" : 4,
    ""members"" : [ ]
  }, {
    ""generation"" : 5,
    ""members"" : [ ]
  }, {
    ""generation"" : 6,
    ""members"" : [ ]
  }, {
    ""generation"" : 7,
    ""members"" : [ ]
  } ]
}",,,,,,,,,,,,,,,,23/Oct/12 22:45;yukim;0001-CASSANDRA-3306-test.patch;https://issues.apache.org/jira/secure/attachment/12550547/0001-CASSANDRA-3306-test.patch,30/Oct/12 22:34;yukim;0001-change-DataTracker.View-s-sstables-from-List-to-Set.patch;https://issues.apache.org/jira/secure/attachment/12551435/0001-change-DataTracker.View-s-sstables-from-List-to-Set.patch,30/Oct/12 22:34;yukim;0002-fail-stream-session-for-invalid-request.patch;https://issues.apache.org/jira/secure/attachment/12551436/0002-fail-stream-session-for-invalid-request.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-10-04 15:27:07.848,,,no_permission,,,,,,,,,,,,44375,,,Wed Mar 11 20:32:13 UTC 2015,,,,,,0|i0aym7:,61881,slebresne,slebresne,,,,,,,,,"04/Oct/11 15:18;hsn;another problem. why not store data in some system CF? would be probably safer choice.

ERROR [CompactionExecutor:5] 2011-10-04 17:13:13,922 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[CompactionExecutor:5,5,main]
java.io.IOError: java.io.IOException: Failed to rename \var\lib\cassandra\data\test\sipdb.json to \var\lib\cassandra\data\test\sipdb-old.json
	at org.apache.cassandra.db.compaction.LeveledManifest.serialize(LeveledManifest.java:382)
	at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:182)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:152)
	at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:466)
	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:275)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:232)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:960)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:199)
	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:47)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:131)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:114)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Failed to rename \var\lib\cassandra\data\test\sipdb.json to \var\lib\cassandra\data\test\sipdb-old.json
	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:64)
	at org.apache.cassandra.db.compaction.LeveledManifest.serialize(LeveledManifest.java:375)
	... 15 more
","04/Oct/11 15:27;jbellis;Because then you get into hairy cyclical situations where you can't read the manifest until you replay the commitlog, but replaying the commitlog requires writing new sstables and thus knowing the manifest","04/Oct/11 15:52;hsn;as i understand new flushed tables are placed at level 0. Just replay commitlog and put all new stuff in lvl 0. after comitlog is done, it can do voodoo shuffles.

but why not to rename tables like table-h-333-l1-Data.db?

idea to have stables with non overlapping key ranges is interesting, but read performance is kinda slow (about 50% of normal) here. Its cassandra core modified to get advantage of leveled tables? i.e. search one sstable at level1, one at lvl2 using bloom filters for key?",04/Oct/11 16:01;jbellis;This isn't really a great place to rehash http://leveldb.googlecode.com/svn/trunk/doc/impl.html and CASSANDRA-1608.,"04/Oct/11 16:04;brandon.williams;bq. why not store data in some system CF? would be probably safer choice.

This has historically been a bad idea, see CASSANDRA-1155, then CASSANDRA-1318 and finally CASSANDRA-1430.","24/Oct/11 14:25;slebresne;I don't suppose you were using column family truncation in your tests, where you? ","24/Oct/11 16:42;hsn;no truncation, no supercolumns.","24/Oct/11 16:45;slebresne;Are you still able to reproduce reliably? Because we aren't and being able to would help considerably, so if you are and could share whatever script you're using to reproduce, that would be awesome.",25/Oct/11 06:33;hsn;i tested it on 1.0 final and it worked without error for 1 test run. i will give it another test without index.,"26/Oct/11 16:14;slebresne;I'll note that Ramesh Natarajan reported on the mailing list what clearly appears to be the same bug (http://www.mail-archive.com/user@cassandra.apache.org/msg18146.html), but while not using leveled compaction. I also think he was using the 1.0.0 final.","04/Nov/11 18:32;slebresne;I'll note that more info have been added to the messages thrown by the exception here in 1.0.1. So if someone can reproduce this issue on 1.0.1, it would be useful to get the stacktrace (the full system.log would actually be even better).","24/Nov/11 02:16;jonma;This　AssertionError happened always in cassandra1.0.0 ,not just only in LeveledCompactionStrategy",24/Nov/11 02:24;jonma;I suppose it's a bug in DataTracker .,"24/Nov/11 08:10;slebresne;As I already said, if you are able to reproduce this, please try reproducing with 1.0.3. And if you are still able to, please attach you system.log with the exception here because it will have more info on the error that should help. And if you're not able to reproduce with 1.0.3, then I guess it means we've fixed it without knowing.","01/Dec/11 19:42;joelastpass;Running under 1.0.4, can easily reproduce this by just kicking of a repair of any LeveledCompactionStrategy CF.  

The 'zero' on the assert indicates the value (added that to the code to see what the value was): 

java.lang.AssertionError: 0
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:178)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:481)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:275)
        at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
        at org.apache.cassandra.db.DataTracker.addStreamedSSTable(DataTracker.java:242)
        at org.apache.cassandra.db.ColumnFamilyStore.addSSTable(ColumnFamilyStore.java:920)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:141)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:103)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)


Relevant lines from system.log leading up the it: 

 INFO [FlushWriter:794] 2011-12-01 14:23:22,966 Memtable.java (line 275) Completed flushing /var/lib/cassandra/data/sso/Sessions-hc-12524-Data.db (1119784 bytes)
 INFO [CompactionExecutor:2379] 2011-12-01 14:23:22,969 CompactionTask.java (line 112) Compacting [SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12501-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12517-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12513-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12512-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12502-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12507-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12519-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12500-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12508-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12504-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12510-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12515-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12509-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12524-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12514-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12518-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12505-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12516-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12511-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12506-Data.db')]
 INFO [AntiEntropyStage:1] 2011-12-01 14:25:06,321 AntiEntropyService.java (line 186) [repair #ea080b70-1c51-11e1-0000-692e0c239dfd] Received merkle tree for Sessions from /xxxxxxxx
ERROR [Thread-177] 2011-12-01 14:25:17,863 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Thread-177,5,main]
java.lang.AssertionError: 0  [see above]

If you want more let me know I can reproduce instantly.
","02/Dec/11 05:36;jbellis;Joe, your assertion is the one in CASSANDRA-3536 (where I've attached a patch fixing it).  Closing this other one as cantrepro.","23/Oct/12 22:44;yukim;This error actually happens on 1.1. And I can easily reproduce with unit test(Test code attached).

{code}
    [junit] ERROR 17:34:46,696 Fatal exception in thread Thread[CompactionExecutor:3,1,main]
    [junit] java.lang.AssertionError: Expecting new size of 2, got 1 while replacing [SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-1-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-5-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-4-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-2-Data.db')] by [SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-6-Data.db')] in View(pending_count=0, sstables=[SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-1-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-2-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-4-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-4-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-5-Data.db')], compacting=[SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-1-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-5-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-4-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-2-Data.db')])
    [junit] 	at org.apache.cassandra.db.DataTracker$View.newSSTables(DataTracker.java:651)
    [junit] 	at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:616)
    [junit] 	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
    [junit] 	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:253)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:994)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:154)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:680)
{code}

The cause is actually in streaming. StreamInSession can add duplicate reference to SSTable to DataTracker when it is left even after stream session finishes. This typically happens when source node is marked as dead by FailureDetector during streaming session(GC storm is the one I saw) and keep sending file in same session after the node comes back.",23/Oct/12 22:45;yukim;Test code attached. Compaction strategy is not related.,"24/Oct/12 12:11;slebresne;Good analysis Yuki. I'm not really sure what is the right fix though. Given that this should very rarely happen (repair uses a much higher failure detection threshold than the normal one, though maybe we can increase it even more to make this even less likely) and that I don't seen any obvious way to avoid that kind of situation, maybe making DataTracker handle duplicate addition of a SSTableReader is the simplest thing to do. The obvious way to do that would be to change the View sstables List to a Set, which leads me to the current commentary in the code:
{noformat}
        // We can't use a SortedSet here because ""the ordering maintained by a sorted set (whether or not an
        // explicit comparator is provided) must be <i>consistent with equals</i>.""  In particular,
        // ImmutableSortedSet will ignore any objects that compare equally with an existing Set member.
        // Obviously, dropping sstables whose max column timestamp happens to be equal to another's
        // is not acceptable for us.  So, we use a List instead.
{noformat}
I think that comment is obsolete. Namely, it was added with CASSANDRA-2498 and at the time the list of sstable was kept in max timestamp order at all time. But since then, we've moved the sorting in max timestamp in CollationController directly (which is less fragile), so the order inside DataTracker doesn't matter anymore.","24/Oct/12 20:46;jbellis;bq. This typically happens when source node is marked as dead by FailureDetector during streaming session(GC storm is the one I saw) and keep sending file in same session after the node comes back

But we close the session on convict, so shouldn't it start a new one?","24/Oct/12 22:00;yukim;bq. But we close the session on convict, so shouldn't it start a new one?

Yes, StreamInSession gets closed and removed on convict _once_. But if GC pause happens in the middle of streaming session, the node resumes streaming in the same session after GC. Since resumed stream carries session ID that is once closed on receiver side, StreamInSession is created again with the same old session ID and this time just 1 file to receive.
This continues again and again until source node's StreamingOutSession sends all files.
You can see this in receiver's log file like below:

{code}
INFO [Thread-50] 2012-10-20 13:13:26,574 StreamInSession.java (line 214) Finished streaming session 10 from /10.xx.xx.xx
INFO [Thread-51] 2012-10-20 13:13:29,691 StreamInSession.java (line 214) Finished streaming session 10 from /10.xx.xx.xx
INFO [Thread-52] 2012-10-20 13:13:32,957 StreamInSession.java (line 214) Finished streaming session 10 from /10.xx.xx.xx
{code}

Duplication happens during this partially broken streaming session. Because StreamInSession is removed after sending SESSION_FINISHED reply, and StreamOutSession keeps sending files, sometimes the same StreamInSession instance receives more than 1 file and calls closeIfFinished every time it received the file.
(Sorry, this is hard to explain in words.
https://github.com/apache/cassandra/blob/cassandra-1.1.6/src/java/org/apache/cassandra/streaming/StreamInSession.java#L181 this part is executed multiple times with _readers_ growing by received new file.)

So as Sylvain stated above, changing DataTracker.View's sstable to Set is one way to eliminate duplicate reference and we should do it. In addition, I'm thinking not to create duplicate StreamInSession by checking StreamHeader.pendingFiles because this field is only filled when initiating streaming.","25/Oct/12 07:44;slebresne;That code is a mess so let me give a shot at describing what happens for the record. Say node1 wants to stream files A, B and C to node2. If everything goes well what happens is:
# node1 sends the first file A with a StreamHeader that says that A, B and C are pending files and A is the currently sent file. On node2, a new StreamInSession is created with those information.
# Once A is finished, node2 remove A from the pending file in the StreamInSession send an acknowledgement to node1, and then node1 sends B with a StreamHeader with no pending files (basically the list of pending files is only sent the first time so that the StreamInSession on node2 knows when everything is finished) and B as current file. When node2 received that StreamHeader, it retrieve the StreamInSession, setting B as the current files.
# Once B is finished, node2 removes it from pending files, acks to node1 and node1 sends C with a StreamHeader with no pending file and C as current file.  Node2 retrieven the StreamInSession and modify it accordingly.
# At last, once C is finished, node2 removes it from the pending files. Then it realizes the pending files are empty and so that the streaming is finished and at that point it adds all the SSTableReader created so far to the cfs (and acks to node1 the end of the streaming).

Now, the problem is if say node1 is marked dead by mistake by node2 during say the streaming of A. I that happens, the only thing we do on node2 is to close the session and remove the streamInSession from the global sessions map.  However we don't shutdown the stream or anything, so if node1 is in fact still alive, what will happen is:
# A will finish his transfer correctly. Once that's done, node2 will still send an acknowledgement (probably the first mistake, we could check that the session has been closed and send an error instead).
# Node1 getting it's acknowledgement will send B with a StreamHeader that has B as current file and no pending files as usual. On reception, node2 will not find any StreamInSession (it has been removed during the close), and so it will create a new one as if that was the beginning of a transfer. And that session will have no pending file (second mistake: if we have to create a new StreamInSession but there is no pending file at all something wrong has happened).
# Once B is fully streamed, node2 will acknowledge it to node1 and remove it from it's streamInSession. But that session the new one we just created with no pending file. So the streamInSession will consider the streaming is finished, and it will thus add the SSTableReader for B to the cfs.
# Because B has been acknowledged, node1 will start sending C (again, with no pending file in the StreamHeader). This will be done as soon as B was finished, and so concurrently with the streamInSession on node2 closing itself.
# So when node2 receives the StreamHeader with C, it will try to retrieve the session and will find the previous session. And will happily add C as the current file for that session (third and fourth mistake: StreamInSession should not add a file as current unless it is a pending file for this session, and a session could detect that it's being reused even though it has just detected itself as finished).
# Now when C transfer finishes, the seesion will be notify and since it still has no pending files, it will once again consider the streaming as complete.  But since it's still the same session, it still has the SSTableReader for B in its list of created reader (as well as the one for C now). And that's when it adds B for a second time to the DataTracker.

I also not that we end up without having ever add the SSTableReader for A to the cfs since the very first StreamInSession was never finished. This is not a big deal in that the stream itself has been indicated as failed to the client anyway, but just to say that it's not just a problem of duplicating a SSTableReader preference.

Anyway, let me back on what I said earlier. We should definitively fix some if not all of the ""mistake"" above (and send a SESSION_FAILURE to node1 as soon as we detect something is wrong).

But that being said, my comment on the comment in DataTracker being obsolete still stand, and replacing the list by a set in there would have at least the advantage of slightly simplifying the code of DataTracker.View.newSSTables(), as well as being more resilient if a SSTableReader is added twice. Not a big deal though.
","29/Oct/12 20:28;yukim;Attaching first attempt.
I changed DataTracker.View's sstables to Set, and made stream fail when file arrives after StreamInSession failed.

Changing List to Set for sstables sometimes makes CollationControllerTest fail. It was introduced in CASSANDRA-4116, and I think the test and CollationController#collectAllData expect sstables to be ordered by timestamp. I'm not sure if the test is obsolete or we really need sstables to be sorted all the time.
0002 patch alone will fix the issue, so we can apply that for now.","30/Oct/12 08:15;slebresne;For patch 0002, we shouldn't check the FailureDetector otherwise we don't really fix the issue. The only way we know this bug can happen is wher the FailureDetector *had* marked a node down while it shouldn't have (besides, we just got something from a node so it's fair to assume it is alive).

bq. and I think the test and CollationController#collectAllData expect sstables to be ordered by timestamp

It doesn't seem to me that collectAllData needs sstable ordered. In fact, I think that it does a second pass over the sstables iterators just because it doesn't assume sstables are ordered by max timestamp. Moreover, I'm pretty sure it would be a bug to assume that. If you look at DataTracker.View.newSSTables, it ends by {{Iterables.addAll(newSSTables, replacements)}} which clearly won't maintain any specific ordering of sstables.

bq. I'm not sure if the test is obsolete.

I don't think the test is obsolete but I think we have a minor bug in CollationController. The test want to test that we correctly exclude sstable whose maxTimestamp is less than the most recent row tombstone we have. But that test checks controller.getSstablesIterated(), and for collectAllData, it will count every sstable it include in the first iteration of collectAllData but don't remove those that are remove by the second pass. In other words, I think the correct fix is to decrement stablesIterated in CollationController when in the second pass we remove a sstable (or more simply to set it to iterators.size() just before we collate everything).","30/Oct/12 22:34;yukim;bq. we shouldn't check the FailureDetector otherwise we don't really fix the issue.

Ok. I've fixed this and reattached 0002.

bq. The test want to test that we correctly exclude sstable whose maxTimestamp is less than the most recent row tombstone we have.

Right. But I think the test assumes that SSTables are added to List in order of flush, and that's true as long as we use List. So what I suggest is to remove that part from the test since we no longer use List.
And sstablesIterated counter in collectAllData is doing fine because we actually read the data from sstable when we go over
{code}
IColumnIterator iter = filter.getSSTableColumnIterator(sstable);
{code}
before incrementing counter.

So I removed that test from CollationControllerTest in 0001-change-DataTracker.View-s-sstables-from-List-to-Set.patch.","31/Oct/12 09:48;slebresne;bq. Ok. I've fixed this and reattached 0002.

Alright, +1 on 0002. Let's commit that for now to 1.1/1.2 as this fix this ticket.

{quote}
the test assumes that SSTables are added to List in order of flush
And sstablesIterated counter in collectAllData is doing fine because we actually read the data
{quote}

Right. I guess what I meant is that what is tested right now is not really sensible. Relying on the order of flush is only valid for a small, controlled test, but in reality as soon as compaction kicks in, the order of sstable in DataTracker will be meaningless even with a List instead of a Set. Basically the guarantee collectAll gives us today is that it will eliminate sstables whose maxTimestamp < mostRecentTombstone with just having read the sstable row header, not the full data. But that's not what sstablesIterated counts so it's broken.

That being said, I think we can improve collectAll in the way described in CASSANDRA-4883. If we do so, the test will pass again without relying on any assumption of the order of sstables in DataTracker. So overall I suggest moving all of this to CASSANDRA-4883.",31/Oct/12 16:13;yukim;Committed 0002 to 1.1 and trunk.,"11/Mar/15 19:44;dashv;I understand that this has been fixed in newer versions of Cassandra.

But I'm currently seeing this exact issue on a production 1.1.1 node in my cluster.

What should be my next step?

Do I simply restart it?

Run cleanup? Scrub? Repair?

Sounds like repair would just fail with the same problem.

Any advice would be appreciated.","11/Mar/15 19:56;yukim;Yes, restarting the node will help.
No need to clean up/scrub.

Please use user@cassandra.apache.org mailing list for these type of questions.",11/Mar/15 20:32;dashv;Thanks for the swift reply and I will use the mailing list in the future.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool Doesnt close the open JMX connection causing it to leak Threads,CASSANDRA-3309,12525728,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,04/Oct/11 20:22,12/Mar/19 14:16,13/Mar/19 22:26,04/Oct/11 21:21,0.8.7,1.0.0,,,,,0,,,,,"When nodetool is used intensively we will see 1000's of ""JMX server connection timeout""

Fix is to close the connections when no longer needed.",,,,,,,,,,,,,,,,04/Oct/11 20:55;vijay2win@yahoo.com;0001-fixing-jmx-thread-leak.patch;https://issues.apache.org/jira/secure/attachment/12497705/0001-fixing-jmx-thread-leak.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-04 21:21:06.005,,,no_permission,,,,,,,,,,,,45238,,,Tue Oct 04 22:15:03 UTC 2011,,,,,,0|i0gicf:,94402,brandon.williams,brandon.williams,,,,,,,,,04/Oct/11 20:55;vijay2win@yahoo.com;Simple fix which just fixes nodetool to close the connection which it creates. tested and works fine. ,04/Oct/11 21:21;brandon.williams;Committed,"04/Oct/11 22:15;hudson;Integrated in Cassandra-0.8 #360 (See [https://builds.apache.org/job/Cassandra-0.8/360/])
    Nodetool closes JMX connections to avoid leaking timer threads.
Patch by vijay, reviewed by brandonwilliams for CASSANDRA-3309

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1178957
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/NodeCmd.java
* /cassandra/branches/cassandra-1.0.0/CHANGES.txt
* /cassandra/branches/cassandra-1.0.0/src/java/org/apache/cassandra/tools/NodeCmd.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stop Cassandra result in hang,CASSANDRA-3302,12525624,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,cywjackson,cywjackson,04/Oct/11 05:08,12/Mar/19 14:16,13/Mar/19 22:26,27/Oct/11 13:16,1.0.1,,,,,,0,,,,,"testing this under trunk via a hacked package (replacing jars from 0.8.6 deb installation)

When calling service cassandra stop, the Cassandra process hang:

http://aep.appspot.com/display/i6aIUCkt4kz0HG5l2VszMM7QvLo/

The following logs is observed in the C* log:

 INFO [main] 2011-10-03 23:20:46,434 AbstractCassandraDaemon.java (line 270) Cassandra shutting down...
 INFO [main] 2011-10-03 23:20:46,434 CassandraDaemon.java (line 218) Stop listening to thrift clients

Re-run this using 1.0.0 branch, (following the same ""hack"" procedure), C* stop properly, and the following is observed in the log:

 INFO [main] 2011-10-04 05:02:08,048 AbstractCassandraDaemon.java (line 270) Cassandra shutting down...
 INFO [main] 2011-10-04 05:02:08,049 CassandraDaemon.java (line 218) Stop listening to thrift clients
 INFO [Thread-2] 2011-10-04 05:02:08,318 MessagingService.java (line 482) Shutting down MessageService...
 INFO [Thread-2] 2011-10-04 05:02:08,319 MessagingService.java (line 497) Waiting for in-progress requests to complete
 INFO [ACCEPT-/10.83.77.171] 2011-10-04 05:02:08,319 MessagingService.java (line 637) MessagingService shutting down server thread.


could this be related to CASSANDRA-3261 ?",,,,,,,,,,,,,,,,27/Oct/11 11:03;slebresne;3302.patch;https://issues.apache.org/jira/secure/attachment/12501074/3302.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-13 18:59:18.132,,,no_permission,,,,,,,,,,,,44007,,,Thu Oct 27 13:16:33 UTC 2011,,,,,,0|i0gi9b:,94388,tjake,tjake,,,,,,,,,"13/Oct/11 18:59;jbellis;Jackson, if I remember correctly, this is broken in trunk but not 1.0.0?  Hence the suspicion of CASSANDRA-3261.","13/Oct/11 19:47;cywjackson;indeed i was testing on trunk but not 1.0.0

but i have not tried to test using 1.0.0 to see if it fails there. i could give it a quick try",13/Oct/11 22:26;cywjackson;smoke test quickly repeating the same step except swaps with 1.0.0 jars cannot reproduce the problem,24/Oct/11 14:30;tjake;Jackson is this still happening? Or is it ok to close?,"25/Oct/11 00:17;cywjackson;still happen in 1.0 branch, so NOT OK to close.

stack trace looks the same as in the earlier link",27/Oct/11 11:03;slebresne;This is indeed a bug from CASSANDRA-3261 that forgot to override the interrupt method.,27/Oct/11 12:55;tjake;+1,"27/Oct/11 13:16;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java Stress Tool:  COUNTER_GET reads from CounterSuper1 instead of SuperCounter1,CASSANDRA-3301,12525608,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,cdaw,cdaw,04/Oct/11 00:39,12/Mar/19 14:16,13/Mar/19 22:26,04/Oct/11 14:40,0.8.7,1.0.0,,,,,0,,,,,"Output from stress tool - COUNTER_ADD works fine bug COUNTER_GET does not
{code}
./stress --operation=COUNTER_ADD --family-type=Super --num-keys=1 --consistency-level=TWO --replication-factor=3 --nodes=cathy1
Unable to create stress keyspace: Keyspace already exists.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
1,0,0,0.0060,0
END


./stress --operation=COUNTER_GET --family-type=Super --num-keys=1 --consistency-level=QUORUM --nodes=cathy1
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Operation [0] retried 10 times - error reading counter key 0 ((InvalidRequestException): unconfigured columnfamily CounterSuper1)

0,0,0,NaN,0
END
{code}

The CF created is called *SuperCounter1* and not *CounterSuper1*
{code}
 INFO 00:34:21,344 ColumnFamilyStore(table='Keyspace1', columnFamily='SuperCounter1') liveRatio is 9.167798032786886 (just-counted was 9.167798032786886).  calculation took 1281ms for 9883 columns
{code}

",,,,,,,,,,,,,,,,04/Oct/11 07:07;slebresne;3301.patch;https://issues.apache.org/jira/secure/attachment/12497594/3301.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-04 07:07:46.121,,,no_permission,,,,,,,,,,,,43978,,,Tue Oct 04 14:40:08 UTC 2011,,,,,,0|i0gi8v:,94386,jbellis,jbellis,,,,,,,,,"04/Oct/11 01:05;cdaw;Workaround is to edit: tools/stress/src/org/apache/cassandra/stress/operations/CounterGetter.java:
ColumnParent parent = new ColumnParent(""CounterSuper1"").setSuper_column(superColumn.getBytes());
","04/Oct/11 07:07;slebresne;Oops, thanks Cathy. Patch attached.",04/Oct/11 13:19;jbellis;+1,"04/Oct/11 14:31;hudson;Integrated in Cassandra-0.8 #359 (See [https://builds.apache.org/job/Cassandra-0.8/359/])
    Fix stress COUNTER_GET options
patch by slebresne; reviewed by jbellis for CASSANDRA-3301

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1178785
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/tools/stress/src/org/apache/cassandra/stress/operations/CounterGetter.java
","04/Oct/11 14:40;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
truncate can still result in data being replayed after a restart,CASSANDRA-3297,12525557,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,03/Oct/11 18:02,12/Mar/19 14:16,13/Mar/19 22:26,05/Oct/11 18:35,0.8.8,1.0.0,,,,,0,commitlog,,,,Our first stab at fixing this was CASSANDRA-2950.,,,,,,,,,,,,,,,,03/Oct/11 18:37;jbellis;3297.txt;https://issues.apache.org/jira/secure/attachment/12497509/3297.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-05 16:56:48.814,,,no_permission,,,,,,,,,,,,43845,,,Wed Oct 05 19:23:19 UTC 2011,,,,,,0|i0gi73:,94378,slebresne,slebresne,,,,,,,,,"03/Oct/11 18:37;jbellis;The primary fix here is this:

{noformat}
+        // flush the CF being truncated before forcing the new segment
+        forceBlockingFlush();
{noformat}

Without this, forcing a new segment doesn't help us if the CF-to-truncate was dirty, since its last memtable will be in the new, non-deletable (since it is the last) segment.

The rest of the patch does three things:

- removes redundant code from RMTruncateTest
- fixes CL.resetUnsafe for windows by making it close the segments it's clearing
- adds debug logging",03/Oct/11 18:42;jbellis;(patch is against trunk),"05/Oct/11 16:56;slebresne;nitpick: the assert that segments is not empty in CL.createNewSegment() could be moved one line up since sync() already assume this.

But otherwise, patch lgtm. +1.",05/Oct/11 18:35;jbellis;moved assert + committed,"05/Oct/11 19:23;hudson;Integrated in Cassandra-0.8 #364 (See [https://builds.apache.org/job/Cassandra-0.8/364/])
    fix truncate allowing data to be replayed post-restart
patch by jbellis; reviewed by slebresne for CASSANDRA-3297

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1179359
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/Truncation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
local writes timing out cause attempt to hint to self,CASSANDRA-3440,12529794,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,02/Nov/11 02:11,12/Mar/19 14:16,13/Mar/19 22:26,24/Nov/11 20:06,1.0.4,,,,,,0,hintedhandoff,,,,"As reported by Ramash Natarajan on the mailing list:

{noformat}
We have a 8 node cassandra cluster running 1.0.1. After running a load
test for a day we are seeing this exception in system.log file.

ERROR [EXPIRING-MAP-TIMER-1] 2011-11-01 13:20:45,350
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[EXPIRING-MAP-TIMER-1,5,main]
java.lang.AssertionError: /10.19.102.12
       at org.apache.cassandra.service.StorageProxy.scheduleLocalHint(StorageProxy.java:339)
       at org.apache.cassandra.net.MessagingService.scheduleMutationHint(MessagingService.java:201)
       at org.apache.cassandra.net.MessagingService.access$500(MessagingService.java:64)
       at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:175)
       at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:152)
       at org.apache.cassandra.utils.ExpiringMap$1.run(ExpiringMap.java:89)
       at java.util.TimerThread.mainLoop(Timer.java:512)
       at java.util.TimerThread.run(Timer.java:462)
{noformat}",,,,,,,,,,,,,,,,24/Nov/11 05:49;jbellis;3440-v2.txt;https://issues.apache.org/jira/secure/attachment/12504988/3440-v2.txt,05/Nov/11 05:17;jbellis;3440.txt;https://issues.apache.org/jira/secure/attachment/12502578/3440.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-08 10:12:24.712,,,no_permission,,,,,,,,,,,,215653,,,Thu Nov 24 20:06:51 UTC 2011,,,,,,0|i0gjxj:,94659,slebresne,slebresne,,,,,,,,,05/Nov/11 05:17;jbellis;patch to (1) allow retrying a write-to-self that timed out and (2) improve defense against cascading failure when nodes are overwhelmed but not dead.,"08/Nov/11 10:12;slebresne;I'm obviously missing something but I don't find how that assertion could be triggered in the first place.
More precisely, I don't see that a node can hint itself, since a callback in MessagingService is only put through sendRR which isn't called for local writes (unless OPTIMIZE_LOCAL_REQUESTS is false, which it shouldn't).",24/Nov/11 04:43;jbellis;The assertion can be triggered by a read-repair mutation timing out.  Read-repair mutations (from RowRepairResolver.scheduleRepairs) are sent over MessagingService.,"24/Nov/11 05:49;jbellis;v2 retains the assertion, and moves read repair updates back to the READ_REPAIR message type, where they won't be hinted.  v2 retains the code to make HH more robust against causing coordinator OOMs.","24/Nov/11 09:48;slebresne;Nit: it doesn't seem we use the row mutations saved in hintsInProgess, so maybe we could use a simple AtomicInteger, rather than a full concurrent map.

But otherwise patch looks good, +1.","24/Nov/11 20:06;jbellis;Updated to use this instead and committed:

{code}
.   private static final Map<InetAddress, AtomicInteger> hintsInProgress = new MapMaker().concurrencyLevel(1).makeComputingMap(new Function<InetAddress, AtomicInteger>()
    {
        public AtomicInteger apply(InetAddress inetAddress)
        {
            return new AtomicInteger(0);
        }
    });
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CompressionMetadata is not shared across threads, we create a new one for each read",CASSANDRA-3427,12529483,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,31/Oct/11 13:57,12/Mar/19 14:16,13/Mar/19 22:26,31/Oct/11 15:44,1.0.2,,,,,,1,compression,,,,"The CompressionMetada holds the compressed block offsets in memory. Without being absolutely huge, this is still of non-negligible size as soon as you have a bit of data in the DB. Reallocating this for each read is a very bad idea.

Note that this only affect range queries, since ""normal"" queries uses CompressedSegmentedFile that does reuse a unique CompressionMetadata instance.

( Background: http://thread.gmane.org/gmane.comp.db.cassandra.user/21362 )",,,,,,,,,,,,,,,,14/Nov/11 14:22;slebresne;0001-debugging.patch;https://issues.apache.org/jira/secure/attachment/12503620/0001-debugging.patch,31/Oct/11 15:16;slebresne;3427.patch;https://issues.apache.org/jira/secure/attachment/12501622/3427.patch,03/Nov/11 10:47;slebresne;3427_v2.patch;https://issues.apache.org/jira/secure/attachment/12502124/3427_v2.patch,03/Nov/11 04:55;michaelsembwever;CASSANDRA-3427.patch;https://issues.apache.org/jira/secure/attachment/12502099/CASSANDRA-3427.patch,13/Nov/11 16:55;michaelsembwever;jmx_jvm_memory-month.png;https://issues.apache.org/jira/secure/attachment/12503537/jmx_jvm_memory-month.png,13/Nov/11 16:55;michaelsembwever;jmx_jvm_memory-week.png;https://issues.apache.org/jira/secure/attachment/12503536/jmx_jvm_memory-week.png,,,,,,6.0,,,,,,,,,,,,,,,,,,,2011-10-31 15:01:48.338,,,no_permission,,,,,,,,,,,,215343,,,Mon Nov 14 21:01:45 UTC 2011,,,,,,0|i0gjrj:,94632,xedin,xedin,,,,,,,,,"31/Oct/11 15:01;michaelsembwever;This is unfortunately a showstopper for our hadoop jobs querying our production cluster.

With 1.0.1 is there any workaround for this issue?
Is it correct that this ""compressed block offsets"" totals to 
  (<sstable-size> / <chunk_length>) * 8bytes

Therefore a change to a higher chunk_length should be an intermediate workaround?","31/Oct/11 15:20;slebresne;Quite honestly, the best workaround is likely to apply the attached patch on top of 1.0.1 (you can wait for someone to review to get a bit more confidence).

Because yes, a bigger chunk_length would diminish the problem, but if you do enough range_queries you would likely still OOM and there is point after which a chunk_length too big is just counter-productive. ",31/Oct/11 15:44;xedin;Committed.,"31/Oct/11 20:10;michaelsembwever;Rolled out into production. Works a charm! Even on 200Gb sstables.

Sincere appreciations on this one.","02/Nov/11 21:25;michaelsembwever;Won't the cache here leak?
Many (most?) sstables are transient (gone after the next minor compaction), but this cache will just grow...","02/Nov/11 21:30;xedin;Oh yes, it seems like I missed that one - we should remove entry from the cache when SSTable gets compacted out. What do you think, Sylvain?",02/Nov/11 21:38;michaelsembwever;Or use a ConcurrentLinkedHashMap w/ fixed capacity?,"02/Nov/11 21:43;xedin;yes but that will also imply that we should weight it in memory size instead of number of entries so need to use jamm which is calculation overhead, better just remove unused because we know precisely when to do that...",03/Nov/11 04:55;michaelsembwever;patch according to Pavel's suggestion,"03/Nov/11 10:47;slebresne;Ok, I think using an object cache is ugly (I know that it was my idea).

At first, I tried going with the natural idea, to add the compressionMetadata as a final field of SSTableReader and use that everywhere, ensuring we use one per sstable. Turned out that for SSTableWriter you need to have the metadata existing before the SSTableReader is created and that seemed like a bit of a mess so I backtracked and decided to go with an object cache in CompressionMetada, but more out of laziness than anything else.

That was wrong of me to be lazy. We don't need that object cache and if its use is going to leak out of CompressionMetada (like hard coding the addition of the notifier in the DataTracker constructor; which defeats the purpose of the notifier abstraction in the first place) then it's not even clean as far as code is concerned.

Attaching a v2 patch that remove the cache and do a slight modification of the initial idea, that is it just let CompressedSegmentedFile create the CompressionMetada and have the rest of the code use that. Turns out that once I plug my brain, it's only a few lines of code.
",03/Nov/11 14:09;xedin;+1,"03/Nov/11 14:17;slebresne;Alright, committed this new version, thanks","13/Nov/11 11:39;michaelsembwever;Handling jvm memory since upgrading to cassandra-1.0 and enabling compression is still a headache.
Where i used to be able to run w/ Xmx8G i'm now struggling to run with Xmx20G (all caches are disabled) and during startup can hit
{noformat}java.lang.OutOfMemoryError: Java heap space
        at org.apache.cassandra.utils.BigLongArray.<init>(BigLongArray.java:53)
        at org.apache.cassandra.utils.BigLongArray.<init>(BigLongArray.java:39)
        at org.apache.cassandra.io.compress.CompressionMetadata.readChunkOffsets(CompressionMetadata.java:122){noformat}

I could keep increasing chunk_length (it's already at 256) but this seems awkward just to get a cluster running smoothly. At minimum the calculations for memory requirements for cassandra should be re-written if compression is to take such a large chunk of heap.",13/Nov/11 13:33;jbellis;Does heap usage stay high-post startup?  Can you try forcing a full GC to check that?,"13/Nov/11 16:45;michaelsembwever;bq. Does heap usage stay high-post startup? Can you try forcing a full GC to check that?
Yes. GC doesn't seem to help, i'll attach a munin graph that shows it over time. It was running for a number of days just under 20G, but you can see from that how ""squeezed"" it was.

(invoking full gc via jmx has no noticeable effect on heap used)","13/Nov/11 16:55;michaelsembwever;0.8 was running on Xmx8G up until week 44.
at that point we upgraded to 1.0 and enabled compression. The very high memory usage at the beginning of week 44 was to handle the change from chunk_length 16 to 256.
Then for most of week 44 and week 45 we ran with Xmx16G, but this was very ""squeezed"". Now that's OOM, and raising it to 20G didn't help. Currently it's on 30G.

Also note we're always used -XX:CMSInitiatingOccupancyFraction=60 for this cluster.
(full java opts are ""-Xss128k -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42  -XX:SurvivorRatio=8  -XX:MaxTenuringThreshold=1  -XX:+UseParNewGC  -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:CMSInitiatingOccupancyFraction=60 -XX:+UseCMSInitiatingOccupancyOnly -Xmx30g -Xmx30g -Xmn800M   -XX:ParallelCMSThreads=4  -XX:+CMSIncrementalMode  -XX:+CMSIncrementalPacing  -XX:CMSIncrementalDutyCycleMin=0  -XX:CMSIncrementalDutyCycle=10"". the last 5 were added during week 44 to try and help, ref http://blog.mikiobraun.de/2010/08/cassandra-gc-tuning.html)",14/Nov/11 14:12;jbellis;What version exactly are you running now?  1.0.2?  1.0.0 + 3427?  Something else?,14/Nov/11 14:25;slebresne;I've attached a tiny patch (0001-debugging.patch) that prints in the log the size of the long array we allocate for the chunk offsets. Would you mind trying with this and attach a log of when startup hits one of the OOM you pasted earlier (feel free to use a 8GB heap if it's easier to reproduce). I'd like to know if those offsets are indeed the problem.,"14/Nov/11 14:30;jbellis;I can get you a place to upload the heap dump from an OOM too.  (8GB would be best, since heap analysis requires ram proportional to the heap size.)

To rule out the obvious, have you tried running 1.0.x w/o compression?","14/Nov/11 18:14;michaelsembwever;version: 1.0.2 snapshot (pretty close to release date) + CASSANDRA-3197.
w/o compression: that would require a full compact/scrub. that takes close to 24hrs :-(
patch: i attach that and hopefully have output soon. a heap dump can be done at the same time...","14/Nov/11 20:06;michaelsembwever;startup log with debug (off 1.0.2 release){noformat}INFO  48:34,688 DatabaseDescriptor: Loading settings from file:/iad/finn/countstatistics/conf/cassandra-prod.yaml
INFO  48:34,782 DatabaseDescriptor: DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  48:34,792 DatabaseDescriptor: Global memtable threshold is enabled at 512MB
INFO  48:34,890 AbstractCassandraDaemon: JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_24
INFO  48:34,891 AbstractCassandraDaemon: Heap size: 760414208/8506048512
INFO  48:34,891 AbstractCassandraDaemon: Classpath: /iad/finn/countstatistics/jar/countstatistics.jar:/iad/common/apps/cassandra/lib/jamm-0.2.5.jar
INFO  48:37,158 CLibrary: JNA mlockall successful
INFO  48:37,879 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/Versions-h-42 (256 bytes)
INFO  48:37,879 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/Versions-h-41 (256 bytes)
INFO  48:37,879 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/Versions-h-40 (256 bytes)
INFO  48:37,959 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/IndexInfo-h-3 (223 bytes)
INFO  48:38,001 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/Schema-h-15 (34257 bytes)
INFO  48:38,045 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/Migrations-h-15 (78524 bytes)
INFO  48:38,096 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/LocationInfo-h-150 (80 bytes)
INFO  48:38,096 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/LocationInfo-h-149 (628 bytes)
INFO  48:38,096 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/LocationInfo-h-151 (163 bytes)
INFO  48:38,192 DatabaseDescriptor: Loading schema version 1940c630-0be4-11e1-0000-d1695892b1ff
INFO  51:35,136 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191473 (38646535 bytes)
INFO  51:35,136 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-190467 (2284524668 bytes)
INFO  51:35,136 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191469 (254927460 bytes)
INFO  51:35,136 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191475 (30477008 bytes)
INFO  51:35,136 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-114136 (156044360682 bytes)
INFO  51:35,137 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191294 (4585008988 bytes)
INFO  51:35,137 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-190415 (15857295280 bytes)
INFO  51:35,137 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-183183 (196289440978 bytes)
INFO  51:35,137 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191472 (1346076 bytes)
INFO  51:35,137 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-190736 (4626053255 bytes)
INFO  51:35,137 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191435 (1188223188 bytes)
INFO  51:35,187 CompressionMetadata: Allocating chunks index for 5745 chunks for uncompressed size of 1470519 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191472-CompressionInfo.db)
INFO  51:35,421 CompressionMetadata: Allocating chunks index for 129646 chunks for uncompressed size of 33189311 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191475-CompressionInfo.db)
INFO  51:35,544 CompressionMetadata: Allocating chunks index for 165602 chunks for uncompressed size of 42393918 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191473-CompressionInfo.db)
INFO  51:37,171 CompressionMetadata: Allocating chunks index for 1091377 chunks for uncompressed size of 279392485 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191469-CompressionInfo.db)
INFO  51:41,148 CompressionMetadata: Allocating chunks index for 5086138 chunks for uncompressed size of 1302051278 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191435-CompressionInfo.db)
INFO  51:46,351 CompressionMetadata: Allocating chunks index for 9766541 chunks for uncompressed size of 2500234376 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-190467-CompressionInfo.db)
INFO  51:56,717 CompressionMetadata: Allocating chunks index for 19828434 chunks for uncompressed size of 5076078986 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-190736-CompressionInfo.db)
INFO  51:56,897 CompressionMetadata: Allocating chunks index for 19626358 chunks for uncompressed size of 5024347477 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191294-CompressionInfo.db)
INFO  52:21,670 CompressionMetadata: Allocating chunks index for 67865822 chunks for uncompressed size of 17373650297 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-190415-CompressionInfo.db)
INFO  55:55,920 CompressionMetadata: Allocating chunks index for 666981588 chunks for uncompressed size of 170747286320 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-114136-CompressionInfo.db)
INFO  56:49,620 CompressionMetadata: Allocating chunks index for 840404671 chunks for uncompressed size of 215143595584 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-183183-CompressionInfo.db)
ERROR 57:51,112 AbstractCassandraDaemon: Fatal exception in thread Thread[SSTableBatchOpen:8,5,main]
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.utils.BigLongArray.<init>(BigLongArray.java:53)
	at org.apache.cassandra.utils.BigLongArray.<init>(BigLongArray.java:39)
	at org.apache.cassandra.io.compress.CompressionMetadata.readChunkOffsets(CompressionMetadata.java:127)
        ...{noformat}","14/Nov/11 21:01;slebresne;That's the stupidest bug ever. It happens we interpret the chunk_length_in_kb not in kb but in bytes.
Anyway, I've created CASSANDRA-3492 to address this.
Turns out if you don't update the chunk_length you're fine because the default is ok, but I guess hitting this issue initially has put you in the wrong spot :(",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can create a table with DecimalType comparator but CQL explodes trying to actually use it.,CASSANDRA-3421,12529365,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ardot,kreynolds,kreynolds,29/Oct/11 16:25,12/Mar/19 14:16,13/Mar/19 22:26,03/Nov/11 04:53,1.0.2,,,,,,0,cql,,,,"CREATE KEYSPACE CassandraCQLTestKeyspace WITH strategy_class='org.apache.cassandra.locator.SimpleStrategy' AND strategy_options:replication_factor=1

USE CassandraCQLTestKeyspace

CREATE COLUMNFAMILY comparator_cf_decimal (id text PRIMARY KEY) WITH comparator='DecimalType'

INSERT INTO comparator_cf_decimal (id, 15.333) VALUES ('test', 'test')

# This leads to failure
SELECT 15.333 FROM comparator_cf_decimal WHERE id = 'test' 

ERROR 12:22:56,909 Internal error processing execute_cql_query
java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:480)
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:336)
	at org.apache.cassandra.cql.jdbc.JdbcDecimal.compose(JdbcDecimal.java:90)
	at org.apache.cassandra.db.marshal.DecimalType.compose(DecimalType.java:43)
	at org.apache.cassandra.db.marshal.DecimalType.compare(DecimalType.java:38)
	at org.apache.cassandra.db.marshal.DecimalType.compare(DecimalType.java:30)
	at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:351)
	at java.util.TreeMap.getEntry(TreeMap.java:322)
	at java.util.TreeMap.get(TreeMap.java:255)
	at org.apache.cassandra.db.TreeMapBackedSortedColumns.getColumn(TreeMapBackedSortedColumns.java:138)
	at org.apache.cassandra.db.AbstractColumnContainer.getColumn(AbstractColumnContainer.java:128)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:627)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1236)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
",,,,,,,,,,,,,,,,02/Nov/11 18:50;ardot;3421-v2.txt;https://issues.apache.org/jira/secure/attachment/12502010/3421-v2.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-01 13:49:01.307,,,no_permission,,,,,,,,,,,,215225,,,Thu Nov 03 04:53:32 UTC 2011,,,,,,0|i0gjov:,94620,jbellis,jbellis,,,,,,,,,"29/Oct/11 16:32;kreynolds;FYI, this is the only Type that does this, every other type appears to function properly including Float and Double",01/Nov/11 13:49;ardot;This occurs because the {{compose}} method in {{DecimalType}} does not protect itself  by checking for an empty {{ByteBuffer}} argument which is what occurs if a single item is in the CF.,01/Nov/11 14:21;jbellis;Can you include a unit test for the problem?,"01/Nov/11 14:53;ardot;Sure. Are you looking for a unit test for {{DecimalType}}, or for the problem as stated? I can see where there are various unit tests for data types, but there does not seem to be a package in {{test/unit}} for CQL unit tests? Did I miss it?","01/Nov/11 14:56;jbellis;Just for DecimalType.

(We don't have any CQL unit tests in C* since we moved all the drivers out of tree.)",02/Nov/11 18:50;ardot;V2 contains fix and unit test,"03/Nov/11 04:53;jbellis;Thanks, Rick!

Committed with a minor change: switched from mark/reset to duplicate as a ""don't modify the original buffer"" approach.  mark/modify/reset can cause problems if another thread also tries to use the buffer in between.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in PrecompactedRow.write via CommutativeRowIndexer during bootstrap,CASSANDRA-3394,12528300,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,thepaul,thepaul,21/Oct/11 17:09,12/Mar/19 14:16,13/Mar/19 22:26,21/Oct/11 19:43,0.8.8,1.0.1,,,,,0,,,,,"{noformat}
ERROR [CompactionExecutor:5] 2011-10-21 15:48:16,138 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:5,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:107)
        at org.apache.cassandra.io.sstable.SSTableWriter$CommutativeRowIndexer.doIndexing(SSTableWriter.java:514)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:359)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:314)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1118)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1109)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

{{<sylvain> The bug is that the compacted row was gcing *every* tombstones instead of none.}}",,,,,,,,,,,,,,,,21/Oct/11 17:10;slebresne;0001-Don-t-expire-tombstones-in-CommutativeRowIndexer.patch;https://issues.apache.org/jira/secure/attachment/12500210/0001-Don-t-expire-tombstones-in-CommutativeRowIndexer.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-21 17:12:42.388,,,no_permission,,,,,,,,,,,,97927,,,Fri Oct 21 20:00:32 UTC 2011,,,,,,0|i0gjcv:,94566,jbellis,jbellis,,,,,,,,,21/Oct/11 17:12;slebresne;Note: attached patch is for 0.8. The fix for 1.0.1 is the same but that line is now in IncomingStreamReader. I'll handle that during merge once this is reviewed.,21/Oct/11 17:15;jbellis;+1,21/Oct/11 19:43;slebresne;Committed,"21/Oct/11 20:00;hudson;Integrated in Cassandra-0.8 #385 (See [https://builds.apache.org/job/Cassandra-0.8/385/])
    Don't expire counter tombstones after streaming
patch by slebresne; reviewed by jbellis for CASSANDRA-3394

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1187477
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Truncate disregards running compactions when deleting sstables,CASSANDRA-3399,12528489,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,slebresne,slebresne,24/Oct/11 14:24,12/Mar/19 14:16,13/Mar/19 22:26,31/Oct/11 22:10,0.8.8,1.0.2,,,,,0,,,,,"All truncation do is `cfs.markCompacted(truncatedSSTables)` without holding any lock or anything. Which have the effect of actually deleting sstables that may be compacting. More precisely there is three problems:
# It removes those compacting sstables from the current set of active sstables for the cfs. But when they are done compacting, DataTracker.replaceCompactedSSTables() will be called and it assumes that the compacted sstable are parts of the current set of active sstables. In other words, we'll get an exception looking like the one of CASSANDRA-3306.
# The result of the compaction will be added as a new active sstable (actually no, because the code will throw an exception before because of the preceding point, but that's something we should probably deal with).
# Currently, compaction don't 'acquire references' on SSTR. That's because the code assumes we won't compact twice the same sstable and that compaction is the only mean to delete an sstable. With these two assumption, acquiring references is not necessary, but truncate break that first assumption.

As for solution, I see two possibilities:
# make the compaction lock be per-cf instead of global (which I think is easy and a good idea anyway) and grab the write lock to do the markCompacted call. The big downside is that truncation will potentially take much longer.
# had two phases: mark the sstable that are not compacting as compacted and set the dataTracker as 'truncated at', and let it deal with the other sstable when their compaction is done. A bit like what is proposed for CASSANDRA-3116 
",,,,,,,,,,,,,,,,31/Oct/11 16:48;jbellis;3399.txt;https://issues.apache.org/jira/secure/attachment/12501633/3399.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-26 17:42:33.146,,,no_permission,,,,,,,,,,,,214349,,,Tue Nov 01 00:24:00 UTC 2011,,,,,,0|i0gjf3:,94576,slebresne,slebresne,,,,,,,,,"26/Oct/11 17:42;jbellis;I think we should:

- grab the Big Lock for this ticket
- create another ticket to take a 3116 approach in 1.1
- create another ticket to break the Big Lock apart to a per-CF lock in 1.0.2",31/Oct/11 16:32;jbellis;created CASSANDRA-3429 and CASSANDRA-3430.,31/Oct/11 16:41;jbellis;big lock patch attached.,"31/Oct/11 16:42;jbellis;(patch is against 1.0, will also commit to 0.8)",31/Oct/11 16:48;jbellis;new patch w/ less crack smoking,31/Oct/11 16:52;slebresne;+1,31/Oct/11 22:10;jbellis;committed,"01/Nov/11 00:24;hudson;Integrated in Cassandra-0.8 #391 (See [https://builds.apache.org/job/Cassandra-0.8/391/])
    acquire compactionlock during truncate
patch by jbellis; reviewed by slebresne for CASSANDRA-3399

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1195695
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"AssertionError when adding a node and doing repair, repair hangs",CASSANDRA-3369,12527360,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,christianmovi,christianmovi,16/Oct/11 16:35,12/Mar/19 14:16,13/Mar/19 22:26,21/Oct/11 13:33,0.8.8,1.0.1,,,,,0,,,,,"Hi again,

I was playing aroung with Cassandra 1.0.0-rc2 and got an AssertionError. The cluster I set up was two cassandra nodes on one laptop using different 127.0.0.* loopback devices. Both nodes have separate folders on the harddisk.


Here is what I did:


1. Started node1 and inserted some data into it using a simple singlethreaded testprogram (uses hector 0.8.0-2):
127.0.0.1       datacenter1 rack1       Up     Normal  583.55 MB       100.00% Token(bytes[63e5b6995466cd3221cba16646ae19ed])

2. I started another node, node 2 = 127.0.0.2:
127.0.0.2       datacenter1 rack1       Up     Normal  147.57 KB       50.00%  Token(bytes[4d6ccfeaa8bb59551751a2816fde9343])
127.0.0.1       datacenter1 rack1       Up     Normal  583.55 MB       50.00%  Token(bytes[63e5b6995466cd3221cba16646ae19ed])

3. I triggered a ""nodetool -h 127.0.0.1  repair"" on the first node that had the data from my test.


This repair does not seem to ever end. The nodetool is hanging now but my computer is idle. I get an AssertionError on the first node:
java.lang.AssertionError
	at org.apache.cassandra.service.AntiEntropyService$Validator.prepare(AntiEntropyService.java:283)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:825)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:63)
	at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:432)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)


Update: I dont know if it is important but here is the schema of my test:
create keyspace Test with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = {replication_factor:2};
CREATE COLUMN FAMILY Response WITH key_validation_class=BytesType AND compression_options={sstable_compression:DeflateCompressor};


kind regards,
Christian","Ubuntu Linux amd64, 1.0.0-rc2",,,,,,,,,,,,,,,18/Oct/11 10:23;slebresne;3369.patch;https://issues.apache.org/jira/secure/attachment/12499528/3369.patch,16/Oct/11 16:38;christianmovi;NODE1_cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12499204/NODE1_cassandra.yaml,16/Oct/11 16:38;christianmovi;NODE1_system.log;https://issues.apache.org/jira/secure/attachment/12499205/NODE1_system.log,16/Oct/11 16:38;christianmovi;NODE2_cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12499206/NODE2_cassandra.yaml,16/Oct/11 16:38;christianmovi;NODE2_system.log;https://issues.apache.org/jira/secure/attachment/12499207/NODE2_system.log,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-10-18 10:23:31.437,,,no_permission,,,,,,,,,,,,87508,,,Fri Oct 21 16:00:27 UTC 2011,,,,,,0|i0gj1r:,94516,jbellis,jbellis,,,,,,,,,16/Oct/11 16:38;christianmovi;Added config and log files from both cassandra nodes.,"16/Oct/11 23:09;christianmovi;Update: I also had problems because I was using Defalte compression, which seems to be broken. The AssertionError during repair still occurs with Snappy Compression.","18/Oct/11 10:23;slebresne;This is due to the code not handling correctly a case of having no sample keys for one of it's token range.

Let's not that:
  * this is not specific to 1.0.0, 0.8 is affected too.
  * this only affect order preserving partitioner (not because the order matter but because this code path is not taken with RandomPartitioner since a few releases now, at least as far as repair is concerned).
  * this would be unlikely to show in any serious production setting, since having no sample for a given range means that either you have almost no rows in the cluster or that the cluster is extremely badly balanced.

Patch attached to fix (against 0.8).",20/Oct/11 21:54;jbellis;+1,21/Oct/11 13:33;slebresne;Committed,"21/Oct/11 16:00;hudson;Integrated in Cassandra-0.8 #384 (See [https://builds.apache.org/job/Cassandra-0.8/384/])
    fix assertion error during repair with ordered partitioners
patch by slebresne; reviewed by jbellis for CASSANDRA-3369

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1187333
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra inferred schema and actual data don't match,CASSANDRA-3371,12527421,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,petewarden,petewarden,17/Oct/11 10:25,12/Mar/19 14:16,13/Mar/19 22:26,13/Feb/12 23:45,1.0.8,,,,,,2,,,,,"It's looking like there may be a mismatch between the schema that's being reported by the latest CassandraStorage.java, and the data that's actually returned. Here's an example:

rows = LOAD 'cassandra://Frap/PhotoVotes' USING CassandraStorage();
DESCRIBE rows;
rows: {key: chararray,columns: {(name: chararray,value: bytearray,photo_owner: chararray,value_photo_owner: bytearray,pid: chararray,value_pid: bytearray,matched_string: chararray,value_matched_string: bytearray,src_big: chararray,value_src_big: bytearray,time: chararray,value_time: bytearray,vote_type: chararray,value_vote_type: bytearray,voter: chararray,value_voter: bytearray)}}
DUMP rows;
(691831038_1317937188.48955,{(photo_owner,1596090180),(pid,6855155124568798560),(matched_string,),(src_big,),(time,Thu Oct 06 14:39:48 -0700 2011),(vote_type,album_dislike),(voter,691831038)})

getSchema() is reporting the columns as an inner bag of tuples, each of which contains 16 values. In fact, getNext() seems to return an inner bag containing 7 tuples, each of which contains two values. 

It appears that things got out of sync with this change:
http://svn.apache.org/viewvc/cassandra/branches/cassandra-0.8/contrib/pig/src/java/org/apache/cassandra/hadoop/pig/CassandraStorage.java?r1=1177083&r2=1177082&pathrev=1177083

See more discussion at:
http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/pig-cassandra-problem-quot-Incompatible-field-schema-quot-error-tc6882703.html
",,,,,,,,,,,,,,,,08/Feb/12 14:01;brandon.williams;0001-Rework-pig-schema.txt;https://issues.apache.org/jira/secure/attachment/12513811/0001-Rework-pig-schema.txt,08/Feb/12 14:01;brandon.williams;0002-Output-support-to-match-input.txt;https://issues.apache.org/jira/secure/attachment/12513812/0002-Output-support-to-match-input.txt,19/Oct/11 20:46;brandon.williams;3371-v2.txt;https://issues.apache.org/jira/secure/attachment/12499755/3371-v2.txt,20/Oct/11 12:03;brandon.williams;3371-v3.txt;https://issues.apache.org/jira/secure/attachment/12499851/3371-v3.txt,19/Jan/12 19:34;brandon.williams;3371-v4.txt;https://issues.apache.org/jira/secure/attachment/12511156/3371-v4.txt,31/Jan/12 22:18;brandon.williams;3371-v5-rebased.txt;https://issues.apache.org/jira/secure/attachment/12512674/3371-v5-rebased.txt,31/Jan/12 20:59;brandon.williams;3371-v5.txt;https://issues.apache.org/jira/secure/attachment/12512626/3371-v5.txt,13/Feb/12 22:58;xedin;3371-v6-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12514413/3371-v6-cleanup.patch,13/Feb/12 21:37;brandon.williams;3371-v6.txt;https://issues.apache.org/jira/secure/attachment/12514398/3371-v6.txt,17/Oct/11 20:49;petewarden;pig.diff;https://issues.apache.org/jira/secure/attachment/12499429/pig.diff,13/Feb/12 21:37;brandon.williams;smoke_test.txt;https://issues.apache.org/jira/secure/attachment/12514399/smoke_test.txt,11.0,,,,,,,,,,,,,,,,,,,2011-10-17 16:45:47.056,,,no_permission,,,,,,,,,,,,88124,,,Mon Feb 13 23:45:18 UTC 2012,,,,,,0|i0gj2n:,94520,xedin,xedin,,,,,,,,,"17/Oct/11 16:45;brandon.williams;Pete, can you attach this patch as a file?  It's difficult to read in the description.",17/Oct/11 20:49;petewarden;Messy patch to implement matching schema and data return in CassandraLoader.,"17/Oct/11 20:55;petewarden;I've attached the patch as a file (couldn't see how when creating the ticket). It's definitely not ready for prime-time, but hopefully should get across the idea of what I'm going for. Some notes:

- I think the Hex handling has changed between top-of-tree and the 0.8.7 version I'm on, so the diff includes some reversions so I can compile it against my older version. This shouldn't be in any final version of the patch.

- It has been a long time since I last wrote Java code, so my changes are probably very non-idiomatic.

- I don't attempt to handle super-columns in getNext(). Looking at the original getSchema() code, it looks like that case might be unhandled anyway? I see 'if (cfDef.column_type.equals(""Super"")) return null;' near the top.
","17/Oct/11 20:57;petewarden;And for completeness, here's the definition of the column family in the examples above:


create column family PhotoVotes with
  comparator = UTF8Type and
  column_metadata =
  [
    {column_name: voter, validation_class: UTF8Type, index_type: KEYS},
    {column_name: vote_type, validation_class: UTF8Type},
    {column_name: photo_owner, validation_class: UTF8Type, index_type: KEYS},
    {column_name: src_big, validation_class: UTF8Type},
    {column_name: pid, validation_class: UTF8Type, index_type: KEYS},
    {column_name: matched_string, validation_class: UTF8Type},
    {column_name: time, validation_class: UTF8Type},
  ];
","19/Oct/11 20:46;brandon.williams;This approach has many problems, such as column name and key collisions.  Instead, what we need to is wrap the column/value pairs in their own tuple and then insert these into an outer tuple that goes in the bag, causing the schema the match our actual output.  v2 does this.","19/Oct/11 20:54;petewarden;Is there a reason the columns can't at least go into a map? As things stand, it's painfully hard to do the natural row.column.value lookup in a script. Or to put it as a concrete example, I can currently do this:

all_votes = LOAD 'cassandra://Frap/PhotoVotes' USING CassandraStorage();
album_votes = FILTER all_votes BY ((vote_type EQ 'album_like') OR (vote_type EQ 'album_dislike'));

What does this example look like with your approach?","20/Oct/11 12:03;brandon.williams;I had what I thought would be a good idea to accomodate this: I'd alias the tuples themselves after the index names, allowing you to do something like this:

{noformat}
album_votes = FILTER all_votes BY (columns.vote_type.value EQ 'album_like') OR (columns.vote_type.value EQ 'album_dislike');
{noformat}

It's not that easy, however.  When you dereference a bag, it automatically dereferences the tuple inside it (programmatically, a bag can only have one tuple, but that tuple can contain anything, and thus the automatic deref to mask this from the user.)  Unfortunately, it appears to also deref any other tuples inside as well, so you end up in a situation where 'columns.$0' returns the column name, and weird aliasing issues like 'columns.<first index name>' also returns the column name and the second index returns the value, with the rest being null.

To get around this, I thought I'd nest each tuple inside another bag.  This of course results in a crazy looking schema:
{noformat}
votes: {key: bytearray,columns: {(matched_string: {(name: chararray,value: chararray)},photo_owner: {(name: chararray,value: chararray)},pid: {(name: chararray,value: chararray)},src_big: {(name: chararray,value: chararray)},time: {(name: chararray,value: chararray)},vote_type: {(name: chararray,value: chararray)},voter: {(name: chararray,value: chararray)})}}
{noformat}
but if it still derefs as elegantly, it won't be too bad.  The problem here is that it still can't deref correctly, you end up with nonsensical parsing errors such as:
{noformat}
ERROR 1200: Pig script failed to parse: 
<file photo.pig, line 5, column 8> pig script failed to validate: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1128: Cannot find field name in matched_string:bag{:tuple(name:chararray,value:chararray)
{noformat}
even though the 'name' alias is clearly there and 'matched_string' was already being deref'd.

So I decided to just get rid of the all the bags, and instead did a tuple of tuples.  This worked!  But the problem here is that a tuple has to fit into memory, where a bag can spill to disk if it is very large.  Currently this won't be an issue since we have to fit the entire row into memory via thrift anyway, but in the future when we have large row support the contract may have to change again.  Maybe this is the best option though since right now it Just Works and nothing else is viable.

While going through all this I noticed two other problems we currently have.  We need to put the indexed columns at the beginning so they can match the schema (since any amount of non-indexed columns may fall between them in sorting order) and the default validator is always being used due to a bad lookup.  These are trivial to solve, however.  v3 fixes these issues and takes the 'tuple of tuples' approach, for better or worse.","20/Oct/11 12:09;brandon.williams;bq. Is there a reason the columns can't at least go into a map? 

Maps only support string keys as far as I know (and also have the unspillable problem.)","20/Oct/11 22:02;petewarden;Thanks Brandon, sounds very promising, excited to try it out.","21/Oct/11 05:28;petewarden;Works like a charm, thanks again Brandon.
","09/Nov/11 20:58;jeromatron;Brandon, Jacob Perkins, and Jeremy (me) had a long discussion about how to address this as well as other issues with CassandraStorage.  We came down to a list of 3-4 things that if implemented would resolve the problem with pig 0.9 as well as make CassandraStorage much more usable.

1. Fix schema so that this ticket's problem is resolved - this goes along with #2.

2. have the default return value from CassandraStorage be (key, column, value) as is thought of for transposing wide rows.  If in the constructor, something like pygmalion's FromCassandraBag is specified, then return that.  See pygmalion's doc for that.

3. Inspect what is passed in for the output and if it conforms to pygmalion's ToCassandraBag - namely a key and a bunch of columns, it will introspect the pig schema and use those pig variable names for the column names.

4. Optionally handle the uniqueness case with some kind of context/random identifier so that multiple CassandraStorage instances writing out don't get confused with the schema.","11/Nov/11 21:31;petewarden;Thanks for digging into this. To help me understand better, do you have a concrete example of how the example script I mentioned would look? With Brandon's current solution it's like this:

all_votes = LOAD 'cassandra://Frap/PhotoVotes' USING CassandraStorage();
album_votes = FILTER all_votes BY ((columns.vote_type.value EQ 'album_like') OR (columns.vote_type.value EQ 'album_dislike'));

This is very usable, and my only concern would be that alternative solutions wouldn't make this common use case as easy.","15/Nov/11 18:16;jeromatron;I think with the pygmalion style of specification, it might require a bit more on the load, but less on the filter.  The columns var wouldn't be needed on the filter, for example.  It would just look like the pygmalion examples except you wouldn't need FromCassandraBag, you would specify that in ""USING CassandraStorage('vote_type')""

So I think it would just be:
all_votes = LOAD 'cassandra://Frap/PhotoVotes' USING CassandraStorage('vote_type'); -- you can add any additional columns
album_votes = FILTER all_votes BY (vote_type EQ 'album_like') OR (vote_type EQ 'album_dislike');

You can add other columns as you need them in the CassandraStorage section.","16/Nov/11 20:14;petewarden;That makes sense, and looks very approachable, thanks. I'll keep an eye on the comments here so I can give it a try as soon as a prototype is available. Thanks again Brandon, Jacob and Jeremy for your hard work on this one.","01/Dec/11 22:28;verbal;I've been following this thread and am actively trying to use CassandraStorage. Before I found this thread, I tried a bunch of stuff on my own with varying degrees of success. My most pressing issue right now is regarding rows with different columns. I do not believe the Pig schema that gets generated supports that. Please advise. Thanks!","19/Jan/12 19:44;brandon.williams;bq. 1. Fix schema so that this ticket's problem is resolved

v4 does this, however it's not quite all of what we want.

bq. 2. have the default return value from CassandraStorage be (key, column, value) as is thought of for transposing wide rows

After thinking about this more, that's the wrong way to approach that, because if you DO want to work within the row, now you have to do an expensive group to get back what we had before -- a nest structure -- where breaking that structure up into (k, c, v) is extremely cheap if that's what you want.  So ultimately, we need to stick with a bag for spillage, and thus keep the existing schema.  v4 does this.

v4 also names the *values* of indexed/validated columns after their name, which is more pygmalion-style, since you'll always want to filter the value, not the name.

The problem, however, is strange parsing problems again:

{noformat}
ERROR 1200: Pig script failed to parse: 
<file foo.pig, line 3, column 7> pig script failed to validate: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1128: Cannot find field owner_id in :tuple(name:chararray,owner_id:chararray)
{noformat}

The seems related to the fact that schema-wise, a bag can only contain a single tuple - but that tuple can then contain any number of items.  Apparently this is only a hard requirement in 0.9 or later, but I tested it up to trunk so it doesn't look like it's going anywhere.

In practice, however, getNext doesn't actually return this 'container' tuple.  If you do you get casting errors.

I'm not really sure how we can fix this, and finding other examples of LoadMetadata implemented with bags are hard to come by.

","31/Jan/12 20:58;brandon.williams;I resolved PIG-2485 as invalid.  You can read the explanation there, but I'll go ahead and summarize: a bag's schema can only contain one tuple because it is assumed that all tuples in the bag have the same schema.  Obviously this won't be true in Cassandra since we allow any column to have any schema that you like.  However, after talking with Dmitriy Ryaboy, I have a plan.  We got good results out of tuple-of-tuples, but this won't work with wide rows.  Another thing it won't work with is small rows where some columns have metadata, and some do not, because when you define a tuple-of-tuples that is a hard constraint; you can't define 4 and then return 20.  So what I propose is that we change the output format to be a tuple-of-tuples for all columns that have metadata, and then a bag with the rest of the columns with a single schema (the default comparator/validator.)  This will work for both static and wide rows, unless you manage to define metadata on so many columns in a wide row that they themselves qualify as wide.

To give an example, let's continue with what Pete started with a slight modification:
{noformat}
create column family PhotoVotes with
comparator = UTF8Type and
column_metadata =
[
{column_name: voter, validation_class: UTF8Type, index_type: KEYS},
{column_name: vote_type, validation_class: UTF8Type},
{column_name: photo_owner, validation_class: UTF8Type, index_type: KEYS},
{column_name: src_big, validation_class: UTF8Type},
{column_name: pid, validation_class: UTF8Type, index_type: KEYS},
{column_name: matched_string, validation_class: UTF8Type},
{column_name: time, validation_class: LongType},
];
{noformat}

Loading this from pig produces a schema like:
(key: bytearray,matched_string: (name: chararray,value: chararray),photo_owner: (name: chararray,value: chararray),pid: (name: chararray,value: chararray),src_big: (name: chararray,value: chararray),time: (name: chararray,value: long),vote_type: (name: chararray,value: chararray),voter: (name: chararray,value: chararray),columns: {(name: chararray,value: bytearray)})

This should allow you do things like:

FILTER rows by vote_type.value eq 'album_like'

Note that the *tuple* is named after the index, and inside the tuple we still have 'name' and 'value'.  This is because if we don't have the name accessible, this is going to be hard to store later (and schema introspection is a bit more magic than I'd care to use.)",31/Jan/12 20:59;brandon.williams;v5 implements this schema.  No work on the putNext side of things yet since I haven't quite decided how to handle that.,31/Jan/12 22:18;brandon.williams;Rebased v5 after CASSANDRA-3251,08/Feb/12 10:35;jalkanen;This issue also affects 1.0.7. Banged my head against the wall for an hour or so before I found this issue. The patch does not apply cleanly on 1.0.7 either :-/,"08/Feb/12 14:01;brandon.williams;New patches build upon v5 and adds output storage to match.  The catch, however, is that either the tuples (indexed/validated columns) or the bag (unknown columns) are optional, so if you are dealing with narrow output, you can just make a tuple like (key, (name,value), (name,value)) and that will work.","13/Feb/12 21:37;brandon.williams;v6 is rebased and contains minor cleanups, smoke_test contains a file to be replayed by the cli and a pig script to exercise loading/storing every cassandra type.","13/Feb/12 22:58;xedin;+1 on the v6 with cleanup patch attached - replaced ArrayList, HashMap with interfaces, added generic description to reader/writer so no more blind casts, changed thrift {Super}Column to user setX(...) methods and removed whitespaces. ",13/Feb/12 23:45;brandon.williams;Committed w/cleanup patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadResponseSerializer.serializedSize() calculation is wrong,CASSANDRA-3390,12528014,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,yangyangyyy,yangyangyyy,20/Oct/11 19:44,12/Mar/19 14:16,13/Mar/19 22:26,24/Oct/11 20:24,0.8.8,1.0.1,,,,,0,,,,,"in ReadResponse.java


the following code

    public long serializedSize(ReadResponse response, int version)
    {
        int size = DBConstants.intSize;
        size += (response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER).remaining();
        size += DBConstants.boolSize;
        if (response.isDigestQuery())
            size += response.digest().remaining();
        else
            size += Row.serializer().serializedSize(response.row(), version);
        return size;
    }


adds the digest size 2 times

this triggers assertion error in at least ReadVerbHandler


",,,,,,,,,,,,,,,,20/Oct/11 19:45;yangyangyyy;3390.patch;https://issues.apache.org/jira/secure/attachment/12499916/3390.patch,23/Oct/11 14:18;jbellis;3390.txt;https://issues.apache.org/jira/secure/attachment/12500365/3390.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-20 20:18:17.103,,,no_permission,,,,,,,,,,,,95644,,,Mon Oct 24 23:10:09 UTC 2011,,,,,,0|i0gjbb:,94559,slebresne,slebresne,,,,,,,,,20/Oct/11 19:45;yangyangyyy;should add the digest size only once,"20/Oct/11 19:57;yangyangyyy;seems there are still other places where the count is wrong

ERROR [ReadStage:129] 2011-10-20 15:54:50,932 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:1
29,5,main]
java.lang.AssertionError: Final buffer length 1778 to accomodate data size of 969 (predicted 888)
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:56)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
","20/Oct/11 20:08;yangyangyyy;it seems that with the remaining error

buffer.getData().length = size*2 + 1

 buffer.getLength() = size + 81

this may be of some help in debugging
","20/Oct/11 20:09;yangyangyyy;the part i fixed is on the isDigest() path,

I'm suspecting that the remaining error is on the non-digest path, since the sizes are pretty large",20/Oct/11 20:18;jbellis;The first one was fixed in CASSANDRA-3373.  You should probably retest on latest trunk to make sure the other isn't fixed too.,"20/Oct/11 20:49;yangyangyyy;tested with latest on 1.0 branch (I checked that it does have the 3373 fix)


still same:


        at java.lang.Thread.run(Thread.java:662)
ERROR [ReadStage:57] 2011-10-20 16:48:50,117 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:57,5,main]
java.lang.AssertionError: Final buffer length 560 to accomodate data size of 360 (predicted 279)
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:56)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [ReadStage:45] 2011-10-20 16:48:50,200 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:45,5,main]
java.lang.AssertionError: Final buffer length 1002 to accomodate data size of 574 (predicted 500)
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:56)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
","20/Oct/11 20:50;yangyangyyy;btw how can I find the github commit corresponding to the 1.0.0 official release?
thanks
Yang","20/Oct/11 21:36;brandon.williams;This is the wrong place to ask, but just switch to the 1.0.0 branch: https://github.com/apache/cassandra/tree/cassandra-1.0.0",21/Oct/11 09:34;slebresne;Do you have an easy way to reproduce that remaining error ? Do you use something specific: super columns ? counters ? expiring columns ?,"21/Oct/11 18:27;yangyangyyy;standard columns
not counter,
yes they are expiring columns


I found this seems to have to do with thread issues, since it works fine with single client ","21/Oct/11 20:00;yangyangyyy;I went through the serialize() and serializedSize() code and they seem to agree, 

now the most likely cause is thread issue,  would the serialize() ever be called from multiple threads?


the buffer.data().length  should not be in the assert check, that discrepancy comes from the result of expand(), really that is an internal thing, should not be checked.


","21/Oct/11 20:47;jbellis;Sure it should be, because that's what tells us if serializedSize has a bug.","21/Oct/11 20:53;yangyangyyy;I mean  

buffer.getData().length  returns the underlying byte[] length, which is always doubled on expansion, so this will not be the same as the actual bytes written. actually this agrees with what we are seeing, getData().length() == (size + 1 ) * 2, because the buffer is doubled on the last write of one byte.

buffer.getLength()  returns the ""count"" var, yes, this should be the same as the serializedSize() calculation

","21/Oct/11 21:05;jbellis;But we pre-allocate the buffer size:

{code}
        int size = (int) serializer.serializedSize(object, version);
        DataOutputBuffer buffer = new DataOutputBuffer(size);
{code}

The point is to avoid copies during buffer re-allocations, so again, this is a good check to have.",21/Oct/11 21:08;jbellis;I added the object being serialized to the assertion failure message in r1187539.  Does that give us anything useful?,"21/Oct/11 21:13;yangyangyyy;I did this too myself

doesn't seem particularly helpful, except for telling us that it's a normal column, with TTL


I'm intrigued by why the difference is always 81 or 77 bytes


ERROR [ReadStage:176] 2011-10-21 14:45:04,099 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:1
76,5,main]
java.lang.AssertionError:  row:Row(key=DecoratedKey(58613415544222752604144955634601649391, 3164323233656134), cf=ColumnFamily(mea
suredSession [0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656135:false:4@1319222
703553!600,0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656137:false:4@1319222703
556!600,0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656139:false:4@1319222703562
!600,0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656162:false:4@1319222703567!60
0,0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656164:false:4@1319222703571!600,0
000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656166:false:4@1319222703584!600,0000
013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656231:false:4@1319222703589!600,0000013
327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656233:false:4@1319222703592!600,]))Final b
uffer length 1152 to accomodate data size of 652 (predicted 575)
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:683)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:56)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
",21/Oct/11 21:31;jbellis;I think you're right about the threading being the cause.  Are you using row cache?,"21/Oct/11 21:35;yangyangyyy;yes, let me disable rowcache and see if it goes away...","21/Oct/11 22:17;yangyangyyy;yeah, disabled row cache, with multiple threads running, no assertion error so far","23/Oct/11 06:57;yangyangyyy;Jonathan: I see what you mean:

the returned CF from cachedRow, in CFS.java:1169 :



    /**
     *  Filter a cached row, which will not be modified by the filter, but may be modified by throwing out
     *  tombstones that are no longer relevant.
     *  The returned column family won't be thread safe.
     */
    ColumnFamily filterColumnFamily(ColumnFamily cached, QueryFilter filter, int gcBefore)
    {

it seems that for some CFs, the buffer.length() was calculated, then another thread did the 
removeDeletedColumns() for expiring columns in filterColumnFamily(), then serializedSize() was calculated again, something like that.

if that is the case, would it actually cause correctness problems, or is it just an annoying discrepancy between 2 reports on the size (which reflect the size at 2 times in history)?



","23/Oct/11 07:39;yangyangyyy;well, expiring column would lead to the size to be bigger than buffer.length(), 
but what we saw is the reverse, so it could be caused because new columns are added to the cached CF.","23/Oct/11 14:18;jbellis;It looks like the culprit is this part:

{code}
                    if (sliceFilter.count >= cached.getColumnCount())
{code}

... then we don't clone the cached CF.  But this is broken as-written; it's possible for new columns to be inserted, increasing the cached's row size after the check.

I think the right thing to do here is jut get rid of this special case.  We could try to preserve it by only skipping thie copy when the requested count == MAXINT, but that's contrary to best-practices (we strongly discourage using MAXINT as the limit).","23/Oct/11 15:51;yangyangyyy;tried the latest patch, seems to work fine. but understandably, the performance is a bit slower due to the extra copy (even with arraybackedSortedColumns )","24/Oct/11 08:14;slebresne;bq.  We could try to preserve it by only skipping thie copy when the requested count == MAXINT

I don't think that would work either because If I understand correctly the problem, it's that when we serialize, we may add a column between when we compute the serialized size and when we do the actual serialization. The request count shouldn't make any difference here (besides, I agree with the contrary to best-practices argument).

I'll note that I think this optimization has always been wrong, because the cf can also change between when the column count is written in the serialization form and the actual write of the columns. If columns are added, we're kind of fine, we'll serialize more columns that advertised but the code will be ok. But it's possible to have a race where we actually remove columns, because a tombstone could be gced by another thread, in which case we could have a EOFException or something like that during deserialization (it's sufficiently unlikely that either nobody ran into it, or got it only once and never again and so didn't report it or something).

Anyway +1 on the patch, but I believe it would be correct to commit to 0.8 too for the above reason.","24/Oct/11 14:26;jbellis;bq. we may add a column between when we compute the serialized size and when we do the actual serialization

Right, I meant that if we were willing to drop the assert and allow the buffer to expand in the rare case of a race here.

I'll commit to 0.8 and 1.0.",24/Oct/11 20:24;jbellis;committed,"24/Oct/11 23:10;hudson;Integrated in Cassandra-0.8 #387 (See [https://builds.apache.org/job/Cassandra-0.8/387/])
    remove incorrect optimization from slice read path
patch by jbellis; reviewed by slebresne for CASSANDRA-3390

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1188353
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in hintedhandoff - 1.0.5,CASSANDRA-3579,12534022,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,ramesh25,ramesh25,06/Dec/11 13:11,12/Mar/19 14:16,13/Mar/19 22:26,11/Jan/12 19:53,1.0.7,,,,,,0,,,,,"We are running a 8 node cassandra cluster running cassandra 1.0.5.
All our CF use leveled compaction.  We ran a test where we did a lot
of inserts for 3 days. After that we started to run tests where some
of the reads could ask for information that was inserted a while back.
In this scenario we are seeing this assertion error in HintedHandoff.

ERROR [HintedHandoff:3] 2011-12-05 15:42:04,324
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[HintedHandoff:3,1,main]
java.lang.RuntimeException: java.lang.RuntimeException:
java.util.concurrent.ExecutionException: java.lang.AssertionError:
originally calculated column size of 470937164 but now it is 470294247
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException:
java.util.concurrent.ExecutionException: java.lang.AssertionError:
originally calculated column size of 470937164 but now it is 470294247
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:330)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
Caused by: java.util.concurrent.ExecutionException:
java.lang.AssertionError: originally calculated column size of
470937164 but now it is 470294247
       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
       at java.util.concurrent.FutureTask.get(FutureTask.java:83)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:326)
       ... 6 more
Caused by: java.lang.AssertionError: originally calculated column size
of 470937164 but now it is 470294247
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
       at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
       at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
       at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       ... 3 more
ERROR [HintedHandoff:3] 2011-12-05 15:42:04,333
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[HintedHandoff:3,1,main]
java.lang.RuntimeException: java.lang.RuntimeException:
java.util.concurrent.ExecutionException: java.lang.AssertionError:
originally calculated column size of 470937164 but now it is 470294247
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException:
java.util.concurrent.ExecutionException: java.lang.AssertionError:
originally calculated column size of 470937164 but now it is 470294247
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:330)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
Caused by: java.util.concurrent.ExecutionException:
java.lang.AssertionError: originally calculated column size of
470937164 but now it is 470294247
       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
       at java.util.concurrent.FutureTask.get(FutureTask.java:83)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:326)
       ... 6 more
Caused by: java.lang.AssertionError: originally calculated column size
of 470937164 but now it is 470294247
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
       at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
       at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
       at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       ... 3 more
ERROR [CompactionExecutor:9931] 2011-12-05 15:42:04,333
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[CompactionExecutor:9931,1,main]
java.lang.AssertionError: originally calculated column size of
470937164 but now it is 470294247
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
       at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
       at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
       at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)","RHEL 6.1 64 bit, 32 GB RAM, 8 GB allocated to JVM, running XFS filesystem for commit/data directories",,,,,,,,,,,,,,,11/Jan/12 19:41;jbellis;3579-fix-text.txt;https://issues.apache.org/jira/secure/attachment/12510227/3579-fix-text.txt,06/Jan/12 22:39;jbellis;3579-v2.txt;https://issues.apache.org/jira/secure/attachment/12509729/3579-v2.txt,04/Jan/12 11:03;slebresne;3579.patch;https://issues.apache.org/jira/secure/attachment/12509396/3579.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-12-07 01:31:26.839,,,no_permission,,,,,,,,,,,,219748,,,Wed Jan 11 19:53:32 UTC 2012,,,,,,0|i0glnb:,94937,jbellis,jbellis,,,,,,,,,"07/Dec/11 01:31;maedhroz;FWIW, I've seen the same error when bringing a node back up after a brief (5 minute) down-time.

ERROR [HintedHandoff:3] 2011-12-06 16:42:58,822 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:3,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:301)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)","07/Dec/11 02:31;jbellis;Caleb, your assertion means you had some hint corruption from 1.0.0 (CASSANDRA-3466).  You should remove your Hint column families from the system/ keyspace.","08/Dec/11 16:44;jbellis;Ramesh, are you using counters?  If so, this may be the same as CASSANDRA-3481.",08/Dec/11 20:37;ramesh25;No we are not using counters.,"20/Dec/11 17:30;cumarana;I ran into this problem as well. I don't know if it is related, but I started to see Gossip errors for the node being down and up constantly even when I'm not writing to Cassandra. Restarting Cassandra fixed the problem.


INFO [CompactionExecutor:648] 2011-12-19 20:41:17,399 CompactionController.java (line 133) Compacting large row system/HintsColumnFamily:
77777777777777777777777777777770 (73427721 bytes) incrementally  INFO [FlushWriter:99] 2011-12-19 20:41:17,410 Memtable.java (line 275) Completed flushing /data/data/system/HintsColumnFamily-hb-141-Data
.db (3022 bytes)
ERROR [CompactionExecutor:648] 2011-12-19 20:41:19,445 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Compaction Executor:648,1,main]
java.lang.AssertionError: originally calculated column size of 66102951 but now it is 66009419
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
        at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [HintedHandoff:1] 2011-12-19 20:41:19,445 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:1,1 ,main]
java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calc ulated column size of 66102951 but now it is 66009419
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column siz e of 66102951 but now it is 66009419
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:330)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
    at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 66102951 but now it is66009419
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:326)
        ... 6 more
Caused by: java.lang.AssertionError: originally calculated column size of 66102951 but now it is 66009419
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
        at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more

","28/Dec/11 18:16;jbellis;The fact that all these stack traces show that the row size got *smaller*, and that HH sets a time to live on its hints starting with CASSANDRA-2034, makes me suspect that somehow columns are expiring in between the first and second LCR passes.  But I don't see anything obviously wrong with how we are using controller.gcBefore to theoretically prevent that.","04/Jan/12 11:03;slebresne;I believe I know what is going on. This is bug with CF having gc_grace == 0 (which hints have). The problem lies in the following line of removeDeleted:
{noformat}
if ((c.isMarkedForDelete() && c.getLocalDeletionTime() <= gcBefore)
    || c.timestamp() <= cf.getMarkedForDeleteAt())
{
    iter.remove();
}
{noformat}
and more precisely the first condition. When that is executed, we have gcbefore <= now but we *can* have gcbefore == now if gc_grace == 0 (and since the resolution is the second, it's not even a very unlikely race).

Now for expiring columns it is further possible that localExpirationTime == gcbefore == now. When that happens, c.isMarkedForDelete() will return false (thus the column will be kept) because this method is defined as:
{noformat}
public boolean isMarkedForDelete()
{
    return (int) (System.currentTimeMillis() / 1000 ) > localExpirationTime;
}
{noformat}

However, during the second pass, now has changed and it is possible that we have now > gcbefore. But since gcbefore == localExpirationTime, this means that isMarkedForDelete() will be true *and* getLocalDeletionTime() <= gcbefore, so the column will be considered tombstone and gcable.

In other word, the current code does not respect the condition that at all time a column is considered gcable only if it is considered deleted.

A rather simple fix consist in changing the condition in removeDeleted to be
{noformat}
if ((c.isMarkedForDelete() && c.getLocalDeletionTime() < gcBefore)
    || c.timestamp() <= cf.getMarkedForDeleteAt())
{
    iter.remove();
}
{noformat}
Note the strict lesser than operator for the first condition. It fixes it because since we know that we always have gcbefore <= now, localExpirationTime < gcBefore always imply that isMarkedForDelete() is true.

Attached patch to make that fix (with hopefully useful comments).
","05/Jan/12 23:30;jbellis;I think this change is an improvement, and I also think it's a good thing to make the semantics of gcBefore match what its name implies, i.e., we collect tombstones from *before* that epoch and not before-or-equal-to, but:

- It's slightly more intuitive for 0 gcgs to mean ""this is always gc'd immediately,"" not ""gc'd after one second.""  In practice, I can't see how this could matter, but it does bother my OCD. :)
- A search for < gcBefore, <= gcBefore, > gcBefore, >= gcBefore shows some of each.  We're not consistent in whether we treat it as before or before-or-equal-to, and we need to be.
- The isMarkedForDelete behavior you point out scares me, because that's going to change during the two LCR passes as well.  I suspect that's going to cause subtle bugs if we don't create a fixed point in time for which we treat a compaction as happening.  (Maybe just replace controller.gcBefore with controller.compactionTime and derive gcBefore from that as a method, instead of a field.)","05/Jan/12 23:32;jbellis;We have the same problem in 0.7 and 0.8 for any CF with 0 gcgs.  For those versions IMO we should just say ""don't do that.""
","06/Jan/12 08:57;slebresne;bq. It's slightly more intuitive for 0 gcgs to mean ""this is always gc'd immediately,""

True, though gc is more a an internal detail in that it doesn't affect whether a column is returned by queries or not. It only affect whether it will be removed by a compaction, but that is already so dependent of other timings than it cannot matter. Or to put it otherwise, that one doesn't bother my own OCD. But it doesn't really bother me either if gcBefore means before-or-equal so ...

bq. A search for < gcBefore, <= gcBefore, > gcBefore, >= gcBefore shows some of each.

One of the goal of this patch was to actually adds consistency and I though I had eliminated all '<=' but maybe I missed one. I forgot to search for '>' and '>=' however. I'll fix.

bq. The isMarkedForDelete behavior you point out scares me, because that's going to change during the two LCR passes as well. I suspect that's going to cause subtle bugs if we don't create a fixed point in time for which we treat a compaction as happening. (Maybe just replace controller.gcBefore with controller.compactionTime and derive gcBefore from that as a method, instead of a field.)

It's an option, and it was even my first idea. But it means that isMarkedForDelete will take a timestamp, and it's called a lot in the code base. That and the other related change, I'd be tempted to commit the patch as is for 1.0 and maybe do the switch to 'compactionTime' for 1.1 onward.
","06/Jan/12 22:30;jbellis;bq. I'd be tempted to commit the patch as is for 1.0 and maybe do the switch to 'compactionTime' for 1.1 onward

All right, for 1.0 let's just audit for > >= gcBefore correctness.

bq. isMarkedForDelete will take a timestamp, and it's called a lot in the code base

The more I think about it the less I like ""boolean isMarkedForDelete()""...  I like ""getExpirationTime()"" a lot better.  That allows combining iMFD and getLDT into a single method (that can return Integer.MAX_VALUE for Column) that doesn't need any parameters, but doesn't change value during compaction.",06/Jan/12 22:39;jbellis;v2 changes two >= instances to >.,"09/Jan/12 16:58;slebresne;+1 on v2, committed on 1.0 branch. How do we merge --record-only with git again?

bq. The more I think about it the less I like ""boolean isMarkedForDelete()""... I like ""getExpirationTime()"" a lot better.

Agreed, that's much cleaner. I'll do that for 1.1.","09/Jan/12 17:12;jbellis;bq. How do we merge --record-only with git again

The closest I found is {{git merge X --strategy=ours}}.",10/Jan/12 17:37;jbellis;Created CASSANDRA-3716 for the 1.1 followup,11/Jan/12 19:09;jbellis;this is causing the SystemTableTest failure mentioned in CASSANDRA-3727,"11/Jan/12 19:41;jbellis;Switching from <= to < means that getColumnFamily will now include tombstones for just-deleted column with gcgs == 0.

For clients this is harmless (since we drop tombstones in the coordinator after RR), but internal code needs to be more careful.  patch attached to have loadTokens expunge all tombstones.",11/Jan/12 19:48;slebresne;+1,11/Jan/12 19:53;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CounterColumnFamily Compaction error (ArrayIndexOutOfBoundsException),CASSANDRA-3514,12532195,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,efalcao,efalcao,21/Nov/11 19:13,12/Mar/19 14:16,13/Mar/19 22:26,23/Nov/11 08:04,0.8.8,1.0.4,,,,,0,compaction,,,,"On a single node, I'm seeing the following error when trying to compact a CounterColumnFamily. This appears to have started with version 1.0.3.

nodetool -h localhost compact TRProd MetricsAllTime
Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:250)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1471)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1523)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ArrayIndexOutOfBoundsException
	at org.apache.cassandra.utils.ByteBufferUtil.arrayCopy(ByteBufferUtil.java:292)
	at org.apache.cassandra.db.context.CounterContext$ContextState.copyTo(CounterContext.java:792)
	at org.apache.cassandra.db.context.CounterContext.removeOldShards(CounterContext.java:709)
	at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:260)
	at org.apache.cassandra.db.CounterColumn.mergeAndRemoveOldShards(CounterColumn.java:306)
	at org.apache.cassandra.db.CounterColumn.mergeAndRemoveOldShards(CounterColumn.java:271)
	at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:86)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:102)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:133)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:102)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:87)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:116)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:99)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:172)
	at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:277)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more",,,,,,,,,,,,,,,,22/Nov/11 19:05;slebresne;3514.patch;https://issues.apache.org/jira/secure/attachment/12504782/3514.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-22 19:05:26.072,,,no_permission,,,,,,,,,,,,217931,,,Wed Nov 23 08:15:02 UTC 2011,,,,,,0|i0gkuf:,94807,yukim,yukim,,,,,,,,,22/Nov/11 01:57;efalcao;This also just started happening on another node (without user-triggered major compaction). Not sure what's causing this (old 0.8.* counter data perhaps?),"22/Nov/11 19:05;slebresne;That is a bug, attaching patch to fix. Note that there is actually two bugs, so the patch fix both and adds unit tests for each of them.","22/Nov/11 19:07;slebresne;Note that this is due to CASSANDRA-3178 so this affects the 0.8 branch, but not any released version.",23/Nov/11 01:05;yukim;+1,"23/Nov/11 08:04;slebresne;Committed, thanks","23/Nov/11 08:15;hudson;Integrated in Cassandra-0.8 #402 (See [https://builds.apache.org/job/Cassandra-0.8/402/])
    Fix array out of bounds error in counter shard removal
patch by slebresne; reviewed by yukim for CASSANDRA-3514

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1205316
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/context/CounterContext.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/context/CounterContextTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion error during bootstraping cassandra,CASSANDRA-3536,12532941,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,ramesh25,ramesh25,28/Nov/11 17:44,12/Mar/19 14:16,13/Mar/19 22:26,02/Dec/11 15:47,1.0.6,,,,,,1,compaction,,,," I have a 3 node cassandra cluster. I have RF set to 3 and do reads
and writes using QUORUM.

Here is my initial ring configuration

[root@CAP4-CNode1 ~]# /root/cassandra/bin/nodetool -h localhost ring
Address         DC          Rack        Status State   Load
Owns    Token

       113427455640312821154458202477256070484
10.19.104.11    datacenter1 rack1       Up     Normal  1.66 GB
33.33%  0
10.19.104.12    datacenter1 rack1       Up     Normal  1.06 GB
33.33%  56713727820156410577229101238628035242
10.19.104.13    datacenter1 rack1       Up     Normal  1.61 GB
33.33%  113427455640312821154458202477256070484

I want to add 10.19.104.14 to the cluster.

I edited the 10.19.104.14 cassandra.yaml file and set the token to
127605887595351923798765477786913079296 and set auto_bootstrap to
true.

When I started cassandra I am getting Assertion Error.  

thanks
Ramesh




[root@CAP4-CNode4 cassandra]#  INFO 10:29:46,093 Logging initialized
 INFO 10:29:46,099 JVM vendor/version: Java HotSpot(TM) 64-Bit Server
VM/1.6.0_25
 INFO 10:29:46,100 Heap size: 8304721920/8304721920
 INFO 10:29:46,100 Classpath:
bin/../conf:bin/../build/classes/main:bin/../build/classes/thrift:bin/../lib/antlr-3.2.jar:bin/../lib/apache-cassandra-1.0.2.jar:bin/../lib/apache-cassandra-clientutil-1.0.2.jar:bin/../lib/apache-cassandra-thrift-1.0.2.jar:bin/../lib/avro-1.4.0-fixes.jar:bin/../lib/avro-1.4.0-sources-fixes.jar:bin/../lib/commons-cli-1.1.jar:bin/../lib/commons-codec-1.2.jar:bin/../lib/commons-lang-2.4.jar:bin/../lib/compress-lzf-0.8.4.jar:bin/../lib/concurrentlinkedhashmap-lru-1.2.jar:bin/../lib/guava-r08.jar:bin/../lib/high-scale-lib-1.1.2.jar:bin/../lib/jackson-core-asl-1.4.0.jar:bin/../lib/jackson-mapper-asl-1.4.0.jar:bin/../lib/jamm-0.2.5.jar:bin/../lib/jline-0.9.94.jar:bin/../lib/jna.jar:bin/../lib/json-simple-1.1.jar:bin/../lib/libthrift-0.6.jar:bin/../lib/log4j-1.2.16.jar:bin/../lib/mx4j-examples.jar:bin/../lib/mx4j-impl.jar:bin/../lib/mx4j.jar:bin/../lib/mx4j-jmx.jar:bin/../lib/mx4j-remote.jar:bin/../lib/mx4j-rimpl.jar:bin/../lib/mx4j-rjmx.jar:bin/../lib/mx4j-tools.jar:bin/../lib/servlet-api-2.5-20081211.jar:bin/../lib/slf4j-api-1.6.1.jar:bin/../lib/slf4j-log4j12-1.6.1.jar:bin/../lib/snakeyaml-1.6.jar:bin/../lib/snappy-java-1.0.4.1.jar:bin/../lib/jamm-0.2.5.jar
 INFO 10:29:48,713 JNA mlockall successful
 INFO 10:29:48,726 Loading settings from
file:/root/apache-cassandra-1.0.2/conf/cassandra.yaml
 INFO 10:29:48,883 DiskAccessMode 'auto' determined to be mmap,
indexAccessMode is mmap
 INFO 10:29:48,898 Global memtable threshold is enabled at 2640MB
 INFO 10:29:49,203 Couldn't detect any schema definitions in local storage.
 INFO 10:29:49,204 Found table data in data directories. Consider
using the CLI to define your schema.
 INFO 10:29:49,220 Creating new commitlog segment
/var/lib/cassandra/commitlog/CommitLog-1321979389220.log
 INFO 10:29:49,227 No commitlog files found; skipping replay
 INFO 10:29:49,230 Cassandra version: 1.0.2
 INFO 10:29:49,230 Thrift API version: 19.18.0
 INFO 10:29:49,230 Loading persisted ring state
 INFO 10:29:49,235 Starting up server gossip
 INFO 10:29:49,259 Enqueuing flush of
Memtable-LocationInfo@122130810(192/240 serialized/live bytes, 4 ops)
 INFO 10:29:49,260 Writing Memtable-LocationInfo@122130810(192/240
serialized/live bytes, 4 ops)
 INFO 10:29:49,317 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-1-Data.db (300 bytes)
 INFO 10:29:49,340 Starting Messaging Service on port 7000
 INFO 10:29:49,349 JOINING: waiting for ring and schema information
 INFO 10:29:50,759 Applying migration
4b0e20f0-1511-11e1-0000-c11bc95834d7 Add keyspace: MSA, rep
strategy:SimpleStrategy{}, durable_writes: true
 INFO 10:29:50,761 Enqueuing flush of
Memtable-Migrations@1507565381(6744/8430 serialized/live bytes, 1 ops)
 INFO 10:29:50,761 Writing Memtable-Migrations@1507565381(6744/8430
serialized/live bytes, 1 ops)
 INFO 10:29:50,761 Enqueuing flush of
Memtable-Schema@1498835564(2889/3611 serialized/live bytes, 3 ops)
 INFO 10:29:50,776 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-1-Data.db (6808 bytes)
 INFO 10:29:50,777 Writing Memtable-Schema@1498835564(2889/3611
serialized/live bytes, 3 ops)
 INFO 10:29:50,797 Completed flushing
/var/lib/cassandra/data/system/Schema-h-1-Data.db (3039 bytes)
 INFO 10:29:50,814 Applying migration
4b6f2cb0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@1639d811[cfId=1000,ksName=MSA,cfName=modseq,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=5000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@2f984f7d,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:50,815 Enqueuing flush of
Memtable-Migrations@948613108(7482/9352 serialized/live bytes, 1 ops)
 INFO 10:29:50,816 Writing Memtable-Migrations@948613108(7482/9352
serialized/live bytes, 1 ops)
 INFO 10:29:50,816 Enqueuing flush of
Memtable-Schema@421910828(3294/4117 serialized/live bytes, 3 ops)
 INFO 10:29:50,831 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-2-Data.db (7546 bytes)
 INFO 10:29:50,832 Writing Memtable-Schema@421910828(3294/4117
serialized/live bytes, 3 ops)
 INFO 10:29:50,846 Completed flushing
/var/lib/cassandra/data/system/Schema-h-2-Data.db (3444 bytes)
 INFO 10:29:50,854 Applying migration
4b8c9fc0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@1bd97d0d[cfId=1001,ksName=MSA,cfName=msgid,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@63a0eec3,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO 10:29:50,855 Enqueuing flush of
Memtable-Migrations@1520138062(7750/9687 serialized/live bytes, 1 ops)
 INFO 10:29:50,856 Writing Memtable-Migrations@1520138062(7750/9687
serialized/live bytes, 1 ops)
 INFO 10:29:50,856 Enqueuing flush of
Memtable-Schema@347459675(3630/4537 serialized/live bytes, 3 ops)
 INFO 10:29:50,878 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-3-Data.db (7814 bytes)
 INFO 10:29:50,879 Writing Memtable-Schema@347459675(3630/4537
serialized/live bytes, 3 ops)
 INFO 10:29:50,894 Completed flushing
/var/lib/cassandra/data/system/Schema-h-3-Data.db (3780 bytes)
 INFO 10:29:50,900 Applying migration
4ba1ae60-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@6a095b8a[cfId=1002,ksName=MSA,cfName=participants,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@c58f769,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO 10:29:50,900 Enqueuing flush of
Memtable-Migrations@618337492(8194/10242 serialized/live bytes, 1 ops)
 INFO 10:29:50,901 Writing Memtable-Migrations@618337492(8194/10242
serialized/live bytes, 1 ops)
 INFO 10:29:50,902 Enqueuing flush of
Memtable-Schema@724860211(4020/5025 serialized/live bytes, 3 ops)
 INFO 10:29:50,917 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-4-Data.db (8258 bytes)
 INFO 10:29:50,918 Writing Memtable-Schema@724860211(4020/5025
serialized/live bytes, 3 ops)
 INFO 10:29:50,925 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-3-Data.db')]
 INFO 10:29:50,934 Completed flushing
/var/lib/cassandra/data/system/Schema-h-4-Data.db (4170 bytes)
 INFO 10:29:50,935 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-3-Data.db')]
 INFO 10:29:50,940 Applying migration
4bb4e840-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@318c69a9[cfId=1003,ksName=MSA,cfName=subinfo,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=5000.0,keyCacheSize=5000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=14400,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@796cefa8,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:50,941 Enqueuing flush of
Memtable-Migrations@1682081063(8618/10772 serialized/live bytes, 1
ops)
 INFO 10:29:50,941 Writing Memtable-Migrations@1682081063(8618/10772
serialized/live bytes, 1 ops)
 INFO 10:29:50,941 Enqueuing flush of
Memtable-Schema@1083461053(4427/5533 serialized/live bytes, 3 ops)
 INFO 10:29:50,977 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-5-Data.db (8682 bytes)
 INFO 10:29:50,978 Writing Memtable-Schema@1083461053(4427/5533
serialized/live bytes, 3 ops)
 INFO 10:29:50,991 Compacted to
[/var/lib/cassandra/data/system/Schema-h-5-Data.db,].  14,433 to
14,106 (~97% of original) bytes for 5 keys at 0.269051MB/s.  Time:
50ms.
 INFO 10:29:50,995 Completed flushing
/var/lib/cassandra/data/system/Schema-h-7-Data.db (4577 bytes)
 INFO 10:29:51,000 Applying migration
4bc6e9a0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@20b00ec2[cfId=1004,ksName=MSA,cfName=transactions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@698f352,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:51,001 Enqueuing flush of
Memtable-Migrations@596545504(9027/11283 serialized/live bytes, 1 ops)
 INFO 10:29:51,002 Writing Memtable-Migrations@596545504(9027/11283
serialized/live bytes, 1 ops)
 INFO 10:29:51,003 Enqueuing flush of
Memtable-Schema@1686621532(4835/6043 serialized/live bytes, 3 ops)
 INFO 10:29:51,029 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-7-Data.db (9091 bytes)
 INFO 10:29:51,029 Writing Memtable-Schema@1686621532(4835/6043
serialized/live bytes, 3 ops)
 INFO 10:29:51,031 Compacted to
[/var/lib/cassandra/data/system/Migrations-h-6-Data.db,].  30,426 to
30,234 (~99% of original) bytes for 1 keys at 0.272013MB/s.  Time:
106ms.
 INFO 10:29:51,044 Completed flushing
/var/lib/cassandra/data/system/Schema-h-8-Data.db (4985 bytes)
 INFO 10:29:51,049 Applying migration
4bd76460-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@4ab4faeb[cfId=1005,ksName=MSA,cfName=uid,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1500000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@2fc5809e,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:51,050 Enqueuing flush of
Memtable-Migrations@1333730706(9421/11776 serialized/live bytes, 1
ops)
 INFO 10:29:51,050 Writing Memtable-Migrations@1333730706(9421/11776
serialized/live bytes, 1 ops)
 INFO 10:29:51,051 Enqueuing flush of
Memtable-Schema@577668356(5236/6545 serialized/live bytes, 3 ops)
 INFO 10:29:51,065 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-9-Data.db (9485 bytes)
 INFO 10:29:51,066 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-6-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-9-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-7-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-5-Data.db')]
 INFO 10:29:51,066 Writing Memtable-Schema@577668356(5236/6545
serialized/live bytes, 3 ops)
 INFO 10:29:51,081 Completed flushing
/var/lib/cassandra/data/system/Schema-h-9-Data.db (5386 bytes)
 INFO 10:29:51,083 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-5-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-9-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-8-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-7-Data.db')]
 INFO 10:29:51,114 Compacted to
[/var/lib/cassandra/data/system/Schema-h-10-Data.db,].  29,054 to
28,727 (~98% of original) bytes for 8 keys at 0.913207MB/s.  Time:
30ms.
 INFO 10:29:51,144 Compacted to
[/var/lib/cassandra/data/system/Migrations-h-10-Data.db,].  57,492 to
57,300 (~99% of original) bytes for 1 keys at 0.700584MB/s.  Time:
78ms.
 INFO 10:29:51,410 Node /10.19.104.13 is now part of the cluster
 INFO 10:29:51,412 InetAddress /10.19.104.13 is now UP
 INFO 10:29:51,414 Enqueuing flush of
Memtable-LocationInfo@709342045(35/43 serialized/live bytes, 1 ops)
 INFO 10:29:51,415 Writing Memtable-LocationInfo@709342045(35/43
serialized/live bytes, 1 ops)
 INFO 10:29:51,428 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db (89 bytes)
 INFO 10:29:51,439 Node /10.19.104.12 is now part of the cluster
 INFO 10:29:51,439 InetAddress /10.19.104.12 is now UP
 INFO 10:29:51,441 Enqueuing flush of
Memtable-LocationInfo@1292444743(35/43 serialized/live bytes, 1 ops)
 INFO 10:29:51,441 Writing Memtable-LocationInfo@1292444743(35/43
serialized/live bytes, 1 ops)
 INFO 10:29:51,455 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db (89 bytes)
 INFO 10:29:51,456 Node /10.19.104.11 is now part of the cluster
 INFO 10:29:51,457 InetAddress /10.19.104.11 is now UP
 INFO 10:29:51,459 Enqueuing flush of
Memtable-LocationInfo@1891328597(20/25 serialized/live bytes, 1 ops)
 INFO 10:29:51,459 Writing Memtable-LocationInfo@1891328597(20/25
serialized/live bytes, 1 ops)
 INFO 10:29:51,471 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-4-Data.db (74 bytes)
 INFO 10:29:51,473 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db')]
 INFO 10:29:51,497 Compacted to
[/var/lib/cassandra/data/system/LocationInfo-h-5-Data.db,].  552 to
444 (~80% of original) bytes for 3 keys at 0.018410MB/s.  Time: 23ms.
 INFO 10:30:19,349 JOINING: getting bootstrap token
 INFO 10:30:19,352 Enqueuing flush of
Memtable-LocationInfo@225265367(36/45 serialized/live bytes, 1 ops)
 INFO 10:30:19,353 Writing Memtable-LocationInfo@225265367(36/45
serialized/live bytes, 1 ops)
 INFO 10:30:19,364 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-7-Data.db (87 bytes)
 INFO 10:30:19,374 JOINING: sleeping 30000 ms for pending range setup
 INFO 10:30:49,375 JOINING: Starting to bootstrap...
ERROR 10:31:13,444 Fatal exception in thread Thread[Thread-49,5,main]
java.lang.AssertionError
       at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:178)
       at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
       at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:466)
       at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:275)
       at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
       at org.apache.cassandra.db.DataTracker.addStreamedSSTable(DataTracker.java:242)
       at org.apache.cassandra.db.ColumnFamilyStore.addSSTable(ColumnFamilyStore.java:922)
       at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:141)
       at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:102)
       at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
       at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)","RHEL6 linux x86-64 guest on ESXi 5.0 (host),  kernel: 2.6.32-71.24.1.el6.x86_64, 8 cpu, 20GB RAM",,,,,,,,,,,,,,,02/Dec/11 05:30;jbellis;3536.txt;https://issues.apache.org/jira/secure/attachment/12505850/3536.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-29 18:28:19.93,,,no_permission,,,,,,,,,,,,218673,,,Fri Dec 02 15:47:18 UTC 2011,,,,,,0|i0gl47:,94851,bcoverston,bcoverston,,,,,,,,,"29/Nov/11 18:28;eparusel;I may have a similar, or the same problem.

On a node where I was running repair (with no other load), I am seeing these exceptions:

ERROR [Thread-90] 2011-11-29 17:44:22,753 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Thread-90,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:178)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:481)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:275)
        at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
        at org.apache.cassandra.db.DataTracker.addStreamedSSTable(DataTracker.java:242)
        at org.apache.cassandra.db.ColumnFamilyStore.addSSTable(ColumnFamilyStore.java:920)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:141)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:103)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)","02/Dec/11 05:30;jbellis;LeveledManifest.promote only really knows how to handle compaction -- swapping out one set of sstables, for another.  When adding a new sstable, we need to call LM.add instead.  Refactored DataTracker to do this correctly for streaming (and load-new-sstables-from-jmx); in the process, got rid of addStreamedSSTable (becomes just addSSTables(singletonlist) and added addInitialSSTables, to add to the tracker with no notifications (since Strategy creation loads the manifest separately).",02/Dec/11 05:52;bcoverston;+1,"02/Dec/11 15:32;joelastpass;Thanks Jonathan, the patch works for me.",02/Dec/11 15:47;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Header class not thread safe, but mutated by multiple threads",CASSANDRA-3530,12532664,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,sgbridges,sgbridges,25/Nov/11 00:11,12/Mar/19 14:16,13/Mar/19 22:26,25/Nov/11 09:55,1.0.4,,,,,,0,,,,,"With Cassandra 1.0.3 we are getting exceptions like,

Fatal exception in thread Thread[WRITE-/xx.xx.xx.xx,5,main]java.util.ConcurrentModificationException        
        at java.util.Hashtable$Enumerator.next(Unknown Source)
        at org.apache.cassandra.net.Header.serializedSize(Header.java:97)        
        at org.apache.cassandra.net.OutboundTcpConnection.messageLength(OutboundTcpConnection.java:164)
        at org.apache.cassandra.net.OutboundTcpConnection.write(OutboundTcpConnection.java:154)        
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:115)        
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:94)

and,

ERROR [WRITE-/xx.xx.xx.xx] 2011-11-24 22:08:28,981 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[WRITE-/10.30.12.79,5,main]java.lang.NullPointerException        
        at org.apache.cassandra.net.Header.serializedSize(Header.java:101)
        at org.apache.cassandra.net.OutboundTcpConnection.messageLength(OutboundTcpConnection.java:164)
        at org.apache.cassandra.net.OutboundTcpConnection.write(OutboundTcpConnection.java:154)
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:115)        
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:94)

It looks like Header is not thread safe, but the same header instance is modified concurrently while being sent to several threads in StorageProxy.sendMessages. 

This bug eventually causes the node to OOM, as it kills the OutboundTcpConnection thread, which means nothing is dequeing from queue.
",,,,,,,,,,,,,,,,25/Nov/11 07:58;jbellis;3530-0.8.txt;https://issues.apache.org/jira/secure/attachment/12505074/3530-0.8.txt,25/Nov/11 07:43;jbellis;3530-v2.txt;https://issues.apache.org/jira/secure/attachment/12505073/3530-v2.txt,25/Nov/11 00:15;sgbridges;CASSANDRA-3530.patch;https://issues.apache.org/jira/secure/attachment/12505045/CASSANDRA-3530.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-11-25 03:38:20.88,,,no_permission,,,,,,,,,,,,218397,,,Fri Nov 25 15:53:13 UTC 2011,,,,,,0|i0gl1j:,94839,slebresne,slebresne,,,,,,,,,"25/Nov/11 00:15;sgbridges;Patch makes Header nearly immutable.  setDetails and removeDetails now return a copy of the Header with the modifications, rather than modifying the original.",25/Nov/11 03:38;jbellis;Are you running in a multi-DC environment?,"25/Nov/11 03:39;jbellis;bq. Patch makes Header nearly immutable

What are the changes in OutboundTcpConnection trying to do?","25/Nov/11 04:22;sbridges;We are running in a multi dc environment.

The changes in OutboundTcpConnection prevent the thread from dieing in case of a RuntimeException.  When the thread dies the node eventually runs out of memory as the queue never gets emptied.","25/Nov/11 07:43;jbellis;v2 attached (against 1.0 branch):

- simplifies OTC change to just catch Exception in existing try/catch in writeConnected
- some style updates to Header/Message changes, but functionality is unchanged
- updated StorageProxy to only need one header copy instead of the horribly inefficient process used previously",25/Nov/11 07:49;jbellis;This affects 0.8 as well (and would have affected 0.7 if not for CASSANDRA-3472).,"25/Nov/11 07:58;jbellis;version of v2 for 0.8.  Omitted the OTC change since in 0.8 it's just shoving a bytebuffer over the wire, there's not really any non-IOExceptions to worry about.","25/Nov/11 10:17;hudson;Integrated in Cassandra-0.8 #404 (See [https://builds.apache.org/job/Cassandra-0.8/404/])
    avoid race in OutboundTcpConnection in multi-DC setups
patch by jbellis; reviewed by slebresne for CASSANDRA-3530

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1206098
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/Header.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/Message.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
","25/Nov/11 14:44;jbellis;Looking at this in the morning, I think I was wrong about it affecting 0.8, since all the Message wrangling (including the Message -> byte[] conversion in sendOneWay) happens on the StorageProxy thread.  I think we should revert from 0.8.8.","25/Nov/11 15:33;slebresne;There is no StorageProxy thread per-se, is there? Why wouldn't 0.8 be subject to the same concurrency bug as reported on that issue?","25/Nov/11 15:43;jbellis;Because in 0.8 there is only one thread touching any given Message in StorageProxy + MessagingService.  Once OTC gets it, MS has already turned it into a byte[] (which OTC then copies to its Socket buffer).

To avoid the unnecessary intermediate byte[], for 1.0 we switch to OTC getting the Message objects.  So in the multi-DC case you can have a 2nd thread (the OTC one) sending a Message, while the SP thread updates its Header.","25/Nov/11 15:48;slebresne;Ok. I'm fine reverting this then, though I would also be fine keeping it in. I prefer the new copying methods, and it does improve the code a bit in StorageProxy, and I don't see many chance for this to introduce new bugs (but obviously there is always one).","25/Nov/11 15:53;jbellis;I'm going to revert then; if I hadn't incorrectly thought this bug affected 0.8, it would be clear we shouldn't backport it just to clean things up a bit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"During repair, ""incorrect data size"" & ""Connection reset"" errors. Repair unable to complete.",CASSANDRA-3481,12530929,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,efalcao,efalcao,09/Nov/11 21:31,12/Mar/19 14:16,13/Mar/19 22:26,16/Nov/11 13:39,1.0.3,,,,,,0,connection,repair,,,"This has been happening since 1.0.2. I wasn't on 1.0 for very long but I'm fairly certain repair was working ok. Repair worked decently for me in 0.8 (data bloat sucked). All my SSTables are version h.

On one node:

java.lang.AssertionError: incorrect row data size 596045 written to /mnt/cassandra/data/TRProd/Metrics1m-tmp-h-25036-Data.db; correct is 586675
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:253)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:146)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:87)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)

On the other node:

4999 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-24953-Data.db sections=1707 progress=0/1513497639 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25000-Data.db sections=635 progress=0/53400713 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25002-Data.db sections=570 progress=0/709993 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25003-Data.db sections=550 progress=0/449498 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25005-Data.db sections=516 progress=0/316301 - 0%], 6 sstables.
 INFO [StreamStage:1] 2011-11-09 19:45:22,795 StreamOutSession.java (line 203) Streaming to /10.38.69.192
ERROR [Streaming:1] 2011-11-09 19:47:47,964 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Streaming:1,1,main]
java.lang.RuntimeException: java.net.SocketException: Connection reset
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
	at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:133)
	at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
	at com.ning.compress.lzf.LZFOutputStream.write(LZFOutputStream.java:97)
	at org.apache.cassandra.streaming.FileStreamTask.write(FileStreamTask.java:181)
	at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:145)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
ERROR [Streaming:1] 2011-11-09 19:47:47,970 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Streaming:1,1,main]
java.lang.RuntimeException: java.net.SocketException: Connection reset
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
	at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:133)
	at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
	at com.ning.compress.lzf.LZFOutputStream.write(LZFOutputStream.java:97)
	at org.apache.cassandra.streaming.FileStreamTask.write(FileStreamTask.java:181)
	at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:145)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
",,,,,,,,,,,,,,,,10/Nov/11 18:19;slebresne;3481-v2.patch;https://issues.apache.org/jira/secure/attachment/12503265/3481-v2.patch,10/Nov/11 12:56;slebresne;3481.patch;https://issues.apache.org/jira/secure/attachment/12503201/3481.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-10 08:53:57.443,,,no_permission,,,,,,,,,,,,216667,,,Wed Nov 16 13:39:47 UTC 2011,,,,,,0|i0gkfj:,94740,jbellis,jbellis,,,,,,,,,"10/Nov/11 08:53;slebresne;What is in this 'Metrics' column family? Is it normal columns, some expiring ones, super columns, counters?","10/Nov/11 12:56;slebresne;We do have a problem with counters that leads to that exact problem.

During streaming, we must ensure that whatever we deserialize from the wire has the same size once reserialized. For counters, whenever we should have clean deltas, instead we just 'mark' the delta to be clean in a way that doesn't change the size and let them be clean later. Problem is, if you stream a file that still have those 'marked' delta, it will remove them, changing the size of the data and thus triggering the exception at end.

Fixing this require knowing at deserialization that we are in this 'preserve the size' mode. Attaching a patch for that. Instead of adding a new flag to deserialization, I replaced 'fromRemote' boolean by an enum flag. The patch includes a unit test.
",10/Nov/11 14:10;jbellis;is this 1.x only or 0.8 too?,"10/Nov/11 14:16;slebresne;It's 1.x only as far as the bug is concerned (it's really linked to single-pass streaming). That being said, the bulk of the patch, i.e, replacing the fromRemote boolean by a Flag, could be backported in 0.8 for purpose of making future merge easier. But I'm skeptical this is worth it.","10/Nov/11 15:27;efalcao;Metrics1m is a Counter CF. I originally tried CounterColumns with TTL's but was advised against it. They have no expiration.

Let me know if I can be any more help. I might be able to try the patch a bit later in the day.","10/Nov/11 18:03;jbellis;Shouldn't IncomingStreamReader use PRESERVE_SIZE instead of FROM_REMOTE?

What does the commented-out test do, and why is it commented out?

nit: this comment needs updating:

{noformat}
             // deserialize column with fromRemote false, in order to keep size of streamed column
{noformat}
","10/Nov/11 18:19;slebresne;v2 attached

bq. Shouldn't IncomingStreamReader use PRESERVE_SIZE instead of FROM_REMOTE?

That would work too but we don't need to, because we won't echo the columns like in the other branch. So it's better to use FROM_REMOTE and have the counter delta cleaned right away rather than later (as this is slightly more efficient). I added a comment.

bq. What does the commented-out test do, and why is it commented out?

Oups, attached the wrong patch. I just copied a previous test but commented it out when I realized it was simpler to just add a few lines into the existing test. v2 removes the commented-out one.
",10/Nov/11 18:27;jbellis;+1,16/Nov/11 13:39;slebresne;Forgot to close this one but it's been committed already.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recursion bug in CollationController,CASSANDRA-3491,12531347,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,tjake,tjake,14/Nov/11 17:54,12/Mar/19 14:16,13/Mar/19 22:26,14/Nov/11 21:30,1.0.3,,,,,,0,,,,,"The following stack trace seems to indicate a recursion bug in CollationController

Where the stats collection mutation is itself having stats collected on and so fourth

http://pastebin.com/raw.php?i=35Rt7ryB",,,,,,,,,,,,,,,,14/Nov/11 18:10;jbellis;3491.txt;https://issues.apache.org/jira/secure/attachment/12503646/3491.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-14 18:10:56.581,,,no_permission,,,,,,,,,,,,217083,,,Mon Nov 14 21:30:20 UTC 2011,,,,,,0|i0gkk7:,94761,slebresne,slebresne,,,,,,,,,14/Nov/11 18:10;jbellis;Patch to revert CASSANDRA-2503.  Clearly putting that in 1.0.x was a mistake (mine).,14/Nov/11 21:03;slebresne;+1,14/Nov/11 21:30;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load from `nodetool ring` does not update after cleanup.,CASSANDRA-3496,12531527,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,cywjackson,bcoverston,bcoverston,15/Nov/11 20:18,12/Mar/19 14:16,13/Mar/19 22:26,16/Nov/11 18:10,1.0.4,,,,,,0,,,,,"Repro:
Bring up a node.

Insert 1M rows:
127.0.0.1       datacenter1 rack1       Up     Normal  406.92 MB       100.00% 77747037169725419723056812679314618801
(Already looks wrong, 406.92 is higher than I'm used to seeing from a single run of stress)

Bootstrap a second node into the cluster:

162877269496252595336256012556853953561
127.0.0.1       datacenter1 rack1       Up     Normal  407.03 MB       49.96%  77747037169725419723056812679314618801
127.0.0.2       datacenter1 rack1       Up     Normal  157.91 MB       50.04%  162877269496252595336256012556853953561

Cleanup
162877269496252595336256012556853953561
127.0.0.1       datacenter1 rack1       Up     Normal  551.2 MB       49.96%  77747037169725419723056812679314618801
127.0.0.2       datacenter1 rack1       Up     Normal  157.91 MB       50.04%  162877269496252595336256012556853953561

Looks like each operation that adds and removes SSTables only adds to the total and doesn't remove the old sstables from the total size count.

",,,,,,,,,,,,,,,,15/Nov/11 20:29;jbellis;3496.txt;https://issues.apache.org/jira/secure/attachment/12503789/3496.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-15 20:27:07.058,,,no_permission,,,,,,,,,,,,217263,,,Wed Nov 16 18:10:07 UTC 2011,,,,,,0|i0gkmf:,94771,slebresne,slebresne,,,,,,,,,"15/Nov/11 20:27;jbellis;Jackson pointed out that doing it in the current order, the data will be removed by the time we call length() on it:

{code}
.           sstable.markCompacted();
            sstable.releaseReference();
            liveSize.addAndGet(-sstable.bytesOnDisk());
{code}",15/Nov/11 20:29;jbellis;patch to reduce size before releasing reference,"15/Nov/11 20:34;cywjackson;my test case (which seems difference from Ben's?):
1) put a break between the releaseReference and releaseReference
2) insert to a CF, flush, insert again, flush, now you have 2 sstables
3) force major compaction

with the following break/sleep in the test:

{code}
            sstable.releaseReference();
            try
            {
                logger.debug(""xxxxxxxxxxxx zz for 10s"");
                Thread.sleep(7000);
            }
            
            catch (InterruptedException e)
            {
                // TODO Auto-generated catch block
                e.printStackTrace();
            }
            long bytesOnDisk = sstable.bytesOnDisk();
            logger.debug(""xxxxxxxxxx wake up -------- size to remove from livesize"" + bytesOnDisk);
liveSize.addAndGet(-bytesOnDisk);
{code}

the following is observed in the log:
{noformat}
DEBUG [CompactionExecutor:6] 2011-11-15 12:29:43,050 DataTracker.java (line 345) removing /var/lib/cassandra/data/testsize/testcfsize-hb-1 from list of files tracked for testsize.testcfsize
DEBUG [CompactionExecutor:6] 2011-11-15 12:29:43,050 SSTableReader.java (line 742) Marking /var/lib/cassandra/data/testsize/testcfsize-hb-1-Data.db compacted
DEBUG [CompactionExecutor:6] 2011-11-15 12:29:43,050 MmappedSegmentedFile.java (line 139) All segments have been unmapped successfully
DEBUG [CompactionExecutor:6] 2011-11-15 12:29:43,051 MmappedSegmentedFile.java (line 139) All segments have been unmapped successfully
DEBUG [CompactionExecutor:6] 2011-11-15 12:29:43,051 DataTracker.java (line 353) xxxxxxxxxxxx zz for 10s
DEBUG [NonPeriodicTasks:1] 2011-11-15 12:29:43,051 FileUtils.java (line 51) Deleting testcfsize-hb-1-Index.db
DEBUG [NonPeriodicTasks:1] 2011-11-15 12:29:43,051 FileUtils.java (line 51) Deleting testcfsize-hb-1-Filter.db
DEBUG [NonPeriodicTasks:1] 2011-11-15 12:29:43,052 FileUtils.java (line 51) Deleting testcfsize-hb-1-Digest.sha1
DEBUG [NonPeriodicTasks:1] 2011-11-15 12:29:43,052 FileUtils.java (line 51) Deleting testcfsize-hb-1-Statistics.db
DEBUG [NonPeriodicTasks:1] 2011-11-15 12:29:43,052 SSTable.java (line 144) Deleted /var/lib/cassandra/data/testsize/testcfsize-hb-1
DEBUG [CompactionExecutor:6] 2011-11-15 12:29:50,051 DataTracker.java (line 363) xxxxxxxxxx wake up -------- size to remove from livesize0
{noformat}

cfstats:
                Column Family: testcfsize
                SSTable count: 1
                Space used (live): 14395
                Space used (total): 5491
","16/Nov/11 10:35;slebresne;patch lgtm, +1","16/Nov/11 18:10;jbellis;Committed.

(Tried to catch the bug w/ a check on live size in RecoveryManagerTruncateTest, but I couldn't reproduce the race without adding sleeps inside the deletion task.  So I just committed the patch as above.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted handoff not working after rolling upgrade from 0.8.7 to 1.0.2,CASSANDRA-3466,12530617,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jborgstrom,jborgstrom,07/Nov/11 19:16,12/Mar/19 14:16,13/Mar/19 22:26,11/Nov/11 19:54,1.0.3,,,,,,0,hintedhandoff,,,,"While testing rolling upgrades from 0.8.7 to 1.0.2 on a test cluster I've noticed that hinted hand-off didn't always work properly. Hints generated on an upgraded node does not seem to be delivered to other newly upgraded nodes once they rejoin the ring. They only way I've found to get a node to deliver its hints is to restart it.

Here's some steps to reproduce this issue:

1. Install cassandra 0.8.7 on node1 and node2 using default settings.
2. Create keyspace foo with {replication_factor: 2}. Create column family bar
3. Shutdown node2 
4. Insert data into bar and verify that HintsColumnFamily on node2 contains hints
5. Start node2 and verify that hinted handoff is performed and HintsColumnFamily becomes empty again.

6. Upgrade and restart node1
7. Shutdown node2 
8. Insert data into bar and verify that HintsColumnFamily on node2 contains hints
9. Upgrade and start node2
10. Notice that hinted handoff is *not* performed when ""node2"" comes back. (Only if node1 is restarted)
",,,,,,,,,,,,,,,,10/Nov/11 19:02;jbellis;3466.txt;https://issues.apache.org/jira/secure/attachment/12503277/3466.txt,10/Nov/11 12:55;jborgstrom;CASSANDRA-3466-2.tar.gz;https://issues.apache.org/jira/secure/attachment/12503200/CASSANDRA-3466-2.tar.gz,09/Nov/11 14:49;jborgstrom;CASSANDRA-3466.tar.gz;https://issues.apache.org/jira/secure/attachment/12503083/CASSANDRA-3466.tar.gz,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-11-07 22:57:50.267,,,no_permission,,,,,,,,,,,,216355,,,Fri Nov 11 19:54:48 UTC 2011,,,,,,0|i0gk93:,94711,brandon.williams,brandon.williams,,,,,,,,,"07/Nov/11 22:57;jbellis;Do you observe the same behavior if both nodes start life as 1.x?

Is node2 down long enough for node1 failure detector to notice?  (i.e. shows as DOWN in nodetool ring)","08/Nov/11 07:40;jborgstrom;> Do you observe the same behavior if both nodes start life as 1.x?

No. Hinted handoff seems to work well if I either start with 1.0.2 nodes or restart all nodes once all nodes have been upgraded to 1.0.2. Probably something in the gossip state confusing hinted hand-offs.

> Is node2 down long enough for node1 failure detector to notice? (i.e. shows as DOWN in nodetool ring)

Yes. This from the node1 log:
{code}
Node2 is shut down
  INFO 19:39:08,331 InetAddress /127.0.0.2 is now dead.
Hint is triggered using set bar[x][x]='x'
  INFO 19:40:52,126 Node /127.0.0.2 has restarted, now UP
  INFO 19:40:52,127 InetAddress /127.0.0.2 is now UP
  INFO 19:40:52,127 Node /127.0.0.2 state jump to normal
Nothing happens, hint stays in HintsColumnFamily forever
{code}

Calling deliverHints() using JMX also does not seem to work. Restarting node1 will though...
","08/Nov/11 08:41;jborgstrom;> Do you observe the same behavior if both nodes start life as 1.x?

I spoke too soon. I did some more testing with fresh 1.0.2 installs (and empty data directories).
{code}
1. Start node1 and node2 and create keyspace and cf.
2. Stop node2, wait until ""is now dead"".
3. Trigger hint using ""set bar[x][x]='x';""
4. Start node2, wait for hint delivery... Nothing happens
5. Restart node1, hints are finally delivered.
{code}
Now it gets really weird...
{code}
2. Stop node2, wait until ""is now dead"".
3. Trigger hint using ""set bar[x][x]='x';""
4. Start node2. Node1 attempts to deliver hints but fails with:
 INFO 09:29:35,153 Started hinted handoff for token: 23495828435496583962471242736585511198 with IP: /127.0.0.2
ERROR 09:29:35,212 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.AssertionError
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:301)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
	at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)

On node1:
[default@system] list HintsColumnFamily;
Using default limit of 100
-------------------
RowKey: 11ad21c7d112e903d71645d1d951911e
=> (super_column=b2cd983009e311e10000fe8ebeead9cf,
     (column=6b6579, value=78, timestamp=1320740935987, ttl=864000)
     (column=6d75746174696f6e, value=0003666f6f00017800000001000003e801000003e880000000800000000000000000000001000178000004b134f65401d80000000139, timestamp=1320740935987, ttl=864000)
     (column=7461626c65, value=666f6f, timestamp=1320740935987, ttl=864000)
     (column=76657273696f6e, value=00000003, timestamp=1320740935987, ttl=864000))
=> (super_column=f929a36009e211e10000fe8ebeead9df,
     (column=6b6579, value=78, timestamp=1320740624538, ttl=864000)
     (column=7461626c65, value=666f6f, timestamp=1320740624538, ttl=864000))
1 Row Returned.
{code}

Is that a 0.8 hint showing up out of the blue on a pure 1.0.2 cluster? Or did I mess up my test somehow?
","09/Nov/11 12:50;brandon.williams;I haven't been able to reproduce the assertion errors, but I did find what is preventing hint delivery in some cases:

{noformat}
        if (hintStore.getSSTables().isEmpty())
            return; // nothing to do, don't confuse users by logging a no-op handoff
{noformat}

If you're testing with a small enough amount of hints that the hints table never flushes, we never deliver the hints in the current memtable.","09/Nov/11 13:00;hsn;Thats true. i did more testing in this area today. Deleted hints sstables from disk to be sure that no 0.8.x possible hints are there - but they should not be there anyway because i had snapshot from CF hints truncate around. And i could not get any hints delivered to second node (no exceptions today).

Thats probably deserves to roll new cassandra version soon, because it is very important to get hints properly delivered as soon as possible.","09/Nov/11 13:19;brandon.williams;{noformat}
=> (super_column=f929a36009e211e10000fe8ebeead9df,
     (column=6b6579, value=78, timestamp=1320740624538, ttl=864000)
     (column=7461626c65, value=666f6f, timestamp=1320740624538, ttl=864000))
1 Row Returned.
{noformat}

The odd thing here is that you have the 'table' and 'key' subcolumns, but not the 'mutation' or 'version' subcolumns.  Since these are all applied at once I'm not sure how they could be missing.",09/Nov/11 13:23;hsn;application is doing row delete and then row insert,"09/Nov/11 13:56;jborgstrom;> I haven't been able to reproduce the assertion errors, but I did find what is preventing hint delivery in some cases

Brandon, Did you verify that removing those lines of code actually fixes hint delivery? 

Instead of changing the code I just did a quick experiment with ""nodetool flush"" on the node holding the hints and then restarting the other node but that was not enough to trigger hints delivery:

{code}
Node1 notices that node2 is backup up
  INFO 14:41:50,752 Node /127.0.0.2 has restarted, now UP
  INFO 14:41:50,752 InetAddress /127.0.0.2 is now UP
  INFO 14:41:50,753 Node /127.0.0.2 state jump to normal
But no hints are delivered...

nodetool flush is used to make sure hints hit the disk on node1:

  INFO 14:42:32,675 Enqueuing flush of Memtable-Versions@1503666327(83/103 serialized/live bytes, 3 ops)
  INFO 14:42:32,675 Writing Memtable-Versions@1503666327(83/103 serialized/live bytes, 3 ops)
  INFO 14:42:32,681 Completed flushing /tmp/node1/data/data/system/Versions-h-1-Data.db (247 bytes)
  INFO 14:42:32,682 Enqueuing flush of Memtable-HintsColumnFamily@737188401(177/221 serialized/live bytes, 1 ops)
  INFO 14:42:32,682 Writing Memtable-HintsColumnFamily@737188401(177/221 serialized/live bytes, 1 ops)
  INFO 14:42:32,688 Completed flushing /tmp/node1/data/data/system/HintsColumnFamily-h-1-Data.db (277 bytes)
  INFO 14:42:32,691 Enqueuing flush of Memtable-bar@1831941861(17/21 serialized/live bytes, 1 ops)
  INFO 14:42:32,691 Writing Memtable-bar@1831941861(17/21 serialized/live bytes, 1 ops)
  INFO 14:42:32,694 Completed flushing /tmp/node1/data/data/foo/bar-h-1-Data.db (68 bytes)

Node2 is restarted once more to check if this will trigger hints delivery:
  INFO 14:42:54,650 InetAddress /127.0.0.2 is now dead.
  INFO 14:43:02,628 Node /127.0.0.2 has restarted, now UP
  INFO 14:43:02,629 InetAddress /127.0.0.2 is now UP
  INFO 14:43:02,629 Node /127.0.0.2 state jump to normal

Still nothing...  Restarting node 1 will deliver the hints within a few seconds though...
{code}

Regarding reproducing the assertion error it's a bit tricky. But after letting my two node test cluster performing hints delivery for each other a few times I was able to reproduce it once more. Is there anything special you would like me to test?


","09/Nov/11 14:04;brandon.williams;bq. Brandon, Did you verify that removing those lines of code actually fixes hint delivery?

Yes.

bq. Instead of changing the code I just did a quick experiment with ""nodetool flush"" on the node holding the hints and then restarting the other node but that was not enough to trigger hints delivery

Hints aren't delivered immediately, there's up to a 60s random delay to stagger the replay.  Did you wait long enough to be sure it passed?","09/Nov/11 14:16;jborgstrom;bq. Hints aren't delivered immediately, there's up to a 60s random delay to stagger the replay. Did you wait long enough to be sure it passed?

Yes, I've waited a couple of minutes now and still nothing. So I guess issuing ""nodetool flush"" isn't enough. I'll leave it running while I recompile with your changes and re-test.
","09/Nov/11 14:47;jborgstrom;Brandon, you're right. After commenting out those two lines I can no longer reproduce the hint delivery problem.

The AssertionError is still there though and I managed to reproduce it with my two node setup.
I'll attach a tarball with my config, DEBUG-level log files and data directories shortly.
What I did was basically:

# Start a new cluster with a patched 1.0.2 and create keyspace and column family.
# Bring down node2, trigger hints on node1
# Bring up node2, notice that hints are delivered, YAY!
# Bring down node1, trigger hints on node2
# Bring up node1, noticed AssertionError in log file for node1   (node1/system.log:2036)

To me it looks like the assertionError on node1 is triggered when node2 connects to deliver its hints.","09/Nov/11 14:49;jborgstrom;Logs, config files and data files from a setup that triggered AssertionError in HintedHandoffManager","09/Nov/11 17:56;brandon.williams;Jonas,

Thanks for the data files.  Looking at them with sstable2json I see the all the subcolumns are actually there, so I think what you may be seeing is CASSANDRA-3446, could you try to reproduce with that patch applied?","09/Nov/11 20:10;jborgstrom;bq. Jonas,

bq. Thanks for the data files. Looking at them with sstable2json I see the all the subcolumns are actually there, so I think what you may be seeing is CASSANDRA-3446, could you try to reproduce with that patch applied?

Jikes, that one looks pretty bad.

Anyway, with the patches in CASSANDRA-3446 in combination with the modifications of HintedHandOffManager.py mentioned earlier I've so far not been able to reproduce any of the issues I've observed earlier.

Thanks for your help!
","10/Nov/11 12:55;jborgstrom;I've unfortunately managed to reproduce this AssertionError even with the CASSANDRA-3446 patches and the HintedHandOffManager.java modification.

I used the same two node setup as before initialized with:

{code}
create keyspace foo with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = {replication_factor:2};
use foo;
create column family bar with comparator=UTF8Type and key_validation_class=UTF8Type;
{code}

# Shutdown node2 and wait until it's detected as down
trigger hints on node1 (using random values):
{code}
use foo;
set bar[0][0]='1';
set bar[2][3]='1';
set bar[3][3]='1';
{code}
Verify that `list HintsColumnFamily` looks OK
# Start node2 and wait for hints delivery
Verify that `list HintsColumnFamily` is empty after hints delivery

After repeating step 1-2 a few times (sometimes once is enough) ""HintsColumnFamily"" will become ""corrupt"" after hints delivery instead of empty. And if node1 is restarted after that it will trigger the AssertionError.

The attached file contains DEBUG-log files, data directories and conf-directories from one test where HintsColumnFamily ended up looking like this:
{code}
[default@system] list HintsColumnFamily;
Using default limit of 100
-------------------
RowKey: 1b226da5af66854850abdcc6ab4ce9c6
=> (super_column=1f2860e00b9811e10000fe8ebeead9ff,
     (column=6b6579, value=33, timestamp=1320928378352, ttl=864000))

1 Row Returned.
Elapsed time: 6 msec(s).
{code}
",10/Nov/11 15:12;slebresne;The system log indicates an assertion error on HintedHandoffManager line 298. But it's not an assertion on this line in 1.0.2. Would you mind trying to see if you can reproduce on the current 1.0 branch (or 1.0.2 patched with CASSANDRA-3446)? Just want to make sure you're not hitting something fixed already.,"10/Nov/11 15:26;jborgstrom;I'm running 1.0.2 with the 3446 patches *and* the following three lines removed from HintedHandOffManager.java (As discussed earlier in this ticket)
{code}
        if (hintStore.getSSTables().isEmpty())
            return; // nothing to do, don't confuse users by logging a no-op handoff
// An empty line
{code}
That should explain the line number mismatch (-3).
Without this change hinted handoff wouldn't work at all for me unless HintsColumnFamily is large enough to flow to disk before the other node gets back up.","10/Nov/11 18:39;jbellis;So I should be looking at node1 HintsColumnFamily for the corruption, right?","10/Nov/11 19:02;jbellis;Fix attached.  The problem is we were deleting the hint with the timestamp from the {{version}} subcolumn, but the subcolumn timestamps were generated independently so it's possible for there to be ""orphaned"" subcolumns left over.

Patch (1) uses same ts for all hint subcolumns and (2) creates the tombstone w/ the max ts from the subcolumns.

Also fixes the isEmpty delivery bug by checking memtable contents as well.",11/Nov/11 19:47;brandon.williams;+1,11/Nov/11 19:54;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadResponseSerializer doesn't compute serialized size correctly,CASSANDRA-3373,12527592,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,18/Oct/11 13:42,12/Mar/19 14:16,13/Mar/19 22:26,18/Oct/11 14:10,1.0.1,,,,,,0,,,,,,,,,,,,,,,,,,,,,18/Oct/11 13:43;slebresne;3373.patch;https://issues.apache.org/jira/secure/attachment/12499542/3373.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-18 13:48:34.635,,,no_permission,,,,,,,,,,,,88836,,,Tue Oct 18 14:10:45 UTC 2011,,,,,,0|i0gj3j:,94524,jbellis,jbellis,,,,,,,,,18/Oct/11 13:43;slebresne;The digest bytes are added twice when it's a digest query. Patch attached.,18/Oct/11 13:48;jbellis;+1,"18/Oct/11 14:10;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_count NullPointerException with counters,CASSANDRA-3601,12534422,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,ghinkle,ghinkle,09/Dec/11 02:05,12/Mar/19 14:16,13/Mar/19 22:26,09/Dec/11 08:54,1.0.6,,,,,,0,counters,,,,get_count doesn't currently work for counter columns or super counter columns. The fix seems to be pretty simple.,,,,,,,,,,,,,,,,09/Dec/11 02:08;ghinkle;trunk-3601.txt;https://issues.apache.org/jira/secure/attachment/12506696/trunk-3601.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-09 08:54:14.128,,,no_permission,,,,,,,,,,,,220144,,,Fri Dec 09 08:54:14 UTC 2011,,,,,,0|i0glwv:,94980,slebresne,slebresne,,,,,,,,,09/Dec/11 02:08;ghinkle;Patch to handle counter_columns or super_counter_columns,"09/Dec/11 02:24;ghinkle;Incidentally, this is the stack trace for the problem.

ERROR 21:23:33,504 Internal error processing get_count
java.lang.NullPointerException
	at org.apache.cassandra.thrift.CassandraServer.get_count(CassandraServer.java:458)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_count.process(Cassandra.java:3075)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)","09/Dec/11 08:54;slebresne;+1, committed. Thanks Greg.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fatal exception in thread Thread[MigrationStage:1,5,main] (LeveledCompaction)",CASSANDRA-3614,12534825,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,asuffield-bossa,asuffield-bossa,12/Dec/11 19:36,12/Mar/19 14:16,13/Mar/19 22:26,12/Dec/11 20:30,1.0.7,,,,,,0,,,,,"ERROR 19:29:39,712 Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:156)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:488)
        at org.apache.cassandra.db.DataTracker.removeAllSSTables(DataTracker.java:257)
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:267)
        at org.apache.cassandra.db.Table.unloadCf(Table.java:361)
        at org.apache.cassandra.db.Table.dropCf(Table.java:343)
        at org.apache.cassandra.db.migration.DropColumnFamily.applyModels(DropColumnFamily.java:87)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:73)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
",,,,,,,,,,,,,,,,12/Dec/11 20:00;jbellis;3614.txt;https://issues.apache.org/jira/secure/attachment/12507047/3614.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-12 20:00:27.44,,,no_permission,,,,,,,,,,,,220509,,,Mon Dec 12 20:30:52 UTC 2011,,,,,,0|i0gm2n:,95006,asuffield,asuffield,,,,,,,,,12/Dec/11 19:38;asuffield-bossa;(This is the Debian build of the 1.0.6 RC that Sylvain just put up),"12/Dec/11 19:53;asuffield-bossa;Bigger log chunk. This has got something to do with schema migrations. I had just created the CF, and was trying to do some operations on it. Probably a truncate followed by a batch insertion of a few dozen rows.

 INFO [MigrationStage:1] 2011-12-12 19:42:48,182 Migration.java (line 119) Applying migration 784303e0-24f9-11e1-0000-0282f5487ffc Add column family: org.a
pache.cassandra.config.CFMetaData@e90e23[cfId=1083,ksName=CoreQA,cfName=Deal,cfType=Standard,comparator=org.apache.cassandra.db.marshal.UTF8Type,subcolumnc
omparator=<null>,comment=,rowCacheSize=1.0,keyCacheSize=1.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.ca
ssandra.db.marshal.UTF8Type,keyValidator=org.apache.cassandra.db.marshal.UTF8Type,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSe
conds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@9f2588,mergeSh
ardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStra
tegyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO [MigrationStage:1] 2011-12-12 19:42:48,183 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-Migrations@5834000(29724/37155 serialized/li
ve bytes, 1 ops)
 INFO [MigrationStage:1] 2011-12-12 19:42:48,183 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-Schema@3824284(25583/31978 serialized/live b
ytes, 8 ops)
 INFO [FlushWriter:2] 2011-12-12 19:42:48,184 Memtable.java (line 240) Writing Memtable-Migrations@5834000(29724/37155 serialized/live bytes, 1 ops)
 INFO [FlushWriter:2] 2011-12-12 19:42:48,219 Memtable.java (line 277) Completed flushing /var/lib/cassandra/data/system/Migrations-hc-135-Data.db (29788 b
ytes)
 INFO [FlushWriter:2] 2011-12-12 19:42:48,220 Memtable.java (line 240) Writing Memtable-Schema@3824284(25583/31978 serialized/live bytes, 8 ops)
 INFO [CompactionExecutor:3] 2011-12-12 19:42:48,221 CompactionTask.java (line 113) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/Migratio
ns-hc-132-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/Migrations-hc-133-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/Mig
rations-hc-134-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/Migrations-hc-135-Data.db')]
 INFO [FlushWriter:2] 2011-12-12 19:42:48,247 Memtable.java (line 277) Completed flushing /var/lib/cassandra/data/system/Schema-hc-135-Data.db (25733 bytes
)
 INFO [CompactionExecutor:3] 2011-12-12 19:42:48,371 CompactionTask.java (line 218) Compacted to [/var/lib/cassandra/data/system/Migrations-hc-136-Data.db,
].  1,521,844 to 1,521,704 (~99% of original) bytes for 1 keys at 9.674733MB/s.  Time: 150ms.
 INFO [CompactionExecutor:3] 2011-12-12 19:42:48,372 CompactionTask.java (line 113) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/Schema-h
c-135-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/Schema-hc-134-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/Schema-hc-1
33-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/Schema-hc-132-Data.db')]
 INFO [CompactionExecutor:3] 2011-12-12 19:42:48,482 CompactionTask.java (line 218) Compacted to [/var/lib/cassandra/data/system/Schema-hc-136-Data.db,].  
1,187,815 to 1,187,488 (~99% of original) bytes for 83 keys at 10.389695MB/s.  Time: 109ms.
 INFO [MigrationStage:1] 2011-12-12 19:43:58,220 Migration.java (line 119) Applying migration a20e7a60-24f9-11e1-0000-0282f5487ffc Drop column family: Core
QA.Deal
 INFO [MigrationStage:1] 2011-12-12 19:43:58,221 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-Migrations@19716308(29025/36281 serialized/l
ive bytes, 1 ops)
 INFO [MigrationStage:1] 2011-12-12 19:43:58,224 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-Schema@13515618(25227/31533 serialized/live 
bytes, 8 ops)
 INFO [FlushWriter:3] 2011-12-12 19:43:58,225 Memtable.java (line 240) Writing Memtable-Migrations@19716308(29025/36281 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2011-12-12 19:43:58,248 Memtable.java (line 277) Completed flushing /var/lib/cassandra/data/system/Migrations-hc-138-Data.db (29089 b
ytes)
 INFO [FlushWriter:3] 2011-12-12 19:43:58,249 Memtable.java (line 240) Writing Memtable-Schema@13515618(25227/31533 serialized/live bytes, 8 ops)
 INFO [FlushWriter:3] 2011-12-12 19:43:58,275 Memtable.java (line 277) Completed flushing /var/lib/cassandra/data/system/Schema-hc-138-Data.db (25377 bytes)
ERROR [MigrationStage:1] 2011-12-12 19:43:58,276 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:156)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:488)
        at org.apache.cassandra.db.DataTracker.removeAllSSTables(DataTracker.java:257)
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:267)
        at org.apache.cassandra.db.Table.unloadCf(Table.java:361)
        at org.apache.cassandra.db.Table.dropCf(Table.java:343)
        at org.apache.cassandra.db.migration.DropColumnFamily.applyModels(DropColumnFamily.java:87)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:846)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [pool-2-thread-25] 2011-12-12 19:43:58,277 Cassandra.java (line 3878) Internal error processing system_drop_column_family
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.thrift.CassandraServer.applyMigrationOnStage(CassandraServer.java:861)
        at org.apache.cassandra.thrift.CassandraServer.system_drop_column_family(CassandraServer.java:903)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.process(Cassandra.java:3872)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.thrift.CassandraServer.applyMigrationOnStage(CassandraServer.java:853)
        ... 7 more
Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:156)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:488)
        at org.apache.cassandra.db.DataTracker.removeAllSSTables(DataTracker.java:257)
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:267)
        at org.apache.cassandra.db.Table.unloadCf(Table.java:361)
        at org.apache.cassandra.db.Table.dropCf(Table.java:343)
        at org.apache.cassandra.db.migration.DropColumnFamily.applyModels(DropColumnFamily.java:87)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:846)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
 INFO [MutationStage:62] 2011-12-12 19:43:59,004 NodeId.java (line 206) Saved local node id: 0d098030-24e6-11e1-0000-0282f5487fff
ERROR [ReadStage:60] 2011-12-12 19:44:00,322 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:60,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown table/cf pair (CoreQA.Deal)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:60)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException: Unknown table/cf pair (CoreQA.Deal)
        at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:159)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:48)
        ... 4 more
",12/Dec/11 20:00;jbellis;Thanks for testing -- attached patch should fix that.,"12/Dec/11 20:01;jbellis;(Affects at least back to 1.0.3 with CASSANDRA-3399, possibly older.)",12/Dec/11 20:10;slebresne;+1,12/Dec/11 20:22;asuffield-bossa;That fixed it. Thanks for quick response.,12/Dec/11 20:30;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Temp SSTable and file descriptor leak,CASSANDRA-3616,12534843,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,eparusel,eparusel,12/Dec/11 22:07,12/Mar/19 14:16,13/Mar/19 22:26,19/Dec/11 09:28,1.0.7,,,,,,0,,,,,"Discussion about this started in CASSANDRA-3532.  It's on it's own ticket now.

Anyhow:
The nodes in my cluster are using a lot of file descriptors, holding open tmp files. A few are using 50K+, nearing their limit (on Solaris, of 64K).

Here's a small snippet of lsof:
java 828 appdeployer *162u VREG 181,65540 0 333884 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776518-Data.db
java 828 appdeployer *163u VREG 181,65540 0 333502 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776452-Data.db
java 828 appdeployer *165u VREG 181,65540 0 333929 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776527-Index.db
java 828 appdeployer *166u VREG 181,65540 0 333859 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776514-Data.db
java 828 appdeployer *167u VREG 181,65540 0 333663 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776480-Data.db
java 828 appdeployer *168u VREG 181,65540 0 333812 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776506-Index.db

I spot checked a few and found they still exist on the filesystem too:
rw-rr- 1 appdeployer appdeployer 0 Dec 12 07:16 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776506-Index.db


After more investigation, it seems to happen during a CompactionTask.
I waited until I saw some -tmp- files hanging around in the data dir:

-rw-r--r--   1 appdeployer appdeployer       0 Dec 12 21:47:10 2011 messages_meta-tmp-hb-788904-Data.db
-rw-r--r--   1 appdeployer appdeployer       0 Dec 12 21:47:10 2011 messages_meta-tmp-hb-788904-Index.db

and then found this in the logs:
 INFO [CompactionExecutor:18839] 2011-12-12 21:47:07,173 CompactionTask.java (line 113) Compacting [SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760408-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760413-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760409-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-788314-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760407-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760412-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760410-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760411-Data.db')]

INFO [CompactionExecutor:18839] 2011-12-12 21:47:10,461 CompactionTask.java (line 218) Compacted to [/data1/cassandra/data/MA_DDR/messages_meta-hb-788896-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788897-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788898-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788899-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788900-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788901-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788902-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788903-Data.db,].  83,899,295 to 83,891,657 (~99% of original) bytes for 75,662 keys at 24.332518MB/s.  Time: 3,288ms.

Note that the timestamp of the 2nd log line matches the last modified time of the files, and has IDs leading up to, *but not including 788904*.

I thought this might be relavent information, but I haven't found the specific cause yet.","1.0.5 + CASSANDRA-3532 patch
Solaris 10",,,,,,,,,,,,,,,15/Dec/11 19:47;slebresne;3616.patch;https://issues.apache.org/jira/secure/attachment/12507568/3616.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-14 16:56:50.244,,,no_permission,,,,,,,,,,,,220527,,,Mon Dec 19 09:28:15 UTC 2011,,,,,,0|i0gm3b:,95009,jbellis,jbellis,,,,,,,,,"14/Dec/11 16:56;jborgstrom;Just a quick ""me too"".

After upgrading from 1.0.3 to 1.0.6 I see a bunch of left over 0 byte tmp files:
{code}
-rw-r--r--  1 cassandra 215          0 Dec 14 16:25 LogTokenIndex-tmp-hc-3175-Data.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:25 LogTokenIndex-tmp-hc-3175-Index.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:33 LogToken-tmp-hc-3093-Data.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:33 LogToken-tmp-hc-3093-Index.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:37 LogToken-tmp-hc-3104-Data.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:37 LogToken-tmp-hc-3104-Index.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:37 LogToken-tmp-hc-3105-Data.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:37 LogToken-tmp-hc-3105-Index.db
{code}
The timestamp suggests that they were created during a nodetool repair session.","15/Dec/11 16:17;jbellis;Eric, do you also see correlation w/ repair operations?","15/Dec/11 16:32;eparusel;I haven't run a repair lately (no deletes at this time, I plan on setting up scheduled repairs though), but can run one to find out in the next few hours.

So far I've only correlated it with compaction.
I should note we're using LeveledCompactionStrategy.",15/Dec/11 18:29;brandon.williams;I can reproduce this with SizeTieredStrategy.,15/Dec/11 18:35;jbellis;Just with compaction?  Did you get a debug log?,15/Dec/11 19:05;brandon.williams;CASSANDRA-3532 is the culprit here.,"15/Dec/11 19:47;slebresne;I believe the problem is that when compaction is over we were creating an empty (and useless) writer. Previously we were just deleting it right away because the 'cleanupIfNeeded' was in the finally.

Patch attached to not create it in the first place.",15/Dec/11 20:41;jbellis;+1,"16/Dec/11 21:02;eparusel;1.0.6 + 3616.patch fixes this issue for me, thanks!","19/Dec/11 09:28;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLog BufferOverflowException,CASSANDRA-3615,12534826,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,rbranson,rbranson,rbranson,12/Dec/11 19:41,12/Mar/19 14:16,13/Mar/19 22:26,28/Dec/11 18:27,1.1.0,,,,,,1,,,,,"Reported on mailing list http://mail-archives.apache.org/mod_mbox/cassandra-dev/201112.mbox/%3CCAJHHpg2Rw_BWFJ9DycRGSYkmwMwrJDK3%3Dzw3HwRoutWHbUcULw%40mail.gmail.com%3E

ERROR 14:07:31,215 Fatal exception in thread
Thread[COMMIT-LOG-WRITER,5,main]
java.nio.BufferOverflowException
at java.nio.Buffer.nextPutIndex(Buffer.java:501)
at java.nio.DirectByteBuffer.putInt(DirectByteBuffer.java:654)
at
org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:259)
at
org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:568)
at
org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
at java.lang.Thread.run(Thread.java:662)
 INFO 14:07:31,504 flushing high-traffic column family CFS(Keyspace='***',
ColumnFamily='***') (estimated 103394287 bytes)

It happened during a fairly standard load process using M/R.",,,,,,,,,,,,,,,,20/Dec/11 05:54;rbranson;3615.txt;https://issues.apache.org/jira/secure/attachment/12508049/3615.txt,18/Dec/11 09:45;pkolaczk;cl-buffer-overflow.patch;https://issues.apache.org/jira/secure/attachment/12507829/cl-buffer-overflow.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-15 10:06:08.224,,,no_permission,,,,,,,,,,,,220510,,,Wed Jan 04 13:59:22 UTC 2012,,,,,,0|i0gm33:,95008,jbellis,jbellis,,,,,,,,,"15/Dec/11 10:06;tivv;I've got similar problem on 1.0.5:


ERROR [COMMIT-LOG-WRITER] 2011-12-13 21:11:57,004 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.nio.BufferOverflowException
        at java.nio.Buffer.nextPutIndex(Buffer.java:518)
        at java.nio.DirectByteBuffer.putInt(DirectByteBuffer.java:664)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:244)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:567)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:679)

It seems that there is an incosistency between space checker and  actual write
org.apache.cassandra.db.commitlog.CommitLogSegment#hasCapacityFor checks for serialized length + ENTRY_OVERHEAD_SIZE = 4 + 8 + 8

At the same time, write also writes END_OF_SEGMENT_MARKER int, so ENTRY_OVERHEAD_SIZE should be 4+8+8+4
",16/Dec/11 05:31;jbellis;sounds like write should only add END_OF_SEGMENT_MARKER if it's not already at EOF,"16/Dec/11 05:34;jbellis;But that's trunk-only, not 1.0.5...  There is no CLS:244 in 1.0.5. I don't think you're running the version you think you're running.",16/Dec/11 09:20;tivv;Ooops. It seems that while fixing https://issues.apache.org/jira/browse/CASSANDRA-3573 I've deployed trunk. Will try to downgrade now since trunk does not seem to perform well for me.,"18/Dec/11 09:45;pkolaczk;A patch, increasing the segment overhead constant.",19/Dec/11 18:48;rbranson;Any hints on how to reproduce this one?,"20/Dec/11 05:54;rbranson;I didn't have any luck with Piotr's patch specifically fixing the issue, but it's fairly close to finding the actual issue. 

Since the end-of-commit-log write is only necessary if it can actually write another mutation out to the log, it's unnecessary for cases that would trigger this BufferOverflowException.

Attached patch fixes the exception and includes a test case to reproduce & prevent regression.",28/Dec/11 18:27;jbellis;committed Rick's patch,"04/Jan/12 11:43;tivv;I am sorry, but I don't see it committed in https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java
","04/Jan/12 13:59;brandon.williams;bq. I am sorry, but I don't see it committed in https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java

That is because the github mirror is still tracking svn, and we recently switched to git.  Try here:

https://git-wip-us.apache.org/repos/asf?p=cassandra.git",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Packaging should increase vm.max_map_count to accommodate leveled compaction,CASSANDRA-3563,12533686,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,02/Dec/11 20:54,12/Mar/19 14:16,13/Mar/19 22:26,08/Dec/11 21:31,1.0.6,,,,,,0,,,,,"As the title says, leveled can create a lot of files and you can run into an IOError trying to mmap all of them.",,,,,,,,,,,,,,,,02/Dec/11 21:40;brandon.williams;0001-increase-debian-limit.txt;https://issues.apache.org/jira/secure/attachment/12505934/0001-increase-debian-limit.txt,07/Dec/11 21:44;thepaul;0002-fix-sysctl.d-installation.txt;https://issues.apache.org/jira/secure/attachment/12506522/0002-fix-sysctl.d-installation.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-07 21:44:59.026,,,no_permission,,,,,,,,,,,,219413,,,Thu Dec 08 21:31:29 UTC 2011,,,,,,0|i0glg7:,94905,thepaul,thepaul,,,,,,,,,"07/Dec/11 21:44;thepaul;This won't work as is; dh_install doesn't support renaming a file while putting it into a directory (it will make a directory named /etc/sysctl.d/cassandra.conf/ and put cassandra-sysctl.conf in it).

Attachment applies on top of original patch, does file rename in debian/rules.

+1 with this fix",07/Dec/11 21:59;brandon.williams;Committed.,08/Dec/11 21:23;thepaul;Looks like debian/cassandra-sysctl.conf didn't actually get committed.,08/Dec/11 21:31;brandon.williams;You're right.  Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimeoutException When using QuorumEach or ALL consistency on Multi-DC,CASSANDRA-3577,12533983,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,06/Dec/11 03:30,12/Mar/19 14:16,13/Mar/19 22:26,08/Dec/11 20:30,0.8.9,1.0.6,1.1.0,,,,0,,,,,"Currently we have 
1) StorageProxy.sendMessages() sending messages to the first node in the other DC...  
2) A node in the other DC will remove the ForwardHeader and sendRR (Adding a MessageID to the Queue).
3) The receiving node receives the mutation, updates and sends the response to the Original Co-ordinator.
4) Co-Ordinator now checks for the MessageID (which it never had)

All the Quorum_Each updates fail in the co-ordinator, this issue started showing up after CASSANDRA-3472 the code was introduced in CASSANDRA-2138 .

Simple Fix is to remove the optimization in 0.8 and fix it in 1.x because it seems to me like it needs a change to the Message service version.

Possible Solution: We might want send the message ID's to be used by the all the nodes in other DC (Which is currently generated by the node which receives the Forward request see: (2) ).",JVM,,,,,,,,,,,,,,,07/Dec/11 18:06;vijay2win@yahoo.com;0001-Mutation-Optimization-for-MultiDC-v2.patch;https://issues.apache.org/jira/secure/attachment/12506491/0001-Mutation-Optimization-for-MultiDC-v2.patch,07/Dec/11 08:20;vijay2win@yahoo.com;0001-Mutation-Optimization-for-MultiDC.patch;https://issues.apache.org/jira/secure/attachment/12506416/0001-Mutation-Optimization-for-MultiDC.patch,06/Dec/11 06:35;vijay2win@yahoo.com;0001-removing-mutation-MultiDC-optimization.patch;https://issues.apache.org/jira/secure/attachment/12506219/0001-removing-mutation-MultiDC-optimization.patch,08/Dec/11 17:35;jbellis;3577-v3.txt;https://issues.apache.org/jira/secure/attachment/12506620/3577-v3.txt,06/Dec/11 03:42;jbellis;3577.txt;https://issues.apache.org/jira/secure/attachment/12506214/3577.txt,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-12-06 03:42:34.632,,,no_permission,,,,,,,,,,,,219709,,,Sat Dec 10 15:51:36 UTC 2011,,,,,,0|i0glmf:,94933,jbellis,jbellis,,,,,,,,,"06/Dec/11 03:42;jbellis;All we need to do is forward with the original id, no?  Patch attached to do that.",06/Dec/11 03:42;jbellis;(Patch is against 0.8.),"06/Dec/11 03:48;vijay2win@yahoo.com;But when the Co-Ordinator receives the response with the message ID the message is already removed because ResponseVerbHandler does
MessagingService.instance().removeRegisteredCallback(id);
We wont have the ID there.



","06/Dec/11 04:25;jbellis;You're right, we switched to using unique message IDs per target in CASSANDRA-2058 so that we can track timeouts for the dynamic snitch, so my patch won't work.

I agree that pre-generating extra IDs on the coordinator is the easiest fix, and also that we should just disable this behavior in 0.8 (which was the case until CASSANDRA-3472 anyway).","06/Dec/11 06:35;vijay2win@yahoo.com;removing mutation optimization for .8, i will work on the update to 1.1 shortly. Thanks!",06/Dec/11 13:28;jbellis;committed .8 patch w/ comment pointing to this issue,"06/Dec/11 14:23;hudson;Integrated in Cassandra-0.8 #409 (See [https://builds.apache.org/job/Cassandra-0.8/409/])
    remove nonlocal DC write optimization
patch by Vijay; reviewed by jbellis for CASSANDRA-3577

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1210902
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
","07/Dec/11 08:18;vijay2win@yahoo.com;Testing took some additional time, This patch is on 1.1 with an updated MessagingService.version to handle both older version and new version mutations.","07/Dec/11 15:23;jbellis;Patch doesn't apply to latest trunk for me, can you rebase?","07/Dec/11 18:06;vijay2win@yahoo.com;Sorry, Rebased to the the trunk. Thanks!","08/Dec/11 17:35;jbellis;v3 attached.  Some cleanup of StorageProxy, switches to FastBAIS, and does a version check on the receiving side as well as the sending (since we do have released versions in the wild sending out ""bad"" FORWARD_HEADERs).",08/Dec/11 18:49;vijay2win@yahoo.com;+1 Thanks!,08/Dec/11 20:30;jbellis;committed,"08/Dec/11 21:51;hudson;Integrated in Cassandra #1249 (See [https://builds.apache.org/job/Cassandra/1249/])
    multi-dc replication optimization supporting CL > ONE
patch by Vijay and jbellis for CASSANDRA-3577

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1212088
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java
* /cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
","10/Dec/11 15:51;jbellis;This can actually cause the more subtle problem of CASSANDRA-3585: Node A (DC1) sends a write to node B (DC2), which forwards to node C (DC2).  Node C replies to node A with the message ID it received from node B.  If the message generation on A and B is far enough apart, then A will not have a callback for the reply and all you will see happen is the write timeout (at CL > ONE).  But if A *does* have a callback (for a different operation) waiting, then A will try to apply the mutation response to that callback, which (if the callback is for a read) will result in the error see in that ticket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inconsistent/corrupt counters w/ broken shards never converge,CASSANDRA-3641,12535364,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,15/Dec/11 20:09,12/Mar/19 14:16,13/Mar/19 22:26,02/Jan/12 17:21,1.1.0,,,,,,0,,,,,"We ran into a case (which MIGHT be related to CASSANDRA-3070) whereby we had counters that were corrupt (hopefully due to CASSANDRA-3178). The corruption was that there would exist shards with the *same* node_id, *same* clock id, but *different* counts.

The counter column diffing and reconciliation code assumes that this never happens, and ignores the count. The problem with this is that if there is an inconsistency, the result of a reconciliation will depend on the order of the shards.

In our case for example, we would see the value of the counter randomly fluctuating on a CL.ALL read, but we would get consistent (whatever the node had) on CL.ONE (submitted to one of the nodes in the replica set for the key).

In addition, read repair would not work despite digest mismatches because the diffing algorithm also did not care about the counts when determining the differences to send.

I'm attaching patches that fixes this. The first patch is against our 0.8 branch, which is not terribly useful to people, but I include it because it is the well-tested version that we have used on the production cluster which was subject to this corruption.

The other patch is against trunk, and contains the same change.

What the patch does is:

* On diffing, treat as DISJOINT if there is a count discrepancy.
* On reconciliation, look at the count and *deterministically* pick the higher one, and:
** log the fact that we detected a corrupt counter
** increment a JMX observable counter for monitoring purposes

A cluster which is subject to such corruption and has this patch, will fix itself with and AES + compact (or just repeated compactions assuming the replicate-on-compact is able to deliver correctly).
",,,,,,,,,,,,CASSANDRA-3178,CASSANDRA-3070,,,15/Dec/11 20:12;scode;3641-0.8-internal-not-for-inclusion.txt;https://issues.apache.org/jira/secure/attachment/12507575/3641-0.8-internal-not-for-inclusion.txt,15/Dec/11 20:12;scode;3641-trunk.txt;https://issues.apache.org/jira/secure/attachment/12507574/3641-trunk.txt,24/Dec/11 03:15;scode;CASSANDRA-3641-trunk-nojmx.txt;https://issues.apache.org/jira/secure/attachment/12508582/CASSANDRA-3641-trunk-nojmx.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-12-19 10:47:00.048,,,no_permission,,,,,,,,,,,,221048,,,Mon Jan 02 17:21:34 UTC 2012,,,,,,0|i0gmfb:,95063,slebresne,slebresne,,,,,,,,,"17/Dec/11 00:44;scode;Sorry, this depends on CASSANDRA-3603. I'll submit the patch there in a minute.","19/Dec/11 10:47;slebresne;The fix lgtm, but I'd be in favor of removing the JMX reporting. Logging at ERROR seems enough and I fear a JMX counter will confuse users more than anything else (you can always use a specific log4j logger if you're so inclined to monitor logged errors through JMX).

Nit: I'd also change the comment from ""We should never see shards w/ same id+clock but different counts"" to ""We should never see *non-delta* shards w/ same id+clock but different counts"", just to make sure someone reading this comment too quickly don't leave with the wrong info. ","19/Dec/11 17:36;scode;I'll fix the comment (it was written before I understood fully the role of deltas).

As for the JMX counter: I kind of see your concern, but at the same time - most people that have monitoring of Cassandra at all will have setups to easily monitor/graph/alert on JMX exposed values and I really think it's a shame if we can't add additional instrumentation because it would confuse users.

How about putting it somewhere else, where it's clearly nothing you need to worry about normally? I actually had an original patch before I submitted upstream where I had created a separate MBean I called ""RedFlags"" because I found no good place to put the counter. The idea was that it felt completely overkill to have a dedicated MBean for the purpose, but at the same time I really wanted it accounted for. RedFlags was intended as a place to put counters that you essentially always expect to be exactly 0 during healthy production use.

I could see putting more stuff there like exception counts in places where any exception indicates a sever problem, or a count of out of disk space conditions preventing or affecting (different bucket) compaction, or a count of GC pauses above a certain threshold, etc.

If you agree I'll volunteer to go through and add some things I can think of, along with this count.

Else I can certainly re-submit without the JMX counter. Or just submit a separate JIRA for it (but that's only worth it if you might be okay with a RedFlags style approach and it's not just this one counter).
",20/Dec/11 12:19;slebresne;Let's open a separate ticket to discuss that. So far we've use the log only for recording errors so let's keep it at that for this ticket.,"24/Dec/11 03:15;scode;New version attached. Rebased to current trunk, and no JMX. Otherwise identical.","02/Jan/12 17:21;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad code in org.apache.cassandra.cql.QueryProcessor,CASSANDRA-3604,12534571,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,zolyfarkas,zolyfarkas,10/Dec/11 01:26,12/Mar/19 14:16,13/Mar/19 22:26,12/Dec/11 19:51,1.1.0,,,,,,0,,,,,"line 206:
            if (rows.get(0).key.key.equals(startKey))
                rows.remove(0);

the equals will always return false because object of different types are compared",all,300,300,,0%,300,300,,,,,,,,,12/Dec/11 18:26;slebresne;3604.patch;https://issues.apache.org/jira/secure/attachment/12507025/3604.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-10 02:34:32.738,,,no_permission,,,,,,,,,,,,220293,,,Mon Dec 12 20:26:36 UTC 2011,,,,,,0|i0glyf:,94987,jbellis,jbellis,,,,,,,,,"10/Dec/11 01:28;zolyfarkas;similar issue also on line:214:
            if (rows.get(lastIndex).key.key.equals(finishKey))
                rows.remove(lastIndex);",10/Dec/11 02:34;jbellis;All the objects in question are ByteBuffers.,"10/Dec/11 02:55;zolyfarkas;that is not correct:

     if (rows.get(0).key.key.equals(startKey))
                rows.remove(0);

rows is of type List<org.apache.cassandra.db.Row>

org.apache.cassandra.db.Row.key is of type DecoratedKey<?>

DecoratedKey<?>.key is of type ByteBuffer

startKey is of type RowPosition 

as such

ByteBuffer is compared with a RowPosition

can you please explain how startKey is of type ByteBuffer ?",10/Dec/11 02:55;zolyfarkas;see my comment,"10/Dec/11 03:47;jbellis;""ByteBuffer startKey"", line 155","10/Dec/11 04:19;zolyfarkas;OK, I believe we are looking at different versions I see the issue in:

trunk: Revision 1212726 last modified Dec 06

        ByteBuffer startKeyBytes = (select.getKeyStart() != null)
Line 155:                                   ? select.getKeyStart().getByteBuffer(keyType)
                                   : null;

        ByteBuffer finishKeyBytes = (select.getKeyFinish() != null)
                                    ? select.getKeyFinish().getByteBuffer(keyType)
                                    : null;

THe actual definition
        RowPosition startKey = RowPosition.forKey(startKeyBytes, p), finishKey = RowPosition.forKey(finishKeyBytes, p);
","10/Dec/11 04:21;zolyfarkas;re-opened it agains release 1.1, I see this issue in trunk, se revision detail in my previous comment.","12/Dec/11 18:26;slebresne;Good catch, patch attached.",12/Dec/11 19:34;jbellis;+1,"12/Dec/11 19:51;slebresne;Committed, thanks","12/Dec/11 20:26;hudson;Integrated in Cassandra #1253 (See [https://builds.apache.org/job/Cassandra/1253/])
    Fix typo introduced by #1034 in QueryProcessor
patch by slebresne; reviewed by jbellis for CASSANDRA-3604

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1213394
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping nodes don't ensure schema is ready before continuing,CASSANDRA-3629,12535105,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,14/Dec/11 07:40,12/Mar/19 14:16,13/Mar/19 22:26,14/Dec/11 21:25,1.0.7,,,,,,0,,,,,"A bootstrapping node will assume that after it has slept for RING_DELAY it has all of the schema migrations and can continue the bootstrap process.  However, with a large enough amount of migrations this is not sufficient and causes problems.",,,,,,,,,,,,,,,,14/Dec/11 07:41;brandon.williams;0001-Wait-until-the-highest-known-schema-has-been-reached.txt;https://issues.apache.org/jira/secure/attachment/12507313/0001-Wait-until-the-highest-known-schema-has-been-reached.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-14 15:38:12.714,,,no_permission,,,,,,,,,,,,220789,,,Thu Apr 05 16:45:28 UTC 2012,,,,,,0|i0gm9b:,95036,jbellis,jbellis,,,,,,,,,"14/Dec/11 15:38;jbellis;+1

should we put this in 0.8.9 as well?","14/Dec/11 18:45;brandon.williams;bq. should we put this in 0.8.9 as well?

The 0.8.9 ship looks sailed unless we want to reroll for this, do you mean 0.8.10?

Committed to 1.0, the 0.8 backport isn't trivial in any case, so leaving the ticket open for that.","14/Dec/11 21:25;jbellis;If it's a big backport, let's leave it as a known limitation of 0.8.  We need to keep 0.8 stable more than we need this fixed there.","05/Apr/12 10:02;jbellis;The easiest workaround for this is CASSANDRA-3600, which is also 1.0-only. That much should be safe to backport...","05/Apr/12 16:45;j.casares;A valid workaround is to save your current schema, drain and stop all the nodes, move all Migration and Schema sstables from all nodes to another directory (for temporary backup), stop all nodes, restart one seed node, paste the old schema (to create the first mutation and recreate the schema), restart that node one more time, then start the rest of the nodes.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted Handoff - related OOM,CASSANDRA-3624,12535015,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,krummas,krummas,13/Dec/11 20:34,12/Mar/19 14:16,13/Mar/19 22:26,04/Jan/12 16:04,1.0.7,,,,,,1,hintedhandoff,,,,"One of our nodes had collected alot of hints for another node, so when the dead node came back and the row mutations were read back from disk, the node died with an OOM-exception (and kept dying after restart, even with increased heap (from 8G to 12G)). The heap dump contained alot of SuperColumns and our application does not use those (but HH does). 

I'm guessing that each mutation is big so that PAGE_SIZE*<mutation_size> does not fit in memory (will check this tomorrow)

A simple fix (if my assumption above is correct) would be to reduce the PAGE_SIZE in HintedHandOffManager.java to something like 10 (or even 1?) to reduce the memory pressure. The performance hit would be small since we are doing the hinted handoff throttle delay sleep before sending every *mutation* anyway (not every page), thoughts?

If anyone runs in to the same problem, I got the node started again by simply removing the HintsColumnFamily* files.",,,,,,,,,,,,,,,,29/Dec/11 02:58;jbellis;3624-rebased.txt;https://issues.apache.org/jira/secure/attachment/12508815/3624-rebased.txt,14/Dec/11 03:42;jbellis;3624.txt;https://issues.apache.org/jira/secure/attachment/12507299/3624.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-14 03:36:46.697,,,no_permission,,,,,,,,,,,,220699,,,Wed Jan 04 16:04:14 UTC 2012,,,,,,0|i0gm6v:,95025,brandon.williams,brandon.williams,,,,,,,,,"14/Dec/11 03:36;jbellis;That makes sense.  (How big are your mutations?)

We added adaptive page sizing back in CASSANDRA-2652, but apparently removed it for the CASSANDRA-2045 redesign.","14/Dec/11 03:42;jbellis;Patch to add back adaptive page sizing, and drops the default size to 128 columns.","14/Dec/11 03:46;jbellis;bq. The performance hit would be small since we are doing the hinted handoff throttle delay sleep before sending every mutation anyway 

True, but this is likely to change (see Jake's comments to CASSANDRA-3554).","22/Dec/11 21:06;jbellis;Marcus, are you able to test the attached patch?","23/Dec/11 06:10;krummas;not yet, i might be able to do it today though

this happened in a production cluster so i will have to try to reproduce somewhere else","23/Dec/11 08:49;hsn;I have this problem too but i do not have large rows, i have huge number of small rows (max 180 bytes serialized)","28/Dec/11 10:47;hsn;My OOM during HH must have been caused by something else, most likely background compaction. I tried to use HH to deliver 2 million rows on 750MB heap and it worked fine. ",29/Dec/11 02:58;jbellis;rebased,04/Jan/12 15:31;brandon.williams;+1,04/Jan/12 15:50;brandon.williams;minor nit: typo in 'behavr' in the comment,04/Jan/12 16:04;jbellis;fixed typo and committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race between cf flush and  its secondary indexes flush,CASSANDRA-3547,12533432,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,01/Dec/11 11:19,12/Mar/19 14:16,13/Mar/19 22:26,02/Dec/11 10:50,1.0.6,,,Feature/2i Index,,,0,,,,,"When a CF with indexes is flushed, it's indexes are flushed too. In particular their memtable is switched, but without making the old memtable frozen. This can conflict with a concurrent flush of the index itself, as reported on the user list by Michael Vaknine:
{noformat}
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 java.lang.AssertionError
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.maybeSwitchMemtable(ColumnFamilyStore.java:671)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:745)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.forceBlockingFlush(ColumnFamilyStore.java:750)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.keys.KeysIndex.forceBlockingFlush(KeysIndex.java:119)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.SecondaryIndexManager.flushIndexesBlocking(SecondaryIndexManager.java:258)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.SecondaryIndexManager.maybeBuildSecondaryIndexes(SecondaryIndexManager.java:123)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:151)
{noformat}",,,,,,,,,,,,,,,,01/Dec/11 11:29;slebresne;3547.patch;https://issues.apache.org/jira/secure/attachment/12505754/3547.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-01 21:24:09.676,,,no_permission,,,,,,,,,,,,219160,,,Fri Dec 02 10:50:02 UTC 2011,,,,,,0|i0gl8v:,94872,jbellis,jbellis,,,,,,,,,01/Dec/11 11:29;slebresne;Patch to freeze every memtable we actually flush.,01/Dec/11 11:39;slebresne;Actually I see no reason this wouldn't affect 0.8. The patch is against 1.0 but it will be trivial to rebase to 0.8 if needed.,"01/Dec/11 21:24;jbellis;+1, and I think we do need this for 0.8 as well","01/Dec/11 21:24;jbellis;However...  0.8 is stable enough, and this bug is rare enough, and flush code is tricky enough, that maybe we should leave it be.  I could go either way.","02/Dec/11 10:50;slebresne;You know what, I agree on 0.8. It's the first time anyone reports this and it's not like it corrupt data or anything bad, so I've committed to 1.0 only.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error during multi-threaded compaction in 0.8,CASSANDRA-3270,12525107,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,kmueller,kmueller,28/Sep/11 21:40,12/Mar/19 14:16,13/Mar/19 22:26,29/Sep/11 22:09,1.0.0,,,,,,0,compaction,,,,"I'm running 0.8.6 plus the multi-threaded compaction patch in issue 2901.  I'm getting an error compacting last night:


Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.ClassCastException: java.util.concurrent.ThreadPoolExecutor cannot be cast to org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager.performMajor(CompactionManager.java:278)
        at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1856)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1447)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: java.util.concurrent.ThreadPoolExecutor cannot be cast to org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:53)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:92)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getCompactedRow(ParallelCompactionIterable.java:211)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getReduced(ParallelCompactionIterable.java:185)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getReduced(ParallelCompactionIterable.java:146)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:105)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:92)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.compaction.CompactionManager.doCompactionWithoutSizeEstimation(CompactionManager.java:573)
        at org.apache.cassandra.db.compaction.CompactionManager.doCompaction(CompactionManager.java:507)
        at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:320)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
",,,,,,,,,,,,,,,,30/Sep/11 17:03;jbellis;2901-0.8-v2.txt;https://issues.apache.org/jira/secure/attachment/12497187/2901-0.8-v2.txt,29/Sep/11 22:09;jbellis;2901-0.8-v2.txt;https://issues.apache.org/jira/secure/attachment/12497070/2901-0.8-v2.txt,28/Sep/11 22:04;jbellis;3270.txt;https://issues.apache.org/jira/secure/attachment/12496934/3270.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-09-28 22:04:33.63,,,no_permission,,,,,,,,,,,,37561,,,Fri Sep 30 21:28:43 UTC 2011,,,,,,0|i0ghvj:,94326,slebresne,slebresne,,,,,,,,,28/Sep/11 22:04;jbellis;Patch (against trunk) to fix the problem.  Should also apply to 0.8 + the old 2901 patch with minimal effort.,"28/Sep/11 22:47;kmueller;Applying the patch by hand to 0.8 gives me:


    [javac] src/java/org/apache/cassandra/db/compaction/ParallelCompactionIterable.java:89: cannot find symbol
    [javac] symbol  : constructor Reducer(org.apache.commons.collections.iterators.CollatingIterator)
    [javac] location: class org.apache.cassandra.db.compaction.ParallelCompactionIterable.Reducer
    [javac]         return new Unwrapper(new Reducer(iter), controller);
    [javac]                              ^
    [javac] src/java/org/apache/cassandra/db/compaction/ParallelCompactionIterable.java:146: cannot find symbol
    [javac] symbol  : constructor ReducingIterator()
    [javac] location: class org.apache.cassandra.utils.ReducingIterator<org.apache.cassandra.db.compaction.ParallelCompactionIterable.RowContainer,org.apache.cassandra.db.compaction.ParallelCompactionIterable.CompactedRowContainer>
    [javac]     private class Reducer extends ReducingIterator<RowContainer, CompactedRowContainer> implements CloseableIterator<CompactedRowContainer>
    [javac]             ^

This is probably something trivial, but I don't know the rest to make it work in 0.8?","29/Sep/11 08:56;slebresne;lgtm, +1

@Karl: that error seems completely unrelated to the patch on this ticket, but seems more like you applied the wrong patch from 2901 (the one for 1.0.0).",29/Sep/11 22:09;jbellis;Patch attached with both 2901 and 3270 rebased and combined for 0.8.,29/Sep/11 22:09;jbellis;committed to trunk,"29/Sep/11 23:23;kmueller;Thanks Jonathan.  

I will apply this patch to 0.8 trunk and test MT compaction performance as we discussed in email.",30/Sep/11 07:27;kmueller;Getting this issue applying 2901-v8-2.txt against 0.8 trunk: http://pastebin.com/81WQxkYV,30/Sep/11 17:03;jbellis;updated v2 to include new files,"30/Sep/11 19:57;kmueller;Sorry to be a pain in the butt, but it's not starting up.  I get the same error with 0.8.6 based source or 0.8 trunk.  This is with the updated v2 patch. 


Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/CassandraDaemon
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.CassandraDaemon
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.cassandra.thrift.CassandraDaemon.  Program will exit.
","30/Sep/11 20:35;jbellis;Usually that just means ant got confused.  Try ""ant realclean"" first.",30/Sep/11 21:28;kmueller;That worked.  Going to run some tests over the weekend for this and 2901 as discussed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
-Dreplace_token leaves old node (IP) in the gossip with the token.,CASSANDRA-3736,12538279,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,cywjackson,cywjackson,13/Jan/12 01:13,12/Mar/19 14:16,13/Mar/19 22:26,19/Jan/12 21:08,1.0.8,1.1.0,,,,,1,,,,,"https://issues.apache.org/jira/browse/CASSANDRA-957 introduce a -Dreplace_token,

however, the replaced IP keeps on showing up in the Gossiper when starting the replacement node:

{noformat}
 INFO [Thread-2] 2012-01-12 23:59:35,162 CassandraDaemon.java (line 213) Listening for thrift clients...
 INFO [GossipStage:1] 2012-01-12 23:59:35,173 Gossiper.java (line 836) Node /50.56.59.68 has restarted, now UP
 INFO [GossipStage:1] 2012-01-12 23:59:35,174 Gossiper.java (line 804) InetAddress /50.56.59.68 is now UP
 INFO [GossipStage:1] 2012-01-12 23:59:35,175 StorageService.java (line 988) Node /50.56.59.68 state jump to normal
 INFO [GossipStage:1] 2012-01-12 23:59:35,176 Gossiper.java (line 836) Node /50.56.58.55 has restarted, now UP
 INFO [GossipStage:1] 2012-01-12 23:59:35,176 Gossiper.java (line 804) InetAddress /50.56.58.55 is now UP
 INFO [GossipStage:1] 2012-01-12 23:59:35,177 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO [GossipTasks:1] 2012-01-12 23:59:45,048 Gossiper.java (line 818) InetAddress /50.56.58.55 is now dead.
 INFO [GossipTasks:1] 2012-01-13 00:00:06,062 Gossiper.java (line 632) FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip
 INFO [GossipStage:1] 2012-01-13 00:01:06,320 Gossiper.java (line 838) Node /50.56.58.55 is now part of the cluster
 INFO [GossipStage:1] 2012-01-13 00:01:06,320 Gossiper.java (line 804) InetAddress /50.56.58.55 is now UP
 INFO [GossipStage:1] 2012-01-13 00:01:06,321 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO [GossipTasks:1] 2012-01-13 00:01:16,106 Gossiper.java (line 818) InetAddress /50.56.58.55 is now dead.
 INFO [GossipTasks:1] 2012-01-13 00:01:37,121 Gossiper.java (line 632) FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip
 INFO [GossipStage:1] 2012-01-13 00:02:37,352 Gossiper.java (line 838) Node /50.56.58.55 is now part of the cluster
 INFO [GossipStage:1] 2012-01-13 00:02:37,353 Gossiper.java (line 804) InetAddress /50.56.58.55 is now UP
 INFO [GossipStage:1] 2012-01-13 00:02:37,353 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO [GossipTasks:1] 2012-01-13 00:02:47,158 Gossiper.java (line 818) InetAddress /50.56.58.55 is now dead.
 INFO [GossipStage:1] 2012-01-13 00:02:50,162 Gossiper.java (line 818) InetAddress /50.56.58.55 is now dead.
 INFO [GossipStage:1] 2012-01-13 00:02:50,163 StorageService.java (line 1156) Removing token 122029383590318827259508597176866581733 for /50.56.58.55
{noformat}

in the above, /50.56.58.55 was the replaced IP.

tried adding the ""Gossiper.instance.removeEndpoint(endpoint);"" in the StorageService.java where the message 'Nodes %s and %s have the same token %s.  Ignoring %s"",' seems only have fixed this temporary. Here is a ring output:

{noformat}
riptano@action-quick:~/work/cassandra$ ./bin/nodetool -h localhost ring
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
50.56.59.68     datacenter1 rack1       Up     Normal  6.67 KB         85.56%  60502102442797279294142560823234402248      
50.56.31.186    datacenter1 rack1       Up     Normal  11.12 KB        14.44%  85070591730234615865843651857942052864 
{noformat}

gossipinfo:
{noformat}
$ ./bin/nodetool -h localhost gossipinfo
/50.56.58.55
  LOAD:6835.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.58.55
  STATUS:NORMAL,85070591730234615865843651857942052864
  RELEASE_VERSION:1.0.7-SNAPSHOT
/50.56.59.68
  LOAD:6835.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.59.68
  STATUS:NORMAL,60502102442797279294142560823234402248
  RELEASE_VERSION:1.0.7-SNAPSHOT
action-quick2/50.56.31.186
  LOAD:11387.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.31.186
  STATUS:NORMAL,85070591730234615865843651857942052864
  RELEASE_VERSION:1.0.7-SNAPSHOT
{noformat}

Note that at 1 point earlier it seems to have been removed:

$ ./bin/nodetool -h localhost gossipinfo
/50.56.59.68
  LOAD:13815.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.59.68
  STATUS:NORMAL,60502102442797279294142560823234402248
  RELEASE_VERSION:1.0.7-SNAPSHOT
action-quick2/50.56.31.186
  LOAD:13725.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.31.186
  STATUS:NORMAL,85070591730234615865843651857942052864
  RELEASE_VERSION:1.0.7-SNAPSHOT

riptano@action-quick2:~/work/cassandra$  INFO [GossipStage:1] 2012-01-13 01:03:30,073 Gossiper.java (line 838) Node /50.56.58.55 is now part of the cluster

 INFO [GossipStage:1] 2012-01-13 01:03:30,073 Gossiper.java (line 804) InetAddress /50.56.58.55 is now UP

 INFO [GossipStage:1] 2012-01-13 01:03:30,074 StorageService.java (line 1017) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55",,,,,,,,,,,,,,,,13/Jan/12 03:43;vijay2win@yahoo.com;0001-CASSANDRA-3736.patch;https://issues.apache.org/jira/secure/attachment/12510456/0001-CASSANDRA-3736.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-13 01:37:45.636,,,no_permission,,,,,,,,,,,,223785,,,Tue Nov 13 12:16:49 UTC 2012,,,,,,0|i0gnlz:,95255,brandon.williams,brandon.williams,,,,,,,,,"13/Jan/12 01:37;vijay2win@yahoo.com;Hi Jackson, Just a clarification is 50.56.58.55 up and cassandra is running?

 INFO [GossipStage:1] 2012-01-12 23:59:35,177 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55

This happens when the replaced node is running or resurrected. ","13/Jan/12 01:50;davidstrauss;Hi Vijay,

I filed the ticket with DataStax that prompted this issue. I'm not 100% certain whether the node we replaced was fully and consistently offline from the point we performed the replacement. I *believe* it was, especially because the -Dreplace_token refuses to work if the node being replaced is online --- and we took no further action to bring the replaced node back (its VM wasn't initializing any network interfaces other than ""lo"").

Even if the replaced node comes back, it shouldn't be allowed to re-join the ring with a token already owned by an ""Up"" node. It should be subjected to the same condition -Dreplace_token is, where the token being used by the new ring member must be owned by a ""Down"" node.

David","13/Jan/12 01:59;davidstrauss;Alternatively, it would be good for Cassandra to provide a convenient (nodetool) way to drop the ""Down"" IP when a token is simultaneously occupied by one ""Up"" IP and at least one ""Down"" IP.","13/Jan/12 02:03;brandon.williams;bq. Alternatively, it would be good for Cassandra to provide a convenient (nodetool) way to drop the ""Down"" IP when a token is simultaneously occupied by one ""Up"" IP and at least one ""Down"" IP.

CASSANDRA-3337 is designed to handle these kinds of situations (where gossip is not doing the right thing naturally)","13/Jan/12 02:29;cywjackson;bq. is 50.56.58.55 up and cassandra is running?

no. the Cassandra on 50.56.58.55 was not UP/had shutdown. But the IP is available, though i don't think that matters.

so my test case was simply:
1) start 2 nodes (A , B).  With A being the seed, B bootstrap into it (by specifying a token)
2) stop B (after B had successfully joined)
3) start C with -Dcassandra.replace_token=<B's token>

continuing restarting C (without the replace_token param) could observe the behavior.",13/Jan/12 03:43;vijay2win@yahoo.com;Simple patch to remove from SYSTEM_TABLE/RING_KEY when token is replaced.,"13/Jan/12 21:39;cywjackson;fix no good.

and to ensure fix is deployed, checked the compiled class:

$ javap -c -private -classpath ./build/classes/main/ org.apache.cassandra.db.SystemTable | grep ""updateToken(java.net.Inet"" -A10
public static synchronized void updateToken(java.net.InetAddress, org.apache.cassandra.dht.Token);
  Code:
   0:   aload_0
   1:   invokestatic    #51; //Method org/apache/cassandra/utils/FBUtilities.getLocalAddress:()Ljava/net/InetAddress;
   4:   if_acmpne       12
   7:   aload_1
   8:   invokestatic    #52; //Method removeToken:(Lorg/apache/cassandra/dht/Token;)V
   11:  return

to ensure removeToken is added (per the patch)

and the classpath of the jvm is using it:

 INFO 20:32:57,083 Classpath: ./bin/../conf:./bin/*../build/classes/main*:./bin/../build/classes/thrift:./bin/../lib/antlr-3.2.jar:./bin/../lib/avro-1.4.0-fixes.jar:./bin/../lib/avro-1.4.0-sources-fixes.jar:./bin/../lib/commons-cli-1.1.jar:./bin/../lib/commons-codec-1.2.jar:./bin/../lib/commons-lang-2.4.jar:./bin/../lib/compress-lzf-0.8.4.jar:./bin/../lib/concurrentlinkedhashmap-lru-1.2.jar:./bin/../lib/guava-r08.jar:./bin/../lib/high-scale-lib-1.1.2.jar:./bin/../lib/jackson-core-asl-1.4.0.jar:./bin/../lib/jackson-mapper-asl-1.4.0.jar:./bin/../lib/jamm-0.2.5.jar:./bin/../lib/jline-0.9.94.jar:./bin/../lib/json-simple-1.1.jar:./bin/../lib/libthrift-0.6.jar:./bin/../lib/log4j-1.2.16.jar:./bin/../lib/servlet-api-2.5-20081211.jar:./bin/../lib/slf4j-api-1.6.1.jar:./bin/../lib/slf4j-log4j12-1.6.1.jar:./bin/../lib/snakeyaml-1.6.jar:./bin/../lib/snappy-java-1.0.4.1.jar:./bin/../lib/jamm-0.2.5.jar

log from the replacement node:
{noformat}
 INFO 20:34:27,856 Listening for thrift clients...
 INFO 20:35:28,750 Node /50.56.58.55 is now part of the cluster
 INFO 20:35:28,750 InetAddress /50.56.58.55 is now UP
 INFO 20:35:28,751 Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO 20:35:38,841 InetAddress /50.56.58.55 is now dead.
 INFO 20:35:58,852 FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip
 INFO 20:36:59,786 Node /50.56.58.55 is now part of the cluster
 INFO 20:36:59,787 InetAddress /50.56.58.55 is now UP
 INFO 20:36:59,787 Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO 20:37:09,887 InetAddress /50.56.58.55 is now dead.
 INFO 20:37:29,898 FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip
{noformat}

",13/Jan/12 22:00;brandon.williams;I suspect we have the same issue as I outlined in CASSANDRA-3737,"18/Jan/12 23:53;cywjackson;looks like fix from CASSANDRA-3747 got the fix.

the replacement node would still get this once:
 INFO [GossipStage:1] 2012-01-18 23:45:56,412 Gossiper.java (line 834) Node /50.56.58.55 is now part of the cluster
 INFO [GossipStage:1] 2012-01-18 23:45:56,412 Gossiper.java (line 800) InetAddress /50.56.58.55 is now UP
 INFO [GossipStage:1] 2012-01-18 23:45:56,413 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO [GossipTasks:1] 2012-01-18 23:46:05,805 Gossiper.java (line 814) InetAddress /50.56.58.55 is now dead.
 INFO [GossipTasks:1] 2012-01-18 23:46:26,819 Gossiper.java (line 628) FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip

but its quiet after that.

the other node would receive the same info also:

 INFO [GossipTasks:1] 2012-01-18 23:45:57,486 Gossiper.java (line 628) FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip

and the gossipinfo of those nodes are the matching:


$ ./bin/nodetool -h 50.56.31.186 gossipinfo
/50.56.59.68
  RELEASE_VERSION:1.0.7-SNAPSHOT
  LOAD:6820.0
  RPC_ADDRESS:50.56.59.68
  STATUS:NORMAL,0
  SCHEMA:00000000-0000-1000-0000-000000000000
action-quick2/50.56.31.186
  RELEASE_VERSION:1.0.7-SNAPSHOT
  RPC_ADDRESS:50.56.31.186
  STATUS:NORMAL,85070591730234615865843651857942052864
  LOAD:11372.0
  SCHEMA:00000000-0000-1000-0000-000000000000

$ ./bin/nodetool -h 50.56.59.68 gossipinfo
action-quick/50.56.59.68
  SCHEMA:00000000-0000-1000-0000-000000000000
  RELEASE_VERSION:1.0.7-SNAPSHOT
  LOAD:6820.0
  RPC_ADDRESS:50.56.59.68
  STATUS:NORMAL,0
/50.56.31.186
  SCHEMA:00000000-0000-1000-0000-000000000000
  RELEASE_VERSION:1.0.7-SNAPSHOT
  LOAD:11372.0
  RPC_ADDRESS:50.56.31.186
  STATUS:NORMAL,85070591730234615865843651857942052864
","19/Jan/12 04:33;vijay2win@yahoo.com;Yes and the fix attached with this ticket will also remove the node from the System table, while replacing hence you wont even see the following message...

>>> INFO [GossipStage:1] 2012-01-18 23:45:56,412 Gossiper.java (line 800) InetAddress /50.56.58.55 is now UP

The problem is that we remove the node after 30 seconds.... Meanwhile the gossip will make the other node know about .55 and hence the message in the other node. 
The patch will fix this by removing the information from the System table in the first place instead of restart which triggering it to reappear. Can you try redoing the test? it doesn't appear back in my tests.","19/Jan/12 19:11;brandon.williams;If CASSANDRA-3747 solved this, then I don't think there's any full solution here worth applying, since this is mostly just a cosmetic problem and not worth introducing a possibly destabilizing change over.  Anyone running into this can use CASSANDRA-3337 to remove it, or avoid replacing tokens.

+1 to this patch for 1.0 and trunk, though.",19/Jan/12 21:08;vijay2win@yahoo.com;Committed both in 1.0 and trunk. Thanks!,"13/Nov/12 12:16;tamarfraenkel;I have the same issue with Cassandra 1.0.11 (used DataStax AMI).
I thought it was supposed to be solved already.
I see those messages on the node that was started using -Dcassandra.replace_token=<token>.

From time to time I also see 
{color:blue} 
 INFO [GossipTasks:1] 2012-11-13 12:26:38,195 Gossiper.java (line 818) InetAddress /<dead_node_ip> is now dead.
 INFO [GossipTasks:1] 2012-11-13 12:26:58,203 Gossiper.java (line 632) FatClient /<dead_node_ip> has been silent for 30000ms, removing from gossip
 INFO [GossipStage:1] 2012-11-13 12:27:59,210 Gossiper.java (line 838) Node /<dead_node_ip> is now part of the cluster
 INFO [GossipStage:1] 2012-11-13 12:27:59,210 Gossiper.java (line 804) InetAddress /<dead_node_ip> is now UP
 INFO [GossipStage:1] 2012-11-13 12:27:59,210 StorageService.java (line 1017) Nodes /<dead_node_ip> and /<replacing_node_ip> have the same token 113427455640312821154458202477256070484.  Ignoring /<dead_node_ip>

{color}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json doesn't work for secondary index sstable due to partitioner mismatch,CASSANDRA-3738,12538304,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,skamio,skamio,13/Jan/12 10:35,12/Mar/19 14:16,13/Mar/19 22:26,20/Jan/12 23:16,1.0.8,,,Feature/2i Index,,,0,tools,,,,"sstable2json doesn't work for secondary index sstable in 1.0.6 while it worked in version 0.8.x.


$ bin/sstable2json $DATA/data/Keyspace1/users-hc-1-Data.db 
{
""1111"": [[""birth_year"",""1973"",1326450301786000], [""full_name"",""Patrick Rothfuss"",1326450301782000]],
""1020"": [[""birth_year"",""1975"",1326450301776000], [""full_name"",""Brandon Sanderson"",1326450301716000]]
}

$ bin/sstable2json $DATA/data/Keyspace1/users.users_birth_year_idx-hc-1-Data.db 
Exception in thread ""main"" java.lang.RuntimeException: Cannot open data/Keyspace1/users.users_birth_year_idx-hc-1 because partitioner does not match org.apache.cassandra.dht.RandomPartitioner
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:145)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:123)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:118)
	at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:360)
	at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:373)
	at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:431)


I tested with following sample data via cli:

create keyspace Keyspace1;
use Keyspace1;
create column family users with comparator=UTF8Type and
 column_metadata=[{column_name: full_name, validation_class: UTF8Type},
{column_name: email, validation_class: UTF8Type},
  {column_name: birth_year, validation_class: LongType, index_type: KEYS},
  {column_name: state, validation_class:  UTF8Type, index_type: KEYS}];
set users[1020][full_name] = 'Brandon Sanderson';
set users[1020][birth_year] = 1975;  
set users[1111][full_name] = 'Patrick Rothfuss';     
set users[1111][birth_year] = 1973;
get users where birth_year = 1973;
",linux,,,,,,,,,,,,,,,20/Jan/12 22:50;jbellis;3738-v2.txt;https://issues.apache.org/jira/secure/attachment/12511328/3738-v2.txt,20/Jan/12 21:40;yukim;cassandra-1.0-3738.txt;https://issues.apache.org/jira/secure/attachment/12511324/cassandra-1.0-3738.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-01-13 17:33:33.657,,,no_permission,,,,,,,,,,,,223810,,,Fri Jan 20 23:16:00 UTC 2012,,,,,,0|i0gnmv:,95259,jbellis,jbellis,,,,,,,,,"13/Jan/12 17:33;yukim;In what version do you use to create sstable? If that's 1.0.4, secondary index is created in wrong way.(CASSANDRA-3540)
In that case, you have to drop index and rebuild index first.","16/Jan/12 01:49;skamio;I've tested it with sstable created on 1.0.6.
","20/Jan/12 21:40;yukim;SSTableExport trying to open secondary index sstable with storage partitioner causes this error.

Patch attached to use correct partitioner when exporting secondary index sstable.",20/Jan/12 22:50;jbellis;Can we just move that code into SSTR.open so it's fixed for any use and not just SSTableExport?  v2 attached w/ that approach.,"20/Jan/12 23:09;yukim;v2 is much better.
I tested with it and worked as expected.
+1.",20/Jan/12 23:16;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
While using BulkOutputFormat  unneccessarily look for the cassandra.yaml file.,CASSANDRA-3740,12538311,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,samarthg1986,samarthg1986,13/Jan/12 12:14,12/Mar/19 14:16,13/Mar/19 22:26,14/Feb/12 13:41,1.1.0,,,,,,0,cassandra,hadoop,mapreduce,,"I am trying to use BulkOutputFormat to stream the data from map of Hadoop job. I have set the cassandra related configuration using ConfigHelper ,Also have looked into Cassandra code seems Cassandra has taken care that it should not look for the cassandra.yaml file.
But still when I run the job i get the following error:

{
12/01/13 11:30:04 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
12/01/13 11:30:04 INFO input.FileInputFormat: Total input paths to process : 1
12/01/13 11:30:04 INFO mapred.JobClient: Running job: job_201201130910_0015
12/01/13 11:30:05 INFO mapred.JobClient:  map 0% reduce 0%
12/01/13 11:30:23 INFO mapred.JobClient: Task Id : attempt_201201130910_0015_m_000000_0, Status : FAILED
java.lang.Throwable: Child Error
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)
Caused by: java.io.IOException: Task process exit with nonzero status of 1.
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:258)

attempt_201201130910_0015_m_000000_0: Cannot locate cassandra.yaml
attempt_201201130910_0015_m_000000_0: Fatal configuration error; unable to start server.
}

Also let me know how can i make this cassandra.yaml file available to Hadoop mapreduce job?",,,,,,,,,,CASSANDRA-3907,,CASSANDRA-3839,,CASSANDRA-3839,,03/Feb/12 00:20;brandon.williams;0001-Make-DD-the-canonical-partitioner-source.txt;https://issues.apache.org/jira/secure/attachment/12513076/0001-Make-DD-the-canonical-partitioner-source.txt,03/Feb/12 00:20;brandon.williams;0002-Prevent-loading-from-yaml.txt;https://issues.apache.org/jira/secure/attachment/12513077/0002-Prevent-loading-from-yaml.txt,03/Feb/12 14:11;brandon.williams;0003-use-output-partitioner.txt;https://issues.apache.org/jira/secure/attachment/12513129/0003-use-output-partitioner.txt,03/Feb/12 00:20;brandon.williams;0005-BWR-uses-any-if.txt;https://issues.apache.org/jira/secure/attachment/12513080/0005-BWR-uses-any-if.txt,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-01-13 19:22:53.244,,,no_permission,,,,,,,,,,,,223817,,,Tue Feb 14 13:55:49 UTC 2012,,,,,,0|i0gnnr:,95263,lenn0x,lenn0x,,,,,,,,,13/Jan/12 19:22;brandon.williams;Are you trying this on trunk?  Because specifically patch 0002 on CASSANDRA-3045 addresses this problem.,"14/Jan/12 04:33;samarthg1986;I am using ""apache-cassandra-2011-12-27_22-01-50-bin.tar.gz"" build from jenkins . Let me know if I am using the correct binaries . ","14/Jan/12 09:02;samarthg1986;I have checked into the code and I can see following issues:
 1) ""org.apache.cassandra.config.Config"" do not initialize the all the properties and which results into the null pointer exception in a static block of class ""DatabaseDescriptor"" for example ""conf.commitlog_sync""

 2) I cant see any method in ConfigHelper to specify that I am using ""Supercolumn"" and clustername(dont know if cluster name is really needed)

 3) Also there is no method to specify comparator and subcomparator in ConfigHelper and it seems like comparators have been hard coded to ""BytesType""
","14/Jan/12 12:25;samarthg1986;One More issue:

I can set the ""Partitioner"" into jobconf using ""ConfigHelper"" ,but it is no where used to set into the ""DatabaseDescriptor"" and not even initialized into the ""DatabaseDescriptor.initDefaultsOnly()"".
But ""AbstractSSTableSimpleWriter"" uses partioner (at line number 94) which will result into the null pointer.","18/Jan/12 07:32;samarthg1986;Apart From these issues I do not think that we are considering the TTL case in BulkOutputFormat.
Have looked into the code and can only see the ""addColumn()"" method and the ""addExpiringColumn()"" is not used anywhere.","19/Jan/12 17:08;brandon.williams;bq. 1) ""org.apache.cassandra.config.Config"" do not initialize the all the properties and which results into the null pointer exception in a static block of class ""DatabaseDescriptor"" for example ""conf.commitlog_sync""

Patches to address that.

bq. 2) I cant see any method in ConfigHelper to specify that I am using ""Supercolumn""

""mapreduce.output.bulkoutputformat.issuper"" controls that.

bq. 3) Also there is no method to specify comparator and subcomparator in ConfigHelper and it seems like comparators have been hard coded to ""BytesType""

BytesType will sort correctly, the comparators are in the schema on the remote nodes.

bq. Apart From these issues I do not think that we are considering the TTL case in BulkOutputFormat.

CASSANDRA-3754 will handle this.","01/Feb/12 11:03;samarthg1986;I Tested the patches all the patches except the yaml one works fine.
I have checked the Yaml patch and my job still look for the cassandra.yaml file and fails.","01/Feb/12 19:56;brandon.williams;I added a way to get around the yaml, then forgot to make BOF use it.  Updated patches should work.","02/Feb/12 07:39;samarthg1986;Cool! Its Working Perfect with the updated patches.
Can you please explain 
1) what is the significance of ""INPUT_INITIAL_THRIFT_ADDRESS"" for BulkOutPutFormat.
2) What am I suppose to provide there?(If it is needed)
3) Is there any need to provide Listen address of the Hadoop Nodes for BulkOutputFormat if yes How to provide the same?

Actually we are experiencing the problem while loading the data where it fails to connect if the host the M/R job is running on is dualstack, i.e. has both IPv4 and IPv6. 
Also it works when cassandra.yaml is provided ,may be it is reading listen address or something from cassandra.yaml.","02/Feb/12 13:41;brandon.williams;bq. what is the significance of ""INPUT_INITIAL_THRIFT_ADDRESS"" for BulkOutPutFormat.

For an output format, this won't be used, it's only for input formats.

bq. Is there any need to provide Listen address of the Hadoop Nodes for BulkOutputFormat if yes How to provide the same?

I'm not sure what you mean, hadoop nodes themselves won't have a listen address, and BOF will discover the cassandra nodes' listen address via thrift.

bq. Actually we are experiencing the problem while loading the data where it fails to connect if the host the M/R job is running on is dualstack, i.e. has both IPv4 and IPv6. Also it works when cassandra.yaml is provided ,may be it is reading listen address or something from cassandra.yaml.

Hmm, I can't think of any reason that would work with the yaml, can you give more details of the setup?","02/Feb/12 13:46;forsberg;>Actually we are experiencing the problem while loading the data where it fails to connect if the host the M/R job is running on is dualstack, i.e. has both IPv4 >and IPv6. 
>Also it works when cassandra.yaml is provided ,may be it is reading listen address or something from cassandra.yaml.

Described this problem in more detail in CASSANDRA-3839.","02/Feb/12 14:26;brandon.williams;bq. Described this problem in more detail in CASSANDRA-3839.

I see, let's address this there.",02/Feb/12 19:06;brandon.williams;Two more patches to address CASSANDRA-3839,03/Feb/12 00:20;brandon.williams;Rebased.,03/Feb/12 14:11;brandon.williams;Fix incorrect variable scope causing NPE.,"13/Feb/12 16:35;brandon.williams;Samarth/Erik,

How does this patch look?","13/Feb/12 17:11;samarthg1986;First 4 patches working fine.
About the patch related to CASSANDRA-3839 Erik can explain properly.","14/Feb/12 02:55;lenn0x;Brandon,

I just filed CASSANDRA-3906, this fixes counter column NPE. Also we tested out these patches and based on patch: 0004-update-BOF-for-new-dir-layout.txt

That has a scoping issue, where you set File output, so scope becomes local.","14/Feb/12 02:59;brandon.williams;bq. That has a scoping issue, where you set File output, so scope becomes local.

Are you sure you tried the latest patch on this ticket?

Edit: nevermind, I squashed 004 into 003 and never deleted it.","14/Feb/12 05:24;lenn0x;Brandon, other than that, I +1 this patch. If you don't mind reviewing my other patches :) I am about to post another for making compression work with bulk writer too, and it depends on this going in.",14/Feb/12 13:41;brandon.williams;Committed.,"14/Feb/12 13:55;forsberg;bq. Samarth/Erik, How does this patch look?

Overall, it's looking good. I haven't tested the latest version of this patch on a large data set yet, hoping to find time to do that tomorrow.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOMs because delete operations are not accounted,CASSANDRA-3741,12538312,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,akolyadenko,tivv,tivv,13/Jan/12 12:33,12/Mar/19 14:16,13/Mar/19 22:26,03/May/12 15:46,1.1.1,,,,,,0,,,,,"Currently we are moving to new data format where new format is written into new CFs and old one is deleted key-by-key. 
I have started getting OOMs and found out that delete operations are not accounted and so, column families are not flushed (changed == 0 with delete only operations) by storage manager.

This is pull request that fixed this problem for me: https://github.com/apache/cassandra/pull/5",FreeBSD,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-01-13 18:46:30.286,,,no_permission,,,,,,,,,,,,223818,,,Wed May 22 05:43:18 UTC 2013,,,,,,0|i0gno7:,95265,jbellis,jbellis,,,,,,,,,"13/Jan/12 18:46;jbellis;Thanks, Vitalii!

Unfortunately we can't use that patch as is because adding ops * 20 in there is going to throw off the size calculation for other workloads.

Note that the ""throughput"" size for a deletion is NOT zero (see Column.size implementation).  It sounds like you abruptly changed your workload from doing a bunch of larger inserts, then hit it with a ton of deletes all at once and OOMed before it was able to update its liveRatio estimate.

So the real problem is that if you change workloads dramatically enough, Cassandra's estimates can be off.",13/Jan/12 20:43;hsn;How can you OOM if you replace large inserts with small deletes?,"15/Jan/12 08:54;tivv;Throughput size for deletion IS 0 for me. I am not deleting for some columns, but all columns in a key and operation has 0 columns in this case, this means 0 throughput so I did introduce an overhead for memory operation storage that looks good for me (20 bytes is ~ConcurrentMap size).

To note: I was getting OOM on startup/log replay. 

The problem is that I have started delete-only workload from some Column Families and this won't change as all inserts go into another column families: we are moving. And for full-key delete-only throughput is 0","03/May/12 06:20;akolyadenko;Another attempt to fix this: https://github.com/apache/cassandra/pull/10

Please consider my patch. It's very annoying to have Cassandra dying in such situations.","03/May/12 15:36;jbellis;Thanks, Andriy.  You (and Vitalii) are right; whole-row deletions did indeed have zero throughput because of that behavior.

Committed with a change to 12 bytes (long + int from deletion info) to match what we do with Column sizes.  (We measure the serialized size in Memtable.currentThroughput, then multiply by liveRatio to get a better estimate of the in-memory size.  Mixing internal overhead as in CSLM would actually double count that.  I've also introduced CASSANDRA-4215 to clean this up more for 1.2.","03/May/12 15:39;jbellis;(That said, truncate is still the right solution for mass deletes.)","03/May/12 20:33;jjordan;Procedural question, for git stuff should we still be attaching patches to JIRA?  Or is a link to the github diff enough?","03/May/12 20:38;jbellis;Intent to contribute in public is enough.  github pull requests (and patches sent to the mailing list for that matter) are fine.

For the purposes of record-keeping though, we like to associate these with Jira tickets.",19/Sep/12 16:56;hsn;was this backported to cassandra 1.0?,"19/Sep/12 17:08;jbellis;No, this is a sensitive area of the code and we don't want to risk destabilizing it.","19/Sep/12 18:32;hsn;1.0 is already destabilized by this bug. If you want to have minimal effect, then add just 1 byte to live bytes count for every delete. In non delete only workload, it will have minimal effect.

Its important to have non zero count otherwise it will not be flushed to disk on memory pressure.","24/Sep/12 08:20;hsn;If you do not want to get fixed, then it should be documented as known bug in NEWS.TXT",10/May/13 07:17;akolyadenko;Just would like to report that I have the same behavior with 1.2.4.,"22/May/13 05:43;hsn;i retested it. bug from 1.0 do not exists in 1.1 and 1.2. But its still not optimal and can lead to OOM because it do not adds enough bytes count for tombstone to live data count.

If i remember some hardcoded constant was used, it needs to be raised.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException during hinted handoff,CASSANDRA-3694,12537198,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,04/Jan/12 15:15,12/Mar/19 14:16,13/Mar/19 22:26,04/Jan/12 16:13,1.1.0,,,,,,0,,,,,"{noformat}
ERROR 08:51:00,200 Fatal exception in thread Thread[OptionalTasks:1,5,main]
java.lang.ClassCastException: org.apache.cassandra.dht.BigIntegerToken cannot be cast to org.apache.cassandra.db.RowPosition
        at org.apache.cassandra.db.ColumnFamilyStore.getSequentialIterator(ColumnFamilyStore.java:1286)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1356)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleAllDeliveries(HintedHandOffManager.java:351)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:119)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,04/Jan/12 15:23;slebresne;3694.patch;https://issues.apache.org/jira/secure/attachment/12509411/3694.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-04 15:23:30.496,,,no_permission,,,,,,,,,,,,222707,,,Wed Jan 04 16:13:46 UTC 2012,,,,,,0|i0gn3j:,95172,jbellis,jbellis,,,,,,,,,"04/Jan/12 15:23;slebresne;Fix attached.

For the record, this was introduced by CASSANDRA-3554. I think we should try to make sure we use generics for Range object now as this would typically have caught this one.",04/Jan/12 16:07;jbellis;+1,"04/Jan/12 16:13;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeveledCompactionStrategy is broken because of generation pre-allocation in LeveledManifest.,CASSANDRA-3691,12537079,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,xedin,xedin,03/Jan/12 17:01,12/Mar/19 14:16,13/Mar/19 22:26,03/Jan/12 19:34,1.1.0,,,,,,0,lcs,,,,"LeveledManifest constructor has the following code:

{code}
for (int i = 0; i < generations.length; i++)
{
    generations[i] = new ArrayList<SSTableReader>();
    lastCompactedKeys[i] = new DecoratedKey(cfs.partitioner.getMinimumToken(), null);
}
{code}

But in the DecoratedKey constructor we have:

{code}
assert token != null && key != null && key.remaining() > 0;
{code}

so when you tried to create a CF with LeveledCompressionStrategy that will result in 

{noformat}
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.cassandra.thrift.CassandraServer.applyMigrationOnStage(CassandraServer.java:865)
	at org.apache.cassandra.thrift.CassandraServer.system_add_keyspace(CassandraServer.java:953)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_keyspace.process(Cassandra.java:4103)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:3078)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:188)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.thrift.CassandraServer.applyMigrationOnStage(CassandraServer.java:857)
	... 7 more
Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.cassandra.config.CFMetaData.createCompactionStrategyInstance(CFMetaData.java:770)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:209)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:300)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:281)
	at org.apache.cassandra.db.Table.initCf(Table.java:339)
	at org.apache.cassandra.db.Table.<init>(Table.java:288)
	at org.apache.cassandra.db.Table.open(Table.java:117)
	at org.apache.cassandra.db.migration.AddKeyspace.applyModels(AddKeyspace.java:72)
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
	at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:850)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.cassandra.config.CFMetaData.createCompactionStrategyInstance(CFMetaData.java:752)
	... 14 more
Caused by: java.lang.AssertionError
	at org.apache.cassandra.db.DecoratedKey.<init>(DecoratedKey.java:55)
	at org.apache.cassandra.db.compaction.LeveledManifest.<init>(LeveledManifest.java:79)
	at org.apache.cassandra.db.compaction.LeveledManifest.create(LeveledManifest.java:85)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.<init>(LeveledCompactionStrategy.java:74)
	... 19 more
ERROR 19:52:44,029 Fatal exception in thread Thread[MigrationStage:1,5,main]
{noformat}",,,,,,,,,,,,,,,,03/Jan/12 18:47;slebresne;3691.patch;https://issues.apache.org/jira/secure/attachment/12509326/3691.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-03 18:47:29.676,,,no_permission,,,,,,,,,,,,222589,,,Tue Jan 03 19:34:53 UTC 2012,,,,,,0|i0gn27:,95166,xedin,xedin,,,,,,,,,"03/Jan/12 18:47;slebresne;Oops, missed this one. Patch attached to fix.",03/Jan/12 19:34;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding another datacenter's node results in 0 rows returned on first datacenter,CASSANDRA-3696,12537228,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,j.casares,j.casares,04/Jan/12 19:02,12/Mar/19 14:16,13/Mar/19 22:26,05/Jan/12 22:42,0.8.10,1.0.7,,,,,0,,,,,"On Cassandra-1.0.5:
1. Create a node in C* with a fresh installation and create a keyspace on that node with one column family -

CREATE KEYSPACE test 
WITH placement_strategy = 'SimpleStrategy' 
and strategy_options={replication_factor:1};

use test; 
create column family cf1;

2. Insert values into cf1 -

set cf1[ascii('k')][ascii('c')] = ascii('v');

get cf1[ascii('k')]; 
=> (column=63, value=v, timestamp=1325689630397000) 
Returned 1 results.

3. update the strategy options from simple to networktopology with {Cassandra:1, Backup:1} 
4. read from cf1 to make sure the options change doesn't affect anything -

consistencylevel as LOCAL_QUORUM; 
get cf1[ascii('k')]; 
=> (column=63, value=v, timestamp=1325689630397000) 
Returned 1 results.

5. start a second node in the Backup datacenter 
6. read from cf1 again (on the first node) -

consistencylevel as LOCAL_QUORUM; 
get cf1[ascii('k')]; 
Returned 0 results.

After about 60 seconds, ""get cf1[ascii('k')]"" started to return results again. 

Also, when running at a CL of ONE on 1.0's head, we were able to see issues as well.

But, if more than one node was added to the second datacenter, then replication_strategy is changed, it seems okay.",,,,,,,,,,,,,,,,05/Jan/12 03:10;jbellis;3696.txt;https://issues.apache.org/jira/secure/attachment/12509499/3696.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-04 19:24:52.205,,,no_permission,,,,,,,,,,,,222737,,,Thu Jan 05 22:42:28 UTC 2012,,,,,,0|i0gn4n:,95177,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,"04/Jan/12 19:24;jbellis;ONE is expected to not return results in this situation, if it happens to query the new node.  But LOCAL_QUORUM against the first node should be fine.

Can you post the debug log of a query failing at LOCAL_QUORUM?  1.0 head preferred but 1.0.5 is fine if you already have it.","05/Jan/12 01:13;brandon.williams;I can reproduce this reliably, but only if read repair is on.  With it off, everything works.  Here is failed read:

{noformat}
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 CassandraServer.java (line 323) get_slice
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 StorageProxy.java (line 603) Command/ConsistencyLevel is SliceFromReadCommand(table='Keyspace1', key='303637', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)/LOCAL_QUORUM
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 PropertyFileSnitch.java (line 90) Could not find end point information for cassandra-1/10.179.65.102, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 ReadCallback.java (line 77) Blockfor/repair is 1/true; setting up requests to /10.179.64.227,/10.179.111.137
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 StorageProxy.java (line 624) reading data from /10.179.64.227
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,524 StorageProxy.java (line 644) reading digest from /10.179.111.137
DEBUG [RequestResponseStage:9] 2012-01-05 01:00:51,526 ResponseVerbHandler.java (line 44) Processing response on a callback from 4145@/10.179.111.137
DEBUG [RequestResponseStage:10] 2012-01-05 01:00:51,526 ResponseVerbHandler.java (line 44) Processing response on a callback from 4144@/10.179.64.227
DEBUG [RequestResponseStage:9] 2012-01-05 01:00:51,526 AbstractRowResolver.java (line 66) Preprocessed digest response
DEBUG [RequestResponseStage:10] 2012-01-05 01:00:51,526 AbstractRowResolver.java (line 66) Preprocessed data response
DEBUG [RequestResponseStage:9] 2012-01-05 01:00:51,527 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,527 StorageProxy.java (line 672) Read: 3 ms.
{noformat}

In this case, 10.179.65.102 and 10.179.111.137 are in the first datacenter, and 10.179.64.227 is in the second.  Both DCs have an RF of one, .102 is always the coordinator and the client is using a single connection.

For comparison, here is a successful local read where read repair did not fire (set to 10%):
{noformat}

DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,519 CassandraServer.java (line 323) get_slice
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,519 StorageProxy.java (line 603) Command/ConsistencyLevel is SliceFromReadCommand(table='Keyspace1', key='303632', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)/LOCAL_QUORUM
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,519 PropertyFileSnitch.java (line 90) Could not find end point information for cassandra-1/10.179.65.102, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,520 ReadCallback.java (line 77) Blockfor/repair is 1/false; setting up requests to cassandra-1/10.179.65.102
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,520 PropertyFileSnitch.java (line 90) Could not find end point information for cassandra-1/10.179.65.102, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,520 StorageProxy.java (line 619) reading data locally
DEBUG [ReadStage:93] 2012-01-05 01:00:51,520 StorageProxy.java (line 763) LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='303632', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG [ReadStage:93] 2012-01-05 01:00:51,520 CollationController.java (line 192) collectAllData
DEBUG [ReadStage:93] 2012-01-05 01:00:51,521 SliceQueryFilter.java (line 123) collecting 0 of 5: C0:false:34@1325724968371
DEBUG [ReadStage:93] 2012-01-05 01:00:51,521 SliceQueryFilter.java (line 123) collecting 1 of 5: C1:false:34@1325724968371
DEBUG [ReadStage:93] 2012-01-05 01:00:51,521 SliceQueryFilter.java (line 123) collecting 2 of 5: C2:false:34@1325724968371
DEBUG [ReadStage:93] 2012-01-05 01:00:51,521 SliceQueryFilter.java (line 123) collecting 3 of 5: C3:false:34@1325724968371
DEBUG [ReadStage:93] 2012-01-05 01:00:51,521 SliceQueryFilter.java (line 123) collecting 4 of 5: C4:false:34@1325724968371
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,522 StorageProxy.java (line 672) Read: 2 ms.
{noformat}

And one where it asked .137 for data (but did not leave the DC or repair):

{noformat}
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,514 CassandraServer.java (line 323) get_slice
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,514 StorageProxy.java (line 603) Command/ConsistencyLevel is SliceFromReadCommand(table='Keyspace1', key='303731', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)/LOCAL_QUORUM
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,514 PropertyFileSnitch.java (line 90) Could not find end point information for cassandra-1/10.179.65.102, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,514 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,514 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,515 ReadCallback.java (line 77) Blockfor/repair is 1/false; setting up requests to /10.179.111.137
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,515 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,515 StorageProxy.java (line 624) reading data from /10.179.111.137
DEBUG [RequestResponseStage:12] 2012-01-05 01:00:51,517 ResponseVerbHandler.java (line 44) Processing response on a callback from 4143@/10.179.111.137
DEBUG [RequestResponseStage:12] 2012-01-05 01:00:51,517 AbstractRowResolver.java (line 66) Preprocessed data response
DEBUG [RequestResponseStage:12] 2012-01-05 01:00:51,517 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,517 StorageProxy.java (line 672) Read: 2 ms.
{noformat}","05/Jan/12 03:10;jbellis;Fix attached.  The problem is we were only pulling out the local-dc replicas when RR was enabled.  We still need to force a local-DC replica to the front of the endpoints list otherwise, since that will get the data read.",05/Jan/12 15:33;jbellis;(patch is against 1.0 but probably needed by 0.8 too.),05/Jan/12 18:38;vijay2win@yahoo.com;+1,05/Jan/12 22:42;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when running upgradesstables,CASSANDRA-3655,12535918,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,tupshin,tupshin,20/Dec/11 22:33,12/Mar/19 14:16,13/Mar/19 22:26,03/Jan/12 15:27,1.0.7,,,,,,0,compaction,,,,"Running a test upgrade from 0.7(version f sstables) to 1.0.
upgradesstables runs for about 40 minutes and then NPE's when trying to retrieve a key.

No files have been succesfully upgraded. Likely related is that scrub (without having run upgrade) consumes all RAM and OOMs.

Possible theory is that a lot of paths call IPartitioner's decorateKey, and, at least in the randompartitioner's implementation, if any of those callers pass a null ByteBuffer, they key will be null in the stack trace below.


java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
	at org.apache.cassandra.db.compaction.CompactionManager.performSSTableRewrite(CompactionManager.java:219)
	at org.apache.cassandra.db.ColumnFamilyStore.sstablesRewrite(ColumnFamilyStore.java:970)
	at org.apache.cassandra.service.StorageService.upgradeSSTables(StorageService.java:1540)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:65)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:92)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:137)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:102)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:87)
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:200)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:172)
	at org.apache.cassandra.db.compaction.CompactionManager$4.perform(CompactionManager.java:229)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
",1.0.5 + patch for https://issues.apache.org/jira/browse/CASSANDRA-3618,,,,,,,,,,,,,,,22/Dec/11 19:37;jbellis;3655.txt;https://issues.apache.org/jira/secure/attachment/12508422/3655.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-22 19:37:18.483,,,no_permission,,,,,,,,,,,,221601,,,Wed Jan 04 20:03:58 UTC 2012,,,,,,0|i0gmmn:,95096,slebresne,slebresne,,,,,,,,,"22/Dec/11 19:37;jbellis;The stacktrace indicates that either the controller or the cf parameters have to be null.  (key being null would NOT cause this exception.)

Controller is always initialized to non-null except in one place in the Streaming code which should be irrelevant here.  And cf comes from merge(rows) which should also return non-null unless there's a deserialization exception.  I bet that's what's happening -- there should be errors in the log about ""Skipping row X"" in that case.

Still, we shouldn't be ignoring those exceptions -- we should kill the compaction (or upgrade) and let the operator decide how to deal with them (e.g. with scrub).

Patch to fix that behavior, clean up the Streaming controller use, and add extra assertions.","22/Dec/11 21:49;slebresne;+1 (It would help to know if the log from the error had ""Skipping row X"" to know if the hypothesis is correct but in any case I agree we shouldn't ignore those exceptions)",03/Jan/12 15:27;slebresne;Committed as even if didn't fixed the issue (which it probably did) it's an improvement.,04/Jan/12 20:03;tupshin;Original problem confirmed fixed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression chunk_length_kb is not correctly returned for thrift/avro,CASSANDRA-3558,12533603,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,02/Dec/11 11:59,12/Mar/19 14:16,13/Mar/19 22:26,06/Dec/11 12:31,1.0.6,,,,,,0,compression,,,,"CASSANDRA-3492 fixed the interpretation of chunk_length_kb as a size in bytes but infortunately forgot to convert it back to kb when returning it for thrift/avro. In particular, this means that a {{describe cf}} would return things like {{chunk_length_kb: 65535}}.

I'm afraid that because migration uses Avro this is kind of a problem. One may have to issue an 'update column family' with the right chunk_length_kb to be sure to be in a safe place.",,,,,,,,,,,,,,,,02/Dec/11 12:16;slebresne;0001-Correctly-handle-chunk_length_in_kb.patch;https://issues.apache.org/jira/secure/attachment/12505878/0001-Correctly-handle-chunk_length_in_kb.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-06 12:31:40.462,,,no_permission,,,,,,,,,,,,219331,,,Tue Dec 06 12:31:40 UTC 2011,,,,,,0|i0gldz:,94895,xedin,xedin,,,,,,,,,"02/Dec/11 12:16;slebresne;Fix attached. I've added a test for this in the distributed tests (at https://github.com/riptano/cassandra-dtest).

We could have a unit test for this but without CASSANDRA-3559, it's more pain that it should be.",06/Dec/11 12:31;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible livelock during commit log playback,CASSANDRA-3751,12538739,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jchakerian,jchakerian,18/Jan/12 05:19,12/Mar/19 14:16,13/Mar/19 22:26,01/Feb/12 19:43,0.8.10,1.0.8,,,,,0,commitlog,,,,"In CommitLog.recover, there seems to be the possibility of concurrent inserts to tablesRecovered (a HashSet) in the Runnables instantiated a bit below (line 323 in 1.0.7). This apparently happened during a commit log playback during startup of a node that had not shut down cleanly (the cluster was under heavy load previously and there were several gigabytes of commit logs), resulting in two threads running in perpetuity (2 cores were at 100% from running these threads), preventing the node from coming up. The relevant portion of the stack trace is:

{noformat}
INFO   | jvm 1    | 2012/01/16 16:54:42 | ""MutationStage:25"" prio=10 tid=0x00002aaad01e0800 nid=0x6f62 runnable [0x0000000044d54000]
INFO   | jvm 1    | 2012/01/16 16:54:42 |    java.lang.Thread.State: RUNNABLE
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashMap.put(HashMap.java:374)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashSet.add(HashSet.java:200)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:338)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.lang.Thread.run(Thread.java:662)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 
INFO   | jvm 1    | 2012/01/16 16:54:42 | ""MutationStage:21"" prio=10 tid=0x00002aaad00a2800 nid=0x6f5e runnable [0x0000000044950000]
INFO   | jvm 1    | 2012/01/16 16:54:42 |    java.lang.Thread.State: RUNNABLE
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashMap.put(HashMap.java:374)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashSet.add(HashSet.java:200)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:338)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.lang.Thread.run(Thread.java:662)

{noformat}

The most recently modified file in the commit log directory was this entry:
{noformat}
-rw-r----- 1 <redacted> <redacted>    0 Jan 16 16:03 CommitLog-1326758622599.log
{noformat}
though I'm not sure if this was related or not. ",Linux (CentOS 5.7),,,,,,,,,,,,,,,27/Jan/12 23:34;jbellis;3751.txt;https://issues.apache.org/jira/secure/attachment/12512257/3751.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-18 05:42:35.736,,,no_permission,,,,,,,,,,,,224242,,,Wed Feb 01 19:43:04 UTC 2012,,,,,,0|i0gnsn:,95285,rbranson,rbranson,,,,,,,,,18/Jan/12 05:42;rbranson;Which commitlog_sync option do you have configured on this node?,18/Jan/12 05:52;jchakerian;periodic,"27/Jan/12 23:34;jbellis;You're absolutely right, tablesRecovered is not threadsafe but it's mutated by up to concurrent_writes threads during log replay.  Patch attached to switch to NBHS.",27/Jan/12 23:35;jbellis;patch is against 1.0 but applies cleanly to 0.8 as well.,"31/Jan/12 00:59;rbranson;Reviewed, +1",01/Feb/12 19:43;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't cleanup after I moved a token.,CASSANDRA-3712,12537765,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,hnicol,hnicol,09/Jan/12 14:37,12/Mar/19 14:16,13/Mar/19 22:26,13/Feb/12 23:30,1.0.8,,,,,,0,,,,,"Before cleanup failed, I moved one node's token.
My cluster had 10GB data on 2 nodes. Data repartition was bad, tokens were 165[...] and 155[...].
I moved 155 to 075[...], then adjusted to 076[...]. The moves were correctly processed, with no exception.
But then, when I wanted to cleanup, it failed and keeps failing, on both nodes.

Other maintenance procedures like repair, compact or scrub work.
All the data is in the URLs CF.

Example session log:
nodetool cleanup fails:
$ ./nodetool --host cnode1 cleanup
Error occured during cleanup
java.util.concurrent.ExecutionException: java.lang.AssertionError
 at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
 at java.util.concurrent.FutureTask.get(FutureTask.java:83)
 at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
 at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:237)
 at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:958)
 at org.apache.cassandra.service.StorageService.forceTableCleanup(StorageService.java:1604)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
 at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
 at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
 at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
 at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
 at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
 at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
 at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
 at sun.rmi.transport.Transport$1.run(Transport.java:159)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
 at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
 at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.AssertionError
 at org.apache.cassandra.db.Memtable.put(Memtable.java:136)
 at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:780)
 at org.apache.cassandra.db.index.keys.KeysIndex.deleteColumn(KeysIndex.java:82)
 at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:438)
 at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:754)
 at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:63)
 at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:241)
 at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
 at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
 at java.util.concurrent.FutureTask.run(FutureTask.java:138)
 ... 3 more


The server log looks like this:
 INFO [CompactionExecutor:260] 2012-01-09 14:08:41,716 CompactionManager.java (line 702) Cleaning up SSTableReader(path='/ke/cassandra/data/kev3/URLs-hc-457-Data.db')
 INFO [OptionalTasks:1] 2012-01-09 14:08:47,220 MeteredFlusher.java (line 62) flushing high-traffic column family CFS(Keyspace='kev3', ColumnFamily='URLs') (estimated 156787206 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:08:47,226 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-URLs.URLs_1_idx@1347180703(16324791/156973287 serialized/live bytes, 173288 ops)
 INFO [FlushWriter:23] 2012-01-09 14:08:47,236 Memtable.java (line 240) Writing Memtable-URLs.URLs_1_idx@1347180703(16324791/156973287 serialized/live bytes, 173288 ops)
 INFO [pool-1-thread-1] 2012-01-09 14:08:51,003 Memtable.java (line 180) CFS(Keyspace='kev3', ColumnFamily='URLs.URLs_1_idx') liveRatio is 7.692510757866615 (just-counted was 4.512127842861816).  calculation took 8648ms for 97329 columns
 INFO [FlushWriter:23] 2012-01-09 14:08:54,360 Memtable.java (line 277) Completed flushing /ke/cassandra/data/kev3/URLs.URLs_1_idx-hc-143-Data.db (26375495 bytes)
 INFO [ScheduledTasks:1] 2012-01-09 14:08:55,566 GCInspector.java (line 123) GC for ParNew: 206 ms for 1 collections, 934108624 used; max is 2034237440
 INFO [OptionalTasks:1] 2012-01-09 14:08:57,289 MeteredFlusher.java (line 62) flushing high-traffic column family CFS(Keyspace='kev3', ColumnFamily='URLs') (estimated 188842513 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:08:57,297 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-URLs.URLs_1_idx@164871630(19662738/189069779 serialized/live bytes, 208494 ops)
 INFO [FlushWriter:23] 2012-01-09 14:08:57,297 Memtable.java (line 240) Writing Memtable-URLs.URLs_1_idx@164871630(19662738/189069779 serialized/live bytes, 208494 ops)
 INFO [ScheduledTasks:1] 2012-01-09 14:08:57,619 GCInspector.java (line 123) GC for ParNew: 402 ms for 2 collections, 981893424 used; max is 2034237440
 INFO [FlushWriter:23] 2012-01-09 14:09:05,944 Memtable.java (line 277) Completed flushing /ke/cassandra/data/kev3/URLs.URLs_1_idx-hc-144-Data.db (31755390 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:09:06,447 MeteredFlusher.java (line 62) flushing high-traffic column family CFS(Keyspace='kev3', ColumnFamily='URLs') (estimated 174605041 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:09:06,447 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-URLs.URLs_1_idx@284469330(18158445/174605041 serialized/live bytes, 192702 ops)
 INFO [FlushWriter:23] 2012-01-09 14:09:06,447 Memtable.java (line 240) Writing Memtable-URLs.URLs_1_idx@284469330(18158445/174605041 serialized/live bytes, 192702 ops)
ERROR [CompactionExecutor:260] 2012-01-09 14:09:06,448 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[CompactionExecutor:260,1,RMI Runtime]
java.lang.AssertionError
	at org.apache.cassandra.db.Memtable.put(Memtable.java:136)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:780)
	at org.apache.cassandra.db.index.keys.KeysIndex.deleteColumn(KeysIndex.java:82)
	at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:438)
	at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:754)
	at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:63)
	at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:241)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)




","java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)

Ubuntu 10.04.2 LTS 64-Bit
RAM: 2GB / 1GB free
Data partition: 80% free on the most used server.",,,,,,,,,,,,,,,08/Feb/12 15:44;yukim;0001-Add-flush-and-cleanup-race-test.patch;https://issues.apache.org/jira/secure/attachment/12513825/0001-Add-flush-and-cleanup-race-test.patch,10/Feb/12 18:46;yukim;0002-Acquire-lock-when-updating-index.patch;https://issues.apache.org/jira/secure/attachment/12514136/0002-Acquire-lock-when-updating-index.patch,10/Feb/12 21:26;jbellis;3712-v3.txt;https://issues.apache.org/jira/secure/attachment/12514159/3712-v3.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-01-09 23:16:10.507,,,no_permission,,,,,,,,,,,,223271,,,Mon Feb 13 23:30:35 UTC 2012,,,,,,0|i0gnbj:,95208,jbellis,jbellis,,,,,,,,,"09/Jan/12 23:16;jbellis;We'll look into the root cause, but in the meantime, restarting the node should fix this.","10/Jan/12 13:32;hnicol;I restarted my nodes, but it didn't fix this.","10/Jan/12 17:30;jbellis;Then you'll probably need to drop your indexes, then rebuild after the cleanup.","11/Jan/12 17:07;hnicol;Thanks, that fixed cleanup.","08/Feb/12 15:44;yukim;When updating/deleting CFS backed secondary index, currently it doesn't acquire Memtable lock. So when flush and cleanup occurred at the same time on indexed column family, there is a chance of getting this AssertionError.

Test case attached to reproduce the same error. Note that test does not fail always, so you may run several times to see the error.

To fix this, I added CFS#applyDirect method which just acquire and release lock before/after CFS update, and call it from KeysIndex.

Both patches are for 1.0 branch.","08/Feb/12 23:47;jbellis;A couple comments:

- I can't get the new test to fail after a dozen tries. If there isn't a way to make it more robust (say, with explicit sleeps) maybe we should just leave that out.
- Currently the switch locking is done by the callers of the SIM methods, i.e., Table.apply and Table.indexRow. Locking at the column level is not sufficient there, but doing it in both places is redundant. So maybe the right place to lock here would be in the doCleanupCompaction method.","10/Feb/12 18:52;yukim;bq. I can't get the new test to fail after a dozen tries. If there isn't a way to make it more robust (say, with explicit sleeps) maybe we should just leave that out.

In my env, it fails 1/3 or 1/4 try. I cannot think of better test program, so you can leave it out.

bq. Currently the switch locking is done by the callers of the SIM methods, i.e., Table.apply and Table.indexRow. Locking at the column level is not sufficient there, but doing it in both places is redundant. So maybe the right place to lock here would be in the doCleanupCompaction method.

You are right. Previous patch acquires lock too often. I placed lock/unlock inside doCleanupCompaction in newer patch.
In order to do that, I have to expose Table.switchlock to public, but I don't know if that is the right way.","10/Feb/12 21:26;jbellis;The new patch acquires the switchlock for the entire cleanup, so no flushes can happen.  For large sstables this could cause a problem.  How about attached v3, is that sufficient for it to be safe?","13/Feb/12 18:05;yukim;I ran my unit test enough and I see no error.
+1",13/Feb/12 23:30;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Its impossible to removetoken joining down node,CASSANDRA-3737,12538297,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,tivv,tivv,13/Jan/12 09:32,12/Mar/19 14:16,13/Mar/19 22:26,14/Jan/12 00:42,1.0.8,,,,,,0,,,,,"We have a node that incidentaly started to join cluster. Admins made it down quicky, so now it looks :
10.112.0.234    datacenter1 rack1       Down   Joining 46.83 GB        2,90%   15893087653239874101909022095979644640  
And I can't removetoken such a node:

 nodetool -h tap9600 removetoken 15893087653239874101909022095979644640
Exception in thread ""main"" java.lang.UnsupportedOperationException: Token not found.
	at org.apache.cassandra.service.StorageService.removeToken(StorageService.java:2376)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
",FreeBSD ,,,,,,,,,,,,,,,13/Jan/12 23:13;brandon.williams;3737.txt;https://issues.apache.org/jira/secure/attachment/12510540/3737.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-13 18:17:49.183,,,no_permission,,,,,,,,,,,,223803,,,Sat Jan 14 00:42:52 UTC 2012,,,,,,0|i0gnmf:,95257,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,13/Jan/12 18:17;brandon.williams;You shouldn't need to remove this node; it should be removed after a minute or so since it is a fat client.,"13/Jan/12 21:50;brandon.williams;bq. You shouldn't need to remove this node; it should be removed after a minute or so since it is a fat client.

But we have a bug introduced by CASSANDRA-957; bootstrapping nodes start in hibernate, which is a dead state, resulting in setHasToken being set to true in handleMajorStateChange:

{noformat}
            epState.setHasToken(true); // fat clients won't have a dead state
{noformat}

That comment is no longer true. :(","13/Jan/12 23:13;brandon.williams;Since the boolean value of hibernate has no bearing, only its presence in STATUS, there is no need for a bootstrapping node to ever use the hibernation state.",14/Jan/12 00:30;vijay2win@yahoo.com;+1,14/Jan/12 00:42;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing compaction strategy from Leveled to SizeTiered logs millions of messages about nothing to compact,CASSANDRA-3666,12536283,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,vjevdokimov,vjevdokimov,23/Dec/11 13:09,12/Mar/19 14:16,13/Mar/19 22:26,03/Jan/12 15:32,1.0.7,,,,,,0,compaction,,,,"When column family compaction strategy is changed from Leveled to SizeTiered and there're Leveled compaction tasks pending, Cassandra starting to flood in logs with thousands per sec messages:

Nothing to compact in ColumnFamily1.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)

As a result, log disk is full and system is down.",Windows Server 2008 R2 64bit,,,,,,,,,,,,,,,23/Dec/11 20:21;jbellis;3666.txt;https://issues.apache.org/jira/secure/attachment/12508564/3666.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-23 20:11:44.548,,,no_permission,,,,,,,,,,,,221966,,,Tue Jan 03 15:32:42 UTC 2012,,,,,,0|i0gmrr:,95119,vjevdokimov,vjevdokimov,,,,,,,,,23/Dec/11 20:11;jbellis;Were there any error messages logged?,23/Dec/11 20:21;jbellis;I bet the culprit is how LCS sets the min/max compaction threshold but STCS does not.  Can you try the attached patch?,"03/Jan/12 14:45;slebresne;+1 on the patch (but we can wait on Viktor to confirm it does fix it for him).

(I actually think that it's the settings of the threshold in LCS that is the ugly part, but let's not bother with that)",03/Jan/12 15:32;slebresne;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrade problems from 1.0 to trunk,CASSANDRA-3804,12540242,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,28/Jan/12 04:01,12/Mar/19 14:16,13/Mar/19 22:26,23/Feb/12 17:10,1.1.0,,,,,,0,,,,,"A 3-node cluster is on version 0.8.9, 1.0.6, or 1.0.7 and then one and only one node is taken down, upgraded to trunk, and started again. An rpc timeout exception happens if counter-add operations are done. It usually takes between 1 and 500 add operations before the failure occurs. The failure seems to happen sooner if the coordinator node is NOT the one that was upgraded. Here is the error: 

{code}

======================================================================
ERROR: counter_upgrade_test.TestCounterUpgrade.counter_upgrade_test
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 187, in runTest
    self.test(*self.arg)
  File ""/home/tahooie/cassandra-dtest/counter_upgrade_test.py"", line 50, in counter_upgrade_test
    cursor.execute(""UPDATE counters SET row = row+1 where key='a'"")
  File ""/usr/local/lib/python2.7/dist-packages/cql/cursor.py"", line 96, in execute
    raise cql.OperationalError(""Request did not complete within rpc_timeout."")
OperationalError: Request did not complete within rpc_timeout.

{code}

","ubuntu, cluster set up with ccm.",,,,,,,,,,,,,,,23/Feb/12 10:26;xedin;CASSANDRA-3804-1.1-v2.patch;https://issues.apache.org/jira/secure/attachment/12515740/CASSANDRA-3804-1.1-v2.patch,15/Feb/12 07:07;xedin;CASSANDRA-3804-1.1.patch;https://issues.apache.org/jira/secure/attachment/12514605/CASSANDRA-3804-1.1.patch,11/Feb/12 19:17;xedin;CASSANDRA-3804.patch;https://issues.apache.org/jira/secure/attachment/12514228/CASSANDRA-3804.patch,16/Feb/12 16:21;slebresne;node1.log;https://issues.apache.org/jira/secure/attachment/12514813/node1.log,16/Feb/12 16:21;slebresne;node2.log;https://issues.apache.org/jira/secure/attachment/12514814/node2.log,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-01-30 14:01:11.703,,,no_permission,,,,,,,,,,,,225656,,,Tue Apr 24 17:01:37 UTC 2012,,,,,,0|i0gofr:,95389,slebresne,slebresne,,,,,,,,,"30/Jan/12 14:01;slebresne;This is not counter related (but you have to use CL.ALL to reproduce without counters as otherwise it's hidden by the fact that only the non-upgraded coordinator acknowledges writes) and it is related to CASSANDRA-1391.

This is due to the inability of doing schema changes in a mixed pre/post-1.1 cluster, if I trust the following log (from the upgraded node):
{noformat}
java.lang.RuntimeException: java.io.IOException: Can't accept schema migrations from Cassandra versions previous to 1.1, please update first.
        at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:544)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Can't accept schema migrations from Cassandra versions previous to 1.1, please update first.
        at org.apache.cassandra.service.MigrationManager.deserializeMigrationMessage(MigrationManager.java:233)
        at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:231)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
{noformat}

There is however two problems imho:
# Not supporting migrations during the upgrade process is one thing, but it should put the cluster in a broken state, which I'm not sure it doesn't do. Ideally, new nodes would still accept old migrations from old nodes, but would refuse to schema changes themselves until they know all nodes are upgraded. We could then throw an UnavailableException with a message.
# On top of the exception above, the logs during that test are filled with errors that don't sound too reassuring. On every node (upgraded or not), there is a handful of:
{noformat}
ERROR [MutationStage:34] 2012-01-30 14:35:39,041 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[MutationStage:34,5,main]
java.io.IOError: java.io.EOFException
        at org.apache.cassandra.db.TruncateVerbHandler.doVerb(TruncateVerbHandler.java:66)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
        at java.io.DataInputStream.readUTF(DataInputStream.java:572)
        at java.io.DataInputStream.readUTF(DataInputStream.java:547)
        at org.apache.cassandra.db.TruncationSerializer.deserialize(Truncation.java:80)
        at org.apache.cassandra.db.TruncationSerializer.deserialize(Truncation.java:70)
        at org.apache.cassandra.db.TruncateVerbHandler.doVerb(TruncateVerbHandler.java:44)
        ... 4 more
{noformat}
On the upgraded node, there is a few:
{noformat}
ERROR [MutationStage:38] 2012-01-30 14:35:50,772 RowMutationVerbHandler.java (line 61) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1000
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:129)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:401)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:409)
        at org.apache.cassandra.db.RowMutation.fromBytes(RowMutation.java:357)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:42)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}
And on the non-upgraded ones, there is a few:
{noformat}
ERROR [GossipStage:1] 2012-01-30 14:35:13,363 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:1,5,main]
java.lang.UnsupportedOperationException: Not a time-based UUID
        at java.util.UUID.timestamp(UUID.java:308)
        at org.apache.cassandra.service.MigrationManager.updateHighestKnown(MigrationManager.java:121)
        at org.apache.cassandra.service.MigrationManager.rectify(MigrationManager.java:99)
        at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:83)
        at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:806)
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:849)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:908)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:68)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}","30/Jan/12 14:39;xedin;This was discussed with Jonathan on the process of CASSANDRA-1391, users should make sure that all of the nodes are updated to 1.1 before running any schema changes because it's impossible to apply old migrations even if we accept them and users will be getting exceptions from your #2 anyway.","30/Jan/12 14:52;jbellis;We can't support mixed-version schema changes, but we should make sure that we don't leave the cluster broken if a user does that anyway.","30/Jan/12 15:31;xedin;This exception (taken from Sylvain's #2) explains what will happen when you only partially migrate:

{noformat}
ERROR [GossipStage:1] 2012-01-30 14:35:13,363 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:1,5,main]
java.lang.UnsupportedOperationException: Not a time-based UUID
        at java.util.UUID.timestamp(UUID.java:308)
        at org.apache.cassandra.service.MigrationManager.updateHighestKnown(MigrationManager.java:121)
        at org.apache.cassandra.service.MigrationManager.rectify(MigrationManager.java:99)
        at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:83)
        at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:806)
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:849)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:908)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:68)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat} 

As we switched from Time-based UUID for schema versions MigrationManager on the old nodes will fail all the time when nodes with new schema start-up or when they will request migrations from it (because they see that their schema version is different from others). Even if we make a fix in MigrationManager.rectify(...) method for 1.0.x, nodes with new/old schema will never come to agreement because of different types of the UUID and because they unable to run schema mutations anymore.","30/Jan/12 15:43;jbellis;""Never come to agreement"" is fine as long as normal reads/writes (against existing CFs) continue to work.","30/Jan/12 15:59;xedin;bq. ""Never come to agreement"" is fine as long as normal reads/writes (against existing CFs) continue to work.

reads/writes should work against existing CFs. failure from description and first comment are related to the way how cassandra-dtest works because it tries to re-create schema for every test-case which won't work for in the mixed version cluster, if, for example, it was to create a ColumnFamily before updating one of the nodes to trunk, reads/writes to that ColumnFamily would still work after update even tho nodes will be in schema disagreement.","10/Feb/12 05:01;jbellis;Can we log something telling the user ""you can't change schema until you finish upgrade everything"" instead of a scary-looking ""not a time-based UUID"" uncaught exception?","11/Feb/12 19:17;xedin;patch for cassandra-1.0 branch, which does a UUID version check in MigrationManager.rectify(UUID, InetAddress) to determine if version is time-based UUID if not it would log an error and return.","13/Feb/12 23:34;jbellis;It's the 1.0 side that has the problem?  I missed that...  I'd rather not require people upgrade to 1.0.8+, before upgrading to 1.1.

Can we just have 1.1 use MS version info to not send schema to nodes that can't understand it?","13/Feb/12 23:37;xedin;We can do that. ""java.lang.UnsupportedOperationException: Not a time-based UUID"" happens on 1.0 nodes so I thought that it would be appropriate to fix it there also.",15/Feb/12 07:07;xedin;changes the MigrationManager.announce to skip the nodes with versions older than 1.1,"16/Feb/12 14:38;slebresne;Tried the two attached patches. It does remove a bunch of the exceptions. For some reason there is still quite a few EOFExceptions related to truncation, but the test doesn't truncate at all so I'm not sure were that coming from.  I'm attaching the logs from the nodes for reference (that's the log after the two patches are applied). node1.log is the 1.1 node and node2.log is a 1.0 node. The thing that triggers those exception is the creation of a CF on node2 (the old one).

So it'd be nice to figure out what triggers those exception, but if we're going to patch both 1.1 and 1.0, why not just (or rather in addition) have schema changes check the (known) version of all other nodes before doing anything and just throw an InvalidRequestException if we know the schema change will fail?
",16/Feb/12 15:19;xedin;I will investigate how truncate is related to the schema modifications. I don't think that we have any intention to patch both 1.0 and 1.1 because we don't want to require people to update to 1.0.8+ before upgrading to 1.1. ,"16/Feb/12 15:37;slebresne;I'm personally fine saying that it is recommended to upgrade to 1.0.8+ before 1.1 if that allow smoother/safer upgrade. Besides, I don't think we really have a choice if we want to avoid scary looking logs issue. And I don't really understand your remark anyway since the patches *you* attached to this issue targets *both* 1.0 and 1.1.","16/Feb/12 16:01;jbellis;If 1.1 doesn't send new-style schema changes to 1.0, that should be sufficient to avoid scary-looking logs.","16/Feb/12 16:02;xedin;if you take a look at the comment from Jonathan at ""13/Feb/12 23:34"" it will give you a better understanding why there are both 1.0 and 1.1 patches.","16/Feb/12 16:40;slebresne;Reattaching the two logs with only the 1.1 patch applied. Putting asides all the truncation related EOFException for now:
* on node1 (the 1.1 node), we do see a ""Can't accept schema migrations from Cassandra versions previous to 1.1, please update first"", though I suppose it would be nice to  avoid the 'Fatal exception' and stacktrace that tends to scare people.
* on node2, we still have ""UnsupportedOperationException: Not a time-based UUID"" exceptions.

That being said, even if we fix all those exceptions (which we should, there no question on that), it is still the case that the schema change is applied to the 1.0 nodes and no error is returned to the user (that is, outside of the log file), but the added column family is not really usable (the user will get timeouts) at least until the cluster upgrade is complete. Even if the user do watch the log and see the error (and will probably assume the creation failed), the column family is still created. That's not really user friendly. So I still think it would be a good idea to add code to 1.0 and 1.1 to refuse upfront schema changes when the cluster is known to have mixed pre-1.1/post-1.1 versions. That don't necessarily mean we would *require* an upgrade to 1.0.8+ before upgrading to 1.1, but it would mean that for all those that does upgrade from 1.0.8 (possibly a majority of users), we're being more user friendly.","19/Feb/12 20:12;xedin;I have found the problem with EOFException in Truncate - it was caused by MIGRATION_REQUEST  misinterpreted by 1.0 as TRUNCATE message, I have added a check into MM.rectifySchema(UUID, InetAddress) to skip that request if node with changed schema is older than 1.1. Two patches in combination now return no exceptions but node 1.0 without patch would still throw UnsupportedOperationException because Gossiper always propagates passive schema announce.

Edit: I have also changed ""Can't accept schema migrations from Cassandra versions previous to 1.1, please update first."" in 1.1 to be log error instead of IOException.","23/Feb/12 10:13;slebresne;bq. it was caused by MIGRATION_REQUEST misinterpreted by 1.0 as TRUNCATE message

That shouldn't be. In the StorageService.Verb enumeration, MIGRATION_REQUEST needs to be moved to the end of the enum. Otherwise lots of stuff will be misinterpreted.","23/Feb/12 10:23;xedin;yeah, it seems like I have missed the comment about that, but fact is fact it was misinterpreted because of that. Have placed MIGRATION_REQUEST at the end of the Verb enum in updated v2.",23/Feb/12 10:26;xedin;fixed the comment about migration request.,"23/Feb/12 17:10;slebresne;Ok, it does leave some InsupportedOperationException on the 1.0 but it clearly improve things and the misplaced MIGRATION_REQUEST is a real bug so I've committed v2 (putting MIGRATION_REQUEST before the UNUSED ones as that's where it should be added). I'll open another issue to see if we can make smooth even more the case where people (mistakenly) do schema update in a mixed cluster.","24/Apr/12 17:01;tpatterson;I removed the dtest because it always fails, and will always fail in a properly running cluster.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bulk loader no longer finds sstables,CASSANDRA-3752,12538787,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,18/Jan/12 13:06,12/Mar/19 14:16,13/Mar/19 22:26,22/Jan/12 17:24,1.1.0,,,,,,0,,,,,"It looks like CASSANDRA-2749 broke it:

{noformat}
 WARN 13:02:20,107 Invalid file 'Standard1' in data directory /var/lib/cassandra/data/Keyspace1.
{noformat}",,,,,,,,,,CASSANDRA-3740,,,,,,19/Jan/12 16:53;brandon.williams;3752.txt;https://issues.apache.org/jira/secure/attachment/12511136/3752.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-22 17:24:19.265,,,no_permission,,,,,,,,,,,,224290,,,Sun Jan 22 17:24:19 UTC 2012,,,,,,0|i0gnt3:,95287,xedin,xedin,,,,,,,,,"19/Jan/12 16:53;brandon.williams;Patch to use new directory layout, also add stream throttling to prevent an NPE when there is no yaml config (in anticipation of CASSANDRA-3740)",22/Jan/12 17:24;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sub-columns removal is broken in 1.1,CASSANDRA-3872,12541652,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,07/Feb/12 16:56,12/Mar/19 14:16,13/Mar/19 22:26,14/Feb/12 11:01,1.1.0,,,,,,0,,,,,"CASSANDRA-3716 actually broke sub-columns deletion. The reason is that in QueryFilter.isRelevant, we've switched in checking getLocalDeletionTime() only (without looking for isMarkedForDelete). But for columns containers (in this case SuperColumn), the default local deletion time when not deleted is Integer.MIN_VALUE. In other words, a SC with only non-gcable tombstones will be considered as not relevant (while it should).

This is caught by two unit tests (RemoveSuperColumnTest and RemoveSubColumnTest) that are failing currently.",,,,,,,,,,,,,,,,07/Feb/12 17:06;slebresne;3872.patch;https://issues.apache.org/jira/secure/attachment/12513634/3872.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-07 23:54:21.916,,,no_permission,,,,,,,,,,,,226939,,,Tue Feb 14 11:01:22 UTC 2012,,,,,,0|i0gp93:,95521,jbellis,jbellis,,,,,,,,,"07/Feb/12 17:06;slebresne;Attaching patch for this. The idea is to make containers use MAX_VALUE instead of MIN_VALUE  for localDeleteionTime. There is however two things to consider:
# for containers, we do serialize the localDeletionTime, so we have to be careful for upgrade (mixed version cluster). What the patch does is to recognize when someone provided MIN_VALUE and transforming to MAX_VALUE. This only work when old node send use MIN_VALUE, however *I think* it is actually OK to send MAX_VALUE to old node (pre-1.1 code uses markedForDeleteAt to decide if a container is deleted, not localDeletionTime, so we shouldn't break anything).
# CFS.removeDeleted has to be able to distinguish between an empty container that is marked for deletion (but not gcable) and one that is not marked for deletion. In the former, we should keep the container, not in the latter. This means that this patch actually reintroduce the use of isMarkedForDelete() for containers in CFS.removeDeleted (for containers, isMarkedForDelete is not timing dependent).

Note that there would likely be other ways to fix this issue (reverting CASSANDRA-3716 would be one).

In any case, the patch fixes the two unit tests.","07/Feb/12 23:54;jbellis;Could we do a simpler fix by turning column.mostRecentLiveChangeAt into column.mostRecentChangeAt? I don't think restricting to live columns is actually useful here (although it is in SQF, so we'd need a new method instead of replacing mRLCA).","08/Feb/12 11:11;slebresne;Apologies, I didn't realized that there was actually kind of 2 problems and I mixed both, so let's be more precise.

The regression that CASSANDRA-3716 introduced and that is making the tests fail is that a *live* super column with only non-gcable tombstones is now considered irrelevant, while it is relevant in say 1.0.

That led me to realize that containers are using MIN_VALUE for gLDT when not deleted. Now, leaving aside the tests failure, I do think we should change that. Post-CASSANDRA-3716, we've made sure that for normal columns, testing {{getLocalDeletionTime() < gcbefore}} allows us solely to decide whether the column is gcable (and thus by extension tombstoned) or not. I think it's a good thing, the local deletion time is here for the tombstone gc and so that makes sense. But currently, this is not true for SuperColumns nor ColumnFamily (but it's worst for SC since they actually are IColumn), where {{getLocalDeletionTime() < gcbefore}} means 'either gcable or not deleted at all'. Leaving things that way would be very error-prone imho and defeat the purpose of CASSANDRA-3716 since we're reintroducing subtle differences that are hard to work with.

Now, as it happens, I think we have an old bug in QF.isRelevant (i.e. including in 0.8 and probably before that). Namely that a *live* SC with only non-gcable tombstones is considered relevant, but a *gcable* SC with only non-gcable is considered irrelevant but I don't think it should. To fix that, we can indeed do the mostRecentChangeAt change (and that would fix the unit tests in particular but as said above, I think we should still do the MIN_VALUE->MAX_VALUE change for other reasons). However, that would imply that *gcable* SC with only *gcable* tombstones would be considered relevant (if the max tombstone timestamp is greater than the SC timestamp), so maybe we want to switch to a column.mostRecentNonGcableChangeAt() instead.
I'll open a separate ticket for that change, as this affect previous version too. I'm leaving this one open to focus on the MIN_VALUE->MAX_VALUE change.","10/Feb/12 23:46;jbellis;Isn't this patch kind of a wash?  The MIN_VALUE checks go away but in return we need to add extra isMarkedForDelete checks.
","13/Feb/12 11:15;slebresne;I do not pretend this reduce line of codes, but I do think that it makes it easier to not make subtle mistakes.

Currently, there is a mismatch between how Column (the class) and the two IColumnContainer classes (CF and SC) handles getLocalDeletionTime() for non-deleted. The former uses MAX_VALUE, the latter uses MIN_VALUE. The lack of consistency alone is annoying but as long as SC lives it is made much worst by the fact that SC is both a IColumn and a IColumnContainer.

The attached patch tries to make things more consistent. The localDeletionTime is here for the purpose of tombstone garbage collection, so it seems to me that it is cleaner to use it for that purpose and that purpose only. In other words, with this patch, {{(getLocalDeletionTime() < gcbefore)}} tells you without ambiguity if you're dealing with a gcable tombstone or not.

Now there is the fact that live but empty containers are not returned to the user. I believe that was one of the reason of using MIN_VALUE for live containers. But imho this is a hack and it's much more clear in removeDeleted to read:
{noformat}
if (cf.getColumnCount() == 0 && (!cf.isMarkedForDelete() || cf.getLocalDeletionTime() < gcBefore))
{noformat}
which directly translate into: if the cf is empty and it's either a gcable tombstone or a live cf, we can skip it, rather that having to check the code of ColumnFamily to understand why that does skip live empty CF *and* to have to remember each time you use CF.localDeletionTime that it may be MIN_VALUE for non-deleted CF and assert if it matters or not.","13/Feb/12 23:59;jbellis;+1

Can you add a reference to this ticket to the ""new default is MAX_VALUE"" comment on commit?","14/Feb/12 11:01;slebresne;Committed (with added link to the issue), thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkOutputFormat binds to wrong client address when client is Dual-stack and server is IPv6,CASSANDRA-3839,12540923,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,forsberg,forsberg,02/Feb/12 13:46,12/Mar/19 14:16,13/Mar/19 22:26,02/Feb/12 23:19,1.1.0,,,,,,0,bulkloader,,,,"Trying to run a map/reduce job with BulkOutputFormat, in an environment where the Hadoop nodes have Dual-stack (IPv4+IPv6) and the Cassandra servers are IPv6-only, it seems like the TCP connection setup for streaming is explicitly setting the source address to the IPv4 address of the Hadoop node, even though the destination address is IPv6. 

I'm seeing connection attempts where source address is an IPv4-represented-in-IPv6 address and destination is IPv6 of cassandra node. 

In the log output from the Hadoop M/R job, I see:

{noformat}
2012-02-01 16:49:19,909 WARN org.apache.cassandra.streaming.FileStreamTask: Failed attempt 1 to connect to /2001:4c28:a030:30:72f3:95ff:fe02:2936 to stream /var/lib/hadoop/mapred/local/taskTracker/forsberg/jobcache/job_201201120812_0204/attempt_201201120812_0204_m_000000_0/test/Histograms/test-Histograms-hc-1-Data.db sections=1 progress=0/749048 - 0%. Retrying in 4000 ms. (java.net.ConnectException: Connection timed out)
{noformat}

So, digging a bit down the code, I see that org.apache.cassandra.hadoop.BulkRecordWriter successfully creates a Thrift connection to my Cassandra cluster, over IPv6. It successfully retrieves tokenrange information.

Later on, in org.apache.cassandra.streaming.FileStreamTask, it fails to connect to the destination cassandra node. It seems to me that the problem is that org.apache.cassandra.net.OutboundTcpConnectionPool is asking FBUtilities.getLocalAddress for the address to bind to, and getLocalAddress is returning an IPv4 address when DatabaseDescriptor has not been initialized. And DatabaseDescriptor has not been initialized, becase in BulkOutputFormat we're not reading cassandra.yaml. 

I actually have a workaround for this which involves not applying patch that removes need to read cassandra.yaml, then point to a cassandra.yaml generated specifically for the purpose on each hadoop node, with listen_address set to the IPv6 address of the node. 

This is with net.ipv6.bindv6only=0 in Linux sysctl - something you must have for Hadoop to run. 

Also tried -D mapred.child.java.opts=""-Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true"", i.e. setting properties to prefer IPv6 stack to M/R job, but didn't help.

In this case, we would probably be better of not explicitly binding to any address - the OS would do that for us. I understand binding explicitly makes sense when this code is running inside Cassandra server.","Linux 2.6.32-5-amd64, Java 1.6.0_26-b03",,,,,,,,,,,,,,,02/Feb/12 21:37;brandon.williams;0005-Allow-using-any-interface-for-outgoing-connections.txt;https://issues.apache.org/jira/secure/attachment/12513038/0005-Allow-using-any-interface-for-outgoing-connections.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-02 14:34:42.953,,,no_permission,,,,,,,,,,,,226276,,,Thu Feb 02 23:19:09 UTC 2012,,,,,,0|i0gou7:,95454,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,"02/Feb/12 14:34;brandon.williams;bq. It seems to me that the problem is that org.apache.cassandra.net.OutboundTcpConnectionPool is asking FBUtilities.getLocalAddress for the address to bind to, and getLocalAddress is returning an IPv4 address when DatabaseDescriptor has not been initialized. And DatabaseDescriptor has not been initialized, becase in BulkOutputFormat we're not reading cassandra.yaml.

bq. I actually have a workaround for this which involves not applying patch that removes need to read cassandra.yaml, then point to a cassandra.yaml generated specifically for the purpose on each hadoop node, with listen_address set to the IPv6 address of the node.

What's actually happening here is it's treating listen_address as being blank, which means we fall back to hostname resolution to determine the interface, so alternative you could modify your /etc/hosts and /etc/hostname to point to the ipv6 address.  However, I don't think we need to explicitly bind an outgoing address in OTCP either.  If we do, we can add some logic there to allow overriding it though.  Vijay, what do you think?","02/Feb/12 18:21;vijay2win@yahoo.com;Hi Brandon, allowing override might be better in this case. Because if we change this, cassandra's stream might get confused in some cases... in the constructor of IncomingStreamReader, as we do socket.getRemoteSocketAddress() to see where the stream comes from. if we dont bind it to the right address there may be some edge cases where it might not work. ","02/Feb/12 19:08;brandon.williams;Patch 0005 on CASSANDRA-3740 includes these changes, but I attached it there because it depends on it.",02/Feb/12 21:37;brandon.williams;Attaching here too for review.,02/Feb/12 23:06;vijay2win@yahoo.com;+1,02/Feb/12 23:19;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should throw a useful error when the destination CF doesn't exist,CASSANDRA-3847,12541137,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,03/Feb/12 21:30,12/Mar/19 14:16,13/Mar/19 22:26,06/Feb/12 21:11,1.0.8,,,,,,0,,,,,"When trying to store data to nonexistent CF, no good error is returned.

Instead you get a message like:

{noformat}
[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
{noformat}

Which, if you follow its advice, will eventually lead you to an NPE in initSchema.",,,,,,,,,,,,,,,,06/Feb/12 20:49;brandon.williams;3847.txt;https://issues.apache.org/jira/secure/attachment/12513497/3847.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-06 21:00:43.859,,,no_permission,,,,,,,,,,,,226489,,,Mon Feb 06 21:11:37 UTC 2012,,,,,,0|i0goxr:,95470,xedin,xedin,,,,,,,,,"06/Feb/12 20:49;brandon.williams;Unfortunately, there's nothing we can do about pig catching errors and rethrowing the ""org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false."" message, but we can at least throw a helpful error into the log instead of an NPE.",06/Feb/12 21:00;xedin;+1,06/Feb/12 21:11;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowCache misses Updates,CASSANDRA-3862,12541448,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,doubleday,doubleday,06/Feb/12 17:17,12/Mar/19 14:16,13/Mar/19 22:26,28/Feb/12 18:03,1.1.0,,,,,,0,,,,,"While performing stress tests to find any race problems for CASSANDRA-2864 I guess I (re-)found one for the standard on-heap row cache.

During my stress test I hava lots of threads running with some of them only reading other writing and re-reading the value.

This seems to happen:

- Reader tries to read row A for the first time doing a getTopLevelColumns
- Row A which is not in the cache yet is updated by Writer. The row is not eagerly read during write (because we want fast writes) so the writer cannot perform a cache update
- Reader puts the row in the cache which is now missing the update

I already asked this some time ago on the mailing list but unfortunately didn't dig after I got no answer since I assumed that I just missed something. In a way I still do but haven't found any locking mechanism that makes sure that this should not happen.

The problem can be reproduced with every run of my stress test. When I restart the server the expected column is there. It's just missing from the cache.

To test I have created a patch that merges memtables with the row cache. With the patch the problem is gone.

I can also reproduce in 0.8. Haven't checked 1.1 but I haven't found any relevant change their either so I assume the same aplies there.",,,,,,,,,,,,,,,,24/Feb/12 23:19;jbellis;3862-7.txt;https://issues.apache.org/jira/secure/attachment/12515996/3862-7.txt,13/Feb/12 18:26;jbellis;3862-cleanup.txt;https://issues.apache.org/jira/secure/attachment/12514380/3862-cleanup.txt,08/Feb/12 13:58;slebresne;3862-v2.patch;https://issues.apache.org/jira/secure/attachment/12513810/3862-v2.patch,15/Feb/12 09:36;slebresne;3862-v4.patch;https://issues.apache.org/jira/secure/attachment/12514613/3862-v4.patch,22/Feb/12 22:19;jbellis;3862-v5.txt;https://issues.apache.org/jira/secure/attachment/12515652/3862-v5.txt,23/Feb/12 16:01;jbellis;3862-v6.txt;https://issues.apache.org/jira/secure/attachment/12515760/3862-v6.txt,27/Feb/12 15:03;jbellis;3862-v8.txt;https://issues.apache.org/jira/secure/attachment/12516165/3862-v8.txt,07/Feb/12 16:04;slebresne;3862.patch;https://issues.apache.org/jira/secure/attachment/12513625/3862.patch,13/Feb/12 13:30;slebresne;3862_v3.patch;https://issues.apache.org/jira/secure/attachment/12514350/3862_v3.patch,27/Feb/12 16:22;slebresne;3862_v8_addon.txt;https://issues.apache.org/jira/secure/attachment/12516179/3862_v8_addon.txt,06/Feb/12 17:19;doubleday;include_memtables_in_rowcache_read.patch;https://issues.apache.org/jira/secure/attachment/12513452/include_memtables_in_rowcache_read.patch,11.0,,,,,,,,,,,,,,,,,,,2012-02-06 18:04:05.072,,,no_permission,,,,,,,,,,,,226735,,,Wed Jul 04 23:16:47 UTC 2012,,,,,,0|i0gp4n:,95501,jbellis,jbellis,,,,,,,,,06/Feb/12 17:19;doubleday;Dunno if there's a better way to do it...,"06/Feb/12 18:04;slebresne;I believe you are absolutely right that this is a bug.

Unfortunately I don't think including the memtables during cache reads really solves it. If you miss an update, it won't ever get added to the cached row, but the update itself will be flushed at some point and thus not be in any memtable anymore.

One partial solution I see could be that when a read 'reads for caching', it starts adding some sentinel object in the cache for the given row key. That sentinel would need to be an actual (empty) row but marked with the fact it's only a sentinel. When a write look if the row is cache, if it's a sentinel we would add the write to the sentinel. Once the read returns and we actually put the row in cache, we would it (atomically) with the content of the sentinel. A read that check the cache and see a sentinel would just skip the cache (and would not put it's result into the cache). Adapting that to the serializingCache is trivial.

Unfortunately, this is not perfect because this would screw counters. Though I guess for counters we could do the same thing as we would do for the serializingCache, i.e, if a read that 'reads for caching' see that the sentinel is not empty, we would just not cache the result (i.e, a row would be cache only if we are sure no write were done concurrently to the read).","06/Feb/12 19:32;doubleday;bq.  If you miss an update, it won't ever get added to the cached row, but the update itself will be flushed at some point and thus not be in any memtable anymore.

Very true ...

How about adopting the strategy we apply with CASSANDRA-2864:

- Writers dont update the cache at all
- Readers merge cache with memtables
- Upon flush merge memtables with cache","06/Feb/12 19:58;slebresne;{quote}
How about adopting the strategy we apply with CASSANDRA-2864:

* Writers dont update the cache at all
* Readers merge cache with memtables
* Upon flush merge memtables with cache
{quote}
The problem with that is that I don't see how we can make that work for counters at all. I also think it would be nice not having to merge on reads if we can avoid it (even if it's in-memory, it still uses CPU).

As a side note, I also suspect it's not bulletproof in theory, as a memtable could be fully flushed while a 'read to be cached' happens and with a bad timing during that, we could still miss an update. Of course, that kind of timing have almost no chance to happen. But in the case where a user triggers a flush manually, a memtable with only a handful of columns could be flushed very quickly, and I suspect the behavior could be observed. However unlikely that is, it'd be better if we can fix this problem once and for all.

I'll probably give a shot to my 'sentinel' proposal described above, I don't think it's too much code.","06/Feb/12 21:25;doubleday;Hmokay ... don't want to abuse Jira as an educational forum but maybe as a reward for the bugreport :-) ... are you saying that a reader could see a memtable view where flushing memtables are gone (flushed) and sstables don't contain the flushed memtables?

If that's the case than yes the cache would lose an update. But that what also imply that a read could miss an update without caching being in place at all no?

Otherwise (and that's how I read the code) given that the memtable switch will only happen after the merge the reader will read all updates because they are either in (flushing) memtables or in sstables and the cache will be in fact valid.

","07/Feb/12 07:38;slebresne;To be precise, what I'm saying is that (at least in theory) the following scenario would be possible:
* A read-for-cache read the memtables grabing updates
* then it start reading the sstables
* while the previous happens, a new update arrives. The memtable is then flushed and happens to be fully flushed *before* our read-for-cache completes.

In that case, the new update won't be part of the cached row (ever) because during the flush (when we would merge the memtable to the cache) the row was not in the cache yet. That may seem far fetched but consider a simple implementation of you proposition, where the 'upon flush merge memtables with cache' phase happens in the same loop over rows that is used for flushing. It is actually possible for a new write to be ""flushed"" within a few milliseconds of being received by the node: if the update triggers the memtable threshold *and* sorts at the very beginning of the memtable. But don't get me wrong, it would probably be possible to deal with that problem, but it feels a bit complicated and error prone.

","07/Feb/12 16:04;slebresne;Patch attached with my ""sentinel"" idea (The patch is against 1.1 currently). I *think* this fixes the problem, and this deal with counters.","07/Feb/12 21:48;doubleday;Just had a look at it and maybe I got it wrong but:

CFS.getRawCachedRow returns null for a sentinel and CFS.updateRowCache calls this.
Isn't updateRowCache supposed to add changes to the sentinel so that cacheRow can detect the race?","08/Feb/12 13:58;slebresne;You're right, thanks for catching that. Attached v2 fixed this (I realized that when we hit the cache during range_slice queries we don't update the statistics, which I'm not sure is what we want, but it's unrelated to that issue so haven't changed it).","11/Feb/12 00:01;jbellis;Looks to me like we might be able to simplify things by splitting the ""initialize row cache"" code (which can assume the cache is empty, and does not need a filter) out from the ""look up a cached row and cache it if it is not present"" method.

Nit: although not perfect, IMO ""getRawCachedRow"" is a better method name than ""getCachedRowNoStats"" -- the important thing to convey is that we're only inspecting the cache's contents, not changing them.","13/Feb/12 13:33;slebresne;Attaching v3. This mostly fix a but of the previous version where sentinels were not handled correctly in cacheRow(). I've also switch back to getRawCachedRow.
I'm not fully sure what you proposed to split exactly, but v3 does split cacheRow() in the hope of increasing clarity. ","13/Feb/12 18:23;jbellis;Attached cleanup patch that applies on top of v3.  Most of the changes are adding docstrings/comments and cleaning up typos.

A minor change to the code was to make cacheRow take just cfId and filter, removing the redundant filter.key as a parameter.

I also renamed cacheRow to getThroughCache.  Still not 100% happy with that, but my goal is to make the distinction between readAndCache more obvious.

Finally, I've modified the logic in invalidateCachedRow according to the reasoning in this comment:

{noformat}
.       // This method is used to (1) drop obsolete entries from a copying cache after the row in question was updated
        // and to (2) make sure we're not wasting cache space on rows that don't exist anymore post-compaction.
        // Sentinels complicate this because it means we've caught a read thread in the process of loading
        // the cache, and we don't know (in case 2) if it will do so with rows from before the compaction or after,
        // so we need to loop until the load completes.
{noformat}

(I also negated the loop condition, which looked like an oversight.)","14/Feb/12 11:16;slebresne;The cleanup lgtm.

For the change to invalidateCacheRow however, I wonder if it's worth it. By waiting when we found a sentinel, we may have writes waiting on a read to complete, which could involve a non negligible latency spike. On the other side, if we just leave the sentinel in that case, the only risk we take is that a read may end up putting tombstone in the cache that are already expired. But it doesn't seem like a big deal, especially given that it will very rarely happen.

But in any case, you're right about negating the loop condition. ","14/Feb/12 13:52;jbellis;bq. By waiting when we found a sentinel, we may have writes waiting on a read to complete, which could involve a non negligible latency spike

You're right, that's a worse negative than leaving tombstones in the cache.  I'm fine with changing it back if you update the comments accordingly. :)","15/Feb/12 09:36;slebresne;Actually then handling of the copying patch by the preceding patches is wrong.  When a put arrives and there is a sentinel, the patch does not add the put to the sentinel correctly. But thinking about it, for the copying cache, we should avoid having writes check the current value in the cache, because that have a non-negligible performance impact. What we should do is let invalidate actually invalidate sentinels. The only problem we're faced with if we do that, is that when a read-for-caching returns, it must make sure his own sentinel hasn't been invalidated. And in particular it must be careful of the case where the sentinel has been invalidated and another read has set another sentinel.

Anyway, attaching a v4 (that include the comments cleanups) that choose that strategy instead (and thus is (hopefully) not buggy even in the copying cache case). Note that it means that reads must be able to identify sentinels uniquely (not based on the content), so the code assign a unique ID to sentinel and use that for comparison.
","22/Feb/12 22:19;jbellis;Attached v5 with a simpler approach: for serializing cache, getThroughCache does a classic CAS loop with a sentinel vs the write's invalidate.

v5 also adds a containsCachedRow method to CFS so that callers that don't care about the value don't force a deserialize in the serializing cache case.","23/Feb/12 09:11;slebresne;I don't think v5 works. All sentinels are empty CF, so all sentinels will be equal (in SerializingCache.contentsEqual()). Which means we can have the following sequence of actions:
* a read r1 comes, the cache is empty, it sets sentinel s1 and start reading from disk
* a write w comes and invalidate s1.
* a read r2 comes, the cache is (now) empty, it sets sentinel s2 and start reading from disk
* r1 finish reading from disk having missed w. It'll do the replace, but since all sentinel are equals this will succeed (even though the current sentinel is the one of the second read) and we'll end up having missed w.

That's the reason of the sentinel IDs of v4.","23/Feb/12 15:56;jbellis;v6 pulls RCS out to a separate file and adds a uuid version and equals/hashcode methods.  SerializingCacheProvider uses a custom CF serializer that is RCS-aware.  SerializingCache.replace is simplified to use RCS.equals.  CAS loop is extended to non-serializing cache: since cache/write race is extremely rare, I'd rather take the occasional re-read penalty, than increase the overhead of every row in the cache by making them RCS objects permanently.","24/Feb/12 09:53;slebresne;Remarks on v6:
* Since we don't add stuffs to the sentinel, it has no reason to be a subclass of ColumnFamily. We should probably create a CachedRow class extended by both Sentinel (that would really just be an identifier, no metadata needed) and ColumnFamily and use that as cache values. It'll be cleaner and more importantly more type safe (a cache lookup won't be able to ignore by mistake that it could get a sentinel).
* Not adding stuffs to the sentinel also mean that in getThroughCache the counter special case is not needed anymore.
* In getThroughCache, if we fail to replace the sentinel, I think we should still better return the data rather than looping and re-reading. Better let the next client read cache the data than getting a crappy latency on the current read.
* Is it really an improvement to use UUIDs (over an AtomicLong)? I have nothing against UUID per se but it takes twice the space (and we serialize them) and without having benchmarked it, I'm willing to bet are much faster to generate. And let's be honest, the risk of overflow with an AtomicLong is science-fiction (or to be precise, at 1 millions sentinels created per seconds (which is *way* more than we'll ever see), you'd need more than 100,000 year of uptime to overflow).
","24/Feb/12 23:19;jbellis;v7 attached.

bq. Since we don't add stuffs to the sentinel, it has no reason to be a subclass of ColumnFamily

True, but when I tried this I ended up with a LOT of casting cache values to CF.  I think it might be the lesser of evils the way it is.

bq. the counter special case is not needed anymore

Updated.

bq. if we fail to replace the sentinel, I think we should still better return the data rather than looping and re-reading

Makes sense, updated.

bq. Is it really an improvement to use UUIDs (over an AtomicLong)? 

I'd rather have the reduced contention on instantiation than the 8 bytes of space (during the sentinel lifetime -- this goes away once the sentinel is replaced by the data CF).","27/Feb/12 09:28;slebresne;* In SerializingCache, remove misses a ""not"" in the while condition (this date back from one of my earlier patch). We don't need that new remove method anymore though so it's probably as simple to just remove it from the patch.
* In CFS.getThroughCache, the following line
{noformat}
boolean sentinelSuccess = !CacheService.instance.rowCache.putIfAbsent(key, sentinel);
{noformat}
should not be negated.
* Also in CFS.getThroughCache, we won't remove the sentinel if there is an exception during the read. It's not a big deal but it doesn't cost much to prevent it from happening.

bq. True, but when I tried this I ended up with a LOT of casting cache values to CF. I think it might be the lesser of evils the way it is.

In that case, I think I wouldn't mind too much casts and I would prefer getting the type safety of knowing that a method that take a ColumnFamily can't ever get a sentinel (and to make it explicit when you need to care about sentinel or not), but that's a bit subjective. There would also be some small wins like the fact we wouldn't need to save the cfId when serializing a sentinel.

bq. I'd rather have the reduced contention on instantiation than the 8 bytes of space

My point was that the UUID don't reduce contention. UUID.randomUUID() uses SecureRandom.nextBytes() that is synchronized (and thus likely entail a much bigger degradation in face of contention than an AtomicLong) and probably a bit CPU intensive. For reference, I did a quick micro-benchmark having 50 threads generating 10,000 ids simultaneously using both methods, using an AtomicLong is two orders of magnitude faster.
",27/Feb/12 15:03;jbellis;v8 attached w/ long sentinel and IRowCacheEntry.,27/Feb/12 15:49;slebresne;v8 lgtm mostly except for the 3 remarks at the beginning of my previous comment. Added simple patch on top of v8 that does the proposed modifications.,27/Feb/12 16:17;jbellis;shouldn't {{if (data == null)}} in the finally block be {{if (sentinelSuccess && data == null)}} ?,"27/Feb/12 16:22;slebresne;Oups, you're right. Patch updated.",27/Feb/12 16:24;jbellis;+1,"28/Feb/12 18:03;slebresne;Committed, thanks","04/Jul/12 23:16;hudson;Integrated in Cassandra #1646 (See [https://builds.apache.org/job/Cassandra/1646/])
    restore pre-CASSANDRA-3862 approach to removing expired tombstones during compaction (Revision fbb5ec0374e1a5f1b24680f1604b6e9201fb535f)
fix build - re-add CompactionController.removeDeletedInCache for commit fbb5ec0374e1a5f1b24680f1604b6e9201fb535f restore pre-CASSANDRA-3862 approach to removing expired tombstones during compaction (Revision 086c06ad7fb211de6be877c3c1ea2ee4f86c6d7e)

     Result = ABORTED
jbellis : 
Files : 
* src/java/org/apache/cassandra/db/compaction/CompactionIterable.java
* CHANGES.txt

dbrosius : 
Files : 
* src/java/org/apache/cassandra/db/compaction/CompactionController.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't start a node with row_cache_size_in_mb=1,CASSANDRA-3812,12540413,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,tpatterson,tpatterson,30/Jan/12 19:25,12/Mar/19 14:16,13/Mar/19 22:26,30/Jan/12 23:44,1.1.0,,,,,,0,,,,,"I consistently get the following error when trying to run 'bin/cassandra':

{code}
ERROR 12:20:28,144 Fatal exception during initialization
org.apache.cassandra.config.ConfigurationException: Found system table files, but they couldn't be loaded!
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:279)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:174)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:367)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
{code}","'trunk' in the git repo as of 1/20/2012. Use the standard cassandra.yaml config file, except change row_cache_size_in_mb to 1. Happened on both ubuntu and osx.",,,,,,,,,,,,,,,30/Jan/12 23:19;yukim;3812.txt;https://issues.apache.org/jira/secure/attachment/12512494/3812.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-30 23:19:52.066,,,no_permission,,,,,,,,,,,,225826,,,Mon Jan 30 23:44:44 UTC 2012,,,,,,0|i0goj3:,95404,xedin,xedin,,,,,,,,,"30/Jan/12 23:19;yukim;Setting row_cache_size_in_mb > 0 causes particular reads from column family whose row cache is disabled to force cache row, and results in reading null.

Attached patch makes sure value is read from column family, not from row cache when row caching is disabled.",30/Jan/12 23:44;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nosetests / system tests fail,CASSANDRA-3827,12540778,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,mallen,mallen,01/Feb/12 21:06,12/Mar/19 14:16,13/Mar/19 22:26,07/Feb/12 15:34,1.1.0,,,Legacy/Testing,,,0,,,,,"CQL Driver version used: 1.0.8.

{code}
EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_bad_batch_calls
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 381, in setUp
    try_run(self.inst, ('setup', 'setUp'))
  File ""/usr/local/lib/python2.7/site-packages/nose/util.py"", line 478, in try_run
    return func()
  File ""/var/lib/jenkins/jobs/Cassandra/workspace/test/system/__init__.py"", line 113, in setUp
    self.define_schema()
  File ""/var/lib/jenkins/jobs/Cassandra/workspace/test/system/__init__.py"", line 158, in define_schema
    Cassandra.CfDef('Keyspace1', 'Super1', column_type='Super', subcomparator_type='LongType', row_cache_size=1000, key_cache_size=0),
TypeError: __init__() got an unexpected keyword argument 'key_cache_size'
{code}
",,,,,,,,,,,,,,,,01/Feb/12 23:03;xedin;CASSANDRA-3827.patch;https://issues.apache.org/jira/secure/attachment/12512864/CASSANDRA-3827.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-01 21:19:49.675,,,no_permission,,,,,,,,,,,,226173,,,Tue Feb 07 15:34:01 UTC 2012,,,,,,0|i0goov:,95430,brandon.williams,brandon.williams,,,,,,,,,"01/Feb/12 21:19;xedin;you can just remove key_cache_size and row_cache_size from ThriftTester.define_schema(self), we don't support them anymore.","01/Feb/12 23:03;xedin;removed {row,key}_cache_size as deprecated from ThriftTester.define_schema(self) and fixed removed FilterClause to KeyRange for index tests. Now all tests pass for me:

{noformat}
PYTHONPATH=test nosetests --tests=system.test_thrift_server
............................................................................................
----------------------------------------------------------------------
Ran 92 tests in 626.972s

OK
{noformat}","02/Feb/12 00:38;mallen;It still fails for me with the patch applied, though I'm getting a different error now:

{code}
PYTHONPATH=/Users/mallen/cql-1.0.8:/Users/mallen/git-repos/cassandra/interface/thrift/gen-py/cassandra nosetests -x test/system
.E
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_bad_calls
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose/case.py"", line 187, in runTest
    self.test(*self.arg)
  File ""/Users/mallen/dstax/repos/git/cassandra/test/system/test_thrift_server.py"", line 710, in test_bad_calls
    _expect_exception(lambda: get_range_slice(client, ColumnParent('S'), SlicePredicate(column_names=['', '']), '', '', 5, ConsistencyLevel.ONE), InvalidRequestException)
  File ""/Users/mallen/dstax/repos/git/cassandra/test/system/test_thrift_server.py"", line 207, in _expect_exception
    r = fn()
  File ""/Users/mallen/dstax/repos/git/cassandra/test/system/test_thrift_server.py"", line 710, in <lambda>
    _expect_exception(lambda: get_range_slice(client, ColumnParent('S'), SlicePredicate(column_names=['', '']), '', '', 5, ConsistencyLevel.ONE), InvalidRequestException)
  File ""/Users/mallen/dstax/repos/git/cassandra/test/system/test_thrift_server.py"", line 217, in get_range_slice
    kr = KeyRange(start, end, count=count, row_filter=row_filter)
TypeError: __init__() got an unexpected keyword argument 'row_filter'

----------------------------------------------------------------------
Ran 2 tests in 7.296s

FAILED (errors=1)
{code}
",02/Feb/12 00:43;xedin;I forgot to mention that I re-generated Python Thrift bindings using `ant gen-thrift-py` before running tests. This is probably what you need because as I can see KeyRange does have a `row_filter` parameter.,"02/Feb/12 01:29;mallen;Oh right. Thanks, Pavel.  All tests are passing for me now too.",02/Feb/12 01:31;xedin;Great! Let's wait up what Jonathan has to say about code.,"07/Feb/12 15:26;brandon.williams;LGTM, +1",07/Feb/12 15:34;xedin;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System test failures in 1.1,CASSANDRA-3917,12542706,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,15/Feb/12 16:14,12/Mar/19 14:16,13/Mar/19 22:26,15/Feb/12 17:40,1.1.0,,,,,,0,,,,,"On branch 1.1, I currently see two system test failures:
{noformat}
======================================================================
FAIL: system.test_thrift_server.TestMutations.test_get_range_slice_after_deletion
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 187, in runTest
    self.test(*self.arg)
  File ""/home/mcmanus/Git/cassandra/test/system/test_thrift_server.py"", line 1937, in test_get_range_slice_after_deletion
    assert len(result[0].columns) == 1
AssertionError
{noformat}
and
{noformat}
======================================================================
FAIL: Test that column ttled expires from KEYS index
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 187, in runTest
    self.test(*self.arg)
  File ""/home/mcmanus/Git/cassandra/test/system/test_thrift_server.py"", line 1908, in test_index_scan_expiring
    assert len(result) == 1, result
AssertionError: []

----------------------------------------------------------------------
{noformat}",,,,,,,,,,,,,,,,15/Feb/12 17:11;slebresne;3917.txt;https://issues.apache.org/jira/secure/attachment/12514662/3917.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-15 17:37:16.441,,,no_permission,,,,,,,,,,,,227992,,,Wed Feb 15 17:40:23 UTC 2012,,,,,,0|i0gps7:,95607,brandon.williams,brandon.williams,,,,,,,,,"15/Feb/12 17:11;slebresne;The test_get_range_slice_after_deletion failure is actually due to a typo during a merge (my fault). Attached trivial patch to fix.

Somehow I'm not able to reproduce the second failure so far, I'm not sure why.",15/Feb/12 17:37;brandon.williams;+1,"15/Feb/12 17:40;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool removetoken force causes an inconsistent state,CASSANDRA-3876,12541794,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,soverton,soverton,soverton,08/Feb/12 12:41,12/Mar/19 14:16,13/Mar/19 22:26,08/Feb/12 14:29,1.0.8,,,,,,0,force,nodetool,removetoken,,"Steps to reproduce (tested on 1.0.7 and trunk):
* Create a cluster of 3 nodes
* Insert some data
* stop one of the nodes
* Call removetoken on the token of the stopped node
* Immediately after, do removetoken force 
  - this will cause the original removetoken to fail with an error after 30s since the generation changed for the leaving node, but this is a convenient way of simulating the case where a removetoken hangs at streaming since the cleanup logic at the end of StorageService.removeToken is never executed.
  - if you want a more realistic reproduction then get a removetoken to hang in streaming, then do removetoken force

Effects:
* ""removetoken status"" now throws an exception because StorageService.removingNode is not cleared, but the endpoint is no longer a member of the ring:

$ nodetool -h localhost removetoken status
{noformat}
Exception in thread ""main"" java.lang.AssertionError
	at org.apache.cassandra.locator.TokenMetadata.getToken(TokenMetadata.java:304)
	at org.apache.cassandra.service.StorageService.getRemovalStatus(StorageService.java:2369)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:205)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:683)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:672)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:619)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{noformat}

* truncate no longer works in the cli because the removed endpoint is not removed from Gossiper.unreachableEndpoints. 
The cli errors immediately with:
{noformat}
[default@ks1] truncate cf1;
null
UnavailableException()
	at org.apache.cassandra.thrift.Cassandra$truncate_result.read(Cassandra.java:20978)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_truncate(Cassandra.java:942)
	at org.apache.cassandra.thrift.Cassandra$Client.truncate(Cassandra.java:929)
	at org.apache.cassandra.cli.CliClient.executeTruncate(CliClient.java:1417)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:270)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)
{noformat}

The logs show:
{noformat}
INFO [Thrift:11] 2012-02-08 11:55:50,135 StorageProxy.java (line 1172) Cannot perform truncate, some hosts are down
{noformat}

* there are probably other schema related things that fail for the same reason although this wasn't tested

Workaround:
* Restart the affected node.

Fix:
It looks like StorageService.forceRemoveCompletion is missing some cleanup logic which is present at the end of StorageService.removeToken. Adding this cleanup logic to forceRemoveCompletion fixes the above issues (see attached).",,,,,,,,,,,,,,,,08/Feb/12 12:42;soverton;3876.patch;https://issues.apache.org/jira/secure/attachment/12513805/3876.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-08 14:29:04.357,,,no_permission,,,,,,,,,,,,227081,,,Wed Feb 08 14:29:04 UTC 2012,,,,,,0|i0gpav:,95529,brandon.williams,brandon.williams,,,,,,,,,08/Feb/12 12:42;soverton;Attached patch against trunk,"08/Feb/12 14:29;brandon.williams;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SuperColumn may ignore relevant tombstones,CASSANDRA-3875,12541791,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,08/Feb/12 11:39,12/Mar/19 14:16,13/Mar/19 22:26,08/Feb/12 15:07,0.8.10,,,,,,0,,,,,"QueryFilter.isRelevant() consider that a super column that is gc-able but contains only non-gcable tombstone is irrelevant (if the tombstone timestmap is greater than the super column timestamp, which is almost implied by the fact that the tombstone are non-gcable while the SC is).",,,,,,,,,,,,,,,,08/Feb/12 11:41;slebresne;3875.patch;https://issues.apache.org/jira/secure/attachment/12513800/3875.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-08 13:56:26.039,,,no_permission,,,,,,,,,,,,227078,,,Wed Feb 08 15:07:11 UTC 2012,,,,,,0|i0gpaf:,95527,jbellis,jbellis,,,,,,,,,"08/Feb/12 11:41;slebresne;Attaching patch with a unit test.

Some more details about this issue can be found in the comments of CASSANDRA-3872.",08/Feb/12 13:56;jbellis;+1,"08/Feb/12 15:07;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal error processing batch_mutate: java.util.ConcurrentModificationException on CounterColumn,CASSANDRA-3870,12541595,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,amorton,vjevdokimov,vjevdokimov,07/Feb/12 10:30,12/Mar/19 14:16,13/Mar/19 22:26,22/Feb/12 08:32,1.0.8,,,,,,0,counters,,,,"Cassandra throws an exception below while performing batch_mutate with counter column insertion mutation to increment column with 1:

ERROR [Thrift:134] 2012-02-03 15:51:02,800 Cassandra.java (line 3462) Internal error processing batch_mutate
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:532)
        at org.apache.cassandra.service.AbstractWriteResponseHandler.waitForHints(AbstractWriteResponseHandler.java:89)
        at org.apache.cassandra.service.AbstractWriteResponseHandler.get(AbstractWriteResponseHandler.java:58)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:201)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:639)
        at org.apache.cassandra.thrift.CassandraServer.internal_batch_mutate(CassandraServer.java:590)
        at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:598)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:3454)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Column family definition:

create column family CountersColumnFamily1
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and rows_cached = 1000000.0
  and row_cache_save_period = 0
  and row_cache_keys_to_save = 2147483647
  and keys_cached = 0.0
  and key_cache_save_period = 14400
  and read_repair_chance = 0.1
  and gc_grace = 43200
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'SerializingCacheProvider'
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy';","Debian 6.0 x64
x64 Sun Java 6u26
Cassandra 1.0.7
JNA
2 DCs, 1 ring/DC, 8 nodes/ring, RF=3/DC, Random partitioner.
Disk access auto (mmap)",,,,,,,,,,,,,,,21/Feb/12 18:43;slebresne;3870.txt;https://issues.apache.org/jira/secure/attachment/12515463/3870.txt,21/Feb/12 10:50;amorton;cassandra-1.0-3870-V2.txt;https://issues.apache.org/jira/secure/attachment/12515342/cassandra-1.0-3870-V2.txt,21/Feb/12 19:46;amorton;cassandra-1.0-3870-V3.txt;https://issues.apache.org/jira/secure/attachment/12515477/cassandra-1.0-3870-V3.txt,16/Feb/12 17:41;amorton;cassandra-1.0-3870.txt;https://issues.apache.org/jira/secure/attachment/12514823/cassandra-1.0-3870.txt,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-02-08 09:44:07.525,,,no_permission,,,,,,,,,,,,226882,,,Tue Apr 23 12:37:03 UTC 2013,,,,,,0|i0gp87:,95517,slebresne,slebresne,,,,,,,,,"08/Feb/12 09:44;amorton;I've done some digging and this is what I think is happening...

StorageProxy.counterWriteOnCoordinatorPerformer() creates a counterWriteTask and put it in the local MUTATION TP. This will run the mutation and then put a task in REPLICATE_ON_WRITE to send if the counter write to other nodes, if a node is down hints a scheduled and AbstractWriteResponseHandler.addFutureForHint() is called with the hint mutation future. 

However the response handler was originally created in the request thread, which may now have called get() to wait on it. This may miss hints that have not already been added or may get the CME in the error here. 

It looks like this is only a problem for counter writes where the coordinator is a replica, and then only in certain circumstances. Counters where another replica is the leader look ok and so do regular writes.

I was thinking:
* Change the AWRH.hintFutures to a CopyOnWriteArrayList (to be cautious) and
* Change AWRH.get() to wait on hints after the normal request has complet instead of before.
 
I'll try to pick it up again tomorrow.","08/Feb/12 10:08;vjevdokimov;In our case coordinator almost always a replica, since our Cassandra client is aware about token range and selects endpoints calculating token for a key.","08/Feb/12 16:41;amorton;Realized when I went to bed I will also need to change StorageProxy.counterWriteTask to call responeHandler.response() after the hints have been sent, which happening on the REPLICATE_ON_WRITE TP. So needs additional coordination. 

Should be able to find some time today to keep working on this.  ",16/Feb/12 17:41;amorton;Patch to add IWriteResponseHandler.finalizeHints() - called so the handler knows that all hints have been sent and it is safe to begin waiting on them. ,"17/Feb/12 09:11;slebresne;I think you're right on the reasons that causes the exception, and that our fix would avoid that exception. There is however two problems/annoying things (not really related to the patch per se):
# this break the current property that counter writes at CL.ONE don't have to wait on the read. In other words, that would increase the latency of CL.ONE counter writes.
# this completely break counters when replicate_on_write == false. This may not be a problem anymore on 1.1 if we remove that option (CASSANDRA-3868) but breaking it in 1.0 (on a minor release) is more problematic.

A ""solution"" to prevent those two problems would be to add a way to specify that we don't want to wait on hints being locally written and use that for counter at CL.ONE, but that means we would skip the current property of 'when a write returns, we guarantee that all hint for nodes that are *known* dead have been written locally'. Which leads me to a question: what does that latter property really give us?","19/Feb/12 20:15;amorton;Sorry got some sick kids at home. Will try to find some time this week. 
","21/Feb/12 10:50;amorton;added -V2 

Fixed problem where StorageProxy.mutateCounter() would not call handler.finalizeHints() if 
it was not a replica for the row.

Improved timeout when waiting for hints.

bq. this break the current property that counter writes at CL.ONE don't have to wait on the read. In other words, that would increase the latency of CL.ONE counter writes.

Added IWriteResponseHanlder.dontBlockOnHints() called from StorageProxy.counterWriteTask() if CL is ONE. 

bq. this completely break counters when replicate_on_write == false. This may not be a problem anymore on 1.1 if we remove that option (CASSANDRA-3868) but breaking it in 1.0 (on a minor release) is more problematic.

Not sure what you mean. In StoragrProxy.counterWriteTask() if not cm.shouldReplicateOnWrite() i finalize the hints with the handler. So that get() will not wait for more futures. 

bq. guarantee that all hint for nodes that are known dead have been written locally'. Which leads me to a question: what does that latter property really give us?

It slows down the client and reduces the chance that a node can get overwhelmed if it has to store a lot of hints. But at CL ONE the client is asking the server to do the minimal amount of work before returning so it makes sense.  ","21/Feb/12 14:12;slebresne;bq. if not cm.shouldReplicateOnWrite() i finalize the hints with the handler

Sorry I missed that.

bq. it slows down the client and reduces the chance that a node can get overwhelmed if it has to store a lot of hints

I don't think that is the goal of that code. We already have code for that (make sure a node don't get overwhelm writing hints locally) in sendToHintedEndpoints.

The only reasonable intent of that code (the code that makes writes wait that hint are locally written) that I can see would be to ensure that when we acknowledge the client we can guarantee that hints for dead nodes are stored (and thus will eventually get delivered). However, this only works for known dead nodes, so we cannot really make that guarantee (unfortunately).

I'm not saying the attached patch won't work, but it does help making the write path more complicated and 'messy' that I'd like it to be. Typically having to not forget to call finalizeHints() is a bit error-prone, etc...  So I'm wondering, do we really have a strong reason for waiting for hints during writes in the first place.


On the patch though:
* We should raise a timeout exception if waiting for the hints to be finalized timeouts inserting of throwing an assertion error. It is possible (at least in theory) for that to timeout.
* Not sure I understand dontBlockOnHints(). It seems to only skip the fact that we signal hintsFinalized. I think we should just call finalizeHints() when we don't want to block on hints.
","21/Feb/12 16:00;jbellis;bq. We already have code for that (make sure a node don't get overwhelm writing hints locally) in sendToHintedEndpoints

I think you're right. Originally we had ""wait for hint delivery before ack"" as an attempt to prevent overload and OOM, but then we added the more fine-grained checks in sTHE instead.","21/Feb/12 18:43;slebresne;Attaching a version that take the approch of removing the 'wait on hints future' from the write path. Again the rational is that:
# Since that code was added, we've introduced a better way to deal with node being overwhelm by hints in sTHE.
# Since we'll have hint future only for node that are known dead, waiting on only those doesn't really give any special new guarantee to the user.","21/Feb/12 19:41;amorton;bq. I don't think that is the goal of that code. We already have code for that (make sure a node don't get overwhelm writing hints locally) in sendToHintedEndpoints.

Missed the totalHintsInProgress check.

bq. So I'm wondering, do we really have a strong reason for waiting for hints during writes in the first place.

IMHO no, other than CL ANY. I know it's different in 1.0 but HH provides weak guarantees. Recording a hint locally does not guarantee that it will be applied to the endpoint before the next read for the row. 

bq. I'm not saying the attached patch won't work, but it does help making the write path more complicated and 'messy' that I'd like it to be

Agree. 

If we stick with waiting for hints I could change it to *not* wait for acknowledgement that all hint futures collected by default. Counter mutations on the coordinator are the exception, they can tell the handler to wait.

bq. We should raise a timeout exception if waiting for the hints to be finalized timeouts inserting of throwing an assertion error. It is possible (at least in theory) for that to timeout.

Changed.

bq. Not sure I understand dontBlockOnHints(). It seems to only skip the fact that we signal hintsFinalized. I think we should just call finalizeHints() when we don't want to block on hints.

finalizeHints() stops modifications to the hints collection, so calling addFutureForHint() after it will raise an exception. dontBlockOnHints() tells get() to ignore the hints collection and prevents finalizeHints() from stopping modification to the hints collection. 

This is to support sendToHintedEndpoints passing hint futures to the handler when using CL ONE. I wanted calls to addFutureForHint() to always work unless finalizeHints() had been called.    


","21/Feb/12 19:46;amorton;version 3

raise TimeoutException if finaliseHints() not called.","21/Feb/12 19:53;slebresne;Just to be clear, I'm *really* leaning towards the path of removing the 'wait for hint future' from the writeHandler (as of 3870.txt) unless someone remember a reason why that code is still useful that I've missed.","21/Feb/12 20:40;amorton;Agree.

+1 for your patch. 

Not in your patch but if sendToHintedEndpoints() throws a TimeoutException because too many inflight hints when performing a counter write on a leader that is not the coordinator. CounterMutationVerbHandler will swallow the exception and wait for the coordinator to time out.  Would it be better to use a different exception and return to the coordinator ? ","22/Feb/12 08:32;slebresne;Committed, thanks


bq. Would it be better to use a different exception and return to the coordinator ?

The thing is that the leader is using a WriteHandler to wait on the response of the coordinator, which in its current form doesn't have any kind of failure mode (A WriteResponse does have a boolean to indicate success but no code use it ever). So basically it would be possible but would add much complication when just waiting on the coordinator to timeout is probably good enough.
","23/Apr/13 12:29;sumit.thakur@rancoretech.com;Hello All,

I got same exception in Apache Cassandra 1.1.5  

ERROR [Thrift:32] 2013-02-23 19:07:53,581 Cassandra.java (line 3462) Internal error processing batch_mutate
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:60)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
	at org.apache.cassandra.service.StorageProxy.insertLocal(StorageProxy.java:419)
	at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:308)
	at org.apache.cassandra.service.StorageProxy$2.apply(StorageProxy.java:120)
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:255)
	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:194)
	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:639)
	at org.apache.cassandra.thrift.CassandraServer.internal_batch_mutate(CassandraServer.java:590)
	at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:598)
	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:3454)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)


SUMIT THAKUR","23/Apr/13 12:37;slebresne;This is not at all the same exception. Your exception mean that some write has been attempted while the node was shutting down. Though noisy, it's harmless.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
reduce computational complexity of processing topology changes,CASSANDRA-3881,12541955,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,soverton,scode,scode,09/Feb/12 10:31,12/Mar/19 14:16,13/Mar/19 22:26,03/Jul/12 18:24,1.2.0 beta 1,,,,,,0,vnodes,,,,"This constitutes follow-up work from CASSANDRA-3831 where a partial improvement was committed, but the fundamental issue was not fixed. The maximum ""practical"" cluster size was significantly improved, but further work is expected to be necessary as cluster sizes grow.

_Edit0: Appended patch information._

h3. Patches
||Compare||Raw diff||Description||
|[00_snitch_topology|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/00_snitch_topology...p/3881/00_snitch_topology]|[00_snitch_topology.patch|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/00_snitch_topology...p/3881/00_snitch_topology.diff]|Adds some functionality to TokenMetadata to track which endpoints and racks exist in a DC.|
|[01_calc_natural_endpoints|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/01_calc_natural_endpoints...p/3881/01_calc_natural_endpoints]|[01_calc_natural_endpoints.patch|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/01_calc_natural_endpoints...p/3881/01_calc_natural_endpoints.diff]|Rewritten O(logN) implementation of calculateNaturalEndpoints using the topology information from the tokenMetadata.|

----

_Note: These are branches managed with TopGit. If you are applying the patch output manually, you will either need to filter the TopGit metadata files (i.e. {{wget -O - <url> | filterdiff -x*.topdeps -x*.topmsg | patch -p1}}), or remove them afterward ({{rm .topmsg .topdeps}})._",,,,,,,,,,CASSANDRA-4119,,CASSANDRA-3831,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-15 15:44:40.179,,,no_permission,,,,,,,,,,,,227242,,,Tue Jul 03 18:24:25 UTC 2012,,,,,,0|i0gpdb:,95540,jbellis,jbellis,,,,,,,,,"26/Feb/12 01:26;scode;Even with post-CASSANDRA-3831 code, the need here is great. When bootstrapping a new node into a pre-existing cluster of 200+ nodes, the time it takes being CPU bound in gossip stage on the node being bootstrapped, is so large that we're close to seeing it initiate the bootstrap streaming process before being done CPU spinning i gossip stage.

We're increasing the bootstrap wait delay for now to work around this.","26/Feb/12 01:26;scode;Another work-around that is still hacky is to wait for the gossip stage pending to also be 0, after waiting for the ring delay, before proceeding with bootstrap.","26/Feb/12 01:32;scode;Apologies for spamming, but I want to make clear: This applies when bootstrapping nodes into a ring with at least one other node already bootstrapping (otherwise calculatePendingRanges won't get called repeatedly as the ring is populated in the storage service). The greater the number of bootstrapping nodes, the greater the probability of pending range calculations having to be done more often (it is determined by how many nodes get populated into the storage service' notion of the ring before the first bootstrapping node is seen), and the greater the probability of initiating bootstrap based on an incomplete ring view.","15/May/12 15:44;soverton;First the important bit: With these patches, StorageService.calculatePendingRanges is almost three orders of magnitude faster when calculating two nodes bootstrapping into a cluster with 2048 nodes (22ms vs 14.6sec). See the [graph here|https://github.com/acunu/cassandra/wiki/images/calculate_pending_ranges.png]. This was tested with 1 DC and 1 rack with RF=2.

The problem lies in NetworkTopologyStrategy.calculateNaturalEndpoints. The main problems with the existing implementation are:

1. for each datacentre:
2. it iterates through all the tokens in the ring at least once
3. then does an NlogN sort of those tokens
4. then if number of racks < RF it will iterate through all tokens again because it can't tell if it has exhausted the racks in that DC
5. then if number of hosts in that DC < RF it will iterate through all tokens again, otherwise it will iterate through until it has RF hosts in that DC

so it's doing O(DC * (N + NlogN + N + N)) operations just to work out the endpoints for a single token. StorageService.calculatePendingRanges then puts this inside other loops (such as AbstractReplicationStrategy.getAddressRanges) which makes it at least O(N^2*logN).

These patches fix (1) by iterating through the tokens only once, and processing all DCs simultaneously.

(2,3&5) relate to knowing which endpoints exist in a given DC, (4) relates to knowing which racks appear in a DC, so the first patch adds this knowledge to the snitch. The second patch makes use of this knowledge to simplify calculateNaturalEndpoints.

||branch|| || ||description
|p/3881/00_snitch_topology|[compare|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/00_snitch_topology...p/3881/00_snitch_topology]|[raw diff|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/00_snitch_topology...p/3881/00_snitch_topology.diff]|Adds some functionality to AbstractEndpointSnitch to track which endpoints and racks exist in a DC (allows for fixing of 2-5).
|p/3881/01_calc_natural_endpoints|[compare|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/01_calc_natural_endpoints...p/3881/01_calc_natural_endpoints]|[raw diff|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/01_calc_natural_endpoints...p/3881/01_calc_natural_endpoints.diff]|Rewritten O(logN) implementation of calculateNaturalEndpoints using the topology information from the snitch.


Note: These branches are managed with [TopGit|http://repo.or.cz/w/topgit.git].  If you are applying the patch output manually, you will either need to filter the TopGit metadata files ( {{wget -O - <url> | filterdiff -x*.topdeps -x*.topmsg | patch -p1}} ), or remove them afterward ( {{rm .topmsg .topdeps}} ).","15/May/12 16:59;jbellis;Peter, are you available to review?","15/May/12 17:19;soverton;Just added an additional test to NetworkTopologyStrategyTest. 

The above patches will be up to date, or if you already downloaded the diff then here is the [incremental patch|https://github.com/acunu/cassandra/commit/b199d14f1a33115c66552b3c821af7dc1f0ceedb] ([raw diff|https://github.com/acunu/cassandra/commit/b199d14f1a33115c66552b3c821af7dc1f0ceedb.diff])","16/May/12 10:58;soverton;AbstractEndpointSnitch should handle every state change, not just onChange and onRemove

[incremental patch|https://github.com/acunu/cassandra/commit/b462500e7b8edad0581c60b8e2011cf76e1c7a92] ([raw diff|https://github.com/acunu/cassandra/commit/b462500e7b8edad0581c60b8e2011cf76e1c7a92.diff])","16/May/12 11:51;soverton;Some remaining issues:
1. Handling changes to dc/rack assignment. Currently the topology maps would get out of sync if any endpoint changes dc/rack. This will require Ec2Snitch and PropertyFileSnitch to notify the superclass when they detect a topology change.
2. Delegate getTopology in DynamicSnitch to the subsnitch.

","18/May/12 08:20;scode;Jonathan, I'll try to do so within the next few days.","25/May/12 10:26;soverton;The original approach was not quite there. The snitch was tracking the topology of nodes in NORMAL state for the benefit of NTS.calculateNaturalEndpoints, but calculateNaturalEndpoints is called with modified TokenMetadata (eg, with leaving nodes removed, or a bootstrapped node added or some other modification) to calculate ranges for some future state of the ring, not the current state as tracked by the snitch.

The correct solution is to have TokenMetadata track the topology of the nodes which it considers to be part of the ring, so that when a tokenMetadata is cloned and modified it also updates its view of the topology. This is also much simpler and cleaner.

Patches above are updated.","25/Jun/12 17:03;jbellis;This looks reasonable.  I have two concerns:

- Topology syncronization: a mix between ""Topology synchronizes internally"" and ""caller must synchronize externally"" is a recipe for trouble.  Maybe just synchronizing getDatacenterEndpoints/getDatacenterRacks and returning copies, would be enough.  Alternatively, we could just say ""you must clone TMD before calling calculateNaturalEndpoints"" and possibly get rid of all the Topology synchronization (relying on TMD's on the update path)
- I think there is a hole in the rack-handling logic in cNE: we only check skippedDcEndpoints when a new rack is found.  So if there is (for instance) a single rack in a DC w/ RF=3, we'll add the first endpoint in that rack, then the rest will get added to the skipped list, but never added to replicas.

00 nits:
- would like to see javadoc for Topology
- type specification is not necessary for HMM.create calls (this is why guava prefers factories to constructors)
- I recognize that you were following precedent here, but I would prefer the param-less TMD constructor to call new TMD(HashBimap.create(), new Topology()) instead of (null, null) + special casing in the 2-param version

01 nits:
- @Override is redundant for calculateNaturalEndpoints since parent declares it abstract
- some ""} else"" formatting issues in cNE
- ""!skippedDcEndpoints.get(dc).isEmpty()"" is redundant since the empty iterator case will just be a no-op in the following loop
- perhaps dcReplicas would be a better name than replicaEndpoints, for symmetry w/ ""replicas""
- would be cleaner to move the ""replicaEndpoints.get(dc).size() >= Math.min(allEndpoints.get(dc).size(), getReplicationFactor)"" check into ""have we already found all replicas for this dc"", instead of playing games w/ mutating replicaEndpoints as an (unimportant) optimization.  (Note that ""datacenters.containsKey(dc)"" remains a sufficient check for ""is this a dc we care about at all."")","26/Jun/12 15:58;soverton;Thanks Jonathan. I've addressed the above in the following commits:
7eab101 [incremental patch|https://github.com/acunu/cassandra/commit/7eab101fd1649737682d4ce30004a15d0c6c2343] ([raw diff|https://github.com/acunu/cassandra/commit/7eab101fd1649737682d4ce30004a15d0c6c2343.diff])
1474cf0 [incremental patch|https://github.com/acunu/cassandra/commit/1474cf090f5b2c80bbe573d16041458f1782ecbb] ([raw diff|https://github.com/acunu/cassandra/commit/1474cf090f5b2c80bbe573d16041458f1782ecbb.diff])

(patch links above updated)

except for the following:

{quote}
* Topology syncronization: a mix between ""Topology synchronizes internally"" and ""caller must synchronize externally"" is a recipe for trouble. Maybe just synchronizing getDatacenterEndpoints/getDatacenterRacks and returning copies, would be enough. Alternatively, we could just say ""you must clone TMD before calling calculateNaturalEndpoints"" and possibly get rid of all the Topology synchronization (relying on TMD's on the update path)
{quote}

I was trying to avoid any copying, as calculateNaturalEndpoints will be called thousands of times with vnodes in some code paths. I prefer the latter solution of cloning TMD before using it in any method which will use the Topology. The only places where cloning will be necessary to avoid concurrent updates are those where StorageService.instance.tokenMetadata is used directly. I'll update the patches shortly.

{quote}
* I think there is a hole in the rack-handling logic in cNE: we only check skippedDcEndpoints when a new rack is found. So if there is (for instance) a single rack in a DC w/ RF=3, we'll add the first endpoint in that rack, then the rest will get added to the skipped list, but never added to replicas.
{quote}
I think this case is already handled: the subsequent endpoints for that duplicate rack will hit this line first:
{noformat}
    // can we skip checking the rack?
    if (seenRacks.get(dc).size() == racks.get(dc).keySet().size())
{noformat}
and they get added as a replica immediately because we know we have exhausted the racks for that DC. Did I miss something?

","26/Jun/12 18:06;jbellis;bq. The only places where cloning will be necessary to avoid concurrent updates are those where StorageService.instance.tokenMetadata is used directly. I'll update the patches shortly.

Sounds good, I'll have a look when that hits.

bq. I think this case is already handled: the subsequent endpoints for that duplicate rack will hit this line first

Ah, right.  LGTM.","27/Jun/12 11:15;soverton;I tried out the different approaches for removing this sychronization requirement on Topology:

* [clone TokenMetadata before passing to calculateNaturalEndpoints|https://github.com/acunu/cassandra/compare/bb6f4b40c8a8276971e9e4c50e92e6801dba08bf...3881-review-clone-tmd] ([raw diff|https://github.com/acunu/cassandra/compare/bb6f4b40c8a8276971e9e4c50e92e6801dba08bf...3881-review-clone-tmd.diff])

This looks much more hairy than having synchronization - the places where tokenMetadata needs to be cloned look arbitrary and it's much less obvious what the convention is for other people looking at this (eg. why do you need to clone before passing into AbstractReplicationStrategy.getAddressRanges() but not AbstractReplicationStrategy.getPendingAddressRanges() ? A: it's because getPendingAddressRanges makes its own clone to update a token). 

* [return copies from Topology.getDatacenterEndpoints() and Topology.getDatacenterRacks()|https://github.com/acunu/cassandra/compare/bb6f4b40c8a8276971e9e4c50e92e6801dba08bf...3881-review-copy-topo] ([raw diff|https://github.com/acunu/cassandra/compare/bb6f4b40c8a8276971e9e4c50e92e6801dba08bf...3881-review-copy-topo.diff])

This is less error-prone because Topology now handles all its own synchonization. Unfortunately copying those maps adds a large overhead which removes most of the benefit of this ticket. See the [updated graph of calculatePendingRanges vs. cluster size|https://github.com/acunu/cassandra/wiki/images/calculate_pending_ranges_topo_copy.png]. This is not surprising because these copies require O(N) time. 

* [document the synchronization requirements of Topology so that it's clear what is necessary|https://github.com/acunu/cassandra/commit/6d25673c372587beea3971d8ce31b06372525861] ([raw diff|https://github.com/acunu/cassandra/commit/6d25673c372587beea3971d8ce31b06372525861.diff])

This is my preferred solution. Currently only calculateNaturalEndpoints uses the TMD.Topology, so requiring to synchronize around it seems like a reasonable solution to me. The javadoc should make it obvious to any new uses of it that they need to synchronize.","27/Jun/12 17:43;dr-alves;Hi

small nit:
I was using the ctor {code}public TokenMetadata(BiMap<Token, InetAddress> tokenToEndpointMap){code} in non-commited code. That ctor has now become {code}public TokenMetadata(BiMap<Token, InetAddress> tokenToEndpointMap, Topology topology){code} since the ctor in Topology is protected this ctor in TokenMetadata can't be called outside of protected scope from now on. I suggest either broadening the scope of the ctor in Topology or restricting the scope in TokenMetadata.","28/Jun/12 09:45;soverton;Hi David, is this non-committed code that's part of another ticket?","28/Jun/12 09:54;dr-alves;hi sam

yes I was using that ctor to test StorageService.effectiveOwnership, included in the CASSANDRA-3047 patch. 

I worked around it since it makes sense that it makes sense that TokenMetadata receives token->endpoint mappings through {code}updateNormalTokens{code}, in order to build topology. The thing was that while previously the ctor {code}TokenMetadata(BiMap<Token, InetAddress> tokenToEndpointMap){code} was usable, now it is not, at least not without getting topology from another TokenMetadata instance.","28/Jun/12 10:29;soverton;Ok, I was going to suggest using updateNormalTokens, or you could change ctor visibility in your patch if required.","28/Jun/12 21:59;jbellis;bq. the places where tokenMetadata needs to be cloned look arbitrary and it's much less obvious what the convention is for other people looking at this

What if we just added an assert ({{assert tokenMetadata != StorageService.tokenMetadata}}) to enforce this requirement?

Granted that we are choosing lesser evils here, but I like that better than trying to reason about synchronized(Topology) mixed with locks mixed with synchronized(bootstrapTokens).",28/Jun/12 22:48;jbellis;Pushed this to https://github.com/jbellis/cassandra/tree/3881-clone-tmd with some extra synchronization cleanup,28/Jun/12 22:57;jbellis;More general assert instead (against TM.getTopology) pushed to https://github.com/jbellis/cassandra/tree/3881-clone-tmd-2,"02/Jul/12 17:38;soverton;Agreed that having a single lock is easier to reason about its correctness.

There was one more extra synchronization to cleanup - pushed to [https://github.com/acunu/cassandra/tree/3881-clone-tmd-3].

I'll get these changes merged into the TopGit branches. They'll possibly need separating out for the two separate patches.",02/Jul/12 17:57;jbellis;I can merge/squash/commit from here.,"02/Jul/12 18:12;jbellis;I take it back, history is a mess w/o topgit. :)  Will wait for your update.","03/Jul/12 13:31;soverton;It's all merged in now, so the patch links in the ticket description are up to date.

There was one more place in some tests that TMD needed to be cloned: https://github.com/acunu/cassandra/commit/08620c55a77ba1dc257b853610386297ab0c379b
","03/Jul/12 18:24;jbellis;thanks, committed!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFIF WideRowIterator only returns batch size columns,CASSANDRA-3883,12541989,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,09/Feb/12 16:15,12/Mar/19 14:16,13/Mar/19 22:26,11/Apr/12 18:28,1.1.0,,,,,,0,,,,,"Most evident with the word count, where there are 1250 'word1' items in two rows (1000 in one, 250 in another) and it counts 198 with the batch size set to 99.",,,,,,,,,,CASSANDRA-3909,,,,,,13/Feb/12 16:02;brandon.williams;3883-v1.txt;https://issues.apache.org/jira/secure/attachment/12514368/3883-v1.txt,09/Apr/12 17:41;jbellis;3883-v2.txt;https://issues.apache.org/jira/secure/attachment/12521974/3883-v2.txt,09/Apr/12 21:59;jbellis;3883-v3.txt;https://issues.apache.org/jira/secure/attachment/12522025/3883-v3.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-02-13 18:41:37.653,,,no_permission,,,,,,,,,,,,227276,,,Wed Apr 11 18:28:36 UTC 2012,,,,,,0|i0gpe7:,95544,brandon.williams,brandon.williams,,,,,,,,,"12/Feb/12 20:02;brandon.williams;At least one problem here is that after retrieving the first page of a row, we set the startToken to the token of the last row:
{noformat}
startToken = partitioner.getTokenFactory().toString(partitioner.getToken(lastRow.key));
{noformat}

which is exclusive and thus advances to the next row, without completely paging through the first.","13/Feb/12 14:06;brandon.williams;My original description here is incorrect; I can't repro the 198 count (not sure what happened there) but now the wide row tests counts 1033 'word1' items.  As far as I can tell, WordCountSetup actually inserts a total of 2002 'word1' matches, one in each of text1 and text2, and a thousand in each of text3 and text4.  I'm not sure what is causing the count discrepancy, but in any case 1033 is far above the batch size of 99, and and the 4th word count test using a secondary index is counting 197 items, so I think something may be fundamentally wrong with word count.

That said, I've been adding wide row support to pig and testing with that, and the problem of not being able to completely paginate wide rows is a definite problem.","13/Feb/12 16:02;brandon.williams;v1 isn't perfect but it's a start; if the batch starts on a wide row, we reuse the token and iterate until we're done.  Unfortunately if we don't start on one, I'm not sure if there's a way to detect that we're in a wide row without making an extra rpc against the last row seen every time.","13/Feb/12 18:41;jbellis;bq. Unfortunately if we don't start on one, I'm not sure if there's a way to detect that we're in a wide row without making an extra rpc against the last row seen every time.

If we can easily address this w/ some extra logic in get_paged_slice then great, otherwise doing one extra rpc call out of (split size * pages per row in split) doesn't seem like a big deal to me.","14/Feb/12 17:24;brandon.williams;There's no sane way to do this with get_paged_slice as it currently is.  We can do the extra rpc to determine if we're at the end of a row, but then we can end up in an ugly situation where there's only one or two more columns outside of the batch size, but when we slice those on the next iteration those are the only columns we can return because our slice predicate is invalid for anything else; even if we happen to get a full batch back.","07/Mar/12 16:35;slebresne;How would get_paged_slice need to be to make that sane, and is there any short term solution to get this fixed for 1.1.0?","07/Mar/12 19:25;brandon.williams;Optimally, we'd have a way to express ""I'm at this column offset in this row, give me the next X number of columns, even if it requires going to the next row.""  But I'm not sure how to do that sanely, either.  I know Jake is using a special CFIF for hive to handle wide rows that basically just grabs one row at a time and paginates it, which is fine if all the rows are wide, but will take a performance hit if they are not.  Still, that might be the best thing to do since using get_page_slices is currently so hairy.","06/Apr/12 22:40;jbellis;bq. Optimally, we'd have a way to express ""I'm at this column offset in this row, give me the next X number of columns, even if it requires going to the next row."" But I'm not sure how to do that sanely, either

What if we allowed mixing (start key, end token) in KeyRange?  Wouldn't that fix it?

- 1st get_paged_slice call: ((start token, end token), empty start column) from slice
- subsequent get_paged_slice calls: ((last row key, end token), last column name)
",06/Apr/12 22:47;brandon.williams;That sounds like it could work.,09/Apr/12 17:41;jbellis;v2 attached w/ that approach,09/Apr/12 21:59;jbellis;v3 to avoid double-counting the startColumn.  also cleans up lastRow cruftiness a bit.,"10/Apr/12 21:16;jbellis;Latest is at https://github.com/jbellis/cassandra/branches/3383-5. I think it's working now, except for being blocked by CASSANDRA-4136.","11/Apr/12 14:29;jbellis;https://github.com/jbellis/cassandra/branches/3883-6 is up, with CASSANDRA-4136 incorporated.  the results look good for the word_count test, as posted on 4136.

(the other minor change with -6 is adding conf/ to the classpath for log4j.)","11/Apr/12 16:21;brandon.williams;LGTM, +1",11/Apr/12 18:28;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saved CF row cache breaks when upgrading to 1.1,CASSANDRA-3849,12541166,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,04/Feb/12 02:03,12/Mar/19 14:16,13/Mar/19 22:26,06/Feb/12 14:18,1.1.0,,,,,,0,,,,,"Enabled row and key caching. Used stress to insert some data. ran nodetool flush, then nodetool compact. Then read the data back to populate the cache. Turned row_cache_save_period and key_cache_save_period really low to force saving the cache data. I verified that the row and key cache files existed in /var/lib/cassandra/saved_caches/.

I then killed cassandra, checked out branch cassandra-1.1, compiled and tried to start the node. The node failed to start, and I got this error:
{code}
 INFO 01:33:30,893 reading saved cache /var/lib/cassandra/saved_caches/Keyspace1-Standard1-RowCache
ERROR 01:33:31,009 Exception encountered during startup
java.lang.AssertionError: Row cache is not enabled on column family [Standard1]
	at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:1050)
	at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:383)
	at org.apache.cassandra.db.Table.open(Table.java:122)
	at org.apache.cassandra.db.Table.open(Table.java:100)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
java.lang.AssertionError: Row cache is not enabled on column family [Standard1]
	at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:1050)
	at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:383)
	at org.apache.cassandra.db.Table.open(Table.java:122)
	at org.apache.cassandra.db.Table.open(Table.java:100)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
Exception encountered during startup: Row cache is not enabled on column family [Standard1]
{code}",1 node cluster running on branch cassandra-1.0. Ubuntu. both key and row caching were enabled.,,,,,,,,,,,,,,,04/Feb/12 19:39;xedin;CASSANDRA-3849.patch;https://issues.apache.org/jira/secure/attachment/12513259/CASSANDRA-3849.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-06 14:18:34.319,,,no_permission,,,,,,,,,,,,226518,,,Mon Feb 06 14:18:34 UTC 2012,,,,,,0|i0goyn:,95474,slebresne,slebresne,,,,,,,,,"06/Feb/12 14:18;slebresne;+1, committed, thks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cfhistograms is transposed/wrong again,CASSANDRA-3222,12523391,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,16/Sep/11 20:58,12/Mar/19 14:16,13/Mar/19 22:26,17/Sep/11 17:45,0.7.10,0.8.7,,,,,0,,,,,"Read/write latencies are transposed, row size is always equal the column count.  I think we've fixed this at least twice before, but here it is again.",,,,,,,,,,,,,,,,16/Sep/11 22:37;brandon.williams;3222.txt;https://issues.apache.org/jira/secure/attachment/12494880/3222.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-17 02:05:52.874,,,no_permission,,,,,,,,,,,,4011,,,Sat Sep 17 17:55:21 UTC 2011,,,,,,0|i0ghaf:,94231,jbellis,jbellis,,,,,,,,,16/Sep/11 22:37;brandon.williams;Turns out that CASSANDRA-2123 never fixed the transposition.  CASSANDRA-2284 broke the row size/column count.,17/Sep/11 02:05;jbellis;+1,17/Sep/11 17:45;brandon.williams;Committed.,"17/Sep/11 17:55;hudson;Integrated in Cassandra-0.7 #553 (See [https://builds.apache.org/job/Cassandra-0.7/553/])
    Fix cfhistograms read/write latency transposition.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-3222

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1172024
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/tools/NodeCmd.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions during start up after schema disagreement,CASSANDRA-3954,12543997,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,mdymarek,mdymarek,24/Feb/12 14:49,12/Mar/19 14:16,13/Mar/19 22:26,16/Mar/12 11:04,1.1.0,,,,,,0,,,,,"Hi,
i`ve got schema disaggreement after dropping down keyspace,
i`ve switched off one nodes in cluster, after starting i`ve got bunch of these exceptions:
{code}
ERROR [SSTableBatchOpen:1] 2012-02-24 14:21:00,759 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer
        at org.apache.cassandra.db.marshal.UTF8Type.compare(UTF8Type.java:27)
        at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
        at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:89)
        at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)
        at java.util.TreeMap.getEntry(TreeMap.java:328)
        at java.util.TreeMap.containsKey(TreeMap.java:209)
        at java.util.TreeSet.contains(TreeSet.java:217)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:393)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:189)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:227)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}
and this one on the end of start up:
{code}
ERROR [MigrationStage:1] 2012-02-24 14:37:22,750 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:282)
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:216)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:330)
        at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:240)
        at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:124)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}
Any ideas why they`ve appeared?",,,,,,,,,,,,,,,,15/Mar/12 22:04;xedin;CASSANDRA-3954-v2.patch;https://issues.apache.org/jira/secure/attachment/12518555/CASSANDRA-3954-v2.patch,14/Mar/12 20:02;xedin;CASSANDRA-3954.patch;https://issues.apache.org/jira/secure/attachment/12518364/CASSANDRA-3954.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-24 14:59:27.492,,,no_permission,,,,,,,,,,,,229235,,,Fri Mar 16 11:04:09 UTC 2012,,,,,,0|i0gq87:,95679,jbellis,jbellis,,,,,,,,,24/Feb/12 14:59;jbellis;Sounds like you changed your cluster's partitioner.  That's not supported.,"24/Feb/12 16:04;forsberg;To change partitioner you would have to edit cassandra.yaml, right? There was no such change before this restart.","24/Feb/12 16:21;jbellis;Can you elaborate then on what exactly changed?

Is this a 1.1 cluster or 1.0?","24/Feb/12 17:05;forsberg;We had a schema disagreement in this two-node cluster. Tried restarting one of the nodes, and these exceptions occured in log.

bq. Is this a 1.1 cluster or 1.0?

You're not going to like this: It's a snapshot from cassandra-1.1 branch from about a week ago. I can find out the exact commit, but I don't have it readily available right now.",24/Feb/12 22:53;jbellis;Pavel fixed CASSANDRA-3884 yesterday -- might be the same thing?,"29/Feb/12 09:12;mdymarek;it might be,
I`ve installed new snapshot(made from commit 449e037195c3c504d7aca5088e8bc7bd5a50e7d0)
and second exception is not thrown any more,
First type of exception( aka java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer)
is still thrown during startup, in fact i`ve got several of them in startup logs...
Note that node is starting up even that these exceptions are thrown...","14/Mar/12 04:23;jbellis;Been thinking about this for a while, and I now have a hypothesis: I bet it's some kind of confusion w/ the new unified key cache. Can you have a look, Pavel?","14/Mar/12 11:16;xedin;Mariusz, can you please provide additional information - do you use secondary indexes and what is the SSTable name it fails to open (it should be on top of the exception ""Opening <sstable-name> (<n> bytes)"")?  My current guess is that it could be related to the secondary indexes.","14/Mar/12 12:45;mdymarek;
yep, we`re using secondary index a lot,
{noformat}
 INFO [SSTableBatchOpen:2] 2012-02-24 14:21:00,752 SSTableReader.java (line 156) Opening /cassandra/test_keyspace/test_cf/test_keyspace-test_cf.test_cf_test_column_with_index_idx-hc-2 (727 bytes)
{noformat}
we have lost some secondary index information after restarting of cluster, we had to rebuild indexes after that...",14/Mar/12 20:02;xedin;We don't support secondary index caching with global caches so I have added condition to skip reading key cache for secondary index CFs.,15/Mar/12 21:28;jbellis;What if we just created the index CFMetadata with caching=NONE instead of adding a special flag?,"15/Mar/12 21:35;xedin;That is a good idea, v2 implements that.","15/Mar/12 21:53;jbellis;better, but we should actually load key cache only for KEYS_ONLY or ALL, rather than !NONE (which would include ROWS ONLY).",15/Mar/12 22:04;xedin;updated v2 which skips key cache read if caching is NONE or ROWS_ONLY.,"16/Mar/12 03:41;jbellis;+1

nit: it's preferred to use == with enums",16/Mar/12 11:04;xedin;Committed with nit fixed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception durint start up after updating cassandra,CASSANDRA-3963,12544234,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,mdymarek,mdymarek,27/Feb/12 09:42,12/Mar/19 14:16,13/Mar/19 22:26,27/Feb/12 16:59,1.1.0,,,,,,0,,,,,"Hi,
i`ve updated cassandra on two-nodes cluster and i`ve got this exception during start up:
{code}
INFO 09:06:06,520 Opening /cassandra/system/HintsColumnFamily/system-HintsColumnFamily-hc-1 (178828054 bytes)
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:384)
        at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:332)
        at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:156)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:514)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:254)
        ... 5 more
Cannot load daemon
Service exit with a return value of 3
{code}
we`re running 2 node cluster on snapshots from cassandra-1.1 branch:
* old snapshot: 5f5e00bc9a83bfd701723cbc7bf2307ea53da342 + patches from CASSANDRA-3740
* new snapshot: d65590ac8a5a3cfbe1529ef77346e84d6961db7d",,,,,,,,,,,,,,,,27/Feb/12 15:08;xedin;CASSANDRA-3963.patch;https://issues.apache.org/jira/secure/attachment/12516166/CASSANDRA-3963.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-27 15:08:58.729,,,no_permission,,,,,,,,,,,,229471,,,Wed Feb 29 09:07:21 UTC 2012,,,,,,0|i0gqdr:,95704,jbellis,jbellis,,,,,,,,,27/Feb/12 15:08;xedin;patch for cassandra-1.1.0 branch (latest commit a15c35b0942f91d6f9e45139d99491e189989bde),27/Feb/12 16:13;jbellis;+1,27/Feb/12 16:59;xedin;Committed,"29/Feb/12 09:07;mdymarek;It works now, thanks for help..",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage can't cast fields from a CF correctly,CASSANDRA-3962,12544197,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,jalkanen,jalkanen,26/Feb/12 20:51,12/Mar/19 14:16,13/Mar/19 22:26,27/Feb/12 17:04,1.0.9,1.1.0,,,,,0,hadoop,pig,,,"Included scripts demonstrate the problem.  Regardless of whether the key is cast as a chararray or not, the Pig scripts fail with 

{code}
java.lang.ClassCastException: org.apache.pig.data.DataByteArray cannot be cast to java.lang.String
	at org.apache.pig.backend.hadoop.HDataType.getWritableComparableTypes(HDataType.java:72)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map.collect(PigGenericMapReduce.java:117)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:269)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:262)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
{code}

","OSX 10.6.latest, Pig 0.9.2.",,,,,,,,,,,,,,,26/Feb/12 22:01;brandon.williams;0001-Add-LoadCaster-to-CassandraStorage.txt;https://issues.apache.org/jira/secure/attachment/12516124/0001-Add-LoadCaster-to-CassandraStorage.txt,27/Feb/12 15:46;brandon.williams;0002-Compose-key-from-marshaller.txt;https://issues.apache.org/jira/secure/attachment/12516167/0002-Compose-key-from-marshaller.txt,26/Feb/12 20:51;jalkanen;test.cli;https://issues.apache.org/jira/secure/attachment/12516115/test.cli,26/Feb/12 20:54;jalkanen;test.pig;https://issues.apache.org/jira/secure/attachment/12516116/test.pig,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-02-26 20:59:11.446,,,no_permission,,,,,,,,,,,,229434,,,Mon Feb 27 17:04:36 UTC 2012,,,,,,0|i0gqcf:,95698,xedin,xedin,,,,,,,,,26/Feb/12 20:51;jalkanen;Generate the test CFs.,"26/Feb/12 20:54;jalkanen;The test pig script for both cases, you might want to comment out the ""dump a"" to let it continue.","26/Feb/12 20:55;jalkanen;Relevant IRC log from #cassandra

{code}
[22:26] driftx: hmm, I think this is the udfcontext signature reuse problem
[22:26] driftx: jeromatron: what's the workaround for that again?
[22:26] Ecyrd: Is there an open bug?
[22:28] driftx: #2869, but we fixed it. hmm.
[22:28] CassBotJr: https://issues.apache.org/jira/browse/CASSANDRA-2869 : CassandraStorage does not function properly when used multiple times in a single pig script due to UDFContext sharing issues
[22:34] driftx:         case DataType.CHARARRAY:
[22:34] driftx:             return new NullableText((String)o);
[22:34] driftx: so it thinks it has a chararray, but it still has a bytearray
[22:42] driftx: I think we have to implement a LoadCaster to get around this
[22:43] Ecyrd: So I'm not insane, this is a real bug 
{code}","26/Feb/12 20:59;brandon.williams;I think implementing LoadCaster will fix this, but it's strange to me that pig doesn't allow going to the other way, casting a chararray to a bytearray since that's the only thing guaranteed to work here, in case the Bytes CF has keys that won't map to UTF8.","26/Feb/12 22:01;brandon.williams;Patch to add a LoadCaster.  It does get used and converts the byte[] to String, but the join still fails with the same error :(","27/Feb/12 12:09;jalkanen;Could the fact that these are row keys, not columns, have something to do with the issue? Looking at CassandraStorage.getNext(), there's a line

    // set the key
    tuple.append(new DataByteArray(ByteBufferUtil.getArray(key)));

So it looks to me like the key is *always* added as a DataByteArray, regardless of it's actual type? getSchema() does seem to read the value from CfDef correctly tho'.","27/Feb/12 15:46;brandon.williams;You're totally right.  It wasn't a problem casting the key from Bytes, but trying to use the one from U8.  Patch to compose the key from the marshaller.",27/Feb/12 16:46;xedin;+1,"27/Feb/12 17:04;brandon.williams;Committed 0002, and also incorporated Janne's test into the existing tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hints Should Be Dropped When Missing CFid Implies Deleted ColumnFamily,CASSANDRA-3975,12544447,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,cherro,cherro,28/Feb/12 16:51,12/Mar/19 14:16,13/Mar/19 22:26,29/Feb/12 03:16,1.0.9,1.1.0,,,,,0,datastax_qa,,,,"If hints have accumulated for a CF that has been deleted, Hinted Handoff repeatedly fails until manual intervention removes those hints. For 1.0.7, UnserializableColumnFamilyException is thrown only when a CFid is unknown on the sending node. As discussed on #cassandra-dev, if the schema is in agreement, the affected hint(s) should be deleted to avoid indefinite repeat failures.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-02-28 23:28:59.461,,,no_permission,,,,,,,,,,,,229684,,,Wed Feb 29 03:16:10 UTC 2012,,,,,,0|i0gqin:,95726,brandon.williams,brandon.williams,,,,,,,,,"28/Feb/12 16:51;cherro;Stack trace from 1.0.7:

 INFO [HintedHandoff:854] 2012-02-27 22:39:51,183 org.apache.cassandra.db.HintedHandOffManager Started hinted handoff for token: Token(bytes[7f]) with IP: /XX.XX.XX.XXX
ERROR [HintedHandoff:854] 2012-02-27 22:39:51,186 org.apache.cassandra.service.AbstractCassandraDaemon Fatal exception in thread Thread[HintedHandoff:854,1,main]
java.lang.RuntimeException: org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=2391
        at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=2391
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:129)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:401)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:409)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:344)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:248)
        at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:416)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)","28/Feb/12 16:55;cherro;Suggest that for 1.0.x, UnserializableColumnFamilyException might be better named UnknownColumnFamilyException (since unknown cfId is the only time its ever thrown. It could then be caught within HHOM.deliverHintsToEndpointInternal and the affected hint deleted. Its unclear to me if this needs to be tackled differently for 1.1.",28/Feb/12 23:28;jbellis;Patch posted to https://github.com/jbellis/cassandra/branches/3975,29/Feb/12 00:33;brandon.williams;+1,29/Feb/12 03:16;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig CounterColumnFamily support,CASSANDRA-3973,12544428,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jalkanen,jalkanen,jalkanen,28/Feb/12 15:08,12/Mar/19 14:16,13/Mar/19 22:26,28/Feb/12 15:48,1.0.9,1.1.0,,,,,0,counters,hadoop,pig,,"Included a patch to the current test script to demonstrate that CassandraStorage does not support counter columns, as well as a patch to fix the issue.

The core reason is that CassandraStorage assumes that counters can be unpacked using CounterColumnType, when in fact the column value is already a Long.","OSX 10.6.latest, Pig 0.9.2",,,,,,,,,,,,,,,28/Feb/12 15:21;brandon.williams;3973-v2.txt;https://issues.apache.org/jira/secure/attachment/12516338/3973-v2.txt,28/Feb/12 15:10;jalkanen;cassandrastorage.txt;https://issues.apache.org/jira/secure/attachment/12516337/cassandrastorage.txt,28/Feb/12 15:10;jalkanen;testpatch.txt;https://issues.apache.org/jira/secure/attachment/12516336/testpatch.txt,28/Feb/12 15:38;jalkanen;v3.txt;https://issues.apache.org/jira/secure/attachment/12516343/v3.txt,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-02-28 15:21:10.324,,,no_permission,,,,,,,,,,,,229665,,,Tue Feb 28 15:48:30 UTC 2012,,,,,,0|i0gqhz:,95723,brandon.williams,brandon.williams,,,,,,,,,28/Feb/12 15:10;jalkanen;Patch for tests; another patch for CassandraStorage.,28/Feb/12 15:21;brandon.williams;v2 to nip counters directly in parseType so we can catch them in super columns too.,"28/Feb/12 15:38;jalkanen;V2 patch wasn't working, getDefaultMarshallers() was invoking TypeParser.parse() directly.  Here's v3 which fixes that issue (and protects against null reference).","28/Feb/12 15:48;brandon.williams;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gossipers notion of schema differs from reality as reported by the nodes in question,CASSANDRA-3931,12543184,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,scode,scode,19/Feb/12 04:09,12/Mar/19 14:16,13/Mar/19 22:26,21/Feb/12 19:28,1.1.0,,,,,,0,,,,,"On a 1.1 cluster we happened to notice that {{nodetool gossipinfo | grep SCHEMA}} reported disagreement:

{code}
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:b0d7bab7-c13c-37d9-9adb-8ab8a5b7215d
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:bcdbd318-82df-3518-89e3-6b72227b3f66
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:bcdbd318-82df-3518-89e3-6b72227b3f66
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
{code}

However, the result of a thrift {{describe_ring}} on the cluster claims they all agree and that {{b0d7bab7-c13c-37d9-9adb-8ab8a5b7215d}} is the schema they have.

The schemas seem to ""actually"" propagate; e.g. dropping a keyspace actually drops the keyspace.",,,,,,,,,,,,,,,,21/Feb/12 19:14;brandon.williams;3931-v2.txt;https://issues.apache.org/jira/secure/attachment/12515469/3931-v2.txt,21/Feb/12 18:40;brandon.williams;3931.txt;https://issues.apache.org/jira/secure/attachment/12515461/3931.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-21 14:49:40.218,,,no_permission,,,,,,,,,,,,228470,,,Wed Dec 05 17:00:10 UTC 2012,,,,,,0|i0gpy7:,95634,xedin,xedin,,,,,,,,,"19/Feb/12 04:11;scode;I forgot to mention that {{describe_ring}} is using a piece of code that actually sends messages to other nodes asking for their schema, while {{gossipinfo}} is giving the information contained in the Gossiper's endpoint state map.

Given that the schema seems to *actually* be propagated, I suspect this is only a gossiping propagation bug but that is not confirmed.","21/Feb/12 14:49;brandon.williams;Hmm, does hinted handoff work in this state?  I ask because we've had this problem before and addressed it there:

{code}
        waited = 0;
        // then wait for the correct schema version.
        // usually we use DD.getDefsVersion, which checks the local schema uuid as stored in the system table.
        // here we check the one in gossip instead; this serves as a canary to warn us if we introduce a bug that
        // causes the two to diverge (see CASSANDRA-2946)
{code}",21/Feb/12 18:40;brandon.williams;Patch to update gossip when merging remote versions.,21/Feb/12 19:14;brandon.williams;v2 replaces all the one-off calls of passiveAnnounce by introducing updateVersionAndAnnounce in Schema.,21/Feb/12 19:25;xedin;+1 with nit: change in Migration is unnecessary because passiveAnnounce get's called as part of Migration.announce() routine so we don't need to change apply() behavior.,21/Feb/12 19:28;brandon.williams;Committed w/Migration.apply change removed.,"22/Feb/12 05:31;scode;@Brandon For the record, we had HH turned off so I don't know.",05/Dec/12 17:00;cherro;FYI the fixes for this issue introduced issue CASSANDRA-5025.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support compression using BulkWriter,CASSANDRA-3907,12542482,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,14/Feb/12 05:18,12/Mar/19 14:16,13/Mar/19 22:26,14/Feb/12 17:59,1.1.0,,,,,,0,,,,,Currently there is no way to enable compression using BulkWriter. ,,,,,,,,,,,,,,,,14/Feb/12 16:59;lenn0x;0001-Add-compression-support-to-BulkWriter.patch;https://issues.apache.org/jira/secure/attachment/12514505/0001-Add-compression-support-to-BulkWriter.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-14 17:43:44.714,,,no_permission,,,,,,,,,,,,227768,,,Wed Feb 15 12:33:49 UTC 2012,,,,,,0|i0gpnz:,95588,brandon.williams,brandon.williams,,,,,,,,,"14/Feb/12 17:43;brandon.williams;LGTM, +1","15/Feb/12 08:26;forsberg;So, with this patch, if I enable compression with BulkWriter, sstables will be compressed on the hadoop side, streamed in compressed form and the cassandra daemon does not have to compress them before writing to disk?

Or am I misunderstanding how this works?",15/Feb/12 12:33;brandon.williams;Your understanding is correct.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove no-longer-valid values from ColumnFamilyArgument enum,CASSANDRA-3888,12542173,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,10/Feb/12 18:52,12/Mar/19 14:16,13/Mar/19 22:26,10/Feb/12 23:43,1.1.0,,,Legacy/Tools,,,0,,,,,"{code}
[default@churnkeyspace] update column family churncf with keys_cached=100;
Exception in thread ""main"" java.lang.AssertionError
	at org.apache.cassandra.cli.CliClient.updateCfDefAttributes(CliClient.java:1244)
	at org.apache.cassandra.cli.CliClient.executeUpdateColumnFamily(CliClient.java:1091)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:234)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)
{code}
",,,,,,,,,,,,,,,,10/Feb/12 19:16;scode;CASSANDRA-3888-1.1.txt;https://issues.apache.org/jira/secure/attachment/12514144/CASSANDRA-3888-1.1.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-10 19:11:46.77,,,no_permission,,,,,,,,,,,,227460,,,Fri Feb 10 23:43:26 UTC 2012,,,,,,0|i0gpg7:,95553,jbellis,jbellis,,,,,,,,,"10/Feb/12 18:57;scode;Due to CASSANDRA-3143. Trying to figure out whether the intent is to not allow keys_cached, or to allow it but silently ignore it. The patch in 3143 certainly removes the switch case.","10/Feb/12 19:11;jbellis;I'm not 100% sure what the intent is either, but if it's the former, it should be IRE instead of AssertionError.","10/Feb/12 19:16;scode;The precedent is IAE due to enum valueOf() failure:

{code}
[default@churnkeyspace] update column family churncf with asdfasfadsfdsf=13;
java.lang.IllegalArgumentException: No enum const class org.apache.cassandra.cli.CliClient$ColumnFamilyArgument.ASDFASFADSFDSF
{code}","10/Feb/12 19:18;scode;(IRE would be cleaner, but applies generally and is not specific to this particular bug.)","10/Feb/12 19:23;jbellis;Oh, this is a CliClient error, not a server one...",10/Feb/12 19:25;scode;Yes. Sorry if I wasn't clear on that.,10/Feb/12 23:43;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disablethrift and Enablethrift can leaves behind zombie connections on THSHA server,CASSANDRA-3867,12541499,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,06/Feb/12 20:43,12/Mar/19 14:16,13/Mar/19 22:26,13/Feb/12 20:43,1.0.8,1.1.1,,,,,0,,,,,"While doing nodetool disable thrift we disable selecting threads and close them... but the connections are still active...
Enable thrift creates a new Selector threads because we create new ThriftServer() which will cause the old connections to be zombies.

I think the right fix will be to call server.interrupt(); and then close the connections when they are done selecting.",,,,,,,,,,,,,,,,11/Feb/12 02:53;vijay2win@yahoo.com;0001-CASSANDRA-3867.patch;https://issues.apache.org/jira/secure/attachment/12514196/0001-CASSANDRA-3867.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-13 16:36:45.755,,,no_permission,,,,,,,,,,,,226786,,,Mon Feb 13 20:43:13 UTC 2012,,,,,,0|i0gp6v:,95511,brandon.williams,brandon.williams,,,,,,,,,11/Feb/12 02:53;vijay2win@yahoo.com;Simple patch to close the selector when disablethrift.,13/Feb/12 16:36;brandon.williams;+1,13/Feb/12 20:43;vijay2win@yahoo.com;Committed to trunk and 1.0 trunk (as it is a trivial bug fix)... Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missing documents for caching in 1.1,CASSANDRA-3984,12544699,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,01/Mar/12 00:26,12/Mar/19 14:16,13/Mar/19 22:26,06/Mar/12 10:48,1.1.0,,,Legacy/Documentation and Website,,,0,,,,,add row cache and key cache setting documentation in CliHelp.yaml,,,,,,,,,,,,,,,,01/Mar/12 00:46;vijay2win@yahoo.com;0001-docs-for-caching.patch;https://issues.apache.org/jira/secure/attachment/12516627/0001-docs-for-caching.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-06 10:48:20.222,,,no_permission,,,,,,,,,,,,229885,,,Tue Mar 06 10:48:20 UTC 2012,,,,,,0|i0gqlz:,95741,slebresne,slebresne,,,,,,,,,06/Mar/12 10:48;slebresne;Committed (I took the liberty to do some text modification). Thanks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Division by zero on get_slice,CASSANDRA-4000,12545192,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,vanger,vanger,05/Mar/12 15:45,12/Mar/19 14:16,13/Mar/19 22:26,07/Mar/12 10:14,1.0.9,,,,,,0,,,,,"We have a column family with String row keys and Long column keys.

Our WideEntityService is trying to get the first column in the range from 0 to Long.MAX. It's a batch operation performed for every row in the CF (rows count is approximately tens of thousands and each row contains from 0 to 1000 columns). 

After processing each row we are removing some of the columns we have queried. Also, at the same time we are writing in this CF in another threads but somewhat less intensive.

An error rises approximately for a one of 100 rows.


Exception itself:
[05-Mar-2012 18:47:25,247] ERROR [http-8095-1 WideEntityServiceImpl.java:142] - get: key1 - {type=RANGE, start=0, end=9223372036854775807, orderDesc=false, limit=1}
me.prettyprint.hector.api.exceptions.HCassandraInternalException: Cassandra encountered an internal error processing this request: TApplicationError type: 6 message:Internal error processing get_slice
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:31)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$7.execute(KeyspaceServiceImpl.java:285)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$7.execute(KeyspaceServiceImpl.java:268)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:233)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:131)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.getSlice(KeyspaceServiceImpl.java:289)
        at me.prettyprint.cassandra.model.thrift.ThriftSliceQuery$1.doInKeyspace(ThriftSliceQuery.java:53)
        at me.prettyprint.cassandra.model.thrift.ThriftSliceQuery$1.doInKeyspace(ThriftSliceQuery.java:49)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
        at me.prettyprint.cassandra.model.thrift.ThriftSliceQuery.execute(ThriftSliceQuery.java:48)","We start getting this exception after upgrading from 1.0.1 -> 1.0.8.

4 nodes cluster on Cassandra v1.0.8. RF = 3. 
Hector v0.8.0-3.",,,,,,,,,,,,,,,06/Mar/12 10:32;slebresne;4000.txt;https://issues.apache.org/jira/secure/attachment/12517216/4000.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-05 17:39:11.721,,,no_permission,,,,,,,,,,,,230378,,,Wed Mar 07 10:14:17 UTC 2012,,,,,,0|i0gqsn:,95771,jbellis,jbellis,,,,,,,,,05/Mar/12 17:39;amorton;What was the error in the server log ?,"06/Mar/12 06:41;vanger;ERROR [Thrift:104] 2012-03-01 20:19:57,332 Cassandra.java (line 3041) Internal error processing get_slice
java.lang.ArithmeticException: / by zero
        at org.apache.cassandra.db.SliceFromReadCommand.maybeGenerateRetryCommand(SliceFromReadCommand.java:87)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:724)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:564)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:283)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:365)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:326)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:3033)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)",06/Mar/12 10:32;slebresne;Slightly stupid bug introduced by CASSANDRA-3934. Fix attached,06/Mar/12 16:18;jbellis;Can we add a test to catch this regression?,"06/Mar/12 16:37;slebresne;I've just pushed a dtest to reproduce (and checked the patch does fix). It's during short reads, so not sure there's an easy way to test that with a unit test.","06/Mar/12 18:04;jbellis;+1, although we might want to query for 2*count instead of count+1 in the no-live-columns case, since if all count columns were dead the odds seem pretty good that just one more column won't help.","06/Mar/12 18:40;slebresne;I hesitate on that one, but in fact I think that this case can realistically happen when:
# either the user requested 1 or 2 columns (like in this ticket). Asking for +1 or *2 doesn't change much.
# the row has been deleted but one node didn't got the tombstone yet (or at all). In that case, you mostly want for the RR to send the missed tombstone to the node missing it. However, we can't be sure the retry will arrive after the RR (i.e. we may need a third retry if we're unlucky), and so asking for twice more data from that one node may be less efficient.

I mean, clearly there is no need to over-think this and I'm good with either +1 and *2 but I'm not really convinced one will be better (nor really worse) than the other in general.",06/Mar/12 19:00;jbellis;Either one works for me.,"07/Mar/12 10:14;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WARN No appenders could be found for logger (org.apache.cassandra.confi,CASSANDRA-4013,12545613,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,leekh,leekh,08/Mar/12 06:05,12/Mar/19 14:16,13/Mar/19 22:26,08/Mar/12 06:16,,,,Legacy/Documentation and Website,,,0,,,,,"i'm installed apache-cassandra-1.1.0-beta1 version.
so, modify window7Config to Basic Linux Config 
and i play cmd mode....
write ""cassandra-cli"" and entered.
so i write ""create keyspace keyspace1;""
but not created. output is error....
what the... help...me...

and.. where is keyspace config?
different 0.6.8version to 1.1.0version
xml....to yaml?
 please send to mail ..
xyzxes@nate.com...
C:\apache-cassandra-1.1.0-beta1\bin>cassandra-cli
Starting Cassandra Client
Connected to: ""Test Cluster"" on 127.0.0.1/9160
Welcome to Cassandra CLI version 1.1.0-beta1

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] create keyspace keyspace1;
log4j:WARN No appenders could be found for logger (org.apache.cassandra.confi
atabaseDescriptor).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more
fo.
Cannot locate cassandra.yaml
Fatal configuration error; unable to start serv","win7 
apacheTomcat6.0
JDK1.6
",259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-03-08 06:16:20.661,,,no_permission,,,,,,,,,,,,230799,,,Sun Mar 18 00:08:26 UTC 2012,,,,,,0|i0gqyf:,95797,leekh,leekh,,,,,,,,,"08/Mar/12 06:16;jbellis;""Cannot locate cassandra.yaml"" was fixed in CASSANDRA-3986.  In the meantime if you run it from one directory up ({{bin\cassandra-cli}}) that should work.

The user mailing list is the right place to address questions like ""where is keyspace config.""","11/Mar/12 04:15;thegillis;Unfortunately the suggestion of running in the cassandra home folder did not work for me. It looks like the CLI bat file clears out the classpath and doesn't add the conf folder to it.

I just took the hint from the main cassandra.bat file and had to change the classpath blank init line to:

REM Ensure that any user defined CLASSPATH variables are not used on startup
set CLASSPATH=""%CASSANDRA_HOME%\conf""

1.0.8 on Windows

","18/Mar/12 00:08;giorgiofran;Thanks Brian, 
  that solved my problem too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"memtable.updateLiveRatio() is blocking, causing insane latencies for writes",CASSANDRA-4032,12545857,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,09/Mar/12 15:10,12/Mar/19 14:16,13/Mar/19 22:26,11/Apr/12 18:27,1.1.0,,,,,,0,,,,,"Reproduce by just starting a fresh cassandra with a heap large enough for live ratio calculation (which is {{O(n)}}) to be insanely slow, and then running {{./bin/stress -d host -n100000000 -t10}}. With a large enough heap and default flushing behavior this is bad enough that stress gets timeouts.

Example (""blocked for"" is my debug log added around submit()):

{code}
 INFO [MemoryMeter:1] 2012-03-09 15:07:30,857 Memtable.java (line 198) CFS(Keyspace='Keyspace1', ColumnFamily='Standard1') liveRatio is 8.89014894083727 (just-counted was 8.89014894083727).  calculation took 28273ms for 1320245 columns
 WARN [MutationStage:8] 2012-03-09 15:07:30,857 Memtable.java (line 209) submit() blocked for: 231135
{code}

The calling code was written assuming a RejectedExecutionException is thrown, but it's not because {{DebuggableThreadPoolExecutor}} installs a blocking rejection handler.
",,,,,,,,,,,,,,,,06/Apr/12 22:29;jbellis;4032-v3.txt;https://issues.apache.org/jira/secure/attachment/12521777/4032-v3.txt,10/Apr/12 16:00;jbellis;4032-v4.txt;https://issues.apache.org/jira/secure/attachment/12522121/4032-v4.txt,09/Mar/12 15:31;scode;CASSANDRA-4032-1.1.0-v1.txt;https://issues.apache.org/jira/secure/attachment/12517729/CASSANDRA-4032-1.1.0-v1.txt,09/Mar/12 18:06;scode;CASSANDRA-4032-1.1.0-v2.txt;https://issues.apache.org/jira/secure/attachment/12517754/CASSANDRA-4032-1.1.0-v2.txt,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-03-09 15:36:03.907,,,no_permission,,,,,,,,,,,,231040,,,Wed Apr 11 18:27:14 UTC 2012,,,,,,0|i0gr67:,95832,slebresne,slebresne,,,,,,,,,09/Mar/12 15:31;scode;Attaching patch that allows us to create both blocking and non-blocking {{DebuggableThreadPoolExecturor}}:s; use that for this particular case.,"09/Mar/12 15:36;jbellis;The DTPE change looks good, but don't you need a catch (Rejected) { } block in updateLiveRatio?","09/Mar/12 15:44;scode;There already is one, it's just a NOOP because of the blocking nature of the DTPE.","09/Mar/12 15:45;scode;This is the code:

{code}
        try
        {
            meterExecutor.submit(runnable);
        }
        catch (RejectedExecutionException e)
        {
            logger.debug(""Meter thread is busy; skipping liveRatio update for {}"", cfs);
        }
{code}
","09/Mar/12 15:51;slebresne;Are we sure that what we want is a SynchronousQueue with task rejected? After all, there is only on global memoryMeter, so we could end up failing to updateLiveRatio just based on a race, even if calculations are fast. I'd suggest instead a bounded queue (but maybe not infinite and we could indeed just skip task if that queue gets full).","09/Mar/12 15:57;jbellis;bq. This is the code:

Either I'm blind or that's not in the attached patch.","09/Mar/12 16:01;scode;{quote}
Either I'm blind or that's not in the attached patch.
{quote}

It's not. It's *already there*, in the branch, committed. The code was written, I presume, without the author realizing that RejectedExecutionException would never be thrown.","09/Mar/12 16:05;scode;{quote}
Are we sure that what we want is a SynchronousQueue with task rejected? After all, there is only on global memoryMeter, so we could end up failing to updateLiveRatio just based on a race, even if calculations are fast. I'd suggest instead a bounded queue (but maybe not infinite and we could indeed just skip task if that queue gets full).
{quote}

I agree it's fishy, though I'd suggest a separate ticket. This patch is intended to make the code behave the way the original commit intended.

This (from the code, not my patch) seems legit though:

{code}
    // we're careful to only allow one count to run at a time because counting is slow
    // (can be minutes, for a large memtable and a busy server), so we could keep memtables
    // alive after they're flushed and would otherwise be GC'd.
{code}

We could have one queue per unique CF and have a consumer that iterates over the set of queues, guaranteeing that each CF gets processed once per ""cycle"". A simpler solution is probably preferable though if we can think of one.
","09/Mar/12 16:07;scode;How about just having an unbounded queue but having each CF just keep a flag that says whether or not there is a calculation currently pending or executing; it would be reset by the task when executed, and CAS:ed in the write path. This; single queue, same DTPE. We'd just submit a task with a reference to the AtomicBoolean which it will set to false on completion.",09/Mar/12 16:09;scode;(That seems simple enough to just do right now and not bother with a separate ticket. If you agree I'll submit a patch.),09/Mar/12 16:17;slebresne;Sounds fine to me.,"09/Mar/12 18:06;scode;Attaching {{v2}}. Keeps NBHM mapping CFS -> AtomicBoolean. Mappings are never removed (assuming reasonably bound number of unique CFS:s during a lifetime). Queue used for DTPE is unbounded.
","09/Mar/12 18:07;scode;(v2 doesn't keep the changes to DTPE, which are no longer used.)","06/Apr/12 22:29;jbellis;v3 attached:

- uses putIfAbsent in the AtomicBoolean creation
- drops the catch block around executor submission",10/Apr/12 15:34;slebresne;Wouldn't using a ConcurrentSkipListSet simply the code? Feels like a lot of boilerplate to record that there is already a metering running (it's not like this is a hot path enough to justify the NBHM). ,"10/Apr/12 16:00;jbellis;you're right; v4 attached w/ Set approach.

(used NBHS instead of CSLS since the latter requires defining a comparator.  I'm not sure the overhead is substantially different.)","11/Apr/12 14:27;slebresne;+1 with two nits:
* In the comment ""to a maximum of one per CFS using this map"" could have a s/map/set/
* Note sure if there was an intent in changing the maximumPoolSize of the meterExecutor to Integer.MAX_VALUE. As it stands, with an unbounded queue the maximumPoolSize is ignored so that doesn't really matter, but just wanted to mention it.","11/Apr/12 18:27;jbellis;Committed, w/ nits amended",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cli shouldn't call FBU.getBroadcastAddress needlessly,CASSANDRA-3986,12544801,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,01/Mar/12 16:19,12/Mar/19 14:16,13/Mar/19 22:26,01/Mar/12 16:34,1.0.9,1.1.0,,,,,1,,,,,"The cli is calling this, which causes yaml to be loaded, but the broadcast address isn't needed.

{code}
            // adding default data center from SimpleSnitch
            if (currentStrategyOptions == null || currentStrategyOptions.isEmpty())
            {
                SimpleSnitch snitch = new SimpleSnitch();
                Map<String, String> options = new HashMap<String, String>();
                options.put(snitch.getDatacenter(FBUtilities.getBroadcastAddress()), ""1"");

                ksDef.setStrategy_options(options);
            }
{code}

because SimpleSnitch always returns 'datacenter1'",,,,,,,,,,,,,,,,01/Mar/12 16:20;brandon.williams;3986.txt;https://issues.apache.org/jira/secure/attachment/12516695/3986.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-01 16:22:50.731,,,no_permission,,,,,,,,,,,,229987,,,Thu Mar 08 06:58:45 UTC 2012,,,,,,0|i0gqmv:,95745,dbrosius@apache.org,dbrosius@apache.org,,,,,,,,,"01/Mar/12 16:22;dbrosius@apache.org;+1

Getting user reports that this is causing cli on windows to fail due to not being able to load the yaml for some reason (when creating keyspaces)",01/Mar/12 16:34;brandon.williams;Committed.  Probably only causes a problem on windows because the cli doesn't have the yaml in the classpath from the bat file.,"08/Mar/12 06:58;leekh;how to solve the proublem???

i don't know...
============================================================
           if (currentStrategyOptions == null || currentStrategyOptions.isEmpty())
            {
                SimpleSnitch snitch = new SimpleSnitch();
                Map<String, String> options = new HashMap<String, String>();
                options.put(snitch.getDatacenter(FBUtilities.getBroadcastAddress()), ""1"");

                ksDef.setStrategy_options(options);
            }
insert in cassandra-cli.bat??
============================================================
please help me..",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EC2 snitch incorrectly reports regions,CASSANDRA-4026,12545730,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,tnine,tnine,08/Mar/12 23:26,12/Mar/19 14:16,13/Mar/19 22:26,12/Mar/12 19:01,1.0.9,1.1.0,,,,,0,,,,,"Currently the org.apache.cassandra.locator.Ec2Snitch reports ""us-west"" in both the oregon and the california data centers.  This is incorrect, since they are different regions.

California => us-west-1
Oregon     => us-west-2

wget http://169.254.169.254/latest/meta-data/placement/availability-zone returns the value ""us-west-2a""


After parsing this returns

DC = us-west Rack = 2a


What it should return

DC = us-west-2 Rack = a


This makes it possible to use multi region when both regions are in the west coast.
",Ubuntu 10.10 64 bit Oracle Java 6,,,,,,,,,,,,,,,09/Mar/12 03:51;vijay2win@yahoo.com;0001-CASSANDRA-4026.patch;https://issues.apache.org/jira/secure/attachment/12517674/0001-CASSANDRA-4026.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-09 00:00:56.845,,,no_permission,,,,,,,,,,,,230913,,,Wed Aug 27 16:05:19 UTC 2014,,,,,,0|i0gr3j:,95820,brandon.williams,brandon.williams,,,,,,,,,"09/Mar/12 00:00;vijay2win@yahoo.com;I ran into this a while ago. But the problem is that we cannot change the settings on the current live clusters which uses Ec2MultiregionSnitch or Ec2Snitch without taking a downtime. (If we change EC2Snitch/Ec2Multiregion snitch, we also need to change the schema for the existing cluster).

Option 1: Leave the existing snitch as it is and add a new snitch.
Option 2: Parse for us-west-1 as us-west and parse us-west-2 as us-west2, as us-west-2 is fairly new it wont affect a lot of us?

Brandon, Thoughts?
","09/Mar/12 00:08;tnine;Could we create a new snitch that corrects the problem and deprecate the existing snitches?  This way people can migrate if they choose to, or keep the old snitches if it does not affect them.","09/Mar/12 00:24;vijay2win@yahoo.com;I am ok with deprecating it, but the problem is unless the users have a way out of this we cannot remove it.","09/Mar/12 00:26;brandon.williams;bq. If we change EC2Snitch/Ec2Multiregion snitch, we also need to change the schema for the existing cluster

Not the schema per se, but the datacenter name.  This is doable though if you're willing to repair afterwards.  Another option is to switch an entire DC's snitch at a time.

bq. Option 1: Leave the existing snitch as it is and add a new snitch.

Ugh, that will cause tremendous confusing for new users.  It would however be nice to get rid of this wart at some point.

bq. Option 2: Parse for us-west-1 as us-west and parse us-west-2 as us-west2, as us-west-2 is fairly new it wont affect a lot of us?

There aren't a lot good options here, I'm not sure how I feel about this one since it's definitely a hack, but only appending the number to the DC if > 1 might be the least painful for existing users.","09/Mar/12 03:51;vijay2win@yahoo.com;>>> This is doable though if you're willing to repair afterwards.
The problem is that StorageProxy will start to write the data to the nodes which are not suppose to have the data (During upgrade, and restart takes a while)... hence after recovery they will not be able to be recovered via repair (Lets say Node A, B, C, D if B and C are upgraded A will start to write the data to D for thinking it as this datacenters replica).

>>> it's definitely a hack, but only appending the number to the DC if > 1 might be the least painful for existing users.
Agree, and attached patch does this.

BTW: The attached patch can break after we AWS has 24 AZ's which is highly unlikely but i will create a ticket requesting for API for Regions instead of AZ.","12/Mar/12 18:16;brandon.williams;bq. hence after recovery they will not be able to be recovered via repair

One replica will always be in the right spot so you can repair.

bq. BTW: The attached patch can break after we AWS has 24 AZ's which is highly unlikely but i will create a ticket requesting for API for Regions instead of AZ.

That would be great.  Unfortunately when we have that, we'll still have to munge the name (and rack names) to be backwards compatible :(

This ticket makes me sad, but +1.","12/Mar/12 19:01;vijay2win@yahoo.com;Committed to trunk and 1.0, Thanks!","26/Aug/14 21:27;ramsperger;Not to reopen ancient tickets, but code was added in CASSANDRA-5897 to pull in SnitchProperties. Would it not be desirable to address this by looking at an additional property and choosing between legacy naming and a full, consistent with other EC2 APIs, naming scheme?

Something similar to this commit [https://github.com/ramsperger/cassandra/commit/bf1ee0251e5cf46b8af28282e22c4fbf29d85f33?w=1]?

",26/Aug/14 21:58;brandon.williams;[~ramsperger] go ahead an open a new ticket for that.,"27/Aug/14 16:05;ramsperger;Thanks [~brandon.williams], opened as CASSANDRA-7839",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent unexpected errors: possibly race condition around CQL parser?,CASSANDRA-3903,12542433,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,thepaul,thepaul,13/Feb/12 19:29,12/Mar/19 14:16,13/Mar/19 22:26,17/Feb/12 08:11,1.1.0,,,,,,0,,,,,"When running multiple simultaneous instances of the test_cql.py piece of the python-cql test suite, I can reliably reproduce intermittent and unpredictable errors in the tests.

The failures often occur at the point of keyspace creation during test setup, with a CQL statement of the form:

{code}
        CREATE KEYSPACE 'asnvzpot' WITH strategy_class = SimpleStrategy
            AND strategy_options:replication_factor = 1
    
{code}

An InvalidRequestException is returned to the cql driver, which re-raises it as a cql.ProgrammingError. The message:

{code}
ProgrammingError: Bad Request: line 2:24 no viable alternative at input 'asnvzpot'
{code}

In a few cases, Cassandra threw an ArrayIndexOutOfBoundsException and this traceback, closing the thrift connection:

{code}
ERROR [Thrift:244] 2012-02-10 15:51:46,815 CustomTThreadPoolServer.java (line 205) Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: 7
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1520)
        at org.apache.cassandra.thrift.ThriftValidation.validateCfDef(ThriftValidation.java:634)
        at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:744)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:898)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1245)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3458)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3446)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}

Sometimes I see an ArrayOutOfBoundsError with no traceback:

{code}
ERROR [Thrift:858] 2012-02-13 12:04:01,537 CustomTThreadPoolServer.java (line 205) Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException
{code}

Sometimes I get this:

{code}
ERROR [MigrationStage:1] 2012-02-13 12:04:46,077 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.IllegalArgumentException: value already present: 1558
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:115)
        at com.google.common.collect.AbstractBiMap.putInBothMaps(AbstractBiMap.java:111)
        at com.google.common.collect.AbstractBiMap.put(AbstractBiMap.java:96)
        at com.google.common.collect.HashBiMap.put(HashBiMap.java:84)
        at org.apache.cassandra.config.Schema.load(Schema.java:392)
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:284)
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:209)
        at org.apache.cassandra.db.migration.AddColumnFamily.applyImpl(AddColumnFamily.java:49)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:66)
        at org.apache.cassandra.cql.QueryProcessor$1.call(QueryProcessor.java:334)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

Again, around 99% of the instances of this {{CREATE KEYSPACE}} statement work fine, so it's a little hard to git bisect out, but I guess I'll see what I can do.","Mac OS X 10.7 with Sun/Oracle Java 1.6.0_29
Debian GNU/Linux 6.0.3 (squeeze) with Sun/Oracle Java 1.6.0_26

several recent commits on cassandra-1.1 branch. at least:

0183dc0b36e684082832de43a21b3dc0a9716d48, 3eefbac133c838db46faa6a91ba1f114192557ae, 9a842c7b317e6f1e6e156ccb531e34bb769c979f

Running cassandra under ccm with one node",,,,,,,,,,,,,,,14/Feb/12 11:45;slebresne;0001-Fix-CFS.all-thread-safety.patch;https://issues.apache.org/jira/secure/attachment/12514483/0001-Fix-CFS.all-thread-safety.patch,14/Feb/12 11:45;slebresne;0002-Fix-fixCFMaxId.patch;https://issues.apache.org/jira/secure/attachment/12514484/0002-Fix-fixCFMaxId.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-14 11:45:31.575,,,no_permission,,,,,,,,,,,,227719,,,Fri Feb 17 08:11:31 UTC 2012,,,,,,0|i0gpmf:,95581,thepaul,thepaul,,,,,,,,,"13/Feb/12 19:36;thepaul;I should mention that I adjusted the python-cql tests to be able to run cleanly in parallel, in the parallel-tests branch.","14/Feb/12 11:45;slebresne;For
{noformat}
java.lang.ArrayIndexOutOfBoundsException: 7
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1520)
        at org.apache.cassandra.thrift.ThriftValidation.validateCfDef(ThriftValidation.java:634)
{noformat}
that's because CFS.all() is not threadSafe as a new keyspace can be added between the allocation of the array and the addition of the column family stores. Attaching patch that use an ArrayList instead of a plain array so that it can grow when that happens (It still use the same ""estimate"" for the initial size of the ArrayList as 99% of the time this will be the right size, but the point is that it doesn't crash if there is a concurrent modification).

For
{noformat}
java.lang.IllegalArgumentException: value already present: 1558
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:115)
        at com.google.common.collect.AbstractBiMap.putInBothMaps(AbstractBiMap.java:111)
        at com.google.common.collect.AbstractBiMap.put(AbstractBiMap.java:96)
        at com.google.common.collect.HashBiMap.put(HashBiMap.java:84)
        at org.apache.cassandra.config.Schema.load(Schema.java:392)
{noformat}
I believe that's because Schema.fixCFMaxId() may reset cfIdGen to a smaller value since it doesn't check the current value of cfIdGen. Patch attached for that too.

Not sure what's wrong with the ArrayOutOfBoundsError without stacktrace though.

I'm also not sure at all that this will fix the 'no viable alternative at input' error, as I don't think any of those error should trigger that (if only because both happens after the parsing). Not sure how we could have a race in the parser actually, since in theory the parsing of each request should be fully mono-threaded.
","15/Feb/12 22:32;thepaul;I can't reproduce any of the failure modes above with these patches applied-- not even the ""no viable alternative"" one, so they must help.

In reviewing the patches, though, I don't think I understand how 0001-Fix-CFS.all-thread-safety.patch helps anything. Isn't that array wholly made and manipulated on the stack?","16/Feb/12 07:39;slebresne;bq. Isn't that array wholly made and manipulated on the stack?

It is, the ""thread safety"" I'm talking about about is that a new keyspace can be added while CFS.all() is running. If so, there can be say 6 keyspace when the array is created, but on the next line, when Table.all() is called, it could return 7 entries.","16/Feb/12 18:31;thepaul;Ohhhh, I see. Derp. Thanks for the explanation.

I +1 these changes.","17/Feb/12 08:11;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig can't store some types after loading them,CASSANDRA-3886,12542147,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,10/Feb/12 15:59,12/Mar/19 14:16,13/Mar/19 22:26,13/Feb/12 20:55,1.0.8,,,,,,0,,,,,"In CASSANDRA-2810, we removed the decompose methods in putNext instead relying on objToBB, however it cannot sufficiently handle all types.  For instance, if longs are loaded and then an attempt to store them is made, this causes a cast exception: java.io.IOException: java.io.IOException: java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.pig.data.DataByteArray Output must be (key, {(column,value)...}) for ColumnFamily or (key, {supercolumn:{(column,value)...}...}) for SuperColumnFamily
",,,,,,,,,,,,,,,,10/Feb/12 16:35;brandon.williams;3886.txt;https://issues.apache.org/jira/secure/attachment/12514114/3886.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-10 17:47:25.312,,,no_permission,,,,,,,,,,,,227434,,,Mon Feb 13 20:55:49 UTC 2012,,,,,,0|i0gpfb:,95549,xedin,xedin,,,,,,,,,"10/Feb/12 16:17;brandon.williams;Patch to convert all possible types.  Includes UUID, even though setTupleValue should make it impossible for a UUID to get here, just in case.","10/Feb/12 17:47;xedin;+1, with following nit: I think we should make objToBB method uniform and return in each case e.g.

{noformat}
private ByteBuffer objToBB(Object o)
{
    if (o == null)
        return (ByteBuffer)o;
    if (o instanceof java.lang.String)
        return new DataByteArray((String)o);
    if (o instanceof Integer)
        return IntegerType.instance.decompose((BigInteger)o);
    if (o instanceof Long)
        return LongType.instance.decompose((Long)o);
    if (o instanceof Float)
        return FloatType.instance.decompose((Float)o);
    if (o instanceof Double)
        return DoubleType.instance.decompose((Double)o);
    if (o instanceof UUID)
        return ByteBuffer.wrap(UUIDGen.decompose((UUID) o));
    
    return null;
}
{noformat}",10/Feb/12 18:10;brandon.williams;Committed w/nit fixed.,"13/Feb/12 20:51;brandon.williams;We actually do need the catch-all:

{noformat}
return ByteBuffer.wrap(((DataByteArray) o).get());
{noformat}

To cast all the pig-native types like CharArray, but these are all guaranteed to be castable to DataByteArray.",13/Feb/12 20:52;xedin;+1,13/Feb/12 20:55;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
index scan uses incorrect comparator on non-indexed expressions,CASSANDRA-2347,12501647,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,rjtg,jbellis,jbellis,17/Mar/11 03:35,12/Mar/19 14:16,13/Mar/19 22:26,18/Mar/11 02:55,0.7.5,,,,,,0,,,,,"When multiple index expressions are specified, the column name comparator is used when evaluating secondary (non-indexed) expressions after an indexed expression match.",,,,,,,,,,,,,,,,17/Mar/11 03:51;jbellis;2347.txt;https://issues.apache.org/jira/secure/attachment/12473878/2347.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-18 04:06:55.308,,,no_permission,,,,,,,,,,,,20571,,,Wed Mar 30 18:54:25 UTC 2011,,,,,,0|i0gaun:,93188,jbellis,jbellis,,,,,,,,,17/Mar/11 03:51;jbellis;patch w/ failing test + fix,"18/Mar/11 02:55;jbellis;committed (original fix by Roland Gude, not sure if he has a Jira account)","18/Mar/11 04:06;hudson;Integrated in Cassandra-0.7 #391 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/391/])
    fix comparator used for non-indexed secondary expressions inindex scan
patch by Roland Gude and jbellis for CASSANDRA-2347
",30/Mar/11 08:57;rjtg;i just created an account. just in case it is needed and maybe i can contribute more in the future.,30/Mar/11 18:54;jbellis;Great!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair transfers more data than necessary,CASSANDRA-2324,12501390,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,14/Mar/11 19:43,12/Mar/19 14:16,13/Mar/19 22:26,10/Apr/11 18:16,0.8 beta 1,,,,,,2,,,,,"To repro: 3 node cluster, stress.java 1M rows with -x KEYS and -l 2.  The index is enough to make some mutations drop (about 20-30k total in my tests).  Repair afterwards will repair a large amount of ranges the first time.  However, each subsequent run will repair the same set of small ranges every time.  INDEXED_RANGE_SLICE in stress never fully works.  Counting rows with sstablekeys shows there are 2M rows total as expected, however when trying to count the indexed keys, I get exceptions like:
{noformat}
Exception in thread ""main"" java.io.IOException: Key out of order! DecoratedKey(101571366040797913119296586470838356016, 0707ab782c5b5029d28a5e6d508ef72f0222528b5e28da3b7787492679dc51b96f868e0746073e54bc173be927049d0f51e25a6a95b3268213b8969abf40cea7d7) > DecoratedKey(12639574763031545147067490818595764132, 0bc414be3093348a2ad389ed28f18f0cc9a044b2e98587848a0d289dae13ed0ad479c74654900eeffc6236)
        at org.apache.cassandra.tools.SSTableExport.enumeratekeys(SSTableExport.java:206)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:388)
{noformat}",,,,,,,,,,,,,,,,08/Apr/11 01:29;slebresne;0001-Make-repair-operate-over-a-node-token-range-v2.patch;https://issues.apache.org/jira/secure/attachment/12475769/0001-Make-repair-operate-over-a-node-token-range-v2.patch,01/Apr/11 15:00;slebresne;0001-Make-repair-operate-over-a-node-token-range.patch;https://issues.apache.org/jira/secure/attachment/12475228/0001-Make-repair-operate-over-a-node-token-range.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-14 19:44:51.302,,,no_permission,,,,,,,,,,,,20562,,,Sun Apr 10 18:37:30 UTC 2011,,,,,,0|i0gapj:,93165,stuhood,stuhood,,,,,,,,,14/Mar/11 19:44;jbellis;Key out of order is because sstableexport doesn't know that index sstables use LocalPartitioner instead of the cluster partitioner RP or BOPP.,"14/Mar/11 21:24;brandon.williams;It looks like INDEXED_RANGE_SLICE is broken in stress.java, so the only problem here is repair doing superfluous work.","28/Mar/11 12:57;slebresne;The problem is, the ranges repair hashes are not actual node ranges.

Let's consider the following ring (RF=2), where I consider token being in [0..12] to simplify, and where everything is consistent:
{noformat}
                  _.-""""""""-._
 C (token: 11)  .'          `.
 [11,3][3,7]   /              \
              |                |
              |                | A(token: 3)
              |                | [3,7],[7,11]
               \              /
                `._        _.'
       B (token: 7)`-....-'
       [7,11],[11,3]
{noformat}
Now say I run a repair on node A. The problem is that the Merkle tree ranges are built by dividing the full range by 2 recursively. This means that in this example, the ranges in the tree will for instance be [0,2], [2, 4], [4, 6], [6, 8], [8,10] and [10,12].

If you compare the hashes for A and B on those ranges, changes are you'll find mismatches for [6,8] and [10,12] (because A don't have anyone on [11, 12] while B have, and B don't have anyone on [6, 7] while A have). As a consequence, the range [7,8] and [10,11] will be repaired, even though there is no inconsistencies.

What that means in practice is that it will be very rare for anti-antropy to actually consider the nodes in sync, it will almost surely ""repair"" something, even if the nodes are perfectly consistent. It's Very easy to check btw: with a cluster right the one above (3 nodes, RF=2), with as few as 5 keys for the whole cluster I'm able to have a repair do repairs over and over again.

Now the good question is: how bad is it ? I'm not sure, I depends a bit.

On a 3 nodes cluster (RF=2), I tried inserting 1M keys with stress (stress -l 2) and triggered repair afterwards. The amount of (unnecessarily) repaired keys was around 150 keys for a given node (it varies slightly for run to run because there is some randomness in the creation of the Merkle tree), corresponding to ~44KB streamed (that is the amount transfered to the node where repair has been ran, so for the total operation its twice this, since we stream in both ways). That's ~0.02% of keys (a given node have ~666 666 keys).  It's bad to do useless work, but not a really big deal.

However, the less keys we'll have, the worst it gets (and the bigger our rows are, the more useless transfer we do). With the same experiment inserting only 10K keys, there is 190 keys uselessly repaired. That's now close to 3% of the load. It also gets worst with increasing replication factor.


To fix this, we would need for the range in the Merkle tree to ""share"" the node range boundaries. An interesting way to do this would be to have the coordinating node give a list a range for which to calculate Merkle trees, and the node would compute one tree by range (for the coordinating node, that would be #RF's tree). A nice think with this is that it would leave room to optimizing repair since a node would need to do a validation compaction only on the range asked for, which means that only the coordinator node would validate all its data. The neighbors would do less work.
","28/Mar/11 13:44;jbellis;bq. To fix this, we would need for the range in the Merkle tree to ""share"" the node range boundaries

couldn't we just take the interesection of the computed ranges w/ the range actually being repaired? ","28/Mar/11 13:58;slebresne;bq. couldn't we just take the interesection of the computed ranges w/ the range actually being repaired?

We do that. But the problem is: you're node A and you receive a merkle tree from B that in particular says that for the range [0..10] the hash is x. And on [0..10] your has is x'. The problem is when [0..10] is partly one of your range, partly not. For instance it can be that you're a replica for [8..10] but not at all for [0..8].
This is due to the fact that the ranges for which the hashes are computed are computed without concern for actual node ranges. So now you know there is some inconsistency on [0..10] but it may just be that B is responsible for [0..8] and have data for it (and we don't since we are not in charge of that).
In that case, the code do take the intersection of [0..10] with the local range and will stream only [8..10]. But it's still useless.","28/Mar/11 14:14;jbellis;I thought repair is per-token-range, i.e., if I say ""nodetool repair A"" then range (11, 3] and (3, 7] will be repaired independently.","28/Mar/11 14:34;slebresne;No, not if I read this code correctly (but I think it should, that's roughly what I'm proposing to do).

Actually thinking about it, there is probably no need to construct multiple merkle trees, it will be enough for neighbors to only add to the tree the keys that are in the range of the node asking for the tree.","28/Mar/11 14:47;jbellis;So what about this:

- change the atom of repair (in nodetool + StorageService) to be a single token range, so it's unambiguous what we're repairing.  This has the side benefit of making it enormously easier to repair an entire cluster w/o doing redundant work.
- provide backwards compatibility w/ existing repair command by splitting it into RF repair ranges and waiting on each of those futures in StorageService mbean
","28/Mar/11 15:00;slebresne;Sounds good, will do.","01/Apr/11 15:00;slebresne;Attached patch modify repair to operate on one token range at a time. Nodetool repair schedule as many repair session than the node have ranges to perform a full node repair. Note that this is more efficient than previously, since the neighbors of the node will only do a validation compaction on the range they have in common with the node coordinating the repair (instead of validating everything).

This moreover makes it trivial to add an option to nodetool so that the node only repair it's primary range. That way, you can repair a full cluster by calling this operation on every node and there is no duplication of work. The patch doesn't add this option yet though.

The patch is against trunk. Because the way we construct the merkleTree is fundamentally different, the trees created by 0.7 cannot be compared to the ones created with this patch. The strategy this patch adopts with respect to talking to 0.7 nodes is this:
  * If a 0.7 node asks for a merkleTree, since we are still able to do a full compaction validation, we do it and answer with that.
  * Since a 0.7 node cannot do a merkleTree that would be ok for us, we simply exclude 0.7 nodes from the endpoints we ask merkleTree from.

I don't feel this is a trivially enough patch to go to the 0.7 branch.","01/Apr/11 22:24;stuhood;This change definitely makes sense: thanks for tackling it. The original implementation was intended to take advantage of naturally occurring compactions: I would still like to get in a position where that is possible, but living with the existing implementation until then isn't worth it.

From a quick skim: forceTableRepair incorrectly reports that the session has failed if the client thread dies: the repair will continue in the background (or used to).",01/Apr/11 23:03;stuhood;I'll give this a more complete review over the weekend.,"07/Apr/11 08:22;stuhood;* SSTableBoundScanner might be much simpler if it iterates within a list of file offsets, as returned by SSTableReader.getPositionsForRanges
* SSTableReader.getKeySamples could perform two binary searches for min and max rather than doing sequential comparisons to the keys

Thanks again Sylvain: this is great!","08/Apr/11 01:29;slebresne;bq. SSTableBoundScanner might be much simpler if it iterates within a list of file offsets, as returned by SSTableReader.getPositionsForRanges

Good call, that's much simpler. Thanks.

bq. SSTableReader.getKeySamples could perform two binary searches for min and max rather than doing sequential comparisons to the keys

Yeah, realized getKeySamples was buggy anyway since it wasn't handling wrapping ranges correctly.

Attaching patch that simplify the bounded scanner and fixes getKeySamples.
","08/Apr/11 04:45;stuhood;+1
Thanks!",10/Apr/11 18:16;slebresne;Committed as r1090840. Thanks.,"10/Apr/11 18:37;hudson;Integrated in Cassandra #844 (See [https://hudson.apache.org/hudson/job/Cassandra/844/])
    Make repair work on a token range instead of the full ring
patch by slebresne; reviewed by stuhood for CASSANDRA-2324
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary index and index expression problems,CASSANDRA-2406,12502972,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,muga_nishizawa,muga_nishizawa,31/Mar/11 03:18,12/Mar/19 14:16,13/Mar/19 22:26,18/Apr/11 13:28,0.7.5,,,Feature/2i Index,,,0,,,,,"When I iteratively get data with secondary index and index clause, result of data acquired by consistency level ""one"" is different from the one by consistency level ""quorum"".  The one by consistecy level ""one"" is correct result.  But the one by consistecy level ""quorum"" is incorrect and is dropped by Cassandra.  

You can reproduce the bug by executing attached programs.

- 1. Start Cassandra cluster.  It consists of 3 cassandra nodes and distributes data by ByteOrderedPartitioner.  Initial tokens of those nodes are [""31"", ""32"", ""33""].  
- 2. Create keyspace and column family, according to ""create_table.cli"",
- 3. Execute ""secondary_index_insertv2.py"", inserting a few hundred columns to cluster
- 4. Execute ""secondary_index_checkv2.py"" and get data with secondary index and index clause iteratively.  ""secondary_index_insertv2.py"" and ""secondary_index_checkv2.py"" require pycassa.

You will be able to execute  4th ""secondary_index_checkv2.py"" script with following option so that 
you get data with consistency level ""one"".  

% python ""secondary_index_checkv2.py"" -one

On the other hand, to acquire data with consistency level ""quorum"", you will need to use following option.  

% python ""secondary_index_checkv2.py"" -quorum

You can check that result of data acquired by consistency level ""one"" is different from one by consistency level ""quorum"".  ","CentOS 5.5 (64bit), JDK 1.6.0_23",,,,,,,,,,,,,,,07/Apr/11 13:20;skamio;CASSANDRA-2406-debug.patch;https://issues.apache.org/jira/secure/attachment/12475709/CASSANDRA-2406-debug.patch,13/Apr/11 13:20;xedin;CASSANDRA-2406.patch;https://issues.apache.org/jira/secure/attachment/12476234/CASSANDRA-2406.patch,31/Mar/11 03:19;muga_nishizawa;create_table.cli;https://issues.apache.org/jira/secure/attachment/12475053/create_table.cli,08/Apr/11 02:52;skamio;node-1.system.log;https://issues.apache.org/jira/secure/attachment/12475775/node-1.system.log,31/Mar/11 03:20;muga_nishizawa;secondary_index_checkv2.py;https://issues.apache.org/jira/secure/attachment/12475055/secondary_index_checkv2.py,31/Mar/11 03:20;muga_nishizawa;secondary_index_insertv2.py;https://issues.apache.org/jira/secure/attachment/12475054/secondary_index_insertv2.py,,,,,,6.0,,,,,,,,,,,,,,,,,,,2011-04-07 13:20:07.397,,,no_permission,,,,,,,,,,,,20604,,,Mon Apr 18 04:51:21 UTC 2011,,,,,,0|i0gb6n:,93242,jbellis,jbellis,,,,,,,,,07/Apr/11 13:20;skamio;an experimental patch,"07/Apr/11 13:32;skamio;I've attached an experimental patch. The problem is gone with this patch. But it's inefficient when a large number of rows are requested.

The main problem was that the rows collected in ColumnFamilyStore.scan() can have duplicates. So, it returns less unique rows than requested. Then, StorageProxy.scan() asks more results from next range. That means the last returned row gets wrong.

As for inefficiency of the patch, if the rows are added in order, the uniqueness check should be done only for the last row. But I don't known if I can assume the order or not. So, please improve the patch if so.
","07/Apr/11 15:34;jbellis;Hmm. There are two ways we could get duplicate rows:

1. the ranges we iterate through overlap.  but if that were the bug, we would see it on ONE as well as QUORUM
2. the ReadCallback/RangeSliceResponseResolver object (probably the resolver) returns duplicates from incorrect merging of the quorum replies

If 2. is the problem we should fix it in callback/resolver instead of in StorageProxy.

Even with it narrowed down there the problem is not obvious to me -- each response should come back in sorted (token) order, and RSRR uses a collating + reducing iterator to merge duplicates, in theory.","08/Apr/11 02:49;skamio;I forgot to say, but 1. is right. The duplicate problem is visible on CL.ONE as well.
The above test script returns rows of ""173"", ""174"", ""174"" (duplicate), ""175"", ""176"" in the first iteration. It has duplicate.
And if you see my debug log attached, ColumnFamilyStore collects the row ""174"" twice in CL.ONE.

The only case it works correcly is when I specify start_key. In the above script, set start_key = ""173"". The result is ""173"", ""174"", ""175"", ""176"", ""177"" in the first iteration. It is correct, no duplicate.
",08/Apr/11 02:52;skamio;I've attached debug log (node-1.system.log) with various debug prints. This is debug log in running the first iteration of test script with CL.ONE. The result has duplicate.,"12/Apr/11 15:24;jbellis;Pavel, can you take a stab at figuring out why the ranges overlap?  They are not supposed to.

(I am using https://github.com/pcmanus/ccm for testing, it saves a lot of time.)","15/Apr/11 14:33;jbellis;Pavel's patch fixes a 3rd kind of bug: CFS.scan itself can return duplicate rows, even for a single node and range.  Added a unit test and committed.

Shotaro, does this fix what you are seeing?","18/Apr/11 04:51;skamio;Yes, it fixes our problem. Thanks.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"getColumnFamily() return null, which is not checked in ColumnFamilyStore.java scan() method, causing Timeout Exception in query",CASSANDRA-2401,12502672,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,karshiang,karshiang,29/Mar/11 07:15,12/Mar/19 14:16,13/Mar/19 22:26,14/May/11 15:03,0.7.6,,,,,,0,,,,,"ColumnFamilyStore.java, line near 1680, ""ColumnFamily data = getColumnFamily(new QueryFilter(dk, path, firstFilter))"", the data is returned null, causing NULL exception in ""satisfies(data, clause, primary)"" which is not captured. The callback got timeout and return a Timeout exception to Hector.

The data is empty, as I traced, I have the the columns Count as 0 in removeDeletedCF(), which return the null there. (I am new and trying to understand the logics around still). Instead of crash to NULL, could we bypass the data?

About my test:
A stress-test program to add, modify and delete data to keyspace. I have 30 threads simulate concurrent users to perform the actions above, and do a query to all rows periodically. I have Column Family with rows (as File) and columns as index (e.g. userID, fileType).

No issue on the first day of test, and stopped for 3 days. I restart the test on 4th day, 1 of the users failed to query the files (timeout exception received). Most of the users are still okay with the query.
","Hector 0.7.0-28, Cassandra 0.7.4, Windows 7, Eclipse",,,,,,,,,,,,,,,05/May/11 17:12;jbellis;2401-v2.txt;https://issues.apache.org/jira/secure/attachment/12478298/2401-v2.txt,05/May/11 17:31;jbellis;2401-v3.txt;https://issues.apache.org/jira/secure/attachment/12478303/2401-v3.txt,04/May/11 17:47;jbellis;2401.txt;https://issues.apache.org/jira/secure/attachment/12478188/2401.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-03-29 13:35:25.982,,,no_permission,,,,,,,,,,,,20603,,,Sat May 14 15:03:01 UTC 2011,,,,,,0|i0gb5r:,93238,slebresne,slebresne,,,,,,,,,29/Mar/11 13:35;jbellis;Are you querying for zero columns?,"29/Mar/11 16:19;karshiang;Hi, nope. 

It is a query for 4 columns. 

I cheked that only 1 row has this problem (no column found), out of the 948 records returned; I skipped the row with zero columns. 

In my stress-test, all rows have 4 columns; i.e. row is the file, the 4 columns (index) are like its version, modified time, type, etc. I added all the columns when added each file. The addition should be working since there is no such exception on day 1, and I start and stop the stress tests until each users have around 1500 files. Row with 0 column only found on the 4th day after I continue to run it.

I will keep picking up cassandra logics, as I have little understanding about how data loaded, stored and deleted. Any suggestion / guide on how I should go on with my study is greatly appreciated. Thank you!

Btw, for this test, I have not yet going to 2 nodes / 3 nodes. It is only a single-node cassandra runnning on my localhost.
",29/Mar/11 16:32;jbellis;Is there any data from earlier than 0.7.4?,"30/Mar/11 01:15;karshiang;Hi, 
This is a clean 0.7.4 setup, with zero data to start with. Dynamically, the keyspace schema is creted on the run, when required keyspace does not exist.","30/Mar/11 02:18;karshiang;Hi,

New finding here:
For the 0-column data, it is because it is never read from the file. As I step through the line, here it returns -1 position from org.apache.cassandra.io.sstable.SSTableReader.java::getPosition(DecoratedKey decoratedKey, Operator op), line 448 (bf.isPresent(decoratedKey.key) is returning false) - key is missing.

There seem to be a missing record which is indexed or indexed column itself not updated when the record is removed (?). 

As for the data returned with 0-column, simply because a container is always created (final ColumnFamily returnCF = ColumnFamily.create(metadata)) and returned from getTopLevelColumns even if there is no read taken.

As for this case, it causes Timeout exception to Hector when null exception thrown without captured.","03/Apr/11 18:55;rjtg;can you provide some unit tests that reproduce your error? i'd like to look into it, but i am not sure whether i understand the issue correctly.","05/Apr/11 09:42;karshiang;Hi Roland,

Sure, as we are trying to do that. In the mean time, I would like to update you more about our findings:
We built a test case on the PC with the existing DB and to produce same issue, without hector API. The test case works (able to create null exception) on the original PC. 

java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.satisfies(ColumnFamilyStore.java:1787)
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1727)
	at TestScan.main(TestScan.java:74)

line 1787: IColumn column = data.getColumn(expression.column_name); where data is NULL


Zipping the 0.7.4 cassandra data to another new PC gives the same issue, but the missing key order may slightly different, e.g. on original PC it is at 430th, on the new PC it is 431th. Both keys appears to be same though (content in ByteBuffer).
(Edited: the new PC also found the problem - which makes more sense)

We will continue to check if it is due to the ""if (column.isMarkedForDelete())"" is not working on the PC with have the null encountered. Since we checked that, both PCs have the same number of columns returned in ""scan"" method at line ""ColumnFamily indexRow = indexCFS.getColumnFamily(indexFilter);"", where ""indexRow.getColumnCount()"" both giving 1996, with some rows already deleted as tombstones. 
","05/Apr/11 09:47;karshiang;Some information i missed in update:
In the PC with NULL exception, I do a continue when found ""data"" is null, and ignore that. I will get 1040 columns returned. On the 2nd (new) PC, without the NULL exception nor additional code to bypass null data, it is getting 1040 records as well. From here, we will study more our DB to find out where it went wrong/different.",05/Apr/11 11:33;rjtg;Sounds As if the Index is still pointing to deleted entriss. ,"05/Apr/11 14:23;karshiang;hi, yes. it seems to me so. Here, we create a table ""FileMap"", in which we store columns e.g. ""content"", ""authorID"", ""Version"", ""Modified Time"", ""File Type"", etc. Among them, sorted indices are ""authorID"" (as UserIndex), ""File Type"", ""Modified Time"", and ""CassType""; where CassType means generally 'file type' here in our case. It is not used though.

{{03/30/2011  09:34 AM        11,366,878 FileMap-f-53-Data.db}}
{{03/30/2011  09:34 AM            78,496 FileMap-f-53-Filter.db}}
{{03/30/2011  09:34 AM           735,930 FileMap-f-53-Index.db}}
{{03/30/2011  09:34 AM             4,264 FileMap-f-53-Statistics.db}}
{{03/30/2011  05:37 PM             4,055 FileMap-f-54-Data.db}}
{{03/30/2011  05:37 PM                40 FileMap-f-54-Filter.db}}
{{03/30/2011  05:37 PM               270 FileMap-f-54-Index.db}}
{{03/30/2011  05:37 PM             4,264 FileMap-f-54-Statistics.db}}
{{04/04/2011  04:07 PM            24,068 FileMap-f-55-Data.db}}
{{04/04/2011  04:07 PM               200 FileMap-f-55-Filter.db}}
{{04/04/2011  04:07 PM             1,746 FileMap-f-55-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap-f-55-Statistics.db}}
{{04/04/2011  04:07 PM           961,808 FileMap.CassTypeIndex-f-53-Data.db}}
{{04/04/2011  04:07 PM             1,936 FileMap.CassTypeIndex-f-53-Filter.db}}
{{04/04/2011  04:07 PM                11 FileMap.CassTypeIndex-f-53-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap.CassTypeIndex-f-53-Statistics.db}}
{{03/29/2011  02:52 PM           961,386 FileMap.FileTypeIndex-f-50-Data.db}}
{{03/29/2011  02:52 PM             1,936 FileMap.FileTypeIndex-f-50-Filter.db}}
{{03/29/2011  02:52 PM                11 FileMap.FileTypeIndex-f-50-Index.db}}
{{03/29/2011  02:52 PM             4,264 FileMap.FileTypeIndex-f-50-Statistics.db}}
{{03/30/2011  05:37 PM               404 FileMap.FileTypeIndex-f-51-Data.db}}
{{03/30/2011  05:37 PM                16 FileMap.FileTypeIndex-f-51-Filter.db}}
{{03/30/2011  05:37 PM                11 FileMap.FileTypeIndex-f-51-Index.db}}
{{03/30/2011  05:37 PM             4,264 FileMap.FileTypeIndex-f-51-Statistics.db}}
{{04/04/2011  04:07 PM             2,358 FileMap.FileTypeIndex-f-52-Data.db}}
{{04/04/2011  04:07 PM                16 FileMap.FileTypeIndex-f-52-Filter.db}}
{{04/04/2011  04:07 PM                11 FileMap.FileTypeIndex-f-52-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap.FileTypeIndex-f-52-Statistics.db}}
{{03/29/2011  02:52 PM         3,298,947 FileMap.ModifiedIndex-f-50-Data.db}}
{{03/29/2011  02:52 PM            78,016 FileMap.ModifiedIndex-f-50-Filter.db}}
{{03/29/2011  02:52 PM           731,106 FileMap.ModifiedIndex-f-50-Index.db}}
{{03/29/2011  02:52 PM             4,264 FileMap.ModifiedIndex-f-50-Statistics.db}}
{{03/30/2011  05:37 PM             2,065 FileMap.ModifiedIndex-f-51-Data.db}}
{{03/30/2011  05:37 PM                64 FileMap.ModifiedIndex-f-51-Filter.db}}
{{03/30/2011  05:37 PM               450 FileMap.ModifiedIndex-f-51-Index.db}}
{{03/30/2011  05:37 PM             4,264 FileMap.ModifiedIndex-f-51-Statistics.db}}
{{04/04/2011  04:07 PM            13,835 FileMap.ModifiedIndex-f-52-Data.db}}
{{04/04/2011  04:07 PM               328 FileMap.ModifiedIndex-f-52-Filter.db}}
{{04/04/2011  04:07 PM             3,006 FileMap.ModifiedIndex-f-52-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap.ModifiedIndex-f-52-Statistics.db}}
{{04/04/2011  04:07 PM           962,874 FileMap.UserIndex-f-53-Data.db}}
{{04/04/2011  04:07 PM             1,936 FileMap.UserIndex-f-53-Filter.db}}
{{04/04/2011  04:07 PM               420 FileMap.UserIndex-f-53-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap.UserIndex-f-53-Statistics.db}}

In the search, we are using IndexClause as:
		ByteBuffer field_author = ByteBuffer.wrap(new byte[]{'a'});
		ByteBuffer author_1 = IntegerSerializer.get().toByteBuffer(1);
		
		ByteBuffer file_type = ByteBuffer.wrap(new byte[]{'t'});
		ByteBuffer filetype_3 = ByteBuffer.wrap(new byte[]{3}); //file type 3
		
		IndexClause indexClause = new IndexClause();
		indexClause.setCount(3000);
		ArrayList<IndexExpression> expressions = new ArrayList();
		expressions.add(new IndexExpression(field_author, IndexOperator.EQ, author_1)); //user ID = 1
		expressions.add(new IndexExpression(file_type, IndexOperator.EQ, filetype_3)); //file type = 3
		
		indexClause.setExpressions(expressions);
		indexClause.setStart_key(new byte[]{});

}}
In the search, it scans all the indices from ""FileMap.UserIndex"", within which there seems having a key (index) which is not found in the table ""FileMap""; and I roughly get that it breaks at data retrieval with ""FileMap-f-53-Data"", when the position for the key is not found / available in ""FileMap-f-53-Data"".","19/Apr/11 02:46;jbellis;So when you created the data, you did not use any expiring columns (TTL), correct?","19/Apr/11 02:52;jbellis;The more I think about it, the more I think that there is a rare race condition here -- we do a kind of row lock during updates of indexed data, but we do not lock during reads. So it's possible for an index read to say ""row X has this value"" and then have that value deleted (by another client's request) before we can read row X.

BUT that does not look like what you are seeing because if I understand correctly you are seeing that the index has permanently missed a delete operation.","19/Apr/11 03:03;karshiang;hi Jon,

sorry for less updates for past few days as we were busy on other tasks. We are thinking to stress-test with 0.7.5 when it is out.

In the test, we have all operations e.g. ""insert, replace, and delete"". If not wrong, we have simulated 20-users to run concurrently, however, they likely not able to delete key of different user. I think there is no such a case when 1 user is modifying his record, when another user deleting the record.

There is no expiring columns (TTL) in this test.

Same data on another PC will able to give the same exception, though we found the index position (n variable) can be shifted by 1 or 2.

Thanks, Jon and your team for the gd work!","19/Apr/11 03:11;karshiang;Hi Jon,

Allow me to add more information:

Each simulated user thread will do the following in repeatitive manner:

loop = 0;
while( running )
{
    if( loop % 5 ==0 ) { list all files in folder; }

    create around 4~10 files but cap the total files around 2000 files only.
    modified around 20 files;
    delete 1~4 files;

    loop ++;
}

The ""list all files in folder"" is the scan action, where it will later for 1 or 2 users giving us ""no file"" in return after the next few days when restarted the same test, without resetting data. Found out it is due to the issue above. ","19/Apr/11 13:36;rjtg;i just looked a little closer at your index expressions again.
If i understand them correctly they are subject to https://issues.apache.org/jira/browse/CASSANDRA-2347
Although i don't really think it is the issue you are describing it would be nice if you could apply the patch and see if the error still occurs.

You are creating the bytebuffers for author_id and file_type in a different way. Is this a mistake? ","20/Apr/11 03:46;karshiang;Hi Roland,

The 'mistake' is intended by reusing some Hector API code.

Hector has a Integer Serializer, which will generate 4-byte[] from given integer. The file_type is a 1-byte array. It is to produce exact effected client call into a test case, solely running cassandra. ","20/Apr/11 04:24;karshiang;for issue: https://issues.apache.org/jira/browse/CASSANDRA-2347, I suspect we encountered that in another case. It has a validation failure at times.

I applied the change. As expected, the error is still there, data is missing or indexed key extra then throw NULL exception out.","04/May/11 17:47;jbellis;I found *a* bug that could cause this: Cassandra will re-create a deleted index entry if it gets a write with an obsolete timestamp, but the data row tombstone will correctly suppress an update there. (So when you do an index query for value=X, and the index says ""row K has that value,"" then you get an error trying to read row K that doesn't exist.)

I don't think this is the bug Tey Kar is hitting, though, because unless I'm mistaken you won't get this NPE until after the data row tombstone is removed by compaction after gc_grace_seconds.  4 days isn't enough to see that unless you've tweaked gc_g_s.

Still, it's worth fixing.  Patch attached.  (Also adds an assert w/ more information if/when another way of triggering this is found.)","04/May/11 22:07;cywjackson;I have an existing data that was resulting similar NPE  before the patch. After applying the patch, the following observed:

{noformat}
DEBUG [ReadStage:82] 2011-05-04 21:23:27,114 ColumnFamilyStore.java (line 1514) fetched data row ColumnFamily(inode -deleted at 1304363600008- [70617468:false:49@1304363600219,])
DEBUG [ReadStage:82] 2011-05-04 21:23:27,114 ColumnFamilyStore.java (line 1532) row ColumnFamily(inode -deleted at 1304363600008- [70617468:false:49@1304363600219,]) satisfies all clauses
DEBUG [ReadStage:82] 2011-05-04 21:23:27,115 ColumnFamilyStore.java (line 1514) fetched data row ColumnFamily(inode [70617468:false:10@1304353355296,])
ERROR [ReadStage:82] 2011-05-04 21:23:27,115 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[ReadStage:82,5,main]
java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(29842926756667498147838693957802723793, 3134346637326336393966396130336561376538623330316566383561616131):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 73656e74696e656cEQ78
    at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1512)
    at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}

was the fix intend to avoid future problem, as such existing problem would need a workaround solution?","04/May/11 22:48;jbellis;bq. was the fix intend to avoid future problem

yes.  as discussed above, once you corrupt your index this way the corruption is recorded permanently and you need to drop the index and recreate it.","05/May/11 15:19;slebresne;Comments on the patch:
  * ignoreObsoleteMutation() now forgot to actually remove the obsolete mutation from cf.
  * not sure why mutatedIndexColumns need to be concurrent. There is no concurrency in ignoreObsoleteMutation, is there ?
  * really minor: change to debug log ""Scanning index row %s ..."" seems misleading since the first argument is not a row name.

Other than that, I do agree with you that there is quite probably a race between reads and concurrent writes. But also agree that it doesn't seem to be the problem here","05/May/11 15:54;jbellis;bq. ignoreObsoleteMutation() now forgot to actually remove the obsolete mutation from cf

this isn't actually necessary, though, since if it's taken out of the list of mutated index columns the obsolete columns will only be applied to the ""main"" data row, and including obsolete columns there is harmless.

bq. not sure why mutatedIndexColumns need to be concurrent

because we might remove from the collection while iterating over it.  treeset will throw concurrentmodificationexception.  but maybe iterator.remove would work, now that you mention it?","05/May/11 16:35;slebresne;bq. this isn't actually necessary, though, since if it's taken out of the list of mutated index columns the obsolete columns will only be applied to the ""main"" data row, and including obsolete columns there is harmless.

Very true.

bq. but maybe iterator.remove would work

I think it will","05/May/11 17:12;jbellis;v2 attached w/ iterator/Set change.

bq. change to debug log ""Scanning index row %s ..."" seems misleading since the first argument is not a row name

it actually is the same CF+row as before, I just encapsulated it in getExpressionString so I can re-use the method in case of assertion failure later. Tweaked format a bit in v2, here's an example debug output:

{noformat}
Scanning index 'world2 EQ 15' starting with
{noformat}",05/May/11 17:20;slebresne;+1 v2,"05/May/11 17:35;jbellis;Oops, that's actually column + value, not CF.

For the record, v3 adds CF:
{noformat}
Scanning index 'CF1.world2 EQ 15' starting with
{noformat}

Will commit based on v2 +1.","05/May/11 18:14;hudson;Integrated in Cassandra-0.7 #470 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/470/])
    improve ignoring of obsoletemutations in index maintenance
patch by jbellis; reviewed by slebresne for CASSANDRA-2401
","07/May/11 23:40;cywjackson;Here is 1 way that i could 100% reproduce the issue with data being null:

Need 2 nodes, 1 is gonna to autobootstrap to the other. Also assuming completely clean start (blow up the /var/lib/cassandra/ or where ever data are stored

i am also using brisk beta to test

to start:
node-A:
1) get brisk
2) start brisk  with -t (jobtracker)
3) run a simple hive query : 
 3a) bin/brisk hive 
 3b) create table foo (bar INT);
 3c) select count(*) from foo;
 3d) exit;
4) every thing should be so far so good, let the brisk node continue to be up

node-B:
1) get brisk
2) modify the resources/cassandra/conf/cassandra.yaml:
 2a) to enable autobootstrap. 
 2b) point seeds to node-A

3) put a sleep or break point in o.a.c.service.StorageService.joinTokenRing method, right after ""Map<InetAddress, Double> loadinfo = StorageLoadBalancer.instance.getLoadInfo();"" (personal preference: log a sleep line, add a thread.sleep(a_long_time))
4) start brisk with -t on node-B 
5) wait till the log line ""Joining: getting bootstrap token"" , it should now reaches your break point (or zz)
6) crash the jvm (personal preference: kill -9 <pid>)

back to node-A
1) exit the jvm (BriskDaemon) ""normally"" (kill <pid>)
2) start the brisk node again (with -t):

log from node-A: 
{noformat}
 INFO 23:25:00,213 Logging initialized
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/riptano/work/brisk/resources/cassandra/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/riptano/work/brisk/resources/hadoop/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
 INFO 23:25:00,235 Heap size: 510263296/511311872
 INFO 23:25:00,237 JNA not found. Native methods will be disabled.
 INFO 23:25:00,263 Loading settings from file:/home/riptano/work/brisk/resources/cassandra/conf/cassandra.yaml
 INFO 23:25:00,470 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 23:25:00,496 Detected Hadoop trackers are enabled, setting my DC to Brisk
 INFO 23:25:00,696 Global memtable threshold is enabled at 162MB
 INFO 23:25:00,846 Opening /var/lib/cassandra/data/system/IndexInfo-f-1
 INFO 23:25:00,912 Opening /var/lib/cassandra/data/system/Schema-f-2
 INFO 23:25:00,926 Opening /var/lib/cassandra/data/system/Schema-f-1
 INFO 23:25:00,951 Opening /var/lib/cassandra/data/system/Migrations-f-2
 INFO 23:25:00,954 Opening /var/lib/cassandra/data/system/Migrations-f-1
 INFO 23:25:00,970 Opening /var/lib/cassandra/data/system/LocationInfo-f-2
 INFO 23:25:00,989 Opening /var/lib/cassandra/data/system/LocationInfo-f-1
 INFO 23:25:01,089 Loading schema version c4fd2440-7900-11e0-0000-ba846f9adcf7
 INFO 23:25:01,499 Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1304810701499.log
 INFO 23:25:01,530 Replaying /var/lib/cassandra/commitlog/CommitLog-1304810455288.log
 INFO 23:25:01,675 Finished reading /var/lib/cassandra/commitlog/CommitLog-1304810455288.log
 INFO 23:25:01,730 Enqueuing flush of Memtable-MetaStore@102170028(869/1086 serialized/live bytes, 3 ops)
 INFO 23:25:01,735 Writing Memtable-MetaStore@102170028(869/1086 serialized/live bytes, 3 ops)
 INFO 23:25:01,743 Enqueuing flush of Memtable-sblocks@1075051425(3044096/3805120 serialized/live bytes, 17 ops)
 INFO 23:25:01,747 Enqueuing flush of Memtable-inode.path@780298059(2848/3560 serialized/live bytes, 59 ops)
 INFO 23:25:01,748 Enqueuing flush of Memtable-inode.sentinel@1934329031(2848/3560 serialized/live bytes, 59 ops)
 INFO 23:25:01,748 Enqueuing flush of Memtable-inode@1660575731(6393/7991 serialized/live bytes, 134 ops)
 INFO 23:25:01,821 Completed flushing /var/lib/cassandra/data/HiveMetaStore/MetaStore-f-1-Data.db (989 bytes)
 INFO 23:25:01,832 Writing Memtable-sblocks@1075051425(3044096/3805120 serialized/live bytes, 17 ops)
 INFO 23:25:01,927 Completed flushing /var/lib/cassandra/data/cfs/sblocks-f-1-Data.db (3045448 bytes)
 INFO 23:25:01,928 Writing Memtable-inode.path@780298059(2848/3560 serialized/live bytes, 59 ops)
 INFO 23:25:01,968 Completed flushing /var/lib/cassandra/data/cfs/inode.path-f-1-Data.db (5346 bytes)
 INFO 23:25:01,969 Writing Memtable-inode.sentinel@1934329031(2848/3560 serialized/live bytes, 59 ops)
 INFO 23:25:02,035 Completed flushing /var/lib/cassandra/data/cfs/inode.sentinel-f-1-Data.db (1735 bytes)
 INFO 23:25:02,036 Writing Memtable-inode@1660575731(6393/7991 serialized/live bytes, 134 ops)
 INFO 23:25:02,085 Completed flushing /var/lib/cassandra/data/cfs/inode-f-1-Data.db (8582 bytes)
 INFO 23:25:02,087 Log replay complete
 INFO 23:25:02,092 Cassandra version: 0.8.0-beta2-SNAPSHOT
 INFO 23:25:02,092 Thrift API version: 19.10.0
 INFO 23:25:02,092 Loading persisted ring state
 INFO 23:25:02,092 load token size: 0
 INFO 23:25:02,093 Starting up server gossip
 INFO 23:25:02,104 Enqueuing flush of Memtable-LocationInfo@22262475(29/36 serialized/live bytes, 1 ops)
 INFO 23:25:02,105 Writing Memtable-LocationInfo@22262475(29/36 serialized/live bytes, 1 ops)
 INFO 23:25:02,127 Completed flushing /var/lib/cassandra/data/system/LocationInfo-f-3-Data.db (80 bytes)
 INFO 23:25:02,149 Starting Messaging Service on port 7000
 INFO 23:25:02,172 Using saved token 152036150612811635197207268153837644139
 INFO 23:25:02,173 Enqueuing flush of Memtable-LocationInfo@1977026981(53/66 serialized/live bytes, 2 ops)
 INFO 23:25:02,174 Writing Memtable-LocationInfo@1977026981(53/66 serialized/live bytes, 2 ops)
 INFO 23:25:02,190 Completed flushing /var/lib/cassandra/data/system/LocationInfo-f-4-Data.db (163 bytes)
 INFO 23:25:02,193 Compacting Major: [SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-f-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-f-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-f-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-f-4-Data.db')]
 INFO 23:25:02,196 Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO 23:25:02,196 Starting up Hadoop trackers
 INFO 23:25:02,197 Waiting for gossip to start
 INFO 23:25:02,225 Major@1830423861(system, LocationInfo, 438/741) now compacting at 16777 bytes/ms.
 INFO 23:25:02,257 Compacted to /var/lib/cassandra/data/system/LocationInfo-tmp-f-5-Data.db.  741 to 447 (~60% of original) bytes for 3 keys.  Time: 64ms.
 INFO 23:25:07,272 Chose seed 10.179.96.212 as jobtracker
 WARN 23:25:09,331 Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties, hadoop-metrics2.properties
 INFO 23:25:09,994 Chose seed 10.179.96.212 as jobtracker
 INFO 23:25:10,139 Updating the current master key for generating delegation tokens
 INFO 23:25:10,143 Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
 INFO 23:25:10,143 Scheduler configured with (memSizeForMapSlotOnJT, memSizeForReduceSlotOnJT, limitMaxMemForMapTasks, limitMaxMemForReduceTasks) (-1, -1, -1, -1)
 INFO 23:25:10,144 Updating the current master key for generating delegation tokens
 INFO 23:25:10,145 Refreshing hosts (include/exclude) list
 INFO 23:25:10,223 Starting jobtracker with owner as riptano
 INFO 23:25:10,245 Starting SocketReader
 INFO 23:25:10,374 Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
 INFO 23:25:10,623 Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
 INFO 23:25:10,673 Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 50030
 INFO 23:25:10,673 listener.getLocalPort() returned 50030 webServer.getConnectors()[0].getLocalPort() returned 50030
 INFO 23:25:10,674 Jetty bound to port 50030
 INFO 23:25:10,674 jetty-6.1.21
 INFO 23:25:11,140 Started SelectChannelConnector@0.0.0.0:50030
 INFO 23:25:11,147 JobTracker up at: 8012
 INFO 23:25:11,147 JobTracker webserver: 50030
 WARN 23:25:11,276 Incorrect permissions on cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system. Setting it to rwx------
ERROR 23:25:11,321 Fatal exception in thread Thread[ReadStage:4,5,main]
java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(55249227080490826413412398468829851220, 3165333533353736613164333836353061346636333465656437326131353939):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 'inode.73656e74696e656c EQ 78'
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1513)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:46)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 23:25:20,059 Deleted /var/lib/cassandra/data/system/LocationInfo-f-3
 INFO 23:25:20,060 Deleted /var/lib/cassandra/data/system/LocationInfo-f-4
 INFO 23:25:20,576 Deleted /var/lib/cassandra/data/system/LocationInfo-f-1
 INFO 23:25:20,577 Deleted /var/lib/cassandra/data/system/LocationInfo-f-2
 INFO 23:25:21,297 problem cleaning system directory: cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system
java.io.IOException: TimedOutException()
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths(CassandraFileSystemThriftStore.java:523)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listSubPaths(CassandraFileSystemThriftStore.java:529)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystem.listStatus(CassandraFileSystem.java:171)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2374)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2174)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:303)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:294)
        at org.apache.cassandra.hadoop.trackers.TrackerInitializer$1.run(TrackerInitializer.java:93)
        at java.lang.Thread.run(Thread.java:662)
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.CassandraServer.get_indexed_slices(CassandraServer.java:673)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths(CassandraFileSystemThriftStore.java:506)
        ... 8 more
 WARN 23:25:31,300 Incorrect permissions on cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system. Setting it to rwx------
ERROR 23:25:31,315 Fatal exception in thread Thread[ReadStage:6,5,main]
java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(55249227080490826413412398468829851220, 3165333533353736613164333836353061346636333465656437326131353939):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 'inode.73656e74696e656c EQ 78'
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1513)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:46)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 23:25:41,303 problem cleaning system directory: cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system
java.io.IOException: TimedOutException()
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths(CassandraFileSystemThriftStore.java:523)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listSubPaths(CassandraFileSystemThriftStore.java:529)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystem.listStatus(CassandraFileSystem.java:171)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2374)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2174)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:303)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:294)
        at org.apache.cassandra.hadoop.trackers.TrackerInitializer$1.run(TrackerInitializer.java:93)
        at java.lang.Thread.run(Thread.java:662)
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.CassandraServer.get_indexed_slices(CassandraServer.java:673)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths(CassandraFileSystemThriftStore.java:506)
        ... 8 more
 WARN 23:25:51,308 Incorrect permissions on cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system. Setting it to rwx------
ERROR 23:25:51,321 Fatal exception in thread Thread[ReadStage:8,5,main]
java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(55249227080490826413412398468829851220, 3165333533353736613164333836353061346636333465656437326131353939):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 'inode.73656e74696e656c EQ 78'
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1513)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:46)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

{noformat}

","08/May/11 02:36;jbellis;kill -9 w/o the bootstrap is not sufficient to cause the problem?

If you allow the bootstrap to finish does it work correctly if you kill -9 node A?

Bootstrap shouldn't cause anything to be written to node A (except the presence of a new node, to system table) so I'm inclined to think the kill -9 of A is the important part.","08/May/11 02:41;jbellis;bq. Bootstrap shouldn't cause anything to be written to node A

Hmm, but it does cause A to flush. I wonder if that's the connection.

Can you try with invoking nodetool flush against A, instead of doing a bootstrap?","08/May/11 03:15;jbellis;Another thing to try: after kill -9 of A but before restarting it, remove the commitlog *header* files (just the header ones). This should force full CL replay on restart.","12/May/11 17:54;slebresne;From irc:
{noformat}
pcmanus : jbellis: do you know what's up with #2401 ?
jbellis : jackson can't reproduce anymore either, but he wants to test more before calling it fixed
{noformat}
So I'm going to mark this resolved as this fixed a legit bug and I don't want to push it 0.7.7.
If there is still related problems, let's open another ticket.","13/May/11 21:53;thobbs;With these changes, using a count of 0 in the SlicePredicate produces the following AssertionError (and a TimedOutExc for the client):

{noformat}
ERROR 16:13:38,864 Fatal exception in thread Thread[ReadStage:16,5,main]
java.lang.AssertionError: No data found for SliceQueryFilter(start=java.nio.HeapByteBuffer[pos=10 lim=10 cap=30], finish=java.nio.HeapByteBuffer[pos=17 lim=17 cap=30], reversed=false, count=0] in DecoratedKey(81509516161424251288255223397843705139, 6b657931):QueryPath(columnFamilyName='cf', superColumnName='null', columnName='null') (original filter SliceQueryFilter(start=java.nio.HeapByteBuffer[pos=10 lim=10 cap=30], finish=java.nio.HeapByteBuffer[pos=17 lim=17 cap=30], reversed=false, count=0]) from expression 'cf.626972746864617465 EQ 1'
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1517)
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

This was during a get_indexed_slices().","14/May/11 15:03;jbellis;Created CASSANDRA-2653 to address this, since it will probably be in a different release than the original 2401 fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error running cqlsh from .tar file -- global name 'SchemaDisagreementException' is not defined,CASSANDRA-2501,12504612,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,cdaw,cdaw,18/Apr/11 22:51,12/Mar/19 14:16,13/Mar/19 22:26,19/Apr/11 13:33,0.8 beta 1,,,,,,0,cql,,,,"*Error when running cqlsh*
{code}
[cassandra@cdaw-qa1 cql-1.0.0]$ cqlsh cdaw-qa1
Traceback (most recent call last):
  File ""/usr/bin/cqlsh"", line 212, in <module>
    password=options.password)
  File ""/usr/bin/cqlsh"", line 55, in __init__
    self.conn = cql.connect(hostname, port, user=username, password=password)
  File ""/usr/lib/python2.6/site-packages/cql/__init__.py"", line 51, in connect
    return connection.Connection(host, port, keyspace, user, password)
  File ""/usr/lib/python2.6/site-packages/cql/connection.py"", line 53, in __init__
    c.execute('USE %s;' % keyspace)
  File ""/usr/lib/python2.6/site-packages/cql/cursor.py"", line 126, in execute
    except SchemaDisagreementException, sde:
NameError: global name 'SchemaDisagreementException' is not defined
{code}


*Build*
* Install the cassandra binary from the nightly build
wget https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/apache-cassandra-2011-04-18_11-02-29-bin.tar.gz

* Install cql from .tar file on nightly build
wget https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/cql-1.0.0.tar.gz

*CQL Install Output*
{code}
[cassandra@cdaw-qa1 cql-1.0.0]$ sudo python2.6 ./setup.py install
[sudo] password for cassandra: 
running install
running build
running build_py
running build_scripts
creating build/scripts-2.6
copying and adjusting cqlsh -> build/scripts-2.6
changing mode of build/scripts-2.6/cqlsh from 644 to 755
running install_lib
creating /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/results.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/marshal.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/connection.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/cursor.py -> /usr/lib/python2.6/site-packages/cql
creating /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/__init__.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/Cassandra.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/constants.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/ttypes.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/decoders.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/__init__.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/errors.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/connection_pool.py -> /usr/lib/python2.6/site-packages/cql
byte-compiling /usr/lib/python2.6/site-packages/cql/results.py to results.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/marshal.py to marshal.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/connection.py to connection.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cursor.py to cursor.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/__init__.py to __init__.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/Cassandra.py to Cassandra.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/constants.py to constants.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/ttypes.py to ttypes.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/decoders.py to decoders.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/__init__.py to __init__.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/errors.py to errors.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/connection_pool.py to connection_pool.pyc
running install_scripts
copying build/scripts-2.6/cqlsh -> /usr/bin
changing mode of /usr/bin/cqlsh to 755
running install_egg_info
Writing /usr/lib/python2.6/site-packages/cql-1.0.0-py2.6.egg-info

{code}
","Running on 3-node Centos 5.5. The cql package was installed with Python 2.6 and prior to installation, I downloaded and installed thrift05-0.5.0.",,,,,,,,,,,,,,,18/Apr/11 23:18;jbellis;2501.txt;https://issues.apache.org/jira/secure/attachment/12476664/2501.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-18 23:18:20.247,,,no_permission,,,,,,,,,,,,20658,,,Tue Apr 19 14:16:54 UTC 2011,,,,,,0|i0gbqf:,93331,cdaw,cdaw,,,,,,,,,"18/Apr/11 23:18;jbellis;i think the SDE error is masking another problem -- try with this patch (""ant release"" to build the same artifacts hudson does) and see if that exposes something else.","19/Apr/11 02:05;cdaw;The error still occurs after running ""ant release"".  I also upgraded to thrift 6.0.0 and that didn't resolve the issue either.",19/Apr/11 02:12;jbellis;just to doublecheck: you applied the patch (patch -p0 < 2501.txt) before running ant release?,"19/Apr/11 02:41;cdaw;Sorry about that ... applied the patch and got a new error:

{code}

[cassandra@cdaw-qa1 cql-1.0.0]$ cqlsh cdaw-qa1
Traceback (most recent call last):
  File ""/usr/bin/cqlsh"", line 212, in <module>
    password=options.password)
  File ""/usr/bin/cqlsh"", line 55, in __init__
    self.conn = cql.connect(hostname, port, user=username, password=password)
  File ""/usr/lib/python2.6/site-packages/cql/__init__.py"", line 51, in connect
    return connection.Connection(host, port, keyspace, user, password)
  File ""/usr/lib/python2.6/site-packages/cql/connection.py"", line 53, in __init__
    c.execute('USE %s;' % keyspace)
  File ""/usr/lib/python2.6/site-packages/cql/cursor.py"", line 133, in execute
    raise cql.InternalError(""Internal application error"")
cql.InternalError: Internal application error
{code}","19/Apr/11 02:56;jbellis;Good, that's what I thought.

Cassandra always logs a stacktrace for ""internal application error,"" can you grab that from /var/log/cassandra?

(if i were to take a wild-ass guess I would say it's not handling USE correctly when given a keyspace that doesn't exist.)","19/Apr/11 05:08;cdaw;User Error.  I noticed in my log file that the Cassandra version was 0.74 but I was running from the trunk.  I had put $CASSANDRA_HOME in my .bashrc file, but when I removed it, everything was fine.

I would normally resolve this as Will Not Fix, but not sure about the patch you provided, and if you want to associate this with that.",19/Apr/11 13:33;jbellis;Will mark Fixed since the missing SchemaDisagreementException import was a real bug.,19/Apr/11 13:34;jbellis;(committed in r1095082),"19/Apr/11 14:16;hudson;Integrated in Cassandra-0.8 #21 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/21/])
    add SchemaDisagreementException import
patch by jbellis; tested by Cathy Daw for CASSANDRA-2501
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python CQL driver does not decode most values,CASSANDRA-2507,12504681,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,urandom,urandom,19/Apr/11 16:33,12/Mar/19 14:16,13/Mar/19 22:26,19/Apr/11 20:02,0.8 beta 1,,,Legacy/CQL,,,0,cql,,,,"Most keys, and column name/values are not decoded properly.  The attached CQL input can be used to demonstrate:

_Note: requires the patch from CASSANDRA-2505 to be applied_

{noformat}
$ drivers/py/cqlsh localhost 9170 < repro.cql 
 | '\x00\x00\x00\x00\x00\x00\x00\x01','\x00\x00\x00\x00\x00\x00\x00\x01' | '\x00\x00\x00\x00\x00\x00\x00\x02','\x00\x00\x00\x00\x00\x00\x00\x02'
e�#j������ | 'e\xe2#\x01j\xa2\x11\xe0\x00\x00\xfe\x8e\xbe\xea\xd9\xff','e\xe2#\x02j\xa2\x11\xe0\x00\x00\xfe\x8e\xbe\xea\xd9\xff'
{noformat}

For all practical purposes, this renders the driver useless for everything but strings.",,,,,,,,,,,,,,,,19/Apr/11 19:34;thobbs;2507.txt;https://issues.apache.org/jira/secure/attachment/12476762/2507.txt,19/Apr/11 17:34;urandom;repro.cql;https://issues.apache.org/jira/secure/attachment/12476749/repro.cql,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-19 19:34:42.418,,,no_permission,,,,,,,,,,,,20661,,,Tue Apr 19 21:28:03 UTC 2011,,,,,,0|i0gbrr:,93337,jbellis,jbellis,,,,,,,,,"19/Apr/11 19:34;thobbs;Attached patch fixes the issue by:

1. Not creating a new cursor for every line of CQL.
2. Updating the driver's view of the schema after DDL statements instead of before them.",19/Apr/11 20:02;jbellis;committed,"19/Apr/11 21:28;hudson;Integrated in Cassandra-0.8 #23 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/23/])
    cqlsh fixes
patch by thobbs; reviewed by jbellis for CASSANDRA-2507
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update py_test to use strategy_options,CASSANDRA-2509,12504708,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,rwjblue,rwjblue,rwjblue,19/Apr/11 20:19,12/Mar/19 14:16,13/Mar/19 22:26,19/Apr/11 20:26,0.8 beta 1,,,Legacy/Tools,,,0,,,,,"KsDef was changed in cassandra.thrift to accept a hash of options as strategy_options.  py_test/stress.py needs to be updated with the new method arguments.

CASSANDRA-1263 changed the parameters to KsDef.
CASSANDRA-2462 fixed this issue in the native Java stress package.",,,,,,,,,,,,,,,,19/Apr/11 20:21;rwjblue;0001-Update-py_stress-to-make-replication_factor-part-of-.patch;https://issues.apache.org/jira/secure/attachment/12476772/0001-Update-py_stress-to-make-replication_factor-part-of-.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-19 20:26:25.265,,,no_permission,,,,,,,,,,,,20663,,,Tue Apr 19 21:28:03 UTC 2011,,,,,,0|i0gbs7:,93339,jbellis,jbellis,,,,,,,,,19/Apr/11 20:21;rwjblue;Attached patch fixes issue with py_test.,19/Apr/11 20:25;rwjblue;Patch uploaded.,"19/Apr/11 20:26;jbellis;committed, thanks!","19/Apr/11 21:28;hudson;Integrated in Cassandra-0.8 #23 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/23/])
    update stress.py for KsDef replication_factor change
patch by Robert Jackson; reviewed by jbellis for CASSANDRA-2509
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missing imports in CQL Python driver,CASSANDRA-2508,12504694,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,urandom,urandom,19/Apr/11 17:44,12/Mar/19 14:16,13/Mar/19 22:26,25/Apr/11 17:56,0.8.0 beta 2,,,Legacy/Tools,,,0,cql,,,,"Try:

bq. cd drivers/py && python -c 'from cql import DateFromTicks; DateFromTicks(1)'

Also:
{{cql.connection}} is missing an import of {{AuthenticationRequest}} from {{ttypes}}, and the exceptions {{NotSupportedError}}, and {{InternalError}}.

Also:
{{marshal.unmarshal_long}} has a NameError waiting to happen in the form of ""unpack""",,,,,,,,,,,,,,,,19/Apr/11 20:13;jbellis;2508.txt;https://issues.apache.org/jira/secure/attachment/12476770/2508.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-19 20:13:36.009,,,no_permission,,,,,,,,,,,,20662,,,Tue Apr 19 21:28:03 UTC 2011,,,,,,0|i0gbrz:,93338,thobbs,thobbs,,,,,,,,,"19/Apr/11 20:13;jbellis;bq. python -c 'from cql import DateFromTicks; DateFromTicks(1)'

fixed

bq. cql.connection is missing an import of AuthenticationRequest from ttypes

fixed

bq. NotSupportedError

changed to builtin NotImplementedError

bq. InternalError

ttypes InternalError should be internal errors on the server; change this to a no-op (in the case of repeated closes) and ValueError (in the case of operation-on-closed-handle), which match the behavior of the file class.

bq. marshal.unmarshal_long has a NameError waiting to happen in the form of ""unpack""

fixed, and also pulled the if out of the unmarshal definition to only execute once.","19/Apr/11 20:28;thobbs;Looks good with the exception of changing the exceptions that are raised.

Internal error refers to cql.InternalError here, and PEP 249 outlines its usage in a way that closely matches the way it was used.
{noformat}
InternalError 
                      
Exception raised when the database encounters an internal
error, e.g. the cursor is not valid anymore, the
transaction is out of sync, etc.  It must be a subclass of
DatabaseError.
{noformat}

PEP 249 also mentions using NotSupportedError explicitly in reference to rollback():
{noformat}
NotSupportedError
          
Exception raised in case a method or database API was used
which is not supported by the database, e.g. requesting a
.rollback() on a connection that does not support
transaction or has transactions turned off.  It must be a
subclass of DatabaseError.
{noformat}
I think it should be kept here.","19/Apr/11 20:40;jbellis;You're right re NSE. Fixed.

InternalError is for _database_ errors not driver errors. Left re-close() as ok; changed cursor()-from-closed-conn to ProgrammingError, matching the sqlite3 behavior.

committed w/ above changes.","19/Apr/11 21:28;hudson;Integrated in Cassandra-0.8 #23 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/23/])
    fix imports in python cql driver
patch by jbellis; reviewed by thobbs for CASSANDRA-2508
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updating a column's validation class from AsciiType to UTF8Type does not actually work,CASSANDRA-2512,12504720,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,cdaw,cdaw,19/Apr/11 21:59,12/Mar/19 14:16,13/Mar/19 22:26,21/Apr/11 13:29,0.7.5,,,,,,0,,,,,"Please note this is reproducible on both Cassandra 0.74 and the April 18th trunk build.

*Reproduction Steps*
{code}
create column family users with comparator = UTF8Type
and column_metadata = [{column_name: password, validation_class: UTF8Type},
{column_name: gender, validation_class: AsciiType}];

update column family users with comparator = UTF8Type
and column_metadata = [{column_name: password, validation_class: UTF8Type}
{column_name: gender, validation_class: UTF8Type}];
{code}

*Before & After quitting cassandra-cli:  Notice the validation class for the gender client still shows AsciiType*
{code}
[default@demo] describe keyspace demo;
Keyspace: demo:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [datacenter1:1]
  Column Families:
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.29062499999999997/62/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: gender
          Validation Class: org.apache.cassandra.db.marshal.AsciiType
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type

{code}
","Single Node on MacOSX, installed from 0.7.4 binary .tar ball.",,,,,,,,,,,,,,,20/Apr/11 16:41;jbellis;2512.txt;https://issues.apache.org/jira/secure/attachment/12476912/2512.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-20 16:41:26.623,,,no_permission,,,,,,,,,,,,20666,,,Tue Apr 26 22:22:11 UTC 2011,,,,,,0|i0gbsv:,93342,slebresne,slebresne,,,,,,,,,20/Apr/11 16:41;jbellis;patch fixes CFMetaData.apply to include validation class.,"21/Apr/11 13:29;slebresne;+1
Committed.","21/Apr/11 18:52;hudson;Integrated in Cassandra-0.7 #452 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/452/])
    Fix updating column metadata validation class
patch by jbellis; reviewed by slebresne for CASSANDRA-2512
","21/Apr/11 20:01;hudson;Integrated in Cassandra #861 (See [https://builds.apache.org/hudson/job/Cassandra/861/])
    merge CASSANDRA-2512 from 0.8
","21/Apr/11 20:03;hudson;Integrated in Cassandra-0.8 #31 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/31/])
    merge CASSANDRA-2512 from 0.7
",26/Apr/11 22:22;cdaw;Retested and verified this is fixed in current build.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add INSERT support to CQL,CASSANDRA-2409,12503053,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,31/Mar/11 18:13,12/Mar/19 14:16,13/Mar/19 22:26,04/Apr/11 20:05,0.8 beta 1,,,Legacy/CQL,,,0,,,,,"There are two reasons to support INSERT:

- It helps new users feel comfortable (everyone's first statement will be to try to INSERT something, we should make that a positive experience instead of slapping them)
- Even though it is synonymous with update it is still useful in your code to have both to help communicate intent, similar to choosing good variable names

The only downside is explaining how INSERT isn't a ""true"" insert because it doesn't error out if the row already exists -- but we already have to explain that same concept for UPDATE; the cognitive load is extremely minor.",,,,,,,,,,,,,,,,04/Apr/11 18:05;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2409-CQL-INSERT-implementation.txt;https://issues.apache.org/jira/secure/attachment/12475392/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2409-CQL-INSERT-implementation.txt,02/Apr/11 17:09;xedin;CASSANDRA-2409.patch;https://issues.apache.org/jira/secure/attachment/12475286/CASSANDRA-2409.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-31 18:45:37.838,,,no_permission,,,,,,,,,,,,20606,,,Mon Apr 04 20:05:45 UTC 2011,,,,,,0|i0gb7b:,93245,urandom,urandom,,,,,,,,,"31/Mar/11 18:45;urandom;I don't think this is a good idea. {{UPDATE}} already has semantics that are going to be a surprise to folks coming from SQL. Implementing {{INSERT}} to have identical semantics is going to be (IMO) more surprising still, not less.

But, I don't want to get wrapped up in a protracted debate about it, so I'll leave this as ""-0"", and let you guys carry-on as you see fit.

P.S. Please don't wait too long, the freeze is looming.","31/Mar/11 18:50;xedin;Maybe I have a solution which will satisfy both sides - we can just make INSERT as an alias for UPDATE, so no need to implement anything just a simple change in the grammar, what do you think? ","01/Apr/11 23:04;urandom;I don't think this what Jonathan is after, he's looking for the equivalent of SQL's {{INSERT}}. For example:

{code:style=SQL}
INSERT INTO cf (foo, bar, baz) VALUES (1, 20 300) WHERE KEY = fnord;
{code}","02/Apr/11 00:56;jbellis;Close.

{code}
INSERT INTO cf (KEY, foo, bar, baz) VALUES (fnord, 1, 20 300);
{code}
","02/Apr/11 17:09;xedin;Format:
{noformat}
INSERT INTO
    <CF>
    (KEY, <column>, <column>, ...)
VALUES
    (<key>, <value>, <value>, ...)
(USING
  CONSISTENCY <level>)?;
{noformat}

Consistency level is set to ONE by default","02/Apr/11 17:10;xedin;Only thing for you, Eric, to change is exception message when column names size not equal to values size...","02/Apr/11 18:40;urandom;Thanks Pavel, I'll have a look at it and get back to you.","04/Apr/11 18:08;urandom;Attached is an updated version of the patch that refactors things a bit.  The biggest problem here is that we need to raise Thrift exceptions.

Also included is some minimal system tests.","04/Apr/11 18:58;xedin;Can you please explane a part about exceptions, what should be done?","04/Apr/11 19:28;urandom;If you raise a RuntimeException from the lexer or parser, it will not be propagated down to the client (it's swallowed and turned into an internal server error).

If you look at the revised patch, I moved the size equality test and conversion to Map into {{UpdateStatement.getColumns()}} where an {{InvalidRequestException}} can be raised (everything that calls {{getColumns()}} already throws {{InvalidRequestException}}. ","04/Apr/11 19:33;xedin;Oh, yeah, I've seen it, thats why I wrote you to change exception... I misunderstood your previous comment and I thought that there are still problems with exceptions left...

+1 on your patch.",04/Apr/11 20:05;urandom;committed; thanks Pavel!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Start up of 0.8-beta1 on Ubuntu,CASSANDRA-2549,12505024,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,selam,drewbroadley,drewbroadley,23/Apr/11 02:09,12/Mar/19 14:16,13/Mar/19 22:26,02/May/11 15:19,0.8.0 beta 2,,,Packaging,,,0,start,,,,"root@home:/home/drew# cassandra -f
 INFO 14:06:03,261 Logging initialized
 INFO 14:06:03,323 Heap size: 1543831552/1543831552
 INFO 14:06:03,332 JNA not found. Native methods will be disabled.
 INFO 14:06:03,379 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 14:06:03,899 DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
ERROR 14:06:04,028 Exception encountered during startup.
java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/UnavailableException
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2444)
	at java.lang.Class.privateGetPublicMethods(Class.java:2564)
	at java.lang.Class.getMethods(Class.java:1427)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.initMaps(MBeanAnalyzer.java:126)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.<init>(MBeanAnalyzer.java:116)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.analyzer(MBeanAnalyzer.java:104)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.getAnalyzer(StandardMBeanIntrospector.java:66)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.getPerInterface(MBeanIntrospector.java:181)
	at com.sun.jmx.mbeanserver.MBeanSupport.<init>(MBeanSupport.java:136)
	at com.sun.jmx.mbeanserver.StandardMBeanSupport.<init>(StandardMBeanSupport.java:64)
	at com.sun.jmx.mbeanserver.Introspector.makeDynamicMBean(Introspector.java:174)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:330)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:516)
	at org.apache.cassandra.service.StorageService.<init>(StorageService.java:231)
	at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:171)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:78)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:429)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:294)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:98)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.UnavailableException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
	... 23 more
Exception encountered during startup.
java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/UnavailableException
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2444)
	at java.lang.Class.privateGetPublicMethods(Class.java:2564)
	at java.lang.Class.getMethods(Class.java:1427)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.initMaps(MBeanAnalyzer.java:126)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.<init>(MBeanAnalyzer.java:116)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.analyzer(MBeanAnalyzer.java:104)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.getAnalyzer(StandardMBeanIntrospector.java:66)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.getPerInterface(MBeanIntrospector.java:181)
	at com.sun.jmx.mbeanserver.MBeanSupport.<init>(MBeanSupport.java:136)
	at com.sun.jmx.mbeanserver.StandardMBeanSupport.<init>(StandardMBeanSupport.java:64)
	at com.sun.jmx.mbeanserver.Introspector.makeDynamicMBean(Introspector.java:174)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:330)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:516)
	at org.apache.cassandra.service.StorageService.<init>(StorageService.java:231)
	at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:171)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:78)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:429)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:294)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:98)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.UnavailableException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
	... 23 more
","Linux home.broadley.org.nz 2.6.32-29-generic-pae #58-Ubuntu SMP Fri Feb 11 19:15:25 UTC 2011 i686 GNU/Linux
",,,,,,,,,,,,,,,23/Apr/11 20:07;selam;cassandra-0.8.0beta1-debian-package.patch;https://issues.apache.org/jira/secure/attachment/12477225/cassandra-0.8.0beta1-debian-package.patch,25/Apr/11 10:41;selam;cassandra_multiple_package_v2.patch;https://issues.apache.org/jira/secure/attachment/12477292/cassandra_multiple_package_v2.patch,26/Apr/11 15:27;selam;cassandra_multiple_package_v3.patch;https://issues.apache.org/jira/secure/attachment/12477420/cassandra_multiple_package_v3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-04-23 20:06:20.322,,,no_permission,,,,,,,,,,,,20691,,,Mon May 02 19:54:49 UTC 2011,,,,,,0|i0gc0v:,93378,urandom,urandom,,,,,,,,,"23/Apr/11 20:06;selam;deb package doesn't contain apache-cassandra-thrift-${VERSION}.jar and apache-cassandra-cql-*.jar 

My patch does not change package version or release number of deb package.","24/Apr/11 00:51;prystupa;Timu, can your patch be applied to the currently latest binary 0.7.4 distribution?","24/Apr/11 02:31;jbellis;No, there is only one jar in 0.7.","24/Apr/11 16:32;urandom;The cql jar is not a dependency here.  If it needs to be packaged, it's something that would get its own package.","25/Apr/11 01:26;jbellis;But from the stacktrace, the thrift jar is a dependency, no?","25/Apr/11 02:36;urandom;Yes, the thrift jar is a dependency, the cql jar is not, the patch adds both.","25/Apr/11 10:41;selam;my second patch generates multiple binary package.
this packages for: libthrift-java, cassandra, cassandra-thrift, cassandra-cql

cassandra and cassandra-cql depends cassandra-thrift.
cassandra-thrift depends libthrift-java

All package versions points to 0.8.0 but for Cql it must be 1.0.0 and for libthrift-java it must be 0.6. i working on for fix this, but i guess deb packaging system doesn't allowed to do this. another solution create separate debian directory for each package, but i guess this is not acceptable.

i build new packages from using this patch and i install 2 nodes without problem. 

patch name: cassandra_multiple_package_v2.patch","26/Apr/11 06:44;drewbroadley;Thanks for the patch, this was enough to get things going again.

Would it be possible to create a separate 0.8 & 0.7 package as I wasn't wanting to jump to 0.8.","26/Apr/11 15:27;selam;sorry,  i was remove some jars from cassandra package and added in dependencies, then after i remove dependencies but i forget to add package install dir. so i believe that patch is final patch. ","26/Apr/11 18:08;urandom;I probably shouldn't have said ""If it needs to be packaged, it's something that would get its own package."", because I don't think it should (not yet at least, and not like this).

For one thing, the cql jar depends on more than the apache-cassandra-thrift jar; I'm not sure what your process was for testing, but it won't work like this.

Secondly, those dependencies (the ones between jars) will eventually be sorted, but then you're faced with the dependency on Thrift.  Our bundling of a private copy of Thrift is already something that brings shame to my ancestors, breaking that out into its own package is going to end in someone being reincarnated as a lemur, or a muskrat, or as the exhaust system of a 1997 Land Rover.  I'll have no part in it.",26/Apr/11 23:32;urandom;+1(ish) to cassandra-0.8.0beta1-debian-package.patch,02/May/11 15:19;jbellis;committed cassandra-0.8.0beta1-debian-package.patch,"02/May/11 19:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to forward merge parts of r1088800 to make the pig CassandraStorage build,CASSANDRA-2511,12504715,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jeromatron,jeromatron,jeromatron,19/Apr/11 21:22,12/Mar/19 14:16,13/Mar/19 22:26,19/Apr/11 21:54,0.8 beta 1,,,,,,0,hadoop,pig,,,"Parts of revision 1088800 weren't forward merged into 0.8/trunk.  So the 0.8/trunk version of CassandraStorage doesn't currently build.  Specifically, it needs the decompose method in the AbstractType hierarchy.",,,,,,,,,,,,,,,,19/Apr/11 23:06;jeromatron;2511-newer-types.txt;https://issues.apache.org/jira/secure/attachment/12476799/2511-newer-types.txt,19/Apr/11 21:50;jeromatron;2511.txt;https://issues.apache.org/jira/secure/attachment/12476786/2511.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-19 21:54:27.393,,,no_permission,,,,,,,,,,,,20665,,,Tue Apr 19 23:06:29 UTC 2011,,,,,,0|i0gbsn:,93341,jbellis,jbellis,,,,,,,,,19/Apr/11 21:50;jeromatron;Forward merged the parts from CASSANDRA-2387 that hadn't made it in.  The only question I had was whether it was worth it to embed a call to decompose in the TimeUUIDType.fromString method.  It may not be worth it.,19/Apr/11 21:51;jeromatron;So this makes it so the pig CassandraStorage can compile and run in 0.8.  Patch is against 0.8-branch.,"19/Apr/11 21:54;jbellis;committed, thanks!",19/Apr/11 23:06;jeromatron;The counter type may have an incorrect decompose (in its parent class) but I think the rest should be fine.  Sorry about missing these the first time.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distributed test scripts not working with Whirr 0.4.0,CASSANDRA-2523,12504822,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,mallen,stuhood,stuhood,20/Apr/11 19:58,12/Mar/19 14:16,13/Mar/19 22:26,03/Aug/11 22:10,0.8.4,,,,,,0,,,,,"I suspect that our runurl based script execution is not working with Whirr 0.4.0, which is causing distributed tests that kill/wipe nodes to timeout. See [this FAQ entry|http://incubator.apache.org/whirr/faq.html#how-can-i-modify-the-instance-installation-and-configuration-scripts] for a description of the change.",,,,,,,,,,CASSANDRA-2464,,CASSANDRA-2504,,,,22/Apr/11 02:31;stuhood;0001-WIP-Update-to-use-jclouds-magical-functions-directory-.txt;https://issues.apache.org/jira/secure/attachment/12477068/0001-WIP-Update-to-use-jclouds-magical-functions-directory-.txt,02/Aug/11 08:13;mallen;2523-1-use-jclouds-magical-functions-directory-revised.txt;https://issues.apache.org/jira/secure/attachment/12488864/2523-1-use-jclouds-magical-functions-directory-revised.txt,02/Aug/11 16:20;mallen;2523-2-use-jclouds-magical-functions-directory-revised.txt;https://issues.apache.org/jira/secure/attachment/12488916/2523-2-use-jclouds-magical-functions-directory-revised.txt,03/Aug/11 21:45;mallen;2523-3-use-jclouds-magical-functions-directory-revised.txt;https://issues.apache.org/jira/secure/attachment/12489258/2523-3-use-jclouds-magical-functions-directory-revised.txt,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-04-22 09:42:24.563,,,no_permission,,,,,,,,,,,,20674,,,Wed Aug 03 23:25:43 UTC 2011,,,,,,0|i0gbvb:,93353,brandon.williams,brandon.williams,,,,,,,,,20/Apr/11 20:01;stuhood;This issue probably relates-to/causes 2504.,"22/Apr/11 02:31;stuhood;A work-in-progress patch to include our own ""functions"" directory as alluded to in the Whirr FAQ. I'm not yet certain that it's getting picked up at all.","22/Apr/11 09:42;xedin;Standard approach works for me most of the time but in like 2-3% of the cases it fails, I don't think that runurl is a good and stable solution, maybe we should hold our scripts in the test/distributed/scripts directory so we always sure that they are in place instead of using whirr's half-broken approach? ",09/May/11 20:16;jbellis;Any more clarity here on what the right way to fix this is?,09/May/11 21:57;stuhood;I'd like to put some more time into the distributed tests once the next revision of 2319/674 is posted: hopefully 2 weeks from now.,"02/Aug/11 08:14;mallen;Stu was headed in the right direction here.  I've attached a working revised version of his patch with a few adjustments.  

The main difference is the use of whirr's StatementBuilder, which uses jclouds ScriptBuilder.  When an instance of ScriptBuilder is rendered and during the 'resolve function dependencies stage', jclouds will search the class path for a directory named ""functions"".  It will then look in that directory for a file named after the function that it is trying to resolve.  For example, if in the ScriptBuilder there was a call to the function ""start_cassandra"", then jclouds would look for a file named functions/start_cassandra.sh and if found, include its content in the ScriptBuilder.  

functions/start_cassandra.sh is expected to contain a single function definition, namely ""start_cassandra"".

This seems to work fairly well in practice, and we no longer use runurl with this solution.

",02/Aug/11 16:20;mallen;updated to fix my use of the incorrect aws-s3 version.  should be 1.0-beta-9b rather than 1.0.,03/Aug/11 20:32;brandon.williams;why does install_cassandra.sh use 0.7.0?,"03/Aug/11 20:44;mallen;Currently, the version is always passed to that function so the default never gets used, but I guess we should update that to default to 0.8 instead?","03/Aug/11 20:46;mallen;and to actually answer the question :)  that install script is provided by whirr and in 0.4.0 release, they've got it set to 0.7.","03/Aug/11 20:52;brandon.williams;bq. Currently, the version is always passed to that function so the default never gets used, but I guess we should update that to default to 0.8 instead?

I think I'd rather not have a default so it's always explicit.  Accidentally testing the wrong version would be pretty bad.

bq. and to actually answer the question  that install script is provided by whirr and in 0.4.0 release, they've got it set to 0.7.

Ah, I see :)","03/Aug/11 21:45;mallen;Attached another version to address the ""let's not use a default version of cassandra"" in install_cassandra.sh",03/Aug/11 22:10;brandon.williams;Committed.,"03/Aug/11 23:25;hudson;Integrated in Cassandra-0.8 #255 (See [https://builds.apache.org/job/Cassandra-0.8/255/])
    Update distributed tests to work with whirr 0.4
Patch by Michael Allen, reviewed by brandonwilliams for CASSANDRA-2523

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1153688
Files : 
* /cassandra/branches/cassandra-0.8/test/distributed/org/apache/cassandra/CassandraServiceController.java
* /cassandra/branches/cassandra-0.8/test/resources/functions/stop_cassandra.sh
* /cassandra/branches/cassandra-0.8/test/resources/functions/start_cassandra.sh
* /cassandra/branches/cassandra-0.8/test/distributed/org/apache/cassandra/TestBase.java
* /cassandra/branches/cassandra-0.8/test/resources/functions/install_cassandra.sh
* /cassandra/branches/cassandra-0.8/test/resources/functions
* /cassandra/branches/cassandra-0.8/test/resources/functions/nodetool_cassandra.sh
* /cassandra/branches/cassandra-0.8/test/resources/functions/wipe_cassandra.sh
* /cassandra/branches/cassandra-0.8/test/resources/functions/configure_cassandra.sh
* /cassandra/branches/cassandra-0.8/test/distributed/org/apache/cassandra/MutationTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update distributed tests for optional Column fields,CASSANDRA-2517,12504752,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,20/Apr/11 03:49,12/Mar/19 14:16,13/Mar/19 22:26,20/Apr/11 04:28,0.8.0 beta 2,,,Legacy/Testing,,,0,,,,,,,,,,,,,,,,,,,,,20/Apr/11 03:51;stuhood;0001-Remove-duped-code-from-MutationTest.txt;https://issues.apache.org/jira/secure/attachment/12476827/0001-Remove-duped-code-from-MutationTest.txt,20/Apr/11 03:51;stuhood;0002-Update-distributed-tests-for-thrift-api-changes.txt;https://issues.apache.org/jira/secure/attachment/12476828/0002-Update-distributed-tests-for-thrift-api-changes.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-20 04:28:42.619,,,no_permission,,,,,,,,,,,,20670,,,Wed Apr 20 04:28:42 UTC 2011,,,,,,0|i0gbtz:,93347,jbellis,jbellis,,,,,,,,,"20/Apr/11 03:51;stuhood;0001 Remove duped code that was left behind when generic utilities were boosted into TestBase
0002 Updates Column construction for optional timestamp and value","20/Apr/11 04:28;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE from PrecompactedRow,CASSANDRA-2528,12504909,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,cywjackson,cywjackson,21/Apr/11 17:55,12/Mar/19 14:16,13/Mar/19 22:26,21/Apr/11 22:48,0.8.0 beta 2,,,,,,0,,,,,"received a NPE from trunk (0.8) on PrecompactedRow:

ERROR [CompactionExecutor:2] 2011-04-21 17:21:31,610 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:86)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:44)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:553)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:146)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:112)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


size of data in /var/lib/cassandra is 11G on this, but there is also report that 1.7G also see the same.

data was previously populated from 0.7.4 cassandra

added debug logging, not sure how much this help (this is logged before the exception.)

 INFO [CompactionExecutor:2] 2011-04-21 17:21:31,588 CompactionManager.java (line 534) Compacting Major: [SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-7-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-6-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-8-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-9-Data.db')]
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-10-Data.db   : 256
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-7-Data.db   : 512
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-6-Data.db   : 768
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,589 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-8-Data.db   : 1024
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,589 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-9-Data.db   : 1280
 INFO [CompactionExecutor:2] 2011-04-21 17:21:31,609 CompactionIterator.java (line 185) Major@1181554512(cfs, inode.path, 523/10895) now compacting at 16777 bytes/ms.
",,,,,,,,,,,,,,,,21/Apr/11 21:04;jbellis;2528.txt;https://issues.apache.org/jira/secure/attachment/12477037/2528.txt,21/Apr/11 19:18;jbellis;2528.txt;https://issues.apache.org/jira/secure/attachment/12477027/2528.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-21 18:29:22.851,,,no_permission,,,,,,,,,,,,20676,,,Thu Apr 21 23:18:50 UTC 2011,,,,,,0|i0gbw7:,93357,cywjackson,cywjackson,,,,,,,,,21/Apr/11 18:29;patricioe;I'm seeing the same stacktrace when writing 1GB of data. ,21/Apr/11 19:18;jbellis;can you try w/ this patch?  it adds additional asserts to narrow down where the error is,"21/Apr/11 19:38;cywjackson;sure, here is the Assertion Error, the null is on metadata:
","21/Apr/11 19:38;cywjackson;DEBUG [CompactionExecutor:2] 2011-04-21 19:31:46,797 PrecompactedRow.java (line 85) debugging controoler true
DEBUG [CompactionExecutor:2] 2011-04-21 19:31:46,798 PrecompactedRow.java (line 87) debugging compactedCF: ColumnFamily(<anonymous> [3636363663643736663936393536343639653762653339643735306363376439:false:0@1303340825329,])
 INFO [CompactionExecutor:2] 2011-04-21 19:31:46,799 CompactionIterator.java (line 185) Major@31583366(cfs, inode.path, 772/11720) now compacting at 8388 bytes/ms.
ERROR [CompactionExecutor:2] 2011-04-21 19:31:46,850 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.AssertionError
    at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:91)
    at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:44)
    at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:553)
    at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:146)
    at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:112)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)","21/Apr/11 19:39;cywjackson;{code:title=PrepcompactedRow.java}
    85          logger.debug(String.format(""debugging controoler %b"", controller.shouldPurge(key)));
    86          compactedCf = controller.shouldPurge(key) ? ColumnFamilyStore.removeDeleted(cf, controller.gcBefore) : cf;
    87          logger.debug(String.format(""debugging compactedCF: %s"", compactedCf.toString()));
    88          //if (compactedCf != null && compactedCf.metadata().getDefaultValidator().isCommutative())
    89          if (compactedCf != null)
    90          {
    91                  assert compactedCf.metadata() != null;
    92                  assert compactedCf.metadata().getDefaultValidator() != null;
    93                  if (compactedCf.metadata().getDefaultValidator().isCommutative())
    94                  {
    95                          CounterColumn.removeOldShards(compactedCf, controller.gcBefore);
    96                  }
{code}","21/Apr/11 19:40;cywjackson;$ grep java.lang.AssertionError -A3 /var/log/cassandra/system.log 
{noformat}

java.lang.AssertionError
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:91)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
--
java.lang.AssertionError
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:91)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
{noformat}","21/Apr/11 21:04;jbellis;Patch to preserve the CFMetaData object used to construct the ColumnFamily, instead of looking it up from the registry (which does not contain metadata for index CFs).",21/Apr/11 22:38;cywjackson;the fix does address the NPE as now all CF has metadata.,21/Apr/11 22:48;jbellis;committed,"21/Apr/11 23:18;hudson;Integrated in Cassandra-0.8 #32 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/32/])
    fix NPE compacting index CFs
patch by jbellis; reviewed by Jackson Chung for CASSANDRA-2528
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log read timeouts at the StorageProxy level,CASSANDRA-2532,12504922,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,21/Apr/11 21:26,12/Mar/19 14:16,13/Mar/19 22:26,25/Apr/11 13:54,0.8.0 beta 2,,,,,,0,,,,,"We log successful reads, but not timeouts (although we have a lovely TimeoutException message, it doesn't look like we are printing it).",,,,,,,,,,,,,,,,21/Apr/11 21:27;stuhood;0001-Log-timeouts-in-SP.txt;https://issues.apache.org/jira/secure/attachment/12477041/0001-Log-timeouts-in-SP.txt,21/Apr/11 21:48;stuhood;0002-Remove-useless-timeout-entries-in-thrift.CS.txt;https://issues.apache.org/jira/secure/attachment/12477044/0002-Remove-useless-timeout-entries-in-thrift.CS.txt,21/Apr/11 22:53;stuhood;0003-Log-write-timeouts-in-SP.txt;https://issues.apache.org/jira/secure/attachment/12477049/0003-Log-write-timeouts-in-SP.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-04-21 21:31:02.225,,,no_permission,,,,,,,,,,,,20678,,,Mon Apr 25 14:19:26 UTC 2011,,,,,,0|i0gbx3:,93361,jbellis,jbellis,,,,,,,,,"21/Apr/11 21:31;jbellis;as discussed on irc, we should centralize timeout logging in SP rather than mixing it into both CassandraServer + SP.",21/Apr/11 21:48;stuhood;0002 removes the useless timeout entry from thrift.CassandraServer for symmetry.,21/Apr/11 21:53;jbellis;can we do inserts too for consistency?,21/Apr/11 22:55;stuhood;Done in 0003,"25/Apr/11 13:54;jbellis;committed, thanks!","25/Apr/11 14:19;hudson;Integrated in Cassandra-0.8 #37 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/37/])
    centralize timeout logging in StorageProxy, and preserve the timeoutexception message
patch by Stu Hood; reviewed by jbellis for CASSANDRA-2532
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UUIDType assumes ByteBuffer has an accessible backing array,CASSANDRA-2682,12507976,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,edanuff,edanuff,edanuff,21/May/11 17:08,12/Mar/19 14:16,13/Mar/19 22:26,23/May/11 01:30,0.8.0,,,,,,0,,,,,"I'm very embarrassed to say this got left out in the UUIDType, but it's not doing a hasArray() check on the bytebuffers passed to it, causing it to break.  I'll make a patch to fix it.",,,,,,,,,,,,,,,,22/May/11 00:35;edanuff;2682.txt;https://issues.apache.org/jira/secure/attachment/12480011/2682.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-21 23:43:33.264,,,no_permission,,,,,,,,,,,,20773,,,Mon May 23 01:30:55 UTC 2011,,,,,,0|i0gctr:,93508,jbellis,jbellis,,,,,,,,,"21/May/11 23:43;jbellis;looks like converting array access to ByteBuffer.get(i) would be a better fix, since you'd avoid an unnecessary copy for direct buffers.  (this is what TimeUUIDType does.)",22/May/11 00:35;edanuff;New patch that uses ByteBuffer.get() instead of direct array accesses,"23/May/11 01:30;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stack overflow while compacting,CASSANDRA-2626,12506563,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,skamio,terjem,terjem,09/May/11 05:04,12/Mar/19 14:16,13/Mar/19 22:26,26/May/11 18:28,0.8.0,,,,,,0,,,,,"This is a trunk build from May 3.

After adding  CASSANDRA-2401, I have gotten the following on several nodes.
I am not 100% sure right now if it is related to 2401 but it may seem likely.

Unfortunately, as often is the case with stack overflows, I don't see the start of the stack

ERROR [CompactionExecutor:17] 2011-05-09 07:56:32,479 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:17,1,main]
java.lang.StackOverflowError
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
",,,,,,,,,,,,,,,,26/May/11 14:03;jbellis;2626-v3.txt;https://issues.apache.org/jira/secure/attachment/12480545/2626-v3.txt,25/May/11 17:50;jbellis;2626.txt;https://issues.apache.org/jira/secure/attachment/12480437/2626.txt,26/May/11 08:24;skamio;CASSANDRA-2626.patch;https://issues.apache.org/jira/secure/attachment/12480519/CASSANDRA-2626.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-05-09 05:22:03.994,,,no_permission,,,,,,,,,,,,20738,,,Thu May 26 18:28:20 UTC 2011,,,,,,0|i0gchr:,93454,jbellis,jbellis,,,,,,,,,"09/May/11 05:22;jbellis;The only place we construct a UnmodifableCollection explicitly is Table.getColumnFamilyStores, which clearly doesn't nest enough UCs to blow a stack.  Something else must be doing that under the hood...

Doesn't look 2401-related at all.","25/May/11 13:28;skamio;I was able to reproduce the stack overflow problem on cassandra 0.8.0 trunk.
The DataTracker creates unmodifiableSet of 'Set<SSTableReader> compacting' on switching memtable and other operations. It creates unmodifiableSet one level deeper every time memtables is switched. When the number of switching memtable reaches some level and its size() is called, the stack overflow exception occurs if the call stack exceeds stack limit.

Stack trace below (dumped by simple wrapper class MyUnmodifiableSet) shows size() is called by constructor of HashSet in View.markCompacting() (DataTracker.java).
If the nesting is unavoidable, a solution is to use UnmodifiableSet in apache commons collections library. It doesn't create nests of unmodifiable collection.


Steps to reproduce the stack overflow:
1. Create single cassandra node with standard column families.
2. Disable compaction and set MemtableThroughputInMB and MemtableOperationsInMillions to small value in order to flush memtables frequently.
3. Insert many data for many keys.
4. When number of sstables exceeds 2000 (this parameter may vary in environments), run ""nodetool compact"". The error will be logged.


-------
* Change for debug in DataTracker.java:
{quote}
        public View(Memtable memtable, Set<Memtable> pendingFlush, Set<SSTableReader> sstables, Set<SSTableReader> compacting)
        \{
            this.memtable = memtable;
  //             this.memtablesPendingFlush = Collections.unmodifiableSet(pendingFlush);
  //             this.sstables = Collections.unmodifiableSet(sstables);
  //             this.compacting = Collections.unmodifiableSet(compacting);

            this.memtablesPendingFlush = new MyUnmodifiableSet(pendingFlush);
            this.sstables              = new MyUnmodifiableSet(sstables);
            this.compacting            = new MyUnmodifiableSet(compacting);
        \}
{quote}
-------
* Stacktrace by Thread.dumpStack() in MyUnmodifiableSet.

        at java.lang.Thread.dumpStack(Thread.java:1249)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:35)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:36)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:36)
   ......
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:36)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:36)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:36)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:36)
        at java.util.HashSet.<init>(HashSet.java:99)
        at org.apache.cassandra.db.DataTracker$View.markCompacting(DataTracker.java:495)
        at org.apache.cassandra.db.DataTracker.markCompacting(DataTracker.java:188)
        at org.apache.cassandra.db.CompactionManager$4.call(CompactionManager.java:312)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

------","25/May/11 17:50;jbellis;Ah, you've nailed it. Thanks for tracking that down.

Patch attached to avoid re-wrapping an already unmodifiable collection. (Surprised Collections.unmodifiable... doesn't do this.)

We may also want to look at using ImmutableSet (http://guava-libraries.googlecode.com/svn/trunk/javadoc/com/google/common/collect/ImmutableSet.html) instead of HashSet wrapped in UnmodifiableSet. (ImmutableSet claims to ""perform significantly better than HashSet for objects with very fast Object.hashCode() implementations."")","25/May/11 18:01;jbellis;(Did a quick check on our other uses of Collections.unmodifiable*, didn't see any red flags.)",25/May/11 18:26;mdennis;+1,25/May/11 19:09;jbellis;committed,"26/May/11 01:24;muga_nishizawa;Thanks for your quick response.  

I'm not sure but I think that the fix doesn't work.  Types of pendingFlush, sstables and compacting objects are java.util.Collections.UnmodifiableSet.  Those are not instances of org.apache.commons.collections.set.UnmodifiableSet.  ","26/May/11 02:07;brandon.williams;Confirmed, stack gets blown with >2k sstables still.","26/May/11 08:24;skamio;This patch prevents stack overflow. Apache commons UnmodifiableSet is used instead of java.util.Collections.UnmodifiableSet because the latter is not accessible due to package private.
",26/May/11 13:16;jbellis;Oh no! Auto-import for the lose :(,"26/May/11 14:03;jbellis;v3 takes a slightly different approach -- instead of accepting any Set in the constructor and decorating it w/ Unmodifiable, v3 restricts constructor to private access and makes the factory methods responsible for ensuring the set is unmodifiable, by using ImmutableSet as mentioned above.","26/May/11 18:06;brandon.williams;+1, I like this approach better too.",26/May/11 18:28;jbellis;committed v3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Empty Result with Secondary Index Queries with ""limit 1""",CASSANDRA-2628,12506607,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,muga_nishizawa,muga_nishizawa,09/May/11 13:29,12/Mar/19 14:16,13/Mar/19 22:26,12/May/11 15:14,0.7.6,0.8.0,,Feature/2i Index,,,0,,,,,"Empty result is returned by secondary index queries with ""limit 1"".  Cassandra returns correct result for other numbers than ""1"" (e.g. limit 2, limit 3, etc.).  

You can reproduce the problem with programs attached on CASSANDRA-2406.  

- 1. Start Cassandra cluster. It consists of 3 cassandra nodes and distributes data by ByteOrderedPartitioner. Initial tokens of those nodes are [""31"", ""32"", ""33""].
- 2. Create keyspace and column family, according to ""create_table.cli"",
- 3. Execute ""secondary_index_insertv2.py"", inserting a few hundred columns to cluster
- 4. Here, when you first use cassandra-cli and execute following lines, you can get correct result.  

{quote}
% bin/cassandra-cli
[default@unknown] connect localhost/9160;
[default@unknown] use SampleKS;
[default@SampleKS] get SampleCF where up = 'up' limit 3;               
-------------------
RowKey: 150
=> (column=date, value=150, timestamp=1304937931)
=> (column=up, value=up, timestamp=1304937931)
-------------------
RowKey: 151
=> (column=date, value=151, timestamp=1304937932)
=> (column=up, value=up, timestamp=1304937932)
-------------------
RowKey: 152
=> (column=date, value=152, timestamp=1304937932)
=> (column=up, value=up, timestamp=1304937932)
3 Rows Returned.  
{quote}

On the other hand, if you set limit to ""1"", you can reproduce the problem.

{quote}
[default@SampleKS] get SampleCF where up = 'up' and date > 150 limit 1;
0 Row Returned.
{quote}

There are two factors to cause this problem:
- 1. scanned first column doesn't match at specified clause like ""date > 150"".
- 2. ""limit 1""

Only one factor doesn't cause problem.  For example, I can get correct data when I specify as following:

- ""limit 1"" -> ""limit 2""
{quote}
[default@SampleKS] get SampleCF where up = 'up' and date > 150 limit 2;
-------------------
RowKey: 151
=> (column=date, value=151, timestamp=1304937932)
=> (column=up, value=up, timestamp=1304937932)
-------------------
RowKey: 152
=> (column=date, value=152, timestamp=1304937932)
=> (column=up, value=up, timestamp=1304937932)
2 Rows Returned.
{quote}

- ""date > 150"" -> ""date >= 150""
{quote}
[default@SampleKS] get SampleCF where up = 'up' and date >= 150 limit 1;
-------------------
RowKey: 150
=> (column=date, value=150, timestamp=1304937931)
=> (column=up, value=up, timestamp=1304937931)
1 Row Returned.
{quote}",CentOS 5.5,,,,,,,,,,,,,,,12/May/11 11:07;slebresne;0001-2628.patch;https://issues.apache.org/jira/secure/attachment/12478955/0001-2628.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-12 11:07:17.094,,,no_permission,,,,,,,,,,,,20740,,,Thu May 12 20:56:28 UTC 2011,,,,,,0|i0gci7:,93456,jbellis,jbellis,,,,,,,,,"12/May/11 11:07;slebresne;Attaching fix along with a unit test.

Problem was we do some paging on the index using clause.count as the page size. And 1 is a pretty bad page size.

Thanks Muga for the report and the clear instructions to reproduce.","12/May/11 14:55;jbellis;To clarify, the problem is that since slice is inclusive-from-start, if our page size is one a 2nd pass will get back the same row the first did, so the scan logic thinks there is no more data and breaks. (since that is exactly what you will see if you just scanned the last column in the index row.)

+1 on the fix.",12/May/11 15:14;slebresne;Committed,"12/May/11 20:56;hudson;Integrated in Cassandra-0.7 #483 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/483/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL greater-than and less-than operators (> and <) result in key ranges that are inclusive of the terms,CASSANDRA-2592,12505959,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,02/May/11 17:42,12/Mar/19 14:16,13/Mar/19 22:26,21/May/11 03:07,0.8.0,,,Legacy/CQL,,,0,,,,,"This affects range queries against keys, but not index queries.

One possible solution: let the coordinator strip out the extra row in QueryProcessor.",,,,,,,,,,,,,,,,07/May/11 02:30;jbellis;2592-v3-in-progress.txt;https://issues.apache.org/jira/secure/attachment/12478486/2592-v3-in-progress.txt,20/May/11 20:38;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2592-handle-empty-result-sets.txt;https://issues.apache.org/jira/secure/attachment/12479951/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2592-handle-empty-result-sets.txt,20/May/11 00:07;xedin;CASSANDRA-2592-fix-when-rows-are-empty.patch;https://issues.apache.org/jira/secure/attachment/12479862/CASSANDRA-2592-fix-when-rows-are-empty.patch,04/May/11 23:13;xedin;CASSANDRA-2592-v2.patch;https://issues.apache.org/jira/secure/attachment/12478228/CASSANDRA-2592-v2.patch,07/May/11 13:09;xedin;CASSANDRA-2592-v3.patch;https://issues.apache.org/jira/secure/attachment/12478510/CASSANDRA-2592-v3.patch,04/May/11 16:15;xedin;CASSANDRA-2592.patch;https://issues.apache.org/jira/secure/attachment/12478174/CASSANDRA-2592.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2011-05-04 23:13:24.973,,,no_permission,,,,,,,,,,,,20717,,,Sat May 21 03:52:54 UTC 2011,,,,,,0|i0gcaf:,93421,jbellis,jbellis,,,,,,,,,"04/May/11 21:51;jbellis;I think this patch makes it so if I say ""LIMIT 10"" I might only get 8 back because one result got chopped off.  QP will need to request more than LIMIT to give back the right number.

Can you add a test for this?",04/May/11 23:13;xedin;added a check which will prevent removing a first/last row if it wasn't start/end key. Tests for LIMIT added.,07/May/11 02:30;jbellis;Some issues around LIMIT. I've added more tests in v3 to demonstrate (but not fix) the problem.,07/May/11 13:09;xedin;v3 with your tests included and insured to pass.,07/May/11 23:16;jbellis;committed w/ minor changes,"10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ","19/May/11 23:43;urandom;
This blows up on queries that return no results.

{noformat}
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.cql.QueryProcessor.multiRangeSlice(QueryProcessor.java:194)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:534)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1131)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}","20/May/11 21:55;jbellis;I think Pavel's patch is more correct since the result could come back with exactly one row equal to the start key on a ""KEY > X AND KEY < Y"" query.  Then we want to remove the extra row on the first check, w/o erroring out on the second.",21/May/11 03:07;urandom;you're right; +1.  committed.,"21/May/11 03:52;hudson;Integrated in Cassandra-0.8 #119 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/119/])
    properly handle empty result set

Patch by Pavel Yaskevich; reviewed by eevans for CASSANDRA-2592

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1125622
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy sends same message multiple times,CASSANDRA-2557,12505158,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,skamio,skamio,26/Apr/11 06:45,12/Mar/19 14:16,13/Mar/19 22:26,26/Apr/11 07:00,0.8.0 beta 2,,,,,,0,,,,,"A cassandra node gets multiple mutation messages (in number of times of replication factor at maximum) for an insert. It may cause high load on the node. The mutation should be only once for each insert.

This bug is visible via MutationStage count in nodetool tpstats.
For instance, if you have 6 node cluster (initial keys are 31, 32, 33, 34, 35 and 36) with replication factor = 4 and a single data (for example, key='2') is inserted, MutationStage count will be as follows:

node 1: MutationStage 0 0 4
node 2: MutationStage 0 0 3
node 3: MutationStage 0 0 2
node 4: MutationStage 0 0 1
node 5: MutationStage 0 0 0
node 6: MutationStage 0 0 0

As you can see, the counts are different in each node.
",linux,,,,,,,,,,,,,,,26/Apr/11 06:48;skamio;StorageProxy.java.patch;https://issues.apache.org/jira/secure/attachment/12477367/StorageProxy.java.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-26 07:00:07.883,,,no_permission,,,,,,,,,,,,20696,,,Tue Apr 26 15:22:04 UTC 2011,,,,,,0|i0gc2n:,93386,slebresne,slebresne,,,,,,,,,26/Apr/11 06:48;skamio;The problem occurs because the sendMessages() is within for-loop of StorageProxy. A patch to fix that is attached.,"26/Apr/11 07:00;slebresne;Good catch. Committed, thanks!","26/Apr/11 07:29;hudson;Integrated in Cassandra-0.8 #40 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/40/])
    Fix sending mutation messages multiple times
patch by skamio; reviewed by slebresne for CASSANDRA-2557
","26/Apr/11 15:22;hudson;Integrated in Cassandra #865 (See [https://builds.apache.org/hudson/job/Cassandra/865/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check for null super column for SC CF in ThriftValidation (and always validate the sc key),CASSANDRA-2571,12505255,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,mbulman,mbulman,26/Apr/11 22:53,12/Mar/19 14:16,13/Mar/19 22:26,29/Apr/11 17:11,0.7.6,0.8.0 beta 2,,,,,0,,,,,"Run the following via cli:
{noformat}
[default@test] use test;
Authenticated to keyspace: test
[default@test] create column family super with column_type=Super and default_validation_class=CounterColumnType;
d41df8e0-7055-11e0-0000-242d50cf1fbf
Waiting for schema agreement...
... schemas agree across the cluster
[default@test] incr super['0']['0'];
Value incremented.
[default@test] incr super['0']['0']['0'];
null
{noformat}

Obviously the first incr call is invalid, even though it reports otherwise, as well as generates this exception:
{noformat}
ERROR 17:38:05,871 Fatal exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.CounterColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.CounterColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:353)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:337)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:88)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:74)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.serialize(RowMutation.java:353)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:236)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:480)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
{noformat}

But the second, proper incr call results in a bunch of exceptions and not a real increment.",,,,,,,,,,,,,,,,27/Apr/11 16:50;slebresne;0001-Improve-ThriftValidation.patch;https://issues.apache.org/jira/secure/attachment/12477563/0001-Improve-ThriftValidation.patch,28/Apr/11 19:26;jbellis;2571.txt;https://issues.apache.org/jira/secure/attachment/12477683/2571.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-27 16:50:11.041,,,no_permission,,,,,,,,,,,,20704,,,Mon May 02 19:54:50 UTC 2011,,,,,,0|i0gc5r:,93400,jbellis,jbellis,,,,,,,,,"27/Apr/11 16:50;slebresne;Turns out this is not a counter related bug. We just don't check when doing a (single) insert (and thus a add) on a super CF that the super column is not null.

Attached patch add a system test and fix it. It also fix another hole in ThriftValidation where the sc key was not validated. The patch is against 0.7 because it's not 0.8 specific.

Note that in 0.8 the cli ""hides"" this error for non counter column, but it is still not counter specific. The fact that the cluster is then ""in a bad state"", is because since we don't refuse the insert, some insert with a 'null' key is inserted in the column family map and thus messed up following insert (I think at least, haven't check super closely).","28/Apr/11 19:26;jbellis;I find boolean (or other :) flags that change method behavior subtly confusing.  Attached is a version that inlines the new check into insert(), which is the only place that wants it.","29/Apr/11 16:27;hudson;Integrated in Cassandra-0.7 #463 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/463/])
    Reject queries with missing mandatory super column and always validate super column name
patch by jbellis and slebresne for CASSANDRA-2571
",29/Apr/11 17:11;slebresne;Committed,"29/Apr/11 17:36;hudson;Integrated in Cassandra #872 (See [https://builds.apache.org/hudson/job/Cassandra/872/])
    merge CASSANDRA-2571 from 0.8
","02/May/11 19:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move gossip heartbeats [back] to its own thread,CASSANDRA-2554,12505126,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,25/Apr/11 17:20,12/Mar/19 14:16,13/Mar/19 22:26,26/Apr/11 17:11,0.7.6,,,,,,0,,,,,"Gossip heartbeat *really* needs to run every 1s or other nodes may mark us down. But gossip currently shares an executor thread with other tasks.

I see at least two of these could cause blocking: hint cleanup post-delivery and flush-expired-memtables, both of which call forceFlush which will block if the flush queue + threads are full.

We've run into this before (CASSANDRA-2253); we should move Gossip back to its own dedicated executor or it will keep happening whenever someone accidentally puts something on the ""shared"" executor that can block.",,,,,,,,,,,,,,,,25/Apr/11 17:30;jbellis;2554-0.7.txt;https://issues.apache.org/jira/secure/attachment/12477316/2554-0.7.txt,25/Apr/11 17:40;jbellis;2554-0.8.txt;https://issues.apache.org/jira/secure/attachment/12477317/2554-0.8.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-26 11:24:30.71,,,no_permission,,,,,,,,,,,,20694,,,Tue Apr 26 17:36:04 UTC 2011,,,,,,0|i0gc1z:,93383,slebresne,slebresne,,,,,,,,,"25/Apr/11 17:40;jbellis;patches against 0.7 and 0.8 to move Gossip to its own executor, and move hint deletion + flush expired memtables + cache saving to the long-execution-time executor.",26/Apr/11 11:24;slebresne;+1,26/Apr/11 17:11;jbellis;committed,"26/Apr/11 17:36;hudson;Integrated in Cassandra-0.7 #458 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/458/])
    movegossip heartbeat back to its own thread
patch by jbellis; reviewed by slebresne for CASSANDRA-2554
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrub could lose increments and replicate that loss,CASSANDRA-2759,12509845,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,10/Jun/11 16:20,12/Mar/19 14:16,13/Mar/19 22:26,14/Jun/11 09:18,0.8.1,,,,,,0,counters,,,,"If scrub cannot 'repair' a corrupted row, it will skip it. On node A, if the row contains some sub-count for A id, those will be lost forever since A is the source of truth on it's current id. We should thus renew node A id when that happens to avoid this (not unlike we do in cleanup).",,,,,,,,,,,,,,,,14/Jun/11 07:38;slebresne;0001-Don-t-skip-rows-on-scrub-for-counter-CFs.patch;https://issues.apache.org/jira/secure/attachment/12482523/0001-Don-t-skip-rows-on-scrub-for-counter-CFs.patch,10/Jun/11 16:23;slebresne;0001-Renew-nodeId-in-scrub-when-skipping-rows.patch;https://issues.apache.org/jira/secure/attachment/12482074/0001-Renew-nodeId-in-scrub-when-skipping-rows.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-10 16:28:41.075,,,no_permission,,,,,,,,,,,,20810,,,Tue Jun 14 17:02:37 UTC 2011,,,,,,0|i0gda7:,93582,jbellis,jbellis,,,,,,,,,"10/Jun/11 16:23;slebresne;Attached patch against 0.8.

The patch also add a new startup option to renew the node id on startup. This could be useful if someone lose one of it's sstable (because of a bad disk for instance) and don't want to fully decommission that node.

This could arguably be splitted in another ticket though.","10/Jun/11 16:28;jbellis;what is ""renewing a node id?""","10/Jun/11 16:54;slebresne;It's picking a new UUID for the current node to use for new counter increment.

The problem is that on a given node we store deltas for it's current nodeId (to avoid synchronized read-before-write, but I'm starting to wonder is that was the smartest ever). Anyway, if scrub skips a row, it may skip some of those deltas. Let's say at first there is no increments coming for this row for A as 'first distinguished replica'. So far we are still kind of good, because on a read (with CL > ONE) the result coming from A will have a 'version' for it's own sub-count smaller that the one on the other replica, so we will us the sub-count on those replica and return the correct value.

However, as soon as A acknowledge new increments for this row, it will start inserting new deltas while he is not intrinsically up to date. Which will result in an definitive undercount.

The goal of renewing the node id of A is to make sure that second part never happen (because after the renew A will add new deltas as A', not A anymore).

Anyway, now that I've plugged the brain this patch doesn't really works because A will never be repaired by the other nodes of it's now inconsistent value.

So I have no clue how to actually fix that.",10/Jun/11 17:21;slebresne;It may be that the best short fix here is to make scrub *not* skipping row on counter column families (though CASSANDRA-2614 would change that to 'never ever skipping row') and just throw a RuntimeException.,"10/Jun/11 20:10;jbellis;bq. make scrub not skip rows on counter column families

+1

bq. CASSANDRA-2614 would change that to 'never ever skipping row'

Only if you actually did have a counter in the column_metadata, right?","14/Jun/11 07:38;slebresne;Attaching patch to simply re-throw the exception instead of skipping the row for counter column families.

bq. Only if you actually did have a counter in the column_metadata, right?

right.","14/Jun/11 08:50;jbellis;+1

can you add a link to this issue in the ""dangerous"" comment?",14/Jun/11 09:18;slebresne;Committed with suggested comment update.,"14/Jun/11 17:02;hudson;Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during node recovery,CASSANDRA-2766,12510276,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,terjem,terjem,14/Jun/11 07:14,12/Mar/19 14:16,13/Mar/19 22:26,16/Jun/11 17:40,0.8.1,,,,,,0,,,,,"Testing some node recovery operations.

In this case:
1. Data is being added/updated as it would in production
2. repair is running on other nodes in the cluster
3. we wiped data on this node and started up again, but before repair was actually started on this node (but it had gotten data through the regular data feed) we got this error.

I see no indication in the logs that outgoing streams has been started, but the node have finished one incoming stream before this (I guess from some other node doing repair).

 INFO [CompactionExecutor:11] 2011-06-14 14:15:09,078 SSTableReader.java (line 155) Opening /data/cassandra/node1/data/JP/test-g-8
 INFO [CompactionExecutor:13] 2011-06-14 14:15:09,079 SSTableReader.java (line 155) Opening /data/cassandra/node1/data/JP/test-g-10
 INFO [HintedHandoff:1] 2011-06-14 14:15:26,623 HintedHandOffManager.java (line 302) Started hinted handoff for endpoint /1.10.42.216
 INFO [HintedHandoff:1] 2011-06-14 14:15:26,623 HintedHandOffManager.java (line 358) Finished hinted handoff of 0 rows to endpoint /1.10.42.216
 INFO [CompactionExecutor:9] 2011-06-14 14:15:29,417 SSTableReader.java (line 155) Opening /data/cassandra/node1/data/JP/Datetest-g-2
ERROR [Thread-84] 2011-06-14 14:15:36,755 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-84,5,main]
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:132)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
ERROR [Thread-79] 2011-06-14 14:15:36,755 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-79,5,main]
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:132)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
ERROR [Thread-83] 2011-06-14 14:15:36,755 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-83,5,main]
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:132)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
ERROR [Thread-85] 2011-06-14 14:15:36,755 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-85,5,main]
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:132)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
",,,,,,,,,,,,,,,,14/Jun/11 13:24;jbellis;2766-v2.txt;https://issues.apache.org/jira/secure/attachment/12482553/2766-v2.txt,14/Jun/11 09:37;jbellis;2766.txt;https://issues.apache.org/jira/secure/attachment/12482534/2766.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-14 09:37:48.908,,,no_permission,,,,,,,,,,,,20816,,,Thu Jun 16 17:40:35 UTC 2011,,,,,,0|i0gdbr:,93589,slebresne,slebresne,,,,,,,,,"14/Jun/11 09:37;jbellis;looks like a straightforward case of ""not using threadsafe collections means there is no happens-before guarantee where a naive reading of the code would expect one"" (i.e., since buildFutures.add is always called before files.remove, after files.isEmpty is true there should be no more changes to buildFutures.add).

patch attached that changes both of these from Arraylist to CSLS.","14/Jun/11 13:08;muga_nishizawa;Thanks for your quick response.  But we've encountered the following exception.    

ERROR [Thread-18] 2011-06-14 21:54:04,065 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-18,5,main]
java.lang.ClassCastException: org.apache.cassandra.streaming.PendingFile cannot be cast to java.lang.Comparable
        at java.util.concurrent.ConcurrentSkipListMap.comparable(ConcurrentSkipListMap.java:621)
        at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:862)
        at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
        at java.util.concurrent.ConcurrentSkipListSet.add(ConcurrentSkipListSet.java:202)
        at org.apache.cassandra.streaming.StreamInSession.addFiles(StreamInSession.java:100)
        at org.apache.cassandra.streaming.IncomingStreamReader.<init>(IncomingStreamReader.java:49)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
",14/Jun/11 13:24;jbellis;oops. v2 w/ collections that don't require a comparable.,14/Jun/11 14:06;slebresne;lgtm +1 on v2,"14/Jun/11 17:02;hudson;Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])
    use threadsafe collections for StreamInSession
patch by jbellis; reviewed by slebresne for CASSANDRA-2766

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1135611
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/StreamInSession.java
",16/Jun/11 17:40;jbellis;(committed),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to set compaction strategy in cli using create column family command,CASSANDRA-2778,12510510,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,alanliang,alanliang,alanliang,16/Jun/11 00:10,12/Mar/19 14:16,13/Mar/19 22:26,16/Jun/11 20:45,,,,,,,0,,,,,"The following command does not set compaction strategy and its options:
{code}
create column family Standard1
    with comparator = BytesType
    and compaction_strategy = 'org.apache.cassandra.db.compaction.TimestampBucketedCompactionStrategy'
    and compaction_strategy_options = [{max_sstable_size:504857600, retention_in_seconds:60}];
{code}",,,,,,,,,,,,,,,,16/Jun/11 20:32;alanliang;0001-2778-allow-for-dynamic-changes-to-compaction-strateg.patch;https://issues.apache.org/jira/secure/attachment/12482852/0001-2778-allow-for-dynamic-changes-to-compaction-strateg.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-16 20:37:46.492,,,no_permission,,,,,,,,,,,,20822,,,Thu Jun 16 20:37:46 UTC 2011,,,,,,0|i0gdef:,93601,lenn0x,lenn0x,,,,,,,,,16/Jun/11 20:37;lenn0x;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC driver does not build,CASSANDRA-2761,12509880,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,ardot,jbellis,jbellis,10/Jun/11 21:36,12/Mar/19 14:16,13/Mar/19 22:26,25/Aug/11 14:27,,,,Legacy/CQL,,,2,,,,,"Need a way to build (and run tests for) the Java driver.

Also: still some vestigal references to drivers/ in trunk build.xml.

Should we remove drivers/ from the 0.8 branch as well?",,,,,,,,,,,,,,,,14/Jul/11 03:25;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2761-cleanup-nits.txt;https://issues.apache.org/jira/secure/attachment/12486401/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2761-cleanup-nits.txt,13/Jun/11 03:57;ardot;jdbc-driver-build-v1.txt;https://issues.apache.org/jira/secure/attachment/12482319/jdbc-driver-build-v1.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-12 15:04:04.424,,,no_permission,,,,,,,,,,,,20812,,,Thu Aug 25 02:42:17 UTC 2011,,,,,,0|i0gdan:,93584,urandom,urandom,,,,,,,,,"12/Jun/11 15:04;ardot;Moved from CASSANDRA-2754.

Just a few clarifications as I am not that familiar with the C* infrastructure and build policies etc...

- I'm a Maven guy, but I guess that is out of the question?
- This needs to be Jenkins ready?
- This should build Source and Javadocs artifacts ready for Maven Repo deployment right?
- The resulting JDBC jar file would contain the C* dependancies from the build directory pointed to by the properties file/system property? It would be self contained?
- The testing part of the build is going to have to reach into the C* build area too I think?","12/Jun/11 17:41;urandom;bq. I'm a Maven guy, but I guess that is out of the question?

I don't know, but if I were a Maven Guy (I am not), I would use Ant anyway.  The vast majority of the active committers wish that Maven would die in a fire, so using it would be the fastest way to engender apathy. 

bq. This needs to be Jenkins ready?

Not sure what this means.

bq. This should build Source and Javadocs artifacts ready for Maven Repo deployment right?

Yes.

bq. The resulting JDBC jar file would contain the C* dependancies from the build directory pointed to by the properties file/system property? It would be self contained?

Yes.

bq. The testing part of the build is going to have to reach into the C* build area too I think?

The classpath for tests will probably contain a few additional items that the runtime classpath does not.","13/Jun/11 03:57;ardot;OK, Patch for an ANT Build for the JDBC Driver suite is attached. 

It builds all the artifacts, but at the moment it does NOT include any other classes but the ones in the o.a.c.cql.jdbc package.

I will update when a few more clarifications of exactly what dependancies we want to declare for the driver and what we want to include in the jar.

I think I got the Eclipse stuff in ther as well, but that could probably use some wringing out.. I was able to successfully run the junit tests from Eclipse.","13/Jun/11 19:39;urandom;Thanks Rick, this is a good start, and a lot better than having no build at all, so I went ahead and committed it.

I made a fewer minor changes (mostly replacing tabs for spaces), so you'll need to rebase against svn before continuing.

bq. I will update when a few more clarifications of exactly what dependancies we want to declare for the driver and what we want to include in the jar.

There is probably some handy tool for this, but I don't know what it is.  Just looking at it I'd say you need to copy-in:

* {{o.a.c.db.marshall.*}}
* {{o.a.c.utils.ByteBufferUtil}}
* {{o.a.c.config.ColumnDefinition}}
* {{o.a.c.config.CFMetaData}}
* {{o.a.c.config.ConfigurationException}}

There might be some transitive dependencies though.","13/Jun/11 20:08;ardot;Thanks Eric. 

Actually you list is the same as mine so we are probably close. Note you will still need the whole C*-Thrift jar. I will build a new driver with the additional classes and test it without depending on the full C*. I'll document the full dependancy list here along with the next patch.","13/Jun/11 20:12;urandom;bq. Note you will still need the whole C*-Thrift jar.

Yeah, but don't copy those.  The driver jar will need to depend on the thrift jar.",15/Jun/11 15:27;ardot;The v2 patch adds inclusion of classes that are required on the client side from the main C* build. ,"15/Jun/11 20:02;ardot;I put together a patch that covered the obvious but I had strange problems in test. So I dug deeper and found a ton (over 50 I think) additional transitive dependencies that I had missed... (silly me) 

I can try an get them all in but I think we should seriously rethink that strategy for now. With this many dependancies they should probably be put in their own jar(s). It would really make the driver jar have to keep up with detailed dependency changes in the server code base. Miss one and it's messy and the errors are non-obvious.

I was a big whiner about not carrying around the whole server just for client access, but until it is re-factored, I can now appreciate the horror that will commence if we piece-meal drag over classes from the server into the JAR.

So fo now I suggest we keep the jar the way it is with just the o.a.c.cql.jdbc.* classes.","16/Jun/11 16:17;urandom;{quote}
I can try an get them all in but I think we should seriously rethink that strategy for now. With this many dependancies they should probably be put in their own jar(s). It would really make the driver jar have to keep up with detailed dependency changes in the server code base. Miss one and it's messy and the errors are non-obvious.
{quote}

What would you call such a jar?  When I looked at this, there didn't seem to be any delineation that made sense.

{quote}
I was a big whiner about not carrying around the whole server just for client access, but until it is re-factored, I can now appreciate the horror that will commence if we piece-meal drag over classes from the server into the JAR.
{quote}

What are these 50 other class dependencies?  Where are they being drug in?","17/Jun/11 03:30;ardot;{quote}
What are these 50 other class dependencies? Where are they being drug in?
{quote}

- o.a.c.db.marshall.*
-- 23 various classes
- o.a.c.utils.ByteBufferUtil
-- org.apache.cassandra.io.util.FileDataInput
-- org.apache.cassandra.io.util.FileUtils
- o.a.c.config.ColumnDefinition
- o.a.c.config.CFMetaData
-- org.apache.cassandra.cache.IRowCacheProvider;
-- org.apache.cassandra.db.migration.avro.ColumnDef
-- org.apache.cassandra.db.ColumnFamilyType;
--- org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor
--- org.apache.cassandra.config.DatabaseDescriptor
---- org.apache.cassandra.auth.AllowAllAuthenticator
---- org.apache.cassandra.auth.AllowAllAuthority
---- org.apache.cassandra.auth.IAuthenticator
---- org.apache.cassandra.auth.IAuthority
---- org.apache.cassandra.config.Config.RequestSchedulerId
---- org.apache.cassandra.db.ColumnFamilyStore
----- 13 new items
---- org.apache.cassandra.db.ColumnFamilyType
---- org.apache.cassandra.db.DefsTable
----- org.apache.cassandra.config.DatabaseDescriptor;
----- org.apache.cassandra.config.KSMetaData;
----- org.apache.cassandra.db.filter.QueryFilter;
----- org.apache.cassandra.db.filter.QueryPath;
----- org.apache.cassandra.db.migration.Migration;
----- org.apache.cassandra.io.SerDeUtils;
----- org.apache.cassandra.service.StorageService;
----- org.apache.cassandra.utils.ByteBufferUtil;
----- org.apache.cassandra.utils.UUIDGen;
---- org.apache.cassandra.db.migration.Migration
---- org.apache.cassandra.dht.IPartitioner
---- org.apache.cassandra.io.sstable.Descriptor
---- org.apache.cassandra.io.util.FileUtils
---- org.apache.cassandra.locator.*
---- org.apache.cassandra.scheduler.IRequestSchedule;
---- org.apache.cassandra.scheduler.NoScheduler
--- org.apache.cassandra.db.compaction.CompactionManager
--- org.apache.cassandra.db.filter.QueryFilter
--- org.apache.cassandra.db.filter.QueryPath
--- org.apache.cassandra.dht.IPartitioner
--- org.apache.cassandra.dht.Range
--- org.apache.cassandra.gms.FailureDetector
--- org.apache.cassandra.gms.Gossiper
--- org.apache.cassandra.gms.ApplicationState
--- org.apache.cassandra.net.MessagingService
---- 10 new classes
--- org.apache.cassandra.service.*
--- org.apache.cassandra.utils.WrappedRunnable
-- org.apache.cassandra.db.HintedHandOffManager;
-- org.apache.cassandra.db.SystemTable;
-- org.apache.cassandra.db.Table;
-- org.apache.cassandra.db.ColumnFamilyStore;
-- org.apache.cassandra.db.migration.Migration;
-- org.apache.cassandra.db.compaction.AbstractCompactionStrategy;
-- org.apache.cassandra.io.SerDeUtils;
-- org.apache.cassandra.utils.Pair;
- o.a.c.config.ConfigurationException


The point is there are lots and they are scattered all over the various packages; It will be very difficult to manage when they change from the driver package (client side), which is supposed to be able to change independent of the server code. If a subset of the server code is to be a dependency then that subset (jar/s) must be managed in the main build not the driver build. 


{quote}
What would you call such a jar? When I looked at this, there didn't seem to be any delineation that made sense.
{quote}

I agree it is not any clear set of packages. They are scattered all over.

As to a name for the jar... I'm not a good namer in the best of circumstances but I think the intent is to pick those files that are used in common between client and server. I guess I'd use that as the basis for the name.


","17/Jun/11 15:31;urandom;{quote}
The point is there are lots and they are scattered all over the various packages; It will be very difficult to manage when they change from the driver package (client side), which is supposed to be able to change independent of the server code. If a subset of the server code is to be a dependency then that subset (jar/s) must be managed in the main build not the driver build.
{quote}

Right, I was curious to see the list of classes (that list is fantastic btw, thanks for that), to see if there was one point in the graph where breaking a dependency would drastically change the scope of the problem.  It looks like the answer is Yes, and the dependency is {{o.a.c.config.CFMetaData}}, (needed by {{ColumnDecoder}}).

Just skimming through the code, I don't think it would be hard to either re-implement the needed parts of CFMetaData, or refactor CFMetaData to limit what it pulled in.","05/Jul/11 19:13;jbellis;What's the status here?  Not having ""ant test"" for the drivers anymore is causing me pain and suffering. :(",05/Jul/11 20:16;ardot;Sorry. I didn't catch that there was no ant task to RUN them... :) {{build-test}} builds the included tests just fine. And I have been running them from Eclipse once built. I'll look into it. ,"14/Jul/11 03:29;urandom;Attached (v1-0001-CASSANDRA-2761-cleanup-nits.txt) is a patch to do a little cleanup (clearer variable naming, some simplification of classpaths, etc), basically stuff that set my OCD off and seemed better to get in *before* diving into test running and dependency management.","21/Jul/11 22:20;urandom;To summarize, it is now possible to build and test w/ ant.  This is currently done by pointing to a local (built) working copy of Cassandra (a site config).  What's left, and seems reasonable to scope with this issue:

* Create an alternate mechanism for specifying the version of Cassandra to build/test against (in order to run the tests against prior releases).  I'm thinking Ivy could be used here to automatically download artifacts when a property is passed (-Dcassandra.release=0.8.0 for example).
* (Re)build Cassandra as needed from the drivers Ant build, or at the very least, handle the case when a build is needed.
* Fix the {{generate-eclipse-files}} target if possible, or remove it otherwise.

Work should also continue to reduce the cross-section of Cassandra that this driver depends on, but I'll open another issue for that.",22/Jul/11 00:27;jbellis;+1 cleanup patch,"22/Jul/11 02:07;ardot;+1 for the cleanup patch

The {{generate-eclipse-files}} seems to be working for me? How does it fail?

","10/Aug/11 15:35;jbellis;Note that CASSANDRA-3010 makes moving drivers out-of-""tree"" even sillier: as things stand, we'll need to grab cassandra from the maven repo to build jdbc, then back in tree we'll need to grab jdbc from the maven repo to build the cql shell.

I'm all for purity but at this point I'm ready to just put things back the way they were* so we can spend time before the 1.0 freeze building things instead of wrestling with maven.
","10/Aug/11 15:53;urandom;The lib/ directory is full of dependency jars, some which exist solely for the cli.  ","10/Aug/11 16:01;brandon.williams;bq. I'm all for purity but at this point I'm ready to just put things back the way they were* so we can spend time before the 1.0 freeze building things instead of wrestling with maven.

I for one would be delighted if I didn't have to use svn to get at the drivers or cqlsh.","10/Aug/11 16:04;jbellis;bq. The lib/ directory is full of dependency jars, some which exist solely for the cli.

But we are updating none of those on a daily or weekly basis.","10/Aug/11 16:41;urandom;bq. But we are updating none of those on a daily or weekly basis

I certainly hope we're not updating the JDBC driver that often, particularly in ways that would impact a simple application like a shell.

Our experience using the JDBC driver as an application dependency shouldn't be any better or worse than it would be for other users.","10/Aug/11 17:30;jbellis;bq. I certainly hope we're not updating the JDBC driver that often

Then you're not very familiar with the current stability or lack there of of the JDBC driver. :)","10/Aug/11 20:53;urandom;bq. Then you're not very familiar with the current stability or lack there of of the JDBC driver. 

No, I guess not, but it can't be any worse on us than for anyone else using it.  A solution that only benefits us doesn't seem very... friendly.

----

I think the difference in opinion here comes from what determines whether the server and driver should be considered different projects.  I can see where people who feel a high degree of ownership over the code of both, and who by virtue of infrastructure have write access to both, might consider it contrived to treat them as separate projects.  I'm not discounting that point of view, but I do think that's more Social than Technical, and is limited to a relatively small group of Cassandra hackers.

As I've said elsewhere, I think it is very important that these drivers be allowed to evolve on their own release schedule with their own versioning that reflects compatibility with a CQL version and not any particular Cassandra version(s).  It's also quite likely, particularly where the driver language != Java that the group of developers is entirely different from those working on the server.  And there is no hard dependency between drivers and Cassandra in either direction (the JDBC->Cassandra dependency is one of convenience).  To me, this pretty solidly points to them being separate projects.

Keeping them separate won't be as convenient as treating them as one monolithic project, but it's no worse an experience than what other application developers are subjected to.  We should be able to eat our own dog food.

I also realize that there is more work needed here to decouple the JDBC driver and make this all work better, work that I've volunteered to do.  I haven't had as much time to spend on this lately, but that should be changing RSN.

",24/Aug/11 13:17;tjake;Is their someway to get a git repo for just drivers? like cassandra.git but cassandra-drivers.git? This is causing major pain.,"24/Aug/11 18:19;jbellis;bq. I also realize that there is more work needed here to decouple the JDBC driver and make this all work better, work that I've volunteered to do. I haven't had as much time to spend on this lately, but that should be changing RSN.

It's been another two weeks and we're still in this no-man's land where git can't see drivers, the jdbc build is a big TODO, and anything that depends on jdbc like 3010 is basically SOL.

Again, I'm not against purity, but it's clear that the original breaking out of drivers/ was done prematurely (was there even a Jira ticket with a patch? It looks like we went from ""that's a good idea"" on -dev to ripping apart svn).  I've reverted things until the breakout can be done properly.","24/Aug/11 19:20;hudson;Integrated in Cassandra #1045 (See [https://builds.apache.org/job/Cassandra/1045/])
    move drivers back in-tree until build issues can be fixed.  see CASSANDRA-2761

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1161216
Files : 
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
* /cassandra/trunk/build.xml
* /cassandra/trunk/contrib
* /cassandra/trunk/drivers/java/README.txt
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
* /cassandra/trunk/drivers
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
* /cassandra/trunk
* /cassandra/trunk/drivers/java/build.properties.default
* /cassandra/trunk/drivers/java/build.xml
",24/Aug/11 19:56;ardot;I don't see any driver code in the move? The {{/driver}} sub-directory now exists but there is nothing underneath?,24/Aug/11 20:10;jbellis;https://svn.apache.org/repos/asf/cassandra/trunk/drivers/,"24/Aug/11 20:43;urandom;{quote}
It's been another two weeks and we're still in this no-man's land where git can't see drivers
{quote}

So does this mean you've decided Git is hard a requirement?  Our project is (unfortunately )managed in SVN.  You know if it were up to me we'd be using Git for source control, but it's not up to me and we don't.

{quote}
...anything that depends on jdbc like 3010 is basically SOL.
{quote}

As I've already mentioned, CASSANDAR-3010 is for a JDBC using _application_.  If it's SOL unless the driver is embedded in the tree, then so is every other application that would make use of the driver.  If the driver is so bad, maybe no one should be using it.  Why are we giving ourselves special treatment?

{quote}
Again, I'm not against purity...
{quote}

By saying ""purity"" it sounds like you're dismissing it as something that's based on aesthetics.  I assure that's not the case, it's about avoiding the inevitable lock-step relationship release-wise. 

{quote}
...but it's clear that the original breaking out of drivers/ was done prematurely (was there even a Jira ticket with a patch?
{quote}

There was a Jira yes, though I'm pretty sure it done as part of larger set of tasks, something with a description other than ""relocate drivers out of tree"".  And, I don't think it was in a patch, no, due to the fact that a large move with some minor changes was (a) straightforward, and (b) would have needed rebasing several times a day.

{quote}
It looks like we went from ""that's a good idea"" on -dev to ripping apart svn). I've reverted things until the breakout can be done properly.
{quote}

So all I need to do is find a ticket and a +1 and I can -1 this, and revert your revert?
","24/Aug/11 21:00;tjake;bq.  Our project is (unfortunately )managed in SVN

putting a sub-project in a svn root isn't the way to do it for SVN.

Another issue is the drivers build depends on cassandra.  That makes this even more strange. Can't drivers live in /trunk and still have it's own releases?

","24/Aug/11 21:13;jbellis;bq. There was a Jira yes

I couldn't find one, and the svn commit messages didn't mention it.  I still can't find one.

bq. So all I need to do is find a ticket and a +1 and I can -1 this, and revert your revert?

The point of making a ticket with a patch (or a shell script if you're throwing svn refactoring around -- you can do mv's from a working copy, not just on the repository, btw) is that people can review it and see if the result matches what they thought they were going to get out of it.  In this case it's clear that it didn't, and we ended up with making things worse in exchange for a promise to clean it up eventually.

(Of course, even with proper review, sometimes we've needed to revert things after unexpected problems arose.  But skipping the review makes that more likely.)
 
It's been *over two months.*  So let's reboot, and do it right.  Note that this time around I got everything in one commit (well, one for trunk, and one for drivers) so ""reverting the revert"" will be easy.","24/Aug/11 21:16;urandom;bq. putting a sub-project in a svn root isn't the way to do it for SVN.

What is?

bq. Another issue is the drivers build depends on cassandra. That makes this even more strange. Can't drivers live in /trunk and still have it's own releases?

We had it that way originally.  It made releasing a pain, and left people with the expectation that the driver version to use was the one that corresponded to source it was with (which is unavoidable).","24/Aug/11 21:17;jbellis;bq. Can't drivers live in /trunk and still have it's own releases?

Right, that seems like the best interim solution to me.  (I thought we could even delete it from old branches to make it clear that it's not in lockstep with the rest of the tree, but Eric pointed out that if something like 3010 is depending on it, we can't do that.  Still, having it present but ""frozen"" in the branches feels like a relatively minor downside.)",24/Aug/11 21:22;urandom;How do we reboot and do it right?  Your requirements as I understand them are structured in such a way that keeping them in-tree is the only solution.  ,"24/Aug/11 21:22;jbellis;bq. left people with the expectation that the driver version to use was the one that corresponded to source it was with

I suspect this was when svn was effectively the only way to get a driver.  My experience is that even most people building ""from source"" use a tarball, not svn.  So the combination of making drivers available on maven, PyPI, etc., with their own version numbers, should address this.","24/Aug/11 21:24;urandom;bq. Right, that seems like the best interim solution to me. (I thought we could even delete it from old branches to make it clear that it's not in lockstep with the rest of the tree, but Eric pointed out that if something like 3010 is depending on it, we can't do that. Still, having it present but ""frozen"" in the branches feels like a relatively minor downside.)

Except that people's expectation will always be that the version they need is the one that came with their software (which will likely be something pre-release).  It is completely unrealistic to expect folks to just Know Better here. ","24/Aug/11 21:38;jbellis;bq. people's expectation will always be that the version they need is the one that came with their software

I don't understand.  What version will ""come with their [server]?""

Consider postgresql: the server is distributed on postgresql.org, and psycopg is distributed over PyPI.  Nobody gets confused about staying on an obsolete version of psycopg.

That's the model I see us moving towards.  (As you know, we recently got the Python cql driver on PyPI.)","24/Aug/11 21:52;jbellis;One final thought: by coincidence, we broke the JDBC build again today with CASSANDRA-3039.  This isn't the first time this has happened.  It's a minor win but not negligible to catch those before commit because ""ant test"" runs both suites.","24/Aug/11 22:07;urandom;{quote}
I don't understand. What version will ""come with their [server]?""
{quote}

Wherever we publish the source, be it an SVN checkout with an SVN revision ID, a date-based development snapshot, or a full-on release artifact, if there is driver source contained within then people are going to be encouraged to think of those drivers as being the same version as the node (e.g. 0.8.9).  They're also going to be encouraged to think that those drivers are the best choice to use with the corresponding node.  It's futile to think they won't, and having other vectors (www.a.o, PyPI, etc), will only add to the confusion. 

{quote}
Consider postgresql: the server is distributed on postgresql.org, and psycopg is distributed over PyPI. Nobody gets confused about staying on an obsolete version of psycopg.

That's the model I see us moving towards. (As you know, we recently got the Python cql driver on PyPI.)
{quote}

You make an excellent point.  Pyscopg is maintained in a completely different repository (git://luna.dndg.it/public/psycopg2.git) than Postgesql (git://git.postgresql.org/git/postgresql.git) and is released (and published) separately, (which is in fact consistent with best practice elsewhere).","24/Aug/11 22:12;urandom;bq. One final thought: by coincidence, we broke the JDBC build again today with CASSANDRA-3039. This isn't the first time this has happened. It's a minor win but not negligible to catch those before commit because ""ant test"" runs both suites.

Broke why?  Because of Cassandra code that the driver depends on?  That would be an argument in favor of stabalizing the dependent code.

And, the inverse of this is that the drivers should ultimately be getting tested against trunk, each active branch, and all past released versions (limited of course to CQL availability).  That is only going to be practical through CI, and is made harder by your monolithic approach.","24/Aug/11 22:19;jbellis;bq. if there is driver source contained

Almost all non-developers get the source from a release tarball, not svn.  Do we publish drivers/ in the source tarballs?  If so that's easy enough to fix.

Anyone actually using svn I'm willing to educate.  You can give them my email. :)

bq. Pyscopg is maintained in a completely different repository

But as far as users are concerned that is irrelevant.

If you want examples of drivers in the same tree that are also not causing confusion, I can point you to https://github.com/mongodb/mongo and https://github.com/mongodb/mongo/tree/master/client.  Also http://hg.basho.com/riak/src/5ffa6ae7e699 (http://hg.basho.com/riak/src/5ffa6ae7e699/client_lib/) and https://github.com/voldemort/voldemort (https://github.com/voldemort/voldemort/tree/master/clients).","24/Aug/11 22:44;urandom;So to summarize: You've decided.

I know that sounds a bit snarky, but you summarily reverted the change, in part based on reasoning that wasn't true (it doesn't build), and in part pending a ""reboot"" to meet unmeetable requirements.","25/Aug/11 02:27;jbellis;If waiting for two months of ""we'll fix it real soon now"" is ""summarily,"" then yeah, I guess guilty as charged.

But it looks like you've restarted discussion on -dev, so I'll move there.","25/Aug/11 02:42;urandom;Up until the July 21st, the scope of the ticket was building and running the unit tests (and as of July 21st that much was working).  It wasn't until Aug 10th (and the creation of CASSANDRA-3010) that the discussion (and scope) changed.  Between the 10th and today there was no response to my last, then 4 hours after Jake's comment the revert was made.

I already told you once today that I ascribed no malice in this, and I don't, but I think ""summarily"" is a fair assessment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when writing SSTable generated via repair,CASSANDRA-2863,12512969,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,hector,hector,06/Jul/11 11:32,12/Mar/19 14:16,13/Mar/19 22:26,10/Oct/11 13:58,0.8.8,,,,,,1,,,,,"A NPE is generated during repair when closing an sstable generated via SSTable build. It doesn't happen always. The node had been scrubbed and compacted before calling repair.

 INFO [CompactionExecutor:2] 2011-07-06 11:11:32,640 SSTableReader.java (line 158) Opening /d2/cassandra/data/sbs/walf-g-730
ERROR [CompactionExecutor:2] 2011-07-06 11:11:34,327 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[CompactionExecutor:2,1,main] 
java.lang.NullPointerException
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
	at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1103)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1094)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",,,,,,,,,,,,,,,,10/Oct/11 13:07;slebresne;2863-v2.patch;https://issues.apache.org/jira/secure/attachment/12498418/2863-v2.patch,10/Oct/11 11:07;slebresne;2863.patch;https://issues.apache.org/jira/secure/attachment/12498412/2863.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-21 12:28:09.977,,,no_permission,,,,,,,,,,,,20870,,,Mon Oct 10 14:26:54 UTC 2011,,,,,,0|i0gdx3:,93685,jbellis,jbellis,,,,,,,,,"08/Jul/11 21:22;hector;I don't now if it's the same one, buy I got another during repair on another node:

ERROR [Thread-1710] 2011-07-08 21:21:00,514 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-1710,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:154)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:162)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:95)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:138)
	... 3 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
	at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1103)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1094)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
","21/Jul/11 12:28;slebresne;I'm a little bit baffled by that one. Trusting the stack trace, apparently when SSTW.RowIndexer.close() is called, the iwriter field is null. But iwriter is set in prepareIndexing() that is called the line before index() in SSTW.Builder. Thus if an exception happens in prepareIndexing, we shouldn't arrive to the index() method (which is the one triggering the close()). And looking at the use of iwriter, no other line set it (so it can't be set back to null after prepareIndexing()).

So I mean we can add a {{if (iwriter != null)}} before calling the close, but the truth is I have no clue how it could ever be null at that point.

Héctor: are you positive that you are using stock 0.8.1 ?","21/Jul/11 12:35;hector.izquierdo;I have a patch from CASSANDRA-2818 (2818-v4) applied, if that's of any help. The patch only touches messaging classes though.","21/Jul/11 20:13;jbellis;Doesn't make any sense to me, either.  The only place close() is called is from index() [as seen in the stacktrace here] and the only place index() is called is after prepareIndexing, which sets iwriter to non-null:

{code}
            long estimatedRows = indexer.prepareIndexing();

            // build the index and filter
            long rows = indexer.index();
{code}
","20/Sep/11 23:19;cce;I just got a similar-looking NPE stack trace.  The node that raised the exception was receiving streams from a node being decommissioned (with ""nodetool decommission"").  Both nodes were running 0.8.6; both had been upgraded a few hours earlier from 0.8.4.

The first:  

ERROR [CompactionExecutor:72] 2011-09-20 22:34:20,892 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:72,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Then half a minute later:

 INFO [CompactionExecutor:72] 2011-09-20 22:34:46,923 SSTableReader.java (line 162) Opening /mnt/cassandra/data/Keyspace/CF1-g-1536
ERROR [Thread-785] 2011-09-20 22:34:52,054 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thread-785,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:154)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:189)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:117)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:138)
        ... 3 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
","21/Sep/11 14:58;cce;So I restarted the node making the NPE above and ran the decommission again (on the other node) and it successfully finished decommissioning.  A few hours later I tried to decommission another node and received this error, on a separate node from the two mentioned before, which is a new node running 0.8.6 for just about a day now.

INFO [COMMIT-LOG-WRITER] 2011-09-21 05:12:46,361 CommitLogSegment.java (line 58) Creating new commitlog segment /raid0/cassandra/commitlog/CommitLog-1316581966361.log
ERROR [Thread-224] 2011-09-21 05:12:52,738 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thread-224,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:154)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:189)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:117)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:138)
        ... 3 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
","23/Sep/11 15:17;cce;Since my last two comments, I increased my RF and started a rolling repair on my nodes.  This has caused this NPE to pop up on all the boxes over the last couple of days as they process SSTables.  Again, all the nodes are fresh 0.8.6 installs from a few days ago using the ComboAMI.  I've seen backtraces like the one below appear at least a couple of times on each node in my cluster as I was repairing..

 INFO [CompactionExecutor:648] 2011-09-22 04:35:51,086 SSTableReader.java (line 162) Opening /raid0/cassandra/data/Keyspace/CF1-g-535
 INFO [CompactionExecutor:648] 2011-09-22 04:35:51,172 SSTableReader.java (line 162) Opening /raid0/cassandra/data/Keyspace/CF2-g-350
 INFO [CompactionExecutor:648] 2011-09-22 04:36:01,721 SSTableReader.java (line 162) Opening /raid0/cassandra/data/Keyspace/CF3-g-456
ERROR [Thread-3658] 2011-09-22 04:36:04,821 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thread-3658,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:154)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:189)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:117)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:138)
        ... 3 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [CompactionExecutor:648] 2011-09-22 04:36:04,823 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:648,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
","23/Sep/11 15:20;cce;Here's another from a node that was streaming out:


 INFO [AntiEntropyStage:1] 2011-09-22 17:56:33,737 StreamOut.java (line 181) Stream context metadata [/raid0/cassandra/data/Keyspace/CF1-g-315-Data.db sections=1073 progress=0/358
 INFO [AntiEntropyStage:1] 2011-09-22 17:56:33,738 StreamOutSession.java (line 174) Streaming to /10.207.38.196
 INFO [ValidationExecutor:14] 2011-09-22 17:56:33,740 ColumnFamilyStore.java (line 1128) Enqueuing flush of Memtable-CF4@516894197(9505947/21294113 serialized/live bytes, 5684 ops)
 INFO [FlushWriter:621] 2011-09-22 17:56:33,743 Memtable.java (line 237) Writing Memtable-CF4@516894197(9505947/21294113 serialized/live bytes, 5684 ops)
 INFO [FlushWriter:621] 2011-09-22 17:56:33,880 Memtable.java (line 254) Completed flushing /raid0/cassandra/data/Keyspace/CF4-g-434-Data.db (9914202 bytes)
 INFO [CompactionExecutor:884] 2011-09-22 17:56:45,912 CompactionManager.java (line 608) Compacted to /raid0/cassandra/data/Keyspace/CF4-tmp-g-332-Data.db.  32,824,572,147 to 18,506,0
ERROR [CompactionExecutor:884] 2011-09-22 17:56:46,134 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:884,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
","23/Sep/11 21:12;alienth;Getting the following after trying to 'move' on 0.8.5:

{code}
ERROR [CompactionExecutor:213] 2011-09-23 14:02:26,571
AbstractCassandraDaemon.java (line 139) Fatal exception in thread
Thread[CompactionExecutor:213,1,main]
java.lang.NullPointerException
       at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
       at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
       at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
       at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
       at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
{code}","24/Sep/11 05:25;alienth;Also got the same NPE when doing a repair on a different node. This time, the NPE was preceded by a different NPE:


{code}
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:154)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:177)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:114)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:138)
        ... 3 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

{code}
",07/Oct/11 17:40;j.casares;We've seen the same error via autobootstrap.,"07/Oct/11 18:01;jbellis;Joaquin, can you give steps to reproduce?","10/Oct/11 11:07;slebresne;Alright, I've tried boostrapping nodes a few times and I'm still not able to reproduce, so  it's likely a race of something.

That being said, I still have no clue how the iwriter field in SSTableWriter.RowIndexer can be null where the stack indicates it to be null. The assignment happens before the access, there is no other assignment of that field and the assignment/access are in the same thread.

Anyway, what we can easily do is to make iwrite being final and assign it in the constructor. If that doesn't make that field being non-null, I don't know what will. Patch attached to make that change.","10/Oct/11 13:07;slebresne;First patch was buggy in that it was now creating the index file *before* we assert that this file doesn't exist.

v2 just moves the maybeOpenIndexer call after the asserts.",10/Oct/11 13:12;jbellis;+1 v2,"10/Oct/11 13:59;slebresne;Committed and marking resolved for now. If that errors come up in another form, we'll see then.","10/Oct/11 14:26;hudson;Integrated in Cassandra-0.8 #366 (See [https://builds.apache.org/job/Cassandra-0.8/366/])
    Make SSTW.RowIndexer.iwriter a final field to avoid NPE
patch by slebresne; reviewed by jbellis for CASSANDRA-2863

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1180958
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Starting 0.8.1 after upgrade from 0.7.6-2 fails,CASSANDRA-2867,12513118,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,kunda,kunda,07/Jul/11 10:20,12/Mar/19 14:16,13/Mar/19 22:26,02/Aug/11 16:05,0.8.3,,,,,,3,exception,index,starting,,"After upgrading the binaries to 0.8.1 I get an exception when starting cassandra:

{noformat}
[root@bserv2 local]#  INFO 12:51:04,512 Logging initialized
 INFO 12:51:04,523 Heap size: 8329887744/8329887744
 INFO 12:51:04,524 JNA not found. Native methods will be disabled.
 INFO 12:51:04,531 Loading settings from file:/usr/local/apache-cassandra-0.8.1/conf/cassandra.yaml
 INFO 12:51:04,621 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 12:51:04,707 Global memtable threshold is enabled at 2648MB
 INFO 12:51:04,708 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,713 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,714 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,716 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,717 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,719 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,770 reading saved cache /vm1/cassandraDB/saved_caches/system-IndexInfo-KeyCache
 INFO 12:51:04,776 Opening /vm1/cassandraDB/data/system/IndexInfo-f-9
 INFO 12:51:04,792 reading saved cache /vm1/cassandraDB/saved_caches/system-Schema-KeyCache
 INFO 12:51:04,794 Opening /vm1/cassandraDB/data/system/Schema-f-194
 INFO 12:51:04,797 Opening /vm1/cassandraDB/data/system/Schema-f-195
 INFO 12:51:04,802 Opening /vm1/cassandraDB/data/system/Schema-f-193
 INFO 12:51:04,811 Opening /vm1/cassandraDB/data/system/Migrations-f-193
 INFO 12:51:04,814 reading saved cache /vm1/cassandraDB/saved_caches/system-LocationInfo-KeyCache
 INFO 12:51:04,815 Opening /vm1/cassandraDB/data/system/LocationInfo-f-292
 INFO 12:51:04,843 Loading schema version 586e70fd-a332-11e0-828e-34b74a661156
ERROR 12:51:04,996 Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 15
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.config.CFMetaData.getDefaultIndexName(CFMetaData.java:971)
        at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:381)
        at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:172)
        at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:479)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:139)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:315)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 15
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.config.CFMetaData.getDefaultIndexName(CFMetaData.java:971)
        at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:381)
        at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:172)
        at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:479)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:139)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:315)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
{noformat}

It seems this has something to do with indexes, and I do have a CF with an index on it, but it is not used.
I can try and remove the index with 0.7.x binaries, but I will wait a bit to see if anyone needs it to reproduce the bug.",CentOS 5.6,,,,,,,,,,,,,,,31/Jul/11 04:15;jbellis;2867-v2.txt;https://issues.apache.org/jira/secure/attachment/12488340/2867-v2.txt,26/Jul/11 13:48;tarasp;trunk-2867.txt;https://issues.apache.org/jira/secure/attachment/12487834/trunk-2867.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-23 20:18:48.344,,,no_permission,,,,,,,,,,,,20871,,,Tue Aug 02 16:36:05 UTC 2011,,,,,,0|i0gdxr:,93688,slebresne,slebresne,,,,,,,,,"23/Jul/11 20:18;andrden;upgrade from 0.8 to 0.8.1 
(update: tried daily trunc build apache-cassandra-2011-07-20_16-02-23 with the same effect)
 - just put new libararies and started cassandra (CentOS 5.6 64bit)
We are not using any CF indexes (only counters if that matters), 
but it won't start, so I don't see a way to upgrade to 0.8.1:

{code}
 INFO [main] 2011-07-23 19:32:23,446 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/NodeIdInfo-g-1
 INFO [main] 2011-07-23 19:32:23,516 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/Schema-g-74
 INFO [main] 2011-07-23 19:32:23,529 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/Schema-g-73
 INFO [main] 2011-07-23 19:32:23,548 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/Migrations-g-73
 INFO [main] 2011-07-23 19:32:23,561 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/Migrations-g-74
 INFO [main] 2011-07-23 19:32:23,570 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/LocationInfo-g-46
 INFO [main] 2011-07-23 19:32:23,587 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/LocationInfo-g-45
 INFO [main] 2011-07-23 19:32:23,597 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/HintsColumnFamily-g-1
 INFO [main] 2011-07-23 19:32:23,656 DatabaseDescriptor.java (line 478) Loading schema version 1e26a500-b538-11e0-0000-f03fd2ec87ff
ERROR [main] 2011-07-23 19:32:23,889 AbstractCassandraDaemon.java (line 332) Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 4
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.config.CFMetaData.getDefaultIndexName(CFMetaData.java:971)
        at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:381)
        at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:172)
        at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:479)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:139)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:315)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
 {code}","24/Jul/11 20:56;jbellis;You have an index name set on at least one CF.  If you don't actually have an index on it, Cassandra shouldn't have let you set the index name, but it's possible an older version was not as rigorous about that.

If you can't figure out where the index name is, you can also drop your schema and migrations system CFs and rebuild your schema from scratch.",26/Jul/11 13:48;tarasp;The patch trunk-2867.txt fixes the MarshalException on startup.,"26/Jul/11 14:35;jbellis;Hmm, you have an index name on a subcolumn?  that's not supposed to be legal either :)","27/Jul/11 10:29;tarasp;No, just create the following column family and restart the server:

 create column family MailStat
  with column_type = Super
    and key_validation_class = UTF8Type
    and comparator = LongType
    and subcomparator = AsciiType
    and column_metadata = [
        {column_name : subj, validation_class : UTF8Type},
        {column_name : body, validation_class : UTF8Type},
        {column_name : comment, validation_class : UTF8Type},
        {column_name : categ, validation_class : AsciiType}
    ];
","27/Jul/11 10:32;tarasp;This problem is not related to migration, just 0.8.1 - 0.8.2 cannot start when there is a super column family with different comparators for supercolumns and metadata.","28/Jul/11 08:06;redpriest;Thanks Taras for commenting on the issue, I just ran into the same problem with a super column family with a TimeUUIDType comparator, and UTF8Type subcomparators; I can insert, get data all day long from the cluster while it is running fresh. But the moment I restart the cassandra node, I get the exception saying that TimeUUIDTypes must be exactly 16 bytes.

Edit: Just verified your fix and it fixes my case as well.","31/Jul/11 04:15;jbellis;Thanks, I see the problem now -- it was generating an index name even if there was no index present on the column (which is always the case for supercolumns).

Alternate patch attached to fix this.","02/Aug/11 15:27;slebresne;The attached patch has a lot of unrelated changes I believe, but considering the fix is the one line change in CFMetadata, then +1.","02/Aug/11 16:05;jbellis;committed, minus the extra stuff from CASSANDRA-2894 that was in the diff","02/Aug/11 16:36;hudson;Integrated in Cassandra-0.8 #252 (See [https://builds.apache.org/job/Cassandra-0.8/252/])
    avoid trying to create index names, when no index exists
patch by jbellis; reviewed by slebresne for CASSANDRA-2867

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1153175
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage does not function properly when used multiple times in a single pig script due to UDFContext sharing issues,CASSANDRA-2869,12513172,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jeromatron,gsingers,gsingers,07/Jul/11 18:26,12/Mar/19 14:16,13/Mar/19 22:26,21/Jul/11 20:11,0.7.9,0.8.2,,,,,0,,,,,"CassandraStorage appears to have threading issues along the lines of those described at http://pig.markmail.org/message/oz7oz2x2dwp66eoz due to the sharing of the UDFContext.

I believe the fix lies in implementing
{code}
public void setStoreFuncUDFContextSignature(String signature)
    {
    }
{code}

and then using that signature when getting the UDFContext.

From the Pig manual:
{quote}
setStoreFunc!UDFContextSignature(): This method will be called by Pig both in the front end and back end to pass a unique signature to the Storer. The signature can be used to store into the UDFContext any information which the Storer needs to store between various method invocations in the front end and back end. The default implementation in StoreFunc has an empty body. This method will be called before other methods.
{quote}",,,,,,,,,,,,,,,,13/Jul/11 14:29;jeromatron;2869-2.txt;https://issues.apache.org/jira/secure/attachment/12486314/2869-2.txt,12/Jul/11 04:31;jeromatron;2869.txt;https://issues.apache.org/jira/secure/attachment/12486144/2869.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-12 04:30:38.157,,,no_permission,,,,,,,,,,,,20873,,,Thu Jul 21 20:30:48 UTC 2011,,,,,,0|i0gdy7:,93690,brandon.williams,brandon.williams,,,,,,,,,"12/Jul/11 04:30;jeromatron;Simple patch to use the load and store signatures instead of the udf context property keys we had been using.  We're running this in our data pipeline and appears to work correctly.  However, I haven't found evidence that the old way wasn't working - that seems to be more related to read consistency level we were using.  But, this is probably the way we should be doing it, as it appears to be the Pig approach.  Also there could be some corner cases that might trip up the current approach.","12/Jul/11 16:57;brandon.williams;Looks like we can remove UDFCONTEXT_SCHEMA_KEY_PREFIX now too, no?",12/Jul/11 21:30;jeromatron;Yes. I was about to post an updated patch last night but got sidetracked. Do you mind removing that if it's otherwise good to go? Otherwise I can do that later today.,13/Jul/11 14:29;jeromatron;Removed that String.  Also removed adding mutation twice and put in the nested exception in putNext into the IOException.  We've been meaning to add those last two items to one of these tickets.,21/Jul/11 20:11;brandon.williams;Committed,"21/Jul/11 20:30;hudson;Integrated in Cassandra-0.7 #534 (See [https://builds.apache.org/job/Cassandra-0.7/534/])
    Use a UDF-specific context signature.
Patch by Jeremy Hanna, reviewed by brandonwilliams for CASSANDRA-2869

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1149341
Files : 
* /cassandra/branches/cassandra-0.7/contrib/pig/src/java/org/apache/cassandra/hadoop/pig/CassandraStorage.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RuntimeException in Pig when using ""dump"" command on column name",CASSANDRA-2810,12511243,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,silvere,silvere,22/Jun/11 14:49,12/Mar/19 14:16,13/Mar/19 22:26,28/Sep/11 22:01,0.8.7,,,,,,1,,,,,"This bug was previously report on [Brisk bug tracker|https://datastax.jira.com/browse/BRISK-232].

In cassandra-cli:
{code}
[default@unknown] create keyspace Test
    with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
    and strategy_options = [{replication_factor:1}];

[default@unknown] use Test;
Authenticated to keyspace: Test

[default@Test] create column family test;

[default@Test] set test[ascii('row1')][long(1)]=integer(35);
set test[ascii('row1')][long(2)]=integer(36);
set test[ascii('row1')][long(3)]=integer(38);
set test[ascii('row2')][long(1)]=integer(45);
set test[ascii('row2')][long(2)]=integer(42);
set test[ascii('row2')][long(3)]=integer(33);

[default@Test] list test;
Using default limit of 100
-------------------
RowKey: 726f7731
=> (column=0000000000000001, value=35, timestamp=1308744931122000)
=> (column=0000000000000002, value=36, timestamp=1308744931124000)
=> (column=0000000000000003, value=38, timestamp=1308744931125000)
-------------------
RowKey: 726f7732
=> (column=0000000000000001, value=45, timestamp=1308744931127000)
=> (column=0000000000000002, value=42, timestamp=1308744931128000)
=> (column=0000000000000003, value=33, timestamp=1308744932722000)

2 Rows Returned.

[default@Test] describe keyspace;
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
{code}
In Pig command line:
{code}
grunt> test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS (rowkey:chararray, columns: bag {T: (name:long, value:int)});

grunt> value_test = foreach test generate rowkey, columns.name, columns.value;

grunt> dump value_test;
{code}
In /var/log/cassandra/system.log, I have severals time this exception:
{code}
INFO [IPC Server handler 3 on 8012] 2011-06-22 15:03:28,533 TaskInProgress.java (line 551) Error from attempt_201106210955_0051_m_000000_3: java.lang.RuntimeException: Unexpected data type -1 found in stream.
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:478)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeBag(BinInterSedes.java:522)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:361)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
	at org.apache.pig.impl.io.InterRecordWriter.write(InterRecordWriter.java:73)
	at org.apache.pig.impl.io.InterStorage.putNext(InterStorage.java:87)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:138)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:97)
	at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:638)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.collect(PigMapOnly.java:48)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:239)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:232)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:369)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:259)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.Child.main(Child.java:253)
{code}
and the request failed.

{code}
grunt> test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS (rowkey:chararray, columns: bag {T: (name:long, value:int)});

grunt> value_test = foreach test generate rowkey, columns.value;

grunt> dump value_test;
{code}

This time, without the column name, it's work (but the value are displayed as char instead of integer). Result:
{code}
(row1,{(#),($),(&)})
(row2,{(-),(*),(!)})
{code}

Now we do the same test but we set a comparator to the CF.
{code}
[default@Test] create column family test with comparator = 'LongType';

[default@Test] set test[ascii('row1')][long(1)]=integer(35);
set test[ascii('row1')][long(2)]=integer(36);
set test[ascii('row1')][long(3)]=integer(38);
set test[ascii('row2')][long(1)]=integer(45);
set test[ascii('row2')][long(2)]=integer(42);
set test[ascii('row2')][long(3)]=integer(33);

[default@Test] list test;
Using default limit of 100
-------------------
RowKey: 726f7731
=> (column=1, value=35, timestamp=1308748643506000)
=> (column=2, value=36, timestamp=1308748643508000)
=> (column=3, value=38, timestamp=1308748643509000)
-------------------
RowKey: 726f7732
=> (column=1, value=45, timestamp=1308748643510000)
=> (column=2, value=42, timestamp=1308748643512000)
=> (column=3, value=33, timestamp=1308748645138000)

2 Rows Returned.

[default@Test] describe keyspace;
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
{code}
{code}
grunt> test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS (rowkey:chararray, columns: bag {T: (name:long, value:int)});

grunt> value_test = foreach test generate rowkey, columns.name, columns.value;

grunt> dump value_test;
{code}
This time it's work as expected (appart from the value displayed as char). Result:
{code}
(row1,{(1),(2),(3)},{(#),($),(&)})
(row2,{(1),(2),(3)},{(-),(*),(!)})
{code}
","Ubuntu 10.10, 32 bits
java version ""1.6.0_24""
Brisk beta-2 installed from Debian packages",,,,,,,,,,,,,,,10/Aug/11 00:55;brandon.williams;2810-v2.txt;https://issues.apache.org/jira/secure/attachment/12489923/2810-v2.txt,23/Aug/11 16:25;brandon.williams;2810-v3.txt;https://issues.apache.org/jira/secure/attachment/12491357/2810-v3.txt,24/Jun/11 14:45;brandon.williams;2810.txt;https://issues.apache.org/jira/secure/attachment/12483714/2810.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-06-24 14:45:15.352,,,no_permission,,,,,,,,,,,,20841,,,Wed Sep 28 22:19:49 UTC 2011,,,,,,0|i0gdlj:,93633,jeromatron,jeromatron,,,,,,,,,"24/Jun/11 14:45;brandon.williams;Patch to use a custom AbstractType in place of BytesType to nip this in the bud, rather than have a bunch of one-off checks.  Also fixes a bug where the supercolumn name is never set.",24/Jun/11 14:52;jbellis;DataByteArray is some kind of Pig thing?,"24/Jun/11 15:07;brandon.williams;Yes, basically a byte array, but it's the pig type.","27/Jun/11 13:07;silvere;I try again after applying [^2810.txt] and the patch from bug [CASSANDRA-2777] and the bug is still here.
With the patch, you need to replace
{code}
test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS (rowkey:chararray, columns: bag {T: (name:long, value:int)});
{code}
by
{code}
test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS ();
{code}
because CassandraStorage takes care of the schema.

I try:
{code}
grunt> describe test;
test: {key: chararray,columns: {(name: long,value: int)}}
{code}
so we can see that the patch from bug 2777 works correctly (I also test with different types for value).
But when I dump test, I still have the same exception.","27/Jun/11 14:47;silvere;After more test (with both patches), path [^2810.txt] doesn't seems to solve the bug.
Here is a new test case:
Create a _Test_ keyspace and a _test_ column family with key_validation_class = 'AsciiType' and comparator = 'LongType' and default_validation_class = 'IntegerType' (don't use the cli because of [#CASSANDRA-2831]).
Insert some data:
{code}
set test[ascii('row1')][long(1)]=integer(35);
set test[ascii('row1')][long(2)]=integer(36);
set test[ascii('row1')][long(3)]=integer(38);
set test[ascii('row2')][long(1)]=integer(45);
set test[ascii('row2')][long(2)]=integer(42);
set test[ascii('row2')][long(3)]=integer(33);
{code}

In Pig cli:
{code}
test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS ();
dump test;
{code}
The same exception as before is raised:
{code}
 INFO [IPC Server handler 4 on 8012] 2011-06-27 16:40:28,562 TaskInProgress.java (line 551) Error from attempt_201106271436_0012_m_000000_1: java.lang.RuntimeException: Unexpected data type -1 found in stream.
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:478)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeBag(BinInterSedes.java:522)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:361)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
	at org.apache.pig.impl.io.InterRecordWriter.write(InterRecordWriter.java:73)
	at org.apache.pig.impl.io.InterStorage.putNext(InterStorage.java:87)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:138)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:97)
	at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:638)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.collect(PigMapOnly.java:48)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:224)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:369)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:259)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.Child.main(Child.java:253)

{code}","08/Jul/11 19:41;brandon.williams;So is the conclusion that this patch by itself works fine, but there is a problem with CASSANDRA-2777?","12/Jul/11 13:54;silvere;No, from my test I arrived to the inverse conclusion: [#CASSANDRA-2777] seems to works fine (Pig has the good type for my column family) but the bug is still here despite the 2 patches.","10/Aug/11 00:55;brandon.williams;It looks like the final problem here is that IntegerType always returns a BigInteger, which pig does not like.  This is unfortunate since IntegerType can't be easily subclassed and overridden to return ints.

v2 instead adds a setTupleValue method that is always used for adding values to tuples, and houses all the special-casing currently needed and provides a spot for more in the future, rather than proliferating custom type converters since I'm sure IntegerType won't be alone here.","23/Aug/11 16:25;brandon.williams;v3 also removes decomposing the values before inserting and instead forces them into a ByteBuffer with objToBB, since we actually don't care about the type. (why did we ever change this?)

This means that a UDF that doesn't preserve the schema and hands us back DataByteArrays when we fed it specific types can't make us fail anymore.",28/Sep/11 21:30;steeve;Fixed it for me on Pig 0.9 and Cassandra 0.8.6 (Brisk).,"28/Sep/11 21:35;jeromatron;+1 - if we find any issues with it in production, we'll submit bug reports.",28/Sep/11 22:01;brandon.williams;Committed.,"28/Sep/11 22:19;hudson;Integrated in Cassandra-0.8 #348 (See [https://builds.apache.org/job/Cassandra-0.8/348/])
    Fix handling of integer types in pig.
Patch by brandonwilliams, reviewed by Jeremy Hanna for CASSANDRA-2810

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177084
Files : 
* /cassandra/branches/cassandra-0.8/contrib/pig/src/java/org/apache/cassandra/hadoop/pig/CassandraStorage.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After a minor compaction, deleted key-slices are visible again",CASSANDRA-2786,12510655,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,kochen,kochen,17/Jun/11 12:42,12/Mar/19 14:16,13/Mar/19 22:26,23/Nov/11 15:39,0.8.8,1.0.4,,,,,2,compaction,,,,"After a minor compaction, deleted key-slices are visible again.

Steps to reproduce:

1) Insert a row named ""test"".
2) Insert 500000 rows. During this step, row ""test"" is included in a major compaction:
   file-1, file-2, file-3 and file-4 compacted to file-5 (includes ""test"").
3) Delete row named ""test"".
4) Insert 500000 rows. During this step, row ""test"" is included in a minor compaction:
   file-6, file-7, file-8 and file-9 compacted to file-10 (should include tombstoned ""test"").
After step 4, row ""test"" is live again.

Test environment:

Single node with empty database.

Standard configured super-column-family (I see this behavior with several gc_grace settings (big and small values):
create column family Customers with column_type = 'Super' and comparator = 'BytesType;

In Cassandra 0.7.6 I observe the expected behavior, i.e. after step 4, the row is still deleted.

I've included a .NET program to reproduce the problem. I will add a Java version later on.","Reproduced on single Cassandra node (CentOS 5.5)
Reproduced on single Cassandra node (Windows Server 2008)",,,,,,,,,,,,,,,21/Jun/11 10:56;slebresne;0001-Fix-wrong-purge-of-deleted-cf.patch;https://issues.apache.org/jira/secure/attachment/12483265/0001-Fix-wrong-purge-of-deleted-cf.patch,30/Jun/11 15:04;slebresne;2786_part2.patch;https://issues.apache.org/jira/secure/attachment/12484782/2786_part2.patch,23/Nov/11 14:02;jbellis;2786_part3-v2.txt;https://issues.apache.org/jira/secure/attachment/12504873/2786_part3-v2.txt,22/Nov/11 17:18;slebresne;2786_part3.patch;https://issues.apache.org/jira/secure/attachment/12504774/2786_part3.patch,17/Jun/11 12:43;kochen;CassandraIssue.zip;https://issues.apache.org/jira/secure/attachment/12482927/CassandraIssue.zip,20/Jun/11 13:18;kochen;CassandraIssueJava.zip;https://issues.apache.org/jira/secure/attachment/12483154/CassandraIssueJava.zip,,,,,,6.0,,,,,,,,,,,,,,,,,,,2011-06-17 14:51:22.513,,,no_permission,,,,,,,,,,,,20828,,,Wed Nov 23 16:24:35 UTC 2011,,,,,,0|i0gdg7:,93609,jbellis,jbellis,,,,,,,,,17/Jun/11 14:51;slebresne;The java version would be really cool :),20/Jun/11 13:19;kochen;I included the Java version. You have to play a little bit with the numbers of rows to insert in order to get the correct compaction timings.,21/Jun/11 10:56;slebresne;We were wrongfully skipping deleted rows with no columns during compaction. This indeed don't affect 0.7 since this was due to a refactor of PrecompactedRow in 0.8. Patch attached with a unit test to catch the error.,"21/Jun/11 12:26;jbellis;+1

(can we make the ""testing"" constructor package-local?)","21/Jun/11 13:48;slebresne;Committed, thanks.

I did not made the ""testing"" constructor package-local because it is used in the AntiEntropyTests which are not on the same package. But I agree it's not the cleanest thing ever.","21/Jun/11 17:41;hudson;Integrated in Cassandra-0.8 #182 (See [https://builds.apache.org/job/Cassandra-0.8/182/])
    Fix wrong purge of deleted cf during compaction
patch by slebresne; reviewed by jbellis for CASSANDRA-2786

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1137984
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/compaction/CompactionsTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/PrecompactedRow.java
",29/Jun/11 16:44;kochen;Tested with 0.8.1 but still doesn't work,"30/Jun/11 15:04;slebresne;Yeah, turns out EchoedRow is also handling Row tombstones with no columns inside badly.

Attaching patch with fix and unit test. 0.7 is not really impacted because it uses EchoedRow only for cleanup and don't use its isEmpty() function there (but I suppose we could make it throw an UnsupporteOperationException to be on the safe side).

The patch actually ship with two changes that are not strictly related to the issue:
# It fixes testEchoedRow in CompactionsTest. It wasn't using EchoedRow anymore (i.e, the test was useless).
# It always forces deserialization for user submitted compaction (by opposition to only when the user submits only 1 sstable). It is done because exposing the forceDeserialization flag was necessary to write the test for this issue. Following that change, it was trivial to do the user submitted compaction change. It also fix a bad comment (forcing deserialization is only useful for forcing expired column to become tombstones, not for purging since purging will happen without force deserialization if it can).","01/Jul/11 20:02;jbellis;Nit: wouldn't it be cleaner to just pass gcBefore rather than the entire controller to EchoedRow constructor?

+1 otherwise.","06/Jul/11 12:16;hudson;Integrated in Cassandra-0.8 #205 (See [https://builds.apache.org/job/Cassandra-0.8/205/])
    Handle row tombstones correctly in EchoedRow
patch by slebresne; reviewed by jbellis for CASSANDRA-2786

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1143352
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/compaction/CompactionsTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/EchoedRow.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
","06/Jul/11 12:19;slebresne;Committed, thanks.

bq. Nit: wouldn't it be cleaner to just pass gcBefore rather than the entire controller to EchoedRow constructor?

I passed the controller because Precompacted and LazilyCompacted do that too, so it felt slightly cleaner, and if we happen to need more info from the controller in the future, it'll be there. But really at the end I did not change it before committing out of laziness :)","06/Jul/11 13:10;jbellis;bq. Precompacted and LazilyCompacted do that too

That makes sense.","21/Nov/11 16:44;kochen;Tested with 0.8.2 and 0.8.7, but still does not work. On 0.7.x it works fine.","21/Nov/11 16:48;kochen;One note: I tested with several grace-periods. With a grace-period of one minute, it is easier to reproduce. On our production site (with grace-priod of 24 hours), the data resurrects after several days.","21/Nov/11 21:14;brandon.williams;bq. With a grace-period of one minute, it is easier to reproduce.

This seems to me to suggest you aren't running repair often enough and are encountering the same effect as CASSANDRA-1316.","21/Nov/11 21:17;jbellis;bq. This seems to me to suggest you aren't running repair often enough 

If you can only reproduce on multiple nodes, that is probably the issue here.",21/Nov/11 23:50;kochen;With the attached program I'm able to reproduce it on a single node.,"22/Nov/11 00:59;mdennis;Per previous phone conversation, I tested this against 0.8.7 twice.

Both times the attached java test ran to completion against a single node and output ""Done"".

When was this last reproduced?",22/Nov/11 01:03;mdennis;I should mention that I did change one line in the test.  I changed {noformat}column.timestamp = getTimestamp();{noformat} to {noformat}column.setTimestamp(getTimestamp());{noformat} because otherwise thrift complained that the timestamp wasn't set.,"22/Nov/11 08:49;kochen;Could you please check again with grace_period of 60.

I use the following to reproduce:

create column family Customers
    with column_type = 'Super' 
    and comparator = 'BytesType'
	and memtable_flush_after = 60
	and gc_grace = 60;

On my system, it crashes every time:

C:\Temp\JavaIssue>java -jar CassandraIssue.jar 127.0.0.1 Traxis
Exception in thread ""main"" java.lang.Exception: test row should be empty
        at cassandraissue.Main.start(Main.java:88)
        at cassandraissue.Main.main(Main.java:178)
		
Thanks",22/Nov/11 13:12;slebresne;I had to tweak the variables a little bit but I'm able to reproduce. Will look into it.,"22/Nov/11 17:18;slebresne;Hopefully we get this right that time. The problem was that we were calling removeDeleted even in case where we shouldn't have been purging. Attaching 'part3' patch to fix, along with an updated unit test for that.","23/Nov/11 14:02;jbellis;This is a little subtle so I'm going to spell it out:

The purpose of AbstractCompactedRow.isEmpty  is to skip rows that consist only of expired tombstones:

{code}
.           writer = cfs.createCompactionWriter(expectedBloomFilterSize, compactionFileLocation, sstables);
            while (nni.hasNext())
            {
                AbstractCompactedRow row = nni.next();
                if (row.isEmpty())
                    continue;
                ...
            }
{code}

However, we can't skip tombstones if we're only compacting some of the sstables for a row (CASSANDRA-1074).  The bug here is that the isEmpty test doesn't check the CompactionController.shouldPurge, which is how the controller lets us know it's okay to drop tombstones.  (In the PR case the shouldPurge check was done correctly during creation of compactedCf, but then we ignored it when checking a second time for isEmpty.)

Sylvain's patch fixes the bug.  Here is a v2 that simplifies isEmpty further:

- ER.isEmpty is actually trivial
- PR doesn't need to do a second check of no columns + no row level tombstone (i.e.: there were expired column tombstones); this case would be taken care of by the removeDeleted in compactedCf creation
",23/Nov/11 14:51;slebresne;+1 on v2,23/Nov/11 15:39;jbellis;committed,"23/Nov/11 16:24;hudson;Integrated in Cassandra-0.8 #403 (See [https://builds.apache.org/job/Cassandra-0.8/403/])
    avoid dropping tombstones when they might still be needed to shadow data in another sstable
patch by slebresne and jbellis for CASSANDRA-2786

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1205452
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/EchoedRow.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/LazilyCompactedRow.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/PrecompactedRow.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/SchemaLoader.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/compaction/CompactionsTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping node stalls. Bootstrapper thinks it is still streaming some sstables. The source nodes do not. Caused by IllegalStateException on source nodes.,CASSANDRA-2792,12510760,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,dccwilliams,dccwilliams,18/Jun/11 13:53,12/Mar/19 14:16,13/Mar/19 22:26,21/Jun/11 19:53,0.7.7,,,,,,0,,,,,"I am bootstrapping a node into a 4 node cluster with RF3 (1 node is currently down due to sstable issues, but the cluster is running without issues). 

There are two keyspaces FightMyMonster and FMM_Studio. The first keyspace successfully streams and the whole operation is probably at 99% when it stalls on some sstables in the much smaller FMM_Studio keyspace.

Netstats on the bootstrapping node reports it is still streaming:

Mode: Bootstrapping
Not sending any streams.
Streaming from: /192.168.1.4
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-101-Data.db sections=1 progress=0/76453 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-103-Data.db sections=1 progress=0/90475 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-102-Data.db sections=1 progress=0/4304182 - 0%
Streaming from: /192.168.1.3
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-158-Data.db sections=2 progress=0/146990 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/AuthorClasses-f-81-Data.db sections=1 progress=0/3992 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/Studio-f-70-Data.db sections=1 progress=0/1776 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-159-Data.db sections=2 progress=0/136829 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-157-Data.db sections=2 progress=0/5779597 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/AuthorClasses-f-82-Data.db sections=1 progress=0/161 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/Studio-f-71-Data.db sections=1 progress=0/135 - 0%
Pool Name                    Active   Pending      Completed
Commands                        n/a         0            334
Responses                       n/a         0         421957

However, running netstats on the source nodes reports they are not streaming:

Mode: Normal
 Nothing streaming to /192.168.1.9
Not receiving any streams.
Pool Name                    Active   Pending      Completed
Commands                        n/a         0        1949476
Responses                       n/a         1        1778768

Examination of the logs on the source nodes show an IllegalStateException that has likely interrupted/broken the streaming process.

17 22:27:05,924 StreamOut.java (line 126) Beginning transfer to /192.168.1.9
 INFO [StreamStage:1] 2011-06-17 22:27:05,925 StreamOut.java (line 100) Flushing memtables for FMM_Studio...
 INFO [StreamStage:1] 2011-06-17 22:27:06,004 StreamOut.java (line 173) Stream context metadata [/var/opt/cassandra/data/FMM_Studio/Classes-f-107-Data.db sections=1 progress=0/1585378 - 0%, /var/opt/cas
sandra/data/FMM_Studio/PartsData-f-100-Data.db sections=1 progress=0/76453 - 0%, /var/opt/cassandra/data/FMM_Studio/PartsData-f-98-Data.db sections=1 progress=0/4309514 - 0%, /var/opt/cassandra/data/FMM
_Studio/PartsData-f-99-Data.db sections=1 progress=0/90475 - 0%], 11 sstables.
 INFO [StreamStage:1] 2011-06-17 22:27:06,005 StreamOutSession.java (line 174) Streaming to /192.168.1.9
 INFO [StreamStage:1] 2011-06-17 22:27:06,006 StreamOut.java (line 126) Beginning transfer to /192.168.1.9
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 StreamOut.java (line 100) Flushing memtables for FightMyMonster...
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-MonsterMarket_1@1054909557(338 bytes, 24 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-UserFights@239934867(1124836 bytes, 965 operations)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,007 Memtable.java (line 157) Writing Memtable-MonsterMarket_1@1054909557(338 bytes, 24 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-Users_CisIndex@1758504250(242 bytes, 8 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,008 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-Tribes@1510979736(18318 bytes, 703 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,008 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-ColumnViews_TimeUUID@864545260(2073 bytes, 63 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,008 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-MonsterMarket_0@537829218(2600 bytes, 129 operations)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,069 Memtable.java (line 172) Completed flushing /var/opt/cassandra/data/FightMyMonster/MonsterMarket_1-f-3799-Data.db (1774 bytes)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,069 Memtable.java (line 157) Writing Memtable-UserFights@239934867(1124836 bytes, 965 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,070 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-UserSigninLog@1692186117(4043 bytes, 137 operations)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,161 Memtable.java (line 172) Completed flushing /var/opt/cassandra/data/FightMyMonster/UserFights-f-8192-Data.db (1179202 bytes)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,161 Memtable.java (line 157) Writing Memtable-Users_CisIndex@1758504250(242 bytes, 8 operations)
 INFO [CompactionExecutor:1] 2011-06-17 22:27:06,161 CompactionManager.java (line 395) Compacting [SSTableReader(path='/var/opt/cassandra/data/FightMyMonster/UserFights-f-8189-Data.db'),SSTableReader(pa
th='/var/opt/cassandra/data/FightMyMonster/UserFights-f-8190-Data.db'),SSTableReader(path='/var/opt/cassandra/data/FightMyMonster/UserFights-f-8191-Data.db'),SSTableReader(path='/var/opt/cassandra/data/
FightMyMonster/UserFights-f-8192-Data.db')]
 INFO [StreamStage:1] 2011-06-17 22:27:06,162 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-TribeFights@321579649(138 bytes, 3 operations)
ERROR [MiscStage:1] 2011-06-17 22:27:06,168 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[MiscStage:1,5,main]
java.lang.IllegalStateException: target reports current file is /var/opt/cassandra/data/FMM_Studio/Classes-f-107-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:166)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MiscStage:1] 2011-06-17 22:27:06,168 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[MiscStage:1,5,main]
java.lang.IllegalStateException: target reports current file is /var/opt/cassandra/data/FMM_Studio/Classes-f-107-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:166)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619) 

There are two problems. Firstly the source nodes should report to the bootstrapping node that there has been a problem, and/or the bootstrapping node should timeout and report the the issue. 

Secondly, there is an issue with what is causing IllegalStateException.",Ubuntu ,14400,14400,,0%,14400,14400,,,,,,,,,21/Jun/11 15:06;slebresne;0001-Make-StreamOutSession-threadSafe.patch;https://issues.apache.org/jira/secure/attachment/12483295/0001-Make-StreamOutSession-threadSafe.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-21 15:06:15.62,,,no_permission,,,,,,,,,,,,20832,,,Wed Jun 22 07:47:04 UTC 2011,,,,,,0|i0gdhj:,93615,jbellis,jbellis,,,,,,,,,"21/Jun/11 15:06;slebresne;bq. the source nodes should report to the bootstrapping node that there has been a problem, and/or the bootstrapping node should timeout and report the the issue.

This will be addressed by CASSANDRA-2433 (or almost). This won't go into 0.7 however and since this change network protocol it's yet unclear it will even go into 0.8 (though my personnal opinion is that this should go in asap).

bq. Secondly, there is an issue with what is causing IllegalStateException.

Attaching a patch that hopefully fixes that. It's a bit of a guess because I don't know how to reproduce but I suppose that since the currentFile field is not volatile in streamOutSession, changing it may not propagate to all threads (at least that explication fits with the exception message). The patch also use a thread safe map for pendingFiles since those are modified in differnt thread, even though I have stronger doubt that this could be a problem (but it doesn't cost us anything to be on the safe side).

",21/Jun/11 15:34;jbellis;+1 on the threadsafety changes,"21/Jun/11 19:53;slebresne;Thanks, committed.","22/Jun/11 07:47;hudson;Integrated in Cassandra-0.7 #507 (See [https://builds.apache.org/job/Cassandra-0.7/507/])
    Improve thread safety in StreamOutSession
patch by slebresne; reviewed by jbellis for CASSANDRA-2792

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1138148
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/streaming/StreamOutSession.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in AntiEntropyService.getNeighbors(),CASSANDRA-2767,12510277,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,14/Jun/11 08:01,12/Mar/19 14:16,13/Mar/19 22:26,14/Jun/11 09:49,0.8.1,,,,,,0,repair,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,14/Jun/11 08:43;slebresne;0001-Fix-ConcurrentModificationException.patch;https://issues.apache.org/jira/secure/attachment/12482530/0001-Fix-ConcurrentModificationException.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-14 08:55:24.894,,,no_permission,,,,,,,,,,,,20817,,,Tue Jun 14 17:02:36 UTC 2011,,,,,,0|i0gdbz:,93590,jbellis,jbellis,,,,,,,,,14/Jun/11 08:55;jbellis;+1,14/Jun/11 09:49;slebresne;Committed,"14/Jun/11 17:02;hudson;Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HintedHandoff fails with could not reach schema agreement,CASSANDRA-2946,12515285,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,richardlow,richardlow,25/Jul/11 16:38,12/Mar/19 14:16,13/Mar/19 22:26,27/Jul/11 15:14,0.8.3,,,,,,0,,,,,"To reproduce, have two nodes A and B.

1. On node A, create a keyspace with replication factor 1 and add a column family
2. Ensure node B has created the keyspace and column family
3. Take down node B
4. Insert some keys to A at CL.ANY, ensuring some keys should be written to B
5. Bring up node B
6. When hints are delivered, I get the error:

ERROR [HintedHandoff:1] 2011-07-25 17:19:14,729 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.RuntimeException: Could not reach schema agreement with /10.2.129.9 in 60000ms
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: Could not reach schema agreement with /10.2.129.9 in 60000ms
        at org.apache.cassandra.db.HintedHandOffManager.waitForSchemaAgreement(HintedHandOffManager.java:290)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:301)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:89)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:394)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more

If I use DatabaseDescriptor.getDefsVersion() instead of gossiper.getEndpointStateForEndpoint(FBUtilities.getLocalAddress()).getApplicationState(ApplicationState.SCHEMA) then the error goes away, and the hints are correctly delivered.

This may be the same issue as Aaron saw here: http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/ApplicationState-Schema-has-drifted-from-DatabaseDescriptor-td6006576.html, and may be related to CASSANDRA-2083.",,,,,,,,,,,,,,,,26/Jul/11 13:57;jbellis;2946.txt;https://issues.apache.org/jira/secure/attachment/12487836/2946.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-26 03:36:40.969,,,no_permission,,,,,,,,,,,,20903,,,Wed Jul 27 16:12:39 UTC 2011,,,,,,0|i0gefj:,93768,richardlow,richardlow,,,,,,,,,"26/Jul/11 03:36;jbellis;does the disagreement show up with ""describe cluster"" from the cli?  if so, at what point do the schemas of A and B diverge?","26/Jul/11 10:28;richardlow;describe cluster always shows agreement.  Selected log messages:

DEBUG [HintedHandoff:1] 2011-07-26 11:22:35,526 HintedHandOffManager.java (line 300) Checking remote schema before delivering hints
...
DEBUG [pool-2-thread-1] 2011-07-26 11:22:44,965 CassandraServer.java (line 1123) checking schema agreement
...
DEBUG [pool-2-thread-1] 2011-07-26 11:22:44,969 StorageProxy.java (line 823) Schemas are in agreement.
...
ERROR [HintedHandoff:1] 2011-07-26 11:23:36,788 AbstractCassandraDaemon.java (line 138) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.RuntimeException: Could not reach schema agreement with /10.2.129.9 in 60000ms

So StorageProxy thinks the schema agrees but HintedHandoffManager doesn't.","26/Jul/11 13:57;jbellis;this should fix the gossip-out-of-sync bug.

if it does we can also change the HH code to use DD.","26/Jul/11 19:21;brandon.williams;+1, what happened was node A never updated it's gossip state, but the other node learned of its schema version by RPC.  When node B restarted, it no longer knew the schema version for A, and tried to get it from gossip where it was still old.

bq. if it does we can also change the HH code to use DD.
I don't know about that, having this as a check to make sure this doesn't happen again is a far easier thing to catch than other ""gossip is out of date"" problems we might encounter.","26/Jul/11 22:05;jbellis;okay, then I'll add a comment to that effect, assuming Richard confirms this fixes the bug.","27/Jul/11 09:03;richardlow;Yes, that fix works.  Thanks.",27/Jul/11 15:14;jbellis;Committed to 0.8.3 and trunk.  (And verified that this is not a problem on the 0.7 branch.),"27/Jul/11 16:12;hudson;Integrated in Cassandra-0.8 #240 (See [https://builds.apache.org/job/Cassandra-0.8/240/])
    keep gossipped version in sync with actual on migration coordinator
patch by jbellis; reviewed by brandonwilliams and tested by Richard Low for CASSANDRA-2946

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1151494
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/Migration.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/HintedHandOffManager.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexRangeSliceQuery results include index column even though it is not in SliceRange,CASSANDRA-2964,12515619,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,rjtg,rjtg,28/Jul/11 09:19,12/Mar/19 14:16,13/Mar/19 22:26,13/Jun/12 10:02,0.7.9,,,,,,0,,,,,When an IndexSlicwQuery is done the result contains the index column even though it was not in the slice range.,,,,,,,,,,,,,,,,03/Aug/11 17:37;jbellis;2964-0.7.txt;https://issues.apache.org/jira/secure/attachment/12489221/2964-0.7.txt,28/Jul/11 10:27;rjtg;TestIndexRangeSliceQuery.java;https://issues.apache.org/jira/secure/attachment/12488084/TestIndexRangeSliceQuery.java,28/Jul/11 10:27;rjtg;cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12488083/cassandra.yaml,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-08-03 17:38:58.052,,,no_permission,,,,,,,,,,,,20915,,,Wed Jun 13 10:02:59 UTC 2012,,,,,,0|i0gejj:,93786,rjtg,rjtg,,,,,,,,,"28/Jul/11 09:21;rjtg;This was working correctly in 0.7.5 but after uzpgrading to 0.7.7 the ranges seem to be wrong.
Will add a testcase soon.",28/Jul/11 10:27;rjtg;testng testcase and cassandra.yaml to be used,"28/Jul/11 10:31;rjtg;Creating the testcase makes this more awkward.
It only fails when run against our production cluster (which was migrated recently) but not against a fresh setup 0.7.7 (in the testcase the embedded server)

Therefor i was not able to check if the error is limited to TimeUUIDType Columns as that is the only index used in our production system.
I will try to figure out why it fails in that cluster but not in fresh one","28/Jul/11 12:54;rjtg;some log output from when the above testcase fails (the columnfamily name is different from the one in the testcase, because it is the original system - i have not yet been able to reproduce the issue on any other system)

DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,103 CassandraServer.java (line 522) scan
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,103 StorageProxy.java (line 666) restricted ranges for query [-1,-1] are [[-1,0], (0,42535295865117307932921825928971026432], (425352958651173079329218259289710
26432,85070591730234615865843651857942052864], (85070591730234615865843651857942052864,127605887595351923798765477786913079296], (127605887595351923798765477786913079296,-1]]
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,103 StorageProxy.java (line 753) scan ranges are [-1,0],(0,42535295865117307932921825928971026432],(42535295865117307932921825928971026432,850705917302346158658
43651857942052864],(85070591730234615865843651857942052864,127605887595351923798765477786913079296],(127605887595351923798765477786913079296,-1]
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,104 ReadCallback.java (line 86) Blockfor/repair is 2/false; setting up requests to /10.234.81.208,/10.226.130.128
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,107 StorageProxy.java (line 780) reading org.apache.cassandra.db.IndexScanCommand@31bcc110 from /10.234.81.208
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,107 StorageProxy.java (line 780) reading org.apache.cassandra.db.IndexScanCommand@31bcc110 from /10.226.130.128
DEBUG [ReadStage:563] 2011-07-28 14:45:19,107 ColumnFamilyStore.java (line 1514) Primary scan clause is 00000000-0000-1000-0000-000000000000
DEBUG [ReadStage:563] 2011-07-28 14:45:19,108 ColumnFamilyStore.java (line 1569) Scanning index 'EventsByUser.00000000-0000-1000-0000-000000000000 EQ 41' starting with
DEBUG [ReadStage:563] 2011-07-28 14:45:19,108 SliceQueryFilter.java (line 123) collecting 0 of 100: 74657374526573756c74446f6573436f6e7461696e436f727265637452616e67654578636c7564696e67496e6465786564526f77:fa
lse:0@1311857119516000

DEBUG [ReadStage:555] 2011-07-28 14:45:19,173 ColumnFamilyStore.java (line 1581) fetched ColumnFamily(<anonymous> [74657374526573756c74446f6573436f6e7461696e436f727265637452616e67654578636c7564696e67496e6465
786564526f77:false:0@1311857119516000,])
DEBUG [ReadStage:555] 2011-07-28 14:45:19,173 IndexScanVerbHandler.java (line 46) Sending RangeSliceReply{rows=} to 14083152@/10.234.81.208
DEBUG [RequestResponseStage:3] 2011-07-28 14:45:19,173 ResponseVerbHandler.java (line 48) Processing response on a callback from 14083152@/10.234.81.208
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,173 StorageProxy.java (line 780) reading org.apache.cassandra.db.IndexScanCommand@22bb5662 from /10.234.81.208
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,173 StorageProxy.java (line 780) reading org.apache.cassandra.db.IndexScanCommand@22bb5662 from /10.227.141.155
DEBUG [RequestResponseStage:4] 2011-07-28 14:45:19,175 ResponseVerbHandler.java (line 48) Processing response on a callback from 14083153@/10.227.141.155
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,175 ReadCallback.java (line 86) Blockfor/repair is 2/false; setting up requests to /10.234.81.208,/10.226.130.128
DEBUG [ReadStage:560] 2011-07-28 14:45:19,175 ColumnFamilyStore.java (line 1514) Primary scan clause is 00000000-0000-1000-0000-000000000000
DEBUG [ReadStage:560] 2011-07-28 14:45:19,176 ColumnFamilyStore.java (line 1569) Scanning index 'EventsByUser.00000000-0000-1000-0000-000000000000 EQ 41' starting with
DEBUG [ReadStage:560] 2011-07-28 14:45:19,176 SliceQueryFilter.java (line 123) collecting 0 of 100: 74657374526573756c74446f6573436f6e7461696e436f727265637452616e67654578636c7564696e67496e6465786564526f77:fa
lse:0@1311857119516000
DEBUG [ReadStage:560] 2011-07-28 14:45:19,176 ColumnFamilyStore.java (line 1581) fetched ColumnFamily(<anonymous> [74657374526573756c74446f6573436f6e7461696e436f727265637452616e67654578636c7564696e67496e6465
786564526f77:false:0@1311857119516000,])
DEBUG [ReadStage:560] 2011-07-28 14:45:19,176 SliceQueryFilter.java (line 123) collecting 0 of 1000: 8ab6d400-1dd2-11b2-8bac-0024e8e90693:true:4@1311850071466000
DEBUG [ReadStage:560] 2011-07-28 14:45:19,177 SliceQueryFilter.java (line 123) collecting 0 of 1000: 8ab6d400-1dd2-11b2-93fb-0024e8e90693:true:4@1311849715863000
DEBUG [ReadStage:560] 2011-07-28 14:45:19,177 SliceQueryFilter.java (line 123) collecting 0 of 1000: 8ab6d400-1dd2-11b2-ba61-0024e8e90693:false:0@1311857119500000
DEBUG [ReadStage:560] 2011-07-28 14:45:19,177 ColumnFamilyStore.java (line 1605) fetched data row ColumnFamily(EventsByUser -deleted at 1311849448186000- [8ab6d400-1dd2-11b2-8bac-0024e8e90693:true:4@13118500
71466000,8ab6d400-1dd2-11b2-93fb-0024e8e90693:true:4@1311849715863000,8ab6d400-1dd2-11b2-ba61-0024e8e90693:false:0@1311857119500000,])
DEBUG [ReadStage:560] 2011-07-28 14:45:19,177 ColumnFamilyStore.java (line 1616) adding extraFilter to cover additional expressions
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,177 StorageProxy.java (line 780) reading org.apache.cassandra.db.IndexScanCommand@39244dbe from /10.234.81.208
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,177 StorageProxy.java (line 780) reading org.apache.cassandra.db.IndexScanCommand@39244dbe from /10.226.130.128
DEBUG [ReadStage:560] 2011-07-28 14:45:19,177 ColumnFamilyStore.java (line 1640) row ColumnFamily(EventsByUser -deleted at 1311849448186000- [00000000-0000-1000-0000-000000000000:false:1@1311857119516000,8ab
6d400-1dd2-11b2-8bac-0024e8e90693:true:4@1311850071466000,8ab6d400-1dd2-11b2-93fb-0024e8e90693:true:4@1311849715863000,8ab6d400-1dd2-11b2-ba61-0024e8e90693:false:0@1311857119500000,]) satisfies all clauses
DEBUG [ReadStage:560] 2011-07-28 14:45:19,177 IndexScanVerbHandler.java (line 46) Sending RangeSliceReply{rows=Row(key=DecoratedKey(162359547821047417387034252314579509322, 74657374526573756c74446f6573436f6e
7461696e436f727265637452616e67654578636c7564696e67496e6465786564526f77), cf=ColumnFamily(EventsByUser -deleted at 1311849448186000- [00000000-0000-1000-0000-000000000000:false:1@1311857119516000,8ab6d400-1dd
2-11b2-8bac-0024e8e90693:true:4@1311850071466000,8ab6d400-1dd2-11b2-93fb-0024e8e90693:true:4@1311849715863000,8ab6d400-1dd2-11b2-ba61-0024e8e90693:false:0@1311857119500000,]))} to 14083154@/10.234.81.208
DEBUG [RequestResponseStage:3] 2011-07-28 14:45:19,178 ResponseVerbHandler.java (line 48) Processing response on a callback from 14083154@/10.234.81.208
DEBUG [RequestResponseStage:4] 2011-07-28 14:45:19,180 ResponseVerbHandler.java (line 48) Processing response on a callback from 14083155@/10.226.130.128
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,181 SliceQueryFilter.java (line 123) collecting 0 of 2147483647: 00000000-0000-1000-0000-000000000000:false:1@1311857119516000
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,181 SliceQueryFilter.java (line 123) collecting 1 of 2147483647: 8ab6d400-1dd2-11b2-8bac-0024e8e90693:true:4@1311850071466000
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,181 SliceQueryFilter.java (line 123) collecting 1 of 2147483647: 8ab6d400-1dd2-11b2-93fb-0024e8e90693:true:4@1311849715863000
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,181 SliceQueryFilter.java (line 123) collecting 1 of 2147483647: 8ab6d400-1dd2-11b2-ba61-0024e8e90693:false:0@1311857119500000
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,181 StorageProxy.java (line 788) read Row(key=DecoratedKey(162359547821047417387034252314579509322, 74657374526573756c74446f6573436f6e7461696e436f72726563745261
6e67654578636c7564696e67496e6465786564526f77), cf=ColumnFamily(EventsByUser -deleted at 1311849448186000- [00000000-0000-1000-0000-000000000000:false:1@1311857119516000,8ab6d400-1dd2-11b2-8bac-0024e8e90693:t
rue:4@1311850071466000,8ab6d400-1dd2-11b2-93fb-0024e8e90693:true:4@1311849715863000,8ab6d400-1dd2-11b2-ba61-0024e8e90693:false:0@1311857119500000,]))
DEBUG [pool-1-thread-27] 2011-07-28 14:45:19,211 StorageService.java (line 1456) computing ranges for 0, 42535295865117307932921825928971026432, 85070591730234615865843651857942052864, 1276058875953519237987
65477786913079296
","03/Aug/11 17:38;jbellis;Hi Roland,

I think the attached patch should fix the problem.  If it does not, what we need to troubleshoot is the debug logs from the data nodes, e.g., 10.234.81.208 when it the coordinator logs, ""reading org.apache.cassandra.db.IndexScanCommand@22bb5662 from /10.234.81.208"".",04/Aug/11 08:46;rjtg;great i will apply the patch and give it a try.,05/Aug/11 09:17;rjtg;Issue seems to be fixed. thanks.,05/Aug/11 15:29;jbellis;committed,"05/Aug/11 18:22;hudson;Integrated in Cassandra-0.7 #538 (See [https://builds.apache.org/job/Cassandra-0.7/538/])
    prune index scan resultset back to original request
patch by jbellis; tested by Roland Gude for CASSANDRA-2964

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1154267
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
","12/Jun/12 14:34;rjtg;Unfortunately this one is back in 1.1.1 (maybe in other versions as well, but i just realized it for 1.1.1 and cannot say anythiing about the intermediate versions)


We just created a new cluster on 1.1.1, imported the data from the old 0.7 cluster with sstableloader and the issue is back in the new cluster.
",12/Jun/12 14:51;jbellis;Can you create a new ticket with debug logs from 1.1.1?,"12/Jun/12 15:06;rjtg;opened CASSANDRA-4332 for it

will attach DEBUG logs asap",13/Jun/12 10:02;rjtg;opened CASSANDRA-4332 for the new problem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Hinted Handoff replay,CASSANDRA-2928,12514860,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,21/Jul/11 03:10,12/Mar/19 14:16,13/Mar/19 22:26,21/Jul/11 03:23,0.7.8,0.8.2,,,,,0,,,,,"Broken in CASSANDRA-2668. Brandon explains:

bq. the Ack and Ack2 verb handlers are applying a new ep state every time there is a generation change via Gossiper.applyStateLocally, so it's always unset initially when the node starts up. state.hasToken() is set in the Gossiper's status check, which won't have happened when the onAlive event is sent to SS.",,,,,,,,,,,,,,,,21/Jul/11 03:14;jbellis;2928.txt;https://issues.apache.org/jira/secure/attachment/12487251/2928.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-21 03:20:46.383,,,no_permission,,,,,,,,,,,,20894,,,Thu Jul 21 03:40:55 UTC 2011,,,,,,0|i0gebb:,93749,brandon.williams,brandon.williams,,,,,,,,,21/Jul/11 03:20;brandon.williams;+1,21/Jul/11 03:23;jbellis;committed,"21/Jul/11 03:40;hudson;Integrated in Cassandra-0.7 #532 (See [https://builds.apache.org/job/Cassandra-0.7/532/])
    fix hint replay
patch by brandonwilliams and jbellis for CASSANDRA-2928

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1149015
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/gms/EndpointState.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/StorageService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE during range slices with rowrepairs,CASSANDRA-2823,12511484,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,terjem,terjem,24/Jun/11 15:39,12/Mar/19 14:16,13/Mar/19 22:26,28/Jun/11 07:55,0.8.2,,,,,,0,,,,,"Doing some heavy testing of relatively fast feeding (5000+ mutations/sec) + repair on all node + range slices.
Then occasionally killing a node here and there and restarting it.

Triggers the following NPE
 ERROR [pool-2-thread-3] 2011-06-24 20:56:27,289 Cassandra.java (line 3210) Internal error processing get_range_slices
java.lang.NullPointerException
	at org.apache.cassandra.service.RowRepairResolver.maybeScheduleRepairs(RowRepairResolver.java:109)
	at org.apache.cassandra.service.RangeSliceResponseResolver$2.getReduced(RangeSliceResponseResolver.java:112)
	at org.apache.cassandra.service.RangeSliceResponseResolver$2.getReduced(RangeSliceResponseResolver.java:83)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:161)
	at org.apache.cassandra.utils.MergeIterator.computeNext(MergeIterator.java:88)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:120)
	at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:43)

Looking at the code in getReduced:

{noformat}
                ColumnFamily resolved = versions.size() > 1
                                      ? RowRepairResolver.resolveSuperset(versions)
                                      : versions.get(0);
{noformat}
seems like resolved becomes null when this happens and versions.size is larger than 1.

RowRepairResolver.resolveSuperset() does actually return null if it cannot resolve anything, so there is definately a case here which can occur and is not handled.

It may also be an interesting question if it is guaranteed that                
versions.add(current.left.cf);
can never return null?

Jonathan suggested on IRC that maybe 
{noformat}
                ColumnFamily resolved = versions.size() > 1
                                      ? RowRepairResolver.resolveSuperset(versions)
                                      : versions.get(0);
                if (resolved == null)
                      return new Row(key, resolved);
{noformat}

could be a fix.
","This is a trunk build with 2521 and 2433
I somewhat doubt that is related however.",,,,,,,,,,,,,,,27/Jun/11 13:54;slebresne;2823.patch;https://issues.apache.org/jira/secure/attachment/12483954/2823.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-27 13:54:59.215,,,no_permission,,,,,,,,,,,,20850,,,Tue Jun 28 12:43:39 UTC 2011,,,,,,0|i0gdof:,93646,jbellis,jbellis,,,,,,,,,"27/Jun/11 13:54;slebresne;I think the problem is with the call to removeDeleted in resolveSuperset() (which is fairly new). Basically, the code is fine with resolved being null as long as this means that all the versions are null. But the removeDeleted call make it possible to have a null removeDeleted even if the versions are not null, if a row tombstone expires between the time it was returned by the node and the time it is resolved by the coordinator for instance.

Attaching patch that skips the maybeScheduleRepair() call if resolved == null since even in that case there is nothing to repair since the tombstone are now expired.",27/Jun/11 15:04;jbellis;+1,"27/Jun/11 15:06;jbellis;although I slightly prefer the ""if == null return"" immediately after initializing resolved, to keep those two pieces of logic together.","27/Jun/11 15:10;slebresne;Yeah, I didn't do that mostly because there is still a few lines of code (besides maybe scheduling repair) that we need to do even if resolved is null (debugging message in RowRepairResolver and more importantly, the clear of versions and versionSources in RangeSliceResolver). ","27/Jun/11 15:14;jbellis;ah, right -- skipping the clear would be buggy.  +1 again. :)","28/Jun/11 07:55;slebresne;Committed, thanks","28/Jun/11 08:23;hudson;Integrated in Cassandra-0.8 #195 (See [https://builds.apache.org/job/Cassandra-0.8/195/])
    Fix potential NPE in range slice read repair
patch by slebresne; reviewed by jbellis for CASSANDRA-2823

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1140470
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/RowRepairResolver.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/RangeSliceResponseResolver.java
",28/Jun/11 11:57;jbellis;does 0.7 need this?,"28/Jun/11 12:15;slebresne;You're right, 0.7 needs that too. I've committed it there too.","28/Jun/11 12:43;hudson;Integrated in Cassandra-0.7 #514 (See [https://builds.apache.org/job/Cassandra-0.7/514/])
    Fix potential NPE during read repair
patch by slebresne; reviewed by jbellis for CASSANDRA-2823

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1140550
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/RangeSliceResponseResolver.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/RowRepairResolver.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair doesn't synchronize merkle tree creation properly,CASSANDRA-2816,12511343,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,23/Jun/11 11:35,12/Mar/19 14:16,13/Mar/19 22:26,21/Jul/11 11:13,0.8.2,,,,,,3,repair,,,,"Being a little slow, I just realized after having opened CASSANDRA-2811 and CASSANDRA-2815 that there is a more general problem with repair.

When a repair is started, it will send a number of merkle tree to its neighbor as well as himself and assume for correction that the building of those trees will be started on every node roughly at the same time (if not, we end up comparing data snapshot at different time and will thus mistakenly repair a lot of useless data). This is bogus for many reasons:
* Because validation compaction runs on the same executor that other compaction, the start of the validation on the different node is subject to other compactions. 0.8 mitigates this in a way by being multi-threaded (and thus there is less change to be blocked a long time by a long running compaction), but the compaction executor being bounded, its still a problem)
* if you run a nodetool repair without arguments, it will repair every CFs. As a consequence it will generate lots of merkle tree requests and all of those requests will be issued at the same time. Because even in 0.8 the compaction executor is bounded, some of those validations will end up being queued behind the first ones. Even assuming that the different validation are submitted in the same order on each node (which isn't guaranteed either), there is no guarantee that on all nodes, the first validation will take the same time, hence desynchronizing the queued ones.

Overall, it is important for the precision of repair that for a given CF and range (which is the unit at which trees are computed), we make sure that all node will start the validation at the same time (or, since we can't do magic, as close as possible).

One (reasonably simple) proposition to fix this would be to have repair schedule validation compactions across nodes one by one (i.e, one CF/range at a time), waiting for all nodes to return their tree before submitting the next request. Then on each node, we should make sure that the node will start the validation compaction as soon as requested. For that, we probably want to have a specific executor for validation compaction and:
* either we fail the whole repair whenever one node is not able to execute the validation compaction right away (because no thread are available right away).
* we simply tell the user that if he start too many repairs in parallel, he may start seeing some of those repairing more data than it should.
",,,,,,,,,,,,,,,,24/Jun/11 14:05;slebresne;0001-Schedule-merkle-tree-request-one-by-one.patch;https://issues.apache.org/jira/secure/attachment/12483711/0001-Schedule-merkle-tree-request-one-by-one.patch,18/Jul/11 14:26;jbellis;2816-v2.txt;https://issues.apache.org/jira/secure/attachment/12486871/2816-v2.txt,20/Jul/11 17:11;jbellis;2816-v4.txt;https://issues.apache.org/jira/secure/attachment/12487185/2816-v4.txt,20/Jul/11 18:08;jbellis;2816-v5.txt;https://issues.apache.org/jira/secure/attachment/12487191/2816-v5.txt,20/Jul/11 10:05;slebresne;2816_0.8_v3.patch;https://issues.apache.org/jira/secure/attachment/12487133/2816_0.8_v3.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-06-23 12:40:12.178,,,no_permission,,,,,,,,,,,,20845,,,Thu Jul 21 12:31:54 UTC 2011,,,,,,0|i0gdmv:,93639,jbellis,jbellis,,,,,,,,,"23/Jun/11 12:40;jbellis;I guess a dedicated validation executor is ok as long as it still obeys the global ""compaction"" i/o limit.","24/Jun/11 04:27;stuhood;I'm a fan of the snapshotting immediately after receiving the request approach. In general, polishing our snapshot support to allow for this kind of usecase is likely to open up other interesting possibilities.","24/Jun/11 04:45;jbellis;Supporting actual live-reading of snapshotted sstables is a little more than ""polishing."" It would be cool, but I wouldn't want it to block fixing repair.","24/Jun/11 07:41;scode;I've thought about this problem too, and it is really significant for some use-cases. Again because so few writes are needed in order to trigger large amounts of data being sent given the merklee tree granularity.

While I'm all for a fixing it by e.g. more immediate snapshotting, I would like to raise the issue that repairs overall have pretty significant side-effects; particularly ones that can self-magnify and cause further problems. Beyond the obvious ""it does disk I/O"" and ""It uses CPU"", we have:

* Over-repair due to merklee tree granularity can cause jumps in CF sizes, killing cache locality
* Combine that with concurrent repairs then repairing the ""size-jumped"" set of sstables and you can magnify that effect on other nodes causing huge size increases.
* Up to recently, mixing large and small cf:s was a significant problem if you wanted to have different frequencies and different gc grace times, due to one repair blocking on another. But fixes to this and the other JIRA about concurrency, might disable the ""fix"" for that that was concurrent compaction - so back to square one.

I guess overall, it seems very easy to shoot yourself in the foot with repair.

Any opinions on CASSANDRA-2699 for longer term changes to repair?

","24/Jun/11 07:55;slebresne;I'm not sure what you mean by ""snapshotting immediately"" or ""polishing our snapshot support"", but one approach that I think is equivalent to that (or maybe that is what you meant by 'snapshotting') would be to grab references to the sstables at the very beginning for each request and use those all throughout the repair. This has however a problem: this means we retain sstables from being deleted during repair, including sstables that are compacted in the meantime. Because repair can take a while, this will be bad. This will also require changes to the wire protocol (because we'll need a way to indicate during streaming the set of sstables to consider), and since we've kind of decided to not do that in minor releases (at least until we've discussed that), this means this cannot be released quickly. Which is bad, because I'm pretty sure this is a good part of the reason why some people with big data sets have had huge pain with repair.

Scheduling the validation one by one avoids those problems. In theory this means we'll do less work in parallel, but in practice I doubt this is a big since the goal is probably to have repair have less impact on the node rather than more. It will also make this more easy to reason about.","24/Jun/11 14:05;slebresne;Attaching patch against 0.8. The patch implements the idea of scheduling the merkle tree requests one by one, to make sure the tree are started as close as possible of ""the same time"". This also put validation compaction in their own executor (to avoid them to be queued up behind standard compactions). That specific executor is created with 2 core threads, to allow for Peter's use case of wanting to do multiple repairs at the same time. That is, by default, you can do 2 repairs involving the same node and be ok. More and you may experience crappy precision in repair. The new concurrent_validators parameter is exposed in case some would want more that 2. That being said, regular compactions and validations are not separated for everything and in particular throttling is shared.

As far as I can test, this successfully fixes the problems from CASSANDRA-2811 and CASSANDRA-2815. This also don't change anything on the on-wire protocol side, so I think we can target that for 0.8.2.

","25/Jun/11 09:03;terjem;This sounds very interesting.

We have also spotted very noticable issues with full GCs when the merkle trees are passed around. Hopefully this could fix that too.

I will see if I can get this patch tested somewhere if it is ready for that.

On a side topic, given the importance of getting tombstones properly synchronized within GCGraceSeconds, would it be an potential interesting idea to separate tombstones in different sstables to reduce the need to scan the whole dataset very frequently in the first place?

Another thought may be to make compaction deterministic or synchronized by a master across nodes so for older data, all we needed was to compare pre-stored md5s of how whole sstables? 

That is, while keeping the masterless design for updates, we could consider a master based design for how older data is being organized by the compactor. so it would be much easier to verify that ""old"" data is the same without any large regular scans and that data is really the same after big compactions etc.
","27/Jun/11 09:24;slebresne;bq. We have also spotted very noticable issues with full GCs when the merkle trees are passed around. Hopefully this could fix that too.

This do make sure that we don't do multiple validation at the same time and that we keep a small number of merkle tree in memory at the same time. So I suppose this could help on the GC side. But overall I don't know if I am too optimistic about that, in part because I'm not sure what causes your issues. But this can't hurt on that side at least.

bq. I will see if I can get this patch tested somewhere if it is ready for that.

I believe it should be ready for that.

bq. would it be an potential interesting idea to separate tombstones in different sstables.

The thing is that some tombstones may be irrelevant become some update supersedes it (this is specially true of row tombstones). Hence basing a repair on tombstone only may transfer irrelevant data. I suppose it may depend on the use case this will be more or less a big deal. Also, this means that a read will be impacted in that we will often have to hit twice as many sstables. Given that it's not a crazy idea either to want to repair data regularly (if only for durability guarantee), I don't know if it is worth the trouble (we would have to separate tombstones from data at flush time, we'll have to maintain the two separate set of data/tombstone sstables, etc...).

bq. make compaction deterministic or synchronized by a master across nodes

Pretty sure we want to avoid going to a master architecture for everything if we can. Having master means that failure handling is more difficult (think network partition for instance) and require leader election and such, and the whole point of the fully distribution of Cassandra is to avoid those. Even without consider those, synchronizing compaction means synchronizing flush somehow and you want to be precise if you're going to use whole sstable md5s, which will be hard and quite probably inefficient.","27/Jun/11 10:36;terjem;I don't know what causes GC when doing repairs either, but fire off repair on a few nodes with 100 million docs/node and there is a reasonable chance that a node here and there will log messages about reducing cache sizes due to memory pressure (I am not really sure it is a good idea to do this at all, reducing caches during stress rarely improves anything) or full GC.

The thought about the master controlled compaction would not really affect network splits etc.

Reconciliation after a network split is really as complex with or without a master. We need to get back to a state where all the nodes have the same data anyway which is a complex task anyway.

This is more a consideration of the fact that we do not necessarily need to live in quorum based world during compaction and we are free to use alternative approaches in the compaction without changing read/write path or affecting availability. Master selection is not really a problem here. Start compaction, talk to other nodes with the same token ranges, select a leader. 

Does not even have to be the same master every time and could consider if we could make compaction part of a background read repair to reduce the amount of times we need to read/write data. 

For instance, if we can verify that the oldest/biggest sstables is 100% in sync with data on other replicas when it is compacted (why not do it during compaction when we go through the data anyway rather than later?),can we use that info to optimize the scans done during repairs by only using data in sstables with data received after some checkpoint in time as the starting point for the consistency check?","27/Jun/11 12:21;jbellis;bq. I am not really sure it is a good idea to do this at all, reducing caches during stress rarely improves anything

(This is on by default because the most common cause of OOMing is people configuring their caches too large.)

It sounds odd to me that repair would balloon memory usage dramatically.  Do you have monitoring graphs that show the difference in heap usage between ""normal"" and ""repair in progress?""","29/Jun/11 16:42;terjem;This is what heap looks like when GC start slowing things down so much that even gossip gets delayed long enough for nodes to be down for some seconds.

  num     #instances         #bytes  class name
----------------------------------------------
   1:       9453188      453753024  java.nio.HeapByteBuffer
   2:      10081546      392167064  [B
   3:       7616875      243740000  org.apache.cassandra.db.Column
   4:       9739914      233757936  java.util.concurrent.ConcurrentSkipListMap$Node
   5:       4131938       99166512  java.util.concurrent.ConcurrentSkipListMap$Index
   6:       1549230       49575360  org.apache.cassandra.db.DeletedColumn

I guess this really ends up maybe being the mix of everything going on in total and all the reading and writing that may occur when repair runs (valiadation compactions, streaming, normal compactions and regular traffic all at the same time and maybe many CFs at the same time).

However, I have suspected for some time that our young size was a bit on the small side and after increasing it and giving the heap a few more GB to work with, it seems like things are behaving quite a bit better.

I mentioned issues with this patch when testing for CASSANDRA-2521. That was a problem caused by me. Was playing around with git for the first time and I manage to apply 2816 to a different branch than the one I used for testing.... :(

My appologies. 

Initial testing with that corrected looks a lot better for my small scale test case, but I noticed one time where I deleted an sstable and restarted. It did not get repaired (repair scanned but did nothing).

Not entirely sure what to make out of that, I then tested to delete another sstable and repair started running.

I will test more over the next days. 
","04/Jul/11 14:02;terjem;Things definitely seems to be improved overall, but weird things still happens.

So... 12 node cluster, this is maybe ugly, I know, but start repair on all of them.
Most nodes are fine, but one goes crazy. Disk use is now 3-4 times what it was before the repair started, and it does not seem to be done yet.

I have really no idea if this is the case, but I am getting the hunch that this node has ended up streaming out some of the data it is getting in. Would this be possible?
","04/Jul/11 14:30;slebresne;bq. So... 12 node cluster, this is maybe ugly, I know, but start repair on all of them.

Is it started on all of them ? If so, this is ""kind of"" expected in the sense that the patch assumes that each node does not do more than 2 repairs (for any column family) at the same time (this is configurable through the new concurrent_validators option, but it's probably better to stick to 2 and stagger the repair). If you do more than that (that is, if you did repair on all node at the same time and RF>2), then we're back on our old demons.

bq. I have really no idea if this is the case, but I am getting the hunch that this node has ended up streaming out some of the data it is getting in. Would this be possible?

Not really. That is, it could be that you create a merkle tree on some data and once you start streaming you, you're picking up data that was just streamed to you and wasn't there when computing the tree. This patch is suppose to fixes this in parts, but this can still happen if you do repairs in parallel on neighboring nodes. However, you shouldn't get into a situation where 2 nodes stream forever because they pick up what is just streamed to them for instance, because what is streaming is determined at the very beginning of the streaming session.

So my first question would be, was all those repair started in parallel. If yes, you shall not do this :). CASSANDRA-2606 and CASSANDRA-2610 are here to help making the repair of a full cluster much easier (and efficient), but right now it's more about getting patch in one at a time.
If the repairs were started one at a time in a rolling fashion, then we do have a unknown problem somewhere.","04/Jul/11 15:37;terjem;Cool!

Then you confirmed what I have sort of believed for a while, but my understanding of code has been a bit in conflict with:
http://wiki.apache.org/cassandra/Operations
which says:
""It is safe to run repair against multiple machines at the same time, but to minimize the impact on your application workload it is recommended to wait for it to complete on one node before invoking it against the next.""

I have always read that as ""if you have the HW, go for it!""

May I change to:
""It is safe to run repair against multiple machines at the same time. However, to minimize the amount of data transferred during a repair, careful synchronization is required between the nodes taking part of the repair. 

This is difficult to do if nodes with the same data replicas runs repair at the same time and doing so can in extreme cases generate excessive transfers of data. 

Improvements is being worked on, but for now, avoid scheduling repair on several nodes with replicas of the same data at the same time.""

","04/Jul/11 23:18;terjem;Regardless of change of documentation however, I don't think it should be possible to actually trigger a scenario like this in the first place.

The system should protect the user from that.

I also noticed that in this case, we have RF3. The node which is going somewhat crazy is number ""6"", however during the repair, it does log that it talks compares and streams data with node 4, 5, 7 and 8.

Seems like a couple of nodes too many?","04/Jul/11 23:18;terjem;Regardless of change of documentation however, I don't think it should be possible to actually trigger a scenario like this in the first place.

The system should protect the user from that.

I also noticed that in this case, we have RF3. The node which is going somewhat crazy is number ""6"", however during the repair, it does log that it talks compares and streams data with node 4, 5, 7 and 8.

Seems like a couple of nodes too many?","05/Jul/11 00:05;jbellis;bq. May I change to

Sure.

bq. The system should protect the user from that

I'm not sure that in a p2p design we can posit an omniscient ""the system.""","05/Jul/11 01:23;terjem;bq.I'm not sure that in a p2p design we can posit an omniscient ""the system.""

Is that a philosophical statement? :)

As Cassandra, at least for now, is a p2p network with fairly clearly defined boundaries, I will continue calling it a ""system"" for now :)

However, looking at it from the p2p viewpoint, the user potentially have no clue about where replicas are stored and given this, it may be impossible for the user to issue repair manually on more than one node at a time without getting in trouble. Given a large enough p2p setup, it would also be non-trivial to actually schedule a complete repair without ending up with 2 or more repairs running on the same replica set.

Since Cassandra do no checkpoint the synchronization so it is forced to rescan everything on every repair, repairs easily take so long that you are forced to run it on several nodes at a time if you are going to manage to finish repairing all nodes in 10 days...

Anyway, this is way outside the scope of this jira :)","05/Jul/11 02:07;terjem;bq. I also noticed that in this case, we have RF3. The node which is going somewhat crazy is number ""6"", however during the repair, it does log that it talks compares and streams data with node 4, 5, 7 and 8.

This is maybe correct. Node 7 will replicate to node 6 and 8 so 6 and 8 would share data.

So, to make things safe, even with this patch, every 4th node can run repair at the same time if RF=3?, but you still need to run repair on each of those 4 nodes to make sure it is all repaired?

As for the comment I made earlier.

To me, it looks like if the repair start triggering transfers on a large scale, the file the node get streamed in will not be streamed out, but this may get compacted before the repair finished and the compacted file I suspect gets streamed out and the repair just never finishe","05/Jul/11 15:42;jbellis;bq. The patch implements the idea of scheduling the merkle tree requests one by one, to make sure the tree are started as close as possible of ""the same time"". 

Can you point out where this happens in AES?

bq. This also put validation compaction in their own executor (to avoid them to be queued up behind standard compactions). That specific executor is created with 2 core threads, to allow for Peter's use case of wanting to do multiple repairs at the same time. That is, by default, you can do 2 repairs involving the same node and be ok

That feels like the wrong default to me.  I think you can make a case for one (minimal interference with the rest of the system) or unlimited (no weird ""cliff"" to catch the unwary repair operator).  But two is weird. :)","05/Jul/11 16:32;slebresne;bq. Can you point out where this happens in AES?

Mostly in AES.rendezvous and AES.RepairSession. Basically, RepairSession creates a queue of jobs, a job representing the repair of a given column family (for a given range, but that comes from the session itself). AES.rendezvous is then call for each received merkleTree. It waits to have all the merkeTree for the first job in the queue. When that is done, it dequeue the job (computing the merkle tree differences and scheduling streaming accordingly) and send the tree request for the next job in the queue.
Moreover, in StorageService.forceTableRepair(), when scheduling the repair for all the ranges of the node, we actually start the session for the first range and wait for all the ""jobs"" for this range to be done before starting the next session.

bq. That feels like the wrong default to me. I think you can make a case for one (minimal interference with the rest of the system) or unlimited (no weird ""cliff"" to catch the unwary repair operator). But two is weird.

Well the rational was the following one: if you set it to two, then you're saying that as soon as you start 2 repairs in parallel, they will start being inaccurate. But as Peter was suggesting (maybe in another ticket but anyway), if you have huge CF and tiny ones, it's nice to be able to repair on the tiny ones while a repair on the huge one(s) is running. Now, making it unlimited feels dangerous, because if you do so, it means that if the use start a lot of repair, all the validation compaction will start right away. This will kill the cluster (at least a few nodes if all those repair were started on the same node). It sounded better to have degraded precision for repair in those cases rather than basically killing the nodes. Maybe 2 or 4 may be a better default than 2, but 1 is a bit limited and unlimited is clearly much too dangerous.","13/Jul/11 21:30;jbellis;bq. making it unlimited feels dangerous, because if you do so, it means that if the use start a lot of repair, all the validation compaction will start right away

But the easy solution is ""don't do that.""

By setting a finite number greater than one, you have to restart machines when you realize ""oh, I want to have 3 simultaneous now.""

I'd rather keep it simple: make it unbounded, no configuration settings.  If you ignore the instructions to only run one repair at once, then either you know what you're doing (maybe you have SSDs) or you will find out very quickly and never do it again. :)","13/Jul/11 21:57;scode;I'm kinda +1 on the simple version w/o bounds but not too fussy since I can obviously set it very high for my use case. In any case, the most important part for mixed small/large type of situation is that concurrent repair is possible, even if configuration changes are needed.
","18/Jul/11 13:09;jbellis;bq. I'm kinda +1 on the simple version w/o bounds

Me too.  +1 with that change.",18/Jul/11 13:24;jbellis;(I'll go ahead and submit a version with that change.),"18/Jul/11 14:26;jbellis;rebased and switched to unbounded executor for validations.

tests do not compile but I believe that was already the case w/ v1 -- not sure what to do with blockUntilRunning, which was removed.","20/Jul/11 10:05;slebresne;bq. rebased

You rebased it against trunk while the fix version is still 0.8.2. I agree that this feel a bigger change that what we would want for a minor release, but repair is really a major pain point for users. And to tell the truth, it's worth in 0.8 than it is in 0.7 because even though the problem this patch solves exists in 0.7 as well, the splitting into range of repair made for 0.8 exacerbate those problems. Moreover this patch is fairly well delimited in what it changes, and it don't make any change to the wire protocol or anything that would make upgrades a problem. So I'm actually in favor of taking the small risk of pushing that in 0.8.2 (and be very vigilant to test repair extensively before the release). So for now, attaching a rebase with tests fixed against 0.8.

bq. and switched to unbounded executor for validations.

Ok, I realize that I'm not sure I understand what you mean by unbounded executor. In you rebased version, the ValidationExecutor apparently use the first constructor of DebuggableThreadPoolExecutor that will construct a mono-threaded executor, which is not what we want. Sure the queue of the executor will be unbounded, but if that was the problem, there was a misunderstanding, because the queue has always been unbounded, even in my initial patch. What concurrent_validators was dealing with is the number of core threads. And we need multiple core threads if we want to allow multiple concurrent repairs to work correctly.

Now the idea behind a default of 2 for the core threads was because I see a reason to want 2 concurrent repairs, but I don't see a very good reason to want more (and it's configurable if someone really need more). I'm glad to say it's not a marvelous default and the patch I've just attached used the same default than concurrent_compactors which is maybe less ""random"". Now we could have an executor with unbounded threads, that spawn a thread if needed making sure we never queue validation compaction, but that seems a little bit dangerous to me. It seems more reasonable to me to have a (configurable) reasonable number of threads and let validation queue up if the user start more than that number of concurrent repair (which will impact the precision of those repair, but it would be the user fault and it's a better way to deal with such fault than starting an unreasonable number of validation compaction that will starve memory (on likely more than one node btw)). But if you still think that it's better to have an unbounded number of threads, I won't fight over this.

bq. tests do not compile but I believe that was already the case w/ v1

Yes, I completely forgot to update the unit tests, sorry. Attached patch fixes those.
",20/Jul/11 17:11;jbellis;v4 attached against 0.8 with a corrected uncapped validation executor.,"20/Jul/11 17:45;slebresne;I think that if we don't want validation executor of v4 to ever queue tasks (which is what we need), then we need the executor queue to be a bounded queue of size 0 (i.e. that doesn't accept element). Indeed, as per the documentation of ThreadPoolExecutor:
{noformat}
If corePoolSize or more threads are running, the Executor always prefers queuing a request rather than adding a new thread.
{noformat} ",20/Jul/11 18:08;jbellis;You're right.  v5 attached.,"21/Jul/11 11:12;slebresne;Alright, v5 looks good to me. Committed, thanks.","21/Jul/11 12:31;hudson;Integrated in Cassandra-0.8 #231 (See [https://builds.apache.org/job/Cassandra-0.8/231/])
    Properly synchronize merkle tree computation
patch by slebresne; reviewed by jbellis for CASSANDRA-2816

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1149121
Files : 
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/conf/cassandra.yaml
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data from truncated CF reappears after server restart,CASSANDRA-2950,12515455,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,cdaw,cdaw,26/Jul/11 20:56,12/Mar/19 14:16,13/Mar/19 22:26,11/Aug/11 19:36,0.8.5,,,,,,1,,,,,"* Configure 3 node cluster
* Ensure the java stress tool creates Keyspace1 with RF=3

{code}
// Run Stress Tool to generate 10 keys, 1 column
stress --operation=INSERT -t 2 --num-keys=50 --columns=20 --consistency-level=QUORUM --average-size-values --replication-factor=3 --create-index=KEYS --nodes=cathy1,cathy2

// Verify 50 keys in CLI
use Keyspace1; 
list Standard1; 

// TRUNCATE CF in CLI
use Keyspace1;
truncate counter1;
list counter1;

// Run stress tool and verify creation of 1 key with 10 columns
stress --operation=INSERT -t 2 --num-keys=1 --columns=10 --consistency-level=QUORUM --average-size-values --replication-factor=3 --create-index=KEYS --nodes=cathy1,cathy2

// Verify 1 key in CLI
use Keyspace1; 
list Standard1; 

// Restart all three nodes

// You will see 51 keys in CLI
use Keyspace1; 
list Standard1; 
{code}


",,,,,,,,,,,,,,,,10/Aug/11 19:46;jbellis;2950-v2.txt;https://issues.apache.org/jira/secure/attachment/12490029/2950-v2.txt,11/Aug/11 12:53;slebresne;2950-v3_0.8.patch;https://issues.apache.org/jira/secure/attachment/12490112/2950-v3_0.8.patch,10/Aug/11 14:57;jbellis;2950.txt;https://issues.apache.org/jira/secure/attachment/12489986/2950.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-08-09 21:31:48.829,,,no_permission,,,,,,,,,,,,20907,,,Thu Aug 11 20:21:40 UTC 2011,,,,,,0|i0gegf:,93772,slebresne,slebresne,,,,,,,,,27/Jul/11 01:37;cdaw;This is a general issue with all CF's. updating bug.,"27/Jul/11 01:47;cdaw;The other permutation of this bug looked like, assuming write with CL.Q:
* Insert 50 (3 nodes up)
* truncate CF (3 nodes up)
* Insert 1 (3 nodes up)
* Bring node3 down
* Delete 1  (2 nodes up)
* Bring up node3 and run repair
* Take down node1 and node2.
* Query node3 with CL.ONE: list Standard1;  --- 30 rows returned

Not sure, but this looked suspicious in my logs:
{code}
 INFO 01:19:45,616 Streaming to /50.57.114.45
 INFO 01:19:45,689 Finished streaming session 698609583499991 from /50.57.107.176
 INFO 01:19:45,690 Finished streaming session 698609609994154 from /50.57.114.45
 INFO 01:19:46,501 Finished streaming repair with /50.57.114.45 for (0,56713727820156410577229101238628035242]: 0 oustanding to complete session
 INFO 01:19:46,531 Compacted to /var/lib/cassandra/data/Keyspace1/Standard1-tmp-g-106-Data.db.  16,646,523 to 16,646,352 (~99% of original) bytes for 30 keys.  Time: 1,509ms.
 INFO 01:19:46,930 Finished streaming repair with /50.57.107.176 for (113427455640312821154458202477256070484,0]: 1 oustanding to complete session
 INFO 01:19:47,619 Finished streaming repair with /50.57.114.45 for (113427455640312821154458202477256070484,0]: 0 oustanding to complete session
 INFO 01:19:48,232 Finished streaming repair with /50.57.107.176 for (56713727820156410577229101238628035242,113427455640312821154458202477256070484]: 1 oustanding to complete session
 INFO 01:19:48,856 Finished streaming repair with /50.57.114.45 for (56713727820156410577229101238628035242,113427455640312821154458202477256070484]: 0 oustanding to complete session
{code}","09/Aug/11 21:31;brandon.williams;Currently, truncate does:
* force a flush
* record the time
* delete any sstables older than the time

This isn't quite enough if the machine crashes shortly afterward, however, since there can be mutations present in the commitlog that were previously truncated and are now resurrected by CL replay.

One thing we could do is record the truncate time for the CF in the system ks and then ignore mutations older than that, however this would require time synchronization between the client and the server to be accurate.
","09/Aug/11 22:12;jbellis;but we record CL ""context"" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.

checked and we do wait for flush to complete in truncate.","09/Aug/11 22:19;brandon.williams;bq. but we record CL ""context"" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.

I think there's something wrong with that, then:

{noformat}
 INFO 21:25:15,274 Replaying /var/lib/cassandra/commitlog/CommitLog-1312924388053.log
DEBUG 21:25:15,290 Replaying /var/lib/cassandra/commitlog/CommitLog-1312924388053.log starting at 0
DEBUG 21:25:15,291 Reading mutation at 0
DEBUG 21:25:15,295 replaying mutation for system.4c: {ColumnFamily(LocationInfo [47656e65726174696f6e:false:4@1312924388140000,])}
DEBUG 21:25:15,321 Reading mutation at 89
DEBUG 21:25:15,322 replaying mutation for system.426f6f747374726170: {ColumnFamily(LocationInfo [42:false:1@1312924388203,])}
DEBUG 21:25:15,322 Reading mutation at 174
DEBUG 21:25:15,322 replaying mutation for system.4c: {ColumnFamily(LocationInfo [546f6b656e:false:16@1312924388204,])}
DEBUG 21:25:15,322 Reading mutation at 270
DEBUG 21:25:15,324 replaying mutation for Keyspace1.3030: {ColumnFamily(Standard1 [C0:false:34@1312924813259,C1:false:34@1312924813260,C2:false:34@1312924813260,C3:false:34@1312924813260,C4:false:34@1312924813260,])}
{noformat}

The last entry there is the first of many errant mutations.","10/Aug/11 14:57;jbellis;Ah, CASSANDRA-2419 keeps on giving...

bq. but we record CL ""context"" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.

The obvious problem with this is that the point of truncate is to blow away such sstables...  Patch attached.  Comment explains the core fix:

{noformat}
// Bonus complication: since we store replay position in sstable metadata,
// truncating those sstables means we will replay any CL segments from the
// beginning if we restart before they are discarded for normal reasons
// post-truncate.  So we need to (a) force a new segment so the currently
// active one can be discarded, and (b) flush *all* CFs so that unflushed
// data in others don't keep any pre-truncate CL segments alive.
{noformat}

Patch also fixes the bug in ReplayManagerTruncateTest that made it miss this.
","10/Aug/11 16:06;slebresne;I think the forceFlush of all the CF is not safe, because if for a given column family the memtable is clean, forceFlush will return immediately, even though there could be a memtable being flush at the same time (or pending flush). So we cannot be sure all the old segment are clean after the waitFutures (I know, it took me some time to figure out some problem with repair for this very reason when the repairs were not properly synchronized).

What we would need is to add to the future we wait on the futures of all the flush being processed at that time. Sounds annoying though. ","10/Aug/11 16:09;brandon.williams;+1, though this patch is against trunk, not 0.8.  Also mistakenly bumps the log4j level to debug.","10/Aug/11 19:46;jbellis;v2:

{noformat}
// Bonus bonus: simply forceFlush of all the CF is not enough, because if
// for a given column family the memtable is clean, forceFlush will return
// immediately, even though there could be a memtable being flush at the same
// time.  So to guarantee that all segments can be cleaned out, we need
// ""waitForActiveFlushes"" after the new segment has been created.
{noformat}","11/Aug/11 12:53;slebresne;Attaching a v3 that is rebased against 0.8. I've also slightly change the logic in Truncate to submit all the flushes and then call waitForActiveFlushes, as this is slightly simpler and should work equally well as far as I can tell.
Apart from that, this lgtm.",11/Aug/11 19:36;jbellis;committed,"11/Aug/11 20:21;hudson;Integrated in Cassandra-0.8 #272 (See [https://builds.apache.org/job/Cassandra-0.8/272/])
    make sure truncate clears out the commitlog
patch by jbellis; reviewed by slebresne for CASSANDRA-2950

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156763
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush memtables on shutdown when durable writes are disabled,CASSANDRA-2958,12515554,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,electrum,electrum,27/Jul/11 17:26,12/Mar/19 14:16,13/Mar/19 22:26,01/Aug/11 18:44,0.8.3,,,,,,0,,,,,"Memtables need to be flushed on shutdown when durable_writes is set to false, otherwise data loss occurs as the data is not available to be replayed from the commit log. ",,,,,,,,,,,,,,,,27/Jul/11 17:43;jbellis;2958.txt;https://issues.apache.org/jira/secure/attachment/12488002/2958.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-27 17:43:38.004,,,no_permission,,,,,,,,,,,,19339,,,Tue Oct 02 18:38:17 UTC 2012,,,,,,0|i0geif:,93781,tjake,tjake,,,,,,,,,27/Jul/11 17:43;jbellis;You're right.  Here's a patch to add flushing of non-durable CFs to the shutdown hook.,27/Jul/11 17:45;jbellis;(Original durable_writes option: CASSANDRA-2683),27/Jul/11 20:38;tjake;+1,30/Jul/11 02:48;jbellis;committed,"30/Jul/11 03:22;hudson;Integrated in Cassandra-0.8 #245 (See [https://builds.apache.org/job/Cassandra-0.8/245/])
    Flush memtables on shutdown when durable writes are disabled
patch by jbellis; reviewed by tjake for CASSANDRA-2958

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152419
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/LazilyCompactedRow.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
",01/Aug/11 17:40;electrum;It looks like waitOnFutures() can return a null pointer exception if forceFlush() returns null?,01/Aug/11 18:44;jbellis;thanks -- fixed in r1152891,"01/Aug/11 19:14;hudson;Integrated in Cassandra-0.8 #250 (See [https://builds.apache.org/job/Cassandra-0.8/250/])
    avoid NPE when flushing in shutdown hook
patch by jbellis for CASSANDRA-2958

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152891
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
","24/Aug/12 22:40;rcoli;If I am running with ""durable_writes off,"" I know this means ""Data in a memtable is not shadowed by data in the commitlog and is therefore lost when memory is cleared.""

As a result, when my node crashes or is shut down, I *expect* to lose data that is not flushed. If I want to prevent this from happening, I can run ""nodetool drain"" to flush my memtables and prevent any new ones from being created.

This patch violates the principle of least surprise by triggering a flush as a side effect of shutdown, but only in certain cases. Now, instead of knowing that unflushed/drained data is lost with durable_writes off, I have to understand that I lose my data if my node crashes and keep my data if I manually shut it down. ""Data in a memtable is not shadowed by data in the commitlog and is therefore lost when memory is cleared"" becomes an incorrect heuristic for understanding durable_writes. After this patch, the formerly unambiguous ""durable_writes off"" setting seems to mean something like ""durable_writes sometimes_off"".

While I laud the goal of not losing peoples data, I assert that when you explicitly tell Cassandra to do something that by design loses data, Cassandra should do what you tell it to do.","24/Aug/12 22:46;jbellis;if you want data loss from non-durable writes, kill -9 is your friend.  otherwise, our goal is to not lose more than necessary.","31/Aug/12 21:49;rcoli;What I ""want"" is to be able to explain Cassandra to customers in simple sentences like ""a memtable holds changes until it is full enough to hit a flush condition or you explicitly flush it"" without having to pepper these sentences with caveats like ""except if you restart your node or if you stop it with durable_writes off, triggering an unexpected flush."" Patches such as this one, where vaguely defined ends appear to justify whatever ad-hoc inconsistent means, do not appear to further this goal.

Let me phrase my objection to this patch in another way...

""What does this patch gain us, and at what cost?""

Your stated goal is to not lose more than ""necessary"" when stopping a node. It seems your goal can be achieved without patching, by simply advising ""durable_writes off"" operators to run ""nodetool drain"" when stopping a node. They are, after all, the ones stopping their node and are perfectly capable of draining it if they do not want to lose the explicitly non-durable non-durable_writes contents of memtables.

From what I can tell, the only thing this patch gains us is ""people who are running with durable_writes off don't have to run 'nodetool drain' before stopping nodes.""

What we trade for that is the until-now universal expectation that stopping a Cassandra node never triggers a flush.

Is ""the very small group of operators who run with non-durable writes don't have to run 'nodetool drain'"" such a compelling win that we should change a fundamental behavior of Cassandra, making it less predictable, in order to obtain it? My answer is no.","31/Aug/12 21:57;brandon.williams;bq. be able to explain Cassandra to customers in simple sentences like ""a memtable holds changes until it is full enough to hit a flush condition or you explicitly flush it""

""a memtable holds changes until it hits a flush condition like being full or graceful shutdown, or you explicitly flush it.""","04/Sep/12 19:25;rcoli;> ""a memtable holds changes until it hits a flush condition like being full or graceful shutdown, or you explicitly flush it.""

This is not an unambiguous summary of the current behavior. It is true only if durable_writes are off. If durable_writes are on, graceful shutdown does not flush.

""a memtable holds changes until it hits a flush condition like being full (or, if durable_writes are disabled, on graceful shutdown), or you explicitly flush it""

Seems to unambiguously describe the current behavior and doesn't read very well.

I gather from CASSANDRA-3564 that there is interest in extending this flush-on-graceful shutdown behavior to occur in all cases of graceful shutdown. If that happens, your sentence will be a correct summation of Cassandra's newly predictable behavior. It will also answer my primary objection here, which is that the flush only occurs in *some* graceful shutdown cases. :)","04/Sep/12 19:31;jbellis;Robert, we use assign-to to track who fixed an issue.  Assigning to yourself at this point is not appropriate.","02/Oct/12 18:38;rcoli;Jonathan, I had no intent of assigning this ticket to myself or wiping the description body, both of which I apparently accidentally did in the process of clicking on this ticket. I will restore the description body from the history, sorry for the confusion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError during compaction of CF with counter columns,CASSANDRA-2968,12515766,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,tarasp,tarasp,29/Jul/11 12:00,12/Mar/19 14:16,13/Mar/19 22:26,02/Aug/11 15:05,0.8.3,,,,,,2,,,,,"Having upgraded from 0.8.0 to 0.8.2 we ran nodetool compact and got

Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager.performMajor(CompactionManager.java:277)
        at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1762)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1358)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.AssertionError                                                                                                                                                                                                          
        at org.apache.cassandra.db.context.CounterContext.removeOldShards(CounterContext.java:593)                                                                                                                                           
        at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:237)                                                                                                                                                     
        at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:256)                                                                                                                                                     
        at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:88)                                                                                                                                                
        at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:140)                                                                                                                            
        at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:123)                                                                                                                                     
        at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:43)                                                                                                                                      
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)                                                                                                                                                 
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)                                                                                                                                            
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)                                                                                                                                                     
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)                                                                                                                                    
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)                                                                                                                                           
        at org.apache.cassandra.db.compaction.CompactionManager.doCompactionWithoutSizeEstimation(CompactionManager.java:569)                                                                                                                
        at org.apache.cassandra.db.compaction.CompactionManager.doCompaction(CompactionManager.java:506)                                                                                                                                     
        at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:319)                                                                                                                                           
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)                                                                                                                                                                
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)                                                                                                                                                                          
        ... 3 more",CentOS release 5.6,,,,,,,,,,,,,,,01/Aug/11 17:22;slebresne;2968.patch;https://issues.apache.org/jira/secure/attachment/12488422/2968.patch,31/Jul/11 12:43;andrden;AffiliateActivity-g-147-Data.db;https://issues.apache.org/jira/secure/attachment/12488352/AffiliateActivity-g-147-Data.db,31/Jul/11 12:43;andrden;AffiliateActivity-g-147-Index.db;https://issues.apache.org/jira/secure/attachment/12488353/AffiliateActivity-g-147-Index.db,30/Jul/11 13:18;andrden;AffiliateActivity-g-195-Data.db;https://issues.apache.org/jira/secure/attachment/12488300/AffiliateActivity-g-195-Data.db,30/Jul/11 13:18;andrden;AffiliateActivity-g-195-Index.db;https://issues.apache.org/jira/secure/attachment/12488301/AffiliateActivity-g-195-Index.db,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-07-30 13:18:57.597,,,no_permission,,,,,,,,,,,,20918,,,Tue Aug 02 16:36:05 UTC 2011,,,,,,0|i0gekf:,93790,jbellis,jbellis,,,,,,,,,"30/Jul/11 13:18;andrden;Reproducible test case for this bug:

create column family AffiliateActivity with default_validation_class=CounterColumnType
   and key_validation_class=LongType and comparator=AsciiType
   and memtable_throughput=30
   and memtable_operations=0.5
   and replicate_on_write=true;

put AffiliateActivity-g-195-Data.db and AffiliateActivity-g-195-Index.db (attached) into cassandra 0.8.2 data directory for some keyspace, then run cassandra server to open the files and run nodetool scrub

Those AffiliateActivity-g-195 data files were originally created with cassandra 0.8

java.io.IOError: java.lang.AssertionError
	at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:775)
	at org.apache.cassandra.db.compaction.CompactionManager.doScrub(CompactionManager.java:631)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:65)
	at org.apache.cassandra.db.compaction.CompactionManager$3.call(CompactionManager.java:251)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.AssertionError
	at org.apache.cassandra.db.context.CounterContext.removeOldShards(CounterContext.java:593)
	at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:237)
	at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:256)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:88)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:140)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:146)
	at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:719)
	... 8 more


","30/Jul/11 14:41;andrden;Debugger shows at the exception breakpoint in the test case described that
""state.getCount()""	-9132236803706623116	
""state.getClock()""	-9132236803706623116	

and context.array() (exactly 100 bytes in size) is the following so the duplication is really there, 
and this record in sstable is likely corrupted - but how come!

000100012335e4a0b2c911e000006bb62b336af7 8143c702fe516b74 8143c702fe516b74
23d44780b2c911e00000f03fd2ec87ff000000000000b3c
0000000000000b3c025188750b2c911e0000058b7809d76fff082e59147f68e9cf082e59147f68e9c
","30/Jul/11 16:59;andrden;yes, sstable likely corrupted but by which code? 0.8.0 or 0.8.2?

sstable2json AffiliateActivity-g-195-Data.db
(first row is parsed ok, second has state.getCount()==state.getClock())

....
""00000000019d499f"": [[""clicks"",""000100012335e4a0b2c911e000006bb62b336af7000000000000285c000000000000285c23d44780b2c911e00000f03fd2ec87ff0000000000000002000000000000000225188750b
2c911e0000058b7809d76ff00000000000000690000000000000069"",1311664683623,""c"",-9223372036854775808]],
""0000000000748b45"": [[""clicks"",""000100012335e4a0b2c911e000006bb62b336af78143c702fe516b748143c702fe516b7423d44780b2c911e00000f03fd2ec87ff000000000000b3c0000000000000b3c025188750b
2c911e0000058b7809d76fff082e59147f68e9cf082e59147f68e9c"",1311665756252,""c"",-9223372036854775808]],
....

cassandra-cli:
get AffiliateActivity[27085215];          
=> (counter=clicks, value=10439) - reasonable value

get AffiliateActivity[7637829]; 
=> (counter=clicks, value=8198429924508872144) - nonsense value

","31/Jul/11 12:43;andrden;actually 0.8.2 continues to write non-compactable sstables :-(

AffiliateActivity-g-147-Data.db is a recent file written 3 days after upgrade from 0.8.0 to 0.8.2 and it cannot be even scrubbed

The pattern I can see is that we have only every other sstable file left in data directory - AffiliateActivity-g-133, 135, 137, 139, 141, 143, 145, 147. Not quite sure - but it seems 1 compaction is done to new files - otherwise where are the even numbered ones? - but those files which resulted from first compaction are broken?

Anyway that's purely 0.8.2 problem, not any upgrade issue",31/Jul/11 17:09;ambroff;For what it's worth I've seen this same bug in 0.8.1.,"01/Aug/11 12:38;andrden;no, I was wrong, compaction is not the culprit

I ran nodetool flush and produced 3 new small sstables - strangely they are all numbered with odd numbers only 153 155 157 - but the point is a freshly made sstable with only 8 rows is already unreadable by compaction/scrub.

Maybe header in counter binary value is not written or what...

{code}
{
""0000000000cc71a4"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af700000000000000010000000000000001"",1312201208019,""c"",-9223372036854775808]],
""0000000000748b45"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af7000000000000009f000000000000009f23d44780b2c911e00000f03fd2ec87ffa6cd8bac6eb8a32d2fd3c2059ade464925188750b2c911e0000058b7809d76ff9a8b961aed94ef799a8b961aed94ef79"",1312201220709,""c"",-9223372036854775808]],
""00000000010a4465"": [[""clicks"",""00002335e4a0b2c911e000006bb62b336af700000000000045ad00000000000045ad23d44780b2c911e00000f03fd2ec87ff0000000000004bfd0000000001fae30325188750b2c911e0000058b7809d76ff000000000000038f000000000000038f"",1312201219737,""c"",-9223372036854775808]],
""0000000001319592"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af70000000000000001000000000000000123d44780b2c911e00000f03fd2ec87ffdf1636876c1c1bc2df1636876c1c1bc225188750b2c911e0000058b7809d76ff0876b1020dc02ac97f12b88d9f9df751"",1312201215475,""c"",-9223372036854775808]],
""0000000001b9a53e"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af700000000000000010000000000000001"",1312201202338,""c"",-9223372036854775808]],
""00000000004dd84f"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af70000000000000001000000000000000123d44780b2c911e00000f03fd2ec87ff000000000017d1550000000059fb2ad925188750b2c911e0000058b7809d76ff00000000000173bf00000000000173bf"",1312201205636,""c"",-9223372036854775808]],
""0000000000d1d52f"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af700000000000000020000000000000002"",1312201210515,""c"",-9223372036854775808]],
""0000000001410d4a"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af700000000000000010000000000000001"",1312201220365,""c"",-9223372036854775808]]
}
{code}","01/Aug/11 17:22;slebresne;This is actually a pretty stupid bug (not that there is smart bug): the old NodeId for the local node were read from the system table in reversed order while they shouldn't. The wrong path was then taken based on that mistake. No data was lost due to that (i.e, the total value of the counters is preserved), but non-sensical counter context were created (hence triggering the assertion).

Fixing the root cause is pretty straightforward. Fixing the nonsensical counter contexts is more subtle, but it is doable up to the fact that the local NodeId on the node(s) where the assertion is triggered will have to be renewed. Attaching a patch that does both (fixing root cause and repairing the bad data). Also add two unit tests, one for the root cause and one to check that the bad data repair code does what it is supposed to do.

After applying that patch (or upgrading on a release shipping it), you will (potentially) need to restart the node with the -Dcassandra.renew_counter_id=true (compaction will still fail if you don't but with a message saying that you should restart with the startup flag).",02/Aug/11 14:17;jbellis;+1 w/ println removed :),"02/Aug/11 15:05;slebresne;Committed (without println), thanks","02/Aug/11 16:36;hudson;Integrated in Cassandra-0.8 #252 (See [https://builds.apache.org/job/Cassandra-0.8/252/])
    Fix assertion error during compaction of counter CFs
patch by slebresne; reviewed by jbellis for CASSANDRA-2968

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1153156
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/LazilyCompactedRow.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/SchemaLoader.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/utils/NodeId.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/context/CounterContext.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/PrecompactedRow.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/CounterMutationTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issues with parameters being escaped correctly in Python CQL,CASSANDRA-2993,12517966,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,bcvisin,bcvisin,04/Aug/11 21:54,12/Mar/19 14:16,13/Mar/19 22:26,10/Aug/11 14:37,0.8.4,,,,,,0,CQL,parameter,python,,"When using parameterised queries in Python CQL strings are not being escaped correctly.


Query and Parameters:
{code}
'UPDATE sites SET :col = :val WHERE KEY = :site_id'

{'col': 'feed_stats:1312493736688033024',
 'site_id': '899d15e8-bd4a-11e0-bc8c-001fe14cba06',
 'val': ""(dp0\nS'1'\np1\n(lp2\nI1\naI2\naI3\naI4\nasS'0'\np3\n(lp4\nI1\naI2\naI3\naI4\nasS'3'\np5\n(lp6\nI1\naI2\naI3\naI4\nasS'2'\np7\n(lp8\nI1\naI2\naI3\naI4\nas.""}
{code}

Query trying to be executed after processing parameters
{code}     
""UPDATE sites SET 'feed_stats:1312493736688033024' = '(dp0\nS''1''\np1\n(lp2\nI1\naI2\naI3\naI4\nasS''0''\np3\n(lp4\nI1\naI2\naI3\naI4\nasS''3''\np5\n(lp6\nI1\naI2\naI3\naI4\nasS''2''\np7\n(lp8\nI1\naI2\naI3\naI4\nas.' WHERE KEY = '899d15e8-bd4a-11e0-bc8c-001fe14cba06'""

{code}",Python CQL,,,,,,,,,,,,,,,08/Aug/11 19:49;thobbs;2993-cql-grammar.txt;https://issues.apache.org/jira/secure/attachment/12489734/2993-cql-grammar.txt,08/Aug/11 19:49;thobbs;2993-pycql.txt;https://issues.apache.org/jira/secure/attachment/12489733/2993-pycql.txt,08/Aug/11 19:49;thobbs;2993-system-test.txt;https://issues.apache.org/jira/secure/attachment/12489735/2993-system-test.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-08-08 19:49:41.397,,,no_permission,,,,,,,,,,,,20928,,,Wed Aug 10 15:24:53 UTC 2011,,,,,,0|i0gepz:,93815,xedin,xedin,,,,,,,,,04/Aug/11 21:55;bcvisin;This is using pickle to serialise objects in python.,"08/Aug/11 19:49;thobbs;I think this is primarily an issue with multiline string literals, both in the CQL grammar and in the python driver.

2993-pycql.txt updates the python cql driver regex to handle multiline SELECT clauses.

2993-cql-grammar.txt makes the STRING_LITERAL token include '\r' and '\n'.

2993-system-test.txt adds a system test for multiline keys, names, and values.",08/Aug/11 22:52;bcvisin;I believe Tyler is correct in his diagnosis and I think that the patches should fix the issues I am having.,"08/Aug/11 23:17;bcvisin;Try:

{code}
cql = 'UPDATE sites SET :col = :val WHERE KEY = :site_id'

vals = {'col': 'feed_stats:1312493736688033024',
 'site_id': '29ffb9d2-c205-11e0-a2a2-001fe14cba06',
 'val': ""(dp0\nS'1'\np1\n(lp2\nI1\naI2\naI3\naI4\nasS'0'\np3\n(lp4\nI1\naI2\naI3\naI4\nasS'3'\np5\n(lp6\nI1\naI2\naI3\naI4\nasS'2'\np7\n(lp8\nI1\naI2\naI3\naI4\nas.""}

cursor = cql.connect('cf')
cursor.execute(cql, vals)
{code}

My Result (after patching with Tyler's pycql.txt, but not with cql-grammar.txt)
I am printing out the prepared query
{code}
USE totalporn;
UPDATE sites SET 'feed_stats:1312493736688033024' = '(dp0
S''1''
p1
(lp2
I1
aI2
aI3
aI4
asS''0''
p3
(lp4
I1
aI2
aI3
aI4
asS''3''
p5
(lp6
I1
aI2
aI3
aI4
asS''2''
p7
(lp8
I1
aI2
aI3
aI4
as.' WHERE KEY = '29ffb9d2-c205-11e0-a2a2-001fe14cba06'
Traceback (most recent call last):
  File ""/home/blake/test.py"", line 10, in <module>
    cursor.execute(sql, vals)
  File ""/usr/local/lib/python2.7/dist-packages/cql/cursor.py"", line 135, in execute
    raise cql.ProgrammingError(""Bad Request: %s"" % ire.why)
cql.ProgrammingError: Bad Request: line 30:3 mismatched character ''' expecting '.'
{code}


The string comes from a pickle'd dict contaning strings as keys and a list as values:

{code}

import pickle

vals = {'col': 'feed_stats:1312493736688033024',
 'site_id': '29ffb9d2-c205-11e0-a2a2-001fe14cba06',
 'val': ""(dp0\nS'1'\np1\n(lp2\nI1\naI2\naI3\naI4\nasS'0'\np3\n(lp4\nI1\naI2\naI3\naI4\nasS'3'\np5\n(lp6\nI1\naI2\naI3\naI4\nasS'2'\np7\n(lp8\nI1\naI2\naI3\naI4\nas.""}

print pickle.loads((vals['val']))

{code}
returns
{code}
{'1': [1, 2, 3, 4], '0': [1, 2, 3, 4], '3': [1, 2, 3, 4], '2': [1, 2, 3, 4]}
{code}


","09/Aug/11 03:08;thobbs;Blake: after applying the cql-grammar patch (and doing ""ant clean"" before ""ant""), this test case works for me.",09/Aug/11 19:57;bcvisin;Works for me too.  Thanks Tyler!,"10/Aug/11 14:37;xedin;committed, thanks!","10/Aug/11 15:24;hudson;Integrated in Cassandra-0.8 #266 (See [https://builds.apache.org/job/Cassandra-0.8/266/])
    Fixes issues with parameters being escaped incorrectly in Python CQL
patch by Tyler Hobbs; reviewed by Pavel Yaskevich for CASSANDRA-2993

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156198
Files : 
* /cassandra/branches/cassandra-0.8/test/system/test_cql.py
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/drivers/py/cql/cursor.py
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/Cql.g
* /cassandra/drivers/py/test/test_regex.py
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Versioning works *too* well,CASSANDRA-2860,12512897,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,05/Jul/11 21:28,12/Mar/19 14:15,13/Mar/19 22:26,19/Jul/11 22:03,0.8.2,,,,,,0,,,,,"The scenario goes something like this: you upgrade from 0.7 to 0.8, but all the nodes remember that the remote side is 0.7, so they in turn speak 0.7, causing the local node to also think the remote is 0.7, even though both are really 0.8.",,,,,,,,,,,,CASSANDRA-3166,,,,13/Jul/11 22:13;jbellis;2860-v2.txt;https://issues.apache.org/jira/secure/attachment/12486379/2860-v2.txt,06/Jul/11 16:10;jbellis;2860.txt;https://issues.apache.org/jira/secure/attachment/12485441/2860.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-06 16:10:44.0,,,no_permission,,,,,,,,,,,,20868,,,Sat Jul 16 02:15:16 UTC 2011,,,,,,0|i0gdwf:,93682,brandon.williams,brandon.williams,,,,,,,,,06/Jul/11 16:10;jbellis;patch to reset protocol-version-to-attempt when MessagingService resets a connection pool (when FD notices it's down),"13/Jul/11 22:13;jbellis;v2 moves reset call into OTC.disconnect, so it will work even if the restart time is too small for FD to kick in",14/Jul/11 18:04;brandon.williams;+1,"16/Jul/11 02:15;hudson;Integrated in Cassandra-0.8 #218 (See [https://builds.apache.org/job/Cassandra-0.8/218/])
    reset protocol-version-to-attempt when reconnecting to a node
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2860

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147355
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush and Compaction Unnecessarily Allocate 256MB Contiguous Buffers,CASSANDRA-2463,12504105,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,cscotta,cscotta,cscotta,12/Apr/11 21:23,12/Mar/19 14:15,13/Mar/19 22:26,13/Apr/11 05:34,0.7.5,,,,,,0,,,,,"Currently, Cassandra 0.7.x allocates a 256MB contiguous byte array at the beginning of a memtable flush or compaction (presently hard-coded as Config.in_memory_compaction_limit_in_mb). When several memtable flushes are triggered at once (as by `nodetool flush` or `nodetool snapshot`), the tenured generation will typically experience extreme pressure as it attempts to locate [n] contiguous 256mb chunks of heap to allocate. This will often trigger a promotion failure, resulting in a stop-the-world GC until the allocation can be made. (Note that in the case of the ""release valve"" being triggered, the problem is even further exacerbated; the release valve will ironically trigger two contiguous 256MB allocations when attempting to flush the two largest memtables).

This patch sets the buffer to be used by BufferedRandomAccessFile to Math.min(bytesToWrite, BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE) rather than a hard-coded 256MB. The typical resulting buffer size is 64kb.

I've taken some time to measure the impact of this change on the base 0.7.4 release and with this patch applied. This test involved launching Cassandra, performing four million writes across three column families from three clients, and monitoring heap usage and garbage collections. Cassandra was launched with 2GB of heap and the default JVM options shipped with the project. This configuration has 7 column families with a total of 15GB of data.

Here's the base 0.7.4 release:
http://cl.ly/413g2K06121z252e2t10

Note that on launch, we see a flush + compaction triggered almost immediately, resulting in at least 7x very quick 256MB allocations maxing out the heap, resulting in a promotion failure and a full GC. As flushes proceeed, we see that most of these have a corresponding CMS, consistent with the pattern of a large allocation and immediate collection. We see a second promotion failure and full GC at the 75% mark as the allocations cannot be satisfied without a collection, along with several CMSs in between. In the failure cases, the allocation requests occur so quickly that a standard CMS phase cannot completed before a ParNew attempts to promote the surviving byte array into the tenured generation. The heap usage and GC profile of this graph is very unhealthy.

Here's the 0.7.4 release with this patch applied:
http://cl.ly/050I1g26401B1X0w3s1f

This graph is very different. At launch, rather than a immediate spike to full allocation and a promotion failure, we see a slow allocation slope reaching only 1/8th of total heap size. As writes begin, we see several flushes and compactions, but none result in immediate, large allocations. The ParNew collector keeps up with collections far more ably, resulting in only one healthy CMS collection with no promotion failure. Unlike the unhealthy rapid allocation and massive collection pattern we see in the first graph, this graph depicts a healthy sawtooth pattern of ParNews and an occasional effective CMS with no danger of heap fragmentation resulting in a promotion failure.

The bottom line is that there's no need to allocate a hard-coded 256MB write buffer for flushing memtables and compactions to disk. Doing so results in unhealthy rapid allocation patterns and increases the probability of triggering promotion failures and full stop-the-world GCs which can cause nodes to become unresponsive and shunned from the ring during flushes and compactions.",Any,259200,259200,,0%,259200,259200,,,,,,,,,12/Apr/11 22:07;jbellis;2463-v2.txt;https://issues.apache.org/jira/secure/attachment/12476182/2463-v2.txt,12/Apr/11 21:28;cscotta;patch.diff;https://issues.apache.org/jira/secure/attachment/12476177/patch.diff,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-12 22:07:20.084,,,no_permission,,,,,,,,,,,,20638,,,Wed Apr 13 05:34:50 UTC 2011,,,,,,0|i0gbiv:,93297,jbellis,jbellis,,,,,,,,,12/Apr/11 21:26;cscotta;[ Patch attached ],12/Apr/11 21:28;cscotta;Patch attached. Applies cleanly to tag 'cassandra-0.7.4'. All tests pass.,"12/Apr/11 22:07;jbellis;I started making it more complicated:

{code}
        // the gymnastics here are because
        //  - we want the buffer large enough that we're not re-buffering when we have to seek back to the
        //    start of a row to write the data size.  Here, ""10% larger than the average row"" is ""large enough,""
        //    meaning we expect to seek and rebuffer about 1/10 of the time.
        //  - but we don't want to allocate a huge buffer unnecessarily for a small amount of data
        //  - and on the low end, we don't want to be absurdly stingy with the buffer size for small rows
        assert estimatedSize > 0;
        long maxBufferSize = Math.min(DatabaseDescriptor.getInMemoryCompactionLimit(), 1024 * 1024);
        int bufferSize;
        if (estimatedSize < 64 * 1024)
        {
            bufferSize = (int) estimatedSize;
        }
        else
        {
            long estimatedRowSize = estimatedSize / keyCount;
            bufferSize = (int) Math.min(Math.max(1.1 * estimatedRowSize, 64 * 1024), maxBufferSize);
        }
{code}

...  but the larger our buffer is, the larger the penalty for guessing wrong when we have to seek back and rebuffer.

Then I went through and added size estimation to the CompactionManager, until I thought ""it's kind of ridiculous to be worrying about saving a few bytes less than 64KB, especially when we expect most memtables to have more data in them than 64K when flushed.""

Thus, I arrived at the patch Antoine de Saint-Exupery would have written, attached as v2.","12/Apr/11 22:44;scode;A noteworthy factor here is that unless an fsync()+fadvise()/madvise() have evicted data, in the normal case this stuff should still be in page cache for any reasonably sized row. For truly huge rows, the penalty of seeking back should be insignificant anyway.

Total +1 on avoiding huge allocations. I was surprised to realize, when this ticket came along, that this was happening ;)

I have been suspecting that the bloom filters are a major concern too with respect to triggering promotion failures (but I haven't done testing to confirm this). Are there other cases than this and the bloom filters where we know that we're doing large allocations?",12/Apr/11 23:58;jbellis;(I wonder if this is the cause of the intermittent load-spikes-after-upgrade-to-0.7 reports we've seen.),"13/Apr/11 00:56;eonnen;As a data point to that question, we hardly ever had CMS collections on 0.6.8 and maybe one full GC ever that I can think of for what was years of cumulative uptime. It surely differs for workloads, but in our case 0.7 got much worse along the CMS dimension.","13/Apr/11 03:45;scode;Filed CASSANDRA-2466 for the bloom filter case.
","13/Apr/11 05:34;jbellis;First time I got a +1 via Twitter: http://twitter.com/#!/cscotta/status/58031493565513728

committed.","13/Apr/11 05:34;jbellis;Thanks for tracking this down, Scott and Erik!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
o.a.c.dht.Range.differenceToFetch() doesn't handle all cases correctly,CASSANDRA-3084,12520286,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thobbs,thobbs,thobbs,26/Aug/11 19:06,12/Mar/19 14:14,13/Mar/19 22:26,30/Aug/11 05:50,0.8.5,,,,,,0,,,,,"It's possible that differenceToFetch is making implicit assumptions about the relationship between the two ranges, but the following cases are not handled correctly (the old range is (A, B], the new is (C, D]:

{noformat}
--C--A-----B--D--
{noformat}

Here, the result will be (C, A] and (D, B], instead of (C, A] and (B, D].

{noformat}
--C--A-----D--B--
{noformat}

The result will be (C, D] instead of just (C, A].

{noformat}
--A--C-----D--B--
{noformat}

The result will be (B, D] when nothing needs to be transfered.

If there is some kind of implicit assumption that these cases won't arise, it either needs to be explicit (assertions, exceptions) or the cases need to be handled.  It should be easy to cover this with unit tests.",,,,,,,,,,,,,,,,29/Aug/11 17:33;thobbs;3084-unit-test.txt;https://issues.apache.org/jira/secure/attachment/12492101/3084-unit-test.txt,29/Aug/11 19:17;thobbs;3084-v2.txt;https://issues.apache.org/jira/secure/attachment/12492114/3084-v2.txt,29/Aug/11 17:33;thobbs;3084.txt;https://issues.apache.org/jira/secure/attachment/12492102/3084.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-08-29 18:15:43.935,,,no_permission,,,,,,,,,,,,20961,,,Tue Aug 30 06:15:16 UTC 2011,,,,,,0|i0gfhb:,93938,stuhood,stuhood,,,,,,,,,"29/Aug/11 17:33;thobbs;3084.txt covers all cases of possible range differences.

3084-unit-test.txt provides unit tests that fail before the patch.

The logic is definitely a bit hairy here, so any thoughts on how to simplify it are welcome.","29/Aug/11 18:15;stuhood;Couldn't a bunch of difference cases be eliminated by taking advantage of the intersection implementation? The difference between ranges x and y would be {{z = x.intersect\(y\); y.subtract(z)}}, and I think a subtract method with a ""y must contain z"" precondition would be much easier to implement.","29/Aug/11 19:17;thobbs;Good idea!

3084-v2.txt simplifies the logic by adding a subtractContained() method.","30/Aug/11 05:16;stuhood;+1 Awesome.
_nitpick: brackets should go on new lines in differenceToFetch_",30/Aug/11 05:50;jbellis;committed with braces fixed.  thanks Tyler and Stu!,"30/Aug/11 06:15;hudson;Integrated in Cassandra-0.8 #300 (See [https://builds.apache.org/job/Cassandra-0.8/300/])
    fix corner cases in Range.differenceToFetch
patch by Tyler Hobbs; reviewed by Stu Hood for CASSANDRA-3084

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163090
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/dht/Range.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/dht/RangeTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot Create Duplicate Compaction Marker,CASSANDRA-2769,12510353,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,bcoverston,bcoverston,14/Jun/11 20:15,12/Mar/19 14:14,13/Mar/19 22:26,17/Jun/11 15:01,0.8.2,1.0.0,,,,,0,,,,,"Concurrent compaction can trigger the following exception when two threads compact the same sstable. DataTracker attempts to prevent this but apparently not successfully.

java.io.IOError: java.io.IOException: Unable to create compaction marker
	at org.apache.cassandra.io.sstable.SSTableReader.markCompacted(SSTableReader.java:638)
	at org.apache.cassandra.db.DataTracker.removeOldSSTablesSize(DataTracker.java:321)
	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:294)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:932)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:173)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:119)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:102)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.io.IOException: Unable to create compaction marker
	at org.apache.cassandra.io.sstable.SSTableReader.markCompacted(SSTableReader.java:634)
	... 12 more",,,,,,,,,,,,,,,,15/Jun/11 11:43;slebresne;0001-0.8.0-Remove-useless-unmarkCompacting-in-doCleanup.patch;https://issues.apache.org/jira/secure/attachment/12482653/0001-0.8.0-Remove-useless-unmarkCompacting-in-doCleanup.patch,17/Jun/11 10:18;slebresne;0001-Do-compact-only-smallerSSTables-v2.patch;https://issues.apache.org/jira/secure/attachment/12482916/0001-Do-compact-only-smallerSSTables-v2.patch,15/Jun/11 11:43;slebresne;0001-Do-compact-only-smallerSSTables.patch;https://issues.apache.org/jira/secure/attachment/12482654/0001-Do-compact-only-smallerSSTables.patch,17/Jun/11 10:18;slebresne;0002-Only-compact-what-has-been-succesfully-marked-as-com-v2.patch;https://issues.apache.org/jira/secure/attachment/12482917/0002-Only-compact-what-has-been-succesfully-marked-as-com-v2.patch,15/Jun/11 11:43;slebresne;0002-Only-compact-what-has-been-succesfully-marked-as-com.patch;https://issues.apache.org/jira/secure/attachment/12482655/0002-Only-compact-what-has-been-succesfully-marked-as-com.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-06-15 05:35:12.699,,,no_permission,,,,,,,,,,,,20819,,,Fri Jun 17 15:21:09 UTC 2011,,,,,,0|i0gdcf:,93592,,,,,,,,,,,"15/Jun/11 05:35;stuhood;Is this in trunk, or in 0.8.0?","15/Jun/11 05:36;bcoverston;Yes, in both.","15/Jun/11 11:43;slebresne;Alright, there is a bunch of problems, one of which affects 0.8 and trunk and could cause this stackTrace. The others are due to CASSANDRA-1610 and thus only affect trunk (but one of those can also result in the attached stackTrace).

The problem affecting 0.8 and trunk is related to a left over line in doCleanup() that is wrongly unmarking a sstable from the compacting set before having removed it from the active set of sstables. Thus another compaction could start compacting this sstable and we'll end up marking the file as compacted twice (and we would have duplicated the sstable, which is a problem for counters).
Patch 0001-0.8.0-Remove-useless-unmarkCompacting-in-doCleanup.patch removes it and is against 0.8.

Trunk has a few problems of its own:
* If disk space is not sufficient to compact all sstables, it computes the smallestSSTables set that fits, but doesn't use it. Attached first patch (0001-Do-compact-only-smallerSSTables.patch) fixes that.
* The CompactionTask logic wrongly decorrelates the set of sstables that are successfully marked from the ones it did compact. That is, it grabs a list of sstables it wants to compact, then call markCompacting on them, but does not check if all of them are successfully marked and compact the original list instead.
  In effect, a task will recompact sstables that are already being compacted by other task and the given file will be compacted twice (or more) and marked compacted multiple times.
  Attached patch (0002-Only-compact-what-has-been-succesfully-marked-as-com.patch) fixes this by changing the sstables set of a given CompactionTask to whatever has been successfully marked only. Since the marking involves updating the task, I've move the logic to AbstractCompactionTask where it seems to make more sense to me.
* For some reason, the markCompacting added for CompactionTasks was refusing to mark (and compact) anything if the set of sstable was bigger that MaxCompactionThreshold. This means that as soon as the number of sstables (of same size) in the column family would exceed the threshold, no compaction would be started. This is not the expected behavior. The second patch also fixes this by reusing the original markCompacting that handles this correctly.
","15/Jun/11 17:49;jbellis;the 0.8 patch looks good.  (i did notice that some of the other post-markCompactiong code checks for null or empty, others just check for null -- one of these is probably wrong.)","15/Jun/11 17:55;slebresne;Alright, I've committed the 0.8 patch. I'll have a look at the checks.","15/Jun/11 22:22;alanliang;Instead of letting DataTracker#markCompacting modify the subset of sstables to be compacted, I think it might be cleaner if it didn't and relied on the CompactionStrategy to select the correct sstables. We can do this by having the CompactionStrategy get the non compacting sstables from the DataTracker and work with those to generate the buckets. The strategy should also be responsible for creating buckets that fit within the min/max thresholds. #markCompacting would then be changed such that it can either accept/reject a bucket to be compacted instead of modifying the subset. #markCompacting will also serve to handle the race condition of the DataTracker being inaccurate, whereby, it will move on to other buckets.

With this, we can avoid generating buckets that are already compacting and it gives full control of what actually is compacted by the CompactionStrategy.

What do you guys think?
","16/Jun/11 17:31;jbellis;For trunk patches, I'm not comfortable w/ 0001 reassigning the sstables field on general principles either.  We could have the compaction proceed using smallerSSTables as a simpler alternative, but in general this organization feels like negative progress from the 0.8 doCompaction/doCompactionWithoutSizeEstimation.

trunk 0002 looks fine.","17/Jun/11 02:20;hudson;Integrated in Cassandra-0.8 #173 (See [https://builds.apache.org/job/Cassandra-0.8/173/])
    ",17/Jun/11 03:59;bcoverston;I think Alan has a good point. I don't think it's an appropriate role of the data tracker to modify the set of sstables to be compacted in a task. It's been a bit of a struggle to hack around the behavior of the DataTracker to ensure my compactions happen in the order I want with the SSTables that I want. That should probably be the exclusive domain compaction strategy.,"17/Jun/11 10:18;slebresne;bq. For trunk patches, I'm not comfortable w/ 0001 reassigning the sstables field on general principles either. We could have the compaction proceed using smallerSSTables as a simpler alternative, but in general this organization feels like negative progress from the 0.8 doCompaction/doCompactionWithoutSizeEstimation.

Attaching v2 that doesn't reassign the sstables field.

bq. I think Alan has a good point. I don't think it's an appropriate role of the data tracker to modify the set of sstables to be compacted in a task.

I do not disagree with that. However I'd like that we fix trunk as a first priority. It's a pain to work on other issues (CASSANDRA-2521 for instance) while it is broken (and the goal must be to do our best to always have a working trunk). The attached patches doesn't really change any behavior, it just fixes the bugs, so let's get that in first before thinking about refactoring.
",17/Jun/11 14:11;bcoverston;I'm sorry I didn't mean to imply it should be fixed _here_. I'll find a more appropriate venue to vent these frustrations :),17/Jun/11 14:54;jbellis;+1 v2,"17/Jun/11 15:01;slebresne;Committed, thanks.","17/Jun/11 15:21;hudson;Integrated in Cassandra #931 (See [https://builds.apache.org/job/Cassandra/931/])
    Fix compaction of the same sstable by multiple thread
patch by slebresne; reviewed by jbellis for CASSANDRA-2769

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1136904
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/DataTracker.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/AbstractCompactionTask.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFMetadata don't set the default for Replicate_on_write correctly,CASSANDRA-2835,12511983,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,28/Jun/11 12:47,12/Mar/19 14:14,13/Mar/19 22:26,28/Jun/11 13:15,0.8.2,,,,,,0,counters,,,,"Replicate_on_write *must* default to true (defaulting to false is very dangerous and imho, the option of setting it to false shouldn't exist in the first place) and CFMetadata.DEFAULT_REPLICATE_ON_WRITE is correctly true. But it doesn't get set correctly. Instead, the code force the value of the cf_def even if it wasn't defined, resulting in setting it to false since that is the default value for a boolean in JAVA.",,,,,,,,,,,,,,,,28/Jun/11 12:48;slebresne;2835.patch;https://issues.apache.org/jira/secure/attachment/12484422/2835.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-28 12:51:17.102,,,no_permission,,,,,,,,,,,,20856,,,Tue Jun 28 21:27:07 UTC 2011,,,,,,0|i0gdqv:,93657,jbellis,jbellis,,,,,,,,,28/Jun/11 12:51;jbellis;+1,28/Jun/11 12:51;jbellis;Add a note to NEWS suggesting checking counter settings on upgrade?,"28/Jun/11 13:15;slebresne;Committed, thanks (I've included a not in the NEWS file as well as updated the wiki).","28/Jun/11 21:27;hudson;Integrated in Cassandra-0.8 #197 (See [https://builds.apache.org/job/Cassandra-0.8/197/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadResponseResolver Race,CASSANDRA-2552,12505091,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,stuhood,stuhood,25/Apr/11 01:21,12/Mar/19 14:14,13/Mar/19 22:26,28/Apr/11 13:26,0.7.6,0.8.0 beta 2,,,,,0,,,,,"When receiving a response, ReadResponseResolver uses a 3 step process to decide whether to trigger the condition that enough responses have arrived:
# Add new response
# Check response set size
# Check that data is present

I think that these steps must have been reordered by the compiler in some cases, because I was able to reproduce a case for a QUORUM read where the condition is not properly triggered:
{noformat}
INFO [RequestResponseStage:15] 2011-04-25 00:26:53,514 ReadResponseResolver.java (line 87) post append for 1087367065: hasData=false in 2 messages
INFO [RequestResponseStage:8] 2011-04-25 00:26:53,514 ReadResponseResolver.java (line 87) post append for 1087367065: hasData=true in 1 messages
INFO [pool-1-thread-54] 2011-04-25 00:27:03,516 StorageProxy.java (line 623) Read timeout: java.util.concurrent.TimeoutException: ReadResponseResolver@1087367065(/10.34.131.109=false,/10.34.132.122=true,)
{noformat}
The last line shows that both results were present, and that one of them was holding data.",,,,,,,,,,,,,,,,26/Apr/11 07:36;stuhood;0001-Move-Resolvers-to-atomic-append-count.txt;https://issues.apache.org/jira/secure/attachment/12477369/0001-Move-Resolvers-to-atomic-append-count.txt,27/Apr/11 20:24;jbellis;2552-v2-07.txt;https://issues.apache.org/jira/secure/attachment/12477577/2552-v2-07.txt,27/Apr/11 20:18;jbellis;2552-v2.txt;https://issues.apache.org/jira/secure/attachment/12477576/2552-v2.txt,26/Apr/11 06:08;stuhood;ResolveRaceTest.java;https://issues.apache.org/jira/secure/attachment/12477366/ResolveRaceTest.java,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-04-25 13:25:24.613,,,no_permission,,,,,,,,,,,,20693,,,Thu Apr 28 13:44:17 UTC 2011,,,,,,0|i0gc1j:,93381,stuhood,stuhood,,,,,,,,,25/Apr/11 02:06;stuhood;I have a patch ready that I believe fixes this: testing it out before posting.,"25/Apr/11 13:25;jbellis;Hard to tell exactly what's going on here w/o knowing where your logging was added.

In particular it's important to note that we don't prevent responses from being processed after we've already given up and decided to call a timeout (but before we've torn down the request callback).","26/Apr/11 06:08;stuhood;Here is a cut down testcase that reproduces the race: it looks like two threads can race on step 2 such that neither accounts for the item added by the other, and both think the set of responses is too small.

I have a patch that makes append + size an atomic operation: I'll post it as soon as I clean it up a bit.","26/Apr/11 07:10;stuhood;Chris pointed out that the example passes if you replace NBHM with CHM, but I don't think NBHM is necessarily to blame here: each thread views a locally consistent copy, likely due to Cliff's use of sun.misc.Unsafe references.

It's possible that a similar race applies to RangeSliceResponseResolver, but I think changes to LBQ (like CHM) will be broadcast to all threads.","26/Apr/11 07:36;stuhood;Attaching a patch that replaces NBHM with an AtomicReferenceArray that is appended to and counted atomically. This patch eliminated the timeouts we were seeing.

CHM may also be a legitimate solution, but it feels a bit like an abuse of a map.","26/Apr/11 17:21;jbellis;That sure sounds like a NBHM bug to me. The javadoc says,

bq. Retrievals reflect the results of the most recently completed update operations holding upon their onset... Similarly, Iterators and Enumerations return elements reflecting the state of the hash table at some point at or since the creation of the iterator/enumeration.

I.e., for at least one thread, *both* update operations will have completed when the iterator is created, so it should see all the entries.

(Will review the actual patch shortly, I'm just saying I think we should report a bug too.)","26/Apr/11 22:39;stuhood;> I.e., for at least one thread, both update operations will have completed when the iterator is created
Note that the race I observed via the debug output for that test was actually on the size() operation, which doesn't put any such guarantees in its javadocs.","26/Apr/11 23:46;jbellis;You're right: size() is implemented as a org.cliffc.high_scale_lib.Counter object, which says

{code}
  // Add the given value to current counter value.  Concurrent updates will
  // not be lost, but addAndGet or getAndAdd are not implemented because but
  // the total counter value is not atomically updated.
  //public void add( long x );

...

  // Current value of the counter.  Since other threads are updating furiously
  // the value is only approximate, but it includes all counts made by the
  // current thread.  Requires a pass over all the striped counters.
  //public long get();
{code}","27/Apr/11 09:38;slebresne;I am no expert of the Java Memory Model, but I can't find anything that preclude this behavior in the CHM docs either (there really is not much on the size function). So I would have liked the CHM solution if we could be sure it always fix that problem (I would have liked it because it was a one line change and I think maps are here to be ""abused""), but as far as I can tell, it may well only make the bug much less frequent or fix it only on some architecture (the code of CHM seems to indicate it is safe but it's complicated enough that I wouldn't bet my life on it).

Note that if that's true, LBQ too could well allow for a race here without breaking it's specification (it seems to use a AtomicInteger for the size internally so it is trivially ok, but if the spec doesn't force anything, I suppose that could change).

So I suppose if we want to do right by the spec, we should probably update both AbstractRowResolver and RangeSliceResponseResolver (note that using an AtomicInteger to count the number of responses could be slightly simpler, but I'm fine with an AtomicReferenceArray). ",27/Apr/11 15:38;jbellis;is there a reason RSRR can't inherit ARR or does it just predate that refactoring?,"27/Apr/11 16:07;jbellis;bq. is there a reason RSRR can't inherit ARR or does it just predate that refactoring?

To answer my own ARR assumes we're returning Rows, which would be easy to fix, and that Messages turn into ReadResponse objects, which would be harder since we'd need to have a <T extends ISerializable> interface where ISerializeable gave us a Serializer class declaring ""void serialize(T, outputstream) and T deserialize(inputstream)"", i.e., we start to get into fixing ICompactSerializer and all the mess that would be.","27/Apr/11 16:29;jbellis;I'm not sure I'm a fan of the ARR solution.  Wouldn't it be similar complexity (one O(N) operation per message received) to keep NBHM and implement getMessageCount as an iterate-entries operation?  (the O(N) op in ARR is of course the search-for-free-slot in append.)

I'm -0 on changing RSRR away from LBQ when LBQ is known to work fine in practice.","27/Apr/11 16:37;jbellis;bq. Wouldn't it be similar complexity (one O(N) operation per message received) to keep NBHM and implement getMessageCount as an iterate-entries operation?

We can actually do size-by-iteration for basically free (with a little refactoring), since we're already iterating for isDataPresent. We can just push the iteration into the callers who care about size-and-data-present and do it with one loop.

But if I am honest that is premature optimization.  We are already using the AtomicInteger approach in DatacenterReadCallback.  I'll submit a patch to standardize on that.
",27/Apr/11 17:23;jbellis;v2 w/ AtomicInteger approach,27/Apr/11 20:18;jbellis;updated v2 fixes AsyncRepairCallback and RepairCallback as well,27/Apr/11 20:24;jbellis;and a v2 for 0.7,"27/Apr/11 20:45;stuhood;Although I didn't reproduce a race between size() and isDataPresent(), isn't that one still possible? IMO, two operations that are atomic independently shouldn't be trusted to compose. The point of the ARR wasn't to improve runtime, it was simply to make all three steps atomic.","27/Apr/11 20:53;jbellis;the correctness criterion is that once the messages are received, at least one thread running response will see that both blockfor and data are satisfied; this meets that need.

note that received(-messages-that-count-towards-blockfor) is NOT the same as size (see: DRC) so you need a separate variable anyway even with ARR.","28/Apr/11 08:45;stuhood;+1

I still think a Map is overkill here, but I can't reproduce a race with the v2 algorithm.",28/Apr/11 13:26;jbellis;committed,"28/Apr/11 13:44;hudson;Integrated in Cassandra-0.7 #459 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/459/])
    fix incorrect use ofNBHM.size in ReadCallback
patch by jbellis; reviewed by stuhood for CASSANDRA-2552
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair streaming forwarding loop,CASSANDRA-3194,12522858,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,awinter,awinter,13/Sep/11 08:02,12/Mar/19 14:14,13/Mar/19 22:26,13/Sep/11 17:37,1.0.0,,,,,,0,,,,,"I am able to reproduce what appears to be a streaming forwarding loop when running repairs.  This affect only nodes using broadcast_address (ec2 external ip) & listen_address of 0.0.0.0. (Configuration is using property file snitch in a multi DC NTS where some DC's are EC2 and others are not).  The hosts in the other dc's not using broadcast_address do not experience this symptom.

on ec2 host dc1host1:
INFO [AntiEntropyStage:1] 2011-09-13 06:34:01,673 StreamingRepairTask.java (line 211) [streaming task #ce793c30-ddd1-11e0-0000-071a4b76fefb] Received task from /0.0.0.0 to stream 12259 ranges to /external.ec2.ip.dc1host3
 INFO [AntiEntropyStage:1] 2011-09-13 06:34:01,673 StreamingRepairTask.java (line 136) [streaming task #ce793c30-ddd1-11e0-0000-071a4b76fefb] Forwarding streaming repair of 12259 ranges to /external.ec2.ip.of.dc1host1 (to be streamed with /external.ip.of.host3)

The above appears to trigger another streaming task and results in saturating the network interfaces dc1host1.  The above log entries are repeated until cassandra is killed.",,,,,,,,,,,,,,,,13/Sep/11 12:53;slebresne;3194.patch;https://issues.apache.org/jira/secure/attachment/12494224/3194.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-13 12:53:51.324,,,no_permission,,,,,,,,,,,,4028,,,Tue Sep 13 17:37:28 UTC 2011,,,,,,0|i0ggy7:,94176,brandon.williams,brandon.williams,,,,,,,,,"13/Sep/11 12:53;slebresne;Oups, forgot to switch to getBroadcastAddress  in StreamingRepairTask. Patch attached.",13/Sep/11 17:17;brandon.williams;+1,"13/Sep/11 17:37;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: cqlsh error running batch update commands,CASSANDRA-2545,12505019,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,cdaw,cdaw,23/Apr/11 00:55,12/Mar/19 14:14,13/Mar/19 22:26,27/Apr/11 02:18,,,,,,,0,,,,,"*CQL Test Case*
{code}
//TEST CASE #1
BEGIN BATCH
UPDATE users SET gender = 'm', birth_year = '1981' WHERE KEY = 'user1';
UPDATE users SET gender = 'm', birth_year = '1982' WHERE KEY = 'user2';
UPDATE users SET gender = 'm', birth_year = '1983' WHERE KEY = 'user3';
APPLY BATCH	

//TEST CASE #2
BEGIN BATCH USING CONSISTENCY ZERO
UPDATE users SET state = 'TX' WHERE KEY = 'user1';
UPDATE users SET state = 'TX' WHERE KEY = 'user2';
UPDATE users SET state = 'TX' WHERE KEY = 'user3';
APPLY BATCH	


//ERROR
Bad Request: line 0:-1 mismatched input '<EOF>' expecting K_APPLY
{code}

*Test Setup*
{code}
CREATE COLUMNFAMILY users (
  KEY varchar PRIMARY KEY,
  password varchar,
  gender varchar,
  session_token varchar,
  state varchar,
  birth_year bigint);

INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');
{code}

*Documented Syntax*
{panel}
BEGIN BATCH [USING <CONSISTENCY>]
UPDATE CF1 SET name1 = value1, name2 = value2 WHERE KEY = keyname1;
UPDATE CF1 SET name3 = value3 WHERE KEY = keyname2;
UPDATE CF2 SET name4 = value4, name5 = value5 WHERE KEY = keyname3;
APPLY BATCH
{panel}",,,,,,,,,,,,,,,,23/Apr/11 02:47;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2545-also-consider-APPLY-BATCH-for-terminati.txt;https://issues.apache.org/jira/secure/attachment/12477184/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2545-also-consider-APPLY-BATCH-for-terminati.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-23 02:32:27.568,,,no_permission,,,,,,,,,,,,20688,,,Wed Apr 27 03:41:23 UTC 2011,,,,,,0|i0gbzz:,93374,,,,,,,,,,,"23/Apr/11 02:32;urandom;This is a cqlsh bug.  The attached patch is an improvement, but could probably be improved upon.","23/Apr/11 02:39;jbellis;i don't think ""cqlsh requires commands to end with semicolon"" is a bug","23/Apr/11 02:52;urandom;bq. i don't think ""cqlsh requires commands to end with semicolon"" is a bug

It's not; That's not the bug.

What's buggy is the way that a multi-line statement is parsed from the input.  The attached patch should take care of it.  It does not require the semi-colon for the individual UPDATE statements (APPLY BATCH is the terminator here), but using them won't hurt anything.",26/Apr/11 19:05;cdaw;The patch works fine.,27/Apr/11 02:18;urandom;committed,"27/Apr/11 03:41;hudson;Integrated in Cassandra-0.8 #44 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/44/])
    CASSANDRA-2545 also consider APPLY BATCH for terminating statements

Patch by eevans for CASSANDRA-2545
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supercolumn serialization assertion failure,CASSANDRA-3957,12544061,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,24/Feb/12 21:51,12/Mar/19 14:14,13/Mar/19 22:26,07/Mar/12 17:39,1.0.9,1.1.0,,,,,1,datastax_qa,,,,"As reported at http://mail-archives.apache.org/mod_mbox/cassandra-user/201202.mbox/%3CCADJL=w5kH5TEQXOwhTn5Jm3cmR4Rj=NfjcqLryXV7pLyASi95A@mail.gmail.com%3E,

{noformat}
ERROR 10:51:44,282 Fatal exception in thread
Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError: Final buffer length 4690 to accomodate data size
of 2347 (predicted 2344) for RowMutation(keyspace='Player',
key='36336138643338652d366162302d343334392d383466302d356166643863353133356465',
modifications=[ColumnFamily(PlayerCity [SuperColumn(owneditem_1019
[]),SuperColumn(owneditem_1024 []),SuperColumn(owneditem_1026
[]),SuperColumn(owneditem_1074 []),SuperColumn(owneditem_1077
[]),SuperColumn(owneditem_1084 []),SuperColumn(owneditem_1094
[]),SuperColumn(owneditem_1130 []),SuperColumn(owneditem_1136
[]),SuperColumn(owneditem_1141 []),SuperColumn(owneditem_1142
[]),SuperColumn(owneditem_1145 []),SuperColumn(owneditem_1218
[636f6e6e6563746564:false:5@1329648704269002
,63757272656e744865616c7468:false:3@1329648704269006
,656e64436f6e737472756374696f6e54696d65:false:13@1329648704269007
,6964:false:4@1329648704269000,6974656d4964:false:15@1329648704269001
,6c61737444657374726f79656454696d65:false:1@1329648704269008
,6c61737454696d65436f6c6c6563746564:false:13@1329648704269005
,736b696e4964:false:7@1329648704269009,78:false:4@1329648704269003
,79:false:3@1329648704269004,]),SuperColumn(owneditem_133
[]),SuperColumn(owneditem_134 []),SuperColumn(owneditem_135
[]),SuperColumn(owneditem_141 []),SuperColumn(owneditem_147
[]),SuperColumn(owneditem_154 []),SuperColumn(owneditem_159
[]),SuperColumn(owneditem_171 []),SuperColumn(owneditem_253
[]),SuperColumn(owneditem_422 []),SuperColumn(owneditem_438
[]),SuperColumn(owneditem_515 []),SuperColumn(owneditem_521
[]),SuperColumn(owneditem_523 []),SuperColumn(owneditem_525
[]),SuperColumn(owneditem_562 []),SuperColumn(owneditem_61
[]),SuperColumn(owneditem_634 []),SuperColumn(owneditem_636
[]),SuperColumn(owneditem_71 []),SuperColumn(owneditem_712
[]),SuperColumn(owneditem_720 []),SuperColumn(owneditem_728
[]),SuperColumn(owneditem_787 []),SuperColumn(owneditem_797
[]),SuperColumn(owneditem_798 []),SuperColumn(owneditem_838
[]),SuperColumn(owneditem_842 []),SuperColumn(owneditem_847
[]),SuperColumn(owneditem_849 []),SuperColumn(owneditem_851
[]),SuperColumn(owneditem_852 []),SuperColumn(owneditem_853
[]),SuperColumn(owneditem_854 []),SuperColumn(owneditem_857
[]),SuperColumn(owneditem_858 []),SuperColumn(owneditem_874
[]),SuperColumn(owneditem_884 []),SuperColumn(owneditem_886
[]),SuperColumn(owneditem_908 []),SuperColumn(owneditem_91
[]),SuperColumn(owneditem_911 []),SuperColumn(owneditem_930
[]),SuperColumn(owneditem_934 []),SuperColumn(owneditem_937
[]),SuperColumn(owneditem_944 []),SuperColumn(owneditem_945
[]),SuperColumn(owneditem_962 []),SuperColumn(owneditem_963
[]),SuperColumn(owneditem_964 []),])])
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:279)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:122)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:599)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:662)
{noformat}
",,,,,,,,,,,,,,,,07/Mar/12 16:19;slebresne;3957.txt;https://issues.apache.org/jira/secure/attachment/12517414/3957.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-02 20:42:59.887,,,no_permission,,,,,,,,,,,,229299,,,Wed Mar 07 18:22:52 UTC 2012,,,,,,0|i0gq9z:,95687,jbellis,jbellis,,,,,,,,,"24/Feb/12 22:04;jbellis;I note that this is the Periodic commitlog executor, which does NOT block for CommitLog.add.  So, if the CF is serialized (later) while another thread modifies it, we could hit this error.

We don't modify the mutation CF directly in CFS.apply, since we clone into the arena allocator.

The only place I see where we modify the mutation CF is in ignoreObsoleteMutations...  but that only happens for indexed columns, so that can't be the cause here since it involves SuperColumns.

(Note that the mutation CF won't be changed by updateRowCache, since the mutation CF is never inserted into the cache; it's only used to add to an existing cache row, if one exists.)

Am I missing something?

Perhaps it would be best to start by creating a test to serialize the above RowMutation and see if it reproduces in the absence of concurrent activity, to rule out the possibility of serializedSize simply having a bug w/ SuperColumns.","02/Mar/12 20:42;cywjackson;below is another similar stacktrace where only standard column (no super, no composite) was used.

{panel}
ERROR 08:50:12,479 Fatal exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError: Final buffer length 2090383 to accomodate data size of 736236 (predicted 2090383) for RowMutation(keyspace='cfs', key='6337343566396330363438373131653130303030653562646562313236356262', modifications=[ColumnFamily(sblocks [6337343839316430363438373131653130303030653562646562313236356262:false:736121@1330707012466,])])
at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:279)
at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:122)
at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:599)
at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
at java.lang.Thread.run(Thread.java:680)
{panel}","06/Mar/12 18:57;jbellis;Jackson's stacktrace involving sblocks was caused by a bug in DSE's CFS code re-using a ByteBuffer it had handed off to Thrift.

Could be that the original report had a similar problem.","06/Mar/12 22:39;cywjackson;this comes from a difference use case. the client code is in hector, maybe we need to look into how hector is handling the bytebuffer too?

ERROR [COMMIT-LOG-WRITER] 2012-02-20 06:16:56,621 org.apache.cassandra.service.AbstractCassandraDaemon Fatal exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError: Final buffer length 275 to accomodate data size of 271 (predicted 275) for RowMutation(keyspace='foo', key='984fb8106d3ff5043365736457c45085', modifications=[ColumnFa
mily(Dora_la [SuperColumn(1329718356206 [5f656e64:false:1@1329718616576004,5f6c6173744576656e7454696d65:false:8@1329718616576003,5f6c61737450617468:false:8@1329718616576005,5f76697369744576
656e74496e646578:false:4@1329718616576002,5f76697369744964:false:16@1329718616576000,5f7669736974496e646578:false:4@1329718616576001,]),])])
at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:279)
at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:122)
at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:599)
at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
at java.lang.Thread.run(Thread.java:662)

ERROR [COMMIT-LOG-WRITER] 2012-02-07 22:33:09,691 org.apache.cassandra.service.AbstractCassandraDaemon Fatal exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError: Final buffer length 550 to accomodate data size of 275 (predicted 271) for RowMutation(keyspace='dyn', key='7836ab62d10ddb48d00668639ebdf6ce', modifications=[ColumnFa
mily(Dora_la [SuperColumn(1328653749409 [5f656e64:false:1@1328653989655004,5f6c6173744576656e7454696d65:false:8@1328653989655003,5f6c61737450617468:false:12@1328653989655005,5f7669736974457
6656e74496e646578:false:4@1328653989655002,5f76697369744964:false:16@1328653989655000,5f7669736974496e646578:false:4@1328653989655001,]),])])
at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:279)
at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:122)
at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:599)
at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
at java.lang.Thread.run(Thread.java:662)","07/Mar/12 16:19;slebresne;Here's my theory.

The stacktrace concerning the sblocks is due to a bug in DSE's CFS. This happened because CFS sometimes communicate with C* in the same JVM without going trough the wire, so the re-use of ByteBuffer was problematic.

The other stacktrace however can't be the same problem. Whatever Hector does, it goes over the wire and thus can't change the size of a ByteBuffer in C*.

However, if we eliminate the sblocks trace, all the other trace uses SuperColumn. And as it turns out, I think we have a race with SC. As Jonathan pointed out, with the period commit log, if we happen to modify a mutation that was sent to the commit log at any time, that would be a bug. This means not modifying the CF object in the RowMutation, but for SuperColumn, this also mean we shouldn't modify those super columns. However, when we update the row cache (and if it's not a SerializingCache), we do potentially store references to the original SCs in the cached row, which could then lead to the stack trace on this issue. I'm attaching a patch that fixes this.

To make sure this could be the issue here, it would help to know if the stacktrace comes from column families where row cache was used and was not set to the serializing cache.

I'll note however that the very small difference in the stacktraces between the predicted size and the actual serialized side support the theory of having say just one column of a super column updated, while in the sblocks case, the difference was much bigger, supporting the theory of the ByteBuffer content being changed to something completely different.
",07/Mar/12 17:26;jbellis;+1,"07/Mar/12 17:39;slebresne;Committed, thanks",07/Mar/12 18:22;jbellis;Thomas (the reporter of the original stack trace in the description) just confirmed on the mailing list that he was using row cache / CLHC.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Composite Column Support for PIG,CASSANDRA-3684,12536603,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jalkanen,bcoverston,bcoverston,29/Dec/11 18:26,12/Mar/19 14:14,13/Mar/19 22:26,29/Feb/12 20:36,1.0.9,1.1.0,,,,,1,,,,,"It appears that some changes need to be made to support CompositeColumns. Right now if you try to load and use a column family that utilizes composite columns you get the following exception[1].

It appears to me that we need to modify the storage handler for Pig to support this scenario.


[1]

================================================================================
Backend error message
---------------------
java.lang.RuntimeException: Unexpected data type -1 found in stream.
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:478)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeBag(BinInterSedes.java:522)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:361)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
	at org.apache.pig.data.BinSedesTuple.write(BinSedesTuple.java:57)
	at org.apache.pig.impl.io.PigNullableWritable.write(PigNullableWritable.java:123)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1061)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:691)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:116)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:239)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:232)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:272)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.Child.main(Child.java:266)

Backend error message
---------------------
java.lang.Throwable: Child Error
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)
Caused by: java.io.IOException: Task process exit with nonzero status of 65.
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:258)",,,,,,,,,,,,,,,,29/Feb/12 20:21;jalkanen;3684-jalkanen-test-v2.txt;https://issues.apache.org/jira/secure/attachment/12516602/3684-jalkanen-test-v2.txt,29/Feb/12 19:37;jalkanen;3684-jalkanen-test.txt;https://issues.apache.org/jira/secure/attachment/12516595/3684-jalkanen-test.txt,28/Feb/12 20:16;jalkanen;3684-jalkanen.txt;https://issues.apache.org/jira/secure/attachment/12516385/3684-jalkanen.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-02-28 20:16:35.353,,,no_permission,,,,,,,,,,,,222286,,,Wed Feb 29 20:36:36 UTC 2012,,,,,,0|i0gmz3:,95152,brandon.williams,brandon.williams,,,,,,,,,"28/Feb/12 20:16;jalkanen;This patch (against cassandra-1.0) brings in basic support for Composite Columns.

I needed a simple way to deconstruct an AbstractCompositeType, so I had to enhance that. Dunno if that's desireable, but it's certainly easy :-P

Also the column name is an untyped tuple; not sure what would be the best way to extract the schema for it.","28/Feb/12 20:18;jalkanen;Also, apologies for extra crap; my OCD demands that my git is configured to remove extra space at the end of the lines :). If this approach looks feasible, I'll make a cleaner patch.",29/Feb/12 19:37;jalkanen;Patch to provide tests against the Composite Columns.,29/Feb/12 20:21;jalkanen;Improved test patch against the tests; this time with also Long:Long composite type.,"29/Feb/12 20:36;brandon.williams;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incompatibility w/ 0.7 schemas,CASSANDRA-2450,12503968,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,11/Apr/11 19:32,12/Mar/19 14:14,13/Mar/19 22:26,15/Apr/11 14:38,0.8 beta 1,,,,,,0,,,,,"If you create a SimpleStrategy keyspace under 0.7, then switch to 0.8, you will get this error on startup:

{noformat}
ERROR 14:31:41,725 Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:277)
	at org.apache.cassandra.db.Table.open(Table.java:109)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:75)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:327)
	at org.apache.cassandra.db.Table.<init>(Table.java:273)
	... 4 more
{noformat}",,,,,,,,,,,,,,,,15/Apr/11 13:58;jbellis;2450.txt;https://issues.apache.org/jira/secure/attachment/12476455/2450.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-15 14:24:23.131,,,no_permission,,,,,,,,,,,,20631,,,Fri Apr 15 14:38:15 UTC 2011,,,,,,0|i0gbfz:,93284,gdusbabek,gdusbabek,,,,,,,,,"15/Apr/11 13:58;jbellis;patch add back replication_factor to the avro class as union { int, null } so we can upgrade old-style schema information seamlessly in KSMetaData.",15/Apr/11 14:24;gdusbabek;+1,15/Apr/11 14:38;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bizarre Compaction Manager Behaviour,CASSANDRA-3484,12531167,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,dhendry,dhendry,11/Nov/11 19:04,12/Mar/19 14:14,13/Mar/19 22:26,11/Nov/11 20:01,1.0.3,,,,,,0,,,,,"It seems the CompactionManager has gotten itself into a bad state. My 1.0.2 node has been up for 20 hours now - checking via JMX, the compaction manager is reporting that it has completed 14,797,412,000 tasks. Yep, thats right 14 billion tasks and increasing at a rate of roughly 208,400/second. 

I should point out that I am currently running a major compaction on the node. My theory is that this problem was introduced by CASSANDRA-3363. It looks like SizeTieredCompactionStrategy.getBackgroundTasks() returns a set of task without consideration for any in-progress compactions. Compactions are only kicked off if task.markSSTablesForCompaction() returns true (CompactionManager line 127) but the task resubmission is based only on the task list not being empty (CompactionManager line 141). Should the logic not be to only reschedule if a task has actually been executed?

I am just waiting now for the major compaction to finish to see if the problem goes away as would be suggested by my theory.","RHEL 6
java version ""1.6.0_26""
6 node cluster (5 nodes 0.8.6, 1 node 1.0.2 minus CASSANDRA-2503)",,,,,,,,,,,,,,,11/Nov/11 19:19;dhendry;3484.txt;https://issues.apache.org/jira/secure/attachment/12503407/3484.txt,11/Nov/11 19:07;dhendry;compaction.png;https://issues.apache.org/jira/secure/attachment/12503403/compaction.png,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-11 19:24:20.337,,,no_permission,,,,,,,,,,,,216905,,,Fri Nov 11 20:01:39 UTC 2011,,,,,,0|i0gkh3:,94747,slebresne,slebresne,,,,,,,,,11/Nov/11 19:07;dhendry;JMX evidence,11/Nov/11 19:19;dhendry;Patch to only reschedule another compaction check when the active check resulted in a task being executed,11/Nov/11 19:24;jbellis;I think the patch on #2407 would also fix this.,11/Nov/11 19:33;dhendry;Yes it would. The issue seemed to result in some pretty significant temporary performance degradation. Any chance of getting 2407 into 1.0.3 instead of 1.1?,11/Nov/11 19:47;slebresne;We can commit either this patch or the one on CASSANDRA-2407 for this issue (since they both fix the issue here). But the goal of #2407 is a bit different so we just should leave that issue solve the problem it want to solve.,"11/Nov/11 20:01;slebresne;Alright, +1 on the patch here, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeveledCompaction has several performance problems,CASSANDRA-3234,12523796,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,20/Sep/11 23:47,12/Mar/19 14:14,13/Mar/19 22:26,23/Sep/11 13:56,1.0.0,,,,,,0,lcs,,,,"Two main problems:

- BF size calculation doesn't take into account LCS breaking the output apart into ""bite sized"" sstables, so memory use is much higher than predicted
- ManyToMany merging is slow.  At least part of this is from running the full reducer machinery against single input sources, which can be optimized away.",,,,,,,,,,,,,,,,21/Sep/11 17:45;slebresne;0005-use-Array-and-Tree-backed-columns-in-compaction-v2.patch;https://issues.apache.org/jira/secure/attachment/12495411/0005-use-Array-and-Tree-backed-columns-in-compaction-v2.patch,21/Sep/11 22:43;jbellis;0005-use-Array-and-Tree-backed-columns-in-compaction-v3.txt;https://issues.apache.org/jira/secure/attachment/12496044/0005-use-Array-and-Tree-backed-columns-in-compaction-v3.txt,22/Sep/11 07:41;slebresne;0005-use-Array-and-Tree-backed-columns-in-compaction-v4.patch;https://issues.apache.org/jira/secure/attachment/12496077/0005-use-Array-and-Tree-backed-columns-in-compaction-v4.patch,23/Sep/11 08:37;jbellis;0006-avoid-echoedRow-when-checking-shouldPurge-is-more-ex.patch;https://issues.apache.org/jira/secure/attachment/12496239/0006-avoid-echoedRow-when-checking-shouldPurge-is-more-ex.patch,23/Sep/11 08:06;jbellis;0006-avoid-echoedRow-when-checking-shouldPurge-is-more-ex.patch;https://issues.apache.org/jira/secure/attachment/12496233/0006-avoid-echoedRow-when-checking-shouldPurge-is-more-ex.patch,22/Sep/11 22:59;jbellis;0006-avoid-echoedRow-when-checking-shouldPurge-is-more-ex.patch;https://issues.apache.org/jira/secure/attachment/12496194/0006-avoid-echoedRow-when-checking-shouldPurge-is-more-ex.patch,21/Sep/11 15:49;jbellis;ASF.LICENSE.NOT.GRANTED--0001-optimize-single-source-case-for-MergeIterator.txt;https://issues.apache.org/jira/secure/attachment/12495388/ASF.LICENSE.NOT.GRANTED--0001-optimize-single-source-case-for-MergeIterator.txt,21/Sep/11 15:49;jbellis;ASF.LICENSE.NOT.GRANTED--0002-add-TrivialOneToOne-optimization.txt;https://issues.apache.org/jira/secure/attachment/12495389/ASF.LICENSE.NOT.GRANTED--0002-add-TrivialOneToOne-optimization.txt,21/Sep/11 15:49;jbellis;ASF.LICENSE.NOT.GRANTED--0003-fix-leveled-BF-size-calculation.txt;https://issues.apache.org/jira/secure/attachment/12495390/ASF.LICENSE.NOT.GRANTED--0003-fix-leveled-BF-size-calculation.txt,21/Sep/11 15:49;jbellis;ASF.LICENSE.NOT.GRANTED--0004-avoid-calling-shouldPurge-unless-necessary.txt;https://issues.apache.org/jira/secure/attachment/12495391/ASF.LICENSE.NOT.GRANTED--0004-avoid-calling-shouldPurge-unless-necessary.txt,21/Sep/11 15:49;jbellis;ASF.LICENSE.NOT.GRANTED--0005-use-Array-and-Tree-backed-columns-in-compaction.txt;https://issues.apache.org/jira/secure/attachment/12495392/ASF.LICENSE.NOT.GRANTED--0005-use-Array-and-Tree-backed-columns-in-compaction.txt,11.0,,,,,,,,,,,,,,,,,,,2011-09-21 17:45:29.112,,,no_permission,,,,,,,,,,,,3531,,,Fri Sep 23 13:56:48 UTC 2011,,,,,,0|i0ghfr:,94255,brandon.williams,brandon.williams,,,,,,,,,21/Sep/11 03:54;jbellis;split OneToOne classes into separate patches and fixed bug in estimated tables calculation,"21/Sep/11 17:45;slebresne;I haven't looked at the 3 first patches, but on patch 4 and 5.

+1 on patch 4 (though I agree with the comment in there that it's not the more beautiful refactor ever :))

On patch 5, it cloneMeShallow the first read column family and basically skip all the columns, so that's wrong. Attaching a v2 that makes SSTII directly use the right ISortedColumn factory (to avoid full cloning). Problem is this doesn't translate to ParallelCompactionIterable too well since the actual read is deep into the code. For it, I think we have 2 easy solutions:
  * Just use ArraySortedColumns all the way. This is actually ok because addAll works whatever the input is, it does a merge.
  * Do a full clone to a TreeMapBacked CF on the first cf read
  * Use TreeMapBack CFs all the way.

I went with the first solution in the patch attached (more because it requires the less changes than anything else), though that's probably not optimal for LeveledCompaction (but I'm not sure ParallelCompaction is useful for LeveledCompaction). ","21/Sep/11 17:48;jbellis;bq. that's probably not optimal for LeveledCompaction

Can you elaborate?","21/Sep/11 17:54;jbellis;{code}
+                thisCF = row.getColumnFamilyWithColumns(cf == null ? TreeMapBackedSortedColumns.factory() : ArrayBackedSortedColumns.factory());
{code}

I don't understand why not just ABSC here.
","21/Sep/11 18:09;slebresne;ABSC.addAll does a merge. So if you do cf1.addAll(cf2) and cf1 and cf2 are comparable in size, that's likely as good as it gets. However, if cf2 has much less columns than cf1, then it's likely that TMBSC.addAll (that adds the columns of cf2 one by one) will be faster.

So I guess it all depends on how many sstables we are merging. If we have a lot of them, then adding to TMBSC will be a win (my claim on LeveledCompaction was misdirected, I was thinking of it as lots of sstables, but that doesn't mean we'll merge lots of sstables (unless L0 is behind)). Anyway, it can very well be that ABSC is better in the vast majority of cases so I'm fine with using just that. ",21/Sep/11 22:43;jbellis;simple 05 v3 attached using just ABSC,"22/Sep/11 07:41;slebresne;The problem is that now CFS.removeDeleted throws a ConcurrentModificationException. Attaching v4 that replaces the
{noformat}
for (IColumn c : cf)
    cf.remove(c.name())
{noformat}
pattern by
{noformat}
Iterator<IColumn> iter = cf.iterator()
while (iter.hasNext())
{
    IColumn c = iter.next();
    iter.remove();
}
{noformat}
",22/Sep/11 18:03;brandon.williams;+1,22/Sep/11 20:47;jbellis;06 attached as well.,22/Sep/11 22:59;jbellis;updated 06,"23/Sep/11 08:06;jbellis;v2 improves leveled expensiveness equation to

L0.size() + count(levels) - ignore.size()","23/Sep/11 08:37;jbellis;updated to ""Sets.difference(L0, sstablesToIgnore).size() + manifest.getLevelCount()""",23/Sep/11 08:52;slebresne;The patch as is doesn't compile because it removes the import of StringUtils in LeveledManifest.java (it also add the import of DataTracker in AbstractCompactionStrategy which I think is not useful). But other than +1 on 06 v3.,23/Sep/11 13:56;jbellis;fixed + committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
After Choosing EC2Snitch you can't migrate off w/o a full cluster restart,CASSANDRA-3114,12520973,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,bcoverston,bcoverston,31/Aug/11 20:40,12/Mar/19 14:14,13/Mar/19 22:26,16/Nov/11 21:47,,,,,,,1,,,,,"Once you choose the Ec2Snitch the gossip messages will trigger this exception if you try to move (for example) to the property file snitch:

ERROR [pool-2-thread-11] 2011-08-30 16:38:06,935 Cassandra.java (line 3041) Internal error processing get_slice 
java.lang.NullPointerException 
at org.apache.cassandra.locator.Ec2Snitch.getDatacenter(Ec2Snitch.java:84) 
at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:122) 
at org.apache.cassandra.service.DatacenterReadCallback.assureSufficientLiveNodes(DatacenterReadCallback.java:77) 
at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:516) 
at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:480) 
at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:109) 
at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:263) 
at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:345) 
at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:306) 
at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:3033) 
at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889) 
at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:662)",,,,,,,,,,,,CASSANDRA-3186,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-08-31 21:16:06.848,,,no_permission,,,,,,,,,,,,1873,,,Wed Nov 16 21:47:54 UTC 2011,,,,,,0|i0gfyf:,94015,,,,,,,,,,,"31/Aug/11 21:16;brandon.williams;I'm not sure there's a good solution here.  We could make PFEPS inject the local nodes dc/rack info into gossip similar to what I suggested in CASSANDRA-1974, but you'd still have to name things with the ec2snitch conventions for things to not break, and it would be very PFEPS-specific; other snitches are out of the question.

Ultimately I'm inclined to say you need to choose your snitch like you choose your partitioner: very carefully.","07/Sep/11 19:01;cywjackson;What if do this in the Abstract?

{code:title=AbstractEndpointSnitch.java}
    public void gossiperStarting()
    {
    	String dc = getDatacenter(FBUtilities.getBroadcastAddress());
    	String rack = getRack(FBUtilities.getBroadcastAddress());
    	logger.info(this.getClass().getSimpleName() +"" adding ApplicationState DC="" + dc + "" Rack="" + rack);
        Gossiper.instance.addLocalApplicationState(ApplicationState.DC, StorageService.instance.valueFactory.datacenter(dc));
        Gossiper.instance.addLocalApplicationState(ApplicationState.RACK, StorageService.instance.valueFactory.rack(rack));
    }
{code}",07/Sep/11 19:04;brandon.williams;I don't see how making your dc/rack names your external IP address is going to solve anything.,"07/Sep/11 19:04;cywjackson;""but you'd still have to name things with the ec2snitch conventions for things to not break"" still hold true with the above.","15/Sep/11 23:32;cywjackson;""I don't see how making your dc/rack names your external IP address is going to solve anything.""

well the NPE was on 
{code}
return Gossiper.instance.getEndpointStateForEndpoint(endpoint).getApplicationState(ApplicationState.DC).value;
{code}

the given endpoint is not the local address; its the address from ""other"" nodes. For those ""other"" nodes, if they are not using the Ec2Snitch, which would have populated the ""ApplicationState.DC"" and ""ApplicationState.RACK"" with the values, getApplicationState(ApplicationState.DC) (and getApplicationState(ApplicationState.RACK) for that matter) is going to be return null. Hence you got a NPE from that line on .value.

Defaulting the AbstractEndpointSnitch's gossiperStarting by populating the ApplicationState.DC,ApplicationState.RACK wll help then any snitch relying the gossip info to getDC and getRack.
","16/Sep/11 02:04;brandon.williams;bq. Defaulting the AbstractEndpointSnitch's gossiperStarting by populating the ApplicationState.DC,ApplicationState.RACK wll help then any snitch relying the gossip info to getDC and getRack.

Yes, but setting DC to 'foo' and rack to 'bar' just creates a new DC and rack and breaks the replication policy and consistency guarantees.","16/Nov/11 21:47;brandon.williams;Closing, see CASSANDRA-3186",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LongCompactionSpeedTest fails,CASSANDRA-2461,12504066,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,12/Apr/11 15:02,12/Mar/19 14:13,13/Mar/19 22:26,14/Apr/11 18:20,0.8 beta 1,,,,,,0,,,,,"ant long-test -Dtest.name=LongCompactionSpeedTest fails.

There are several errors. Here is the first:

{noformat}
    [junit] java.lang.IllegalArgumentException
    [junit] 	at java.nio.ByteBuffer.allocate(ByteBuffer.java:311)
    [junit] 	at org.apache.cassandra.db.context.CounterContext.clearAllDelta(CounterContext.java:444)
    [junit] 	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:100)
    [junit] 	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:36)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:158)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:41)
    [junit] 	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)
    [junit] 	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
    [junit] 	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
    [junit] 	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
    [junit] 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    [junit] 	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    [junit] 	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
    [junit] 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    [junit] 	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    [junit] 	at org.apache.cassandra.db.ColumnIndexer.serializeInternal(ColumnIndexer.java:76)
    [junit] 	at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:50)
    [junit] 	at org.apache.cassandra.io.LazilyCompactedRow.<init>(LazilyCompactedRow.java:87)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableWriter$CommutativeRowIndexer.doIndexing(SSTableWriter.java:462)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:317)
    [junit] 	at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1089)
    [junit] 	at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1080)
{noformat}",,,,,,,,,,,,,,,,13/Apr/11 10:10;slebresne;0001-Fix-LongCompactionSpeedTest.patch;https://issues.apache.org/jira/secure/attachment/12476227/0001-Fix-LongCompactionSpeedTest.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-13 10:10:32.935,,,no_permission,,,,,,,,,,,,20636,,,Thu Apr 14 18:20:22 UTC 2011,,,,,,0|i0gbif:,93295,jbellis,jbellis,,,,,,,,,"13/Apr/11 10:10;slebresne;That was not updated correctly with CASSANDRA-1938, sorry. Patch attached.",14/Apr/11 18:20;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counters in super columns don't preserve correct values after cluster restart,CASSANDRA-3821,12540677,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,tpatterson,tpatterson,01/Feb/12 05:37,12/Mar/19 14:13,13/Mar/19 22:26,22/Feb/12 08:42,1.1.0,,,,,,0,,,,,"Set up a 3-node cluster with rf=3. Create a counter super column family and increment a bunch of subcolumns 100 times each, with cf=QUORUM. Then wait a few second, restart the cluster, and read the values back. They almost all come back different (and higher) then they are supposed to be.

Here are some extra things I've noticed:
 - Reading back the values before the restart always produces correct results.
 - Doing a nodetool flush before killing the cluster greatly improves the results, though sometimes a value will still be incorrect. You might have to run the test several times to see an incorrect value after a flush.
 - This problem doesn't happen on C* 1.0.7, unless you don't sleep between doing the increments and killing the cluster. Then it sometimes happens to a lesser degree.

A dtest has been added to demonstrate this issue. It is called ""super_counter_test.py"".","ubuntu, 'trunk' branch, used ccm to create a 3 node cluster with rf=3. A dtest was created to demonstrate.",,,,,,,,,,,,,,,21/Feb/12 19:09;slebresne;3821.txt;https://issues.apache.org/jira/secure/attachment/12515467/3821.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-06 23:23:56.526,,,no_permission,,,,,,,,,,,,226072,,,Wed Feb 22 08:42:34 UTC 2012,,,,,,0|i0gomf:,95419,yukim,yukim,,,,,,,,,"06/Feb/12 23:23;yukim;Here is my initial look at the issue (might be wrong):

Concurrent counter mutation replay from commitlog and AtomicSortedColumns inside Memtable seem to be the cause of over count.
There is a race condition when adding column to memtable, and when it happens AtomicSortedColumns calls {{{IColumn#reconcile}}} multiple times until column is stored. It causes over count since counter column's {{reconcile}} is not idempotent operation.","07/Feb/12 08:36;slebresne;Damn you super columns, there is nothing super about you!

Yuki is right that super columns are a problem for AtomicSortedColumns. Columns are immutable, so for them the multiple reconcile of ASC is not a problem since it's done on a cloned underlying map. But SuperColumns are not immutable. So even though the ASC (potential) mutliple attempts are made on a cloned map, the super columns structure themselves are not cloned, and so all these attempts modify the original SC. This does mean that SC are not isolated (I did knew about that, but somehow forget about it and certainly didn't realize the consequence on counters).

Anyway, I don't see an easy one line fix, so I would suggest to rather add back the ThreadSafeSortedColumns backing map and to use that for super column family, since super column family are not really isolated anyway. Then I guess, I'll make CASSANDRA-3237 a higher priority on my todo list.

bq. This problem doesn't happen on C* 1.0.7, unless you don't sleep between doing the increments and killing the cluster. Then it sometimes happens to a lesser degree.

The explanation above doesn't explain that. It would be worth investigating that separately (maybe a separate ticket).",21/Feb/12 19:09;slebresne;Attached patch that as said in previous comment reintroduce ConcurrentSkipListMap backed CF and use them for super column family. It fixes the distributed test.,21/Feb/12 22:31;yukim;I also ran dtest and confirmed the issue was fixed. so +1.,"22/Feb/12 08:42;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Auto bootstrapping the 4th node in a 4 node cluster doesn't work, when no token explicitly assigned in config.",CASSANDRA-2825,12511498,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,mallen,mallen,24/Jun/11 18:41,12/Mar/19 14:13,13/Mar/19 22:26,01/Aug/11 17:55,0.8.3,,,,,,0,,,,,"This was done in sequence.  A, B, C, and D.  Node A with token 0 explicitly set in config.  The rest with auto_bootstrap: true and no token explicitly assigned.  B and C work as expected. D ends up stealing C's token.  

from system.log on C:

INFO [GossipStage:1] 2011-06-24 16:40:41,947 Gossiper.java (line 638) Node /10.171.47.226 is now part of the cluster
INFO [GossipStage:1] 2011-06-24 16:40:41,947 Gossiper.java (line 606) InetAddress /10.171.47.226 is now UP
INFO [GossipStage:1] 2011-06-24 16:42:09,432 StorageService.java (line 769) Nodes /10.171.47.226 and /10.171.55.77 have the same token 61078635599166706937511052402724559481.  /10.171.47.226 is the new owner
WARN [GossipStage:1] 2011-06-24 16:42:09,432 TokenMetadata.java (line 120) Token 61078635599166706937511052402724559481 changing ownership from /10.171.55.77 to /10.171.47.226


",,,,,,,,,,,,,,,,20/Jul/11 17:29;brandon.williams;2825-v2.txt;https://issues.apache.org/jira/secure/attachment/12487188/2825-v2.txt,29/Jul/11 18:16;brandon.williams;2825-v3.txt;https://issues.apache.org/jira/secure/attachment/12488234/2825-v3.txt,20/Jul/11 01:17;brandon.williams;2825.txt;https://issues.apache.org/jira/secure/attachment/12487095/2825.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-06-30 17:59:00.601,,,no_permission,,,,,,,,,,,,20852,,,Mon Aug 01 18:16:19 UTC 2011,,,,,,0|i0gdov:,93648,,,,,,,,,,,"30/Jun/11 17:59;brandon.williams;I'm unable to reproduce.  What I don't understand is how token 61078635599166706937511052402724559481 can be involved if the first node started at 0.  My ring ends up looking like:

{noformat}
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               148820376661671484777635343879628161952     
10.179.65.102   datacenter1 rack1       Up     Normal  315.59 MB       12.53%  0                                           
10.179.64.227   datacenter1 rack1       Up     Normal  157.76 MB       49.97%  85015475407349018891169115382886066923      
10.179.111.137  datacenter1 rack1       Up     Normal  78.99 MB        25.01%  127563185643974719468523634380546988377     
10.179.110.154  datacenter1 rack1       Up     Normal  39.55 MB        12.49%  148820376661671484777635343879628161952 
{noformat}",15/Jul/11 23:09;brandon.williams;Can you show the ring at each step when you repro?,"18/Jul/11 22:03;mallen;{noformat}

Address         DC          Rack        Status State   Load            Owns    Token                                       
10.171.46.195   datacenter1 rack1       Up     Normal  8.91 KB         100.00% 0   


Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
10.171.46.195   datacenter1 rack1       Up     Normal  8.91 KB         50.00%  0                                           
10.170.121.70   datacenter1 rack1       Up     Normal  4.55 KB         50.00%  85070591730234615865843651857942052864 

Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
10.171.46.195   datacenter1 rack1       Up     Normal  8.91 KB         50.00%  0                                           
10.171.34.247   datacenter1 rack1       Up     Normal  4.55 KB         35.90%  61078635599166706937511052402724559481      
10.170.121.70   datacenter1 rack1       Up     Normal  13.4 KB         14.10%  85070591730234615865843651857942052864 


AddressDC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
10.171.46.195   datacenter1 rack1       Up     Normal  8.91 KB         50.00%  0                                           
10.171.23.65    datacenter1 rack1       Up     Normal  13.47 KB        35.90%  61078635599166706937511052402724559481      
10.170.121.70   datacenter1 rack1       Up     Normal  13.4 KB         14.10%  85070591730234615865843651857942052864

{noformat}","19/Jul/11 22:45;brandon.williams;The difference here is that when I tried to repro, I started with data in the cluster.  Since I never ran cleanup, token 0's range was always being split.  But if you start without any data, for some reason it decides to split 85070591730234615865843651857942052864 twice.  It's not clear why this is happening, since 0's load was still greater at the time, but at least one problem is that we do not assert that the token isn't in use when we determine it.","19/Jul/11 23:11;brandon.williams;Though 85070591730234615865843651857942052864 shows a load of 4.55KB when it is added, 90s later when another node checks it will have 13.4K because it will have updated.  The reason it has more load than the source at this point is because it flushed LocationInfo three times, where the source only flushed it twice.  That should be rare in practice, since typically if a cluster has no data you just start it up without bootstrapping, but still we should protect against handing out tokens that already are in use.","20/Jul/11 01:17;brandon.williams;Patch to exclude the system ks from key sampling when choosing a token, and assert that the token is not our own.

What was happening is that when 0's range was split, it didn't have more than 3 keys, so it used the midpoint.  When 85070591730234615865843651857942052864 split, it did have enough keys (in the system ks) and it sampled 61078635599166706937511052402724559481.  After this, 61078635599166706937511052402724559481 had the highest load and when it sampled it also arrived at 61078635599166706937511052402724559481 since it was using the system ks.

This patches forces using the midpoint method when there is no data other than the system ks, which is the right thing to do since the system ks data is never moved.","20/Jul/11 01:52;jbellis;can we generalize that to ""check that it's not any known endpoint"" and make it a ""real"" (non assert) check?",20/Jul/11 17:29;brandon.williams;v2 detects any token conflicts and throws a proper exception when they occur.,"20/Jul/11 17:41;jbellis;let's compare w/ Table.SYSTEM_TABLE, otherwise +1",20/Jul/11 17:53;brandon.williams;Committed w/change to Table.SYSTEM_TABLE comparison.,"20/Jul/11 18:18;hudson;Integrated in Cassandra-0.8 #228 (See [https://builds.apache.org/job/Cassandra-0.8/228/])
    Don't sample the system table keys when choosing a bootstrap token.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-2825

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148866
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
",21/Jul/11 10:04;slebresne;Reopening because the patch broke BootStrapperTest (it somehow hangs forever until junit timeout),"21/Jul/11 13:21;brandon.williams;Impressive, I have no idea how this is breaking testTokenRoundtrip():

{noformat}
    public void testTokenRoundtrip() throws Exception
    {  
        StorageService.instance.initServer();
        // fetch a bootstrap token from the local node
        assert BootStrapper.getBootstrapTokenFrom(FBUtilities.getLocalAddress()) != null;
    }
{noformat}

The log just shows a bunch of attempts to connect to the seed (127.0.0.2) which hasn't started yet.",21/Jul/11 15:15;slebresne;I've reverted the patch too so we can do a release of 0.8.2 without having to wait on the unit test fix.,29/Jul/11 18:16;brandon.williams;v3 adds a check that tokenMetadata_.getEndpoint(token) is not null when checking for existing membership.  This fixes the test.,01/Aug/11 17:55;brandon.williams;Committed v3.,"01/Aug/11 18:16;hudson;Integrated in Cassandra-0.8 #249 (See [https://builds.apache.org/job/Cassandra-0.8/249/])
    Don't sample the system table keys when choosing a bootstrap token.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-2825

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152876
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Columns missing after upgrade from 0.8.5 to 1.0.7.,CASSANDRA-3820,12540649,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,alienth,alienth,31/Jan/12 23:25,12/Mar/19 14:13,13/Mar/19 22:26,25/Jan/13 23:16,,,,,,,0,,,,,"After an upgrade, one of our CFs had a lot of rows with missing columns. I've been able to reproduce in test conditions. Working on getting the tables to DataStax(data is private).


0.8 results:

{code}
[default@reddit] get CommentVote[36353467625f63333837336f32];
=> (column=date, value=313332333932323930392e3531, timestamp=1323922909506508)
=> (column=ip, value=REDACTED, timestamp=1327048432717348, ttl=2592000)
=> (column=name, value=31, timestamp=1327048433000740)
=> (column=REDACTED, value=30, timestamp=1323922909506432)
=> (column=thing1_id, value=REDACTED, timestamp=1323922909506475)
=> (column=thing2_id, value=REDACTED, timestamp=1323922909506486)
=> (column=REDACTED, value=31, timestamp=1323922909506518)
=> (column=REDACTED, value=30, timestamp=1323922909506497)
{code}


1.0 results:

{code}
[default@reddit] get CommentVote[36353467625f63333837336f32];
=> (column=ip, value=REDACTED, timestamp=1327048432717348, ttl=2592000)
=> (column=name, value=31, timestamp=1327048433000740)
{code}



A few notes:

* The rows with missing data were fully restored after scrubbing the sstables.
* The row which I reproduced on happened to be split across multiple sstables.
* When I copied the first sstable I found the row on, I was able to 'list' rows from the sstable, but any and all 'get' calls failed.
* These SStables were natively created on 0.8.5; they did not come from any previous upgrade.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-02-01 01:53:03.046,,,no_permission,,,,,,,,,,,,226044,,,Fri Jan 25 23:16:35 UTC 2013,,,,,,0|i0a4yv:,57078,,,,,,,,,,,"01/Feb/12 01:53;scode;Check whether the .bf files contain all zeroes above roughly 235 mb or so. If you have lots of rows, your BF will be that large.

We encountered a bug internally whereby all bloom filters larger than 2^31 bits were large on disk, but everything after the first 2^31 bits were all zeroes.

Unfortunately I don't know whether this is specific to patches made to our branch, and I have been so busy I haven't been able to follow up to figure out whether it affects the upstream version.

But - just ""tail -c 1000 | hexdump"". If you only have zeroes, this is the bug. Make sure to tail on a large .bf file (take the largest, easiest).

","01/Feb/12 01:55;scode;And to be clear, I also discovered this when loading sstables from our 0.8 onto 1.0, and certain counter shards not being read from some sstables for a given row key as a result. As you indicate, I also were able to fix it by scrubbing on 1.0.

Presumably (unverified) the meta data is treated correctly on 1.0 but in the 0.8 version there is still some kind of 2^31 overflow, such that 0.8 can successfully reads its own (but will suffer elevated bloom filter false positives) eve though it's buggy (because it's buggy consistently), while 1.0 reads them correctly - but then suffers from the buggy behavior. This paragraph is speculation, and *not* confirmed.","01/Feb/12 01:56;scode;To be clear, if this is indeed the bug: You are only potentially vulnerable if you have > 143 million row keys. If you have > 143, you still may or may not be vulnerable depending on whether you have individual sstables with > 143 row keys. Anyone < 143 million row keys should be fine (again, *if* it's the same bug).
","01/Feb/12 02:45;brandon.williams;ISTM a bloom filter problem would suppress the entire row, not a portion of columns.  But since this is spread across multiple sstables, I guess that could make sense and is worth checking.","01/Feb/12 04:36;alienth;Peter: You're right! All zeroes on the end of my largest bloomfilter file.

The affected CF has billions of keys, and the sstable in question is 35GB, so I'm guessing it is likely over 143 million.","01/Feb/12 07:26;scode;@Brandon - Yes, the implicit assumption was that the row was spread over multiple sstables.

@Jason - Great, so to speak. Now all that's needed is a fix ;) Good to know what the problem is.
",05/Mar/12 16:13;jjordan;Will running scrub fix this problem?  Will data be lost in the scrub?  Does it have to be scrub or will upgradesstables fix the issue as well?,"05/Mar/12 16:34;jbellis;Yes, scrub and upgradesstables both fix the problem w/o data loss.","25/Jan/13 23:16;jbellis;Closing as Fixed, but it's more of a ""workaround exists and is understood.""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests failures in 1.1,CASSANDRA-3864,12541465,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,slebresne,slebresne,06/Feb/12 19:09,12/Mar/19 14:13,13/Mar/19 22:26,15/Feb/12 15:14,1.1.0,,,,,,0,,,,,"On the current 1.1 branch I get the following errors:
# SSTableImportTest:
{noformat}
[junit] Testcase: testImportSimpleCf(org.apache.cassandra.tools.SSTableImportTest):	Caused an ERROR
[junit] java.lang.Integer cannot be cast to java.lang.Long
[junit] java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long
[junit] 	at org.apache.cassandra.tools.SSTableImport$JsonColumn.<init>(SSTableImport.java:132)
[junit] 	at org.apache.cassandra.tools.SSTableImport.addColumnsToCF(SSTableImport.java:191)
[junit] 	at org.apache.cassandra.tools.SSTableImport.addToStandardCF(SSTableImport.java:174)
[junit] 	at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:290)
[junit] 	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:255)
[junit] 	at org.apache.cassandra.tools.SSTableImportTest.testImportSimpleCf(SSTableImportTest.java:60)
{noformat}
# CompositeTypeTest:
{noformat}
[junit] Testcase: testCompatibility(org.apache.cassandra.db.marshal.CompositeTypeTest):	Caused an ERROR
[junit] Invalid comparator class org.apache.cassandra.db.marshal.CompositeType: must define a public static instance field or a public static method getInstance(TypeParser).
[junit] org.apache.cassandra.config.ConfigurationException: Invalid comparator class org.apache.cassandra.db.marshal.CompositeType: must define a public static instance field or a public static method getInstance(TypeParser).
[junit] 	at org.apache.cassandra.db.marshal.TypeParser.getRawAbstractType(TypeParser.java:294)
[junit] 	at org.apache.cassandra.db.marshal.TypeParser.getAbstractType(TypeParser.java:268)
[junit] 	at org.apache.cassandra.db.marshal.TypeParser.parse(TypeParser.java:81)
[junit] 	at org.apache.cassandra.db.marshal.CompositeTypeTest.testCompatibility(CompositeTypeTest.java:216)
{noformat}
# DefsTest:
{noformat}
[junit] Testcase: testUpdateColumnFamilyNoIndexes(org.apache.cassandra.db.DefsTest):	FAILED
[junit] Should have blown up when you used a different comparator.
[junit] junit.framework.AssertionFailedError: Should have blown up when you used a different comparator.
[junit] 	at org.apache.cassandra.db.DefsTest.testUpdateColumnFamilyNoIndexes(DefsTest.java:539)
{noformat}
# CompactSerializerTest:
{noformat}
[junit] null
[junit] java.lang.ExceptionInInitializerError
[junit] 	at org.apache.cassandra.db.SystemTable.getCurrentLocalNodeId(SystemTable.java:437)
[junit] 	at org.apache.cassandra.utils.NodeId$LocalNodeIdHistory.<init>(NodeId.java:195)
[junit] 	at org.apache.cassandra.utils.NodeId$LocalIds.<clinit>(NodeId.java:43)
[junit] 	at java.lang.Class.forName0(Native Method)
[junit] 	at java.lang.Class.forName(Class.java:169)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:96)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest.scanClasspath(CompactSerializerTest.java:129)
[junit] Caused by: java.lang.NullPointerException
[junit] 	at org.apache.cassandra.config.DatabaseDescriptor.createAllDirectories(DatabaseDescriptor.java:574)
[junit] 	at org.apache.cassandra.db.Table.<clinit>(Table.java:82)
{noformat}

There is also some error RemoveSubColumnTest and RemoveSubColumnTest but I'll open a separate ticket for those as they may require a bit more discussion.",,,,,,,,,,,,,,,,06/Feb/12 19:10;slebresne;0001-Fix-DefsTest.patch;https://issues.apache.org/jira/secure/attachment/12513469/0001-Fix-DefsTest.patch,06/Feb/12 19:10;slebresne;0002-Fix-SSTableImportTest.patch;https://issues.apache.org/jira/secure/attachment/12513470/0002-Fix-SSTableImportTest.patch,06/Feb/12 19:10;slebresne;0003-Fix-CompositeTypeTest.patch;https://issues.apache.org/jira/secure/attachment/12513471/0003-Fix-CompositeTypeTest.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-02-07 14:08:37.735,,,no_permission,,,,,,,,,,,,226752,,,Wed Feb 15 15:14:44 UTC 2012,,,,,,0|i0gp5r:,95506,,,,,,,,,,,"06/Feb/12 19:18;slebresne;Attached patches are fixing failures 1, 2 and 3 above. More precisely:
# SSTableImportTest: the patch speaks for itself. It's likely due a change of the jackson lib version or something like that.
# CompositeTypeTest: this is a regression of commit 96ecaff5 that removed a perfectly fine (and useful) method
# DefsTest: CASSANDRA-3657 mad possible the kind of comparator change the test was testing (UTF8Type -> BytesType). The patch replaces that by a non-compatible change.

For the CompactSerializerTest, I'm a little bit less sure what the right fix is. As weird as that sound, the problem comes from BulkRecordWriter. Somehow the class is loaded by the test, which makes it replace the config field of DatabaseDescriptor by a Config object where data_file_directories is null (more precisely DatabaseDescriptor.initDefaultsOnly() is called). I'm pretty sure there is something a little bit ugly in there, but since I'm not sure how BulkRecordWriter is supposed to be used, I leave that to someone else.",07/Feb/12 14:08;jbellis;+1,"07/Feb/12 16:08;slebresne;Committed, thanks.

I'm keeping this open so that we deal with the CompactSerializerTest/BulkRecordWriter failure, but I'll assign someone that actually know something about BulkRecordWriter.","07/Feb/12 18:32;brandon.williams;Hmm, I had to add SnapshotCommandSerializer to expectedClassNames in CompactSerializerTest, but after that it passes for me.","08/Feb/12 14:56;slebresne;That's weird, as I still get it every time.
But actually I think that is a bit dependent on the class loader, so this may be platform dependent (I'm using sun java 1.6.0_26).

In any case, I think there do is something fishy in there. Namely, BulkRecordWriter override the global configuration in it's static initializer. And the config it override with has data_file_directories == null. Which means that if the class Table is loaded after that, it will try to create default directories as part of the class initialization (unless StorageService.isClientMode == true) and will throw a NPE because data_file_directories == null. I'm not too sure in which context exactly BulkRecordWriter is supposed to be used, but outside of making the unit test fail at least on my machine, it does sound fairly fragile.
Of course, that's another way to say that all our static initialization business is fragile, and we should fix that someday. Nevertheless, would it be reasonable for instance in BulkLoaderWriter to call something to set StorageService.isClientMode to true before changing the configuration (so that if Table is loaded later, we don't try to create the default directories)? ","08/Feb/12 17:41;brandon.williams;bq. That's weird, as I still get it every time.

I think that's because I was running this test by itself, so BRW was never loaded (though I'm pretty sure we need to add SnapshotCommandSerializer if we do get past this.)

bq. BulkRecordWriter override the global configuration in it's static initializer

That was a failed attempt to get around DD's static initialization (and yes, all our static initialization is a mess), but I've reworked the approach in CASSANDRA-3740 and that should pass since there is no static initialization.
","08/Feb/12 17:48;slebresne;bq. I think that's because I was running this test by itself

I get it even if I run the test by itself. That's because CompactSerializerTest actually load pretty much every class during it's scanning. But I suppose the order in which it load stuffs is very platform dependent.

bq. I'm pretty sure we need to add SnapshotCommandSerializer if we do get past this

That may very well be.

bq. I've reworked the approach in CASSANDRA-3740 and that should pass since there is no static initialization

Alright, let's wait for it. I'll retest and close this if that fixes it.","15/Feb/12 15:14;slebresne;Ok, I think the problems of that ticket are now all solved, closing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfBounds in CompressedSequentialWriter.flushData,CASSANDRA-2994,12517968,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,stuhood,stuhood,04/Aug/11 22:07,12/Mar/19 14:13,13/Mar/19 22:26,08/Aug/11 14:16,1.0.0,,,,,,0,,,,,"Near the beginning of a wide row test with CASSANDRA-47 compression enabled on a counter column family, I see the following exception:

{code:java} WARN [CompactionExecutor:5] 2011-08-04 21:50:14,558 FileUtils.java (line 95) Failed closing org.apache.cassandra.io.compress.CompressedSequentialWriter@28f01347
java.lang.IndexOutOfBoundsException
	at java.io.RandomAccessFile.writeBytes(Native Method)
	at java.io.RandomAccessFile.write(RandomAccessFile.java:466)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.flushData(CompressedSequentialWriter.java:88)
	at org.apache.cassandra.io.util.SequentialWriter.flushInternal(SequentialWriter.java:174)
	at org.apache.cassandra.io.util.SequentialWriter.syncInternal(SequentialWriter.java:150)
	at org.apache.cassandra.io.util.SequentialWriter.close(SequentialWriter.java:283)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.close(CompressedSequentialWriter.java:159)
	at org.apache.cassandra.io.util.FileUtils.closeQuietly(FileUtils.java:91)
	at org.apache.cassandra.io.sstable.SSTableWriter.cleanupIfNecessary(SSTableWriter.java:201)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:176)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:120)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:103)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [CompactionExecutor:5] 2011-08-04 21:50:14,561 AbstractCassandraDaemon.java (line 146) Fatal exception in thread Thread[CompactionExecutor:5,1,main]
java.lang.IndexOutOfBoundsException
	at java.io.RandomAccessFile.writeBytes(Native Method)
	at java.io.RandomAccessFile.write(RandomAccessFile.java:466)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.flushData(CompressedSequentialWriter.java:88)
	at org.apache.cassandra.io.util.SequentialWriter.flushInternal(SequentialWriter.java:174)
	at org.apache.cassandra.io.util.SequentialWriter.reBuffer(SequentialWriter.java:226)
	at org.apache.cassandra.io.util.SequentialWriter.writeAtMost(SequentialWriter.java:117)
	at org.apache.cassandra.io.util.SequentialWriter.write(SequentialWriter.java:101)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:105)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:150)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:153)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:120)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:103)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}",,,,,,,,,,,,,,,,05/Aug/11 12:54;slebresne;2994-v2.patch;https://issues.apache.org/jira/secure/attachment/12489471/2994-v2.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-05 08:57:36.378,,,no_permission,,,,,,,,,,,,20929,,,Mon Aug 08 15:23:22 UTC 2011,,,,,,0|i0geq7:,93816,,,,,,,,,,,"04/Aug/11 22:17;stuhood;Before anyone pursues this, lemme confirm that it isn't due to a bad merge... sorry, should have done that before.

EDIT: Confirmed in trunk.","04/Aug/11 22:51;stuhood;The workload was a wide row usecase, but based on the stack trace, the row hadn't reached a width to trigger LazilyCompactedRow.","05/Aug/11 08:57;slebresne;A probable cause is that snappy doesn't seem to guarantee that the compressed output will shorter or equal to the input (indeed, Snappy.maxCompressedLength(65536) == 76490).

Attaching fix for that. I don't see what else it can be actually, but I don't know how to reproduce, so Stu, if you can reproduce and can check if this fixes it, that'd be cool.","05/Aug/11 11:57;xedin;I guess you will also need to replace 

compressed = new byte[metadata.chunkLength]; 

with 

compressed = new byte[Snappy.maxCompressedLength(metadata.chunkLength)];

in the CompressedRandomAccessReader constructor.
","05/Aug/11 12:54;slebresne;Oups, you're right, patch updated.","05/Aug/11 18:49;stuhood;+1
Looks like that was it: thanks!","08/Aug/11 14:16;slebresne;Committed, thanks
","08/Aug/11 15:23;hudson;Integrated in Cassandra #1009 (See [https://builds.apache.org/job/Cassandra/1009/])
    Fix OutOfBounds with compression
patch by slebresne; reviewed by stuhood for CASSANDRA-2994

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1154969
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating or updating CF key_validation_class with the CLI doesn't works,CASSANDRA-2831,12511644,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,silvere,silvere,27/Jun/11 12:54,12/Mar/19 14:13,13/Mar/19 22:26,27/Jun/11 13:54,0.8.1,,,,,,0,,,,,"In the command line:
{code}
create column family test with key_validation_class = 'AsciiType' and comparator = 'LongType' and default_validation_class = 'IntegerType';
describe keyspace;
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
{code}
The ""Default column value validator"" is BytesType instead of IntegerType. Also tested with other types or with the ""update column family"" command, same problem occur.

{code}
[default@Test] update column family test with default_validation_class = 'LongType';
51a37430-a0bb-11e0-0000-ef8993101fdf
Waiting for schema agreement...
... schemas agree across the cluster
[default@Test] describe keyspace;                                                   
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
{code}

Btw, they are a typo in file src/resources/org/apache/cassandra/cli/CliHelp.yaml line 642: key_valiation_class > key_validation_class
Very annoying for people like me who stupidly copy/paste the help.","Ubuntu 10.10, 32 bits
java version ""1.6.0_24""
Brisk beta-2 installed from Debian packages",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-06-27 13:54:38.426,,,no_permission,,,,,,,,,,,,20855,,,Mon Jun 27 13:54:38 UTC 2011,,,,,,0|i0gdq7:,93654,,,,,,,,,,,27/Jun/11 13:54;jbellis;fixed by r1137774,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Range and Bounds objects client-safe,CASSANDRA-3108,12520830,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,michaelsembwever,jbellis,jbellis,30/Aug/11 20:43,12/Mar/19 14:13,13/Mar/19 22:26,30/Aug/11 21:16,0.8.5,,,,,,0,hadoop,,,,"From Mck's comment on CASSANDRA-1125:

Something broke here in production once we went out with 0.8.2. It may have been some poor testing, i'm not entirely sure and a little surprised.

CFIF:135 breaks because inside dhtRange.intersects(jobRange) there's a call to new Range(token, token) which calls StorageService.getPartitioner() and StorageService is null as we're not inside the server.

A quick fix is to change Range:148 from new Range(token, token) to new Range(token, token, partitioner) making the presumption that the partitioner for the new Range will be the same as this Range.
",,,,,,,,,,,,,,,,30/Aug/11 20:45;jbellis;3108.txt;https://issues.apache.org/jira/secure/attachment/12492363/3108.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-30 21:11:52.388,,,no_permission,,,,,,,,,,,,4088,,,Wed Sep 07 09:37:04 UTC 2011,,,,,,0|i0gfw7:,94005,jbellis,jbellis,,,,,,,,,"30/Aug/11 20:45;jbellis;patch to make all internal Range and Bounds construction use the 3-arg constructor.
","30/Aug/11 21:11;michaelsembwever;Tested in production. 
+1","30/Aug/11 21:16;jbellis;committed, thanks!","30/Aug/11 22:25;hudson;Integrated in Cassandra-0.8 #305 (See [https://builds.apache.org/job/Cassandra-0.8/305/])
    make Range and Bounds objects client-safe
patch by Mck SembWever and jbellis for CASSANDRA-3108

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163394
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/dht/Bounds.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/dht/Range.java
","05/Sep/11 11:33;michaelsembwever;Didn't see it until now but your patch Jonathan removes the limitation that ConfigHelper's InputKeyRange cannot wrap.
I've entered CASSANDRA-3137 to allow wrapping intersections in {{ColumnFamilyInputFormat}}.",05/Sep/11 19:19;jbellis;That was unintentional -- how did I do that?,"05/Sep/11 19:40;michaelsembwever;You drastically removed the usage of the {{Range(left, right)}} constructor so that even the usage of {{intersectionBothWrapping(..)}} and {{intersectionOneWrapping(..)}} avoids any server-side calls.

In CFIF there AFAIK doesn't seem any other limitation to wrapping ranges...","06/Sep/11 21:28;jbellis;bq. even the usage of intersectionBothWrapping(..) and intersectionOneWrapping(..) avoids any server-side calls

Right.  What I don't follow is how this changes behavior of allowing wrapping ranges?","07/Sep/11 09:37;michaelsembwever;No, in itself it doesn't implement wrapping ranges. But by making intersectionBothWrapping(..) and intersectionOneWrapping(..) client safe it makes it possible to implement.
I think the best explanation to your question Jonathan is to look at the patch available in CASSANDRA-3137",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test are hanging on 0.8 branch,CASSANDRA-3520,12532289,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,22/Nov/11 08:42,12/Mar/19 14:13,13/Mar/19 22:26,28/Nov/11 16:39,0.8.8,,,Legacy/Testing,,,0,,,,,"As the summary says, the unit test on current 0.8 are just hanging after CliTest (it's apparently not the case on windows, but it is on Linux and MacOSX).
Not sure what's going on, but what I can tell is that it's enough to run CliTest to have it hang after the test successfully pass (i.e, JUnit just wait indefinitely for the VM to exit). Even weirder, it seems that it is the counter increment in the CliTest that make it hang, if you comment those statement, it stop hanging. However, nothing seems to go wrong with the increment itself (the test passes) and it doesn't even trigger anything (typically sendToHintedEndpoint is not called because there is only one node).
Looking at the stack when the VM is hanging (attached), there is nothing specific to counters in there, and nothing that struck me at odd (but I could miss something). There do is a few thrift thread running (CASSANDRA-3335), but why would that only be a problem for the tests in that situation is a mystery to me.",Linux,,,,,,,,,,,,,,,25/Nov/11 15:22;slebresne;0001-Use-durable-writes-for-system-ks.patch;https://issues.apache.org/jira/secure/attachment/12505117/0001-Use-durable-writes-for-system-ks.patch,28/Nov/11 11:13;slebresne;3520.patch;https://issues.apache.org/jira/secure/attachment/12505320/3520.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-22 14:04:21.589,,,no_permission,,,,,,,,,,,,218022,,,Mon Nov 28 16:39:05 UTC 2011,,,,,,0|i0gkx3:,94819,jbellis,jbellis,,,,,,,,,"22/Nov/11 14:04;jbellis;Does it hang if you just run CliTest, or only for the whole suite?","22/Nov/11 14:08;slebresne;Only CliTest. I honestly didn't really try to run other tests (and CliTest is like the 3rd test), so possibly there is other tests failing.","22/Nov/11 15:34;jbellis;On Windows CliTest on 0.8 times out, after all the tests pass, but it does not hang.",22/Nov/11 15:34;jbellis;(My 0.8 checkout was on the 0.8.6 tag when I reported that CliTest worked completely.),"25/Nov/11 15:22;slebresne;So, some form of progress here. The hanging can be bisected to r1185961 (svn). And it is actually due to the switch to non-durable writes for the system keyspace. But I don't know why yet. In particular 1.0 and trunk also use non-durable writes and have no such problem.

I'm attaching a small patch to re-enable durable writes. I think we should figure out what is going on, but if we want to go ahead with the release of 0.8.8 in the meantime, we could apply that.

Last info, CliTest is not the only one to hang, CleanupTest and AntiEntropyServiceCounterTest.java are also hanging.
","25/Nov/11 15:56;slebresne;btw, how sure are we that using non-durable writes for the system keyspace is a good idea? It doesn't seem great for hints at least. Moreover on 1.0, when will the CF be flushed if durable_writes is false unless we do a manual flush?","25/Nov/11 16:54;jbellis;Setting durable_writes back to true does not fix CliTest timing out on windows.  However, I can't think a good reason to have it off for the system KS so I committed 0001.","25/Nov/11 17:13;hudson;Integrated in Cassandra-0.8 #406 (See [https://builds.apache.org/job/Cassandra-0.8/406/])
    set system keyspace back to durable_writes
patch by slebresne; reviewed by jbellis for CASSANDRA-3520

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1206257
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/KSMetaData.java
","25/Nov/11 17:42;jbellis;I can confirm that r1185960 is the last revision that doesn't timeout CliTest.  (r1185961 doesn't compile, and r1185963 times out.)","28/Nov/11 09:50;slebresne;I was a bit fast at calling it victory. I was testing with CleanupTest that was hanging as well, and starting from the same revision, so I assumed that the problem was the exact same as for CliTest. Turns out setting durable writes does fix both CleanupTest and AntiEntropyServiceCounterTest, but CliTest is still hanging (it's now the only one to do so however).","28/Nov/11 11:13;slebresne;So, the whole problem is due to our handling of non durable writes in the shutdown hook. For those, we flush the CFS as part of shutdown. However, flush tries to grab a commitlog context, which blocks because the commit log has been shutdown *before* all this (and for some reason, executor.submit() don't throw any exception if the executor is shutdown).

The reason why r1185960 was triggering this is that it actually fixed a bug by which previously to this commit, adding a new column family to a keyspace would reset the durableWrites option to true, hence hiding the bug as far as CliTest is concerned.

One simple solution is to move the commit log shutdown after the flushes of the non-durable CFs (which 1.0 does, and that's why it isn't affected). Truth is, it doesn't feel like the right fix in that non-durable CF shouldn't query the commit log at all, even during flushes. However, changing that introduces the possibility to have some CL segment retained forever when upgrading a keyspace from non-durable to durable if we're not careful. So overall just pushing the CL shutdown down in the shutdown hook to match 1.0 seems good enough, at least for 0.8. Attaching a patch to do just that. We can then look at making things cleaner with respect to flushing non-durable CFS in 1.0/trunk if we so wish.

Note that while having a non-durable system keyspace was not directly the problem, I think it was a fairly bad idea, and we should leave it to durable for 0.8 and turn it back to durable for 1.0 and trunk.
","28/Nov/11 14:28;jbellis;+1, but can we add a system test w/ a non-durable ks to help prevent regressions?

Edit: never mind, that's what we're already doing in NoCommitlogSpace, hence the continuing clitest timeout","28/Nov/11 15:21;hudson;Integrated in Cassandra-0.8 #407 (See [https://builds.apache.org/job/Cassandra-0.8/407/])
    Shutdown CL after having flushed non-durable CF
patch by slebresne; reviewed by jbellis for CASSANDRA-3520

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1207262
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
",28/Nov/11 16:39;slebresne;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed Streams Break Repair,CASSANDRA-2433,12503646,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,bcoverston,bcoverston,07/Apr/11 15:40,12/Mar/19 14:13,13/Mar/19 22:26,31/Aug/11 16:36,0.8.5,,,,,,5,repair,,,,"Running repair in cases where a stream fails we are seeing multiple problems.

1. Although retry is initiated and completes, the old stream doesn't seem to clean itself up and repair hangs.
2. The temp files are left behind and multiple failures can end up filling up the data partition.

These issues together are making repair very difficult for nearly everyone running repair on a non-trivial sized data set.

This issue is also being worked on w.r.t CASSANDRA-2088, however that was moved to 0.8 for a few reasons. This ticket is to fix the immediate issues that we are seeing in 0.7.",,,,,,,,,,,,,,,,15/Jun/11 12:27;slebresne;0001-Put-repair-session-on-a-Stage-and-add-a-method-to-re-v4.patch;https://issues.apache.org/jira/secure/attachment/12482658/0001-Put-repair-session-on-a-Stage-and-add-a-method-to-re-v4.patch,15/Jun/11 12:27;slebresne;0002-Register-in-gossip-to-handle-node-failures-v4.patch;https://issues.apache.org/jira/secure/attachment/12482659/0002-Register-in-gossip-to-handle-node-failures-v4.patch,15/Jun/11 12:27;slebresne;0003-Report-streaming-errors-back-to-repair-v4.patch;https://issues.apache.org/jira/secure/attachment/12482660/0003-Report-streaming-errors-back-to-repair-v4.patch,15/Jun/11 12:27;slebresne;0004-Reports-validation-compaction-errors-back-to-repair-v4.patch;https://issues.apache.org/jira/secure/attachment/12482661/0004-Reports-validation-compaction-errors-back-to-repair-v4.patch,02/Aug/11 17:52;slebresne;2433.patch;https://issues.apache.org/jira/secure/attachment/12489094/2433.patch,30/Aug/11 15:25;slebresne;2433_v2.patch;https://issues.apache.org/jira/secure/attachment/12492251/2433_v2.patch,31/Aug/11 14:52;slebresne;2433_v3.patch;https://issues.apache.org/jira/secure/attachment/12492464/2433_v3.patch,31/Aug/11 15:46;slebresne;2433_v4.patch;https://issues.apache.org/jira/secure/attachment/12492468/2433_v4.patch,,,,8.0,,,,,,,,,,,,,,,,,,,2011-04-21 11:45:12.903,,,no_permission,,,,,,,,,,,,20622,,,Wed Aug 31 17:16:01 UTC 2011,,,,,,0|i0gbcn:,93269,jbellis,jbellis,,,,,,,,,"21/Apr/11 11:45;slebresne;Attached patches are against 0.8.

This tries to catch what can go wrong with repair and reports it back to the user by making the full repair throw an exception. More precisely:
  * patch 0001: add a method to repair for reporting failure and propagate that up to the repair session. This puts repair session on a specific stage (instead of having RepairSession be a Thread) and use a future to allow waiting on completion. This allows a cleaner API to deal with errors (the Future.get() simply throw an ExecutionException) and this add the advantage of stage management to repair sessions.
  * patch 0002: Make repair session register through gossip to be informed of node dying and failing the session when that happens.
  * patch 0003: Reports errors during streaming to the repair session. This actually introduces a generic way to handle streaming failures and after that we should probably update the other user of streaming to deal correctly with failure too.
  * patch 004: Catch errors during validation compaction and push them up to repair (whether those happens on the coordinator of the repair or not).

Note that this includes streaming failures and thus includes stuffs from the patch of Aaron Morton attached on CASSANDRA-2088, but contrarily to that patch, it takes the approach of failing fast. This means that if streaming fails on a file, it fails the streaming altogether (same for repair). I think this is simpler code-wise and more useful from the point of view of the user, since a failure means the use will have to retry anyway.

Last but not least, this makes some modification to messages. So either this goes into 0.8.0 (which I think it should, because this really is a bug fix and fixes something that is a pain for users), or we should had a new messaging version for 0.8.0 and modify this to take it into account (we should probably add a 0.8.0 version to the messaging service anyway).
",17/May/11 20:01;slebresne;Attaching rebased patch (against 0.8.1). It also change the behavior a little bit so as to not fail repair right away if a problem occur (it still throw an exception at the end if any problem had occured). It turns out to be slightly simpler that way. Especially for CASSANDRA-1610.,"23/May/11 20:09;stuhood;0001
* Since we're not trying to control throughput or monitor sessions, could we just use Stage.MISC?

0002
* I think RepairSession.exception needs to be volatile to ensure that the awoken thread sees it
* Would it be better if RepairSession implemented IEndpointStateChangeSubscriber directly?
* The endpoint set needs to be threadsafe, since it will be modified by the endpoint state change thread, and the AE_STAGE thread

0003
* Should StreamInSession.retries be volatile/atomic? (likely they won't retry quickly enough for it to be a problem, but...)

0004
* Playing devil's advocate: would sending a half-built tree in case of failure still be useful?
* success might need to be volatile as well

Thanks Sylvain!","09/Jun/11 14:09;slebresne;Attaching v3 rebased (on 0.8).

bq. Since we're not trying to control throughput or monitor sessions, could we just use Stage.MISC?

The thing is that repair session are very long lived. And MISC is single threaded. So that would block other task that are not supposed to block. We could make MISC multi-threaded but even then it's not a good idea to mix short lived and long lived task on the same stage.

bq. I think RepairSession.exception needs to be volatile to ensure that the awoken thread sees it

Done in v3.

bq. Would it be better if RepairSession implemented IEndpointStateChangeSubscriber directly?

Good idea, it's slightly simpler, done in v3.

bq. The endpoint set needs to be threadsafe, since it will be modified by the endpoint state change thread, and the AE_STAGE thread

Done in v3. That will probably change with CASSANDRA-2610 anyway (which I have to update)

bq. Should StreamInSession.retries be volatile/atomic? (likely they won't retry quickly enough for it to be a problem, but...)

I did not change that, but if it's a problem for retries to not be volatile, I suspect having StreamInSession.current not volatile is also a problem. But really I'd be curious to see that be a problem.

bq. Playing devil's advocate: would sending a half-built tree in case of failure still be useful?

I don't think it is. Or more precisely, if you do send half-built tree, you'll have to be careful that the other doesn't consider what's missing as ranges not being in sync (I don't think people will be happy with tons of data being stream just because we happen to have a bug that make compaction throw an exception during the validation). So I think you cannot do much with a half-built tree, and it will add complication. For a case where people will need to restart a repair anyway once whatever happened is fixed

bq. success might need to be volatile as well

Done in v3.
",15/Jun/11 12:27;slebresne;Attaching v4 that is rebased and simply set the reties variable in StreamInSession volatile after all (I've removed old version because it was a mess).,18/Jul/11 08:08;stuhood;Hey Sylvain: sorry it took me so long to get back to this one. Would you mind rebasing it?,"02/Aug/11 17:52;slebresne;Attaching a rebase of the two previous first patches as '2433.patch'. That is, this patch adds registering in gossip so that repair fails and report it to the user when a node participating to the repair dies. Compared to the previous version, it fails fast because it's the easier thing to do now and a better option imho.

I should mention that while it is lame that repair get stuck when a node dies and we should fix it, this means that if a node is wrongly marked down, we will fail repair for no reason (but I suppose it's a failure detector problem).

Attached patch is against 0.8. This has no upgrade consequence of any sort and is a reasonably simple patch, so I think it could be worth committing in 0.8.
The rest of what was in previous patch 0003 and 0004 cannot go into 0.8 because it changes the wire protocol, so I will rebase against trunk directly, and maybe in another ticket. Having this first patch committed would help with that though :)","08/Aug/11 17:46;jbellis;Yuki, can you review this patch?","30/Aug/11 15:25;slebresne;Attached v2 is rebased and use a higher conviction threshold before deciding to fail the repair, as the goal here is to avoid having a repair getting stuck for hours, but we want to avoid stopping a repair just because a node got into a longer than usual GC pause.

The threshold used is twice the configured phi_convict_threshold. This give 16 by default, which if I trust the original 'phi accrual failure detection' should give an order of magnitude less false positive than 8 (for about an order of magnitude in the detection time though). It feels reasonable to me but if a FD specialist want to voice his opinion, please do.","30/Aug/11 15:42;jbellis;- Why do we need the new AE_SESSIONS stage?
- I prefer using WrappedRunnable to a Callable when you want to allow exceptions but don't care about a return value
- I think we can avoid a bunch of no-op onConvicts if RepairSession were to subscribe to FD directly instead of going through Gossip (i.e., leave IEndpointStateChangeSubscriber unchanged and expose convict in IFailureDetectionEventListener for when we need to go low-level).  Gossip is about high-level ""events"" which doesn't really fit here.","30/Aug/11 16:37;slebresne;bq. Why do we need the new AE_SESSIONS stage?

If you mean ""why AE_SESSIONS when we already have the AE stage?"", then it is because repair push stuffs on the AE stage that it wait for, so we would deadlock. If you mean ""why a stage?"", it felt cleaner that just a Thread now that we want to check for exception at the end of the exception. If you mean ""why a stage rather than a simple ThreadExecutor?"", it is a good question. I guess it was just some reflex of mine to get a JMXEnabledThreadPool, but it's probably not worth a stage, not even the jmx enabledness maybe.

bq. I prefer using WrappedRunnable to a Callable when you want to allow exceptions but don't care about a return value

Agreed. I'll update the patch.

bq. I think we can avoid a bunch of no-op onConvicts if RepairSession were to subscribe to FD directly instead of going through Gossip

Yeah, I kind of started with that but the problem is that we must deal with the case of a node restarting before it has been convicted (especially if the conviction threshold is higher), which the FD won't see. We could deal of that last situation separately and have Gossip call some trigger into AntiEntropy on a gossip generation change to indicate to stop every started session involving the given endpoint, but creating a dependency of gossip to anti-entropy didn't felt like a good idea a priori.","30/Aug/11 16:44;jbellis;bq. it's probably not worth a stage, not even the jmx enabledness maybe

Someone's probably going to want the JMX information but let's keep Stages for Verb-associated tasks.

bq. the problem is that we must deal with the case of a node restarting before it has been convicted (especially if the conviction threshold is higher), which the FD won't see

How about splitting onDead and onRestart in EndpointStateChange, then?  Then RS could implement convict and onRestart (ignoring onDead); other ESCS listeners could implement onRestart == onDead.  That would maintain the ""ESCS is about events, FDEL is low-level convict information"" separation of roles.","31/Aug/11 14:46;slebresne;bq. Someone's probably going to want the JMX information but let's keep Stages for Verb-associated tasks

Sounds good, updated patch add a new executor directly into AntiEntropy.

bq. How about splitting onDead and onRestart in EndpointStateChange, then?

Done.

bq. I prefer using WrappedRunnable to a Callable

I changed to use WrappedRunnable. However, we still need to have access to both the repair session and the future from the executor so the implementation returns a pair of those two objects. I'm only marginally convinced this is cleaner than the previous solution...
","31/Aug/11 14:52;slebresne;(Sorry, I had attached the wrong version of v3, corrected now)","31/Aug/11 14:56;jbellis;bq. we still need to have access to both the repair session and the future from the executor so the implementation returns a pair of those two objects

You can still use the RepairFuture approach, just use the FutureTask(Runnable, V) constructor","31/Aug/11 15:46;slebresne;You're right, don't know why I got carried away like that. v4 ""fixes"" this.",31/Aug/11 15:51;jbellis;+1,"31/Aug/11 16:36;slebresne;Committed, thanks.

This probably solves most of the case where repair was hanging infinitely. I've created CASSANDRA-3112 to handle the remaining cases, but it is much less urgent imho. Marking that one as resolved","31/Aug/11 17:16;hudson;Integrated in Cassandra-0.8 #306 (See [https://builds.apache.org/job/Cassandra-0.8/306/])
    Make repair report failure when a participating node dies
patch by slebresne; reviewed by jbellis for CASSANDRA-2433

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163677
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/FailureDetector.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/IEndpointStateChangeSubscriber.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/IFailureDetectionEventListener.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/MigrationManager.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageLoadBalancer.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE During Repair In StreamReplyVerbHandler,CASSANDRA-2377,12502261,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,bcoverston,bcoverston,24/Mar/11 06:57,12/Mar/19 14:13,13/Mar/19 22:26,24/Mar/11 18:59,0.7.5,,,,,,0,,,,,"ERROR [MiscStage:4] 2011-03-24 02:45:05,172 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutorjava.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:62)        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
ERROR [MiscStage:4] 2011-03-24 02:45:05,172 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[MiscStage:4,5,main]java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:62)        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)",CentOS,,,,,,,,,,,,,,,24/Mar/11 18:30;brandon.williams;2377.txt;https://issues.apache.org/jira/secure/attachment/12474537/2377.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-24 09:22:49.41,,,no_permission,,,,,,,,,,,,20590,,,Fri Mar 25 17:49:14 UTC 2011,,,,,,0|i0gb13:,93217,,,,,,,,,,,"24/Mar/11 09:22;mishravivek;As per my analysis,
I can see that it is happening because StreamOutSession is not instantiated.
Which is done via ""create"" call in StreamOutSession. 

StreamReplyVerbHandler should create a new session, in case StreamOutSession instance is null?

Will this work as a solution?",24/Mar/11 09:27;mishravivek;Can you please provide more log or details on it?,24/Mar/11 18:30;brandon.williams;Patch to just log a warning when a stream command is received for an unknown session.,24/Mar/11 18:36;jbellis;What could cause unexpected stream commands?  Is there a deeper bug we should be looking for?,"24/Mar/11 18:42;bcoverston;In the case I was looking at there were also some failures in compaction.

This was seen after running a full repair on the node.

I'm not sure that they are related:

ERROR [CompactionExecutor:1] 2011-03-24 01:56:48,515 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.IndexOutOfBoundsException
        at java.nio.Buffer.checkIndex(Unknown Source)
        at java.nio.HeapByteBuffer.getInt(Unknown Source)
        at org.apache.cassandra.db.DeletedColumn.getLocalDeletionTime(DeletedColumn.java:57)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedStandard(ColumnFamilyStore.java:879)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedColumnsOnly(ColumnFamilyStore.java:866)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeleted(ColumnFamilyStore.java:857)
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:94)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:147)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doValidationCompaction(CompactionManager.java:822)
        at org.apache.cassandra.db.CompactionManager.access$800(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$6.call(CompactionManager.java:358)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
","24/Mar/11 18:42;brandon.williams;In this specific case, the machine was performing a repair and then restarted.  The other nodes then sent it session close commands, so really this is just a cosmetic problem.",24/Mar/11 18:46;jbellis;I think I'd rather see it at debug level if it's expected.  +1 otherwise,24/Mar/11 18:59;brandon.williams;Committed with the message moved to debug.,"24/Mar/11 19:15;hudson;Integrated in Cassandra-0.7 #406 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/406/])
    Log a message when a streaming action for an unkown session is received
instead of NPE.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-2377
","25/Mar/11 09:55;mishravivek;I was going through the changes made. I wonder what should be the value of reply.getSessionID for:


 StreamReply reply = StreamReply.serializer.deserialize(new DataInputStream(bufIn), message.getVersion());

Should we check for this, instead of 

if (session == null)
            {
                logger.warn(""Received stream action "" + reply.action + "" for an unknown session from "" + message.getFrom());
                return;
         }
 
As per log it happens for :

 case SESSION_FINISHED:


So changes should be something like this:

if (SESSION_FINISHED.equals(reply.action.))
            {
                logger.warn(""Received stream action "" + reply.action + "" for an unknown session from "" + message.getFrom());
                return;
         }


Idea is to save any additional static call on StreamOutSession.get(message.getFrom(), reply.sessionId), which results in object instantiation for new Pair<InetAddress, Long>(host, sessionId).


","25/Mar/11 17:49;brandon.williams;An unknown stream action could be received for any of them.  It's not hard to imagine a FILE_RETRY happening here, and all of them require a non-null session.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leveled compaction allows multiple simultaneous compaction Tasks,CASSANDRA-3087,12520318,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,26/Aug/11 22:54,12/Mar/19 14:13,13/Mar/19 22:26,31/Aug/11 16:32,1.0.0,,,,,,0,lcs,,,,"CASSANDRA-1608 attempts to restrict itself to one compaction task per CF (see discussion there for why this is necessary) by synchronizing LCS.getBackgroundTasks but this is not sufficient.  Consider this sequence of events:

1. getBackgroundTasks returns a Task for compacting some L0 sstables.  this Task is scheduled.
2. Another SSTable for this CF is flushed, so CompactionManager.submitBackground is called.  getBT is not currently in-progress so the synchronization does not stop another Task from being returned and scheduled.",,,,,,,,,,,,,,,,31/Aug/11 15:56;bcoverston;3087-v2.txt;https://issues.apache.org/jira/secure/attachment/12492469/3087-v2.txt,27/Aug/11 03:03;jbellis;3087.txt;https://issues.apache.org/jira/secure/attachment/12491870/3087.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-31 15:56:22.98,,,no_permission,,,,,,,,,,,,20964,,,Wed Aug 31 17:16:48 UTC 2011,,,,,,0|i0gfin:,93944,bcoverston,bcoverston,,,,,,,,,"27/Aug/11 03:03;jbellis;Fix makes getBT wait for the current Task in that CF, if any, before returning a new one.","31/Aug/11 15:56;bcoverston;Cleaned up a few of the imports.

+1",31/Aug/11 16:32;jbellis;committed,"31/Aug/11 17:16;hudson;Integrated in Cassandra #1061 (See [https://builds.apache.org/job/Cassandra/1061/])
    fix race that allowed multiple simultaneous leveled compaction tasks
patch by jbellis; reviewed by Ben Coverston for CASSANDRA-3087

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163688
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/LeveledCompactionTask.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent SchemaDisagreementException,CASSANDRA-3884,12542015,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,09/Feb/12 19:51,12/Mar/19 14:13,13/Mar/19 22:26,23/Feb/12 15:34,1.1.0,,,,,,0,,,,,"Set up a cluster of two nodes (on cassandra-1.1), create some keyspaces and column families, and then make several schema changes. Everything is being done through only one of the nodes.  About once every 10 times (on my setup) I get a SchemaDisagreementException when creating and dropping keyspaces. 

There is a dtest for this: schema_changes_test.py. If your environment behaves like mine, you might need to run it 10 times to get the error.",using ccm on ubuntu. ,,,,,,,,,,,,,,,23/Feb/12 13:46;xedin;CASSANDRA-3884-v2.patch;https://issues.apache.org/jira/secure/attachment/12515750/CASSANDRA-3884-v2.patch,22/Feb/12 17:19;xedin;CASSANDRA-3884.patch;https://issues.apache.org/jira/secure/attachment/12515617/CASSANDRA-3884.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-11 12:31:20.246,,,no_permission,,,,,,,,,,,,227302,,,Thu Feb 23 15:34:09 UTC 2012,,,,,,0|i0gpen:,95546,jbellis,jbellis,,,,,,,,,"10/Feb/12 17:40;tpatterson;I found another problem that might be related. When I drop and re-create a column family while that column family is being compacted, I sometimes see the following error in the logs: 
{code}
ERROR [NonPeriodicTasks:1] 2012-02-10 00:32:27,773 SSTableDeletingTask.java (line 76) Unable to delete /tmp/dtest-l4IQpd/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-73-Data.db (it will be removed on server restart; we'll also retry after GC)
 INFO [CompactionExecutor:4] 2012-02-10 00:32:27,774 CompactionTask.java (line 226) Compacted to [/tmp/dtest-l4IQpd/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-82-Data.db,].  94,406,208 to 11,542,817 (~12% of original) bytes for 302,584 keys at 1.436150MB/s.  Time: 7,665ms.
ERROR [NonPeriodicTasks:1] 2012-02-10 00:32:27,774 SSTableDeletingTask.java (line 76) Unable to delete /tmp/dtest-l4IQpd/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-69-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR [NonPeriodicTasks:1] 2012-02-10 00:32:27,778 SSTableDeletingTask.java (line 76) Unable to delete /tmp/dtest-l4IQpd/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-80-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR [NonPeriodicTasks:1] 2012-02-10 00:32:27,779 SSTableDeletingTask.java (line 76) Unable to delete /tmp/dtest-l4IQpd/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-79-Data.db (it will be removed on server restart; we'll also retry after GC)
{code}
The dtest failing_tests/change_schema_under_load.py illustrates the problem.","10/Feb/12 18:19;tpatterson;Here is another error that happens intermittently when running failing_tests/change_schema_under_load.py:
{code}
Error occured during compaction
java.util.concurrent.ExecutionException: java.io.IOError: java.io.FileNotFoundException: /tmp/dtest-cf_hbP/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-82-Data.db (No such file or directory)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:236)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1511)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1708)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:303)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOError: java.io.FileNotFoundException: /tmp/dtest-cf_hbP/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-82-Data.db (No such file or directory)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:60)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:821)
	at org.apache.cassandra.db.compaction.CompactionIterable.getScanners(CompactionIterable.java:73)
	at org.apache.cassandra.db.compaction.CompactionIterable.<init>(CompactionIterable.java:56)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:126)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:261)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
Caused by: java.io.FileNotFoundException: /tmp/dtest-cf_hbP/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-82-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:102)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:87)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:965)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:56)
	... 12 more
{code}",11/Feb/12 12:31;xedin;The first rule of the schema migration (which you have violated) is to make sure that cluster is reasonably quiet for KS/CF you do updates upon because different bad things can happen if you migrate under heavy load. Can you please attach to the ticket system.log from both nodes in the situation when you get SchemaDisagreementException?,"11/Feb/12 15:46;jbellis;bq. he first rule of the schema migration (which you have violated) is to make sure that cluster is reasonably quiet for KS/CF you do updates upon because different bad things can happen if you migrate under heavy load

That sounds buggy to me, the goal of CASSANDRA-1391 was to make it so you don't have to care about that kind of thing anymore.","11/Feb/12 17:58;xedin;bq. That sounds buggy to me, the goal of CASSANDRA-1391 was to make it so you don't have to care about that kind of thing anymore.

There is still no way to make migrations atomic so the same rule applies, we made possible to concurrent schema propagation with CASSANDRA-1391 so we don't need to worry about ordering (or time) of the changes but what happens when you update keyspace/column_family simultaneously doing heavy write/read is still unpredictable because that would require some sort of global lock while KSMetaData/CFMetaData are mutated.","11/Feb/12 20:29;jbellis;If you mean you can race inserts into the new CF, with the CF creation, then sure, there's no way we can make that work.  But all the other CFs should be fine.","11/Feb/12 20:37;xedin;Yes, this is what I mean and all existing CFs not involved in schema mutation are not touched by migration process.","20/Feb/12 16:53;slebresne;The test Tyler is talking about is there: https://github.com/riptano/cassandra-dtest/blob/master/schema_changes_test.py.

On my machine, the test fails every time on 1.1 with a schema version disagreement during the column family drop. The test can be simplified a bit (and still fail) so that all it does is creating a keyspace, creating a column family and then dropping that column family. There is no insertion going on at all. There is also more than a 1 second wait between the initial creation and the drop. Last but not least, that same test is working perfectly fine on 1.0. It *is* a regression of 1.1.",22/Feb/12 17:19;xedin;I have localized and fixed the problem which was that coordinator wasn't sending the mutations used to change schema but a snapshot of the schema itself after each of the migrations.,"23/Feb/12 13:24;slebresne;It seems to perfectly make sense to send only the new mutation rather the entire schema so I'm good with that and it does fix the distributed test. But I'm not sure I understand why. Since Migration.announce() was call after applying the mutation, the new changes just applied should be part of the entire schema read from SystemTable, shouldn't they? So why did we get a SchemaDisagreementException before?

Also a few other remarks/nits/questions:
* In MigrationHelper if withSchemaRecord is false the mutations will be null, and most function will return a list containing null. We should return an empty list instead or null (but in that last case, Migration.apply() should deal with null). Also MigrationHelper.dropColumnFamily() directly return null, so we should make it match whatever we do for the other method.
* It's slightly more efficient to use Collections.singleton() than Arrays.asList with one element.
* Why does the tests now need to start gossip?
","23/Feb/12 13:35;xedin;The answer to your main question is - compaction, everything works well when you add new or modify columns but when you e.g. delete cf columns from keyspace and compaction kicks in before you grabbed the whole schema that schema will be missing updates for that columns so they won't be pushed to the remote nodes leaving cf attributes in their schema_columnfamilies.

bq. In MigrationHelper if withSchemaRecord is false the mutations will be null, and most function will return a list containing null. We should return an empty list instead or null (but in that last case, Migration.apply() should deal with null). Also MigrationHelper.dropColumnFamily() directly return null, so we should make it match whatever we do for the other method

Sure, I will make it return Collections.singleton()

bq. It's slightly more efficient to use Collections.singleton() than Arrays.asList with one element.

Sure, will change it in updated v2.

bq. Why does the tests now need to start gossip?

I have experienced gossip related NPE exceptions (in isEnabled() method for example) in the MM.passiveAnnounce method when Gossiper wasn't started.",23/Feb/12 13:46;xedin;v2 with modifications mentioned by Sylvain.,"23/Feb/12 13:58;slebresne;bq. I have experienced gossip related NPE exceptions (in isEnabled() method for example) in the MM.passiveAnnounce method when Gossiper wasn't started.

Is that related to this patch? Otherwise I'd prefer leaving that to another ticket (at least a different commit).","23/Feb/12 14:11;slebresne;+1 with one nit: if I understand correctly Migration.applyImpl() should never return an empty list of RowMutation, so it would be nice to assert that fact in Migration.apply().

And as said above, I'd prefer separating the test changes to another commit (or open a ticket for them, but I can live with those change being committed directly).",23/Feb/12 15:34;xedin;Committed with fixed nit and changes to tests added to the separate commit.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh is missing cqlshlib (ImportError: No module named cqlshlib),CASSANDRA-3767,12539326,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,thepaul,mattman,mattman,22/Jan/12 01:21,12/Mar/19 14:13,13/Mar/19 22:26,22/Feb/12 10:41,1.0.8,,,Legacy/Tools,,,0,cqlsh,,,,"After a clean install of Cassandra, when running cqlsh I get the following error:

{code}>cqlsh
Traceback (most recent call last):
  File ""/usr/local/bin/cqlsh"", line 54, in <module>
    from cqlshlib import cqlhandling, pylexotron
ImportError: No module named cqlshlib
{code}
","Mac OS X 10.7.2 Snow Leopard
Cassandra 1.0.7
Installed using Homebrew",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-02-20 15:53:33.652,,,no_permission,,,,,,,,,,,,224828,,,Wed Feb 22 10:41:18 UTC 2012,,,,,,0|i0gnzb:,95315,slebresne,slebresne,,,,,,,,,"22/Jan/12 02:22;mattman;FYI, I managed to get cqlsh working by downloading http://svn.apache.org/repos/asf/cassandra/trunk/pylib/setup.py, and and cqlshlib from http://svn.apache.org/repos/asf/cassandra/trunk/pylib/cqlshlib/, and then running python install setup.py.

Just not sure why it's not installed by default and why I had to install the cqlshlib manually.

Matt",20/Feb/12 15:53;thepaul;Looks like the homebrew packaging neglects to ship all the necessary parts of cqlsh.,"20/Feb/12 22:11;thepaul;Submitted https://github.com/mxcl/homebrew/pull/10353 ; we'll see what they think about that.

Merge my cqlsh-in-cassandra branch at git@github.com:thepaul/homebrew.git into your Homebrew repo to get this fix earlier.","21/Feb/12 20:12;thepaul;Actually, the change seems like a useful one to merge back upstream here, although it's unnecessary for the other types of deployment we expect to see.

One-liner change is in my 3767 branch on github: https://github.com/thepaul/cassandra/commit/12cbb648f","22/Feb/12 10:41;slebresne;+1, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit Log segments are not recycled,CASSANDRA-3557,12533585,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,rbranson,rbranson,rbranson,02/Dec/11 09:19,12/Mar/19 14:13,13/Mar/19 22:26,02/Dec/11 23:30,1.1.0,,,,,,0,,,,,Cassandra never recycles segments created after recovery.,,,,,,,,,,,,,,,,02/Dec/11 15:57;rbranson;3557.txt;https://issues.apache.org/jira/secure/attachment/12505900/3557.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-02 22:38:04.641,,,no_permission,,,,,,,,,,,,219313,,,Sat Dec 03 01:06:09 UTC 2011,,,,,,0|i0gldj:,94893,jbellis,jbellis,,,,,,,,,02/Dec/11 22:38;jbellis;I'm still not a fan of creating a new Segment object on recycle.  Feels like in this case it makes things more fragile rather than less.  Otherwise lgtm.,"02/Dec/11 23:21;rbranson;There are a few reasons:

* There is some ""reset"" logic that would need to be duplicated from the constructor into the recycle() method. Resetting this mutable state seems fragile vs just discarding immutable state.
* Current code from the constructor for existing files works for both the case of recycling recovered segments and recycling discarded segments. Moving this out means we'd have to split this logic and duplicate it.
* Naming/renaming logic would have to be reworked and likely duplicated in some places, and the segment MUST be renamed for several reasons:
** ReplayPosition objects reference CommitLogSegment's by their id, which is stored in the filename
** CommitLog.recover explicitly replays segments in order of their id, and not giving them a fresh id would break this contract.
* If we re-use CommitLogSegment objects, leaked references could unintentionally operate on the wrong segment file. A CLS object could reference one segment at one point in time, and another at a future point in time. This seems dangerous to me. The patch as it sits rejects any mutations after the segment is closed and any modifications to the ""dirty"" table are meaningless.","02/Dec/11 23:30;jbellis;makes sense, committed","03/Dec/11 01:06;hudson;Integrated in Cassandra #1235 (See [https://builds.apache.org/job/Cassandra/1235/])
    fix commitlog segment recycling
patch by Rick Branson; reviewed by jbellis for CASSANDRA-3557

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1209779
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogAllocator.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java
* /cassandra/trunk/test/unit/org/apache/cassandra/SchemaLoader.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RemoveDeleted dominates compaction time for large sstable counts,CASSANDRA-3855,12541341,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,stuhood,stuhood,05/Feb/12 21:52,12/Mar/19 14:13,13/Mar/19 22:26,20/Jul/12 22:35,1.2.0 beta 1,,,,,,0,compaction,deletes,leveled,,"With very large numbers of sstables (2000+ generated by a `bin/stress -n 100,000,000` run with LeveledCompactionStrategy), PrecompactedRow.removeDeletedAndOldShards dominates compaction runtime, such that commenting it out takes compaction throughput from 200KB/s to 12MB/s.

Stack attached.",,,,,,,,,,,,,,,,20/Jul/12 15:25;yukim;3855.txt;https://issues.apache.org/jira/secure/attachment/12537361/3855.txt,05/Feb/12 21:52;stuhood;with-cleaning-java.hprof.txt;https://issues.apache.org/jira/secure/attachment/12513372/with-cleaning-java.hprof.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-06-14 15:38:37.942,,,no_permission,,,,,,,,,,,,226629,,,Fri Jul 20 23:19:03 UTC 2012,,,,,,0|i0gp1b:,95486,jbellis,jbellis,,,,,,,,,"14/Jun/12 15:38;jbellis;Yuki, can you benchmark trunk and see if this is still a bottleneck?  I think we want to bench both LCS and STCS.","15/Jun/12 18:16;slebresne;I'll precise that I try to do a quick test to see if I could reproduce ""back in the days"" but wasn't really able to reproduce something similar to the attached hprof log. I didn't wait up to 100,000,000 keys though.","19/Jul/12 22:59;yukim;I ran cpu profile on trunk and 1.1 with LCS and about 1000 sstables. On 1.1 branch, there is no indication of dominating removeDeletedAndOldShards. But for trunk, I noticed that it seemed unnecessary CompactionController#shouldPurge is called inside removeDeletedAndOldShards, where shouldPurge is supposed to be called only when CF has tombstones. So I looked up the code and I'm not sure if this line(https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/AbstractColumnContainer.java#L201) is correct. If CF is live, returning false for hasIrrelevantData seems right. Sylvain, what do you think?",19/Jul/12 23:13;jbellis;That's definitely wrong...  I think it should be {{if (info != LIVE) return false}},"20/Jul/12 08:31;slebresne;Agreed that it is wrong, but I think that it's more than the first line that is wrong. I think that method should be:
{noformat}
public boolean hasIrrelevantData(int gcBefore)
{
    if (deletionInfo().isLive())
        return false;

    // Do we have gcable deletion infos?
    if (!deletionInfo().purge(gcbefore).equals(deletionInfo()))
        return true;

    // Do we have colums that are either deleted by the container or gcable tombstone?
    for (IColumn column : columns)
        if (deletionInfo().isDeleteted(column) || column.hasIrrelevantData(gcBefore))
            return true;

    return false;
}
{noformat}","20/Jul/12 14:27;jbellis;We definitely don't want ""if row is live, nothing to do here"" behavior, otherwise we'll never purge column-level tombstones without a full row deletion.","20/Jul/12 14:37;slebresne;Your right, that's a broken optimization, the 2 first lines should be removed.",20/Jul/12 14:44;jbellis;+1 for proposed method w/ first 2 lines removed,20/Jul/12 15:25;yukim;So I summarized and attached patch. Tested on trunk and confirmed it fixed.,20/Jul/12 17:06;jbellis;+1,20/Jul/12 22:35;yukim;Committed to trunk.,"20/Jul/12 23:19;hudson;Integrated in Cassandra #1734 (See [https://builds.apache.org/job/Cassandra/1734/])
    fix incorrect hasIrrelevantData result for live CF; patch by yukim, reviewed by jbellis/slebresne for CASSANDRA-3855 (Revision d74103735126658d64cb92a16f4bb40f63d5e2e8)

     Result = ABORTED
yukim : 
Files : 
* src/java/org/apache/cassandra/db/AbstractColumnContainer.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Truncate shouldn't rethrow timeouts as UA,CASSANDRA-3651,12535793,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,20/Dec/11 07:02,12/Mar/19 14:13,13/Mar/19 22:26,22/Dec/11 16:41,1.1.0,,,,,,0,,,,,"Truncate is a very easy operation to timeout, but the timeouts rethrow as UnavailableException which is somewhat confusing.  Instead it should throw TimedOutException.",,,,,,,,,,,,,,,,22/Dec/11 16:07;brandon.williams;0001-Update-thrift-definition.txt;https://issues.apache.org/jira/secure/attachment/12508405/0001-Update-thrift-definition.txt,20/Dec/11 12:24;brandon.williams;0002-truncate-throws-TOE-on-timeout.txt;https://issues.apache.org/jira/secure/attachment/12508069/0002-truncate-throws-TOE-on-timeout.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-22 15:54:10.785,,,no_permission,,,,,,,,,,,,221476,,,Thu Dec 22 17:33:38 UTC 2011,,,,,,0|i0gmkv:,95088,jbellis,jbellis,,,,,,,,,22/Dec/11 15:54;jbellis;Doesn't updating the Thrift signature mean (Java) clients now have to catch or rethrow TOE where before they did not?  If so we should probably do this in 1.1.,22/Dec/11 16:02;brandon.williams;Yes. :(  Checked exceptions strike again.,22/Dec/11 16:26;jbellis;+1,22/Dec/11 16:41;brandon.williams;Committed.,"22/Dec/11 17:33;hudson;Integrated in Cassandra #1264 (See [https://builds.apache.org/job/Cassandra/1264/])
    Truncate throws TOE on timeout instead of UA.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-3651

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1222340
Files : 
* /cassandra/trunk/interface/cassandra.thrift
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/Constants.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch_mutate is broken for counters,CASSANDRA-2457,12504037,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,12/Apr/11 11:29,12/Mar/19 14:13,13/Mar/19 22:26,20/Apr/11 09:22,0.8.0 beta 2,,,,,,0,,,,,"CASSANDRA-2384 allowed for batch_mutate to take counter and non counter operation, but the code was not updated correctly to handle that case. As it is, the code will use the first mutation in the batch list to decide whether to apply the write code path of counter or not, and will thus break if those are mixed.",,14400,14400,,0%,14400,14400,,,,,,,,,18/Apr/11 15:37;slebresne;0001-Fix-batch_mutate-for-counters-v2.patch;https://issues.apache.org/jira/secure/attachment/12476619/0001-Fix-batch_mutate-for-counters-v2.patch,13/Apr/11 14:31;slebresne;0001-Fix-batch_mutate-for-counters.patch;https://issues.apache.org/jira/secure/attachment/12476243/0001-Fix-batch_mutate-for-counters.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-14 21:18:26.878,,,no_permission,,,,,,,,,,,,20634,,,Wed Apr 20 19:16:47 UTC 2011,,,,,,0|i0gbhj:,93291,stuhood,stuhood,,,,,,,,,"13/Apr/11 14:31;slebresne;Attaching patch against 0.8.

Basically this makes SP.mutate() handle a list of mixed RowMutation and CounterMutation (but then each mutation use the right path). This is needed since a batch_mutate() can now mix those.

This has an unfortunate consequence however in that I don't think there is a way to distinguish the writeStats of standard insert of the ones of counter insert anymore, and thus this patch remove the latter ones (and count everything in writeStat).","14/Apr/11 21:18;stuhood;* writeLocallyAndReplicate doesn't always perform a local mutation, so it should probably be renamed
* Since mutateCounter and writeLocallyAndReplicate are symmetrical and are called depending on whether an IMutation is an instance of CounterMutation, could we move them onto IMutation, and polymorphically decide the behavior?

> ...and thus this patch remove the latter ones (and count everything in writeStat).
I'm fine with this, since a counter is as ""real"" a write as any other. _But_ I do think we should record the latencies for the replicate-on-write stage like we do for the read and mutation stages on a per column family basis. I can tackle it in a separate ticket if you'd like.

PS: Thanks for removing this line!
{code}mutations.iterator().next().getColumnFamilies().iterator().next().metadata().getDefaultValidator().isCommutative(){code}

Thanks Sylvain!","18/Apr/11 15:44;slebresne;Attached rebased version (post-CASSANDRA-2454 in particular)

bq. writeLocallyAndReplicate doesn't always perform a local mutation, so it should probably be renamed

Renamed it to performWrite (since it mostly simply imply a so-called writePerformer).

bq. Since mutateCounter and writeLocallyAndReplicate are symmetrical and are called depending on whether an IMutation is an instance of CounterMutation, could we move them onto IMutation, and polymorphically decide the behavior?

Hum, they are not really so symmetrical. In particular writeLocallyAndReplicate (or performWrite as it is called nowadays) really is polymorphic over the IMutation used.
So the only seem we we could do (at least easily) is moving some of mutateCounter in CounterMutation, but not sure it will look so great. Also, it is probably nice to keep all the code related to the write/read protocol in StorageProxy (doing otherwise would be like moving the query code out of CFStore, nobody wants that :D)

bq. I'm fine with this, since a counter is as ""real"" a write as any other. But I do think we should record the latencies for the replicate-on-write stage like we do for the read and mutation stages on a per column family basis. I can tackle it in a separate ticket if you'd like.

Make sense, but I'm also in favor of moving this to some other ticket. ",20/Apr/11 03:48;stuhood;+1,"20/Apr/11 09:22;slebresne;Committed, thanks","20/Apr/11 10:21;hudson;Integrated in Cassandra #859 (See [https://hudson.apache.org/hudson/job/Cassandra/859/])
    merge CASSANDRA-2457 from 0.8
",20/Apr/11 19:16;stuhood;Thanks Sylvain. Opened CASSANDRA-2522,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkRecordWriter throws NPE for counter columns,CASSANDRA-3906,12542474,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,14/Feb/12 02:48,12/Mar/19 14:13,13/Mar/19 22:26,14/Feb/12 17:13,1.1.0,,,,,,0,,,,,"Using BulkRecordWriter, fails with counters due to an NPE (we used column instead of counter_column). I also noticed this broke for super columns too.",,,,,,,,,,,,,,,,14/Feb/12 02:50;lenn0x;0001-Fixed-BulkRecordWriter-to-not-throw-an-NPE-for-count.patch;https://issues.apache.org/jira/secure/attachment/12514436/0001-Fixed-BulkRecordWriter-to-not-throw-an-NPE-for-count.patch,14/Feb/12 14:13;brandon.williams;3906-v2.txt;https://issues.apache.org/jira/secure/attachment/12514491/3906-v2.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-14 14:13:13.665,,,no_permission,,,,,,,,,,,,227760,,,Tue Feb 14 17:05:45 UTC 2012,,,,,,0|i0gpnr:,95587,brandon.williams,brandon.williams,,,,,,,,,"14/Feb/12 14:13;brandon.williams;This patch doesn't look quite right, at the least the removal of the newSuperColumn call can't be right.  Attaching v2.",14/Feb/12 17:05;lenn0x;Good call on seeing that missing writer.newSuperColumn line. Oops. Looks good.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncompressed sizes are used to estimate space for compaction of compressed sstables,CASSANDRA-3338,12526414,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,10/Oct/11 08:30,12/Mar/19 14:13,13/Mar/19 22:26,10/Oct/11 14:00,1.0.0,,,,,,0,compression,,,,We are using the uncompressed data size when estimating if we have enough to compact sstables. This means we can easily refuse compaction when there is clearly enough room to compact.,,,,,,,,,,,,,,,,10/Oct/11 13:35;slebresne;3338-v2.patch;https://issues.apache.org/jira/secure/attachment/12498419/3338-v2.patch,10/Oct/11 08:42;slebresne;3338.patch;https://issues.apache.org/jira/secure/attachment/12498400/3338.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-10 12:59:13.302,,,no_permission,,,,,,,,,,,,55861,,,Mon Oct 10 14:00:31 UTC 2011,,,,,,0|i0gion:,94457,jbellis,jbellis,,,,,,,,,"10/Oct/11 12:59;jbellis;maybe we should rename .length() to uncompressedLength() as well?

otherwise +1","10/Oct/11 13:35;slebresne;Actually renaming length() to uncompressedLength() proved to be a good idea, as I had missed quite a bunch of places where the onDiskLength should be used. Attached v2.",10/Oct/11 13:46;jbellis;+1,"10/Oct/11 14:00;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scaling to large clusters in GossipStage impossible due to calculatePendingRanges ,CASSANDRA-3831,12540841,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,01/Feb/12 23:44,12/Mar/19 14:13,13/Mar/19 22:26,09/Feb/12 10:31,1.1.0,,,,,,0,,,,,"(most observations below are from 0.8, but I just now tested on
trunk and I can trigger this problem *just* by bootstrapping a ~180
nod cluster concurrently, presumably due to the number of nodes that
are simultaneously in bootstrap state)

It turns out that:

* (1) calculatePendingRanges is not just expensive, it's computationally complex - cubic or worse
* (2) it gets called *NOT* just once per node being bootstrapped/leaving etc, but is called repeatedly *while* nodes are in these states

As a result, clusters start exploding when you start reading 100-300
nodes. The GossipStage will get backed up because a single
calculdatePenginRanges takes seconds, and depending on what the
average heartbeat interval is in relation to this, this can lead to
*massive* cluster-wide flapping.

This all started because we hit this in production; several nodes
would start flapping several other nodes as down, with many nodes
seeing the entire cluster, or a large portion of it, as down. Logging
in to some of these nodes you would see that they would be constantly
flapping up/down for minutes at a time until one became lucky and it
stabilized.

In the end we had to perform an emergency full-cluster restart with
gossip patched to force-forget certain nodes in bootstrapping state.

I can't go into all details here from the post-mortem (just the
write-up would take a day), but in short:

* We graphed the number of hosts in the cluster that had more than 5
  Down (in a cluster that should have 0 down) on a minutely timeline.
* We also graphed the number of hosts in the cluster that had GossipStage backed up.
* The two graphs correlated *extremely* well
* jstack sampling showed it being CPU bound doing mostly sorting under calculatePendingRanges
* We were never able to exactly reproduce it with normal RING_DELAY and gossip intervals, even on a 184 node cluster (the production cluster is around 180).
* Dropping RING_DELAY and in particular dropping gossip interval to 10 ms instead of 1000 ms, we were able to observe all of the behavior we saw in production.

So our steps to reproduce are:

* Launch 184 node cluster w/ gossip interval at 10ms and RING_DELAY at 1 second.
* Do something like: {{while [ 1 ] ; do date ; echo decom ; nodetool decommission ; date ; echo done leaving decommed for a while ; sleep 3 ; date ; echo done restarting; sudo rm -rf /data/disk1/commitlog/* ; sudo rm -rf /data/diskarray/tables/* ; sudo monit restart cassandra ;date ; echo restarted waiting for a while ; sleep 40; done}} (or just do a manual decom/bootstrap once, it triggers every time)
* Watch all nodes flap massively and not recover at all, or maybe after a *long* time.

I observed the flapping using a python script that every 5 second
(randomly spread out) asked for unreachable nodes from *all* nodes in
the cluster, and printed any nodes and their counts when they had
unreachables > 5. The cluster can be observed instantly going into
massive flapping when leaving/bootstrap is initiated. Script needs
Cassandra running with Jolokia enabled for http/json access to
JMX. Can provide scrit if needed after cleanup.

The phi conviction, based on logging I added, was legitimate. Using
the 10 ms interval the average heartbeat interval ends up being like 25
ms or something like that. As a result, a single ~ 2 second delay in
gossip stage is huge in comparison to those 25 ms, and so we go past
the phi conviction threshold. This is much more sensitive than in
production, but it's the *same* effect, even if it triggers less
easily for real.

The best work around currently internally is to memoize
calculatePendingRanges so that we don't re-calculate if token meta
data, list of moving, list of bootstrapping and list of leaving are
all the same as on prior calculation. It's not entirely clear at this
point whether there is a clean fix to avoid executing
calculatePendingRanges more than once per unique node in this state.

It should be noted though that even if that is fixed, it is not
acceptable to spend several seconds doing these calculations on a ~
200 node cluster and it needs to be made fundamentally more efficient.

Here is a dump of thoughts by me in an internal JIRA ticket (not
exhaustive, I just went as far as to show that there is an issue;
there might be worse things I missed, but worse than cubic is bad
enough that I stopped):

(Comment uses 0.8 source.)

{quote}
Okay, so let's break down the computational complexity here.

Suppose ring size is {{n}} and number of bootstrapping/leaving tokens is {{m}}.  One of two places that take time (by measurement) is this part of calculatePendingRanges():

{code}
       // At this stage pendingRanges has been updated according to leave operations. We can
        // now continue the calculation by checking bootstrapping nodes.

        // For each of the bootstrapping nodes, simply add and remove them one by one to
        // allLeftMetadata and check in between what their ranges would be.
        for (Map.Entry<Token, InetAddress> entry : bootstrapTokens.entrySet())
        {
            InetAddress endpoint = entry.getValue();

            allLeftMetadata.updateNormalToken(entry.getKey(), endpoint);
            for (Range range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))
                pendingRanges.put(range, endpoint);
            allLeftMetadata.removeEndpoint(endpoint);
        }
{code}

I'll ignore stuff that's log(n) or better.

The outer loops is {{O(m)}}. The inner loop is {{O(n)}}, making aggregate so far {{O(nm)}}.

We have a call in there to updateNormalTokens() which implies a sorting, which his {{O(n log(n))}}. So now we're at {{O(n log(n) m)}}.

Next up we call {{getAddressRanges()}} which immediately does another {{O(n log(n)}} sort. we're still at {{O(n log(n) m}}. It then iterates (linear) and:

* calls {{getPrimaryRangeFor()}} for each.
* calls {{calculateNaturalEndpoints}} for each.

The former ends up sorting again, so now we're at {{O(n log(n) n log(n) m}} (worse than quadratic).

{{NTS.calculateNaturalEndpoints}} starts by collecting token meta data for nodes in the DC, by using {{updateNormalToken}}, which *implies sorting*. Woha woha. Now we're at {{O(n log(n) n log (n) n log(n) m)}}.

I might have missed things that are even worse, but this is bad enough to warrant this ticket. To put into perspective, 168 ^ 3 is 4.7 million.
{quote}
",,,,,,,,,,,,CASSANDRA-3856,,,,04/Feb/12 01:24;scode;CASSANDRA-3831-memoization-not-for-inclusion.txt;https://issues.apache.org/jira/secure/attachment/12513221/CASSANDRA-3831-memoization-not-for-inclusion.txt,06/Feb/12 00:36;scode;CASSANDRA-3831-trunk-group-add-dc-tokens.txt;https://issues.apache.org/jira/secure/attachment/12513381/CASSANDRA-3831-trunk-group-add-dc-tokens.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-04 15:12:49.118,,,no_permission,,,,,,,,,,,,226195,,,Thu Feb 09 10:31:55 UTC 2012,,,,,,0|i0goqf:,95437,,,,,,,,,,,"02/Feb/12 02:08;scode;Correction: All above applies, except on 'trunk': I have not confirmed that calculatePendingTokens() indeed gets called repeatedly (not just once per node that starts a bootstrap/leave/etc) on trunk. I have only confirmed it being CPU spinning there, and gossip stage being backed up as a result.

On 0.8, it is specifically confirmed that it does get called repeatedly both in my test case and when we saw this happen in production.","04/Feb/12 01:24;scode;I am attaching {{CASSANDRA\-3831\-memoization\-not\-for\-inclusion.txt}} as an ""FYI"" and in case it helps others. It's against 0.8, and implements memoization of calculate pending ranges.

The correct/clean fix is probably to change behavior so that it doesn't get called unnecessarily to begin with (and to make sure the computational complexity is reasonable when it does get called). This patch was made specifically to address the production issue we are having in a minimally dangerous fashion, and is not to be taken as a suggested fix.","04/Feb/12 15:12;jbellis;calculatePendingRanges is only supposed to be called when the ring changes.  So I'd say the right fix would be to eliminate whatever is breaking that design, rather than adding a memoization bandaid.

(I eyeballed 1.1 and didn't see anything obvious, so either it's subtle or it got fixed post-0.8.)

I don't suppose your CPU spinning test got any more of a call tree to go on?","04/Feb/12 19:56;scode;I agree (I did say that myself already ;)). The memoization (+ being ready to change the cluster-wide phi convict threshold through JMX) was just the safest way to fix the situation on our production cluster so that we could continue to add capacity. It was never intended as a suggested fix. But I still wanted to upload it instead of keeping the patch private, in case someone's helped by it.

But the larger issue is that calculatePendingRanges must be faster to begin with. Even if only called once, if it takes 1-4 seconds on a ~ 180 node cluster and it's worse than {{O(n^3)}} it's *way* too slow and won't scale. First due to the failure detector, and of course at some point it's just too slow to even wait for the calculation to complete at all (from a RING_DELAY standpoint for example).

I'll see later this weekend about doing more tests on trunk confirm/deny whether it is getting called multiple times. As I indicated I never confirmed that particular bit on trunk and it's very possible it doesn't happen there.

I haven't had time to seriously look at suggesting changes to fix the computational complexity. Might be very easy for all I know; I just haven't looked at it yet.
","05/Feb/12 23:45;scode;With respect to trunk and whether it gets called repeatedly, I gave that a try now. I picked a random node to tail (and running a version that logs calculate pending ranges and its time), and did a decommission of another node:

{code}
 INFO [GossipStage:1] 2012-02-05 23:33:32,675 StorageService.java (line 1275) calculate pending ranges called, took 885 ms
 INFO [OptionalTasks:1] 2012-02-05 23:34:00,828 HintedHandOffManager.java (line 180) Deleting any stored hints for /XXX.XXX.XXX.XXX
 INFO [GossipStage:1] 2012-02-05 23:34:00,828 StorageService.java (line 1275) calculate pending ranges called, took 0 ms
 INFO [GossipStage:1] 2012-02-05 23:34:00,829 StorageService.java (line 1275) calculate pending ranges called, took 0 ms
 INFO [GossipStage:1] 2012-02-05 23:34:00,829 StorageService.java (line 1217) Removing token 23117008622346363007022731483136427400 for /XXX.XXX.XXX.XXX
{code}

At least two of those are expected - once when it goes into leaving, and once when it drops out of the cluster. Not sure at this point why we see a third call.

For bootstrapping the guy back into the cluster I see:

{code}
 INFO [GossipStage:1] 2012-02-05 23:38:15,832 StorageService.java (line 1275) calculate pending ranges called, took 1413 ms
 INFO [GossipStage:1] 2012-02-05 23:38:45,229 ColumnFamilyStore.java (line 590) Enqueuing flush of Memtable-LocationInfo@659873291(35/43 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-02-05 23:38:45,229 Memtable.java (line 252) Writing Memtable-LocationInfo@659873291(35/43 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-02-05 23:38:45,236 Memtable.java (line 293) Completed flushing /data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-20-Data.db (89 bytes)
 INFO [GossipStage:1] 2012-02-05 23:38:45,236 StorageService.java (line 1275) calculate pending ranges called, took 0 ms
 INFO [CompactionExecutor:21] 2012-02-05 23:38:45,237 CompactionTask.java (line 115) Compacting [SSTableReader(path='/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-20-Data.db'), SSTableReader(path='/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-19-Data.db'), SSTableReader(path='/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-18-Data.db'), SSTableReader(path='/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-17-Data.db')]
 INFO [CompactionExecutor:21] 2012-02-05 23:38:45,248 CompactionTask.java (line 226) Compacted to [/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-21-Data.db,].  7,388 to 7,047 (~95% of original) bytes for 4 keys at 0.610958MB/s.  Time: 11ms.
{code}

So it got called twice - which is expected. Once when it entered in joining state, and once when it flipped into Normal.

Relatedly:

On the other hand, on the node that is *bootstrapping*, I (and this is expected) have calculate pending ranges calls lots of times - presumably (not confirmed) at least once for every node in the cluster, as the gossiper emits events to inform it of each. If there is a single node bootstrapping this is kind of okay because the lack of ""another"" guy bootstrapping means the calculations are quick. But if other nodes are bootstrapping too (highly likely if you're doing capacity adds on large clusters) that would be expected to take a long time to process. This could throw off the node bootstrapping which tries to wait for RING_DELAY on start-up, but is spending a lot of that time doing these calculations rather than staying up-to-date with ring information (for the record though I have not specifically timed/tested this particular case).
","06/Feb/12 00:36;scode;Attaching {{CASSANDRA\-3831\-trunk\-group\-add\-dc\-tokens.txt}} which adds a ""group-update"" interface to TokenMetadata that allows NTS to use it when constructing it's local per-dc meta datas.

This is not even close to a complete fix for this issue, but I do think it is a ""clean"" change because it makes sense in terms of TokenMetadata API to provide a group-update method given the expense involved. And given it's existence, it makes sense for NTS to use it.

This change mitigates the problem significantly on the ~ 180 node test cluster since it takes a way an {{n}} from the complexity, and should significantly raise the bar of how many nodes in a cluster is realistic without other changes.

I think this might be a fix worthwhile committing because it feels safe and is maybe a candidate for the 1.1 release, assuming review doesn't yield anything obvious. But, leaving the JIRA open for a more overarching fix (I'm not sure what that is at the moment; I'm mulling it over).

","06/Feb/12 00:56;scode;I filed CASSANDRA-3856 which might related to a proper fix to this.
","07/Feb/12 19:08;jbellis;committed the group add patch with a minor tweak to move the isempty check to the top so we can skip lock/unlock too, and a more important one from {{sortedTokens = sortedTokens()}} (which is a no-op) to {{sortedTokens = sortTokens()}}.","08/Feb/12 03:29;scode;Wow, that's embarrassing. Thanks for catching that!",09/Feb/12 10:11;slebresne;Since code has been committed so can we close this one and open a separate ticket for the remaining work for the sake of keeping track of what went into 1.1.0 and was doesn't?,"09/Feb/12 10:31;scode;CASSANDRA-3881 filed for further work.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper.addSavedEndpoint should never add itself,CASSANDRA-3485,12531187,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,11/Nov/11 22:22,12/Mar/19 14:13,13/Mar/19 22:26,09/Dec/11 21:20,0.8.9,1.0.6,,,,,0,,,,,"Somehow, people are running into a situation where nodes are adding themselves to the persisted ring cache.  Since SS is initialized after the Gossiper and calls addSavedEndpoint on it, which inits the nodes with a generation of zero, this ends up with nodes using a generation of zero and thus never being marked as alive.",,,,,,,,,,,,CASSANDRA-1518,,,,13/Nov/11 21:38;brandon.williams;3485.txt;https://issues.apache.org/jira/secure/attachment/12503554/3485.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-09 20:20:55.592,,,no_permission,,,,,,,,,,,,216924,,,Sat Dec 10 11:32:26 UTC 2011,,,,,,0|i0gkhj:,94749,thepaul,thepaul,,,,,,,,,11/Nov/11 22:26;brandon.williams;Note that you can get around this with -Dcassandra.load_ring_state=false,"13/Nov/11 21:38;brandon.williams;Patch to prevent adding our own endpoint to saved endpoints, and if detected, remove it.",09/Dec/11 20:20;thepaul;+1.,09/Dec/11 21:20;brandon.williams;Committed.,"10/Dec/11 11:32;hudson;Integrated in Cassandra-0.8 #416 (See [https://builds.apache.org/job/Cassandra-0.8/416/])
    Prevent gossiper from adding itself to saved endpoints.
Patch by brandonwilliams reviewed by Paul Cannon for CASSANDRA-3485.

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1212624
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
estimated row sizes regression,CASSANDRA-3451,12530100,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,03/Nov/11 19:15,12/Mar/19 14:13,13/Mar/19 22:26,04/Nov/11 08:17,1.0.2,,,,,,0,,,,,"CASSANDRA-2753 broke the histogram collection; it got the histogram for column count (which can go up to 2B) switched with the one for row size in bytes (which goes up to ~1.5PB).  So any row over 2GB, will break things.",,,,,,,,,,,,,,,,04/Nov/11 00:12;jbellis;3451.txt;https://issues.apache.org/jira/secure/attachment/12502242/3451.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-04 07:01:23.764,,,no_permission,,,,,,,,,,,,215959,,,Fri Nov 04 08:17:46 UTC 2011,,,,,,0|i0gk2f:,94681,slebresne,slebresne,,,,,,,,,04/Nov/11 00:04;jbellis;patch fixes the regression and also does a minor rename for clarity on the methods involved.,"04/Nov/11 07:01;springrider;""which can go up to 2B"" should be 2GB?","04/Nov/11 08:17;slebresne;+1
Committed, but I've renamed defaultColumnSizeHistogram to defaultColumnCountHistogram for coherence with the rest of the code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader fails,CASSANDRA-3438,12529656,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,jeremypinkham,jeremypinkham,01/Nov/11 13:35,12/Mar/19 14:13,13/Mar/19 22:26,04/Nov/11 07:53,1.0.2,,,,,,0,,,,,"Ticket at the request of driftx in IRC.  

I've attached the files I'm attempting to load (this is fabricated test data for 100 keys).  I generated this using SSTableSimpleUnsortedWriter. I have four dedicated nodes (mine, not ec2 or other cloud host) that I'm using, we'll call them A,B,C, and D.  I first start cassandra on node A and create the test keyspace and CF:

{code}
create keyspace test
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = {replication_factor : 1}
  and durable_writes = true;

use test;

create column family test
  with column_type = 'Super'
  and comparator = 'UTF8Type'
  and subcomparator = 'UTF8Type'
  and default_validation_class = 'UTF8Type'
  and key_validation_class = 'UTF8Type'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and row_cache_keys_to_save = 2147483647
  and keys_cached = 200000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider'
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy';
{code}

Then I perform a load using sstableloader from node D, which works fine with the following output:

{code}
Starting client (and waiting 30 seconds for gossip) ...
 INFO 09:12:17,660 Loading settings from file:/opt/apache-cassandra-1.0.1/conf/cassandra.yaml
 INFO 09:12:17,761 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:12:17,771 Global memtable threshold is enabled at 75MB
 INFO 09:12:17,917 Starting up client gossip
 INFO 09:12:17,934 Starting Messaging Service on port 7000
 INFO 09:12:19,941 Node /172.21.31.244 is now part of the cluster
 INFO 09:12:19,941 InetAddress /172.21.31.244 is now UP
 INFO 09:12:48,003 Opening test/test-h-1 (2884 bytes)
 INFO 09:12:48,017 JNA not found. Native methods will be disabled.
Streaming revelant part of test/test-h-1-Data.db to [/172.21.31.244]
 INFO 09:12:48,052 Stream context metadata [test/test-h-1-Data.db sections=1 progress=0/2884 - 0%], 1 sstables.
 INFO 09:12:48,053 Streaming to /172.21.31.244

progress: [/172.21.31.244 0/1 (0)] [total: 0 - 0MB/s (avg: 0MB/s)] INFO 09:12:48,103 Shutting down MessageService...
 INFO 09:12:48,103 Waiting for in-progress requests to complete
 INFO 09:12:48,104 MessagingService shutting down server thread.
{code}

Then I start over by shutting cassandra, deleting all of the data and commitlog dirs, starting cassandra on Node A and Node B and creating the same keyspace and CF.  When I run the loader against that, I get:

{code}
Starting client (and waiting 30 seconds for gossip) ...
 INFO 09:15:09,316 Loading settings from file:/opt/apache-cassandra-1.0.1/conf/cassandra.yaml
 INFO 09:15:09,417 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:15:09,427 Global memtable threshold is enabled at 75MB
 INFO 09:15:09,572 Starting up client gossip
 INFO 09:15:09,591 Starting Messaging Service on port 7000
 INFO 09:15:10,777 Node /172.21.31.244 is now part of the cluster
 INFO 09:15:10,778 InetAddress /172.21.31.244 is now UP
 INFO 09:15:10,780 Node /172.21.31.245 is now part of the cluster
 INFO 09:15:10,781 InetAddress /172.21.31.245 is now UP
 INFO 09:15:39,664 Opening test/test-h-1 (2884 bytes)
 INFO 09:15:39,691 JNA not found. Native methods will be disabled.
Streaming revelant part of test/test-h-1-Data.db to [/172.21.31.244, /172.21.31.245]
 INFO 09:15:39,730 Stream context metadata [test/test-h-1-Data.db sections=1 progress=0/274 - 0%], 1 sstables.
 INFO 09:15:39,731 Streaming to /172.21.31.244
 INFO 09:15:39,743 Stream context metadata [test/test-h-1-Data.db sections=2 progress=0/2610 - 0%], 1 sstables.
 INFO 09:15:39,743 Streaming to /172.21.31.245

progress: [/172.21.31.244 0/1 (0)] [/172.21.31.245 0/1 (0)] [total: 0 - 0MB/s (avg: 0MB/s)]Exception in thread ""MiscStage:1"" java.lang.AssertionError: Reference counter -1 for test/test-h-1-Data.db
	at org.apache.cassandra.io.sstable.SSTableReader.releaseReference(SSTableReader.java:715)
	at org.apache.cassandra.streaming.StreamOutSession.startNext(StreamOutSession.java:123)
	at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:59)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
Exception in thread ""MiscStage:2"" java.lang.AssertionError: Reference counter -2 for test/test-h-1-Data.db
	at org.apache.cassandra.io.sstable.SSTableReader.releaseReference(SSTableReader.java:715)
	at org.apache.cassandra.streaming.StreamOutSession.close(StreamOutSession.java:150)
	at org.apache.cassandra.streaming.StreamOutSession.close(StreamOutSession.java:132)
	at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:67)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
progress: [/172.21.31.244 1/1 (100)] [/172.21.31.245 1/1 (100)] [total: 100 - 0MB/s (avg: 0MB/s)]
Waiting for targets to rebuild indexes ...
{code}

After that, it never exists.  There are on errors in the logs on the server side for either node.  Additional tests with larger inputs that show the same general error show slightly different behavior, specifically the progress on all but the first node gets past 1/N.  For example, this is the last line on a test of a real data set that had 16 sstables: ""progress: [/172.21.31.244 16/16 (100)] [/172.21.31.245 1/16 (6)] [total: 19 - 0MB/s (avg: 3MB/s)]]""... and it never progresses from there, and avg drops to zero over time indicating nothing is happening.  

I haven't replicated on any previous versions. ",,,,,,,,,,,,,,,,03/Nov/11 16:21;slebresne;3438.patch;https://issues.apache.org/jira/secure/attachment/12502173/3438.patch,01/Nov/11 13:36;jeremypinkham;test-h-1-Data.db;https://issues.apache.org/jira/secure/attachment/12501769/test-h-1-Data.db,01/Nov/11 13:36;jeremypinkham;test-h-1-Index.db;https://issues.apache.org/jira/secure/attachment/12501770/test-h-1-Index.db,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-11-03 16:21:06.691,,,no_permission,,,,,,,,,,,,215515,,,Fri Nov 04 07:53:55 UTC 2011,,,,,,0|i0gjwn:,94655,jbellis,jbellis,,,,,,,,,03/Nov/11 16:21;slebresne;The sstableloader was just forgetting to acquire references. Patch attached to fix (the patch also attach a very minor cosmetic fix for the progress bar always looks like it did completed the transfer).,03/Nov/11 17:33;jbellis;is this something we could add a unit test for?,03/Nov/11 20:57;slebresne;Probably but painfully as everything that has to do with streaming. But I can have a look. In the meantime I would prefer committing this to be sure it makes 1.0.2 as I'm sure that not acquire references was bad.,04/Nov/11 00:21;jbellis;+1 then,04/Nov/11 07:53;slebresne;Committed (writing the unit test is on my todo list :)),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossip should handle 'dead' states,CASSANDRA-2496,12504582,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,18/Apr/11 18:57,12/Mar/19 14:13,13/Mar/19 22:26,25/Jul/11 18:58,0.8.3,,,,,,1,,,,,"For background, see CASSANDRA-2371",,,,,,,,,,,,,,,,01/Jun/11 01:39;brandon.williams;0001-Rework-token-removal-process.txt;https://issues.apache.org/jira/secure/attachment/12481028/0001-Rework-token-removal-process.txt,01/Jun/11 00:28;brandon.williams;0002-add-2115-back.txt;https://issues.apache.org/jira/secure/attachment/12481019/0002-add-2115-back.txt,14/Jul/11 20:54;thepaul;0003-update-gossip-related-comments.patch.txt;https://issues.apache.org/jira/secure/attachment/12486503/0003-update-gossip-related-comments.patch.txt,14/Jul/11 20:56;thepaul;0004-do-REMOVING_TOKEN-REMOVED_TOKEN.patch.txt;https://issues.apache.org/jira/secure/attachment/12486504/0004-do-REMOVING_TOKEN-REMOVED_TOKEN.patch.txt,14/Jul/11 20:57;thepaul;0005-drain-self-if-removetoken-d-elsewhere.patch.txt;https://issues.apache.org/jira/secure/attachment/12486505/0005-drain-self-if-removetoken-d-elsewhere.patch.txt,21/Jul/11 20:09;thepaul;0006-acknowledge-unexpected-repl-fins.patch.txt;https://issues.apache.org/jira/secure/attachment/12487346/0006-acknowledge-unexpected-repl-fins.patch.txt,23/Jul/11 00:20;brandon.williams;0007-Always-update-epstate-timestamps-when-the-node-is-al.patch;https://issues.apache.org/jira/secure/attachment/12487575/0007-Always-update-epstate-timestamps-when-the-node-is-al.patch,23/Jul/11 00:20;brandon.williams;0008-only-handleStateRemoving-if-the-node-is-a-member.patch;https://issues.apache.org/jira/secure/attachment/12487576/0008-only-handleStateRemoving-if-the-node-is-a-member.patch,,,,8.0,,,,,,,,,,,,,,,,,,,2011-07-14 20:54:34.916,,,no_permission,,,,,,,,,,,,20655,,,Mon Jul 25 19:43:56 UTC 2011,,,,,,0|i0anvr:,60142,thepaul,thepaul,,,,,,,,,"01/Jun/11 00:08;brandon.williams;The first patch allows gossip to track dead states and completely changes how removetoken works, since it is the problem with keeping dead state around, and currently broken in many scenarios.  The second patch adds the previously reverted CASSANDRA-2115 back now that removetoken is more resilient.","01/Jun/11 16:02;brandon.williams;Some explanation of what changed and why it was necessary:

Consider nodes A through D. D is partitioned, and C is dead and needs to be removed. A removetoken will be issued to A for this.

In the current way we do things, A will modify it's own state by appending information to its status indicating that it will be removing C. B will see this, re-replicate as needed, then report to A that is is done. The problem however, is that since A modified its own state, A is also free to wipe that state out, either by restarting, or simple remove another token, because there's only space for one. If A reboots and then D's partition heals, D will never know C was removed. Worse, it will still have state for C that neither A nor B do, and so it will repopulate the ring with C again.

This patch changes this by instead having A sleep for RING_DELAY to make sure the generation for C is stable, and then it modifies C's state to indicate it is being removed, just as if C itself had done this. It also appends some extra state to indicate that A will be the removal coordinator. The others nodes see this, re-replicate and report back to A, which then modifies C's state once more to indicate it is completely removed. At this point, it doesn't matter if A dies completely and D's partition heals, since the state is stored in C's gossip information. If A reboots, it will be able to get the correct state information from B, or any other node.

If A fails while the other nodes are re-replicating, a new removetoken can be started elsewhere, or in the case of other replicas being down preventing removetoken from completing, a removetoken force will remove the node and then repair can be run to restore the replica count.","14/Jul/11 17:40;brandon.williams;I see two more things to be done with this patch.  First, when re-replicating nodes report back to the removal coordinator, if the coordinator has restarted it won't understand them, and they will infinitely loop retrying the confirmation.  Second, since we're holding dead states, we need to make sure that bootstrapping/moving nodes can take over these dead tokens.","14/Jul/11 20:54;thepaul;These small patches build on the others.

0003-update-gossip-related-comments.patch.txt: updates gossip-related comments derp derp.","14/Jul/11 20:56;thepaul;0004-do-REMOVING_TOKEN-REMOVED_TOKEN.patch.txt: use REMOVED_TOKEN instead of STATUS_LEFT (would probably be ok either way, but otherwise, the REMOVED_TOKEN state would not be used). Seems this is more the way it was intended.","14/Jul/11 20:57;thepaul;0005-drain-self-if-removetoken-d-elsewhere.patch.txt : when node X was partitioned and removetoken'd but then it shows up again, it should shut itself down, rather than becoming a zombie","14/Jul/11 20:58;thepaul;I'll see what I can do to test the ""infinitely loop retrying the confirmation"" and ""bootstrapping/moving nodes can take over these dead tokens"" situations.","20/Jul/11 23:48;thepaul;Ok, nodes do indeed infinitely retry the replication confirmation in some cases, but it appears it's not just when the former removal coordinator has restarted in the interim- it seems to be when the removetoken is reissued to another, new removal coordinator. In this case, I get this traceback every 10 seconds:

{noformat}
ERROR [MiscStage:9] 2011-07-20 23:42:06,599 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[MiscStage:9,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.StorageService.confirmReplication(StorageService.java:2088)
        at org.apache.cassandra.streaming.ReplicationFinishedVerbHandler.doVerb(ReplicationFinishedVerbHandler.java:38)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

I'll look into this.

Second, it seems that moving/joining nodes can take over the removed token fine, once the removetoken is complete. I haven't tried having a node take over the removed token while the removal is ongoing- I assume we can just document that that probably isn't a great idea?","21/Jul/11 19:50;thepaul;0006-acknowledge-unexpected-repl-fins.patch.txt: don't assert and drop the message when we see an unexpected REPLICATION_FINISHED. Ack it instead, so the sender doesn't continually retry.",21/Jul/11 20:09;thepaul;0006-acknowledge-unexpected-repl-fins.patch.txt (updated): also log at info when acknowledging the unexpected messages,"21/Jul/11 20:52;thepaul;ok, +1 with these patches.","23/Jul/11 00:20;brandon.williams;0007 handles problems when a node has been down longer than aVeryLongTime, and ensures that we advertise the new token states long enough.

0008 makes sure that SS only get involved with removal if the token is a member.",25/Jul/11 18:33;thepaul;+1,25/Jul/11 18:58;brandon.williams;Committed.,"25/Jul/11 19:43;hudson;Integrated in Cassandra-0.8 #238 (See [https://builds.apache.org/job/Cassandra-0.8/238/])
    Gossip handles dead states, token removal actually works, gossip states
are held for aVeryLongTime.
Patch by brandonwilliams and Paul Cannon, reviewed by Paul Cannon for
CASSANDRA-2496.

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1150847
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/VersionedValue.java
* /cassandra/branches/cassandra-0.8/NEWS.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/HeartBeatState.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/ApplicationState.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkOutputFormat shouldn't need flags to specify the output type,CASSANDRA-3828,12540780,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,01/Feb/12 21:13,12/Mar/19 14:13,13/Mar/19 22:26,03/Feb/12 00:09,1.1.0,,,,,,0,,,,,"BOF currently requires the IS_SUPER boolean to be set to determine if the output CF is going to be a super or not, and would similarly use a flag to indicate counters (if there was support for that yet.)  Instead, it should be able to introspect the mutations to determine what kind of columns to write.",,,,,,,,,,,,,,,,02/Feb/12 23:45;brandon.williams;3828-v2.txt;https://issues.apache.org/jira/secure/attachment/12513068/3828-v2.txt,02/Feb/12 22:28;brandon.williams;3828.txt;https://issues.apache.org/jira/secure/attachment/12513049/3828.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-02 22:56:13.293,,,no_permission,,,,,,,,,,,,226175,,,Fri Feb 03 00:09:30 UTC 2012,,,,,,0|i0gopb:,95432,jbellis,jbellis,,,,,,,,,02/Feb/12 22:28;brandon.williams;Patch to determine the CF type once by the very first mutation.,02/Feb/12 22:56;jbellis;It looks like it would be cleaner to split it into container type (super/normal) and data type (counter/normal).,02/Feb/12 23:45;brandon.williams;v2 splits by super/normal and counter/normal.,03/Feb/12 00:05;jbellis;+1,03/Feb/12 00:09;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenBitSet can allocate more bytes than it needs,CASSANDRA-3618,12534885,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,13/Dec/11 00:52,12/Mar/19 14:13,13/Mar/19 22:26,14/Dec/11 15:43,1.0.7,,,,,,0,,,,,"CASSANDRA-2466 changed OpenBitSet to break big long arrays into pages. However, it always allocate full pages, each page being of size 4096 * 8 bytes. This means that we almost always allocate too much bytes, and for a row that has 1 column, the associated row bloom filter allocates 32760 more bytes than it should.

This has a significant impact on performance. In a small test using the SSTableSimpleUnsortedWriter to generate rows with 1 column, 0.8 is about twice as fast as 1.0 because of that (the difference shrink when there is more columns obviously).",,,,,,,,,,,,,,,,13/Dec/11 00:56;slebresne;0001-Fix-openBitSet.patch;https://issues.apache.org/jira/secure/attachment/12507105/0001-Fix-openBitSet.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-13 16:39:07.957,,,no_permission,,,,,,,,,,,,220569,,,Tue Dec 13 16:39:07 UTC 2011,,,,,,0|i0gm4f:,95014,jbellis,jbellis,,,,,,,,,"13/Dec/11 00:56;slebresne;Attached patch to avoid the over-allocation.

Note that it would probably be cleaner to reuse BigLongArray in OpenBitSet to deal with the ""paging"" in only one place, but that would involve far more changes in OpenBitSet than I'm confortable doing in 1.0.","13/Dec/11 16:39;jbellis;Committed.

(This affects 1.0.1+, introduced by CASSANDRA-2466.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction cleanupIfNecessary costly when many files in data dir,CASSANDRA-3532,12532766,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,eparusel,eparusel,eparusel,25/Nov/11 22:58,12/Mar/19 14:13,13/Mar/19 22:26,02/Dec/11 21:01,1.0.6,,,,,,1,compaction,,,,"From what I can tell SSTableWriter.cleanupIfNecessary seems increasingly costly as the number of files in the data dir increases.
It calls SSTable.componentsFor(descriptor, Descriptor.TempState.TEMP) which lists all files in the data dir to find matching components.

Am I roughly correct that   (cleanupCost = SSTable count * data dir size)?


We had been doing write load testing with default compaction throttling (16MB/s) and LeveledCompaction.
Unfortunately we haven't been keeping tabs on sstable counts and it grew out of control.

On a system with 300,000 sstables (!) here is an example of our compaction rate.  Note that as you're probably aware cleanupIfNecessary is included in the timing:

 INFO [CompactionExecutor:48] 2011-11-25 22:25:30,353 CompactionTask.java (line 213) Compacted to [/data1/cassandra/data/MA_DDR/indexes_03-hc-5369-Data.db,].  5,821,590 to 5,306,354 (~91% of original) bytes for 123 keys at 0.163755MB/s.  Time: 30,903ms.

Here's a slightly larger one:
 INFO [CompactionExecutor:43] 2011-11-25 22:23:28,956 CompactionTask.java (line 213) Compacted to [/data1/cassandra/data/MA_DDR/indexes_03-hc-5336-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5337-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5338-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5339-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5340-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5341-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5342-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5343-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5344-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5345-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5346-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5347-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5348-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5349-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5350-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5351-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5352-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5353-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5354-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5355-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5356-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5357-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5358-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5359-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5360-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5361-Data.db,].  140,706,512 to 137,990,868 (~98% of original) bytes for 2,181 keys at 0.338627MB/s.  Time: 388,623ms.


This is with compaction throttling set to 0 (Off).


So I believe because of this it's going to take a very long time to recover from having so many small sstables. 
It might be notable that we're using Solaris 10, possibly listFiles() is faster on other platforms?

Is it feasible to keep track of the temp files and just delete them rather than searching for them for each SSTable using SSTable.componentsFor()?



Here's the stack trace for the CompactionExecutor:14 thread that appears to be occupying the majority of the cpu time on this node:

Name: CompactionExecutor:14
State: RUNNABLE
Total blocked: 3  Total waited: 1,610,714

Stack trace: 
 java.io.UnixFileSystem.getBooleanAttributes0(Native Method)
java.io.UnixFileSystem.getBooleanAttributes(Unknown Source)
java.io.File.isDirectory(Unknown Source)
org.apache.cassandra.io.sstable.SSTable$3.accept(SSTable.java:204)
java.io.File.listFiles(Unknown Source)
org.apache.cassandra.io.sstable.SSTable.componentsFor(SSTable.java:200)
org.apache.cassandra.io.sstable.SSTableWriter.cleanupIfNecessary(SSTableWriter.java:289)
org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:189)
org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:57)
org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:134)
org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:114)
java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
java.util.concurrent.FutureTask.run(Unknown Source)
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
java.lang.Thread.run(Unknown Source)

No matter where I click in the busy Compaction thread timeline in YourKit it's in Running state and showing this above trace, except for short periods of time where it's actually compacting :)

Thanks,
Eric","Solaris 10, 1.0.4 release candidate",,,,,,,,,,,,,,,02/Dec/11 03:49;jbellis;3532-v2.txt;https://issues.apache.org/jira/secure/attachment/12505848/3532-v2.txt,02/Dec/11 20:22;jbellis;3532-v3.txt;https://issues.apache.org/jira/secure/attachment/12505926/3532-v3.txt,28/Nov/11 19:00;eparusel;3532.txt;https://issues.apache.org/jira/secure/attachment/12505369/3532.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-11-26 07:04:37.578,,,no_permission,,,,,,,,,,,,218499,,,Mon Dec 12 22:07:38 UTC 2011,,,,,,0|i0gl2f:,94843,jbellis,jbellis,,,,,,,,,"26/Nov/11 07:04;jbellis;Looks like leveled compaction means that sstable creation can be part of the critical path now:

{noformat}
.   /**
     * Discovers existing components for the descriptor. Slow: only intended for use outside the critical path.
     */
    static Set<Component> componentsFor(final Descriptor desc, final Descriptor.TempState matchState)
{noformat}

bq. Is it feasible to keep track of the temp files and just delete them rather than searching for them for each SSTable using SSTable.componentsFor()?

Simplest would be to just check File.exists on the limited set of possible temp file names.  Next simplest and slightly more performant would be to move the cleanup out of the finally blocks, and into a catch block: the cleanup is a no-op if everything went well.","27/Nov/11 10:21;eparusel;Thanks.
Do I need to try to delete ~all~ components, for that descriptor.asTemporary()?  Or just specific ones?
A Component of type BITMAP_INDEX requires an id, so I'm not sure how I'd find this out without listing the directory contents.",28/Nov/11 14:32;jbellis;BITMAP_INDEX type was never completed (CASSANDRA-1472) so I'm fine with removing that code and doing whatever is simplest.  We can get more sophisticated if/when we get back to working on 1472.,"28/Nov/11 19:00;eparusel;Here's a small patch, and a few notes:

- I wasn't sure how to best document the interaction with BITMAP_INDEX, hope a TODO there is ok.
- I created a copy of descriptor.asTemporary(true).  Is descriptor always guaranteed to be temporary==true?  It left me a little uneasy.
- I used SSTable.delete (while checking ahead of time that the file exists), because I wasn't sure if ordering of deletes was important.
- SSTableReader.open() is the other method that calls SSTable.componentsFor, I only mention this because it may be a suboptimal call as well.","29/Nov/11 00:44;eparusel;I applied the patch in our test environment against the 1.0.4 revision, and compaction is humming along nicely now.
On the (otherwise idle) node I'm watching that had 250000 data/ files, file count is decreasing by 1000-1900/minute.","02/Dec/11 03:49;jbellis;v2 attached, with the new approach moved into componentsFor, so that open() can take advantage of the improvement too. Also, componentsFor now respects the temporary-ness of the descriptor passed, so a separate TempState enum is unnecessary.

Also renamed cleanupIfNecessary to abort, and moved to catch block as discussed above.","02/Dec/11 17:37;eparusel;Thank you Jonathan -- I applied the patch and it works for me.
Cheers","02/Dec/11 18:45;slebresne;* Wrapping exceptions into RuntimeException() blindly will confuse the catcher of UserInterruptedException in DTPE.logExceptionsAfterExecute. We could make make that catcher unwrap the full exception, but truth is I'm not a fan of wrapping exception needlessly. Maybe we could just add a silence method somewhere:
{noformat}
public void silence(Exception e)
{
    if (e instanceof RuntimeException)
        throw (RuntimeException) e;
    else
        throw new RuntimeException(e);
}
{noformat}
and use that (as we already do in WrappedRunnable actually).
* We could remove the bitmap indexes type while were at it (it'd be one less type to check).
","02/Dec/11 20:22;jbellis;added FBUtilities.unchecked(Exception) as suggested, and removed BITMAP component.",02/Dec/11 20:48;slebresne;+1,02/Dec/11 21:01;jbellis;committed,"02/Dec/11 21:39;eparusel;Patch 3532-v3.txt applied here, works for me.  Thanks again!","12/Dec/11 07:56;eparusel;Hmm, I might have spoken too soon.  This could also be a separate bug however.

The nodes in my cluster are using a lot of file descriptors, holding open tmp files.  A few are using 50K+, nearing their limit (on Solaris, of 64K).

Here's a small snippet of lsof:
java        828 appdeployer *146u  VREG          181,65540          0     333376 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776429-Data.db
java        828 appdeployer *147u  VREG          181,65540          0     332952 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776359-Data.db
java        828 appdeployer *148u  VREG          181,65540          0     333079 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776380-Index.db
java        828 appdeployer *149u  VREG          181,65540          0     333080 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776380-Data.db
java        828 appdeployer *150u  VREG          181,65540          0     333224 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776403-Index.db
java        828 appdeployer *151u  VREG          181,65540          0     333025 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776372-Data.db
java        828 appdeployer *152u  VREG          181,65540          0     333225 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776403-Data.db
java        828 appdeployer *154u  VREG          181,65540          0     333858 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776514-Index.db
java        828 appdeployer *155u  VREG          181,65540          0     333426 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776438-Data.db
java        828 appdeployer *156u  VREG          181,65540          0     333326 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776421-Data.db
java        828 appdeployer *157u  VREG          181,65540          0     333553 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776460-Data.db
java        828 appdeployer *158u  VREG          181,65540          0     333501 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776452-Index.db
java        828 appdeployer *159u  VREG          181,65540          0     333597 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776468-Index.db
java        828 appdeployer *160u  VREG          181,65540          0     333598 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776468-Data.db
java        828 appdeployer *162u  VREG          181,65540          0     333884 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776518-Data.db
java        828 appdeployer *163u  VREG          181,65540          0     333502 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776452-Data.db
java        828 appdeployer *165u  VREG          181,65540          0     333929 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776527-Index.db
java        828 appdeployer *166u  VREG          181,65540          0     333859 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776514-Data.db
java        828 appdeployer *167u  VREG          181,65540          0     333663 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776480-Data.db
java        828 appdeployer *168u  VREG          181,65540          0     333812 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776506-Index.db

I spot checked a few and found they still exist on the filesystem too:
-rw-r--r--   1 appdeployer appdeployer       0 Dec 12 07:16 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776506-Index.db",12/Dec/11 13:59;jbellis;That does sound like a separate problem.  What do you see when you grep the log for messages_meta-tmp-hb-776506?,"12/Dec/11 16:07;eparusel;Sure.  Let me know if you'd prefer a separate ticket.

I don't see anything in the logs matching ""776506"".  Any suggestions as to which class(es) I could turn on DEBUG log level for (via JMX), if that would help troubleshoot?","12/Dec/11 16:48;jbellis;Yes, separate ticket.

org.apache.cassandra.db.compaction, org.apache.cassandra.db.Memtable, org.apache.cassandra.db.DataTracker to start with",12/Dec/11 22:07;eparusel;Separate ticket created: CASSANDRA-3616,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleStrategy w/o replication_factor,CASSANDRA-2624,12506525,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,urandom,urandom,08/May/11 02:10,12/Mar/19 14:13,13/Mar/19 22:26,10/May/11 18:57,0.8.0,,,Legacy/CQL,Legacy/Tools,,0,,,,,"It is possible to create a new keyspace using {{SimpleStrategy}} _without_ specifying the {{replication_factor}} option.  Things get more interesting if you shut the node down, since it will refuse to restart (throwing a {{ConfigurationException}}).",,,,,,,,,,,,,,,,10/May/11 16:54;jbellis;2624-v2.txt;https://issues.apache.org/jira/secure/attachment/12478709/2624-v2.txt,10/May/11 15:28;jbellis;2624.txt;https://issues.apache.org/jira/secure/attachment/12478704/2624.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-10 15:28:21.564,,,no_permission,,,,,,,,,,,,20736,,,Tue May 10 22:30:29 UTC 2011,,,,,,0|i0gchb:,93452,xedin,xedin,,,,,,,,,10/May/11 15:28;jbellis;patch to have CQL call validateKsDef.  Also extracts code from CassandraServer into KSM.fromThrift for re-use in QP.,10/May/11 16:54;jbellis;rebased on top of r1101542,10/May/11 18:40;xedin;+1,10/May/11 18:57;jbellis;committed,"10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unstable repo has disappeared from http://www.apache.org/dist/cassandra/debian/dists/,CASSANDRA-2370,12502220,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,brandon.williams,brandon.williams,23/Mar/11 19:39,12/Mar/19 14:13,13/Mar/19 22:26,28/Mar/11 15:14,,,,Packaging,,,2,,,,,,The entire internet,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-28 15:14:35.035,,,no_permission,,,,,,,,,,,,20585,,,Mon Mar 28 15:14:35 UTC 2011,,,,,,0|i0gazj:,93210,,,,,,,,,,,28/Mar/11 15:14;jeromatron;Asked in #asfinfra - sounds like something happened with the ownership of the dirs in there.  It's fixed now.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress performance is artificially limited,CASSANDRA-2578,12505731,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,brandon.williams,brandon.williams,28/Apr/11 22:09,12/Mar/19 14:13,13/Mar/19 22:26,29/Apr/11 01:41,0.7.6,,,Legacy/Tools,,,0,,,,,"With stress I only get about 7k inserts/s against a single server, and the load and cpu usage from stress is higher than the server.  Pystress gets 15-20k inserts/s against the same machine.  Stress isn't cpu-limited however, so there must be something else holding it back.",,,,,,,,,,,,,,,,29/Apr/11 00:08;xedin;CASSANDRA-2578-0.7.patch;https://issues.apache.org/jira/secure/attachment/12477717/CASSANDRA-2578-0.7.patch,29/Apr/11 00:01;xedin;CASSANDRA-2578.patch;https://issues.apache.org/jira/secure/attachment/12477715/CASSANDRA-2578.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-29 00:01:10.726,,,no_permission,,,,,,,,,,,,20709,,,Fri Apr 29 01:55:45 UTC 2011,,,,,,0|i0gc7b:,93407,brandon.williams,brandon.williams,,,,,,,,,29/Apr/11 00:01;xedin;caching generateValues in Inserter/IndexedRangeSlicer. Patch against trunk but can be applied to cassandra-0.8 without any problem.,29/Apr/11 01:41;brandon.williams;Great! Committed.,"29/Apr/11 01:55;hudson;Integrated in Cassandra-0.7 #461 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/461/])
    cache generateValues in Inserter/IndexedRangeSlicer.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2578
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig load/storefunc loads only one schema and BytesType validation class needs fix,CASSANDRA-2465,12504126,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jeromatron,jeromatron,jeromatron,13/Apr/11 01:39,12/Mar/19 14:13,13/Mar/19 22:26,13/Apr/11 17:42,0.7.5,,,,,,0,hadoop,pig,,,"With a recent optimization, it appears that the Pig load/store func gets only one schema from Cassandra and tries to apply it to all CFs in the pig script.  Also, the BytesType validation tries to cast the object in putNext as a DataByteArray and wrap it as a ByteBuffer.  Instead it should just call objToBB which should take care of it.",,,,,,,,,,,,,,,,13/Apr/11 01:41;jeromatron;2465.txt;https://issues.apache.org/jira/secure/attachment/12476202/2465.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-13 17:42:17.052,,,no_permission,,,,,,,,,,,,20639,,,Wed Apr 13 17:42:17 UTC 2011,,,,,,0|i0gbjb:,93299,,,,,,,,,,,13/Apr/11 01:43;jeromatron;Attaching patch to make the udf context key specific to the keyspace and column family so it doesn't get overwritten.  Also changed the putNext case where the validation class is BytesType to use objToBB the way it should to handle things like Strings.,13/Apr/11 01:47;jeromatron;Tested with basic row count pig script as well as multiple join/cogroup operations against multiple column families.  Tested input and output.,13/Apr/11 17:42;brandon.williams;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Try harder to close scanners after a failed compaction,CASSANDRA-2431,12503617,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,07/Apr/11 09:30,12/Mar/19 14:13,13/Mar/19 22:26,07/Apr/11 20:15,0.7.5,,,,,,0,,,,,Forked from 2191.,,,,,,,,,,,,,,,,07/Apr/11 09:32;stuhood;0001-Try-harder-to-close-scanners-in-compaction-close.txt;https://issues.apache.org/jira/secure/attachment/12475692/0001-Try-harder-to-close-scanners-in-compaction-close.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-07 20:15:35.824,,,no_permission,,,,,,,,,,,,20620,,,Thu Apr 07 23:44:17 UTC 2011,,,,,,0|i0gbc7:,93267,slebresne,slebresne,,,,,,,,,"07/Apr/11 20:15;slebresne;+1

(Committed as r1089976 and merged to trunk as 1089980)","07/Apr/11 21:50;hudson;Integrated in Cassandra #835 (See [https://hudson.apache.org/hudson/job/Cassandra/835/])
    Merge CASSANDRA-2431 from 0.7
","07/Apr/11 23:44;hudson;Integrated in Cassandra-0.7 #426 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/426/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
zero-length strings should result in zero-length ByteBuffers,CASSANDRA-2352,12501733,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,17/Mar/11 20:55,12/Mar/19 14:13,13/Mar/19 22:26,17/Mar/11 21:38,0.8 beta 1,,,Legacy/CQL,,,0,cql,,,,"The {{o.a.c.db.marshal.AbstractType.fromString()}} methods should return an empty {{ByteBuffer}} when passed a zero-length string, (empty bytes do {{validate()}} properly).",,,,,,,,,,,,,,,,17/Mar/11 20:56;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2352-AT.fromString-should-return-empty-BB-fo.txt;https://issues.apache.org/jira/secure/attachment/12473943/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2352-AT.fromString-should-return-empty-BB-fo.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-17 21:23:21.683,,,no_permission,,,,,,,,,,,,20575,,,Wed Mar 23 01:28:10 UTC 2011,,,,,,0|i0gavr:,93193,gdusbabek,gdusbabek,,,,,,,,,17/Mar/11 21:23;gdusbabek;+1,17/Mar/11 21:38;urandom;committed,"17/Mar/11 21:55;jbellis;we actually keep BBUtil.EMPTY_BYTE_BUFFER to avoid allocating lots of zero-length BBs, can you update post-commit?",18/Mar/11 02:38;jbellis;went ahead and did this.,18/Mar/11 04:13;urandom;Thanks Jonathan!,18/Mar/11 07:20;stuhood;Does this mean empty strings will validate as column names?,"18/Mar/11 13:16;jbellis;We have separate checks in ThriftValidation for name.remaining() != 0.

I'm guessing this is to make it easier to do slicing, where empty start is valid?","18/Mar/11 15:08;urandom;bq. Does this mean empty strings will validate as column names?

The {{AbstractType.validate()}} methods already allow this.  As Jonathan says, wherever that's not acceptable, {{ThriftValidation}} has tests to ensure name.remaining() != 0.","23/Mar/11 01:28;hudson;Integrated in Cassandra #797 (See [https://hudson.apache.org/hudson/job/Cassandra/797/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows: CliTest broken because of /r/n,CASSANDRA-2337,12501536,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bcoverston,bcoverston,bcoverston,16/Mar/11 01:53,12/Mar/19 14:13,13/Mar/19 22:26,17/Mar/11 02:38,0.7.5,,,,,,0,windows,,,,Somebody thought that windows should emulate a telex machine and we ended up with /r/n.,Windows,,,,,,,,,,,,,,,16/Mar/11 01:55;bcoverston;2337.patch;https://issues.apache.org/jira/secure/attachment/12473759/2337.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-17 02:38:48.137,,,no_permission,,,,,,,,,,,,20568,,,Fri Mar 18 04:06:55 UTC 2011,,,,,,0|i0gasf:,93178,jbellis,jbellis,,,,,,,,,16/Mar/11 01:55;bcoverston;Fixes newline substituting environment variable.,17/Mar/11 02:38;jbellis;fixed,"18/Mar/11 04:06;hudson;Integrated in Cassandra-0.7 #391 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/391/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tombstone are not purged when the row is in only one sstable,CASSANDRA-2801,12511068,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,21/Jun/11 09:57,12/Mar/19 14:13,13/Mar/19 22:26,21/Jun/11 12:48,0.8.1,,,,,,0,,,,,"We messed up the refactor of compactionController. It echoes rows if they are present in only one sstable, even if they could be purged.",,,,,,,,,,,,,,,,21/Jun/11 09:57;slebresne;0001-Don-t-echo-row-when-purge-is-possible.patch;https://issues.apache.org/jira/secure/attachment/12483258/0001-Don-t-echo-row-when-purge-is-possible.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-21 12:32:05.426,,,no_permission,,,,,,,,,,,,20839,,,Tue Jun 21 17:41:40 UTC 2011,,,,,,0|i0gdjj:,93624,,,,,,,,,,,21/Jun/11 12:32;jbellis;+1,"21/Jun/11 12:48;slebresne;Committed, thanks","21/Jun/11 17:41;hudson;Integrated in Cassandra-0.8 #182 (See [https://builds.apache.org/job/Cassandra-0.8/182/])
    Purge tombstones even if the row is in only one sstable
patch by slebresne; reviewed by jbellis for CASSANDRA-2801

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1137982
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_indexed_slices losts index expressions,CASSANDRA-3850,12541180,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,philip.andronov,philip.andronov,04/Feb/12 10:11,12/Mar/19 14:13,13/Mar/19 22:26,04/Feb/12 14:29,1.1.0,,,,,,0,get,indexing,search,,"in trunk 
CassandraServer.get_indexed_slices(ColumnParent , IndexClause , SlicePredicate , ConsistencyLevel)
 looses  index_clause.expressions when calling  constructing RangeSliceCommand by using wrong constructor.

This makes examples on http://wiki.apache.org/cassandra/CassandraCli produce wrong output as well as any get involving ""where"" check.
Patch to fix this issue http://pastebin.com/QQT0Tfpc",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-02-04 14:29:44.01,,,no_permission,,,,,,,,,,,,226532,,,Sat Feb 04 14:29:44 UTC 2012,,,,,,0|i0goz3:,95476,slebresne,slebresne,,,,,,,,,"04/Feb/12 14:29;slebresne;Committed, thanks.

(don't hesitate to attach the patch to the issue next time, it's slightly more convenient :))",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parent POM does not get deployed to the maven repository,CASSANDRA-2562,12505194,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stephenc,stephenc,stephenc,26/Apr/11 13:48,12/Mar/19 14:13,13/Mar/19 22:26,29/Apr/11 17:28,0.7.6,,,,,,0,,,,,"The parent pom does not get deployed to the Maven Central

(for 0.7.5 I am fixing this by hand)",,,,,,,,,,,,,,,,26/Apr/11 13:53;stephenc;CASSANDRA-2562.patch;https://issues.apache.org/jira/secure/attachment/12477413/CASSANDRA-2562.patch,26/Apr/11 13:49;stephenc;CASSANDRA-2562.patch;https://issues.apache.org/jira/secure/attachment/12477412/CASSANDRA-2562.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-29 17:28:17.704,,,no_permission,,,,,,,,,,,,20697,,,Mon May 02 19:54:50 UTC 2011,,,,,,0|i0gc3r:,93391,,,,,,,,,,,26/Apr/11 13:49;stephenc;This patch is for post 0.7.5 unless there is a take#2 of 0.7.5,26/Apr/11 13:53;stephenc;updated patch as I missed one line,29/Apr/11 17:28;slebresne;Committed,"29/Apr/11 17:44;hudson;Integrated in Cassandra-0.7 #464 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/464/])
    Deploy parent POM to maven central
patch by stephenc for CASSANDRA-2562
","29/Apr/11 18:30;hudson;Integrated in Cassandra #873 (See [https://builds.apache.org/hudson/job/Cassandra/873/])
    merge records of CASSANDRA-2562 from 0.8
","02/May/11 19:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeveledCompactionTask is too fragile and can block compactions,CASSANDRA-3408,12529064,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,27/Oct/11 15:14,12/Mar/19 14:13,13/Mar/19 22:26,27/Oct/11 16:21,1.0.1,,,,,,0,,,,,"If any error happens during a LeveledCompactionTask, it will just block every compaction.",,,,,,,,,,,,,,,,27/Oct/11 15:21;slebresne;3408.patch;https://issues.apache.org/jira/secure/attachment/12501104/3408.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-27 15:58:08.925,,,no_permission,,,,,,,,,,,,214924,,,Thu Oct 27 16:21:32 UTC 2011,,,,,,0|i0gjiv:,94593,jbellis,jbellis,,,,,,,,,27/Oct/11 15:58;jbellis;+1,27/Oct/11 16:21;slebresne;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2GB row size limit in ColumnIndex offset calculation,CASSANDRA-3358,12527029,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tho,tho,tho,13/Oct/11 14:36,12/Mar/19 14:13,13/Mar/19 22:26,13/Oct/11 17:34,0.7.10,0.8.8,1.0.1,,,,0,,,,,"Index offset is calculated using int instead of long resulting in overflow at 2GB row size. As a result affected columns can not be retrieved. 

Fix: use long instead of int",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-10-13 15:46:42.673,,,no_permission,,,,,,,,,,,,85391,,,Thu Oct 13 17:40:42 UTC 2011,,,,,,0|i0giwv:,94494,jbellis,jbellis,,,,,,,,,"13/Oct/11 15:46;jbellis;Weird -- you're absolutely right, but I could have sworn we fixed this already. :)","13/Oct/11 17:34;jbellis;committed the proposed fix, thanks!","13/Oct/11 17:40;hudson;Integrated in Cassandra-0.7 #554 (See [https://builds.apache.org/job/Cassandra-0.7/554/])
    fix ColumnIndexer to use long offsets
patch by Thomas Richter; reviewed by jbellis for CASSANDRA-3358

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1183000
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnIndexer.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
secondary index not dropped until restart,CASSANDRA-2619,12506473,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,cywjackson,cywjackson,07/May/11 00:36,12/Mar/19 14:13,13/Mar/19 22:26,09/May/11 07:09,0.7.6,0.8.0,,Feature/2i Index,,,0,,,,,"when dropping the secondary index (via cassandra-cli), the describe keyspace still shows the Built index entry. Only after a restart of the CassandraDaemon then the Built Index entry is gone. This seems indicate a problem with the index not really been dropped completed.

to test, use a single node, create an index, then drop it from the cli (issue an update column family ... with metadata fields but not the index info)

below is the original:

  Column Families:
    ColumnFamily: inode
    ""Stores file meta data""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 0.0/14400
      Memtable thresholds: 0.103125/22/1440 (millions of ops/MB/minutes)
      GC grace seconds: 60
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      {color:red}Built indexes: [inode.path, inode.sentinel]{color}
      Column Metadata:
        Column Name: path (70617468)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
          {color:red}Index Name: path
          Index Type: KEYS{color}
        Column Name: sentinel (73656e74696e656c)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
          {color:red}Index Name: sentinel
          Index Type: KEYS{color}

issue an update:
{noformat}

[default@unknown] use cfs;
Authenticated to keyspace: cfs
[default@cfs] update column family inode with comparator=BytesType and column_metadata=[{column_name:70617468, validation_class:BytesType}, {column_name:73656e74696e656c,validation_class:BytesType}];
fca46d00-783c-11e0-0000-242d50cf1fff
Waiting for schema agreement...
... schemas agree across the cluster
{noformat}

describe the keyspace again:
Keyspace: cfs:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [Brisk:1, Cassandra:0]
  Column Families:
    ColumnFamily: inode
    ""Stores file meta data""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 0.0/14400
      Memtable thresholds: 0.103125/22/1440 (millions of ops/MB/minutes)
      GC grace seconds: 60
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      {color:red}Built indexes: [inode.path, inode.sentinel]{color}
      Column Metadata:
        Column Name: path (70617468)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
        Column Name: sentinel (73656e74696e656c)
          Validation Class: org.apache.cassandra.db.marshal.BytesType

*notice the red line on Built Indexes*

restart CassandraDaemon, describe again:

Keyspace: cfs:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [Brisk:1, Cassandra:0]
  Column Families:
    ColumnFamily: inode
    ""Stores file meta data""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 0.0/14400
      Memtable thresholds: 0.103125/22/1440 (millions of ops/MB/minutes)
      GC grace seconds: 60
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      {color:red}Built indexes: []{color}
      Column Metadata:
        Column Name: path (70617468)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
        Column Name: sentinel (73656e74696e656c)
          Validation Class: org.apache.cassandra.db.marshal.BytesType


on another note, upon re-create the index, it does not appear the index is actually rebuilt. There is no need to restart CassandraDaemon for the Built Index to show up from the describe. But the update goes very fast. We could tell the index is not being rebuilt because we were getting NPE from:

{noformat}
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:51)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.satisfies(ColumnFamilyStore.java:1647)
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1594)
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
{noformat}
and after re-create the index, the exception resurface (the exception does not surface upon drop).

If we drop the index files and remove them, then re-create the index, the NPE is resolved: 

{noformat}
$ find /var/lib/cassandra/data/cfs -name ""*path*"" -o -name ""*sentinel* -exec rm {} \;""
{noformat}",,,,,,,,,,,,,,,,07/May/11 04:54;jbellis;2619.txt;https://issues.apache.org/jira/secure/attachment/12478490/2619.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-07 04:54:36.393,,,no_permission,,,,,,,,,,,,20732,,,Mon May 09 07:09:24 UTC 2011,,,,,,0|i0gcg7:,93447,brandon.williams,brandon.williams,,,,,,,,,"07/May/11 04:54;jbellis;Patch attached. There are 3 main things going on here:

- move DefsTest to config package (so it can access CFMetaData.column_metadata) and add a test to expose the bug
- clarify the CFMetaData.apply code to make it clear where it's dealing w/ column addition/removal and not indexes per se (this is where I thought the bug was at first)
- the actual fix, which is the 3 line change to CFS.reload

(Note that as a workaround, you can simply drop the column definition altogether instead of leaving the validator intact while removing the index type.)","07/May/11 05:08;jbellis;NOTE: because svn is retarded, you need to help patch figure out how to apply this, by first running

{noformat}
cp test/unit/org/apache/cassandra/db/DefsTest.java test/unit/org/apache/cassandra/config/DefsTest.java
{noformat}",07/May/11 06:23;brandon.williams;+1,"07/May/11 07:16;hudson;Integrated in Cassandra-0.7 #474 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/474/])
    recognize attempt todrop just the index while leaving the column definition
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2619
",09/May/11 07:09;jbellis;(committed),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible deadlock for counter mutations,CASSANDRA-2454,12503982,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,kelvin,stuhood,stuhood,11/Apr/11 21:38,12/Mar/19 14:12,13/Mar/19 22:26,14/Apr/11 21:46,0.8 beta 1,,,,,,0,,,,,"{{StorageProxy.applyCounterMutation}} is executed on the mutation stage, but it also submits tasks to the mutation stage, and then blocks for them. If there are more than a few concurrent mutations, this can lead to deadlock.",,,,,,,,,,,,,,,,11/Apr/11 21:48;stuhood;0001-Don-t-re-submit-to-the-mutation-stage.txt;https://issues.apache.org/jira/secure/attachment/12476061/0001-Don-t-re-submit-to-the-mutation-stage.txt,12/Apr/11 10:13;slebresne;0001-Submit-counters-update-on-mutation-stage-only-if-not.patch;https://issues.apache.org/jira/secure/attachment/12476104/0001-Submit-counters-update-on-mutation-stage-only-if-not.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-12 10:13:42.353,,,no_permission,,,,,,,,,,,,20632,,,Thu Apr 14 22:19:44 UTC 2011,,,,,,0|i0gbgv:,93288,slebresne,slebresne,,,,,,,,,11/Apr/11 21:48;stuhood;Patch. (credit to Kelvin Kakugawa),"12/Apr/11 10:13;slebresne;Looking closer that this there is 2 places from which we execute counter write:
  # if the coordinator is a replica, from the thrift thread.
  # otherwise in the CounterMutationVerbHandler on a replica, that is on the MUTATION stage.

In the latter case, we must indeed avoid re-submitting to the MUTATION stage to avoid deadlock. But in the former we should not skip the stage.

Attaching a v2 patch that distinguishes between the 2 cases and 'do the right thing'.

Note that another way to fix this would be to make CounterMutationVerbHandler execute on some other stage that the MUTATION one. Even though that would simpler in the number of line modified, I don't think an existing stage would fit the bill and creating a new one for that doesn't feel right.","14/Apr/11 21:39;stuhood;+1
Looks great, thanks!",14/Apr/11 21:46;jbellis;committed (in case sylvain's not back before we roll a beta),"14/Apr/11 22:19;hudson;Integrated in Cassandra-0.8 #7 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/7/])
    fix possible counter deadlock
patch by Kelvin Kakugawa, Stu Hood, and slebresne for CASSANDRA-2454
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select * doesn't include row key,CASSANDRA-2622,12506497,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,07/May/11 13:08,12/Mar/19 14:12,13/Mar/19 22:26,09/May/11 19:55,0.8.0,,,Legacy/CQL,,,0,,,,,,,,,,,,,,,,,,,,,09/May/11 05:11;jbellis;2622.txt;https://issues.apache.org/jira/secure/attachment/12478568/2622.txt,11/May/11 10:18;xedin;grammar-fix.patch;https://issues.apache.org/jira/secure/attachment/12478796/grammar-fix.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-08 01:40:20.044,,,no_permission,,,,,,,,,,,,20734,,,Wed May 11 10:18:13 UTC 2011,,,,,,0|i0gcgv:,93450,thobbs,thobbs,,,,,,,,,"07/May/11 15:15;jbellis;... in fact, we don't support KEY at all in the select list.","07/May/11 15:26;jbellis;- adds validation that key_alias must be ascii
- adds key to wildcard columns
- adds support for key in named columns list

Stuck getting antlr to be happy with using key as a column name though:

{noformat}
SELECT KEY, birthdate FROM IndexedA WHERE birthdate = 100
{noformat}

gives ""no viable alternative at input 'key'"".

I believe the problem is that KEY is a keyword (K_KEY) but not sure how to make that allowed in column list.","07/May/11 15:56;jbellis;... found the ""term returns"" section of the grammar, proceeding w/ testing now.","08/May/11 01:40;urandom;I thought we had agreed not to do this.  Wasn't the plan to introduce virtual columns, which would allow you to include the key (by name) in the projection, and to return it in results like any other column?

CASSANDRA-2480","08/May/11 02:05;jbellis;Right. But that implies that in the meantime * needs to include KEY or we break semantics when we go from no-key to including-key.

Currently, the Python driver cheats and adds the row key at the beginning of each column list whether it was asked for or not (obviously incorrect) and JDBC forces you to unwrap the CassandraResultSet to get at it, which encourages relying on implementation details that should really be private (and isn't very user-friendly either: http://twitter.com/#!/ampedandwired/status/66795646228762624).

It also means the two drivers we're shipping have different semantics which is also broken.

As you can see from the patch (updated version attached) the actual changes to QueryProcessor are small; the work of key-wrangling is done in extractThriftColumns and the rest is just adjustments to pass the metadata object there.

The patch is complete for QP and cql.py; I'm still working on the JDBC side. Note that all the existing JDBC tests continue to pass (since there weren't actually any wildcards tested there).","08/May/11 02:09;jbellis;bq. The patch is complete for QP and cql.py; I'm still working on the JDBC side

In fact drivers/java is enough of a mess right now that I uploaded a new patch without that lest anyone think the current state is a useful indication of the right way to go there. :)",09/May/11 05:11;jbellis;complete patch including jdbc.,"09/May/11 05:41;thobbs;These semantics make much more sense to me.

I'm +1 on the code changes, as well.",09/May/11 19:55;jbellis;committed,"10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ",11/May/11 10:18;xedin;grammar should build without warnings.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support SQL data types in CQL,CASSANDRA-2445,12503843,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,09/Apr/11 21:40,12/Mar/19 14:12,13/Mar/19 22:26,10/Apr/11 04:45,0.8 beta 1,,,,,,0,,,,,"We should support SQL data types where possible:

(sql -> cassandra)
varint -> int
bigint -> long
varchar, text -> utf8
ascii -> ascii (not strictly correct -- sql defines collations for this -- but close enough for a 1.0)
uuid -> uuid (see CASSANDRA-2233)
bytea -> bytes

IMO the right thing to do is _only_ support the SQL types in CQL CREATE statements, because there is ambiguity otherwise (in particular with ""int"").
",,,,,,,,,,,,,,,,10/Apr/11 00:28;jbellis;2445.txt;https://issues.apache.org/jira/secure/attachment/12475919/2445.txt,10/Apr/11 02:14;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2445-update-documentation.txt;https://issues.apache.org/jira/secure/attachment/12475920/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2445-update-documentation.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-10 02:15:56.372,,,no_permission,,,,,,,,,,,,20629,,,Sun Apr 10 05:21:51 UTC 2011,,,,,,0|i0gbev:,93279,urandom,urandom,,,,,,,,,"10/Apr/11 02:15;urandom;The docs should be updated too (patch attached), otherwise, LGTM. +1",10/Apr/11 04:45;jbellis;thanks!  committed.,"10/Apr/11 05:21;hudson;Integrated in Cassandra #842 (See [https://hudson.apache.org/hudson/job/Cassandra/842/])
    use SQL-ish data types
patch by jbellis and Eric Evans for CASSANDRA-2445
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataTracker.View.MarkCompacting adds ALL sstables and marks them as compacting,CASSANDRA-2765,12510131,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,bcoverston,bcoverston,bcoverston,13/Jun/11 19:17,12/Mar/19 14:12,13/Mar/19 22:26,13/Jun/11 21:22,0.8.1,,,,,,0,,,,,"At some point if the list isn't cleaned up with this symptom compactions will stop until the server is restarted.
",,3600,3600,,0%,3600,3600,,,,,,,,,13/Jun/11 19:27;bcoverston;0001-removed-code-adding-all-sstables-to-the-view-builder.patch;https://issues.apache.org/jira/secure/attachment/12482365/0001-removed-code-adding-all-sstables-to-the-view-builder.patch,13/Jun/11 21:09;jbellis;2765-v2.txt;https://issues.apache.org/jira/secure/attachment/12482460/2765-v2.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-13 21:09:47.168,,,no_permission,,,,,,,,,,,,20815,,,Tue Jun 14 17:02:36 UTC 2011,,,,,,0|i0gdbj:,93588,jbellis,jbellis,,,,,,,,,13/Jun/11 19:27;bcoverston;Attached patch removes code that adds all sstables to the compacting set.,13/Jun/11 21:09;jbellis;I think the intention was to add the new tomark set to the existing currently-being-compacted set.  v2 attached.,"13/Jun/11 21:14;bcoverston;Good call, I missed that part :).",13/Jun/11 21:22;bcoverston;+1,13/Jun/11 21:22;jbellis;committed,"14/Jun/11 17:02;hudson;Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make bootstrap retry,CASSANDRA-2644,12507049,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,12/May/11 22:22,12/Mar/19 14:12,13/Mar/19 22:26,30/May/11 08:52,0.8.1,,,,,,1,,,,,"We ran into a situation where we had rpc_timeout set to 1 second, and the node needing to compute the token took over a second (1.6 seconds). The bootstrapping node hangs forever without getting a token because the expiring map removes it before the reply comes back.",,,,,,,,,,,,,,,,12/May/11 22:41;lenn0x;0001-Make-ExpiringMap-have-objects-with-specific-timeouts.patch;https://issues.apache.org/jira/secure/attachment/12479021/0001-Make-ExpiringMap-have-objects-with-specific-timeouts.patch,12/May/11 22:41;lenn0x;0002-Make-bootstrap-retry-and-increment-timeout-for-every.patch;https://issues.apache.org/jira/secure/attachment/12479022/0002-Make-bootstrap-retry-and-increment-timeout-for-every.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-12 22:52:49.431,,,no_permission,,,,,,,,,,,,20749,,,Mon May 30 14:55:47 UTC 2011,,,,,,0|i0gclj:,93471,stuhood,stuhood,,,,,,,,,"12/May/11 22:23;lenn0x;I have a patch for this I'll be adding within the next day. I make ExpiringMap support custom timeouts per object, and make bootstrap getToken retry, while exponentially increasing the timeout until retries is met.","12/May/11 22:52;stuhood;+1
Thanks Chris!","12/May/11 23:09;jbellis;I think the retry logic is a distraction here. If it doesn't work the first time because of anything other than ""we didn't wait long enough"" (i.e. it errored out) it's not likely to magically unbreak for the second.

Suggest just giving it a long retry to begin with.","13/May/11 04:37;stuhood;Good point. But there are still cases that retries will recover from... flapping/down nodes, etc.

IMO, we should fix the performance issue independently (opened CASSANDRA-2645), and consider retrying to be a best practice that should spread to more places in the codebase.","13/May/11 07:35;slebresne;Not fully related to the discussion here but streaming is another part of bootstrap so I'll mention that CASSANDRA-2433 introduces some mechanism to handle unrecoverable failures during streaming (that is, streaming already retry on errors but 1) it retries indefinitely while the CASSANDRA-2433 introduce a max retry and 2) it doesn't detect the other end being dead). Anyway, just referencing the ticket so that if this ticket becomes ""make bootstrap handle failures better"", we don't duplicate efforts.   ","17/May/11 22:17;jbellis;bq. But there are still cases that retries will recover from... flapping/down nodes

Fair enough, but increasing the timeout is still unwarranted.  Let's just make it wait for max(DEFAULT_TIMEOUT, BOOTSTRAP_TIMEOUT) with B_T equal to, say, 30s.

Committed patch 01 to 0.8.1 branch, btw.","30/May/11 08:48;lenn0x;Made changes to 02 patch, and commited to 0.8.1. Thanks!","30/May/11 14:44;hudson;Integrated in Cassandra #912 (See [https://builds.apache.org/hudson/job/Cassandra/912/])
    ","30/May/11 14:55;hudson;Integrated in Cassandra-0.8 #146 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/146/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClusterTool throws exception for get_endpoints,CASSANDRA-2437,12503667,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,07/Apr/11 19:21,12/Mar/19 14:12,13/Mar/19 22:26,07/Apr/11 19:41,0.8 beta 1,,,,,,0,,,,,"ByteBuffer is not serializable over JMX.

Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
	at $Proxy0.getNaturalEndpoints(Unknown Source)
	at org.apache.cassandra.tools.NodeProbe.getEndpoints(NodeProbe.java:446)
	at org.apache.cassandra.tools.ClusterCmd.printEndpoints(ClusterCmd.java:146)
	at org.apache.cassandra.tools.ClusterCmd.main(ClusterCmd.java:240)
Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1156)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1338)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1146)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:326)
	at java.rmi.MarshalledObject.<init>(MarshalledObject.java:101)
	at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.invoke(RMIConnector.java:990)
	at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:288)
	... 4 more",,,,,,,,,,,,,,,,07/Apr/11 19:23;lenn0x;0001-CASSANDRA-2437-Support-a-byte-key-for-getNaturalEndp.patch;https://issues.apache.org/jira/secure/attachment/12475737/0001-CASSANDRA-2437-Support-a-byte-key-for-getNaturalEndp.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-07 19:27:23.359,,,no_permission,,,,,,,,,,,,20626,,,Thu Apr 07 21:50:06 UTC 2011,,,,,,0|i0gbdb:,93272,,,,,,,,,,,07/Apr/11 19:27;stuhood;+1,"07/Apr/11 21:50;hudson;Integrated in Cassandra #835 (See [https://hudson.apache.org/hudson/job/Cassandra/835/])
    Support a byte[] key for getNaturalEndpoints so clustertool get_endpoints does not throw exception
patch by goffinet; reviewed by stuhood for CASSANDRA-2437
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clientutil jar missing from artifacts,CASSANDRA-3230,12523621,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,19/Sep/11 21:13,12/Mar/19 14:12,13/Mar/19 22:26,20/Sep/11 11:39,1.0.0,,,Packaging,,,0,cql,,,,The new clientutil jar is not being included in binary release artifacts,,,,,,,,,,,,,,,,19/Sep/11 21:14;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3230-copy-clientutil-jar-to-binary-release-a.txt;https://issues.apache.org/jira/secure/attachment/12495149/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3230-copy-clientutil-jar-to-binary-release-a.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-19 21:25:08.537,,,no_permission,,,,,,,,,,,,4006,,,Tue Sep 20 11:39:45 UTC 2011,,,,,,0|i0ghe7:,94248,,,,,,,,,,,19/Sep/11 21:25;jbellis;+1,20/Sep/11 11:39;urandom;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compactions can (seriously) delay schema migrations,CASSANDRA-3116,12521007,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,urandom,urandom,01/Sep/11 02:42,12/Mar/19 14:12,13/Mar/19 22:26,31/Oct/11 13:32,1.1.0,,,,,,0,compaction,,,,A compaction lock is acquired when dropping keyspaces or column families which will cause the schema migration to block if a compaction is in progress.,,,,,,,,,,CASSANDRA-3225,,,,,,26/Oct/11 17:37;jbellis;3116-v2.txt;https://issues.apache.org/jira/secure/attachment/12500916/3116-v2.txt,28/Oct/11 19:30;jbellis;3116-v3.txt;https://issues.apache.org/jira/secure/attachment/12501348/3116-v3.txt,17/Oct/11 17:11;jbellis;3116.txt;https://issues.apache.org/jira/secure/attachment/12499402/3116.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-09-01 02:51:31.214,,,no_permission,,,,,,,,,,,,1872,,,Mon Oct 31 17:16:49 UTC 2011,,,,,,0|i0gfzb:,94019,slebresne,slebresne,,,,,,,,,01/Sep/11 02:51;jbellis;agreed that this sucks.  would like to switch to some kind of test-and-set safety for compaction vs migration instead.  not immediately obvious to me how to do this.,"01/Sep/11 17:00;tom.wilkie;Its worse than this too; a migration tried to get the write lock, which gets blocked on a big compaction (holding the read lock).  This migration waiting on the write lock then blocks all other compactions waiting on the read lock.  So you only get one compaction going on and thousands backing up.

A really hacky temporary fix would be to use a tryLock(timeout) and short sleep in a loop in the migration.  This would at least not starve the merges, but would starve the migrations quite badly.","02/Sep/11 08:29;slebresne;I think the _raison d'être_ of the lock is because we need to mark all sstables compacted for them to be removed when dropping, but that cannot be done correctly if some sstable are being compacted. But couldn't we just ""delay"" the compacted marking ? For instance, we could have a 'isDropped' switch in DataTracker such that when that switch is on the replace() method just remove the 'replacements' sstables. So the drop keyspace/cf would set the isDropped flag first, then grab any sstable files that is not being compacted and mark those right away. It may be a bit tricky to do that atomically but _à priori_ it sounds doable. We'll probably want to add a call to some checkForDropped() method in case a compaction fails to be sure we don't leave sstables behind in that case.

Another option may be to just stop the running compactions (CASSANDRA-1740) so that we can mark everything compacted at once. I may be harder to make that thread safe though, not sure, and CASSANDRA-1740 is not in yet. ","17/Oct/11 17:11;jbellis;Patch to replace locking in migrations + valid checks in CompactionManager with isValid checks in DataTracker.

compactionLock is still used but only for major compaction.  should we get rid of that too and say ""if you want to be absolutely sure you're compacting everything, disable minor compactions before invoking major?""",17/Oct/11 17:11;jbellis;Note: applies after the Rename migration removal in CASSANDRA-3292.,"19/Oct/11 15:19;slebresne;Not sure this work correctly. I believe we first have a problem with DT.removeAllSSTables(), because this is during the drop and really do remove *all* sstables, including the ones that are being compacted (and thus it will unreference those while they are being compacted, which is bad). So we should first change that to only remove the one that are not compacting. Then we must make sure that anything that was not removed by that gets removed later. Which involves removing any flushed memtable (though that doesn't really matter since a dropped CF is flushed before being invalidated) and we must make sure that compacted sstable do are marked compacted but also that replacements are directly marked as compacted too (which mainly involve that we call removeOldSSTablesSize() on them). And I suppose we could make sure no new compaction is automatically triggered on an invalidated CF so we don't have a race or something.

bq. compactionLock is still used but only for major compaction. should we get rid of that too and say ""if you want to be absolutely sure you're compacting everything, disable minor compactions before invoking major?""

I think there is really no much cost to keeping the lock in there if the write lock is only acquired by events triggered by a user and I would prefer having major compaction do what it pretend by default rather that having a ""complicated"" procedure. That being said, I would be for replacing the global compactionLock by one lock per CF (which should be easy).
","26/Oct/11 17:37;jbellis;bq. only remove the one that are not compacting

done.  (renamed removeAllSSTables to unreferenceSSTables, which could still stand improvement...)

bq. removing any flushed memtable ... also that replacements are directly marked as compacted too 

done (both by ultimately funneling through the replace method)

bq. we could make sure no new compaction is automatically triggered on an invalidated CF 

this shouldn't be a problem, if it happens.  I'd rather not go to extra effort to prevent something harmless.

v2 also gets rid of CFS.flushlock (we already flush for the drop snapshot) and removes CFS.isDropped in favor of isValid.","28/Oct/11 17:20;slebresne;The patch already needs rebase, but based on the diff a few comments:
* In unmarkCompacting, calling twice markCompacted is not so harmless has it will trigger an assertion. What we could do is checked the sstableReader isCompacted flag.
* attemptUpdate breaks atomicity for unreferenceSSTable. We should make sure the compareAndSet is done on the view we used to compute notCompacting, otherwise we could have bug in View.replace (like in CASSANDRA-3306). It's probably simpler to move back the compareAndSet in both unreferenceSSTable and replace, and call a 'finalizeReplace' for the addNewSSTableSize and following methods.
* We could rename the unregisterMBean method in KeysIndex to invalidate.
* We may still want to check for isValid before doing a validation compaction because it doesn't call markCompacting, so it could still run after it's invalidated on some sstable that have not yet be removed because are being compacted.
","28/Oct/11 19:30;jbellis;bq. What we could do is checked the sstableReader isCompacted flag

I think I like moving the check into removeOldSSTables instead, since it's clearly threadsafe (even though there are no existing thread safety issues, why take chances with future complications).  The new version also allows markCompacted to ""work"" when called multiple times with asserts turned off; before, the file create IOException would blow up anyway.

bq. attemptUpdate breaks atomicity for unreferenceSSTable

Damn it, you're right.  So many CAS loops feels like we're making this too fragile.  But you're right, that's better than passing View references around.

bq. rename the unregisterMBean method in KeysIndex to invalidate

done.

bq. We may still want to check for isValid before doing a validation compaction 

Done, although I suspect my comment attempting to explain the reason for the check may cause more confusion than it's worth. :)

v3 attached.  CASSANDRA-3409 throws a bit of a wrench into things since I don't see a good way to avoid the lock; still, that's not a frequently used migration.","31/Oct/11 13:11;slebresne;+1 on v3. Nit: for the assert in DT.postReplace(), I believe the msg should include the sstable, not 'this'.",31/Oct/11 13:32;jbellis;fixed + committed,"31/Oct/11 17:16;hudson;Integrated in Cassandra #1177 (See [https://builds.apache.org/job/Cassandra/1177/])
    replace compactionlock use in schema migration by checking CFS.isInvalidD
patch by jbellis; reviewed by slebresne for CASSANDRA-3116

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1195542
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/DataTracker.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/Table.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndex.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/keys/KeysIndex.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/DropColumnFamily.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/DropKeyspace.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/KeyCacheTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/compaction/CompactionsTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test RoundRobinScheduler timeouts,CASSANDRA-3096,12520496,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,28/Aug/11 22:56,12/Mar/19 14:12,13/Mar/19 22:26,01/Sep/11 03:14,1.0.0,,,Legacy/CQL,,,0,,,,,"CASSANDRA-3079 was very hasty, and introduced two bugs that would: 1) cause the scheduler to busywait after a timeout, 2) never actually throw timeouts. This calls for a test.",,,,,,,,,,,,CASSANDRA-3079,,,,28/Aug/11 23:11;stuhood;0001-Properly-throw-timeouts-decrement-the-count-of-waiters.txt;https://issues.apache.org/jira/secure/attachment/12492017/0001-Properly-throw-timeouts-decrement-the-count-of-waiters.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-31 17:59:21.776,,,no_permission,,,,,,,,,,,,20967,,,Thu Sep 01 07:27:27 UTC 2011,,,,,,0|i0gfnj:,93966,kingryan,kingryan,,,,,,,,,"28/Aug/11 22:58;stuhood;0001 - Properly throw timeouts from WeightedQueue, decrement the count of waiters on timeout, fix off-by-one in taskCount, and test all of it.","31/Aug/11 17:59;kingryan;looks good, +1",01/Sep/11 03:14;jbellis;committed,"01/Sep/11 07:27;hudson;Integrated in Cassandra #1064 (See [https://builds.apache.org/job/Cassandra/1064/])
    Properly throw timeouts, decrement the count of waiters on timeout, fix off-by-one in taskCount
patch by Stu Hood; reviewed by Ryan King for CASSANDRA-3096

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163898
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/scheduler/RoundRobinScheduler.java
* /cassandra/trunk/src/java/org/apache/cassandra/scheduler/WeightedQueue.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Escape characters in CQL,CASSANDRA-2898,12513978,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,bcvisin,bcvisin,13/Jul/11 20:07,12/Mar/19 14:12,13/Mar/19 22:26,08/Aug/11 23:18,,,,,,,0,cql,,,,"When trying to get all the columns named ""fmd:"" in cqlsh you can not escape : or ;

As per Jonathan Ellis:
You can escape quotes but I don't think you can escape semicolons.

Try:
sqlsh> select 'fmd:'..'fmd;' from feeds;",,,,,,,,,,,,,,,,01/Aug/11 21:19;brandon.williams;2898.txt;https://issues.apache.org/jira/secure/attachment/12488465/2898.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-29 16:54:06.644,,,no_permission,,,,,,,,,,,,20883,,,Mon Aug 08 23:18:43 UTC 2011,,,,,,0|i0ge47:,93717,,,,,,,,,,,"29/Jul/11 16:54;brandon.williams;I don't see the problem:
{noformat}
cqlsh> create columnfamily foo (KEY text primary key);
cqlsh> update foo set 'fmd:' = 'foo' where key = 'test';
cqlsh> select * from foo where key = 'test';
  KEY | fmd: |
 test |  foo |

cqlsh> 
cqlsh> select 'fmd:' from foo where key = 'test';
 fmd: |
  foo |

cqlsh> update foo set 'fmd;' = 'foo' where key = 'test';
cqlsh> select 'fmd:'..'fmd;' from foo where key = 'test';
 fmd: | fmd; |
  foo |  foo |

cqlsh>
{noformat}","01/Aug/11 17:25;bcvisin;I did not explain exactly.  

Try 

cqlsh> update foo set 'fmd:test' = 'foo' where key = 'test';
Unmatched named substitution: 'test' not given for update foo set 'fmd:test' = 'foo' where key = 'test';

You can not escape a colon.
",01/Aug/11 21:17;brandon.williams;Patch to allow escaping colons with a backslash.,"08/Aug/11 22:59;bcvisin;Patch looks to have solved the problem.  I have been using it for about a week now, and have had no further problems.  You can escape colons by using \ (backslash).  

Thanks Brandon!",08/Aug/11 23:18;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossip teardown causes test failures,CASSANDRA-3193,12522843,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,13/Sep/11 04:28,12/Mar/19 14:12,13/Mar/19 22:26,13/Sep/11 21:38,1.0.0,,,,,,0,,,,,Just look at any recent jenkins or buildbot attempt for the semi-nonsensical NBHM.remove NPE,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-09-13 21:38:51.832,,,no_permission,,,,,,,,,,,,4029,,,Tue Sep 13 21:38:51 UTC 2011,,,,,,0|i0ggxr:,94174,,,,,,,,,,,13/Sep/11 21:38;jbellis;fixed in r1170360,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrations announce on startup attempts to set local gossip state before gossiper is started.,CASSANDRA-2638,12506881,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,11/May/11 17:37,12/Mar/19 14:12,13/Mar/19 22:26,11/May/11 20:08,0.7.6,0.8.0,,,,,0,,,,,"AbstractCassandraDemon calls MigrationManager.applyMigrations() before the gossiper is initialized (via SS.initServer()).

MM.applyMigrations tries to set the local gossip state before it is initialized via G.start().",,,,,,,,,,,,,,,,11/May/11 18:42;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-init-ep-state-when-prior-to-gossiper-start.txt;https://issues.apache.org/jira/secure/attachment/12478857/ASF.LICENSE.NOT.GRANTED--v1-0001-init-ep-state-when-prior-to-gossiper-start.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-11 19:05:06.883,,,no_permission,,,,,,,,,,,,20745,,,Fri May 13 00:52:38 UTC 2011,,,,,,0|i0gck7:,93465,,,,,,,,,,,"11/May/11 17:57;gdusbabek;java.lang.NullPointerException
	at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:883)
	at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:125)
	at org.apache.cassandra.service.MigrationManager.applyMigrations(MigrationManager.java:183)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)","11/May/11 18:45;gdusbabek;patch is for trunk, but the same thing basically needs to be done for 0.7.",11/May/11 19:05;brandon.williams;+1,"11/May/11 19:44;hudson;Integrated in Cassandra-0.7 #480 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/480/])
    initialize local ep state prior to gossip startup if needed. patch by gdusbabek, reviewed by brandonwilliams. CASSANDRA-2638
","13/May/11 00:52;hudson;Integrated in Cassandra-0.8 #103 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/103/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress will not use existing keyspaces,CASSANDRA-2580,12505741,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,brandon.williams,brandon.williams,29/Apr/11 01:43,12/Mar/19 14:12,13/Mar/19 22:26,29/Apr/11 19:44,0.8.0 beta 2,,,Legacy/Tools,,,0,,,,,"cassandra-3:/srv/cassandra# tools/stress/bin/stress -n 1 -d cassandra-2 -i 1 -o insert
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
1,1,1,0.049,0
cassandra-3:/srv/cassandra# tools/stress/bin/stress -n 1 -d cassandra-2 -i 1 -o insert
Unable to create stress keyspace: Keyspace already exists.
",,,,,,,,,,,,,,,,29/Apr/11 11:08;xedin;CASSANDRA-2580.patch;https://issues.apache.org/jira/secure/attachment/12477746/CASSANDRA-2580.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-29 11:08:07.886,,,no_permission,,,,,,,,,,,,20710,,,Mon May 02 19:54:50 UTC 2011,,,,,,0|i0gc7r:,93409,brandon.williams,brandon.williams,,,,,,,,,"29/Apr/11 11:08;xedin;System.exit(1); was added by Gary Dusbabek at 2011-04-15 12:48:15 +0000, only cassandra-0.8 and trunk are affected this patch applies for both of them.","29/Apr/11 19:44;brandon.williams;Looks like Jonathan actually added this on the 14th (Gary merged it to trunk), probably errantly while debugging something else.  Committed.","02/May/11 19:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes are not reported as alive after being marked down,CASSANDRA-2565,12505235,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,26/Apr/11 19:36,12/Mar/19 14:12,13/Mar/19 22:26,26/Apr/11 22:12,0.8.0 beta 2,,,,,,0,,,,,"To reproduce: start two nodes in foreground mode, then suspend (^Z) one of them.  Once it is marked down, resume the process (fg) and it will not be marked up again.",,,,,,,,,,,,,,,,26/Apr/11 19:48;brandon.williams;2565.txt;https://issues.apache.org/jira/secure/attachment/12477435/2565.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-26 21:32:18.978,,,no_permission,,,,,,,,,,,,20699,,,Tue Apr 26 22:56:43 UTC 2011,,,,,,0|i0gc4f:,93394,,,,,,,,,,,"26/Apr/11 19:38;brandon.williams;Looks like in the Big Gossip Refactor there was a slight regression.  We avoid notifying if there's not a major state change, however we also need to see if the node was previously dead and if so, notify.",26/Apr/11 21:32;jbellis;+1,26/Apr/11 22:12;brandon.williams;Committed.,"26/Apr/11 22:56;hudson;Integrated in Cassandra-0.8 #43 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/43/])
    Mark nodes that were previously down as alive, even without a major
state change.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-2565
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh can't decode column names/values,CASSANDRA-2505,12504678,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,19/Apr/11 16:27,12/Mar/19 14:12,13/Mar/19 22:26,19/Apr/11 17:35,0.8 beta 1,,,Legacy/CQL,Legacy/Tools,,0,cql,,,,"The way results are accessed, cqlsh is displaying the raw thrift results and not the decoded values.",,,,,,,,,,,,,,,,19/Apr/11 16:29;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2505-use-dbapi-interface-for-decoded-values.txt;https://issues.apache.org/jira/secure/attachment/12476746/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2505-use-dbapi-interface-for-decoded-values.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-19 17:15:27.41,,,no_permission,,,,,,,,,,,,20660,,,Tue Apr 19 19:05:56 UTC 2011,,,,,,0|i0gbrb:,93335,,,,,,,,,,,19/Apr/11 17:15;jbellis;+1,19/Apr/11 17:35;urandom;commmitted,"19/Apr/11 19:05;hudson;Integrated in Cassandra-0.8 #22 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/22/])
    use dbapi interface for decoded values

Patch by eevans; reviewed by jbellis for CASSANDRA-2505
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage LoadPushDown implementation causes heisenbugs,CASSANDRA-2484,12504329,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jeromatron,jeromatron,jeromatron,15/Apr/11 00:49,12/Mar/19 14:12,13/Mar/19 22:26,17/Apr/11 21:11,0.7.5,,,,,,0,hadoop,pig,,,"After pulling hair out about why weird errors were happening loading data from cassandra with seemingly irrelevant changes to the pig scripts (mostly changing the script trying to debug other problems), it looks like the weird errors were because of the implementation we currently have for LoadPushDaown in CassandraStorage.  Unless there is a good reason to implement it, I feel like we should just remove the few lines that are in there until we can spend some serious time doing an implementation of it.",,,,,,,,,,,,,,,,18/Apr/11 18:51;jeromatron;2484-trunk.txt;https://issues.apache.org/jira/secure/attachment/12476645/2484-trunk.txt,15/Apr/11 00:55;jeromatron;2484.txt;https://issues.apache.org/jira/secure/attachment/12476399/2484.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-15 20:01:10.198,,,no_permission,,,,,,,,,,,,20647,,,Mon Apr 18 18:51:21 UTC 2011,,,,,,0|i0gbn3:,93316,,,,,,,,,,,15/Apr/11 00:55;jeromatron;Removed LoadStoreFunc because it was causing heisenbugs.  Also updated JavaDocs a bit.,"15/Apr/11 20:01;brandon.williams;Can you provide the errors you encountered?  At the least, it seems like RequiredFieldResponse should essentially be a no-op.","15/Apr/11 20:09;jeromatron;What would happen was we would be troubleshooting a bug in a complicated script and if we took out part of it we would get errors with isEmpty (something we were using but unrelated at all to the change) and then investigate more and Cassandra wouldn't return anything at all but only with that particular portion of the script running.  Then someone else was going through another kind of complicated script and getting odd null pointer exceptions and traced it back to something similar.  That made us think that pig was trying to ""optimize"" or something behind the scenes.  Then we thought it might be trying to project certain data out of the column family based on the changed script.  That led us to wonder if it had something to do with the LoadPushDown - since that is called when pig thinks it can project things out of a data store/file if it doesn't need the rest.  That would explain some of the odd behavior at least.  We commented out it out like in the patch and both errors that were independent and unexplainable any other way, were gone.

I know that's kind of a round about way of going about it, but it seems like good evidence to me that something is up with it - and if it's essentially a no-op, and it works without it, then I didn't see why we wouldn't just take it out.",17/Apr/11 21:11;brandon.williams;Committed.,"17/Apr/11 21:37;hudson;Integrated in Cassandra-0.7 #436 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/436/])
    Remove LoadPushDown methods from pig storage.
Patch by Jeremy Hanna, reviewed by brandonwilliams for CASSANDRA-2484
",18/Apr/11 18:51;jeromatron;Attaching a patch for trunk - not sure why the other one didn't apply.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction fails to occur,CASSANDRA-3181,12522759,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,12/Sep/11 15:51,12/Mar/19 14:12,13/Mar/19 22:26,14/Sep/11 01:27,1.0.0,,,,,,0,compaction,,,,"Compaction just stops running at some point.  To repro, insert like 20M rows with a 1G heap and you'll get around 1k sstables.  Restarting doesn't help, you have to invoke a major to get anything to happen.",,,,,,,,,,,,,,,,14/Sep/11 13:14;jbellis;3181-2.txt;https://issues.apache.org/jira/secure/attachment/12494442/3181-2.txt,13/Sep/11 22:03;jbellis;3181.txt;https://issues.apache.org/jira/secure/attachment/12494333/3181.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-13 04:30:43.959,,,no_permission,,,,,,,,,,,,4036,,,Wed Sep 14 13:32:33 UTC 2011,,,,,,0|i0ggsf:,94150,bcoverston,bcoverston,,,,,,,,,12/Sep/11 15:51;brandon.williams;Note that this is with the default compaction strategy.,"13/Sep/11 04:30;bcoverston;Tested this a bit and I was able to reproduce the problem.

Basically what I think happened was:

# There were compactions in-flight and scheduled. 
# Something happened to halt the compactions (probably restarted the server)
# https://issues.apache.org/jira/browse/CASSANDRA-2444 got in the way

I propose one of the following: 

* add option in as originally proposed
OR
* put compactions on a timer and ask periodically `does any compaction need to be done`. Also stagger the start of the compactions by some variable amount (5-10 minutes) similar to what we did with Hinted Handoff in 0.7.

 ","13/Sep/11 17:43;slebresne;What is unclear is what did stop the compactions from happening ? Because Brandon said that restarting didn't helped, but I get from that that compaction stopped before the restart. Basically, if the only thing that happened is ""after a restart, if you don't do any inserts, no compaction happens"", then ok, that's not a big deal. But if it is indeed that ""Compaction just stops running at some point"", then there is something that we need to fix.","13/Sep/11 17:50;bcoverston;From his repro I was able to get compactions going again with a simple write/flush.

From Brandon, he's not sure how compactions stopped. There are only two scenarios where I see compactions stopping like this with the tiered compaction strategy:

# the server was restarted
# a compaction failed unexpectedly

The fact that there were no errors in the log makes #2 unlikely. I'm pretty sure that we're just looking at a problem where a server restarts and there is no more activity triggering a flush.","13/Sep/11 17:50;brandon.williams;What started all this is my attempt at running repair.  So I think repair was blocking minors, but then I ran into countless other issues (like CASSANDRA-3179) and was forced to restart.","13/Sep/11 20:09;slebresne;bq. . So I think repair was blocking minors

repair should be on its own executor now, so it shouldn't block minors.

bq. a compaction failed unexpectedly

hum, a compaction failing shouldn't stop other compactions. Otherwise this is worth fixing.

bq. I'm pretty sure that we're just looking at a problem where a server restarts and there is no more activity triggering a flush

If that's the case, then is there really much we want to do ? And even if we want, we should move that to 1.0.1. Just want to make sure this doesn't hide a real, unknown,  problem.","13/Sep/11 20:15;brandon.williams;bq. repair should be on its own executor now, so it shouldn't block minors.

Ok, then maybe I just hit one of the OOM bugs, and compaction had never fully completed.  After restarting I never did any more writes, and we know compaction won't happen at startup.

bq. If that's the case, then is there really much we want to do ? And even if we want, we should move that to 1.0.1. Just want to make sure this doesn't hide a real, unknown, problem.

I think CASSANDRA-2444 was wrong.  It should be an option, and one that is off by default.  Starting a server with 1k sstables and having nothing happen is a bit of a shock, and having no way out of it besides hacks like forcing a flush or a major isn't great.","13/Sep/11 20:16;jbellis;bq. CASSANDRA-2444 got in the way

I'm not sure what the right solution is here.  I buy the premise of 2444 that you don't necessarily want to get hammered by compaction when you're first starting up (warming up caches).  So I don't think ""check for compactions ever N seconds"" is a great policy.  But, I'm not sure ""check every N seconds, starting M minutes after startup"" is great either because it's not something a user will just guess when he's wondering ""why aren't compactions happening yet?""

Any other ideas?","13/Sep/11 22:03;jbellis;Attached patch does a couple things:

- Schedules a compaction submission for each CFS 5 minutes after startup.  My reasoning is, five minutes is (a) enough time for most caches to warm up under load and (b) when it is not, at least it is enough time to reduce the compaction i/o limit.
- removes the permanent check-for-compactions-every-3s task from leveled compaction; I don't like spinning that up for no reason, when we already kick off a check on each flush and end-of-compaction, which should be adequate.  (Every 3s for 1 CFS = every 0.0003s for 10K CFS.)
- makes Leveled getMaximal return a ""normal, leveling"" compaction, if any needs to be done, allowing users of leveldb compaction to kick things off earlier than 5m via ""nodetool compact,"" if desired","13/Sep/11 22:48;bcoverston;Was a little concerned about removing the scheduled compaction from leveldb, but the mechanics are really no different from the tiered compaction in terms of ""it will stop when it's finished"" assuming that nothing goes wrong with the running compactions.

To be (probably overly) pedantic another advantage this has is that you are essentially kicking off only a single compaction where when the server was brought down there were probably multiple compactions in flight.

+1

",14/Sep/11 01:27;jbellis;committed,14/Sep/11 13:14;jbellis;patch to keep shutdown from waiting for the compaction kickoff,14/Sep/11 13:18;slebresne;+1,14/Sep/11 13:32;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make sure we get the output schema when we output to cassandra from pig,CASSANDRA-2421,12503461,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jeromatron,jeromatron,jeromatron,05/Apr/11 21:44,12/Mar/19 14:12,13/Mar/19 22:26,05/Apr/11 21:59,,,,,,,0,contrib,hadoop,pig,,see summary,,,,,,,,,,,,,,,,05/Apr/11 21:46;jeromatron;2421.txt;https://issues.apache.org/jira/secure/attachment/12475532/2421.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-05 21:59:53.517,,,no_permission,,,,,,,,,,,,20615,,,Tue Apr 05 22:58:55 UTC 2011,,,,,,0|i0gb9z:,93257,,,,,,,,,,,"05/Apr/11 21:46;jeromatron;Added initSchema(), made the initSchema method only do its thing once per job, and changed the UDFContext stuff from ResourceSchema to CassandraStorage.",05/Apr/11 21:59;brandon.williams;Committed.,"05/Apr/11 22:58;hudson;Integrated in Cassandra-0.7 #422 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/422/])
    Optimize schema fetch/store.
Patch by Jeremy Hanna, reviewed by brandonwilliams for CASSANDRA-2421
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig output not working with 0.8.0 branch,CASSANDRA-2706,12508335,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,jeromatron,jeromatron,25/May/11 17:22,12/Mar/19 14:12,13/Mar/19 22:26,25/May/11 18:45,0.8.0,,,,,,0,pig,,,,"For some reason running a simple column family copy with pig is not writing out, though pig reports that it is successful.
Steps to reproduce on a local node:
1. Create the schema:
http://aep.appspot.com/display/VgbvdtP6QExc3OTY3HBry9ncC3k/
2. Run the following pig script (I did it with pig 0.8.0 from cdh3) using contrib/pig/bin/pig_cassandra -x local:
http://aep.appspot.com/display/PaWJkCqRGbp7CRgjt7qoyx9izN8/",,,,,,,,,,,,,,,,25/May/11 18:35;brandon.williams;2706.txt;https://issues.apache.org/jira/secure/attachment/12480444/2706.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-25 18:35:33.919,,,no_permission,,,,,,,,,,,,20779,,,Wed May 25 18:45:07 UTC 2011,,,,,,0|i0gcyn:,93530,,,,,,,,,,,"25/May/11 18:35;brandon.williams;Patch to fix timestamp setting for thrift, and remove an extra addition of non-super mutations.","25/May/11 18:41;jeromatron;+1
Fixed the case I mentioned and I tried on a simple use of from/to cassandra bag from the pygmalion stuff just to have something a little different.",25/May/11 18:45;brandon.williams;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keys index skips results,CASSANDRA-3996,12545088,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,darkdimius,darkdimius,04/Mar/12 11:03,12/Mar/19 14:12,13/Mar/19 22:26,27/Mar/12 22:25,1.1.0,,,,,,0,,,,,"While scanning results page if range index meets result already seen in previous result set it decreases columnsRead that causes next iteration to treat columsRead<rowsPerQuery as if last page was not full and scan is done.
",,,,,,,,,,,,,,,,27/Mar/12 22:06;xedin;CASSANDRA-3996-v2.patch;https://issues.apache.org/jira/secure/attachment/12520201/CASSANDRA-3996-v2.patch,27/Mar/12 18:06;xedin;CASSANDRA-3996.patch;https://issues.apache.org/jira/secure/attachment/12520163/CASSANDRA-3996.patch,12/Mar/12 07:25;darkdimius;KeysSearcher_fix_and_refactor-v2.patch;https://issues.apache.org/jira/secure/attachment/12517990/KeysSearcher_fix_and_refactor-v2.patch,04/Mar/12 11:20;darkdimius;KeysSearcher_fix_and_refactor.patch;https://issues.apache.org/jira/secure/attachment/12516991/KeysSearcher_fix_and_refactor.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-03-13 11:15:22.428,,,no_permission,,,,,,,,,,,,230274,,,Tue Mar 27 22:25:37 UTC 2012,,,,,,0|i0gqr3:,95764,jbellis,jbellis,,,,,,,,,"04/Mar/12 11:20;darkdimius;This patch also refactors keysSearcher for more generic way. It creates method nextIndexKey for generic index searchers that is suitable for indexes that return results from multiple indexCfs rows.

Created in Wikimart.ru",12/Mar/12 07:25;darkdimius;git diff output,"13/Mar/12 11:15;xedin;Committed with CodeStyle refactoring, thanks!",13/Mar/12 15:00;jbellis;So...  is this a real bug fix?  We should add a test case if so and commit to 1.1.0 if not 1.0.9.,"13/Mar/12 15:18;xedin;yeah, this is a real thing. Dmitry, can you work on the test-case? I have already committed it to 1.1.1, should I revert?",13/Mar/12 15:58;jbellis;it may or may not be easier to merge forwards if we revert the current changes first.  hard to say with git. :),16/Mar/12 10:23;slebresne;imho it's easier to revert and recommit to the right versions if only to ensure changelog entries ends up where they should.,19/Mar/12 15:18;jbellis;Reverted from 1.1 and trunk,"27/Mar/12 18:06;xedin;This bug was introduced by (3297a96e1849f41d9c61f024282ed52d642e0794 - Merge get_indexed_slices with get_range_slices) in 1.1.0 and does not affect 1.0.x versions, attached patch adds test along side with updated code.",27/Mar/12 22:06;xedin;v2 with actual change to the KeysSearcher.getIndexedIterator method (no refactoring) and test-case.,"27/Mar/12 22:12;jbellis;+1 on v2

I'm not necessarily against doing the rest of this refactor in another ticket, since -- and I think this is the goal -- it sets us up to do a merge of results from multiple indexes), but I'd like to see more comments describing the code organization for a relatively large change like that.  (From a quick look, it also looks like changing ""return endOfData"" to ""resetState"" is unnecessary -- I prefer the former since it makes it clear that once you get to this point in the code, the iterator is finished.)","27/Mar/12 22:25;xedin;Committed v2. Agree with Jonathan, such refactoring could be done in the separate task. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra code base has files with import statements having ""*"" causing compilation failure",CASSANDRA-3965,12544339,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,,harishd,harishd,27/Feb/12 23:22,12/Mar/19 14:12,13/Mar/19 22:26,29/Feb/12 06:58,,,,,,,0,,,,,"I tried to download a jar as part of the new unit test I am writing. I ran my unit test successfully but later if I run ""ant"" without ant clean, I run into the following compilation issue

{code}
build-project:
     [echo] apache-cassandra: /Users/harish/workspace/cassandra/build.xml
    [javac] Compiling 40 source files to /Users/harish/workspace/cassandra/build/classes/thrift
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] Compiling 568 source files to /Users/harish/workspace/cassandra/build/classes/main
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:1607: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Iterable<DecoratedKey<?>> keySamples(Range<Token> range)
    [javac]                                                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:196: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Collection<Range<Token>> getLocalRanges(String table)
    [javac]                       ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:201: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Range<Token> getLocalPrimaryRange()
    [javac]            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:912: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Map<Range<Token>, List<InetAddress>> getRangeToAddressMap(String keyspace)
    [javac]                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1009: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Map<Range<Token>, List<InetAddress>> constructRangeToEndpointMap(String keyspace, List<Range<Token>> ranges)
    [javac]                                                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1009: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Map<Range<Token>, List<InetAddress>> constructRangeToEndpointMap(String keyspace, List<Range<Token>> ranges)
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1438: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1438: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1554: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<Range<Token>, InetAddress> getChangedRangesForLeaving(String table, InetAddress endpoint)
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1975: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public AntiEntropyService.RepairFuture forceTableRepair(final Range<Token> range, final String tableName, final String... columnFamilies) throws IOException
    [javac]                                                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2017: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Range<Token> getPrimaryRangeForEndpoint(InetAddress ep)
    [javac]            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2027: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     Collection<Range<Token>> getRangesForEndpoint(String table, InetAddress ep)
    [javac]                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2038: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public List<Range<Token>> getAllRanges(List<Token> sortedTokens)
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2128: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public List<Token> getSplits(String table, String cfName, Range<Token> range, int keysPerSplit)
    [javac]                                                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2152: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private List<DecoratedKey> keySamples(Iterable<ColumnFamilyStore> cfses, Range<Token> range)
    [javac]                                                                              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2762: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private CountDownLatch streamRanges(final Map<String, Multimap<Range<Token>, InetAddress>> rangesToStreamByTable)
    [javac]                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2816: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private CountDownLatch requestRanges(final Map<String, Multimap<InetAddress, Range<Token>>> ranges)
    [javac]                                                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                                                                                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                                                                                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                     ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:1607: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Iterable<DecoratedKey<?>> keySamples(Range<Token> range)
    [javac]                                                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:196: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Collection<Range<Token>> getLocalRanges(String table)
    [javac]                       ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:201: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Range<Token> getLocalPrimaryRange()
    [javac]            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:912: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Map<Range<Token>, List<InetAddress>> getRangeToAddressMap(String keyspace)
    [javac]                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1009: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Map<Range<Token>, List<InetAddress>> constructRangeToEndpointMap(String keyspace, List<Range<Token>> ranges)
    [javac]                                                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1009: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Map<Range<Token>, List<InetAddress>> constructRangeToEndpointMap(String keyspace, List<Range<Token>> ranges)
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1438: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1438: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1554: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<Range<Token>, InetAddress> getChangedRangesForLeaving(String table, InetAddress endpoint)
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1975: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public AntiEntropyService.RepairFuture forceTableRepair(final Range<Token> range, final String tableName, final String... columnFamilies) throws IOException
    [javac]                                                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2017: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Range<Token> getPrimaryRangeForEndpoint(InetAddress ep)
    [javac]            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2027: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     Collection<Range<Token>> getRangesForEndpoint(String table, InetAddress ep)
    [javac]                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2038: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public List<Range<Token>> getAllRanges(List<Token> sortedTokens)
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2128: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public List<Token> getSplits(String table, String cfName, Range<Token> range, int keysPerSplit)
    [javac]                                                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2152: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private List<DecoratedKey> keySamples(Iterable<ColumnFamilyStore> cfses, Range<Token> range)
    [javac]                                                                              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2762: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private CountDownLatch streamRanges(final Map<String, Multimap<Range<Token>, InetAddress>> rangesToStreamByTable)
    [javac]                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2816: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private CountDownLatch requestRanges(final Map<String, Multimap<InetAddress, Range<Token>>> ranges)
    [javac]                                                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                                                                                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                                                                                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                     ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:393: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 Range range = new Range<Token>(token, token);
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:393: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 Range range = new Range<Token>(token, token);
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:1328: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         assert !(range instanceof Range) || !((Range)range).isWrapAround() || range.right.isMinimum() : range;
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:1328: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         assert !(range instanceof Range) || !((Range)range).isWrapAround() || range.right.isMinimum() : range;
    [javac]                                                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:853: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<Range<Token>,List<InetAddress>> entry : getRangeToAddressMap(keyspace).entrySet())
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:884: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<Range<Token>, List<InetAddress>> entry : getRangeToAddressMap(keyspace).entrySet())
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:904: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<Range<Token>, Collection<InetAddress>> entry : tokenMetadata_.getPendingRanges(keyspace).entrySet())
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:919: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         List<Range<Token>> ranges = getAllRanges(tokenMetadata_.sortedTokens());
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:959: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<Range<Token>, List<InetAddress>> entry : getRangeToAddressMap(keyspace).entrySet())
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:961: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Range range = entry.getKey();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1011: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<Range<Token>, List<InetAddress>> rangeToEndpointMap = new HashMap<Range<Token>, List<InetAddress>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1011: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<Range<Token>, List<InetAddress>> rangeToEndpointMap = new HashMap<Range<Token>, List<InetAddress>>();
    [javac]                                                                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1012: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1356: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<Range<Token>, InetAddress> pendingRanges = HashMultimap.create();
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1368: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<InetAddress, Range<Token>> addressRanges = strategy.getAddressRanges();
    [javac]                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1374: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> affectedRanges = new HashSet<Range<Token>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1374: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> affectedRanges = new HashSet<Range<Token>>();
    [javac]                                                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1380: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : affectedRanges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1399: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1417: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1441: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<Range<Token>, InetAddress> rangeAddresses = Table.open(table).getReplicationStrategy().getRangeAddresses(tokenMetadata_);
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1442: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<InetAddress, Range<Token>> sourceRanges = HashMultimap.create();
    [javac]                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1446: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1507: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<String, Map.Entry<InetAddress, Collection<Range<Token>>>> rangesToFetch = HashMultimap.create();
    [javac]                                                            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1513: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> changedRanges = getChangedRangesForLeaving(table, endpoint);
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1514: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Set<Range<Token>> myNewRanges = new HashSet<Range<Token>>();
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1514: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Set<Range<Token>> myNewRanges = new HashSet<Range<Token>>();
    [javac]                                                         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1515: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Map.Entry<Range<Token>, InetAddress> entry : changedRanges.entries())
    [javac]                            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1520: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<InetAddress, Range<Token>> sourceRanges = getNewSourceRanges(table, myNewRanges);
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1521: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Map.Entry<InetAddress, Collection<Range<Token>>> entry : sourceRanges.asMap().entrySet())
    [javac]                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1530: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Map.Entry<InetAddress, Collection<Range<Token>>> entry : rangesToFetch.get(table))
    [javac]                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1533: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 Collection<Range<Token>> ranges = entry.getValue();
    [javac]                            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1557: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Collection<Range<Token>> ranges = getRangesForEndpoint(table, endpoint);
    [javac]                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1562: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<Range<Token>, List<InetAddress>> currentReplicaEndpoints = new HashMap<Range<Token>, List<InetAddress>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1562: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<Range<Token>, List<InetAddress>> currentReplicaEndpoints = new HashMap<Range<Token>, List<InetAddress>>();
    [javac]                                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1565: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1575: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<Range<Token>, InetAddress> changedRanges = HashMultimap.create();
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1582: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1916: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Collection<Range<Token>> ranges = getLocalRanges(tableName);
    [javac]                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1921: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2045: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         List<Range<Token>> ranges = new ArrayList<Range<Token>>();
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2045: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         List<Range<Token>> ranges = new ArrayList<Range<Token>>();
    [javac]                                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2049: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Range<Token> range = new Range<Token>(sortedTokens.get(i - 1), sortedTokens.get(i));
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2049: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Range<Token> range = new Range<Token>(sortedTokens.get(i - 1), sortedTokens.get(i));
    [javac]                                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2052: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Range<Token> range = new Range<Token>(sortedTokens.get(size - 1), sortedTokens.get(0));
    [javac]         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2052: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Range<Token> range = new Range<Token>(sortedTokens.get(size - 1), sortedTokens.get(0));
    [javac]                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2164: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Range<Token> range = getLocalPrimaryRange();
    [javac]         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2253: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<Range<Token>, InetAddress>> rangesToStream = new HashMap<String, Multimap<Range<Token>, InetAddress>>();
    [javac]                              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2253: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<Range<Token>, InetAddress>> rangesToStream = new HashMap<String, Multimap<Range<Token>, InetAddress>>();
    [javac]                                                                                                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2257: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangesMM = getChangedRangesForLeaving(table, FBUtilities.getBroadcastAddress());
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2323: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<InetAddress, Range<Token>>> rangesToFetch = new HashMap<String, Multimap<InetAddress, Range<Token>>>();
    [javac]                                           ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2323: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<InetAddress, Range<Token>>> rangesToFetch = new HashMap<String, Multimap<InetAddress, Range<Token>>>();
    [javac]                                                                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2324: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<Range<Token>, InetAddress>> rangesToStreamByTable = new HashMap<String, Multimap<Range<Token>, InetAddress>>();
    [javac]                              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2324: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<Range<Token>, InetAddress>> rangesToStreamByTable = new HashMap<String, Multimap<Range<Token>, InetAddress>>();
    [javac]                                                                                                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2336: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Collection<Range<Token>> currentRanges = getRangesForEndpoint(table, localAddress);
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2338: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Collection<Range<Token>> updatedRanges = strategy.getPendingAddressRanges(tokenMetadata_, newToken, localAddress);
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2342: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangeAddresses = strategy.getRangeAddresses(tokenMetadata_);
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2345: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Pair<Set<Range<Token>>, Set<Range<Token>>> rangesPerTable = calculateStreamAndFetchRanges(currentRanges, updatedRanges);
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2345: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Pair<Set<Range<Token>>, Set<Range<Token>>> rangesPerTable = calculateStreamAndFetchRanges(currentRanges, updatedRanges);
    [javac]                                         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2351: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangesToFetchWithPreferredEndpoints = ArrayListMultimap.create();
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2352: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Range<Token> toFetch : rangesPerTable.right)
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2354: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 for (Range<Token> range : rangeAddresses.keySet())
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2367: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangeWithEndpoints = HashMultimap.create();
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2369: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Range<Token> toStream : rangesPerTable.left)
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2379: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<InetAddress, Range<Token>> workMap = RangeStreamer.getWorkMap(rangesToFetchWithPreferredEndpoints);
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2507: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> changedRanges = getChangedRangesForLeaving(table, endpoint);
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2765: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<String, Multimap<Range<Token>, InetAddress>> entry : rangesToStreamByTable.entrySet())
    [javac]                                         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2767: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangesWithEndpoints = entry.getValue();
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2777: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             final Set<Map.Entry<Range<Token>, InetAddress>> pending = new HashSet<Map.Entry<Range<Token>, InetAddress>>(rangesWithEndpoints.entries());
    [javac]                                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2777: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             final Set<Map.Entry<Range<Token>, InetAddress>> pending = new HashSet<Map.Entry<Range<Token>, InetAddress>>(rangesWithEndpoints.entries());
    [javac]                                                                                             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2779: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (final Map.Entry<Range<Token>, InetAddress> endPointEntry : rangesWithEndpoints.entries())
    [javac]                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2781: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 final Range<Token> range = endPointEntry.getKey();
    [javac]                       ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2819: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<String, Multimap<InetAddress, Range<Token>>> entry : ranges.entrySet())
    [javac]                                                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2821: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<InetAddress, Range<Token>> endpointWithRanges = entry.getValue();
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2835: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 Collection<Range<Token>> toFetch = endpointWithRanges.get(source);
    [javac]                            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2868: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> toStream = new HashSet<Range<Token>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2868: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> toStream = new HashSet<Range<Token>>();
    [javac]                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2869: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> toFetch  = new HashSet<Range<Token>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2869: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> toFetch  = new HashSet<Range<Token>>();
    [javac]                                                  ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 100 errors

BUILD FAILED
/Users/harish/workspace/cassandra/build.xml:704: Compile failed; see the compiler error output for details.


{code}

I think the jar that I imported as part of unit tests has further some dependencies. After looking into some of the sources files I see that some files like ColumnFamilyStore.java have statements like 

{code}
import com.google.common.collect.*;
import org.apache.cassandra.dht.*;
{code}

It seems like if instead of importing ""*"" if we import the exact class, we will not run into this compilation failure. I would be happy to patch this if somebody could approve this?",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-02-28 01:45:23.654,,,no_permission,,,,,,,,,,,,229576,,,Wed Feb 29 04:29:58 UTC 2012,,,,,,0|i0gqen:,95708,,,,,,,,,,,"28/Feb/12 01:45;dbrosius@apache.org;I think you are using a newer version of guava (> r08) that has conflicts with the class Range.


I believe this is already fixed in trunk.

",28/Feb/12 17:45;harishd;Yes seems like it is fixed in trunk. Thanks I verified it. I could not find any ticket filed in the jira for this issue?,"28/Feb/12 20:26;dbrosius@apache.org;Yeah there wasn't one, it went in with https://issues.apache.org/jira/browse/CASSANDRA-3949",29/Feb/12 04:29;harishd;Cool. Thanks that helps. I am resolving the issue for now.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avoid distributed deadlock in migration stage,CASSANDRA-3882,12541956,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,richardlow,scode,scode,09/Feb/12 10:36,12/Mar/19 14:12,13/Mar/19 22:26,01/Jun/12 10:56,1.1.1,,,,,,0,,,,,"This is follow-up work for the remainders of CASSANDRA-3832 which was only a partial fix. The deadlock in the migration stage needs to be fixed, as it can cause bootstrap (at least) to take potentially a very very long time to complete, and might also cause a lack of schema propagation until otherwise ""poked"".",,,,,,,,,,,,CASSANDRA-3832,,,,21/May/12 12:00;richardlow;CASSANDRA-3882-async.patch;https://issues.apache.org/jira/secure/attachment/12528412/CASSANDRA-3882-async.patch,10/May/12 03:33;scode;CASSANDRA-3882-hack.txt;https://issues.apache.org/jira/secure/attachment/12526291/CASSANDRA-3882-hack.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-05-10 04:31:03.507,,,no_permission,,,,,,,,,,,,227243,,,Fri Jun 01 10:56:42 UTC 2012,,,,,,0|i0gpdr:,95542,slebresne,slebresne,,,,,,,,,"10/May/12 03:33;scode;Attaching a pure hack that works around this on large clusters (schema migrations are essentially impossible/useless on large clusters without it).

It is not a clean solution, but it was very fast to hack together, should be very safe, and solves our burning problem. It simply spreads out migration tasks slowly over time so that the probability of triggering deadlocks becomes vastly smaller. (We're also running with a hack to make the timeout on migration messages be 500 ms.)

You'll tend to see schema converging very quickly, and then see a flurry of secondly memtable flushes as it processes rectification in the background, even though they don't actually ""do"" anything to the schema.
","10/May/12 03:36;scode;Let me clarify: I'm not suggesting this be committed. I am merely posting it so that others in this position may be helped. Unfortunately I do not have the time to work on a clean solution, but I wanted to at least share the hack instead of sitting on it.",10/May/12 04:31;jbellis;Remind me what problem we're solving post-3832?,"10/May/12 05:33;scode;CASSANDRA-3832 avoided the *gossip* stage being backed up as a result of distributed migration manager deadlock, by having the schema rectification be executed asynchronously with respect to the gossip stage. This allowed bootstrap to complete, among other things (""other things"" being anything blocking on gossip stage getting to execute incoming tasks).

As indicated in CASSANDRA-3832 (https://issues.apache.org/jira/browse/CASSANDRA-3832?focusedCommentId=13201012&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13201012) there was still the underlying distributed deadlock (or almost deadlock, since timeouts allow it to eventually complete) in the migration stage itself.

The posted hack is band-aid for that problem. We're still doing useless work performing migration:s beyond what is required, and it doesn't *eliminate* the problem - just makes it vastly less likely.","10/May/12 14:17;jbellis;Quoting from 3832: ""[the deadlock] is because all nodes, when a node is marked alive, just know that it has a different schema - not who has the ""newer"" schema. So when a node joins it gets migration messages from others while it also tries to send migration messages to others and waiting on the response. Whenever it sends a migration message to someone whose migration stage is busy waiting on a response from the node in question - deadlock (until timeout).""","10/May/12 15:55;scode;I'm not sure what your intention is with the quote. If you mean that it implies it's only during bootstrap, it's because I was only considering bootstrap at the time and didn't think far enough to realize this would be a problem in general for schema changes.

The empirical observation is that all nodes are stuck with lots of migrationstage pending, schema requests time out, schema updates at CLI sit there forever, and it very very very slowly recovers since each guy is waiting on RPC timeout (depending on said timeout).

jstacking shows waiting in the migration stage on that future, for a response.
","10/May/12 16:17;jbellis;My intention was only to clarify what problem we're trying to solve here, so everyone doesn't have to xref w/ 3832.",11/May/12 22:36;jbellis;Why do we have block-for-response at all?  Would anything break if we just made the response processing asynchronous?,"21/May/12 11:59;richardlow;I have observed this problem too, and have implemented and tested Jonathan's suggestion of processing the response asynchronously.  I have a simple reproduction that works about 50% of the time:

1. Create a two node cluster (nodes A and B)
2. Create a keyspace
3. Take down A and wipe its data and commit log directories
4. Bring up A
5. A never learns about the keyspace

The issue is that when A restarts, it sends a MIGRATION_REQUEST message to B.  But B also sends the message to A at about the same time.  Since the processing of this is done synchronously in MigrationTask, the single thread on the MigrationStage is blocked on both A and B.  The messages timeout and retry, and after 30 seconds it gives up.

The simple solution is to process the response asynchronously - the patch CASSANDRA-3882-async.patch makes this change.  With this, node A always learns about the new keyspace very quickly after restarting.

As a general rule, only async processing should be done on stages, otherwise distributed deadlocks are very likely.

Peter, is this the same issue you were seeing?

I can't think of anything that should break doing this asynchronously - DefsTable.mergeSchema is static synchronized so it looks safe.","23/May/12 02:48;scode;Yes, that is the issue I'm seeing. And I completely agree about synchronous (with respect to other nodes) operations on stages.

I also agree that I can't think of why specifically this would be unsafe to do asynchronously; I was just taking the ultra-conservative approach.

Looked through the patch very briefly and it seems reasonable, but I haven't looked at it carefully or tested it. One thing I'm concerned with for large clusters is the total number of messages back and fourth, since I believe it ends up being quadratic in cluster size. That said, maybe that's okay up to at least several hundred nodes.","23/May/12 02:49;scode;I will try to look at it more closely/test it, but if I can't make it by thursday or so it's likely to not happen until a week or two later.","01/Jun/12 10:56;slebresne;Looks good to me, +1. Committed, thanks.

bq. One thing I'm concerned with for large clusters is the total number of messages back and fourth

That's a separate problem so let's leave that for some other ticket.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hints are not replayed unless node was marked down,CASSANDRA-3554,12533568,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,02/Dec/11 05:46,12/Mar/19 14:12,13/Mar/19 22:26,22/Dec/11 21:34,1.0.7,,,,,,1,hintedhandoff,jmx,,,"If B drops a write from A because it is overwhelmed (but not dead), A will hint the write.  But it will never get notified that B is back up (since it was never down), so it will never attempt hint delivery.",,,,,,,,,,,,,,,,15/Dec/11 04:53;jbellis;0001-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12507477/0001-cleanup.patch,15/Dec/11 04:47;jbellis;0002-deliver.patch;https://issues.apache.org/jira/secure/attachment/12507476/0002-deliver.patch,22/Dec/11 20:29;jbellis;3554-1.0-v2.txt;https://issues.apache.org/jira/secure/attachment/12508433/3554-1.0-v2.txt,19/Dec/11 23:34;jbellis;3554-1.0.txt;https://issues.apache.org/jira/secure/attachment/12507999/3554-1.0.txt,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-12-02 08:40:15.931,,,no_permission,,,,,,,,,,,,219296,,,Wed Mar 21 11:59:05 UTC 2012,,,,,,0|i0glc7:,94887,brandon.williams,brandon.williams,,,,,,,,,"02/Dec/11 05:48;jbellis;Unclear how we should tell when it's a good idea to re-attempt delivery in this scenario.

Possibly the best solution is to just make FD smarter and mark nodes as ""effectively down"" in this situation.  A ""local"" FD as in CASSANDRA-3533 could address this.","02/Dec/11 08:40;slebresne;Couldn't B handles this? When it drops writes from A, it could record it. Then B could have a scheduled tasks that looks for locally dropped writes and decide if it's ok to get hints based on the mutation stage queue. It could then request the hint delivery from A.","02/Dec/11 14:11;jbellis;That could work, but I cringe at adding both ""pull"" and ""push"" modes for hint delivery, which has historically been a source of enough bugs that more complexity is counterindicated.","02/Dec/11 16:07;brandon.williams;At risk of heresy, would bringing the hourly scan back be so bad now that our hint model doesn't suck like it did the first time we did that? ","02/Dec/11 16:11;tjake;Can we keep a running tally of hints per endpoint when they are written, when they reach a threshold we deliver them? + hourly scan :)","02/Dec/11 16:31;jbellis;The problem I have with a ""brute force"" hourly scan or hint threshold is that you're likely to run into the same overload scenario that caused the hinting in the first place.",03/Dec/11 00:27;appodictic;Could we use the badness detector in dynamic switch?,"05/Dec/11 15:22;tjake;The other problem is even if we fix the replay issue it's still terribly slow due to excessive throttling

I like the idea of changing from a push to pull mode for hint delivery. Similar to how mysql replication is client pull.  Clients know how swamped they are and can throttle their own delivery.


","05/Dec/11 17:03;jbellis;The problem with a pull model is that the node doing the pulling usually won't know ""I was down, therefore I should ask for hints.""  (The exception is on restart, but hints due to overload conditions or GC pauses are much more common.)","05/Dec/11 17:07;tjake;Right, however its never going to know if hints are on a coordinator node due to the coordinator needed to drop some messages (backpressure?)

So either the clients can poll all nodes slowly and fetch hints or we perhaps gossip hints available flag so nodes know when hints are there to read?","06/Dec/11 01:33;jbellis;Right.  So, not clearly simpler than doing something to our existing push model, but much more code churn.  I'd rather stick with push.","07/Dec/11 16:19;appodictic;How expensive is the process of ""1) Wake Up. 2)Check for hints. 3) try to deliver them."" As for parts 1 & 2 I would think we can do this more often then hourly, maybe a background thread that sleeps for N seconds and attempts again.  

{quote}
The other problem is even if we fix the replay issue it's still terribly slow due to excessive throttling{quote} Side note. This throttle can not currently be adjusted at runtime. This should be JMX able. The default may be too low. Historically the problem was the hint sending node got hammered. In my mind the throttle was protecting that system.  ","15/Dec/11 04:47;jbellis;Brute force fix attached to check for hints-to-deliver every 10 minutes.  If a hint cannot be replayed because of timing out, we abort (to that target).  Reduces default hint throttle delay to 1ms.","15/Dec/11 04:52;jbellis;0001 does some related cleanup, including moving the mbean method StorageService.deliverHints to HHOM.scheduleHintDelivery.",19/Dec/11 22:57;jbellis;combined patch against 1.0,19/Dec/11 23:34;jbellis;updated 1.0 rebase that is pre-1034 friendly,"20/Dec/11 12:06;brandon.williams;I'm not sure how exactly, but obviously the keys being passed here are not quite what we think they are:

{noformat}
DEBUG 11:57:03,907 Started scheduleAllDeliveries
DEBUG 11:57:03,907 deliverHints to /7fff:ffff:ffff:ffff:ffff:ffff:ffff:fffe
DEBUG 11:57:03,908 deliverHints to /5555:5555:5555:5555:5555:5555:5555:5554
DEBUG 11:57:03,908 Checking remote(/7fff:ffff:ffff:ffff:ffff:ffff:ffff:fffe) schema before delivering hints
DEBUG 11:57:03,908 Finished scheduleAllDeliveries
ERROR 11:57:03,909 Fatal exception in thread Thread[HintedHandoff:3,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.waitForSchemaAgreement(HintedHandOffManager.java:206)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:238)
        at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:383)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}
","22/Dec/11 20:29;jbellis;You're right, we switched from InetAddress as the key to Tokens.  v2 attached.",22/Dec/11 21:18;brandon.williams;+1,22/Dec/11 21:34;jbellis;committed,"21/Mar/12 11:59;jonma;+1 
""shedule deliver hints"" should have been more ealier brought in .
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit Log Allocator deadlock after first start with empty commitlog directory,CASSANDRA-3543,12533334,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,rbranson,brandon.williams,brandon.williams,30/Nov/11 19:22,12/Mar/19 14:12,13/Mar/19 22:26,02/Dec/11 22:10,1.1.0,,,,,,0,,,,,"While testing CASSANDRA-3541 at some point stress completely timed out.  I proceeded to shut the cluster down and 2/3 JVMs hang infinitely.  After a while, one of them logged:

{noformat}
WARN 19:07:50,133 Some hints were not written before shutdown.  This is not supposed to happen.  You should (a) run repair, and (b) file a bug report
{noformat}",,,,,,,,,,,,,,,,02/Dec/11 20:55;rbranson;3543.txt;https://issues.apache.org/jira/secure/attachment/12505929/3543.txt,30/Nov/11 19:23;brandon.williams;hung_stack.txt;https://issues.apache.org/jira/secure/attachment/12505649/hung_stack.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-30 20:31:28.233,,,no_permission,,,,,,,,,,,,219062,,,Fri Dec 02 23:17:56 UTC 2011,,,,,,0|i0gl73:,94864,jbellis,jbellis,,,,,,,,,30/Nov/11 19:23;brandon.williams;Here is a thread dump.,30/Nov/11 20:31;rbranson;This is a bug in the new commit log allocator from #3411. The MutationStage threads are all blocked because the CommitLogExecutor queue is full. The COMMIT-LOG-WRITER thread which drains this queue is blocking on fetchSegment() which waits on CommitLogAllocator to push newly created segments onto this queue. Brandon also stated that only 1 commit log segment existed afterwards.,"30/Nov/11 22:12;rbranson;Steps to reproduce:

1) $ rm -rf /var/lib/cassandra/*
2) Start Cassandra
3) $ stress -F1 -n 999999999

The stress will run until requests start timing out, on my box it was ~400,000.",30/Nov/11 22:38;rbranson;A workaround is to restart Cassandra immediately after it comes up the first time with an empty commitlog directory.,02/Dec/11 22:10;jbellis;committed,"02/Dec/11 23:17;hudson;Integrated in Cassandra #1234 (See [https://builds.apache.org/job/Cassandra/1234/])
    enableReserveSegmentCreation even when there is nothing to replay
patch by Rick Branson; reviewed by jbellis for CASSANDRA-3543

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1209724
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nodes can get stuck in UP state forever, despite being DOWN",CASSANDRA-3626,12535056,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,scode,scode,13/Dec/11 23:18,12/Mar/19 14:12,13/Mar/19 22:26,15/Dec/11 19:15,0.8.10,1.0.7,,,,,0,,,,,"This is a proposed phrasing for an upstream ticket named ""Newly discovered nodes that are down get stuck in UP state forever"" (will edit w/ feedback until done):

We have a observed a problem with gossip which, when you are bootstrapping a new node (or replacing using the replace_token support), any node in the cluster which is Down at the time the node is started, will be assumed to be Up and then *never ever* flapped back to Down until you restart the node.

This has at least two implications to replacing or bootstrapping new nodes when there are nodes down in the ring:

* If the new node happens to select a node listed as (UP but in reality is DOWN) as a stream source, streaming will sit there hanging forever.
* If that doesn't happen (by picking another host), it will instead finish bootstrapping correctly, and begin servicing requests all the while thinking DOWN nodes are UP, and thus routing requests to them, generating timeouts.

The way to get out of this is to restart the node(s) that you bootstrapped.

I have tested and confirmed the symptom (that the bootstrapped node things other nodes are Up) using a fairly recent 1.0. The main debugging effort happened on 0.8 however, so all details below refer to 0.8 but are probably similar in 1.0.

Steps to reproduce:

* Bring up a cluster of >= 3 nodes. *Ensure RF is < N*, so that the cluster is operative with one node removed.
* Pick two random nodes A, and B. Shut them *both* off.
* Wait for everyone to realize they are both off (for good measure).
* Now, take node A and nuke it's data directories and re-start it, such that it comes up w/ normal bootstrap (or use replace_token; didn't test that but should not affect it).
* Watch how node A starts up, all the while believing node B is down, even though all other nodes in the cluster agree that B is down and B is in fact still turned off.

The mechanism by which it initially goes into Up state is that the node receives a gossip response from any other node in the cluster, and GossipDigestAck2VerbHandler.doVerb() calls Gossiper.applyStateLocally().

Gossiper.applyStateLocally() doesn't have any local endpoint state for the cluster, so the else statement at the end (""it's a new node"") gets triggered and handleMajorStateChange() is called. handleMajorStateChange() always calls markAlive(), unless the state is a dead state (but ""dead"" here does not mean ""not up"", but refers to joining/hibernate etc).

So at this point the node is up in the mind of the node you just bootstrapped.

Now, in each gossip round doStatusCheck() is called, which iterates over all nodes (including the one falsly Up) and among other things, calls FailureDetector.interpret() on each node.

FailureDetector.interpret() is meant to update its sense of Phi for the node, and potentially convict it. However there is a short-circuit at the top, whereby if we do not yet have any arrival window for the node, we simply return immediately.

Arrival intervals are only added as a result of a FailureDetector.report() call, which never happens in this case because the initial endpoint state we added, which came from a remote node that was up, had the latest version of the gossip state (so Gossiper.reportFailureDetector() will never call report()).

The result is that the node can never ever be convicted.

Now, let's ignore for a moment the problem that a node that is actually Down will be thought to be Up temporarily for a little while. That is sub-optimal, but let's aim for a fix to the more serious problem in this ticket - which is that is stays up forever.

Considered solutions:

* When interpret() gets called and there is no arrival window, we could add a faked arrival window far back in time to cause the node to have history and be marked down. This ""works"" in the particular test case. The problem is that since we are not ourselves actively trying to gossip to these nodes with any particular speed, it might take a significant time before we get any kind of confirmation from someone else that it's actually Up in cases where the node actually *is* Up, so it's not clear that this is a good idea.

* When interpret() gets called and there is no arrival window, we can simply convict it immediately. This has roughly similar behavior as the previous suggestion.

* When interpret() gets called and there is no arrival window, we can add a faked arrival window at the current time, which will allow it to be treated as Up until the usual time has passed before we exceed the Phi conviction threshold.

* When interpret() gets called and there is no arrival window, we can immediately convict it, *and* schedule it for immediate gossip on the next round in order to try to ensure they go Up quickly if they are indeed up. This has an effect of O(n) gossip traffic, as a special case once during node start-up. While theoretically a problem, I personally thing we can ignore it for now since it won't be a significant problem any time soon. However, this is more complicated since the way we queue up messages is asynchronously to background connection attempts. We'd have to make sure the initial gossip message actually gets sent on an open TCP connection (I haven't confirmed whether this will be the case or not).

The first three are simple to implement, possibly the fourth. But in all cases, I am worried about potential negative consequences that I am not seeing.

Thoughts?
",,,,,,,,,,,,,,,,14/Dec/11 19:37;brandon.williams;3626.txt;https://issues.apache.org/jira/secure/attachment/12507408/3626.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-14 19:37:41.463,,,no_permission,,,,,,,,,,,,220740,,,Thu Dec 15 22:28:47 UTC 2011,,,,,,0|i0gm7j:,95028,scode,scode,,,,,,,,,"14/Dec/11 19:37;brandon.williams;bq. Now, let's ignore for a moment the problem that a node that is actually Down will be thought to be Up temporarily for a little while.

This is perfectly fine since this is what the SS.RING_DELAY stabilization period is designed to suss out - the correct ring state.

bq. Steps to reproduce:

This can be simplified to ""start two nodes, shut one down, add a third.""

bq. Considered solutions:

ISTM the simplest and most correct thing to do in this case is report() new nodes.  Patch to do so.",14/Dec/11 19:38;brandon.williams;FWIW I went back to 0.7 to repro this and the bug is there - so we've had this for quite some time.,"15/Dec/11 05:37;scode;I agree off the top of my head, but I'm kinda suspicious as to why I didn't already arrive at that conclusion... I'll have another look at the code tomorrow and see if I can figure out some reason why this is not a good idea, but right now it seems like a +1 to me.",15/Dec/11 18:56;scode;+1. :),15/Dec/11 19:15;brandon.williams;Committed.,"15/Dec/11 22:28;hudson;Integrated in Cassandra-0.8 #419 (See [https://builds.apache.org/job/Cassandra-0.8/419/])
    Prevent new nodes from thinking down nodes are up forever.
Patch by brandonwilliams, reviewed by Peter Schuller for CASSANDRA-3626

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1214916
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing null check in CQL QueryProcessor,CASSANDRA-3473,12530737,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,08/Nov/11 17:12,12/Mar/19 14:12,13/Mar/19 22:26,08/Nov/11 18:01,1.0.3,,,,,,0,CQL,,,,,,,,,,,,,,,,,,,,08/Nov/11 17:13;slebresne;0001-Missing-null-check-in-QueryProcessor.patch;https://issues.apache.org/jira/secure/attachment/12502931/0001-Missing-null-check-in-QueryProcessor.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-08 17:41:21.622,,,no_permission,,,,,,,,,,,,216475,,,Tue Nov 08 18:01:38 UTC 2011,,,,,,0|i0gkc7:,94725,jbellis,jbellis,,,,,,,,,08/Nov/11 17:13;slebresne;That seems to be a 1.0 only problem introduced by CASSANDRA-2734.,"08/Nov/11 17:41;jbellis;+1

(Actually introduced by CASSANDRA-3424, updating Affects accordingly.)",08/Nov/11 18:01;slebresne;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing null check in digest retry part of read path,CASSANDRA-3449,12529992,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,03/Nov/11 03:10,12/Mar/19 14:12,13/Mar/19 22:26,04/Nov/11 08:45,1.0.2,,,,,,0,,,,,,,,,,,,,,,,,,,,,03/Nov/11 03:13;jbellis;3449.txt;https://issues.apache.org/jira/secure/attachment/12502092/3449.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-03 16:37:03.148,,,no_permission,,,,,,,,,,,,215851,,,Fri Nov 04 08:45:57 UTC 2011,,,,,,0|i0gk1j:,94677,slebresne,slebresne,,,,,,,,,"03/Nov/11 16:37;slebresne;I think it is possible for the row to be null but still being in a short read situation. Like if you resolve 2 nodes whose responses look like:
{noformat}
  node1:  1  del  3
  node2: del  2  del
{noformat}
and you've asked for two columns. The merging results in a null row (assuming all del have priority), but it's possible that node1 had a lot of perfectly good value after that 3, so we should retry.

I believe the right fix could be to just have a {{if (row != null)}} before the rows.add(row) ?","04/Nov/11 08:45;slebresne;Ok, I went ahead and while committing CASSANDRA-3303 I added a null check just before the rows.add(row) line. My rational for ninja-shoving it was that this can't introduce a bug. Worst case scenario, we shouldn't do the check for short read code when the row is null in the first place and my added null check is useless. But as said above I believe we do need to do the check for short read in this case, so I'm closing this. But feel free to reopen if you spot a mistake in my reasoning.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clientutil depends on FBUtilities (bad),CASSANDRA-3299,12525590,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,03/Oct/11 21:33,12/Mar/19 14:12,13/Mar/19 22:26,10/Oct/11 15:22,1.0.1,,,Legacy/CQL,,,0,cql,,,,"clientutils' (indirect )dependency on FBUtilities (needed for tests) would result in huge numbers of classes being pulled in transitively.

The attached patch moves the {{bytesToHex}} and {{hexToBytes}} methods into a new class ({{o.a.c.utils.Hex}}), which has no external dependencies.

This should be pretty safe, but I've marked it fixfor-1.0.1 since we're so close to release, and because the JDBC driver can embed a snapshot jar in the meantime.",,,,,,,,,,,,,,,,03/Oct/11 21:35;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3299-move-FBUtils-methods-for-bytes-hex-to-s.txt;https://issues.apache.org/jira/secure/attachment/12497546/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3299-move-FBUtils-methods-for-bytes-hex-to-s.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-03 21:59:57.751,,,no_permission,,,,,,,,,,,,43944,,,Mon Oct 10 16:51:57 UTC 2011,,,,,,0|i0gi7z:,94382,,,,,,,,,,,"03/Oct/11 21:59;jbellis;Nit: can we just name the methods Hex.toBytes and Hex.toString?

+1 otherwise","03/Oct/11 23:39;urandom;I'm OK with that, but are those names going to be too generic if someone decides to do a static import?","07/Oct/11 17:52;jbellis;True enough. Not a big deal either way, let's go with what you have.",10/Oct/11 15:22;urandom;committed,10/Oct/11 15:26;jbellis;I saw this committed to trunk -- if it's intended for 1.0.1 it should also go in the 1.0 branch,"10/Oct/11 15:39;urandom;you're right; my bad, committed to cassandra-1.0 as well","10/Oct/11 16:51;hudson;Integrated in Cassandra #1150 (See [https://builds.apache.org/job/Cassandra/1150/])
    move FBUtils methods for bytes<->hex to separate class

Patch by eevans; reviewed by jbellis for CASSANDRA-3299

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1181018
Files : 
* /cassandra/trunk/build.xml
* /cassandra/trunk/drivers/java/test/org/apache/cassandra/cql/JdbcDriverTest.java
* /cassandra/trunk/drivers/java/test/org/apache/cassandra/cql/jdbc/PreparedStatementTest.java
* /cassandra/trunk/examples/simple_authentication/src/org/apache/cassandra/auth/SimpleAuthenticator.java
* /cassandra/trunk/src/java/org/apache/cassandra/auth/Resources.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/marshal/BytesType.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/AbstractByteOrderedPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/BytesToken.java
* /cassandra/trunk/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/ByteBufferUtil.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/Hex.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/MerkleTree.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/marshal/RoundTripTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/utils/FBUtilitiesTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/utils/HexTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/utils/MerkleTreeTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompressedRandomAccessReaderTest fails on Windows,CASSANDRA-3298,12525564,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,03/Oct/11 18:44,12/Mar/19 14:12,13/Mar/19 22:26,04/Oct/11 17:17,1.0.0,,,,,,0,,,,,"    [junit] Testcase: testResetAndTruncate(org.apache.cassandra.io.compress.CompressedRandomAccessReaderTest):      FAILED
    [junit] expected:<43> but was:<49>
    [junit] junit.framework.AssertionFailedError: expected:<43> but was:<49>
    [junit]     at org.apache.cassandra.io.compress.CompressedRandomAccessReaderTest.testResetAndTruncate(CompressedRandomAccessReaderTest.java:81)
    [junit]     at org.apache.cassandra.io.compress.CompressedRandomAccessReaderTest.testResetAndTruncate(CompressedRandomAccessReaderTest.java:39)",,,,,,,,,,,,,,,,03/Oct/11 23:03;xedin;CASSANDRA-3298.patch;https://issues.apache.org/jira/secure/attachment/12497564/CASSANDRA-3298.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-04 16:35:21.694,,,no_permission,,,,,,,,,,,,43863,,,Tue Oct 04 17:17:14 UTC 2011,,,,,,0|i0gi7r:,94381,jbellis,jbellis,,,,,,,,,04/Oct/11 16:34;jbellis;So it's a bug in the test?,04/Oct/11 16:35;xedin;yes,"04/Oct/11 17:07;jbellis;looks like the fix is to make it use different files for compressed and uncompressed tests.

+1","04/Oct/11 17:17;xedin;Yes and also put them into temp directory.

Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
*.bat files fails when CASSANDRA_HOME contains a white space.,CASSANDRA-3258,12524632,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,talmdal,talmdal,talmdal,26/Sep/11 13:46,12/Mar/19 14:12,13/Mar/19 22:26,29/Sep/11 01:44,0.8.7,,,Packaging,,,0,,,,,"Issues 2952 and 2237 fixed the issue for cassandra.bat. But the following bat files need the same fix:
json2sstable.bat
nodetool.bat
sstable2json.bat
sstablekeys.bat
",Windows 7,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-09-26 23:07:50.593,,,no_permission,,,,,,,,,,,,1833,,,Thu Sep 29 02:22:10 UTC 2011,,,,,,0|i0ghq7:,94302,jbellis,jbellis,,,,,,,,,26/Sep/11 13:47;talmdal;This also applies to 1.0.0,26/Sep/11 23:07;jbellis;Can you submit a patch?,"27/Sep/11 02:52;talmdal;Sure, I'll try getting that done tomorrow","28/Sep/11 14:20;talmdal;Pull request: https://github.com/apache/cassandra/pull/4
Change: https://github.com/talmdal/cassandra/commit/2ffdc581aa758c8bdb2a58d7e43f1e07feca7141","28/Sep/11 15:30;jbellis;please post a patch here, the ASF does not accept patches via github yet","28/Sep/11 15:47;talmdal;diff --git a/bin/json2sstable.bat b/bin/json2sstable.bat
index 158210c..ceb7987 100644
--- a/bin/json2sstable.bat
+++ b/bin/json2sstable.bat
@@ -18,7 +18,7 @@
 if ""%OS%"" == ""Windows_NT"" setlocal
 
 if NOT DEFINED CASSANDRA_HOME set CASSANDRA_HOME=%~dp0..
-if NOT DEFINED CASSANDRA_CONF set CASSANDRA_CONF=%CASSANDRA_HOME%\conf
+if NOT DEFINED CASSANDRA_CONF set CASSANDRA_CONF=""%CASSANDRA_HOME%\conf""
 if NOT DEFINED CASSANDRA_MAIN set CASSANDRA_MAIN=org.apache.cassandra.tools.SSTableImport
 if NOT DEFINED JAVA_HOME goto err
 
@@ -29,10 +29,10 @@ set JAVA_OPTS=^
 REM ***** CLASSPATH library setting *****
 
 REM Ensure that any user defined CLASSPATH variables are not used on startup
-set CLASSPATH=%CASSANDRA_HOME%\conf
+set CLASSPATH=""%CASSANDRA_HOME%\conf""
 
 REM For each jar in the CASSANDRA_HOME lib directory call append to build the CLASSPATH variable.
-for %%i in (%CASSANDRA_HOME%\lib\*.jar) do call :append %%~fi
+for %%i in (""%CASSANDRA_HOME%\lib\*.jar"") do call :append %%~fi
 goto okClasspath
 
 :append
@@ -41,7 +41,7 @@ goto :eof
 
 :okClasspath
 REM Include the build\classes\main directory so it works in development
-set CASSANDRA_CLASSPATH=%CLASSPATH%;%CASSANDRA_HOME%\build\classes\main;%CASSANDRA_CONF%;%CASSANDRA_HOME%\build\classes\thrift
+set CASSANDRA_CLASSPATH=%CLASSPATH%;""%CASSANDRA_HOME%\build\classes\main"";%CASSANDRA_CONF%;""%CASSANDRA_HOME%\build\classes\thrift""
 
 set CASSANDRA_PARAMS=
 set TOOLS_PARAMS=
diff --git a/bin/nodetool.bat b/bin/nodetool.bat
index ea86317..a78c1c9 100644
--- a/bin/nodetool.bat
+++ b/bin/nodetool.bat
@@ -21,7 +21,7 @@ if NOT DEFINED CASSANDRA_HOME set CASSANDRA_HOME=%~dp0..
 if NOT DEFINED JAVA_HOME goto err
 
 REM Ensure that any user defined CLASSPATH variables are not used on startup
-set CLASSPATH=%CASSANDRA_HOME%\conf
+set CLASSPATH=""%CASSANDRA_HOME%\conf""
 
 REM For each jar in the CASSANDRA_HOME lib directory call append to build the CLASSPATH variable.
 rem for %%i in (%CASSANDRA_HOME%\lib*.jar) do call :append %%~fi
@@ -34,7 +34,7 @@ goto :eof
 
 :okClasspath
 REM Include the build\classes\main directory so it works in development
-set CASSANDRA_CLASSPATH=%CLASSPATH%;%CASSANDRA_HOME%\build\classes\main;%CASSANDRA_HOME%\build\classes\thrift
+set CASSANDRA_CLASSPATH=%CLASSPATH%;""%CASSANDRA_HOME%\build\classes\main"";""%CASSANDRA_HOME%\build\classes\thrift""
 goto runNodeTool
 
 :runNodeTool
diff --git a/bin/sstable2json.bat b/bin/sstable2json.bat
index 6ca91e4..cc4cbce 100644
--- a/bin/sstable2json.bat
+++ b/bin/sstable2json.bat
@@ -18,7 +18,7 @@
 if ""%OS%"" == ""Windows_NT"" setlocal
 
 if NOT DEFINED CASSANDRA_HOME set CASSANDRA_HOME=%~dp0..
-if NOT DEFINED CASSANDRA_CONF set CASSANDRA_CONF=%CASSANDRA_HOME%\conf
+if NOT DEFINED CASSANDRA_CONF set CASSANDRA_CONF=""%CASSANDRA_HOME%\conf""
 if NOT DEFINED CASSANDRA_MAIN set CASSANDRA_MAIN=org.apache.cassandra.tools.SSTableExport
 if NOT DEFINED JAVA_HOME goto err
 
@@ -29,10 +29,10 @@ set JAVA_OPTS=^
 REM ***** CLASSPATH library setting *****
 
 REM Ensure that any user defined CLASSPATH variables are not used on startup
-set CLASSPATH=%CASSANDRA_HOME%\conf
+set CLASSPATH=""%CASSANDRA_HOME%\conf""
 
 REM For each jar in the CASSANDRA_HOME lib directory call append to build the CLASSPATH variable.
-for %%i in (%CASSANDRA_HOME%\lib\*.jar) do call :append %%~fi
+for %%i in (""%CASSANDRA_HOME%\lib\*.jar"") do call :append %%~fi
 goto okClasspath
 
 :append
@@ -41,7 +41,7 @@ goto :eof
 
 :okClasspath
 REM Include the build\classes\main directory so it works in development
-set CASSANDRA_CLASSPATH=%CLASSPATH%;%CASSANDRA_HOME%\build\classes\main;%CASSANDRA_CONF%;%CASSANDRA_HOME%\build\classes\thrift
+set CASSANDRA_CLASSPATH=%CLASSPATH%;""%CASSANDRA_HOME%\build\classes\main"";%CASSANDRA_CONF%;""%CASSANDRA_HOME%\build\classes\thrift""
 
 set CASSANDRA_PARAMS=
 set TOOLS_PARAMS=
diff --git a/bin/sstablekeys.bat b/bin/sstablekeys.bat
index a0d7641..7d6446a 100644
--- a/bin/sstablekeys.bat
+++ b/bin/sstablekeys.bat
@@ -18,7 +18,7 @@
 if ""%OS%"" == ""Windows_NT"" setlocal
 
 if NOT DEFINED CASSANDRA_HOME set CASSANDRA_HOME=%~dp0..
-if NOT DEFINED CASSANDRA_CONF set CASSANDRA_CONF=%CASSANDRA_HOME%\conf
+if NOT DEFINED CASSANDRA_CONF set CASSANDRA_CONF=""%CASSANDRA_HOME%\conf""
 if NOT DEFINED CASSANDRA_MAIN set CASSANDRA_MAIN=org.apache.cassandra.tools.SSTableExport
 if NOT DEFINED JAVA_HOME goto err
 
@@ -29,10 +29,10 @@ set JAVA_OPTS=^
 REM ***** CLASSPATH library setting *****
 
 REM Ensure that any user defined CLASSPATH variables are not used on startup
-set CLASSPATH=%CASSANDRA_HOME%\conf
+set CLASSPATH=""%CASSANDRA_HOME%\conf""
 
 REM For each jar in the CASSANDRA_HOME lib directory call append to build the CLASSPATH variable.
-for %%i in (%CASSANDRA_HOME%\lib\*.jar) do call :append %%~fi
+for %%i in (""%CASSANDRA_HOME%\lib\*.jar"") do call :append %%~fi
 goto okClasspath
 
 :append
@@ -41,7 +41,7 @@ goto :eof
 
 :okClasspath
 REM Include the build\classes\main directory so it works in development
-set CASSANDRA_CLASSPATH=%CLASSPATH%;%CASSANDRA_HOME%\build\classes\main;%CASSANDRA_CONF%;%CASSANDRA_HOME%\build\classes\thrift
+set CASSANDRA_CLASSPATH=%CLASSPATH%;""%CASSANDRA_HOME%\build\classes\main"";%CASSANDRA_CONF%;""%CASSANDRA_HOME%\build\classes\thrift""
 
 set CASSANDRA_PARAMS=
 set TOOLS_PARAMS=
","28/Sep/11 15:49;talmdal;Hmm, hope this is the right way to submit the patch.

Given the back and forth, it would have just been easier for you to add the quotes :-/","29/Sep/11 01:44;jbellis;Committed, thanks!

(For future reference, attaching a file with the patch is easiest to work with.)","29/Sep/11 02:22;hudson;Integrated in Cassandra-0.8 #349 (See [https://builds.apache.org/job/Cassandra-0.8/349/])
    Fix tool .bat files when CASSANDRA_HOME contains spaces
patch by Tim Almdal; reviewed by jbellis for CASSANDRA-3258

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177149
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/bin/json2sstable.bat
* /cassandra/branches/cassandra-0.8/bin/nodetool.bat
* /cassandra/branches/cassandra-0.8/bin/sstable2json.bat
* /cassandra/branches/cassandra-0.8/bin/sstablekeys.bat
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missed CQL term rename,CASSANDRA-3266,12524968,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,27/Sep/11 21:02,12/Mar/19 14:12,13/Mar/19 22:26,27/Sep/11 22:10,1.0.0,,,Legacy/CQL,,,0,cql,,,,The CQL grammar was missed in the rename of {{bytea}} to {{blob}}.,,,,,,,,,,,,,,,,27/Sep/11 21:03;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3266-update-for-missed-term-rename-s-bytea-b.txt;https://issues.apache.org/jira/secure/attachment/12496799/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3266-update-for-missed-term-rename-s-bytea-b.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-27 22:00:41.151,,,no_permission,,,,,,,,,,,,33632,,,Tue Sep 27 22:10:47 UTC 2011,,,,,,0|i0ghtr:,94318,,,,,,,,,,,27/Sep/11 22:00;jbellis;+1,27/Sep/11 22:10;urandom;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Describe ring is broken,CASSANDRA-3433,12529528,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,nickmbailey,nickmbailey,nickmbailey,31/Oct/11 18:30,12/Mar/19 14:12,13/Mar/19 22:26,31/Oct/11 20:07,1.0.2,,,Legacy/CQL,Legacy/Tools,,0,thrift,,,,CASSANDRA-2882 broke describe_ring by causing the 'endpoints' field to contain the rpc address rather than the listen address. the rpc_address belongs in the 'rpc_endpoints' field. Hence the name.,,,,,,,,,,,,,,,,31/Oct/11 19:19;nickmbailey;0001-Don-t-use-rpc-address-for-endpoints-field-of-describ.patch;https://issues.apache.org/jira/secure/attachment/12501653/0001-Don-t-use-rpc-address-for-endpoints-field-of-describ.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-31 20:07:28.562,,,no_permission,,,,,,,,,,,,215388,,,Mon Oct 31 20:07:28 UTC 2011,,,,,,0|i0gju7:,94644,brandon.williams,brandon.williams,,,,,,,,,31/Oct/11 19:19;nickmbailey;Fixes the bug as well as adding rack information. ,"31/Oct/11 19:20;nickmbailey;Didn't change the thrift version number in that patch. Assuming it is committed, that should also be bumped.","31/Oct/11 20:07;brandon.williams;Committed, thrift version bumped to 19.19",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support bringing up a new datacenter to existing cluster without repair,CASSANDRA-3483,12531095,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,scode,lenn0x,lenn0x,11/Nov/11 04:59,12/Mar/19 14:11,13/Mar/19 22:26,30/Jan/12 15:24,1.1.0,,,,,,2,,,,,"Was talking to Brandon in irc, and we ran into a case where we want to bring up a new DC to an existing cluster. He suggested from jbellis the way to do it currently was set strategy options of dc2:0, then add the nodes. After the nodes are up, change the RF of dc2, and run repair. 

I'd like to avoid a repair as it runs AES and is a bit more intense than how bootstrap works currently by just streaming ranges from the SSTables. Would it be possible to improve this functionality (adding a new DC to existing cluster) than the proposed method? We'd be happy to do a patch if we got some input on the best way to go about it.
",,,,,,,,,,CASSANDRA-3807,,,,,,30/Jan/12 14:49;jbellis;3483-cleanup.txt;https://issues.apache.org/jira/secure/attachment/12512421/3483-cleanup.txt,30/Jan/12 12:47;slebresne;3483-v3.patch;https://issues.apache.org/jira/secure/attachment/12512409/3483-v3.patch,30/Nov/11 18:29;scode;CASSANDRA-3483-0.8-prelim.txt;https://issues.apache.org/jira/secure/attachment/12505645/CASSANDRA-3483-0.8-prelim.txt,05/Dec/11 01:40;scode;CASSANDRA-3483-1.0.txt;https://issues.apache.org/jira/secure/attachment/12506076/CASSANDRA-3483-1.0.txt,24/Dec/11 04:08;scode;CASSANDRA-3483-trunk-noredesign.txt;https://issues.apache.org/jira/secure/attachment/12508583/CASSANDRA-3483-trunk-noredesign.txt,28/Jan/12 05:31;scode;CASSANDRA-3483-trunk-rebase2.txt;https://issues.apache.org/jira/secure/attachment/12512290/CASSANDRA-3483-trunk-rebase2.txt,28/Jan/12 07:06;scode;CASSANDRA-3483-trunk-refactored-v1.txt;https://issues.apache.org/jira/secure/attachment/12512294/CASSANDRA-3483-trunk-refactored-v1.txt,28/Jan/12 10:00;scode;CASSANDRA-3483-trunk-refactored-v2.txt;https://issues.apache.org/jira/secure/attachment/12512299/CASSANDRA-3483-trunk-refactored-v2.txt,,,,8.0,,,,,,,,,,,,,,,,,,,2011-11-16 20:49:20.283,,,no_permission,,,,,,,,,,,,216833,,,Mon Jan 30 18:12:02 UTC 2012,,,,,,0|i0gkgn:,94745,slebresne,slebresne,,,,,,,,,"11/Nov/11 05:43;lenn0x;Some discussion from irc:

{noformat}
23:43 < goffinet> has datastax ever had a customer add a new datacenter to an existing cluster? No docs or info on web suggest anyone has done this before
23:44 < driftx> yeah
23:44 < goffinet> how is it done? we are running a case where if i modify strategy options before adding nodes, writes will fail since no endpoints for DC have been added
23:44 < goffinet> we were expecting this might work because we want to bootstrap the new DC to the existing cluster
23:44 < goffinet> take on writes + stream data with RF factor
23:45 < driftx> general best practice is (jbellis can correct if I'm outdated) add the dc at rf:0, add the nodes/update snitch, repair
23:45 < driftx> err, update rf, repair
23:46 < goffinet> yeah mind if i open up a jira? that seems extreme to make the cluster do that .. ?
23:46 < goffinet> or is repair smart enough to just stream ranges instead of AES?
23:46 < driftx> 'instead of AES?' that's what repair is, but if just streams ranges
23:46 < driftx> s/if/it/
23:47 < goffinet> right but AES builds merkle tree, scans through all data ?
23:47 < goffinet> isn't bootstrap a different operation?
23:47 < goffinet> when streaming just sstables
23:47 < driftx> yeah, it is
23:47 < goffinet> yeah thats more heavy. dont understand why we couldnt use that instead
23:47 < goffinet> like bootstrap
23:48 < stuhood> now that i think about it, it doesn't really make sense that a CL.ONE write fails if a DC isn't available
23:48 < stuhood> independent of the bootstrap case, that sounds like the real issue
23:49 < stuhood> goffinet: ^
23:50 < driftx> hmm, yeah that doesn't
23:50 < driftx> but the problem with bootstrapping a dc is the first node you bootstrap gets everything
23:50 < goffinet> stuhood: yeah. it was complaining about not enough endpoints 
23:50 < goffinet> driftx: why is that? if you are doubling the cluster, and assign the tokens manually ?
23:51 < driftx> still have to do them 2 mins apart, and they're probably going to be part of the same replica set which I think is troublesome too
23:51 < goffinet> driftx: maybe we can make repair a bit more intelligent? if no data exists on the node .. just stream the ranges instead of using AES
23:52 < driftx> problem is we're pushing AES to do the entire replica set (which is nearly does now)
23:52 < stuhood> goffinet: it shouldn't be as heavyweight as you're thinking
23:53 < goffinet> stuhood: but we have a way currently that is less heavy
23:53 < goffinet> i dont understand why we couldnt use that method
23:53 < stuhood> not implemented =)
23:53 < goffinet> don't cut corners :)
23:53 < stuhood> human time vs cpu time =P
23:54 < driftx> you could almost do something like #3452 and then have a jmx call to say 'ok, finish'
23:54 < CassBotJr> https://issues.apache.org/jira/browse/CASSANDRA-3452 : Create an 'infinite bootstrap' mode for sampling live traffic
23:54 < driftx> except the first one that tries is going to have every node pound it with all the writes
23:54 < goffinet> driftx: ill make a jira ticket so we can discuss there, it doesn't seem like it would be too much trouble to support this use case
23:54 < goffinet> we'd be happy to write the patch after some input
23:55 < driftx> trickier than it sounds I'll bet, but sgtm
23:57 < stuhood> alternatively, is now the right time to add back group bootstrap?
23:58 < stuhood> so you'd 1) add the dc to the strategy, 2) do a group bootstrap of the entire dc
23:58 < stuhood> would also have to fix the CL.ONE problem though.
23:59 < goffinet> how did group bootstrap work again?
23:59 < driftx> #2434 is relevant
23:59 < CassBotJr> https://issues.apache.org/jira/browse/CASSANDRA-2434 : range movements can violate consistency
--- Day changed Fri Nov 11 2011
00:00 < stuhood> goffinet: bootstrapping many nodes at once without the 2 minute wait
00:01 < goffinet> why was it removed?
00:01 < stuhood> used zookeeper
00:01 < goffinet> oh.
00:01 < stuhood> but come to think of it, removing the 2 minute wait would seem to be relatively easy
00:02 < goffinet> stuhood, i thought the 2 minute wait was just waiting for ring state to settle?
00:02 < goffinet> before it streamed from nodes
00:02 < stuhood> goffinet: yea: you could form a ""group"" bootstrap by inverting things and waiting until you -hadn't- seen a new node in 2-10 minutes before you chose a token and started bootstrapping
00:03 < stuhood> so, not terribly simple, but.
00:04 < stuhood> you'd basically have a bunch of nodes sitting around waiting until no new nodes started, and then they have to deterministically choose tokens.
00:05 < goffinet> yes
00:05 < stuhood> well, alternatively, you wouldn't need a new way to deterministically choose tokens
00:05 < stuhood> (easier)
00:05 < stuhood> no… scratch that. you would need a way
00:05 < stuhood> for this DC case, all of the nodes are entering an empty ring
00:06 < stuhood> so the group would need to choose something balanced
00:06 < goffinet> empty ring?
00:06 < stuhood> yea, essentially… there are no tokens in that dc
00:06 < goffinet> but we were going to provide the tokens manually?
00:06 < goffinet> were you thinking of making it automatic?
00:07 < stuhood> yea. fixing bootstrapping groups of nodes would make automatic safe again
00:08 < stuhood> so… whatever state a node is in when it is sitting and waiting for enough information to choose a token, it should just stay that way and watch what other nodes enter that state
00:08 < goffinet> so i have a question about the 120 second window you have to wait..
00:09 < stuhood> mm
00:09 < driftx> hmm, what if they started up at rf:0 but stayed in some dead state (hibernate might work) without doing anything until you changed the rf, then actually bootstrapped?
00:09 < goffinet> so imagine i startup all the nodes in DC2 at same time, does join_ring=false not grab gossip info at all? I was thinking it would be good if we could just start gossip on all nodes, but until operator says 'go' then i could bootstrap them all at same time
00:09 < goffinet> since i would only have to wait at most 120 seconds before kicking them all off
00:10 < stuhood> driftx: yea, that could work too… but you'd still need to choose tokens. (also, the rf=0 thing shouldn't be necessary, right? that's the CL.ONE bug)
00:11 < driftx> well, you really want to choose tokens anyway
00:11 < stuhood> goffinet: it does get gossip… i think that's basically equivalent to the pre-join state
00:11 < driftx> I guess you don't need rf=0 if all the nodes are in hibernate
00:12 < goffinet> yeah i think you do need hibernate in this case, because if i set tokens upfront, i want all nodes to know about ATL ones too
00:12 < goffinet> before i kick off bootstrap
00:12 < stuhood> driftx: i'm confused… what is the difference between rf=0 and not being there?
00:12 < stuhood> is that a workaround for the CL.ONE bug?
00:13 < driftx> you know there's a dc with rf:0, can add one with impacting anything
00:13 < driftx> err, without
00:14 ?? boaz__ (0819c319@gateway/web/freenode/ip.8.25.195.25) has joined #cassandra-dev
00:14 < stuhood> so what was the point of adding it? that's why i'm confused...
00:14 < goffinet> im fine with rf:0, its so you can add the nodes to the cluster before calling repair
00:14 < goffinet> before you add nodes
00:15 < driftx> because the dc is in the schema
00:15 < driftx> so you need it there to have nodes be in it
00:15 < stuhood> ah
00:16 < goffinet> driftx: any reason why we couldnt just fix that? so dc2:3 wont throw an error if nodes are down?
00:16 < goffinet> that way you would needed to do two steps
00:16 < goffinet> dc2:0, add nodes, dc2:3
00:16 < goffinet> wouldn't*
00:16 < driftx> I don't understand, you can already do that
00:17 < driftx> you just have to repair afterwards
00:17 < goffinet> it throws an error currently? if you set dc2:3 and no nodes exist for dc2
00:17 < goffinet> we'll double check on that
00:18 < goffinet> for writes
00:18 < driftx> oh, it does
00:19 < driftx> but only for writes
00:19 < goffinet> yeah
00:19 < goffinet> so thats fine, thats fixable
00:19 < goffinet> im just curious about a) how can we bootstrap nodes without 120s delays between N nodes b) stream from DC1 without AES
00:21 < stuhood> goffinet: if you figure out a, i don't think b is necessary?
00:22 < stuhood> assuming they are aware of the other joining nodes, and can all join the same range
00:22 < stuhood> that would be the keystone for some kind of group bootstrap
00:23 < goffinet> let me test out join_ring, because im curious. if join_ring=false still gossips but doesnt offically join.. it would be nice if node 2 in DC2 knew about that node too somehow?
00:23 < driftx> that's why I proposed cheating, add them all as non-members, then ask them to bootstrap
00:23 < goffinet> because then .. i could just run a command on each node at same time
00:23 < goffinet> since they all know about each other in a hibernate state
00:23 < goffinet> driftx: yes i like that
00:24 < driftx>     private void joinTokenRing(int delay) throws IOException, org.apache.cassandra.config.ConfigurationException
00:24 < driftx>     {
00:24 < driftx>         logger_.info(""Starting up server gossip"");
00:24 < driftx> they don't use gossip with join_ring off
00:24 < stuhood> but will that actually allow them to all join the same range?
00:24 < goffinet> okay cool, yeah we would need to make it join in that special state then
00:25 < stuhood> i think there is an edgecase here… if multiple nodes are joining the same range, and one of them fails, then should they all fail?
00:25 < driftx> no, it basically saves you server startup time that is not ring-related :)
00:25 < goffinet> stuhood, they all know the tokens ahead of time?
00:25 < goffinet> they just need to know the current global state of things
00:25 < stuhood> goffinet: right, but if they are streaming the range that they will be responsible for...
00:26 ?? mw1 (~Adium@8.25.195.29) has quit (Quit: Leaving.)
00:26 < stuhood> Joining nodes don't stick around if they fail
00:26 < goffinet> they shouldnt be allowed to do that until they joined ?
00:26 < stuhood> nah, you stream while you are joining… unless you are talking about repair
00:26 < goffinet> stuhood: was that removed? i thought u had to still remove the node
00:26 < goffinet> using the new options in 1.0
00:26 < stuhood> don't know about 1.0
00:27 < driftx> no, a failed non-member is just a fat client and disappears
00:27 < goffinet> but i thought there was a timeout for fat client ?
00:27 < goffinet> is it 30s or something?
00:27 < driftx> yes
00:28 < goffinet> so nodes that arent fat clients, why might we remove them ? if we didnt..
00:28 < goffinet> and let the operator do it
00:28 < goffinet> or have a larger timeout
00:28 < goffinet> might make this a non-issue
00:28 < driftx> what does a larger timeout/keeping them around buy you?
00:29 < goffinet> because if they go away, and i bootstrap after they failed, wont my view of ring be skewed?
00:29 < stuhood> driftx: i guess in this case, the node would resume bootstrapping from where it left off
00:29 < driftx> it would've missed writes in the meantime and require a repair afterwards anyway
00:29 < stuhood> sorry… ""resume"" in the sense of ""start over"", but yea
00:31 < stuhood> that would be a pretty big change, but it might make sense
00:31 < goffinet> stuhood: what would you change
00:31 < stuhood> what you said, about nodes in joining staying in joining
00:31 < stuhood> so if the machine restarts, it begins joining at the same position again
00:33 < goffinet> if we supported that + letting nodes gossip in hibernate, would allow us to add capacity at operator control
{noformat}","16/Nov/11 20:49;slebresne;I think it wouldn't be crazy and actually very (very) simple to add a new nodetool command (rebuild?) that would basically have the node asks the other replicas to stream all there data to him (for the correct ranges obviously). In other words, a command that force the streaming part of bootstrap without all the join ring part. Or another way to say is to have the streaming part of a repair but without the validation part.

The method to add a new DC would be the same as today except that repair would be replaced by this new operation.","16/Nov/11 20:52;brandon.williams;We would still need to put the nodes into a 'bootstrap' state to get incoming writes forwarded to them, otherwise you have to repair in the end anyway.","17/Nov/11 04:27;lenn0x;Yeah as Brandon mentioned, we would still want to go into the bootstrap state to get those writes. This would also allow us to add capacity in the same way, if we manually pick tokens (auto bootstrap is kinda worthless IMO) to existing DC as well. We can just fire off the bootstrap command from nodetool as needed.","17/Nov/11 06:30;lenn0x;Sorry, I had to re-read what Sylvain said for it to 'click'. So the process he proposes is as follows with RF of 3:

1. strategy options dc2:0
2. bring up new nodes in dc2 with auto_bootstrap off and token set
3. set strategy options dc2:3
4. run 'rebuild' on each node in dc2

this would handle the writes part.

i was kinda hoping though that we could modify the gossip state because I could very well see this playing into the case where you weren't adding DCs but wanted to add lots of nodes (60-100 like we do currently) ... and wanted to have them all added to existing DC.. having the bootstrap defined that way, would allow us to bootstrap nodes as we please in existing DCs, bring them all to the ring at once to have a consistent state without taking on traffic until they transitioned states (joining/normal). Where as this proposal wouldn't be able to satisfy that use case.

","17/Nov/11 09:18;slebresne;So you're proposing to add support for bootstrapping multiple nodes together. I'm not against that, it would be nice and that would give you 90% of what this ticket is about (you'd have to add the ability to multi-boostrap *and* add a DC/augment the replication faction at the same time). But it is orders of magnitude more complicated than what I'm suggesting. Which is not a problem in itself given it's a broader solution, but it means we'll have multi-node boostrap at best for 1.1, while I'm pretty sure I can get the 'rebuild' command wrote in like an hour (and I see no reason why it couldn't be put in the 1.0 series).

","18/Nov/11 00:22;lenn0x;Sylvain, it looks like a state 'HIBERNATE' exists in GOSSIP already based on recovering a dead node in 1.0. Do you have a preference on the name of the new state if we attempted adding multiple nodes patch? WAITING? STANDBY?",18/Nov/11 04:22;scode;I was thinking of GHOST too after I realized we'll need a state where the node receives neither writes nor reads (more details here later).,"21/Nov/11 21:03;jbellis;So, am I understanding correctly that we're talking about two different scenarios?

- Add new DC without repair
- Add many nodes to existing DC without RING_DELAY in between

I think Sylvain's proposal addresses the first nicely.  So what I need help with is understanding what problem you're trying to solve with the second part.  Dealing with overlapping ranges in node movement basically requires a rewrite of that subsystem (CASSANDRA-2434).  But I suspect there is a ""good enough"" solution that we could find if I understood better what your pain point is here.","22/Nov/11 05:57;lenn0x;Jonathan,

You are correct. Sylvain's proposal does satisfy this ticket. It doesn't solve the case of (2) where if you want to add lots of nodes in an existing DC, and you know the tokens they should be at, and want to join them all at once.

Our use case is, we actually add 60-100 nodes in one big capacity add. We would like to avoid the 120 second per node time frame. It's not a deal breaker though. We actually realized though if we are adding that many nodes to our cluster, with a large cluster already, we need to rebalance heavily anyway.  Peter is almost done with the 'rebuild' patch, I'm assigning him to this ticket. 

Our next big focus is improving the rebalancing of a cluster. We have a very large cluster and after adding 100 nodes every month or so, this becomes painful. Almost all of our nodes have over 600GB+ each. We have an application that will require us to be rebalancing at all times to reduce our hot spots.

","23/Nov/11 00:51;scode;We ended up going for the simpler rebuild patch as Chris hinted at. I'll quote myself:

{quote}
I've been looking at this some more.

Here's the proposal so far:

We introduce a GHOST state, in which a node receives neither reads nor writes. This allows us to bring in a group of nodes in the ring without suffering any ill effects. It is completely invisible to reads and writes, and will never count towards e.g. consistency level.

Once all ghosts are ready, we can start bootstrapping, taking ghost nodes into accounts for purposes of determining which range we are responsible for, but streaming only from non-ghost nodes. This accomplishes the goal of not transferring more data than necessary.

In order to avoid a bootstrapping node from taking writes for more than it's eventual share, we'd have to make the write endpoints be aware of ghost nodes. This is dosable, but not critical since we're bisecting a range that was previously handled by a single node anyway so the traffic would be managable. It would just be cleaner to not have to cleanup afterwards.

Once we transition from bootstrapping to being ""up"", we have a bigger problem however. If the read paths and write paths are only aware of non-ghost nodes, the read/write paths would think that these nodes had more ownership than they really do.

So we must really be taking into account the other nodes in the read/write path as well - but only when determining ownership of a completely bootstrapped node that was previously a ghost. That means we must distinguish between a ""normally up"" node and one that's been bootstraped from ghost state (call it ""ghost strapped"").

This suddenly gets complex. The process of group bootstrap would then be:

Add a bunch of nodes in GHOST state
Bootstrap all of them, each of them going into GHOSTSTRAPPED state
Once all are GHOSTSTRAPPED, we can safely transntion from GHOSTSTRAPPED to normal/up
Is there a simpler solution?
{quote}

After some additional discussion we felt this was adding to much complexity and potential edge cases/bugs that it became more cost-effective to just go with the simple rebuild for our immediate needs, hoping to address the problem of adding lots of nodes to a DC separately in some other way.

A patch is forthcoming soon.
","30/Nov/11 18:29;scode;Here is a patch rebased against 0.8 for cursory review. I do not expect this to go into 0.8, and in fact I have not tested this patch other than build against vanilla 0.8 (the original patch is tested, but against our internal 0.8).

If there are no concerns with the overall implementation, I'll submit a rebased version for 1.0/trunk.

There are two components of the change:

* Breaking out the streaming part of BootStrapper into a separate RangeStreamer. Change BootStrapper to use that.

* Implement the rebuild command on top of RangeStreamer.

There are two ways to invoke rebuild:

{code}
nodetool rebuild
nodetool rebuild nameofdc
{code}

The first form streams from nearest endpoints, while the latter streams from nearest endpoints in the specified data center.

","30/Nov/11 18:40;brandon.williams;bq. If there are no concerns with the overall implementation, I'll submit a rebased version for 1.0/trunk.

This looks good to me, I like the RangeStreamer abstraction.",05/Dec/11 01:40;scode;Attached is a version rebased against 1.0 (and tested).,"05/Dec/11 16:09;jjordan;Once this option is in, is this the procedure for running rebuild (with 4 changed to 'rebuild dc1')?

{quote}
1. strategy options dc2:0
2. bring up new nodes in dc2 with auto_bootstrap off and token set
3. set strategy options dc2:3
4. run 'rebuild' on each node in dc2
{quote}

Do we need to stagger issuing the rebuild commands or can they be run all at once?
","05/Dec/11 19:28;scode;Yes, that looks good.

And yes, you can run rebuilds concurrently as long as you're comfortable with the amount of bandwidth you'll be pushing and the load you'll be putting on the source nodes.

However, if you expect to see reasonable performance and streaming at full speed to all nodes, you also need CASSANDRA-3494.

Regardless: I strongly recommend testing this with your exact version of Cassandra before trying it for real.
","14/Dec/11 00:37;slebresne;I haven't applied the patch yet, it needs rebase and preferably against trunk since that is the likely target for this, but a few comments.

We could have more reuse of code between Boostrapper ant the rebuild command.  Typically:
* RangeStreamer.getAllRangeWithSourcesFor does essentially the same thing that Boostrapper.getRangesWithSources, so it would be nice to do some reuse.
* In rebuild, we essentially have the code of Boostrapper.getWorkMap, again would be nice to do some code reuse.

I think we should move all of those in RangeStreamer and ultimately Boostrapper.boostrap() should be just one call to rebuild with the right arguments (mostly the correct tokenMetada instance and the ""myRange"" collection).

A few nits:
* rebuild code could be simplified slightly by using StorageService.getLocalRanges()
* rebuild doesn't fully respect the code style.
","14/Dec/11 01:52;scode;I'll get it rebased once it's otherwise okay.

As for re-use: I had intermediate versions that tried to do this, but ever time I ended up realizing that it was exploding in verbosity at the point where I was using the abstraction so it didn't actually help. However, I think there were a few changes towards the end after which I didn't re-evaluate.

I'll look at it again and see what I can do.","14/Dec/11 01:52;scode;I'll get it rebased once it's otherwise okay.

As for re-use: I had intermediate versions that tried to do this, but ever time I ended up realizing that it was exploding in verbosity at the point where I was using the abstraction so it didn't actually help. However, I think there were a few changes towards the end after which I didn't re-evaluate.

I'll look at it again and see what I can do.",24/Dec/11 04:08;scode;Attaching version rebased to trunk but not yet re-factored.,"25/Jan/12 21:24;jbellis;Peter, are you planning to follow up on Sylvain's comments still?","28/Jan/12 05:31;scode;I do. I'm sorry for the delay, this has been nagging me for quite some time. It's not forgotten, I have just been inundated with urgent stuff to do.

I'm attaching a fresh rebase against current trunk and I hope to submit an improved version later tonight (keyword being ""hope"").","28/Jan/12 07:06;scode;{{CASSANDRA\-3483\-trunk\-refactored\-v1.txt}} addresses the duplication between BootStrapper and RangeStreamer.

Next patch will address rebuild/getworkmap duplication.
",28/Jan/12 07:07;scode;(It also contains the addition of a brace from CASSANDRA-3806; this is intentional to avoid pain.) ,"28/Jan/12 07:12;scode;I borked the unit test, will address that too.","28/Jan/12 10:00;scode;{{CASSANDRA\-3483\-trunk\-refactored\-v2.txt}} I believe addresses the concerns, plus makes other improvements. I'm much more happy with this one.

It addresses CASSANDRA-3807 by supporting fetch ""consistency levels"" (though only ONE is currently usable without patching), and the filtering of hosts is abstracted out.

There is still some duplication between {{Bootstrapper.bootstrap()}} and {{StorageService.rebuild()}} in that both do the dance of iteration over tables to construct the final map. I am not really feeling that abstracting away that is a good idea to include in this ticket, though I think it's worthwhile doing at some point separately.

The unit test is fixed; my adjustment of it was wrong because I wasn't picking pending ranges (in the test).

I've tested both rebuild and bootstrap in a 3 node cluster.

I've added some more logging than what is typically the case; there have been several cases where I wished streaming was logged in more detail at INFO, particularly when bootstrapping or rebuilding. I think it's worthwhile to get that in while at it.","30/Jan/12 12:47;slebresne;bq. It addresses CASSANDRA-3807 by supporting fetch ""consistency levels"" (though only ONE is currently usable without patching)

I think a first issue is that current bootstrap does not fail if no node is alive for a given range, which arguably it should. I'm good with doing that, though it would be worth backporting to 1.0 too so it may be worth splitting that to a separate patch (or rather just create one for the fix in 1.0).

However, that does not solve the problem of bootstrap possibly breaking the consistency contract. The problem being that if we transfer a range from a node that happens to be lacking behind in term of consistency, and we end up replacing a node that was not lacking behind, we could break some consistency contracts. To fix that, I really only see only one solution right off the bat (which doesn't mean there isn't other): it is to ensure that for each range, we transfer it from (at least) the node we will replace for this range.

I believe the FetchConsistencyLevel of this patch is making an attempt to fix this by allowing to fetch from more than one node. While it does make it less likely to break consistency, unless we fetch from all nodes (and thus the one we'll replace), we cannot be sure we won't break the consistency level for people that say write at CL.ALL and read at CL.ONE. Overall, I fully agree this is a problem that we should fix someone, but I'm not sure the FetchConsistencyLevel is the right solution and even if it is it's a complicated enough problem that it's worth it's own ticket. I would agree that the problem with rebuild is a little bit different, but since anyway the patch introduce FCL without using it, let's keep that for later if that's ok.

bq. There is still some duplication between Bootstrapper.bootstrap() and StorageService.rebuild() in that both do the dance of iteration over tables to construct the final map. I am not really feeling that abstracting away that is a good idea to include in this ticket

I think it is, at least for a good chunk of it. It's not very complicated, it clearly improves code readability and since the patch already refactor that code I don't see a good reason to push that to later, especially if we agree it's worthwhile.

Attaching a v3 that 1) remove FetchConsistencyLevel for the reasons above and 2) move most of the details of creating the multimaps in RangeStreamer.
","30/Jan/12 14:04;jbellis;bq. Overall, I fully agree this is a problem that we should fix someone, but I'm not sure the FetchConsistencyLevel is the right solution and even if it is it's a complicated enough problem that it's worth it's own 

This is CASSANDRA-2434 isn't it?","30/Jan/12 14:09;slebresne;bq. This is CASSANDRA-2434 isn't it?

it is.","30/Jan/12 14:49;jbellis;cleanup patch addressing mostly typos and style.  only substantial code change was to RangeStreamer.getRangeFetchMap.  Also, moved OperationType.REBUILD to the end of the enum to make sure we don't break anything depending on ordinal.

+1 from me otherwise.",30/Jan/12 15:24;slebresne;Committed v3 + Jonathan's cleanups (and a fix to the unit test).,"30/Jan/12 18:12;scode;For the record I never intended to fix the general problem of bootstrapping never violating consistency. But in retrospect it's obvious how my choice of naming would make it sound like I did :) I agree it's a problem for its own ticket.

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IN (...) SELECTs don't honor KEY keyword,CASSANDRA-3627,12535058,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,13/Dec/11 23:25,12/Mar/19 14:11,13/Mar/19 22:26,05/Jan/12 23:11,,,,Legacy/CQL,,,0,cql,,,,"The WHERE clause of a SELECT ... IN (...) will not work with the KEY keyword, (but does with named/aliased keys).",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,220742,,,Thu Jan 05 23:11:17 UTC 2012,,,,,,0|i0gm7z:,95030,,,,,,,,,,,05/Jan/12 23:11;urandom;I'm not sure what I was seeing when I opened this; I am not able to reproduce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
range movements can violate consistency,CASSANDRA-2434,12503655,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,scode,scode,07/Apr/11 17:23,12/Mar/19 14:11,13/Mar/19 22:26,01/May/14 13:52,2.1 beta2,,,Legacy/Streaming and Messaging,,,0,,,,,"My reading (a while ago) of the code indicates that there is no logic involved during bootstrapping that avoids consistency level violations. If I recall correctly it just grabs neighbors that are currently up.

There are at least two issues I have with this behavior:

* If I have a cluster where I have applications relying on QUORUM with RF=3, and bootstrapping complete based on only one node, I have just violated the supposedly guaranteed consistency semantics of the cluster.

* Nodes can flap up and down at any time, so even if a human takes care to look at which nodes are up and things about it carefully before bootstrapping, there's no guarantee.

A complication is that not only does it depend on use-case where this is an issue (if all you ever do you do at CL.ONE, it's fine); even in a cluster which is otherwise used for QUORUM operations you may wish to accept less-than-quorum nodes during bootstrap in various emergency situations.

A potential easy fix is to have bootstrap take an argument which is the number of hosts to bootstrap from, or to assume QUORUM if none is given.

(A related concern is bootstrapping across data centers. You may *want* to bootstrap to a local node and then do a repair to avoid sending loads of data across DC:s while still achieving consistency. Or even if you don't care about the consistency issues, I don't think there is currently a way to bootstrap from local nodes only.)

Thoughts?

",,,,,,,,,,CASSANDRA-3516,,,,,,08/Sep/11 21:57;thepaul;2434-3.patch.txt;https://issues.apache.org/jira/secure/attachment/12493677/2434-3.patch.txt,07/Sep/11 20:11;thepaul;2434-testery.patch.txt;https://issues.apache.org/jira/secure/attachment/12493374/2434-testery.patch.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-08 21:00:17.175,,,no_permission,,,,,,,,,,,,20623,,,Mon Jul 21 21:44:49 UTC 2014,,,,,,0|i07z8f:,44475,thobbs,thobbs,,,,,,,,,"08/Apr/11 21:00;jbellis;ISTM the easiest fix is to always stream from the node that will be removed from the replicas for each range, unless given permission from the operator to choose a replica that is closer / less dead.",03/May/11 13:41;jbellis;Related: CASSANDRA-833,"25/Aug/11 23:30;thepaul;So, it looks like it will be possible for the node-that-will-be-removed to change between starting a bootstrap and finishing it (other nodes being bootstrapped/moved/decom'd during that time period); in some cases, that could still lead to a consistency violation.  Is that unlikely enough that we don't care, here?  At least the situation would be better with the proposed fix than it is now.

Second question: what might the ""permission from the operator to choose a replica that is closer/less dead"" look like?  Maybe just a boolean flag saying ""it's ok to stream from any node for any range you need to stream""?  Or would we want to allow specifying precise source nodes for any/all affected address ranges?","26/Aug/11 16:20;jbellis;bq. it looks like it will be possible for the node-that-will-be-removed to change between starting a bootstrap and finishing it

It's always been unsupported to bootstrap a second node into the same ""token arc"" while a previous one is ongoing.  Does that cover what you're thinking of or are we still on the hook?

bq. what might the ""permission from the operator to choose a replica that is closer/less dead"" look like?

It seems to me that the two valid choices are

- Stream from ""correct"" replica
- Stream from closest replica

I can't think of a reason to stream from an arbitrary replica other than those options.","26/Aug/11 17:14;thepaul;bq. It's always been unsupported to bootstrap a second node into the same ""token arc"" while a previous one is ongoing. Does that cover what you're thinking of or are we still on the hook?

Is it also unsupported to decom within X's token arc, or move into/out of that arc, while X is bootstrapping? I think we're safe if so.",26/Aug/11 17:19;jbellis;Yes.,"31/Aug/11 19:03;thepaul;This still needs some testing, but I'm putting it up now in case anyone has some time to take a look and make sure my approach is sane.

Adds an optional ""-n"" argument to ""nodetool join"" to allow bootstrapping from closest live node (n == non-strict). Also recognizes an optional property ""cassandra.join_strict"" which can be set to false when a bootstrap is triggered by cassandra.join_ring.","31/Aug/11 19:30;nickmbailey;Seems to me like the option to stream from the closest replica might just add more confusion without really gaining anything. The node that is leaving the replica set will never be in another datacenter. It could be on a different rack, but if you are following best practices and alternating racks then it is likely either on the same rack or there is only one copy on that rack and the best case possible is streaming from another rack anyway.","31/Aug/11 19:56;thepaul;Judging by the irc channel and user list, assuming people will follow best practices seems a bit of a dead end. Plus, what about the case where the node leaving the replica set is dead? You still want the option to allow choosing another to stream from. And we probably shouldn't default to choosing another without explicit permission, because of the consistency violation stuff.","31/Aug/11 19:56;jbellis;bq. The node that is leaving the replica set will never be in another datacenter

I think that's only strictly true for NTS, but I'm fine leaving it out.  Not worth adding complexity for ONTS at this point.","31/Aug/11 19:57;jbellis;bq. what about the case where the node leaving the replica set is dead

Good point.  We do need something to make that possible.",31/Aug/11 20:02;nickmbailey;Yeah I was assuming ONTS is basically deprecated at this point. Didn't think about the dead case though. I suppose just a 'force' type of option and a warning indicating the possible consistency issues works.,"31/Aug/11 20:03;jbellis;As long as we need to handle the dead case I don't see any harm in having a slightly more generally-useful ""use closest"" option instead of ""force to pick random live replica"" option.","31/Aug/11 20:16;nickmbailey;Well, I imagine the 'force' option would pick the nearest live node. By 'force' I mean the option should be posed to the user as ""We can't guarantee consistency in your cluster after this bootstrap since a node is down, if you would like to do this anyway, specify option X"". Just saying you can either bootstrap or bootstrap from the closest node doesn't convey the implications as well I don't think.

Maybe we are on the same page and arguing over wording though.","02/Sep/11 17:10;nickmbailey;Just as initial feedback, I'm not sure we need a new getRangesWithSources method, especially with so much duplication between them. Seems like strict could be passed to the current method. Also, what about leaving getRangesWithSources how it is and passing strict to getWorkMap? That method can do the endpoint set math if it needs to and throw a more informative exception in the case that strict is set and the endpoint we want to fetch from is dead.","02/Sep/11 20:03;thepaul;I did that (passing strict to getWorkMap) at first, but it wasn't too clean since it required adding a 'table' argument as well as 'strict', and it ended up replacing too much of the getRangesWithSources functionality. So then I added 'strict' as a parameter to getRangesWithSources (actually, it looks like I neglected to update its javadoc comment), but the differences between getRangesWithSources and getRangesWithStrictSource are such that a combined method feels a lot more special-casey and clunky. I like this way best in the end.","02/Sep/11 20:21;nickmbailey;Well I guess it kind of depends on which approach we take as well. Is the option A) bootstrap from the right token or bootstrap from the closest token, or B) bootstrap from the right token, but if that one isn't up, bootstrap from any other token preferring the closer ones.

Like I said, I'd say B, but if you and Jonathan both disagree.","02/Sep/11 21:50;thepaul;Yeah. B is probably easier on everyone, but I would say we simply can't do anything that might violate the consistency guarantee without explicit permission from the user.","02/Sep/11 21:55;nickmbailey;Ok, so if we always prefer to bootstrap from the correct token, then I still think we should combine getRangesWithStrictSource and getRangesWithSources. Basically the logic should be, find the 'best' node to stream from. If the user requested it, also find a list of other candidates and order them by proximity. Right?","04/Sep/11 03:38;jbellis;I'm okay with either A or B.

bq. I would say we simply can't do anything that might violate the consistency guarantee without explicit permission from the user

I'm not sure I understand, are you saying that B would violate this, or just that the status quo does?","05/Sep/11 02:05;hanzhu;Is it possible to make the node does not reply to any request before bootstrap and anti-entrophy repair is finished?

This could fix the consistency problem brought by bootstrap.","05/Sep/11 03:09;jbellis;Repair is a much, much more heavyweight solution to the problem than just ""stream from the node that is 'displaced.'""","05/Sep/11 10:16;hanzhu;Sometimes, the node is replaced because the hardware is crashed. If so, ""streaming from the node being replaced"" is not available.

How about force the repair happens if the user specifies he needs the consistency of quorum while the original node has gone.
","05/Sep/11 15:57;thepaul;bq. Ok, so if we always prefer to bootstrap from the correct token, then I still think we should combine getRangesWithStrictSource and getRangesWithSources. Basically the logic should be, find the 'best' node to stream from. If the user requested it, also find a list of other candidates and order them by proximity. Right?

I don't think so. I would still want to leave the option to stream from the closest even if the strict best node is available.","05/Sep/11 16:00;thepaul;bq. I'm not sure I understand, are you saying that B would violate this, or just that the status quo does?

I'm saying B would violate this, yes. B was ""bootstrap from the right token, but if that one isn't up, bootstrap from any other token preferring the closer ones"", right? I'm saying we can't just automatically choose another token if the user didn't specifically say it's ok.","05/Sep/11 16:38;nickmbailey;Paul,

The suggestion was that if the 'correct' node is down, you can force the bootstrap to complete anyway (probably from the closest node, but that is transparent to the user), but only if the 'correct' node is down. It sounds like you agree with Jonathan on the more general approach though.

Zhu,

Repair doesn't help in the case when you lost data due to a node going down. Also if only one node is down you should still be able to read/write at quorum and achieve consistency (assuming your replication factor is greater than 2).","05/Sep/11 19:25;jbellis;bq. I'm saying we can't just automatically choose another token if the user didn't specifically say it's ok.

Oh, ok.  Right.  (I thought we were just bikeshedding over whether to call the ""manual override"" option ""use closest"" or ""force bootstrap."")

bq. Repair doesn't help in the case when you lost data due to a node going down

Additionally, I don't like the idea of automatically doing expensive things like repair; it feels cleaner to not do it automatically, and allow using the existing tool to perform one if desired, than to do it by default and have to add an option to skip it for when that's not desirable.","05/Sep/11 23:19;hanzhu;bq. Also if only one node is down you should still be able to read/write at quorum and achieve consistency

I suppose quorum read plus quorum write should provide monotonic read consistency. [1] Supposing  quorum write on key1 hits node A and node B, not on node C due to temporal network partition. After that node B is replaced by node D since it is down, and node D streams data from node C. If the following quorum read on key1 hits only node C and node D, the monotonic consistency is violated. This is rare but not unrealistic, especially when hint handoff is disabled. 

Maybe it is more resonable to give the admin an option, to specify that the bootstrapped node should not accept any read request until the admin turn it on manually. So the admin can start a manual repair if he wants to assure everything goes fine.

[1]http://www.allthingsdistributed.com/2007/12/eventually_consistent.html","06/Sep/11 01:41;thepaul;bq. The suggestion was that if the 'correct' node is down, you can force the bootstrap to complete anyway (probably from the closest node, but that is transparent to the user), but only if the 'correct' node is down.

Oh, ok. I misunderstood. This seems reasonable. I'd lean for the more general solution, yeah, but I don't feel very strongly about it.","06/Sep/11 02:00;hanzhu;As peter suggested before, another approach to fix the consistency problem is streaming sstables from all alive peers if the ""correct"" node is down. And then leave them to normal compaction.  

This could be much lightweight than anti-entrophy repair, except the network IO pressure on the bootstrapping node.",07/Sep/11 20:08;thepaul;updated patch fixes the docstring for getRangesWithStrictSource().,07/Sep/11 20:11;thepaul;Patch 2434-testery.patch.txt adds a bit to unit tests to exercise o.a.c.dht.BootStrapper.getRangesWithStrictSource().,"08/Sep/11 21:57;thepaul;2434-3.patch.txt removes the bits that add the ""-n"" option to nodetool join. Apparently no ""nodetool join"" should ever result in a bootstrap, so it doesn't matter whether the caller wants ""strict"" or not.",14/Sep/11 15:29;jbellis;Do we need to do anything special for move/decommission as well?,"14/Sep/11 16:21;nickmbailey;I had a note to remember to create a ticket for that, but if we want to do it here that works as well.

In any case, yes the same concerns exist when giving away ranges as when gaining ranges.","14/Sep/11 17:23;thepaul;bq. Do we need to do anything special for move/decommission as well?

Yes, it looks like we do need to add similar logic for move. Expand the scope of this ticket accordingly?

I don't see any way decommission could be affected by this sort of problem.","14/Sep/11 17:27;thepaul;bq. In any case, yes the same concerns exist when giving away ranges as when gaining ranges.

Oh? I must be missing something. What would a consistency violation failure scenario look like for giving away ranges?","14/Sep/11 17:30;jbellis;bq. Expand the scope of this ticket accordingly?

Yes, let's solve them both here.","14/Sep/11 17:32;nickmbailey;My comment wasn't very clear. Both decom and move currently, attempt to do the right thing. When a node is leaving, there should be one new replica for all the ranges it is responsible for. If it can't stream data to that replica there is a consistency problem.

Both operations currently try to do stream to that replica, but we should use the 'strict' logic in those cases as well and fail if we can't guarantee consistency and the user hasn't disabled strict.","14/Sep/11 17:41;thepaul;If decom can't stream data to the appropriate replica, then it should just fail, right? Do we support decom in cases where a consistency violation would result? Seems like it has to be the user's responsibility to bring up or decom the other node first.

move could introduce a violation when it gains a new range, though, in the same cases as the bootstrap issue explained above.","14/Sep/11 17:47;nickmbailey;If we support it for bootstrapping I don't see why we shouldn't support it for decom. Right, move has the problem in both cases (giving away ranges, gaining ranges).","14/Sep/11 17:53;thepaul;I think we're talking about different things. Requiring the user to have the right nodes available for operation X is not the same as ""cassandra can 'lose' writes when it happens to stream from the wrong node, even if the user did everything right"".

This ticket is about the latter, I think.","14/Sep/11 18:32;thepaul;Conversation on #cassandra-dev resulted in the conclusion that we'll fix this bug for range acquisition (bootstrap and move) now, and plan to allow the same looseness (non-strict mode, or whatever) for range egress (move and decom) in the future.

I think.","20/Sep/11 11:58;jbellis;bq. It's always been unsupported to bootstrap a second node into the same ""token arc"" while a previous one is ongoing.

I'm pretty sure now that this is incorrect; we fixed it back in CASSANDRA-603.  I'm updating the comments in TokenMetadata as follows:

{noformat}
    // Prior to CASSANDRA-603, we just had <tt>Map<Range, InetAddress> pendingRanges<tt>,
    // which was added to when a node began bootstrap and removed from when it finished.
    //
    // This is inadequate when multiple changes are allowed simultaneously.  For example,
    // suppose that there is a ring of nodes A, C and E, with replication factor 3.
    // Node D bootstraps between C and E, so its pending ranges will be E-A, A-C and C-D.
    // Now suppose node B bootstraps between A and C at the same time. Its pending ranges
    // would be C-E, E-A and A-B. Now both nodes need to be assigned pending range E-A,
    // which we would be unable to represent with the old Map.  The same thing happens
    // even more obviously for any nodes that boot simultaneously between same two nodes.
    //
    // So, we made two changes:
    //
    // First, we changed pendingRanges to a <tt>Multimap<Range, InetAddress></tt> (now
    // <tt>Map<String, Multimap<Range, InetAddress>></tt>, because replication strategy
    // and options are per-KeySpace).
    //
    // Second, we added the bootstrapTokens and leavingEndpoints collections, so we can
    // rebuild pendingRanges from the complete information of what is going on, when
    // additional changes are made mid-operation.
    //
    // Finally, note that recording the tokens of joining nodes in bootstrapTokens also
    // means we can detect and reject the addition of multiple nodes at the same token
    // before one becomes part of the ring.
    private BiMap<Token, InetAddress> bootstrapTokens = HashBiMap.create();
    // (don't need to record Token here since it's still part of tokenToEndpointMap until it's done leaving)
    private Set<InetAddress> leavingEndpoints = new HashSet<InetAddress>();
    // this is a cache of the calculation from {tokenToEndpointMap, bootstrapTokens, leavingEndpoints}
    private ConcurrentMap<String, Multimap<Range, InetAddress>> pendingRanges = new ConcurrentHashMap<String, Multimap<Range, InetAddress>>();
{noformat}
","20/Sep/11 15:55;thepaul;bq. I'm pretty sure now that this is incorrect;

Well, doh. That puts us back at my first question:

bq. So, it looks like it will be possible for the node-that-will-be-removed to change between starting a bootstrap and finishing it (other nodes being bootstrapped/moved/decom'd during that time period); in some cases, that could still lead to a consistency violation. Is that unlikely enough that we don't care, here? At least the situation would be better with the proposed fix than it is now.",20/Sep/11 16:01;jbellis;Can you give an example for illustration?,"21/Sep/11 02:48;nickmbailey;Ok, so I think there are really two consistency issues here. Firstly, picking the 'right' node to stream data to/from when making changes to the ring. Secondly, disallowing concurrent changes that have overlapping ranges.

Currently we only disallow nodes from moving/decommissioning when they may potentially have data being streamed to them. There are a few examples of things we currently allow which I think are generally a bad idea.

1) Say you have nodes A and D, if you bootstrap nodes B and C at the same time in between A and D, it may turn out that the correct node to stream from for both nodes is D. Now say node C finishes bootstrapping before node B. At that point, the correct node for B to bootstrap from is technically C, although D still has the data. However, since D is no longer technically responsible for the data, the user could run cleanup on D and delete the data that B is attempting to stream.

2) The above case is also a problem when you bootstrap a node and the node it decides it needs to stream from is moving. Once that node finishes moving you could run cleanup on that node and delete data that the bootstrapping node needs. In this case, all documentation indicates you should do a cleanup after a move in order to remove old data, so it seems possibly more likely.

3) A variation of the above case is when you bootstrap a node and the node it streams from is leaving. In that case the decom may finish and the user could terminate the cassandra process and/or node breaking any streams. Not to mention the idea of a node in a decommissioned state continuing to stream seems like a bad idea. I believe it would work currently, but I'm not sure and it seems likely to break.

I can't really think of any other examples but I think thats enough to illustrate that overlapping concurrent ring changes are a bad idea and we should just attempt to prevent them in all cases. An argument could be made that this would prevent you from doubling your cluster (the best way to grow) all at once, but I don't think that's really a huge deal. At most you would need RF steps to double your cluster.","21/Sep/11 18:21;thepaul;I think we can still allow overlapping concurrent ring changes, with the right set of invariants and/or operational rules. I've been trying to work on defining those. It's pretty tricky but I think I have the right way of approaching the model now. Will update later today with more.

Nick is right though, c* probably shouldn't support overlapping changes as is. Think bootstrapping >N nodes between the same two old nodes, decom'ing one node and bootstrapping another in such a way that the bootstrap source stream node becomes the wrong source, etc.","21/Sep/11 18:30;nickmbailey;So the fact that the 'correct' bootstrap source switches mid stream isn't really the problem I don't think. We set up pending ranges so that when a node gets ready to bootstrap, writes start getting duplicated to both the currently correct replica, and the bootstrapping node. Since all new writes are duplicated we can stream from that node and as long as we get the entire dataset, consistency should be fine. The problem is there is nothing in place preventing someone from running cleanup or killing a decommed node or something.

I'm doubtful that the complexity of a correct set of rules for allowing overlapping ring changes is really worth the time/effort/fragility. It doesn't seem like that much of a loss to me to disallow them. Perhaps your set of rules will be super simple though :).","21/Sep/11 18:40;thepaul;Right, I know how the pending-ranges writes work. But there are still possible openings for consistency violations for previously-written data, similar to the one outlined in the original ticket description. If the bootstrap stream source has an outdated value and it gets duplicated to a bootstrapping node, but then the bootstrap stream source doesn't leave the replication set (because something else changed in the interim), the outdated value is now on more nodes than it used to be- possibly now QUORUM, when previously it was safely below.","22/Sep/11 21:01;thepaul;
Ok, prospective approach to totally safe range movements:

Operational rules:
* Cassandra will not allow two range motion operations (move, bootstrap, decom) at the same time on the same node.
* When a range motion operation is already pending, User should refrain from starting another range motion operation (if either motion operation overlaps the arc-of-effect of the other) until the gossip info about the first change has propagated to all affected nodes. (This is more simply approximated by the ""two minute rule"".)
* Every point in the tokenspace has the same number of natural endpoints, and they're ordered the same from the perspective of all nodes (is this an ok assumption?).
* It is User's responsibility to make sure that the right streaming source nodes are available. If they're not, the range motion operation may fail.

Procedure:
* For any motion involving range _R_, there will be a stream from endpoint _EP_source_ to endpoint _EP_dest_. Given the same information about what range motion operations are pending (_TokenMetadata_) and the range _R_, there is a bijection from _EP_source_ to _EP_dest_, shared by all nodes in the ring.
* Procedure to determine _EP_source_ from _EP_dest_:
** Let _REP_current_ be the existing (ordered) list of natural endpoints for _R_.
** Let _TM_future_ be a clone of the current _TokenMetadata_, but with all ongoing bootstraps, moves, and decoms resolved and completed.
** Let _REP_future_ be the list of (ordered) natural endpoints for _R_ according to _TM_future_.
** Let _EPL_entering_ be the list of endpoints in _REP_future_ which are not in _REP_current_ (preserving their order in _REP_future_).
** Let _EPL_leaving_ be the list of endpoints in _REP_current_ which are not in _REP_future_ (preserving their order in _REP_current_).
** _EPL_entering_ and _EPL_leaving_ are of the same length.
** Let _Pos_ be the position/index of _EP_dest_ in _EPL_entering_.
** Let _EP_source_ be the endpoint at position _Pos_ in _EPL_leaving_.
* Intuitively, this is the same as the rule expressed earlier in this ticket (stream from the node you'll replace), but also handles other ongoing range movements in the same token arc.
* These rules can be pretty trivially inverted to determine _EP_dest_ from _EP_source_.
* When any node gets gossip about a range motion occurring with its token arc-of-effect, it calculates (or recalculates) the streams in which it should be involved. Any ongoing streams which are no longer necessary are canceled, and any newly necessary streams are instigated.

I tried to construct a ruleset without that last rearrange-ongoing-streams rule, but it ended up with a pretty complicated set of extra restrictions, and a more complicated set of procedures than this.

This set of rules might look complicated, but I think it should be fairly straightforward to implement, and may even end up simpler overall than our current code.

Note that this procedure even maintains the consistency guarantee in cases like:

* In an RF=3 cluster with nodes A, E, and F, bootstrap B, C, and D in quick succession (E streams to B, F streams to C, A streams to D)
* In an RF=3 cluster with nodes A, C, and E, bootstrap B, D, and F, and decommission A, C, and E, all in quick succession (A streams to B, C streams to D, E streams to F)
* In an RF=3 cluster with nodes A, B, C, D, and E, decommission B and C in quick succession (B streams to D, C streams to E)","22/Sep/11 22:13;thepaul;Clarification: the cleanup operation would need to consider pending ranges in addition to the current natural range-endpoint mapping.

It would be possible with this proposal for a node _M_ to be streaming range _S_ to a bootstrapping node _Y_, and midway, for _M_ to stop being part of the replication set for _S_ (perhaps some other bootstraps nearby completed first). This should be ok for consistency, but a cleanup operation on _M_ while the stream is ongoing could potentially remove all the data in _S_ unless this change is made.

Further clarification: instead of the W+1 special case we now have for writing to a range _T_ with a pending motion, we would need to write to W+C replicas instead, where C is the number of pending motions within the replication set of _T_.","16/Nov/11 22:12;jbellis;Coming back to this after the 1.0 scramble.

It sounds like you have a reasonable solution here, is there any reason not to implement it for 1.1?","16/Nov/11 23:21;thepaul;bq. It sounds like you have a reasonable solution here, is there any reason not to implement it for 1.1?

Just that it's quite a bit more complex than simply disallowing overlapping ring movements, and the extra problems that come with higher complexity. I think this feature is worth it, on its own, but when i think of how much pain Brandon seems to be going through dealing with streaming code, maybe it's not.","16/Nov/11 23:41;jbellis;No longer very optimistic on the ""may even end up simpler overall than our current code"" front?

TBH this area of the code is fragile and hairy and maybe starting from a clean slate with a real plan instead of trying to patch things in haphazardly would be a good thing.

But, I'd be okay with re-imposing the ""no overlapping moves"" rule and fixing the stream source problem if that's going to be substantially simpler.","17/Nov/11 16:26;thepaul;No, I do think that if we tore out the existing code and replaced it, it would be simpler overall, but (a) that would probably also be true if we rewrote the existing code without implementing this; (b) it will be rather a lot of work; and (c) it may engender a whole new generation of subtle corner-case bugs (or maybe it will eliminate a lot of such bugs that already exist).","17/Nov/11 16:52;jbellis;How much work would it be to add ""just one more bandaid"" for the stream source thing, in comparison?","17/Nov/11 17:06;thepaul;Do you mean to implement the new ""safe range movements"" procedure outlined above, without rewriting the rest of the range movement code?

If so, I submit a SWAG of ""1/3-1/2 the cost of the rewrite option"". The bandaid would still touch a lot of different moving parts.","28/Nov/11 20:01;jbellis;So either way, a substantial amount of work, but the bandaid still leaves us with rules that the operator must enforce or hit subtle problems.

My hang-up with the bandaid is the part where C* can't effectively enforce the guidelines to be safe.  Even ""wait X seconds between bootstrap operations"" is not a prereq I am comfortable with.

Unless the above is incorrect, I think we should bite the bullet and fix it ""right.""","08/Feb/12 04:16;thepaul;So, I believe that the rules outlined above can still work without the ""wait X seconds between bootstrap operations"" prereq, if a pretty simple extra step is added:

If any node learns about conflicting move operations, then some rules are applied to choose which will be honored and which will return an error to its caller (if still possible).

Those rules are:
* A decom for node X beats a move or bootstrap for node X
* Two decoms for node X from coordinator nodes Y and Z: the coordinator with the higher token wins
* Any other conflicts between move/bootstrap operations for the same node (which can arise in certain partition situations) are easily resolved by latest VersionedValue.

This should guarantee convergence of TokenMetadata across any affected parts of a cluster.",09/Feb/12 21:44;jbellis;Sounds reasonable.,"02/May/12 17:32;jbellis;As a half measure, we can stream from the ""right"" node very easily if we continue to make the simplifying assumption that no other node movement happens in overlapping ranges during the operation.","19/Feb/14 17:31;tjake;I've taken a crack at this, initially for 1.2 since it solves my pain. Appreciate a review.

As [~jbellis] mentions above it requires only one node to be added at a time. Also bootstrapping node must add -Dconsistent.bootstrap=true

#code
https://github.com/tjake/cassandra/tree/2434

#dtest showing it works (use ENABLE_VNODES=yes)
https://github.com/tjake/cassandra-dtest/tree/2434

",19/Feb/14 18:33;jbellis;([~thobbs] to review),"19/Feb/14 21:05;thobbs;Thanks, Jake.

I strongly prefer to default to the strict/safe behavior and make the user supply a ""force"" option for non-strict behavior, like Nick and Paul agreed on above.  If the bootstrapping node cannot stream from the correct replica and the ""force"" option isn't set, it should abort the bootstrap with an error that describes the implications and mentions how to use the ""force"" option.

Additionally, I think your logic for picking the preferred replica could be greatly simplified.  Paul's 2434-3.patch.txt has a really simple version of this and also has the strict-by-default behavior.  It might be worthwhile to look at rebasing that patch as a start.

Paul mentioned this:

bq. Conversation on #cassandra-dev resulted in the conclusion that we'll fix this bug for range acquisition (bootstrap and move) now, and plan to allow the same looseness (non-strict mode, or whatever) for range egress (move and decom) in the future.

Looking at the irc logs, there wasn't a strong reason for this.  There's a lot of code overlap there, so it would be ideal to fix both types of operations at once.  Do you think you could take a stab at that?","19/Feb/14 22:29;tjake;bq. Additionally, I think your logic for picking the preferred replica could be greatly simplified. Paul's 2434-3.patch.txt has a really simple version of this and also has the strict-by-default behavior. It might be worthwhile to look at rebasing that patch as a start.

I did look at the patch and I'll see how I can simplify my version.  Most of the complexity comes from multiple ranges living on the same address (vnodes). Which the old version didn't have to worry about.

I do think we can fix the other operations but those are less of a priority IMO and should be part of a follow up.  (does anyone use move with vnodes?)  

","19/Feb/14 22:39;brandon.williams;bq. does anyone use move with vnodes?

No, but now they can use relocate (taketoken in nodetool)",20/Feb/14 18:57;tjake;Pushed an update to https://github.com/tjake/cassandra/tree/2434 that addresses the comments.  I'm going to work on support for move/relocate.,"20/Feb/14 20:41;tjake;Do we care about decommissions?  It seems when we ""push"" data to other nodes there isn't anything todo.  Only when we pick the replica to stream from does this ticket apply","20/Feb/14 21:27;thobbs;bq. Do we care about decommissions? It seems when we ""push"" data to other nodes there isn't anything todo. Only when we pick the replica to stream from does this ticket apply

I checked that {{StorageService.getChangedRangesForLeaving()}} pushes to the correct nodes (those that are gaining a range), so you're right, we don't need to do anything new for decom.",20/Feb/14 22:22;tjake;Updated branches with move/relocate support and added a dtest for move (in dtest branch linked above),20/Feb/14 22:33;brandon.williams;We should probably add a test for relocate too since it's fundamentally different from move.,21/Feb/14 20:12;thobbs;I tested bootstrapping a node while the preferred replica was down.  It turns out that CASSANDRA-6385 makes the bootstrapping node consider the replica up for long enough to pass the checks.  It looks like we need to special case the 6385 behavior for bootstraps if we want this patch to work.,21/Feb/14 20:24;brandon.williams;You can test this now by setting cassandra.fd_initial_value_ms.,"22/Feb/14 00:42;thobbs;Okay, with the workaround on the FD, bootstrap seems to work.  Do we want to split that fix into a separate ticket?

However, relocate seems to be seriously broken.  With a three node cluster and one of the nodes down, I can make relocate fail in a couple of ways:
* {{oldEndpoints}} == {{newEndpoints}}, so the assertion that the difference between them has length 1 fails
* There are no ranges that contain the ""desiredRange"", resulting in the IllegalStateException being thrown (""No sources found for "" + toFetch);

With that said, nothing (including the tools) uses relocate.  (EDIT: shuffle uses it, but nobody uses shuffle in practice due to other problems.) The JMX version doesn't work with jconsole, so I had to add a method to test this.  I'm not even sure that relocate worked before this patch for vnodes, because there's only minimal test coverage for relocate.  IMO, we shouldn't even try to modify this without good test coverage.  But if nothing even uses relocate... I'm not sure what to do.  Thoughts?",26/Feb/14 18:09;tjake;I will try working on a relocate test and look at addressing this,14/Mar/14 13:22;tjake;What version should this go into?  I personally need this for 1.2 but I would put it in with the default of non-strict.,18/Mar/14 19:45;thobbs;[~tjake] I think we want 2.1 with a default of strict.,"29/Apr/14 19:37;tjake;Rebased to 2.1 branch and pushed to https://github.com/tjake/cassandra/tree/2434-2

Also added a relocation dtest https://github.com/tjake/cassandra-dtest/tree/2434 ","30/Apr/14 21:57;thobbs;+1 overall, with a few minor nitpicks on the dtest:

You can replace that string-building loop with:
{noformat}
tl = "" "".join(str(t) for t in tokens[0][:8])
{noformat}

There's also a leftover line: {{#assert 1 == 0}}

Don't forget to mark the new tests with a {{@since('2.1')}} decorator.","01/May/14 13:52;tjake;Committed c* code , I'll push to dtests now","21/Jul/14 21:18;kohlisankalp;This will be very nice to have in 2.0
cc [~brandon.williams]",21/Jul/14 21:44;brandon.williams;Does seem like mostly new code we could just add with the flag defaulting to off to give 2.0 the option.,,,,
rename RandomAccessReader.MAX_BYTES_IN_PAGE_CACHE,CASSANDRA-3948,12543773,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,scode,scode,23/Feb/12 00:35,12/Mar/19 14:10,13/Mar/19 22:26,29/Feb/12 10:10,1.1.1,,,,,,0,,,,,"This should make the fadvising useless (mostly). See CASSANDRA-1470 for why, including links to kernel source. I have not investigated the history of when this broke or whether it was like from the beginning.

For the record I have not confirmed this by testing, only by code inspection. I happened to notice it working on other things, so there is some chance that I'm just mis-reading the code.
",,,,,,,,,,,,,,,,29/Feb/12 09:58;scode;CASSANDRA-3948-trunk.txt;https://issues.apache.org/jira/secure/attachment/12516540/CASSANDRA-3948-trunk.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-23 13:15:13.657,,,no_permission,,,,,,,,,,,,229012,,,Wed Feb 29 10:10:48 UTC 2012,,,,,,0|i0gq5j:,95667,scode,scode,,,,,,,,,"23/Feb/12 13:15;xedin;We algorithm behind it - we do fadvice(fd, <start_position>, 0) after each 128MB of data written, flush is done in the process of each re-buffer (which is each 64KB by default) so we can skip doing sync when we do fadvice() and just use 0 which would hint kernel so skip everything starting from <start_position>. ","26/Feb/12 00:38;scode;So as long as we're only writing few number of very large files, it should in practice work fairly well. Default settings are 30 seconds expiry of dirty buffers, up to 5% of page cache dirty, on Linux.

That said, MAX_BYTES_IN_PAGE_CACHE is thus not really max bytes in page cache, but rather just fadvise interval in bytes.
","26/Feb/12 00:43;xedin;Exactly, we can't really control (measure) the contents of the page cache so instead we just define intervals for our files when to call fadvice.","29/Feb/12 09:58;scode;Suggesting attached patch to rename the variable to reflect this (trunk only, since no functional change).",29/Feb/12 10:00;xedin;+1,29/Feb/12 10:10;scode;Committed (1.1 + trunk after all).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader throwing exceptions when loading snapshot data from compressed CFs,CASSANDRA-3521,12532297,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,pvelas,pvelas,22/Nov/11 09:33,12/Mar/19 14:10,13/Mar/19 22:26,25/Nov/11 09:20,1.0.4,,,,,,0,compression,,,,"Loaded data from snapshot then enabled  `sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor`
Then flush, scrub and compact. I can see actual CompressionRatio in JMX Console and access my data without problems..

Now I snapshot compressed keyspace and when trying to load snapshot to another single node or different Keyspace (the same super CF structure with compression options enabled, even try to truncate snapshoted CFs.) I cant retrieve any records . 


sstableloader command with debug mode dont throw any errors and shows its streaming 

{quote}
sstableloader-cassandra_2/bin/sstableloader --debug Impressions_compressed/
{quote}


Node logs contains repeating the errors bellow.

{quote}
ERROR [Thread-319] 2011-11-22 09:56:01,931 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Thread-319,5,main]
java.lang.AssertionError: attempted to delete non-existing file HidSaid-tmp-hb-260-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.streaming.IncomingStreamReader.retry(IncomingStreamReader.java:170)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:92)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)
 INFO [Thread-320] 2011-11-22 09:56:02,492 StreamInSession.java (line 120) Streaming of file Impressions_compressed/HidSaid-hb-9-Data.db sections=1 progress=0/5616749 - 0% from org.apache.cassandra.streaming.StreamInSession@3cc62c07 failed: requesting a retry.
ERROR [Thread-320] 2011-11-22 09:56:02,493 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Thread-320,5,main]
java.lang.AssertionError: attempted to delete non-existing file HidSaid-tmp-hb-261-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.streaming.IncomingStreamReader.retry(IncomingStreamReader.java:170)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:92)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)
{quote}

Hope its enough if you need more info just tell me what you need to reproduce this bug.","One node cluster (same problem with 4 nodes ) and dedicated machine for sstable loader.
node and loader running jdk1.6.0_29 and cassandra-1.0.3",,,,,,,,,,,,,,,25/Nov/11 06:14;yukim;3521.txt;https://issues.apache.org/jira/secure/attachment/12505061/3521.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-22 16:07:08.976,,,no_permission,,,,,,,,,,,,218030,,,Fri Nov 25 09:20:19 UTC 2011,,,,,,0|i0gkxj:,94821,slebresne,slebresne,,,,,,,,,"22/Nov/11 16:07;jbellis;If sstableloader says it's streaming, what does ""I can't load any data"" mean?","22/Nov/11 17:34;pvelas;progress line is updating and bitrate is changing but it never ends. And my CF is still empty.

{noformat}
sstableloader-cassandra_2/bin/sstableloader --debug Impressions_compressed
Starting client (and waiting 30 seconds for gossip) ...
Streaming revelant part of Impressions_compressed/Hid-hb-9-Data.db Impressions_compressed/Impression-hb-11-Data.db Impressions_compressed/HidSaid-hb-9-Data.db to [/10.20.30.135]

progress: [/10.20.30.135 0/3 (3851)] [total: 3851 - 7MB/s (avg: 6MB/s)]
{noformat}

{noformat}
[default@Impressions_compressed] list Impression;
Using default limit of 100

0 Row Returned.
{noformat}



When I tried to list files from OS point of view (loading only one CF right now)
Its looks like some loop . Streaming data to file and when it reach 200 MB start with new file and increase by 1.

{noformat}
[root@cass101 Impressions_compressed]# ls -sh1
total 193M
 28K Impression-tmp-hb-59-CompressionInfo.db
193M Impression-tmp-hb-59-Data.db
   0 Impression-tmp-hb-59-Index.db

[root@cass101 Impressions_compressed]# ls -sh1
total 2.0M
4.0K Impression-tmp-hb-60-CompressionInfo.db
2.0M Impression-tmp-hb-60-Data.db
   0 Impression-tmp-hb-60-Index.db
{noformat}

Hope now its better described .","22/Nov/11 18:54;pvelas;Better formulation would be ""I cant retrieve any records with cassandra-cli.""","25/Nov/11 06:14;yukim;Thanks for the detail, Peter.
Current version of sstableloader seems it does not pick up sstable's compression info before start streaming, and that causes receiver node handle streaming data incorrectly.
Attached patch let sstableloader to pick up compression information if available.","25/Nov/11 09:20;slebresne;+1, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted handoffs isn't delivered if/when HintedHandOffManager ends up in invalid state.,CASSANDRA-3546,12533423,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,fredrikl74,fredrikl74,01/Dec/11 10:42,12/Mar/19 14:10,13/Mar/19 22:26,02/Dec/11 10:45,1.0.6,,,,,,0,,,,,"Running Cassandra 1.0.3.
I've done some testing with 2 nodes (node A, node B), replication factor 2.
I take node A down, writing some data to node B and then take node A up.
Sometimes hints aren't delivered when node A comes up.

I've done some debugging in org.apache.cassandra.db.HintedHandOffManager and sometimes node B ends up in a strange state in method org.apache.cassandra.db.HintedHandOffManager.deliverHints(final InetAddress to), where org.apache.cassandra.db.HintedHandOffManager.queuedDeliveries already has node A in it's Set and therefore no hints will ever be delivered to node A.

The only reason for this that I can see is that in org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(InetAddress endpoint) the hintStore.isEmpty() check returns true and the endpoint (node A)  isn't removed from org.apache.cassandra.db.HintedHandOffManager.queuedDeliveries. Then no hints will ever be delivered again until node B is restarted.

During what conditions will hintStore.isEmpty() return true?
Shouldn't the hintStore.isEmpty() check be inside the try {} finally{} clause, removing the endpoint from queuedDeliveries in the finally block?

{code}
public void deliverHints(final InetAddress to)
{
    logger_.debug(""deliverHints to {}"", to);
    if (!queuedDeliveries.add(to))
        return;
    .......
}
{code}

{code}
private void deliverHintsToEndpoint(InetAddress endpoint) 
    throws IOException, DigestMismatchException, InvalidRequestException, TimeoutException, InterruptedException
{
     ColumnFamilyStore hintStore = Table.open(Table.SYSTEM_TABLE).getColumnFamilyStore(HINTS_CF);
     if (hintStore.isEmpty())
         return; // nothing to do, don't confuse users by logging a no-op handoff
     try
     {
         ......
     }
     finally
     {
         queuedDeliveries.remove(endpoint);
     }
}
{code} ",,,,,,,,,,,,,,,,01/Dec/11 12:26;slebresne;3546.patch;https://issues.apache.org/jira/secure/attachment/12505762/3546.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-01 11:42:34.981,,,no_permission,,,,,,,,,,,,219151,,,Fri Dec 02 10:45:30 UTC 2011,,,,,,0|i0gl8n:,94871,jbellis,jbellis,,,,,,,,,"01/Dec/11 11:42;slebresne;Patch attached, thanks Fredrik.","01/Dec/11 12:21;fredrikl74;The patch is broken, hintStore must be declared outside the try {} finally {} clause.

","01/Dec/11 12:26;slebresne;Oops, that'll teach me to not even compile before submitting. Patch updated.",01/Dec/11 19:00;jbellis;+1,"02/Dec/11 10:45;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTables iterators are closed and before being used,CASSANDRA-3110,12520904,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,31/Aug/11 10:22,12/Mar/19 14:10,13/Mar/19 22:26,31/Aug/11 14:57,1.0.0,,,,,,0,,,,,"Seems there is misplaced finally blocks in CollationController: we close the sstable iterators and release the sstable references *before* having actually used said iterators/sstables.
Note: this cause a lot of tests to fail in `ant test-compression` in particular.",,,,,,,,,,,,,,,,31/Aug/11 10:22;slebresne;3110.patch;https://issues.apache.org/jira/secure/attachment/12492440/3110.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-31 13:17:29.126,,,no_permission,,,,,,,,,,,,4086,,,Wed Aug 31 15:19:21 UTC 2011,,,,,,0|i0gfx3:,94009,jbellis,jbellis,,,,,,,,,31/Aug/11 13:17;jbellis;+1,31/Aug/11 14:57;jbellis;(committed),"31/Aug/11 15:19;hudson;Integrated in Cassandra #1060 (See [https://builds.apache.org/job/Cassandra/1060/])
    Fix closing sstable iterators before using them
patch by slebresne; reviewed by jbellis for CASSANDRA-3110

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163652
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/CollationController.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
merge from 1.0 (aa20c7206cdc1efc1983466de05c224eccac1084) breaks build,CASSANDRA-3806,12540249,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,28/Jan/12 06:05,12/Mar/19 14:10,13/Mar/19 22:26,28/Jan/12 14:22,1.1.0,,,,,,0,,,,,"{code}
build-project:
     [echo] apache-cassandra: /tmp/cas/cassandra/build.xml
    [javac] Compiling 40 source files to /tmp/cas/cassandra/build/classes/thrift
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] Compiling 296 source files to /tmp/cas/cassandra/build/classes/main
    [javac] StorageService.java:1343: illegal start of expression
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]     ^
    [javac] StorageService.java:1343: ';' expected
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                   ^
    [javac] StorageService.java:1343: ';' expected
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                     ^
    [javac] StorageService.java:1343: illegal start of expression
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                                    ^
    [javac] StorageService.java:1343: illegal start of expression
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                                          ^
    [javac] StorageService.java:1343: ';' expected
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                                           ^
    [javac] 6 errors
{code}
",,,,,,,,,,,,,,,,28/Jan/12 06:08;scode;CASSANDRA-3806.txt;https://issues.apache.org/jira/secure/attachment/12512293/CASSANDRA-3806.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-28 14:22:26.032,,,no_permission,,,,,,,,,,,,225663,,,Sat Jan 28 14:22:26 UTC 2012,,,,,,0|i0gogf:,95392,,,,,,,,,,,28/Jan/12 06:08;scode;Trivial patch attached.,"28/Jan/12 14:22;jbellis;committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsHa broken at startup,CASSANDRA-3346,12526665,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,11/Oct/11 14:12,12/Mar/19 14:10,13/Mar/19 22:26,11/Oct/11 14:51,1.0.0,,,,,,0,,,,,"{noformat}

ERROR 09:10:21,781 Exception encountered during startup
java.lang.IllegalArgumentException
        at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:589)
        at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:514)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.<init>(DebuggableThreadPoolExecutor.java:90)
        at org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor.<init>(JMXEnabledThreadPoolExecutor.java:76)
        at org.apache.cassandra.thrift.CassandraDaemon$ThriftServer.<init>(CassandraDaemon.java:192)
        at org.apache.cassandra.thrift.CassandraDaemon.startServer(CassandraDaemon.java:75)
        at org.apache.cassandra.service.AbstractCassandraDaemon.startRPCServer(AbstractCassandraDaemon.java:281)
        at org.apache.cassandra.service.AbstractCassandraDaemon.start(AbstractCassandraDaemon.java:253)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:350)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,59135,,,Tue Oct 11 14:51:41 UTC 2011,,,,,,0|i0gis7:,94473,,,,,,,,,,,11/Oct/11 14:51;brandon.williams;Stupid mistake fixed in r1181816,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hector NodeAutoDiscoverService fails to resolve hosts due to / being part of the IP address,CASSANDRA-3044,12519055,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,synfinatic,synfinatic,16/Aug/11 23:03,12/Mar/19 14:10,13/Mar/19 22:26,17/Aug/11 22:19,0.8.5,,,,,,0,,,,,"Didn't get this problem with Cassandra 0.8.2- started happening under 0.8.4.  Temporary work around was to disable node auto discovery.  Seems to be related to:

http://svn.apache.org/viewvc?view=rev&revision=1155157
http://issues.apache.org/jira/browse/CASSANDRA-1777

LOG:

240514 [pool-2-thread-1] INFO me.prettyprint.cassandra.connection.NodeAutoDiscoverService - using existing hosts [10.255.255.176(10.255.255.176):9160, 10.255.255.175(10.255.255.175):9160]
240553 [pool-2-thread-1] ERROR me.prettyprint.cassandra.service.CassandraHost - Unable to resolve host /10.255.255.176
240553 [pool-2-thread-1] INFO me.prettyprint.cassandra.connection.NodeAutoDiscoverService - Found a node we don't know about /10.255.255.176(/10.255.255.176):9160 for TokenRange TokenRange(start_token:33370589793653380361461751202224080323, end_token:93518639523624865529944734322199113946, endpoints:[/10.255.255.176])
240553 [pool-2-thread-1] INFO me.prettyprint.cassandra.connection.NodeAutoDiscoverService - Found 1 new host(s) in Ring
240553 [pool-2-thread-1] INFO me.prettyprint.cassandra.connection.NodeAutoDiscoverService - Addding found host /10.255.255.176(/10.255.255.176):9160 to pool
240554 [pool-2-thread-1] ERROR me.prettyprint.cassandra.connection.HConnectionManager - General exception host to HConnectionManager: /10.255.255.176(/10.255.255.176):9160
java.lang.IllegalArgumentException: protocol = socket host = null
        at sun.net.spi.DefaultProxySelector.select(DefaultProxySelector.java:151)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:358)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:178)
        at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
        at me.prettyprint.cassandra.connection.HThriftClient.open(HThriftClient.java:123)
        at me.prettyprint.cassandra.connection.ConcurrentHClientPool.<init>(ConcurrentHClientPool.java:43)
        at me.prettyprint.cassandra.connection.RoundRobinBalancingPolicy.createConnection(RoundRobinBalancingPolicy.java:68)
        at me.prettyprint.cassandra.connection.HConnectionManager.addCassandraHost(HConnectionManager.java:103)
        at me.prettyprint.cassandra.connection.NodeAutoDiscoverService.doAddNodes(NodeAutoDiscoverService.java:68)
        at me.prettyprint.cassandra.connection.NodeAutoDiscoverService$QueryRing.run(NodeAutoDiscoverService.java:53)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)","Cassandra 0.8.4, hector 0.8.0",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-08-17 20:36:18.71,,,no_permission,,,,,,,,,,,,20948,,,Thu Aug 18 10:38:47 UTC 2011,,,,,,0|i0gf13:,93865,,,,,,,,,,,16/Aug/11 23:12;synfinatic;driftx says on IRC that this bug was already fixed so closing it myself.,"17/Aug/11 20:36;brandon.williams;Reopening because http://svn.apache.org/viewvc?view=rev&revision=1155157 should be in 0.8.4, so something else may be going on here.",17/Aug/11 22:19;brandon.williams;We also need to apply the http://svn.apache.org/viewvc?view=rev&revision=1155157 fix in the other direction.  Fixed in r1158940.,18/Aug/11 10:38;t0mas;Issue also affects the Pelops client from Scale7 (https://github.com/s7/scale7-pelops).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in DK,CASSANDRA-3574,12533950,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,05/Dec/11 22:42,12/Mar/19 14:10,13/Mar/19 22:26,06/Dec/11 17:08,1.1.0,,,,,,0,,,,,"When running the dtests:

{noformat}
ERROR [pool-2-thread-1] 2011-12-05 16:22:53,940 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.AssertionError
        at org.apache.cassandra.db.DecoratedKey.<init>(DecoratedKey.java:56)
        at org.apache.cassandra.dht.RandomPartitioner.decorateKey(RandomPartitioner.java:47)
        at org.apache.cassandra.cql.QueryProcessor.multiRangeSlice(QueryProcessor.java:161)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:549)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1249)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

I suspect CASSANDRA-1034 is to blame.",,,,,,,,,,,,,,,,06/Dec/11 15:26;slebresne;3574.patch;https://issues.apache.org/jira/secure/attachment/12506263/3574.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-06 15:26:32.675,,,no_permission,,,,,,,,,,,,219676,,,Tue Dec 06 18:25:10 UTC 2011,,,,,,0|i0gllb:,94928,jbellis,jbellis,,,,,,,,,06/Dec/11 15:26;slebresne;I though I had fixed that already but apparently not. Patch attached.,06/Dec/11 16:08;jbellis;+1,"06/Dec/11 17:08;slebresne;Committed, thanks","06/Dec/11 18:25;hudson;Integrated in Cassandra #1240 (See [https://builds.apache.org/job/Cassandra/1240/])
    Fix assertion error in DK
patch by slebresne; reviewed by jbellis for CASSANDRA-3574

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211030
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
invalidate / unregisterSSTables is confused,CASSANDRA-3437,12529618,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,01/Nov/11 04:07,12/Mar/19 14:10,13/Mar/19 22:26,07/Nov/11 17:34,1.0.3,,,,,,0,indexing,,,,"invalidate doesn't call unregisterSSTables, and vice versa, making it easy to get yourself into a situation that ""shouldn't happen.""  This is causing test failures post-CASSANDRA-3116.",,,,,,,,,,,,,,,,06/Nov/11 02:01;jbellis;3437-v2.txt;https://issues.apache.org/jira/secure/attachment/12502634/3437-v2.txt,01/Nov/11 04:10;jbellis;3437.txt;https://issues.apache.org/jira/secure/attachment/12501732/3437.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-07 11:00:47.69,,,no_permission,,,,,,,,,,,,215477,,,Mon Nov 07 17:34:23 UTC 2011,,,,,,0|i0gjw7:,94653,slebresne,slebresne,,,,,,,,,"01/Nov/11 04:10;jbellis;patch to make invalidate do the equivalent of unregisterSSTables and removes the latter as a separate method.  Uses in tests were replaced by calling clearUnsafe.  Did have to extract CFS.unregisterMBean as a separate method for KeyCacheTest's benefit.

Patch also renames IM.unregisterMBean -> IM.invalidate.

",06/Nov/11 02:07;jbellis;v2 is against 1.0 branch.  Caught a comment change or two that v1 didn't but otherwise the substance is the same.,"07/Nov/11 11:00;slebresne;I haven't been to apply v2 cleanly on 1.0, so I haven't checked the unit tests in particular, but from reading the patch it lgtm.",07/Nov/11 14:47;jbellis;p0 problem again?  Just updated 1.0 and it applies cleanly.,"07/Nov/11 15:23;slebresne;Not a p0 problem this time but somehow I messed up updating my 1.0 branches, not sure how. Anyway, it does apply, +1.",07/Nov/11 17:34;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFS reloading of the compaction strategy is done for every metadata update and is not thread safe,CASSANDRA-3409,12529068,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,27/Oct/11 15:50,12/Mar/19 14:10,13/Mar/19 22:26,27/Oct/11 16:21,1.0.1,,,,,,0,,,,,"The reloading of the compaction strategy done during CFS.reload is not thread safe. In particular, this is a problem for leveled compactions. It could leads to some sstable not being added to the manifest and also breaks the 'only one leveledCompactionTask can run at any given time' assumption (which, at least without CASSANDRA-3408 can likely leads to blocking compactions completely).",,,,,,,,,,,,,,,,27/Oct/11 15:59;slebresne;3409.patch;https://issues.apache.org/jira/secure/attachment/12501107/3409.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-27 16:05:57.654,,,no_permission,,,,,,,,,,,,214928,,,Thu Oct 27 16:21:17 UTC 2011,,,,,,0|i0gjjj:,94596,jbellis,jbellis,,,,,,,,,"27/Oct/11 15:59;slebresne;There really is two problems:
* we reload the strategy every time we reload the CFS, i.e, for each upade_column_family, even if it's just changing gc_grace or something. This makes that bug much more problematic.
* the only easy way I see make the reload of stategy safe is to grab the compaction lock.

Patch attached that fix both (fixing the first problem makes grabing the lock not a big deal imho).",27/Oct/11 16:05;jbellis;+1,27/Oct/11 16:21;slebresne;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replaying a commitlog entry from a dropped keyspace will cause an error,CASSANDRA-2631,12506778,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/May/11 23:35,12/Mar/19 14:10,13/Mar/19 22:26,11/May/11 14:23,0.7.6,0.8.1,,,,,0,,,,,,,,,,,,,,,,,,,,,10/May/11 23:55;jbellis;2631-0.7.txt;https://issues.apache.org/jira/secure/attachment/12478748/2631-0.7.txt,10/May/11 23:55;jbellis;2631-0.8.txt;https://issues.apache.org/jira/secure/attachment/12478747/2631-0.8.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-11 08:46:54.735,,,no_permission,,,,,,,,,,,,20741,,,Wed May 11 14:34:25 UTC 2011,,,,,,0|i0gcin:,93458,slebresne,slebresne,,,,,,,,,10/May/11 23:55;jbellis;also not sure why we were copying the mutations into a new arraylist,11/May/11 08:46;slebresne;+1,"11/May/11 14:34;hudson;Integrated in Cassandra-0.7 #478 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/478/])
    avoid attempting to replay mutationsfrom dropped keyspaces
patch by jbellis; reviewed by slebresne for CASSANDRA-2631
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOFException on commitlogs,CASSANDRA-2604,12506154,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,terjem,terjem,04/May/11 10:00,12/Mar/19 14:10,13/Mar/19 22:26,06/May/11 10:17,0.8.0,,,,,,0,,,,,"I have seen this occasionally since we started testing 0.8.

It happens when reading commitlogs on startups.

However, I have seen it a lot less on 0.8 beta2 (although this is from beta 2)

ERROR [main] 2011-05-04 18:02:38,134 AbstractCassandraDaemon.java (line 330) Exception encountered during startup.
java.io.EOFException
	at java.io.DataInputStream.readByte(DataInputStream.java:250)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:357)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:368)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:252)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:43)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:136)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:126)
	at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:368)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:256)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:157)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:173)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)

Note that the line numbers on columnserializer may be off due to some local changes, but those changes are in code not executed in this case and I am 100% sure they do not trigger this problem.

I looked on this in the debugger in eclipse on a trunk from 0.8 2 weeks ago, and the interesting thing I saw was that according to the debugger, the offset of the inputstream to the deserializer was already at the end (very last byte) of the underlying bytebuffer but according to the stack, it was trying to read the length of the column name (first read done in the deserialized).
",,,,,,,,,,,,,,,,05/May/11 13:47;slebresne;0001-avoid-modifying-original-mutation-during-apply.patch;https://issues.apache.org/jira/secure/attachment/12478274/0001-avoid-modifying-original-mutation-during-apply.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-04 18:24:13.848,,,no_permission,,,,,,,,,,,,20726,,,Fri May 06 10:17:38 UTC 2011,,,,,,0|i0gcd3:,93433,,,,,,,,,,,"04/May/11 18:24;jbellis;I strongly suspect this is present in 0.7, too: CASSANDRA-2197 looks like the same stacktrace.","04/May/11 23:09;terjem;Time for some sleep, but I stepped through this code with a bad commit table (not the same which caused the above error though, but produces same stack).

    public void deserializeColumns(DataInput dis, ColumnFamily cf, boolean intern, boolean fromRemote) throws IOException
    {
        int size = dis.readInt();
        ColumnFamilyStore interner = intern ? Table.open(CFMetaData.getCF(cf.id()).left).getColumnFamilyStore(cf.id()) : null;
        for (int i = 0; i < size; ++i)
        {
            IColumn column = cf.getColumnSerializer().deserialize(dis, interner, fromRemote, (int) (System.currentTimeMillis() / 1000));
            cf.addColumn(column);
        }
    }

The size of the buffer underlying ""dis"" is 153 byte.

""size"" in the above code is 4.

With i == 2 finished, dis has just reached position 153 (perfectly) in the underlying buffer, although I cannot guarantee that the value is completely read.

Obviously, when trying to read at i==3, it crashes as it is already at the end of the buffer.

I would expect the entry which it is trying to decode in this case to have 4, not 3 columns.

I do see some of my own code is actually involved here, but unless the commitlog does its own serialization fully or partly and uses the ""standard"" deserializer my code is unlikely to cause any problems like this.","05/May/11 13:47;slebresne;Terje, do you use secondary indexes in your tests ?

I think I've found a race (related to secondary indexes) that would explain the observed behavior. But without a good way to reproduce, hard to say if that is the problem.

The problem is in Table.apply(), where ignoreObsoleteMutations() may remove some columns from the original mutation. And since the commit log aliases the same mutation and, unless you are in batch mode, can write it anytime during apply(), it is possible for a column to be removed between the time where the commit log serialize the number of columns and the actual serialization of the columns (explaining why there is less columns than advertised on replay).

Attaching a patch that lazily clone the column family in ignoreObsoleteMutation to avoid modifying the original one. Note that 0.7 is not concerned, because in 0.7 we give to the commit log the mutation already serialized.","05/May/11 14:27;terjem;Secondary indexes - Check  (the error at least in the case I studied now is on a CF with secondary index)
Not in batch mode - Check 

I am not 100% sure I understand in what cases a column may be removed. 

There should be no deletes here and I think no updates to columns that already exists (I will have to check that...), but I am messing a bit around with the system while a couple of feeders are continuously running and they will refeed if there are errors. It is very likely that there are are quite a few retries going on at times which I guess could trigger what you are referring too?

I will apply the patch at once and test.

Thanks!","05/May/11 15:47;jbellis;I don't think it's actually necessary to even modify the original mutation, since applying obsolete columns to the data path is harmless.  I removed that entirely in the CASSANDRA-2401 patch (not yet applied).","05/May/11 17:56;slebresne;I propose to see if Terje can reproduce. If not, this was the actual problem and I guess we'll close this on the account that CASSANDRA-2401 fixed it as a side effect.","05/May/11 18:50;terjem;After stressing the system a bit with a bunch of restarts, I have not manage to reproduce the problem again, so looks good so far.

I will leave it feeding overnight and try one more time in the morning and then I will try to reproduce with 2401 instead.

On a side notice... I sometimes see quite a few commitlog segments. Right now, one node had 110 of them and it took 20 minutes to read all the logs.

This seems a bit excessive?",05/May/11 19:02;jbellis;We're working on CASSANDRA-2427 to deal with commitlog segment proliferation.,"06/May/11 10:15;terjem;Not able to reproduce the problem with the patch in this ticket.

Currently using CASSANDRA-2401 and so far so good.

I think you can just close this ticket for now and I will reopen if I see the problem again.

Thanks!","06/May/11 10:16;slebresne;Alright, great. Thanks for the testing.",06/May/11 10:17;slebresne;Resolving as fixed by CASSANDRA-2401,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe_ring topology information is wrong/incomplete,CASSANDRA-3403,12528813,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,patricioe,brandon.williams,brandon.williams,26/Oct/11 02:46,12/Mar/19 14:10,13/Mar/19 22:26,26/Oct/11 19:48,1.0.1,,,,,,0,,,,,"In CASSANDRA-2882, topology information was added to describe_ring, however it asks the gossiper for the DC information, and the gossiper can only have this with a gossip-enabled snitch, which currently means the Ec2Snitch.  Instead, it should be asking the snitch for the DC for each endpoint.

Also, the port information should just be removed: whatever port the client has connected to in order to call describe_ring is the right port to use for all endpoints.",,,,,,,,,,,,,,,,26/Oct/11 17:42;patricioe;trunk-3403-v1.diff;https://issues.apache.org/jira/secure/attachment/12500918/trunk-3403-v1.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-26 17:44:13.785,,,no_permission,,,,,,,,,,,,214673,,,Wed Oct 26 19:48:27 UTC 2011,,,,,,0|i0gjgn:,94583,brandon.williams,brandon.williams,,,,,,,,,"26/Oct/11 17:44;patricioe;Patch tested with SimpleSnitch and PropertyFileSnitch for one and two datacenter.

Also tested using HectorAutoDiscoveryService.","26/Oct/11 19:48;brandon.williams;Commited, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage uses comparator for both super column names and sub column names.,CASSANDRA-3251,12524432,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,danapsimer,danapsimer,23/Sep/11 16:18,12/Mar/19 14:10,13/Mar/19 22:26,31/Jan/12 21:42,0.8.10,1.0.8,,,,,0,cassandra,hadoop,pig,,"The CassandraStorage class uses the same comparator for super and sub column names.

This is because it calls columnsToTuple recursively without any indication that the subsequent call is for sub columns.  Also, the getDefaultMarshallers method does not return the sub column name comparator.",,,,,,,,,,,,,,,,10/Jan/12 19:34;brandon.williams;3251-v2.txt;https://issues.apache.org/jira/secure/attachment/12510086/3251-v2.txt,31/Jan/12 20:25;xedin;CASSANDRA-3251-v3.patch;https://issues.apache.org/jira/secure/attachment/12512623/CASSANDRA-3251-v3.patch,23/Sep/11 16:31;danapsimer;CASSANDRA-3251.patch;https://issues.apache.org/jira/secure/attachment/12496280/CASSANDRA-3251.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-10-19 18:22:57.662,,,no_permission,,,,,,,,,,,,3525,,,Tue Jan 31 21:42:10 UTC 2012,,,,,,0|i0ghn3:,94288,brandon.williams,brandon.williams,,,,,,,,,23/Sep/11 16:29;danapsimer;This is a small fix for the super column issues.,"19/Oct/11 18:22;brandon.williams;Dana, can you rebase?  I can't get this to apply.",10/Jan/12 19:34;brandon.williams;v2 with a slightly different approach.,"31/Jan/12 19:42;xedin;isSub = 2 lead to key_validator, it should be changed to 3. Wouldn't it be a good idea to set 'AbstractType comparator' instead of 'boolean isSub' in columnToTuple(...)?",31/Jan/12 20:25;xedin;v3 with implements proposed parameter change to AbstractType comparator and removes unnecessary 'ByteBuffer name' parameter.,31/Jan/12 21:32;brandon.williams;+1,31/Jan/12 21:42;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Risk of counter over-count when recovering commit log,CASSANDRA-2419,12503450,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,05/Apr/11 20:49,12/Mar/19 14:09,13/Mar/19 22:26,08/May/11 01:42,0.8.0,,,,,,1,counters,,,,"When a memtable was flush, there is a small delay before the commit log replay position gets updated. If the node fails during this delay, all the updates of this memtable will be replay during commit log recovery and will end-up being over-counts.",,28800,28800,,0%,28800,28800,,,,,,,,,29/Apr/11 13:57;slebresne;0001-Record-CL-replay-infos-alongside-sstables-v2.patch;https://issues.apache.org/jira/secure/attachment/12477754/0001-Record-CL-replay-infos-alongside-sstables-v2.patch,07/Apr/11 00:38;slebresne;0001-Record-and-use-sstable-replay-position.patch;https://issues.apache.org/jira/secure/attachment/12475649/0001-Record-and-use-sstable-replay-position.patch,02/May/11 19:53;jbellis;2419-v3.txt;https://issues.apache.org/jira/secure/attachment/12477981/2419-v3.txt,03/May/11 01:57;jbellis;2419-v4.txt;https://issues.apache.org/jira/secure/attachment/12478017/2419-v4.txt,03/May/11 15:37;jbellis;2419-v6.txt;https://issues.apache.org/jira/secure/attachment/12478061/2419-v6.txt,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-04-05 21:25:59.077,,,no_permission,,,,,,,,,,,,20613,,,Tue May 10 22:30:29 UTC 2011,,,,,,0|i0gb9j:,93255,jbellis,jbellis,,,,,,,,,"05/Apr/11 21:02;slebresne;One solution I see to this problem would be to record along with the replay position the time when we last updated this replay position. The during recover, we would first look at all the sstables (for the CF) and if a sstable is freshly flushed (which implies that we have a marker to know that a sstable was never compacted) and have a modification time higher that the last time we updated the replay position, then we'll just remove the sstable since we know it will be fully replayed.

Note that to work correctly we also need a way to mark a freshly flushed sstable as 'non compactable' during the time it takes to mark the commit log.

We would probably only do this for counter CF just to be on the safe side.

Opinions ?","05/Apr/11 21:25;jbellis;What if instead of the CL ""header"" we record the CL context as part of an sstable footer?  (footer is less likely to cause bugs w/ sstable math that assumes 0 = start of first row.)  then there is no race.","05/Apr/11 21:34;jbellis;Hmm, I think we need both the CL header and this information, since this flush footer would only give us when we flushed which is not the same as ""do I need to replay.""

For instance: if there is no flush marker for a commitlog segment in any existing sstable, that does not necessarily mean no data is in the commitlog for that CF.

So replay position would be max(dirty at from CL header, flushed at from sstable footers).

(You would need to allow multiple flush contexts in a single sstable footer, to preserve them during compaction.)",05/Apr/11 21:39;slebresne;What about a new component .metadata for each sstable instead of a footer. I actually think we will have a use for other sstable metadata at some point anyway. For instance we could keep the file format version. That way we wouldn't rely so much on the data file name.,05/Apr/11 21:44;jbellis;A separate component is a better idea.,"05/Apr/11 21:52;slebresne;Actually I don't think this really solves the race condition. We really shouldn't compact the newly flushed sstable until we marked the commit log, because if we compact it, even if we're able to detect that 'some parts' of a sstable will be replay during recover, there is nothing we can do about it.","05/Apr/11 22:01;jbellis;I don't understand the problem.  Say we have this situation:

CommitLog-1302036825548.log: [full of writes to CF Foo counters, up to position 100.  header reads dirty-at 50, our last flush position]
Foo-g-45-Metadata.db: [flushed at position (1302036825548, 50)]
Foo-g-46-Metadata.db: [flushed at position (1302036825548, 100)]

We compact and get
Foo-g-47-Metadata.db: [flushed at position (1302036825548, 100)]

If we die and restart here we will correctly start reply of Foo at position 100 in this segment.

(we can combine to a single flushed-at entry in this case since they were from the same CL segment.  if they were from different segments we would keep both.)","05/Apr/11 22:10;slebresne;Right, that was just me not getting you idea at first. Make sense, sorry.",07/Apr/11 00:38;slebresne;Attaching patch implementing Jonathan's idea to record the CL replay position along with the sstable.,"07/Apr/11 00:40;slebresne;I'm wondering, couldn't we just drop the commit log header if we do that.","07/Apr/11 01:56;jbellis;Yes, I think we can.","14/Apr/11 21:39;jbellis;bq. I'm wondering, couldn't we just drop the commit log header if we do that.

Were you planning to update w/ that change?","29/Apr/11 13:57;slebresne;v2 removes commit log header completely in favor of sstable metadata about where to replay (patch against 0.8).

This differs from v1 in that instead of keeping every (segment, replay_position) pair, we keep for a given sstable, only the position for the most recent segment (that is, we leverage the fact that we use increasing timestamps for commit logs).

The reason for this is twofold:
  # this more compact (and simple)
  # if we remove the commit log header, we need to be able to say if a given segment is dirty or not for a given column family. That is, we don't want to know if some replay position existed on this segment, but if a relevant one still exist. So for a given column family we really only care about the newest (segment, replay_position) pair.

Now there is the question of the update path. With this patch, the (existing) commit log headers will be ignored. This means that ideally before updating to a version having this patch people would use drain. If they do not, then the commit logs will be fully replayed. Pre-0.8, it's not a big deal. With counters, this could mean over-counts (that's exactly what this ticket is about). So I would be in favor of putting this for 0.8.0, since it is a bug fix and it will avoids the problem of upgrading from a version already having counters. But I would admit this is not trivial patch, so ...
","02/May/11 19:53;jbellis;v3 attached with some changes:

- SSTableMetadata removed; replayposition becomes a field in SSTable that is serialized w/ statistics
- RP is final and part of the SSTable constructor
- RP implements Comparable instead of a one-off resolve API; RP.getReplayPosition encapsulates the find-replay-point logic
- RP moved to a top-level class and replaces CLContext

I'd like to make the metadata a json blob so we can extend it more easily, so I probably need to re-introduce SSTM. Consider v3 a work in progress.","03/May/11 01:57;jbellis;I tried two ways of storing metadata as yaml -- first with the metadata as java beans that were stored directly as yaml, and second half-manually serializing to a yaml Map<String, Object> -- and both feel clunkier than just using the version field to deal with adding things. (Especially when you need to do version checks anyway when modifying things rather than just adding new fields.)

So, v4 is substantially the same as v3 but Descriptor version is bumped to g and we use that instead of EOF to when reading RP. (Also, writeStatistics is renamed to writeMetadata.)","03/May/11 12:32;slebresne;v4 looks good, I like those changes (note: I committed r1099037 to fix CFSTest, it had a test with hardcoded sstable filenames using version 'f' and thus was failing)",03/May/11 15:00;jbellis;v5 updates CL replay to use RP.getReplayPosition as well,03/May/11 15:37;jbellis;v6 fixes a test failure in v5,"03/May/11 17:05;slebresne;Minor nitpick: in CommitLog.java:recover(), was there a reason to create a new List<ReplayPosition> positions instead of using cfPositions.values() ?

but other than that, +1 on v6.","03/May/11 17:35;jbellis;bq. in CommitLog.java:recover(), was there a reason to create a new List<ReplayPosition> positions instead of using cfPositions.values()

Nope. I'll fix that before commit.

Asked Paul Cannon to also review first though, since obviously we want to be extra sure not to cause regressions here.",08/May/11 01:42;jbellis;committed,09/May/11 19:12;stuhood;Thanks a ton for this work! Transactionality here we come.,"10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
preserve KsDef backwards compatibility for Thrift clients,CASSANDRA-2486,12504380,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,15/Apr/11 15:07,12/Mar/19 14:09,13/Mar/19 22:26,18/Apr/11 15:51,0.8 beta 1,,,Legacy/CQL,,,0,,,,,CASSANDRA-1263 broke client compatibility; we can't preserve it entirely (we'll continue to resturn replication_factor in strategy option rather than try to guess somehow if client is an old one) but we can accommodate old clients on write and leave the KsDef signature compatible which (I think) will make it easier for client authors.,,,,,,,,,,,,,,,,15/Apr/11 15:08;jbellis;2486.txt;https://issues.apache.org/jira/secure/attachment/12476456/2486.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-15 18:00:52.038,,,no_permission,,,,,,,,,,,,20648,,,Mon Apr 18 15:51:42 UTC 2011,,,,,,0|i0gbnb:,93317,zznate,zznate,,,,,,,,,15/Apr/11 15:08;jbellis;patch attached.,15/Apr/11 18:00;zznate;Patch applies clean (provided thrift classes are rebuilt). Unit tests pass as do hector's system_*_keyspace tests.,"16/Apr/11 01:29;jbellis;The compatibility story doesn't change substantially here -- either way you can do the 99% of operations that don't deal w/ schema changes. But does this make it easier to build a Hector compatible w/ both 0.7 and 0.8 (by looking at the server api version), since you can use the same generated Thrift now?

If so then I'll go ahead and commit.","18/Apr/11 15:51;jbellis;committed, with some fixes to the python tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig cannot use output formats other than CFOF,CASSANDRA-3826,12540773,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,01/Feb/12 20:18,12/Mar/19 14:09,13/Mar/19 22:26,02/Feb/12 19:14,1.1.0,,,,,,0,,,,,Pig has ColumnFamilyOutputFormat hard coded.,,,,,,,,,,,,,,,,02/Feb/12 18:27;brandon.williams;3826.txt;https://issues.apache.org/jira/secure/attachment/12513013/3826.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-01 21:44:54.156,,,no_permission,,,,,,,,,,,,226168,,,Thu Feb 02 19:14:58 UTC 2012,,,,,,0|i0goof:,95428,xedin,xedin,,,,,,,,,"01/Feb/12 21:09;brandon.williams;Patch to keep CFIF/OF as defaults, but allow overriding both with environment variables.","01/Feb/12 21:44;xedin;I guess we better do `if (format.contains("".""))` at the time when {input,output}format is set instead of getter methods? I also can suggest to make ""org.apache.cassandra.hadoop.ColumnFamilyInputFormat"" and ""org.apache.cassandra.hadoop.ColumnFamilyOutputFormat"" as DEFAULT_{INPUT, OUTPUT}_FORMAT and just set them to {input,output}format variables when user didn't give any by System.env(...), what do you think?","02/Feb/12 18:27;brandon.williams;Updated patch.  I wanted to do some of this in ConfigHelper, but it doesn't really make sense because we can't cleanly return instance like getOutputPartitioner does since we have both the old and new hadoop interfaces to comply with, but also because Configuration is not how hadoop determines the input/output formats, those are set on the Job directly.  So it's probably best to keep this pig-specific, since other M/R jobs can already control these classes that way.","02/Feb/12 18:54;xedin;+1 with following nit:

{code}
private String getFullyQualifiedClassName(String classname)
{
    String fqcn = classname.contains(""."") ? classname : ""org.apache.cassandra.hadoop."" + classname;
    return fqcn;
}
{code}

can be changed to 

{code}
private String getFullyQualifiedClassName(String classname)
{
    return classname.contains(""."") ? classname : ""org.apache.cassandra.hadoop."" + classname;
}
{code}",02/Feb/12 19:14;brandon.williams;Committed w/ternary change.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
maybeInit in ColumnFamilyRecordReader can cause rows to be empty but not null,CASSANDRA-3450,12530083,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lannyripple,lannyripple,lannyripple,03/Nov/11 17:20,12/Mar/19 14:09,13/Mar/19 22:26,04/Nov/11 21:29,0.8.8,1.0.3,,,,,0,,,,,"1) In {{ColumnFamilyRecordReader}} {{isPredicateEmpty}} needs bracing to correctly place the {{else if}} to the properly controlling {{if}}.

1a) {{isPredicateEmpty}} should use an || in the getSlice_range predicate rather than &&.

2) In {{ColumnFamilyRecordReader}} {{computeNext()}} calls {{maybeInit()}} and then if {{ros}} is not null it is indexed into.  {{maybeInit()}} could fetch new data, determine the associated slice predicate is empty, and end up removing all the rows if all columns turned out to be empty.  There is no check for {{rows.isEmpty()}} after the possible removal of all rows.

",,,,,,,,,,,,,,,,04/Nov/11 16:23;lannyripple;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3450.txt;https://issues.apache.org/jira/secure/attachment/12502477/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3450.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-04 21:29:00.755,,,no_permission,,,,,,,,,,,,215942,,,Tue Nov 08 16:03:09 UTC 2011,,,,,,0|i0gk1z:,94679,brandon.williams,brandon.williams,,,,,,,,,"04/Nov/11 21:29;brandon.williams;Committed, thanks!","05/Nov/11 00:05;hudson;Integrated in Cassandra-0.8 #393 (See [https://builds.apache.org/job/Cassandra-0.8/393/])
    Fix empty row filtering and check if there are no rows returned.
Patch by Lanny Ripple, reviewed by brandonwilliams for CASSANDRA-3450

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1197786
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
",08/Nov/11 14:00;tjake;Reverted this change due to a bug related to single node test failures.  Going to attempt a fresh fix at this and CASSANDRA-2855  ,"08/Nov/11 16:03;hudson;Integrated in Cassandra-0.8 #395 (See [https://builds.apache.org/job/Cassandra-0.8/395/])
    Revert CASSANDRA-3450

jake : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1199242
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should handle wide rows,CASSANDRA-3909,12542535,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,14/Feb/12 14:29,12/Mar/19 14:08,13/Mar/19 22:26,16/Apr/12 22:42,1.1.1,,,,,,0,,,,,Pig should be able to use the wide row support in CFIF.,,,,,,,,,,,,,,,,16/Apr/12 14:14;brandon.williams;3909.txt;https://issues.apache.org/jira/secure/attachment/12522774/3909.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-16 22:36:17.633,,,no_permission,,,,,,,,,,,,227821,,,Thu Apr 19 15:48:48 UTC 2012,,,,,,0|i0gpon:,95591,xedin,xedin,,,,,,,,,"16/Apr/12 14:14;brandon.williams;CASSANDRA-3264 (and subsequently CASSANDRA-3883) added wide row support to hadoop, by returning one column in the row in every call.  Pig, however, is fancy enough that it could handle a wide row in a bag, since bags spill to disk; it just needs the pagination for transport since thrift doesn't stream.  Also, if we returned what CFIF gave us, a user wanting to work within the row would need another costly M/R job to join the row back to its original state, so we essentially need to 'undo' the pagination and rebuild the row as a bag.   This patch does that, with the caveat that you cannot access any indexes (and frankly if you have indexes on a wide row you're probably doing something wrong) since it's impossible for us to order the indexes correctly ahead of time in a wide row.",16/Apr/12 22:36;xedin;+1,16/Apr/12 22:42;brandon.williams;Committed.,"17/Apr/12 21:20;brandon.williams;Sylvain, any reason we can't put this in 1.1.0?  It has to be explicitly enabled so it can't break anything existing, and it goes well with the hadoop wide row support we already put in 1.1.0.","17/Apr/12 21:21;mdennis;+1 on inclusion in 1.1.0 (and if not, ASAP after 1.1.0)","19/Apr/12 12:54;slebresne;Is that a big deal if it's only in 1.1.1? I mean, personally I do trust you on that ""this can't break anything"" and I don't object on putting it in 1.1.0. I do however think that in general there would be some merit to stick to more strict rules. But that's not a debate related to this issue in particular so let's leave that discussing to some other venue.
","19/Apr/12 15:48;brandon.williams;bq. personally I do trust you on that ""this can't break anything""

<3

bq. I do however think that in general there would be some merit to stick to more strict rules.

I agree, however my reasoning is thus: if we support wide rows in 1.1.0 (and we do) then why not pig?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quorum returns incorrect results during hinted handoff,CASSANDRA-3395,12528316,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,21/Oct/11 20:03,12/Mar/19 14:08,13/Mar/19 22:26,04/Nov/11 08:38,1.0.2,,,,,,0,,,,,"In a 3 node cluster with RF=3 and using a single coordinator, if monotonically increasing columns are inserted into a row and the latest one sliced (both at QUORUM) during HH replay occasionally this column will not be seen.",,,,,,,,,,,,,,,,21/Oct/11 20:05;brandon.williams;logs.tar.bz2;https://issues.apache.org/jira/secure/attachment/12500235/logs.tar.bz2,01/Nov/11 16:21;brandon.williams;ttest.py;https://issues.apache.org/jira/secure/attachment/12501791/ttest.py,21/Oct/11 20:05;brandon.williams;ttestraw.py;https://issues.apache.org/jira/secure/attachment/12500236/ttestraw.py,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-10-27 21:38:38.711,,,no_permission,,,,,,,,,,,,112709,,,Fri Nov 04 08:38:53 UTC 2011,,,,,,0|i0gjd3:,94567,,,,,,,,,,,"21/Oct/11 20:05;brandon.williams;Logs from a failure where 20172 was expected but 20171 was returned and the pycassa script to reproduce.  Procedure is start the script and kill a node, wait, bring it up back up and wait for HH.  Takes a few tries.","27/Oct/11 21:38;jbellis;This sure looks like a case of ""if timestamps are equal, all bets are off"" to me.  Suggest testing w/ explicitly increasing timestamps.","01/Nov/11 16:13;jbellis;Sorry, mis-read the script.  Brandon points out that there are no column overwrites, only appends.","01/Nov/11 16:21;brandon.williams;Equivalent script using telephus that does not repro.  Slightly hackish in that it won't create the ks/cf, but the pycassa script can do that and be killed before it inserts.","01/Nov/11 17:23;jbellis;This is tricky, but we figured out what's happening.

First, hinted handoff isn't important to reproducing, but bouncing nodes is.  You need a node to miss an update to get this.

If you do that, then you can get this situation, as seen in Brandon's log:

all 3 nodes reply: (not strictly necessary, all we need is two nodes that disagree with each other)
{noformat}
DEBUG [ReadRepairStage:8] 2011-10-21 19:35:23,105 RowDigestResolver.java (line 62) resolving 3 responses
{noformat}

The responses don't match:
{noformat}
DEBUG [pool-2-thread-6] 2011-10-21 19:35:23,105 StorageProxy.java (line 615) Digest mismatch: org.apache.cassandra.service.DigestMismatchException: Mismatch for key DecoratedKey(91747740688180627279175449712403223124, 747465737472617732) (6705a2ef7042fd98f2c30c5450d33e17 vs bf8c16eb98f3209d3abb723ee8c33185)
{noformat}

The coordinator requests the actual data from each replica and merges the responses:
{noformat}
DEBUG [pool-2-thread-6] 2011-10-21 19:35:23,107 SliceQueryFilter.java (line 123) collecting 0 of 2147483647: 00004ecb:false:4@1319226304173446
DEBUG [pool-2-thread-6] 2011-10-21 19:35:23,108 SliceQueryFilter.java (line 123) collecting 1 of 2147483647: 00004ecc:false:4@1319226304178304
{noformat}

Note that 00004ecb=20171, and 00004ecc=20172.  So both columns are present, and the coordinator now has a slice of [20171, 20172].  It repairs the missing data, then reverses the order as requested in the query and returns [20172, 20171] to the client.

So, the bug on the Cassandra side is that we don't re-restrict the resultset to the requested count after a digest mismatch, before sending it to the client.

Then, the client calls popitem() on the result, which means you get back the *last* item in the resultset, i.e., 20171.

In short: Cassandra needs to fix sending back more results than requested when there are different versions on different nodes that need to be resolved.  We'll address this as part of the related CASSANDRA-3303.  Unfortunately, this code path is one that we know from experience is easy to introduce regressions to, so I don't think we can safely do this in 0.8; the fix will be in 1.0.2+.

However, a simple workaround exists, which is for clients to consume results from the front of the row, instead of the back.  This is what the telphus script did by accident, which is why Brandon couldn't reproduce with that version.","04/Nov/11 08:38;slebresne;CASSANDRA-3303 has been committed with the fix for this. As said by Jonathan above, the fix is 1.0 only.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion error when forwarding to local nodes,CASSANDRA-3539,12533131,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,29/Nov/11 15:59,12/Mar/19 14:07,13/Mar/19 22:26,29/Nov/11 17:04,1.0.5,,,,,,0,,,,,"CASSANDRA-3530 introduces a regression as reported on irc:
{quote}
Started a rolling upgrade from 1.0.3 to 1.0.4 now all boxes are constantly spitting out this assert: at 
org.apache.cassandra.db.RowMutationVerbHandler.forwardToLocalNodes(RowMutationVerbHandler.java:71)
{quote}",,,,,,,,,,,,,,,,29/Nov/11 16:01;slebresne;3539.patch;https://issues.apache.org/jira/secure/attachment/12505485/3539.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-29 16:05:58.292,,,no_permission,,,,,,,,,,,,218859,,,Tue Nov 29 17:04:09 UTC 2011,,,,,,0|i0gl5j:,94857,jbellis,jbellis,,,,,,,,,29/Nov/11 16:01;slebresne;The problem was that we were writing a FOWARD_HEADER with an empty byte array in case there were no local nodes to forward to.,29/Nov/11 16:05;jbellis;+1,"29/Nov/11 17:04;slebresne;Committed, thanks

(for those running into this, note that you'll have to update *all* nodes before stopping seeing the exception showing up)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkRecordWriter will throw NullExceptions if no data is sent with the reducer,CASSANDRA-3944,12543634,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,22/Feb/12 07:55,12/Mar/19 14:07,13/Mar/19 22:26,22/Feb/12 08:03,1.1.0,,,,,,0,,,,,"In the BulkRecordWriter, in the close() method if no actual output is sent to the reducer, which can be caused by having too many reducers and not enough map data, we throw null exceptions and the job can fail.",,,,,,,,,,,,,,,,22/Feb/12 08:01;lenn0x;0001-Fix-BulkRecordWriter-to-not-throw-NPE-if-reducer-get.patch;https://issues.apache.org/jira/secure/attachment/12515549/0001-Fix-BulkRecordWriter-to-not-throw-NPE-if-reducer-get.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,228873,,,Wed Feb 22 08:03:01 UTC 2012,,,,,,0|i0gq3r:,95659,,,,,,,,,,,22/Feb/12 08:03;lenn0x;Commited,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions during inserting emtpy string as column value on indexed column,CASSANDRA-4031,12545840,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,yukim,mdymarek,mdymarek,09/Mar/12 13:09,12/Mar/19 14:07,13/Mar/19 22:26,20/Mar/12 23:05,1.1.0,,,,,,0,,,,,"Hi,
I`m running one node cluster(issue occurs also on other cluster(which has 2 nodes)) on snapshot from cassandra-1.1 branch(i used 449e037195c3c504d7aca5088e8bc7bd5a50e7d0 commit).
i have simple CF, definition of TestCF:
{noformat}
[default@test_keyspace] describe Test_CF;
    ColumnFamily: Test_CF
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      DC Local Read repair chance: 0.0
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: [Test_CF.Test_CF_test_index_idx]
      Column Metadata:
        Column Name: test_index
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Name: Test_CF_test_index_idx
          Index Type: KEYS
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
      Compression Options:
        sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor
{noformat}
I`m trying to add new row(log from cassandra-cli, note that there is index on test_index):
{noformat}
[default@test_keyspace] list Test_CF;              
Using default limit of 100

0 Row Returned.
Elapsed time: 31 msec(s).
[default@test_keyspace] set Test_CF[absdsad3][test_index]='';
null
TimedOutException()
	at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15906)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:788)
	at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:772)
	at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:894)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:211)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)
[default@test_keyspace] list Test_CF;                        
Using default limit of 100
-------------------
RowKey: absdsad3
=> (column=test_index, value=, timestamp=1331298173009000)

1 Row Returned.
Elapsed time: 7 msec(s).
{noformat}
Exception from system.log:
{noformat}
 INFO [FlushWriter:56] 2012-03-09 13:42:02,500 Memtable.java (line 291) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-3251-Data.db (2077 bytes)
ERROR [MutationStage:2291] 2012-03-09 13:42:22,232 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MutationStage:2291,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.DecoratedKey.<init>(DecoratedKey.java:55)
        at org.apache.cassandra.db.index.SecondaryIndexManager.getIndexKeyFor(SecondaryIndexManager.java:294)
        at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:490)
        at org.apache.cassandra.db.Table.apply(Table.java:441)
        at org.apache.cassandra.db.Table.apply(Table.java:366)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:275)
        at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:446)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1228)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,19/Mar/12 20:21;yukim;4031.txt;https://issues.apache.org/jira/secure/attachment/12518933/4031.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-09 15:43:16.3,,,no_permission,,,,,,,,,,,,231023,,,Tue Mar 20 23:05:20 UTC 2012,,,,,,0|i0gr5r:,95830,jbellis,jbellis,,,,,,,,,"09/Mar/12 15:43;slebresne;It's because indexed value are used as row key (in the index). Previously, empty row key was actually allowed by the code (but imo it was more of a bug we were not aware of).

The right fix may probably be to refuse empty value, though that could appear as a limitation. The other solution would be to create a special constructor for DK that allows an empty byte buffer. But I couldn't swear that other part of the code don't break subtly with empty row keys.","12/Mar/12 22:45;yukim;Same error happens when performing searching for empty value(='') on indexed column.

I agree with Sylvain, empty row key should not be allowed. But for version 1.1, I think it is fine to use empty row key on secondary indices, otherwise we have to perform full data scan and filter out all that have empty value on indexed column(or refuse query which has ""=''"").

I will fix this by adding empty key DK only for secondary index.","19/Mar/12 20:21;yukim;For v1.1, I propose not to check for empty key inside DK, removing assertion in its constructor.
I also added key validation check to CQL insert/update so that no empty key gets inserted(CLI/thrift already do validation).

At first I tried to allow empty key DK only in secondary index by creating static method that generates DK with empty key, but it turned out to be redundant because Memtable#put recreates DK when storing.",20/Mar/12 23:05;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary  ReadRepair request during RangeScan,CASSANDRA-3843,12541058,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,jbellis,philip.andronov,philip.andronov,03/Feb/12 11:40,12/Mar/19 14:07,13/Mar/19 22:26,09/Feb/12 21:43,1.0.8,,,,,,0,,,,,"During reading with Quorum level and replication factor greater then 2, Cassandra sends at least one ReadRepair, even if there is no need to do that. 

With the fact that read requests await until ReadRepair will finish it slows down requsts a lot, up to the Timeout :(

It seems that the problem has been introduced by the CASSANDRA-2494, unfortunately I have no enought knowledge of Cassandra internals to fix the problem and do not broke CASSANDRA-2494 functionality, so my report without a patch.

Code explanations:
{code:title=RangeSliceResponseResolver.java|borderStyle=solid}
class RangeSliceResponseResolver {
    // ....
    private class Reducer extends MergeIterator.Reducer<Pair<Row,InetAddress>, Row>
    {
    // ....

        protected Row getReduced()
        {
            ColumnFamily resolved = versions.size() > 1
                                  ? RowRepairResolver.resolveSuperset(versions)
                                  : versions.get(0);
            if (versions.size() < sources.size())
            {
                for (InetAddress source : sources)
                {
                    if (!versionSources.contains(source))
                    {
                          
                        // [PA] Here we are adding null ColumnFamily.
                        // later it will be compared with the ""desired""
                        // version and will give us ""fake"" difference which
                        // forces Cassandra to send ReadRepair to a given source
                        versions.add(null);
                        versionSources.add(source);
                    }
                }
            }
            // ....
            if (resolved != null)
                repairResults.addAll(RowRepairResolver.scheduleRepairs(resolved, table, key, versions, versionSources));
            // ....
        }
    }
}
{code}


{code:title=RowRepairResolver.java|borderStyle=solid}
public class RowRepairResolver extends AbstractRowResolver {
    // ....
    public static List<IAsyncResult> scheduleRepairs(ColumnFamily resolved, String table, DecoratedKey<?> key, List<ColumnFamily> versions, List<InetAddress> endpoints)
    {
        List<IAsyncResult> results = new ArrayList<IAsyncResult>(versions.size());

        for (int i = 0; i < versions.size(); i++)
        {
            // On some iteration we have to compare null and resolved which are obviously
            // not equals, so it will fire a ReadRequest, however it is not needed here
            ColumnFamily diffCf = ColumnFamily.diff(versions.get(i), resolved);
            if (diffCf == null)
                continue;
        // .... 
{code}

Imagine the following situation:
NodeA has X.1 // row X with the version 1
NodeB has X.2 
NodeC has X.? // Unknown version, but because write was with Quorum it is 1 or 2

During the Quorum read from nodes A and B, Cassandra creates version 12 and send ReadRepair, so now nodes has the following content:
NodeA has X.12
NodeB has X.12

which is correct, however Cassandra also will fire ReadRepair to NodeC. There is no need to do that, the next consistent read have a chance to be served by nodes {A, B} (no ReadRepair) or by pair {?, C} and in that case ReadRepair will be fired and brings nodeC to the consistent state

Right now we are reading from the Index a lot and starting from some point in time we are getting TimeOutException because cluster is overloaded by the ReadRepairRequests *even* if all nodes has the same data :(",,,,,,,,,,,,,,,,09/Feb/12 21:43;jbellis;3843-v2.txt;https://issues.apache.org/jira/secure/attachment/12514012/3843-v2.txt,09/Feb/12 04:29;jbellis;3843.txt;https://issues.apache.org/jira/secure/attachment/12513904/3843.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-09 04:29:53.382,,,no_permission,,,,,,,,,,,,226410,,,Tue Feb 21 16:48:32 UTC 2012,,,,,,0|i0govz:,95462,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,"09/Feb/12 04:29;jbellis;The null version was added for CASSANDRA-2680.  I think the core problem here is that the RSRR is being created with *all* the replica endpoints ({{liveEndpoints}}), not just the ones being asked to respond to the query ({{handler.endpoints}}).  This is a bit tricky since the handler wants to know its resolver at creation time, and unlike RowDigestResolver, RSRR wants to initialize its endpoints at creation time too.  Kind of hackish patch attached.
",09/Feb/12 05:19;vijay2win@yahoo.com;+1,"09/Feb/12 15:57;philip.andronov;> The null version was added for CASSANDRA-2680.
Oh, good point. Sorry, I've should pay more attention on git history, not only on annotations :)

Anyway, thanks for the patch, now we could apply correct patch on our servers.","09/Feb/12 21:43;jbellis;committed, with the same fix for the 2nd occurrence of the RSRR in 1.0 StorageProxy. v2 attached in case that makes it easyer for anyone to test.","13/Feb/12 13:58;jeromatron;We'll be upgrading to 1.0.8 as soon as we can, but this seems like a significant issue for anyone doing range scans - does it make sense to backport to 0.8.x?","13/Feb/12 18:33;jbellis;It's a relatively small patch, but StorageProxy and its callbacks can be fragile...  I almost didn't commit it to 1.0 either.  Tell you what though, I'll post a backported patch here and if you want you can run with it. :)",13/Feb/12 18:36;jbellis;Looks to me like the 1.0 code changes from v2 apply cleanly to 0.8.  (CHANGES diff does not apply but can be ignored.),"14/Feb/12 08:16;jeromatron;I patched the version of 0.8.4 that we use with the change.  I applied it to all of our staging nodes.  However, the problem with writes on the column family it was simply doing range scans of still persists.  I had major compacted a column family on all of the nodes, then did a simple pig job to read the contents of that CF, then I got a lot of minor compactions for that column family.",14/Feb/12 16:58;jbellis;I suggest testing with a single range scan at debug level.  Too much hay to see the needle when you're doing 100s or 1000s of scans.,"14/Feb/12 16:58;jbellis;... You did patch with v2, right?",14/Feb/12 17:07;jeromatron;I did patch with v2.  Doing more testing today and it appears that there are writes occurring but it looks like a definite reduction.  It could be a valid repair thing.  I'll do some more testing and hopefully repair every node and compact every node and then do a scan across a large column family and see what happens.,"21/Feb/12 14:29;jeromatron;I did repairs on all the nodes and then compacts on all the nodes. Then I did a pig job to simply count the number of rows in the column family. Again I think the overall writes were reduced but there are writes going on. I need to turn debug on and do the same test again. I did the compactions at 6:42 and the range scans at 14:16:

{code}
-rw-r--r-- 1 root root 40106228511 Feb 21 06:42 account_snapshot-g-792-Data.db
-rw-r--r-- 1 root root   206884816 Feb 21 06:42 account_snapshot-g-792-Filter.db
-rw-r--r-- 1 root root  2913796038 Feb 21 06:42 account_snapshot-g-792-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 06:42 account_snapshot-g-792-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-793-Compacted
-rw-r--r-- 1 root root      287286 Feb 21 14:16 account_snapshot-g-793-Data.db
-rw-r--r-- 1 root root         976 Feb 21 14:16 account_snapshot-g-793-Filter.db
-rw-r--r-- 1 root root       20857 Feb 21 14:16 account_snapshot-g-793-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:16 account_snapshot-g-793-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-794-Compacted
-rw-r--r-- 1 root root    87770771 Feb 21 14:17 account_snapshot-g-794-Data.db
-rw-r--r-- 1 root root      293944 Feb 21 14:17 account_snapshot-g-794-Filter.db
-rw-r--r-- 1 root root     6377968 Feb 21 14:17 account_snapshot-g-794-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:17 account_snapshot-g-794-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-795-Compacted
-rw-r--r-- 1 root root    78459166 Feb 21 14:17 account_snapshot-g-795-Data.db
-rw-r--r-- 1 root root      262600 Feb 21 14:17 account_snapshot-g-795-Filter.db
-rw-r--r-- 1 root root     5698156 Feb 21 14:17 account_snapshot-g-795-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:17 account_snapshot-g-795-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-796-Compacted
-rw-r--r-- 1 root root    69838937 Feb 21 14:17 account_snapshot-g-796-Data.db
-rw-r--r-- 1 root root      234000 Feb 21 14:17 account_snapshot-g-796-Filter.db
-rw-r--r-- 1 root root     5077447 Feb 21 14:17 account_snapshot-g-796-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:17 account_snapshot-g-796-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-797-Compacted
-rw-r--r-- 1 root root    68094433 Feb 21 14:17 account_snapshot-g-797-Data.db
-rw-r--r-- 1 root root      227808 Feb 21 14:17 account_snapshot-g-797-Filter.db
-rw-r--r-- 1 root root     4943098 Feb 21 14:17 account_snapshot-g-797-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:17 account_snapshot-g-797-Statistics.db
-rw-r--r-- 1 root root   304163307 Feb 21 14:20 account_snapshot-g-798-Data.db
-rw-r--r-- 1 root root     1019776 Feb 21 14:20 account_snapshot-g-798-Filter.db
-rw-r--r-- 1 root root    22096669 Feb 21 14:20 account_snapshot-g-798-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:20 account_snapshot-g-798-Statistics.db
-rw-r--r-- 1 root root    65874829 Feb 21 14:18 account_snapshot-g-799-Data.db
-rw-r--r-- 1 root root      220192 Feb 21 14:18 account_snapshot-g-799-Filter.db
-rw-r--r-- 1 root root     4777809 Feb 21 14:18 account_snapshot-g-799-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:18 account_snapshot-g-799-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-800-Compacted
-rw-r--r-- 1 root root    50067413 Feb 21 14:18 account_snapshot-g-800-Data.db
-rw-r--r-- 1 root root      167416 Feb 21 14:18 account_snapshot-g-800-Filter.db
-rw-r--r-- 1 root root     3632313 Feb 21 14:18 account_snapshot-g-800-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:18 account_snapshot-g-800-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-801-Compacted
-rw-r--r-- 1 root root    50575719 Feb 21 14:18 account_snapshot-g-801-Data.db
-rw-r--r-- 1 root root      169160 Feb 21 14:18 account_snapshot-g-801-Filter.db
-rw-r--r-- 1 root root     3669880 Feb 21 14:18 account_snapshot-g-801-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:18 account_snapshot-g-801-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-802-Compacted
-rw-r--r-- 1 root root    41788766 Feb 21 14:19 account_snapshot-g-802-Data.db
-rw-r--r-- 1 root root      139776 Feb 21 14:19 account_snapshot-g-802-Filter.db
-rw-r--r-- 1 root root     3033069 Feb 21 14:19 account_snapshot-g-802-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:19 account_snapshot-g-802-Statistics.db
-rw-r--r-- 1 root root    46547146 Feb 21 14:19 account_snapshot-g-803-Data.db
-rw-r--r-- 1 root root      155720 Feb 21 14:19 account_snapshot-g-803-Filter.db
-rw-r--r-- 1 root root     3378457 Feb 21 14:19 account_snapshot-g-803-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:19 account_snapshot-g-803-Statistics.db
-rw-r--r-- 1 root root   142719184 Feb 21 14:20 account_snapshot-g-804-Data.db
-rw-r--r-- 1 root root      478576 Feb 21 14:19 account_snapshot-g-804-Filter.db
-rw-r--r-- 1 root root    10356119 Feb 21 14:20 account_snapshot-g-804-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:20 account_snapshot-g-804-Statistics.db
-rw-r--r-- 1 root root    55373874 Feb 21 14:19 account_snapshot-g-805-Data.db
-rw-r--r-- 1 root root      185160 Feb 21 14:19 account_snapshot-g-805-Filter.db
-rw-r--r-- 1 root root     4017391 Feb 21 14:19 account_snapshot-g-805-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:19 account_snapshot-g-805-Statistics.db
-rw-r--r-- 1 root root    46399227 Feb 21 14:19 account_snapshot-g-806-Data.db
-rw-r--r-- 1 root root      155120 Feb 21 14:19 account_snapshot-g-806-Filter.db
-rw-r--r-- 1 root root     3365947 Feb 21 14:19 account_snapshot-g-806-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:19 account_snapshot-g-806-Statistics.db
-rw-r--r-- 1 root root    58491393 Feb 21 14:19 account_snapshot-g-807-Data.db
-rw-r--r-- 1 root root      196048 Feb 21 14:19 account_snapshot-g-807-Filter.db
-rw-r--r-- 1 root root     4253922 Feb 21 14:19 account_snapshot-g-807-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:19 account_snapshot-g-807-Statistics.db
-rw-r--r-- 1 root root    47609635 Feb 21 14:20 account_snapshot-g-808-Data.db
-rw-r--r-- 1 root root      159320 Feb 21 14:20 account_snapshot-g-808-Filter.db
-rw-r--r-- 1 root root     3456985 Feb 21 14:20 account_snapshot-g-808-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:20 account_snapshot-g-808-Statistics.db
-rw-r--r-- 1 root root    46923060 Feb 21 14:20 account_snapshot-tmp-g-809-Data.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-tmp-g-809-Index.db
-rw-r--r-- 1 root root    49693602 Feb 21 14:20 account_snapshot-tmp-g-810-Data.db
-rw-r--r-- 1 root root      166600 Feb 21 14:20 account_snapshot-tmp-g-810-Filter.db
-rw-r--r-- 1 root root     3614750 Feb 21 14:20 account_snapshot-tmp-g-810-Index.db
{code}",21/Feb/12 15:51;brandon.williams;I'm unable to repro against 1.0 HEAD.,21/Feb/12 16:48;jeromatron;Thanks - good to know - we'll upgrade to 1.0.8 as soon as we can then.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PerRowSecondaryIndexes skip the first column on update,CASSANDRA-3441,12529800,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,tjake,tjake,tjake,02/Nov/11 03:03,12/Mar/19 14:07,13/Mar/19 22:26,02/Nov/11 13:57,1.0.2,,,,,,0,,,,,PerRowSecondaryIndexes skip the initial field on apply,,,,,,,,,,,,,,,,02/Nov/11 03:04;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3441-fix-lost-first-column-bug.txt;https://issues.apache.org/jira/secure/attachment/12501902/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3441-fix-lost-first-column-bug.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-02 03:08:59.089,,,no_permission,,,,,,,,,,,,215659,,,Wed Nov 02 13:57:22 UTC 2011,,,,,,0|i0gjxz:,94661,jasonrutherglen,jasonrutherglen,,,,,,,,,02/Nov/11 03:08;jasonrutherglen;Looks good +1,02/Nov/11 13:57;tjake;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SelectStatement start/end key are not set correctly when a key alias is involved,CASSANDRA-3700,12537365,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,05/Jan/12 17:32,12/Mar/19 14:07,13/Mar/19 22:26,10/Jan/12 18:56,1.0.7,,,Legacy/CQL,,,0,cql,,,,"start/end key are set by antlr in WhereClause, but this depends on the ""KEY"" keyword.",,,,,,,,,,,,,,,,10/Jan/12 18:37;jbellis;3700-case-insensitivity.txt;https://issues.apache.org/jira/secure/attachment/12510077/3700-case-insensitivity.txt,06/Jan/12 21:07;xedin;CASSANDRA-3700.patch;https://issues.apache.org/jira/secure/attachment/12509714/CASSANDRA-3700.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-01-06 16:11:15.869,,,no_permission,,,,,,,,,,,,222873,,,Tue Jan 10 18:56:46 UTC 2012,,,,,,0|i0gn6f:,95185,jbellis,jbellis,,,,,,,,,05/Jan/12 17:32;jbellis;My suggestion would be to have antlr just generate a list of expressions and let QueryProcessor determine which are for the key and which are for columns.,"05/Jan/12 23:00;jbellis;This also affects 0.8, but if people haven't hit the problem there yet then they're probably not going to.  (And the workaround of ""just use KEY instead"" is straightforward.)  Let's fix this in 1.0+.",06/Jan/12 16:08;jbellis;EntityType is basically obsolete now isn't it?,"06/Jan/12 16:11;xedin;we will need it in the situation where KEY is involved, we probably can use default alias if it's guaranteed to be KEY tho...","06/Jan/12 16:20;jbellis;Right, it either has to be the key alias or KEY.","06/Jan/12 16:32;slebresne;Is there a real advantage to allow the use of key alias *or* KEY. That is, why don't we just allow the key alias, and have this be KEY (which we already do) by default for CF created through thrift without a proper key_alias (case that is hopefully bound to disappear). The fact that we reserve KEY (and key too I believe) is kind of limiting in that you cannot use KEY for some other column.
Besides, the KEY notation will be slightly less meaningful post-2474 because it will not necessarily correspond to the primary key anymore (i.e, it may correspond only to the first column column that compose the PRIMARY KEY).","06/Jan/12 16:44;jbellis;bq. why don't we just allow the key alias, and have this be KEY (which we already do) by default for CF created through thrift without a proper key_alias 

That makes a lot of sense to me.",06/Jan/12 21:08;xedin;updated patch with does not special-case KEY and removes EntityType.,"06/Jan/12 23:06;jbellis;I think this is obsolete now (in QP.process) since we use the ""realKeyAlias"" in WhereClause now to check if in fact an identifier is a key reference:

{code}
.               if (select.getKeys().size() > 0)
                    validateKeyAlias(metadata, select.getKeyAlias());
{code}","06/Jan/12 23:19;xedin;<key> IN (.., ..., ...) condition is treated directly without relaying on extractKeysFromColumns() so we still need this check in such case.","07/Jan/12 05:37;jbellis;I guess that's okay, although we will probably want generalize IN to allow <column> IN (...) as well at some point.

+1 on this patch",07/Jan/12 14:40;xedin;Committed.,"10/Jan/12 18:35;jbellis;Ugh, we should actually continue to allow using KEY in 1.0.x.  Working on a patch for that and case-insensitivity for the aliases.",10/Jan/12 18:48;xedin;+1,10/Jan/12 18:56;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
catch invalid key_validation_class before instantiating UpdateColumnFamily,CASSANDRA-3102,12520679,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,30/Aug/11 03:05,12/Mar/19 14:21,13/Mar/19 22:26,30/Aug/11 16:58,0.8.5,,,Legacy/CQL,,,0,,,,,,,,,,,,,,,,,,,,,30/Aug/11 03:06;jbellis;3102.txt;https://issues.apache.org/jira/secure/attachment/12492189/3102.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-30 16:58:17.205,,,no_permission,,,,,,,,,,,,20970,,,Tue Aug 30 17:21:58 UTC 2011,,,,,,0|i0gfqf:,93979,xedin,xedin,,,,,,,,,30/Aug/11 03:06;jbellis;one-line fix attached,30/Aug/11 16:58;xedin;Committed.,"30/Aug/11 17:21;hudson;Integrated in Cassandra-0.8 #304 (See [https://builds.apache.org/job/Cassandra-0.8/304/])
    Catch invalid key_validation_class before instantiating UpdateColumnFamily
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3102

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163291
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[patch] BufferedInputStream.skip only skips bytes that are in the buffer, so keep skipping until done",CASSANDRA-3034,12518824,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,14/Aug/11 15:43,12/Mar/19 14:21,13/Mar/19 22:26,15/Aug/11 04:03,1.0.0,,,,,,0,,,,,"code calls skip(remaining) without checking result. Skip isn't guaranteed to skip what you requested, especially BufferedInputStream, so keep skipping until the remaining bytes is 0.",,,,,,,,,,,,,,,,14/Aug/11 15:44;dbrosius@apache.org;skip_fully.diff;https://issues.apache.org/jira/secure/attachment/12490385/skip_fully.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-15 04:03:45.575,,,no_permission,,,,,,,,,,,,20942,,,Mon Aug 15 04:03:45 UTC 2011,,,,,,0|i0geyv:,93855,jbellis,jbellis,,,,,,,,,"15/Aug/11 04:03;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableImportTest fails on Windows because of malformed file path,CASSANDRA-3043,12519054,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vloncar,vloncar,vloncar,16/Aug/11 22:34,12/Mar/19 14:21,13/Mar/19 22:26,17/Aug/11 13:22,0.8.5,,,Legacy/Testing,,,0,windows,,,,SSTableImportTest uses URL.getPath() to create path to JSON files. This fails on Windows in many cases (for example if there are spaces in path which get encoded as %20 which Windows doesn't like). Trick is to create URI from URL which satisfies all platforms.,Windows,,,,,,,,,,,,,,,17/Aug/11 02:31;jbellis;3043-v2.txt;https://issues.apache.org/jira/secure/attachment/12490593/3043-v2.txt,16/Aug/11 22:37;vloncar;windows-sstable-import-fix.patch;https://issues.apache.org/jira/secure/attachment/12490581/windows-sstable-import-fix.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-17 02:31:37.056,,,no_permission,,,,,,,,,,,,20947,,,Wed Aug 17 14:22:07 UTC 2011,,,,,,0|i0gf0v:,93864,jbellis,jbellis,,,,,,,,,16/Aug/11 22:36;vloncar;Patch to fix SSTableImportTest,17/Aug/11 02:31;jbellis;v2 moves the logic to a method and adds comments.  Does that look good to you?,"17/Aug/11 07:18;vloncar;LGTM.

There is a minor typo in the comment, says ""The rick is..."".","17/Aug/11 13:22;jbellis;committed, thanks!","17/Aug/11 14:22;hudson;Integrated in Cassandra-0.8 #283 (See [https://builds.apache.org/job/Cassandra-0.8/283/])
    fix SSTIT on Windows
patch by Vladimir Loncar; reviewed by jbellis for CASSANDRA-3043

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1158693
Files : 
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/tools/SSTableImportTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] EstimatedHIstogram doesn't overide equals correctly,CASSANDRA-3053,12519235,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,18/Aug/11 04:28,12/Mar/19 14:21,13/Mar/19 22:26,18/Aug/11 19:41,1.0.0,,,,,,0,,,,,"EstimatedHistogram declares equals with an EstimatedHistogram parameter, instead of Object, thus only working correctly for statically typed EstimatedHistogram references. Fixed to take Object parm.",,,,,,,,,,,,,,,,18/Aug/11 04:29;dbrosius@apache.org;override_equals.diff;https://issues.apache.org/jira/secure/attachment/12490746/override_equals.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-18 19:41:54.727,,,no_permission,,,,,,,,,,,,20951,,,Thu Aug 18 20:21:00 UTC 2011,,,,,,0|i0gf4v:,93882,jbellis,jbellis,,,,,,,,,"18/Aug/11 19:41;jbellis;committed, thanks!","18/Aug/11 20:21;hudson;Integrated in Cassandra #1034 (See [https://builds.apache.org/job/Cassandra/1034/])
    update EH.equals to work with any Object
patch by Dave Brosius; reviewed by jbellis for CASSANDRA-3053

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1159374
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/utils/EstimatedHistogram.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CLI ""drop index"" doesn't handle numeric-only hex column identifiers properly",CASSANDRA-3054,12519305,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,dkuebric,dkuebric,18/Aug/11 15:39,12/Mar/19 14:21,13/Mar/19 22:26,19/Aug/11 15:03,0.8.5,,,Legacy/Tools,,,0,,,,,"CLI drop index doesn't accept requests to drop columns whose hex names include only numeric characters.  The 617070 column name below is col2.

[default@Host] use MyKeyspace;
Authenticated to keyspace: MyKeyspace
[default@Host] drop index on MyCF.617070; 
Syntax error at position 22: mismatched input '617070' expecting Identifier

While drop index seems to parse correctly with alpha chars included:

[default@Host] drop index on MyCF.617070x;
Column '617070x' definition was not found in ColumnFamily 'MyCF'.
[default@Host] drop index on MyCF.col2;
Column 'col2' definition was not found in ColumnFamily 'MyCF'.

cassandra-user thread: http://mail-archives.apache.org/mod_mbox/cassandra-user/201108.mbox/%3CB2D1533B-C69E-467A-9653-1D086E33227C@thelastpickle.com%3E",,,,,,,,,,,,CASSANDRA-3075,,,,19/Aug/11 12:45;xedin;CASSANDRA-3054.patch;https://issues.apache.org/jira/secure/attachment/12490950/CASSANDRA-3054.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-19 13:40:11.31,,,no_permission,,,,,,,,,,,,20952,,,Fri Aug 19 15:20:07 UTC 2011,,,,,,0|i0gf5b:,93884,jbellis,jbellis,,,,,,,,,19/Aug/11 13:40;jbellis;does allowing those different terms for columnName cause problems elsewhere?  I'm fine requiring quoting for non-Identifier.,19/Aug/11 13:45;xedin;it's used only by drop index statement (because other commands should support function calls too) so it would not affect other statements.,19/Aug/11 14:42;jbellis;+1,19/Aug/11 15:03;xedin;Committed.,"19/Aug/11 15:20;hudson;Integrated in Cassandra-0.8 #286 (See [https://builds.apache.org/job/Cassandra-0.8/286/])
    Fix of numeric-only and string column names handling in CLI ""drop index""
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3054

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1159657
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/Cli.g
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accomodate missing encryption_options in IncomingTcpConnection.stream,CASSANDRA-3212,12523189,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,15/Sep/11 13:09,12/Mar/19 14:21,13/Mar/19 22:26,15/Sep/11 13:57,0.8.6,,,,,,0,,,,,,,,,,,,,,,,,,,,,15/Sep/11 13:19;jbellis;3212.txt;https://issues.apache.org/jira/secure/attachment/12494613/3212.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-15 13:23:56.434,,,no_permission,,,,,,,,,,,,4017,,,Thu Sep 15 14:19:27 UTC 2011,,,,,,0|i0gh5r:,94210,slebresne,slebresne,,,,,,,,,15/Sep/11 13:19;jbellis;one-line fix.,15/Sep/11 13:23;slebresne;+1,15/Sep/11 13:57;jbellis;committed,"15/Sep/11 14:19;hudson;Integrated in Cassandra-0.8 #329 (See [https://builds.apache.org/job/Cassandra-0.8/329/])
    Accomodate missing encryption_options in IncomingTcpConnection.stream
patch by jbellis; reviewed by slebresne for CASSANDRA-3212

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1171091
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/IncomingTcpConnection.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFMetata.getMergeShardChance returns readRepairChance,CASSANDRA-3202,12522911,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,slebresne,slebresne,13/Sep/11 15:39,12/Mar/19 14:21,13/Mar/19 22:26,13/Sep/11 17:00,0.8.6,,,,,,0,counters,,,,"The summary says it all. Note that CASSANDRA-3178 will hopefully make that option useless. But in the meantime, this breaks the tests in 1.0.0 since read repair chance was set to 0.1 by default (while the test assumes merge_shard_chance is 1), so let's fix that quickly.",,,,,,,,,,,,,,,,13/Sep/11 15:40;slebresne;3202.patch;https://issues.apache.org/jira/secure/attachment/12494250/3202.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-13 16:16:26.19,,,no_permission,,,,,,,,,,,,4025,,,Tue Sep 13 17:22:08 UTC 2011,,,,,,0|i0gh1j:,94191,jbellis,jbellis,,,,,,,,,13/Sep/11 16:16;jbellis;+1,"13/Sep/11 17:00;slebresne;Committed, thanks","13/Sep/11 17:22;hudson;Integrated in Cassandra-0.8 #326 (See [https://builds.apache.org/job/Cassandra-0.8/326/])
    Fix CFMetata.getMergeShardChance returning readRepairChance
patch by slebresne; reviewed by jbellis for CASSANDRA-3202

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1170236
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't assume the Table instance has been open when dropping a keyspace,CASSANDRA-3580,12534054,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,06/Dec/11 17:03,12/Mar/19 14:21,13/Mar/19 22:26,10/Dec/11 19:17,0.8.9,1.0.6,,,,,0,,,,,"DropKeyspace assumes that the Table had been open (rather, it checks that Table.clear() don't return null). It has been seen however that in the case of a fat client (the sstableloader in that case, but it would be true for any fat client) the Table may not have been open. Let's just remove that assertion.",,,,,,,,,,,,,,,,06/Dec/11 17:05;slebresne;0001-Super-awesome-fix.patch;https://issues.apache.org/jira/secure/attachment/12506279/0001-Super-awesome-fix.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-10 19:17:54.502,,,no_permission,,,,,,,,,,,,219780,,,Sat Dec 10 20:22:24 UTC 2011,,,,,,0|i0glnr:,94939,jbellis,jbellis,,,,,,,,,10/Dec/11 19:17;jbellis;committed,"10/Dec/11 20:22;hudson;Integrated in Cassandra-0.8 #417 (See [https://builds.apache.org/job/Cassandra-0.8/417/])
    remove invalid assertion that table was opened before dropping it
patch by slebresne; reviewed by jbellis for CASSANDRA-3580

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1212849
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/DropKeyspace.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: HELP for DELETE_USING and DELETE_COLUMNS broken,CASSANDRA-3588,12534260,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,07/Dec/11 21:59,12/Mar/19 14:21,13/Mar/19 22:26,08/Dec/11 16:30,1.0.6,,,Legacy/Tools,,,0,cqlsh,,,,"type ""HELP DELETE_USING"" or ""HELP DELETE_COLUMNS"" in cqlsh. both of those are referred to by the HELP index, by tab-completion after ""HELP"", and in the help text for DELETE.

nothing is shown but a new command prompt.",,,,,,,,,,,,,,,,07/Dec/11 22:00;thepaul;0001-cqlsh-fix-HELP-for-DELETE_USING-DELETE_COLUMNS.txt;https://issues.apache.org/jira/secure/attachment/12506527/0001-cqlsh-fix-HELP-for-DELETE_USING-DELETE_COLUMNS.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-08 16:30:16.783,,,no_permission,,,,,,,,,,,,219982,,,Thu Dec 08 16:30:16 UTC 2011,,,,,,0|i0glrj:,94956,urandom,urandom,,,,,,,,,07/Dec/11 22:00;thepaul;add the missing 'print' calls for those two help functions.,08/Dec/11 16:30;urandom;committed; thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in Table.all(),CASSANDRA-3529,12532653,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,24/Nov/11 20:36,12/Mar/19 14:21,13/Mar/19 22:26,24/Nov/11 20:54,1.0.4,,,,,,0,,,,,"{noformat}
    [junit] java.util.ConcurrentModificationException
    [junit]     at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
    [junit]     at java.util.HashMap$KeyIterator.next(HashMap.java:828)
    [junit]     at com.google.common.collect.Iterators$8.next(Iterators.java:750)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1509)
    [junit]     at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:118)
    [junit]     at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:45)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
    [junit]     at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,24/Nov/11 20:38;jbellis;3529.txt;https://issues.apache.org/jira/secure/attachment/12505036/3529.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-24 20:54:31.182,,,no_permission,,,,,,,,,,,,218386,,,Thu Nov 24 20:54:31 UTC 2011,,,,,,0|i0gl13:,94837,xedin,xedin,,,,,,,,,24/Nov/11 20:38;jbellis;Patch to make Schema.tables a concurrent map,24/Nov/11 20:54;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableImport/Export don't handle tombstone well if value validation != BytesType,CASSANDRA-3357,12527002,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,13/Oct/11 10:46,12/Mar/19 14:21,13/Mar/19 22:26,13/Oct/11 18:50,0.8.8,1.0.1,,Legacy/Tools,,,0,,,,,"SSTableImport/Export use the value validator even on tomstone, but for those the value is the local deletion time, so this don't necessarily validate (with UTF8Type for instance)",,,,,,,,,,,,,,,,13/Oct/11 10:47;slebresne;3357.patch;https://issues.apache.org/jira/secure/attachment/12498866/3357.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-13 15:39:28.338,,,no_permission,,,,,,,,,,,,85332,,,Thu Oct 13 18:50:28 UTC 2011,,,,,,0|i0giwf:,94492,jbellis,jbellis,,,,,,,,,13/Oct/11 15:39;jbellis;+1,"13/Oct/11 17:22;hudson;Integrated in Cassandra-0.8 #369 (See [https://builds.apache.org/job/Cassandra-0.8/369/])
    Fix handling of tombstone by SSTableExport/Import when validation != BytesType
patch by slebresne; reviewed by jbellis for CASSANDRA-3357

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1182950
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/SSTableExport.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/SSTableImport.java
",13/Oct/11 18:50;slebresne;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'show schema' in cli does not show compression_options,CASSANDRA-3368,12527358,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,christianmovi,christianmovi,16/Oct/11 16:09,12/Mar/19 14:21,13/Mar/19 22:26,16/Oct/11 22:43,1.0.1,,,Legacy/Tools,,,0,,,,,"Hi,

using the cassandra-cli command line tool, I realized that a 'show schema' does not print out the compression_options I specified when creating them.
Both, the server and the cli tool, where version 1.0.0-rc2.


Example:

[default@Test] CREATE COLUMN FAMILY Response2 WITH key_validation_class=BytesType AND compression_options {sstable_compression:DeflateCompressor};


[default@Test] show schema;
create keyspace Test
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = [{replication_factor : 2}];
use Test;
create column family Response2
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and keys_cached = 200000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider';


Not really critical, but might be confusing for some people = me ;-)

kind regards,
Christian",Ubuntu Linux amd64,,,,,,,,,,,,,,,16/Oct/11 21:35;xedin;CASSANDRA-3368.patch;https://issues.apache.org/jira/secure/attachment/12499224/CASSANDRA-3368.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-16 21:36:02.738,,,no_permission,,,,,,,,,,,,87501,,,Sun Oct 16 22:43:59 UTC 2011,,,,,,0|i0gj1b:,94514,brandon.williams,brandon.williams,,,,,,,,,16/Oct/11 21:36;xedin;rebased with the latest cassandra-1.0 branch (last commit 757e1273c9ea7fd836b7a5cac03f1e7e8128f68b),16/Oct/11 22:39;brandon.williams;+1,16/Oct/11 22:43;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sstable scrub status persists in compactionstats after scrub is complete,CASSANDRA-3255,12524501,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,alienth,alienth,24/Sep/11 04:37,12/Mar/19 14:21,13/Mar/19 22:26,28/Sep/11 15:52,0.8.7,,,,,,0,compaction,,,,"When scrubbing the sstables on a node, the 'Scrub' info persists in the 'compactionstats' nodetool utility, even after the scrub is complete.",,,,,,,,,,,,,,,,28/Sep/11 12:00;xedin;CASSANDRA-3255.patch;https://issues.apache.org/jira/secure/attachment/12496872/CASSANDRA-3255.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-25 02:06:32.862,,,no_permission,,,,,,,,,,,,1834,,,Wed Sep 28 16:16:32 UTC 2011,,,,,,0|i0ghov:,94296,jbellis,jbellis,,,,,,,,,25/Sep/11 02:06;jeromatron;Is this the same as CASSANDRA-3236?,"25/Sep/11 02:16;alienth;Yep, didn't see that. I'm fine with closing this as a duplicate.","25/Sep/11 04:54;alienth;Actually, the other issue claims that the scrub is hanging. The issue I've listed here is that the information just persists even after completion.

This is very easy to test. Just create a CF, add some data, flush, and then run scrub a few times in a row. The scrubs will all complete, but all of the info from the scrubs will stick around in 'compactionstats'.",28/Sep/11 15:26;jbellis;+1 on Pavel's patch,28/Sep/11 15:52;xedin;Committed.,"28/Sep/11 16:16;hudson;Integrated in Cassandra-0.8 #347 (See [https://builds.apache.org/job/Cassandra-0.8/347/])
    Fix Scrub compaction finishing
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for (CASSANDRA-3255)

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1176926
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcDate.getString(ByteBuffer) appears to not be multithreaded safe,CASSANDRA-3822,12540678,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,01/Feb/12 05:38,12/Mar/19 14:21,13/Mar/19 22:26,01/Feb/12 23:05,,,,,,,0,,,,,"JdbcDate.getString(ByteBuffer) makes use of a static SimpleDateFormat

static final SimpleDateFormat FORMATTER = new SimpleDateFormat(DEFAULT_FORMAT);

SimpleDateFormat is not thread safe, as it uses a field from parent class DateFormat

    protected Calendar calendar;

to convert dates to calendars.

",,,,,,,,,,,,,,,,01/Feb/12 21:57;dbrosius@apache.org;use_thread_local_sdfs.diff;https://issues.apache.org/jira/secure/attachment/12512813/use_thread_local_sdfs.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-01 18:36:08.703,,,no_permission,,,,,,,,,,,,226073,,,Wed Feb 01 23:05:41 UTC 2012,,,,,,0|i0gomv:,95421,urandom,urandom,,,,,,,,,01/Feb/12 18:36;jbellis;I poked at this a bit and decided that replacing SDF w/ Calendar isn't quite as simple as a text substitution. :),01/Feb/12 21:57;dbrosius@apache.org;use thread local for holding SimpleDateFormat,01/Feb/12 23:05;urandom;committed w/ minor changes (style and convention); thanks Dave!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeysIndex is broken by CASSANDRA-1600,CASSANDRA-3766,12539280,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,philip.andronov,philip.andronov,philip.andronov,21/Jan/12 12:41,12/Mar/19 14:21,13/Mar/19 22:26,22/Jan/12 15:50,1.1.0,,,,,,0,,,,,"CASSANDRA-1600 introduces bug which cause KeySearch throws an Exception during search

KeySearcher:163
logger.debug(""Skipping {}"", baseCfs.getComparator().getString(firstColumn.name()));

Here firstColumn is a column from *index* row, so column name is a *key* in the baseCf. 

So the correct version would be:
logger.debug(""Skipping {}"", baseCfs.metadata.getKeyValidator().getString(firstColumn.name()));

Right now it is not possible to query KeysIndex. ",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-01-21 14:42:52.639,,,no_permission,,,,,,,,,,,,224782,,,Sun Jan 22 15:50:48 UTC 2012,,,,,,0|i0gnyv:,95313,jbellis,jbellis,,,,,,,,,"21/Jan/12 14:42;jbellis;committed, thanks!",22/Jan/12 15:50;philip.andronov;no problems :),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] fix bad comparison of column name against * or 1,CASSANDRA-3787,12539856,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,26/Jan/12 02:46,12/Mar/19 14:21,13/Mar/19 22:26,28/Jan/12 03:46,1.1.0,,,,,,0,cql,,,,"code does 

(!selectClause.get(0).equals(""*"") && !selectClause.get(0).equals(""1"")))

which is a ColumnDefinition against a string 

changed to

String columnName = selectClause.get(0).toString();
if (!columnName.equals(""*"") && !columnName.equals(""1""))",,,,,,,,,,,,,,,,26/Jan/12 02:47;dbrosius@apache.org;bad_column_compare.diff;https://issues.apache.org/jira/secure/attachment/12511936/bad_column_compare.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-26 07:16:24.761,,,no_permission,,,,,,,,,,,,225359,,,Thu Jan 26 13:13:58 UTC 2012,,,,,,0|i0go7z:,95354,jbellis,jbellis,,,,,,,,,"26/Jan/12 07:16;jbellis;This doesn't quite preserve the intent which is to throw on *either* more than 1 clause, or the count body mismatch.  Updated to this on commit:

{code}
.               if (selectClause.size() != 1)
                    throw new InvalidRequestException(""Only COUNT(*) and COUNT(1) operations are currently supported."");
                String columnName = selectClause.get(0).toString();
                if (!columnName.equals(""*"") && !columnName.equals(""1""))
                    throw new InvalidRequestException(""Only COUNT(*) and COUNT(1) operations are currently supported."");
{code}
","26/Jan/12 13:13;dbrosius@apache.org;dang it.. thanks, good catch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show Schema; inserts an extra comma in column_metadata,CASSANDRA-3714,12537832,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,j.casares,j.casares,09/Jan/12 23:43,12/Mar/19 14:21,13/Mar/19 22:26,10/Jan/12 17:27,1.0.7,,,,,,0,cli,,,,"create column family inode
  with column_type = 'Standard'
  and comparator = 'DynamicCompositeType(t=>org.apache.cassandra.db.marshal.TimeUUIDType,s=>org.apache.cassandra.db.marshal.UTF8Type,b=>org.apache.cassandra.db.marshal.BytesType)'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and row_cache_keys_to_save = 2147483647
  and keys_cached = 1000000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 60
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider'
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and comment = 'Stores file meta data'
  and column_metadata = [
    {column_name : 'b@70617468',
    validation_class : BytesType,
    index_name : 'path',
    index_type : 0,
},
    {column_name : 'b@73656e74696e656c',
    validation_class : BytesType,
    index_name : 'sentinel',
    index_type : 0,
},
    {column_name : 'b@706172656e745f70617468',
    validation_class : BytesType,
    index_name : 'parent_path',
    index_type : 0,
}];

That's that was outputted when I ran show schema. When I tried it on a new cluster, it failed since the commas after 'index_type: 0' were present.

Proposed fixes:
1. Allow trailing commas
2. Do not output trailing commas",,,,,,,,,,,,,,,,10/Jan/12 00:45;yukim;CASSANDRA-1.0-3714.txt;https://issues.apache.org/jira/secure/attachment/12509982/CASSANDRA-1.0-3714.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-10 00:45:47.679,,,no_permission,,,,,,,,,,,,223338,,,Tue Jan 10 17:27:59 UTC 2012,,,,,,0|i0gncf:,95212,xedin,xedin,,,,,,,,,10/Jan/12 00:44;j.casares;Deleting comment. Wrong window. :),"10/Jan/12 00:45;yukim;Confirmed bug only on cassandra-1.0 branch.
Patch attached to eliminate last comma from hash representation.","10/Jan/12 00:53;j.casares;Tested in 0.8.8 and saw this:

{noformat}
create column family inode
  with column_type = 'Standard'
  and comparator = 'DynamicCompositeType(t=>org.apache.cassandra.db.marshal.TimeUUIDType,b=>org.apache.cassandra.db.marshal.BytesType,s=>org.apache.cassandra.db.marshal.UTF8Type)'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and memtable_operations = 0.29062499999999997
  and memtable_throughput = 62
  and memtable_flush_after = 1440
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and keys_cached = 200000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider'
  and column_metadata = [
    {column_name : 'b@70617468',
    validation_class : BytesType,
    index_name : 'path',
    index_type : 0},
    {column_name : 'b@73656e74696e656c',
    validation_class : BytesType,
    index_name : 'sentinel',
    index_type : 0},
    {column_name : 'b@706172656e745f70617468',
    validation_class : BytesType,
    index_name : 'parent_path',
    index_type : 0}];
{noformat}

0.8.8 is not affected.

Edit: Checked 0.8.9 and the cassandra-0.8 branch as well and saw the same thing.",10/Jan/12 17:27;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RangeTest.java compilation error on Range.rangeSet in Eclipse (not Ant or IntelliJ),CASSANDRA-3784,12539806,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,kirktrue,kirktrue,kirktrue,25/Jan/12 18:50,12/Mar/19 14:21,13/Mar/19 22:26,25/Jan/12 21:27,1.1.0,,,Legacy/Testing,,,0,,,,,"When building from trunk:

{noformat}
The method rangeSet(Range<T>...) in the type Range is not applicable for the arguments (Range[])
RangeTest.java	/cassandra/test/unit/org/apache/cassandra/dht	line 184
{noformat}
",,,,,,,,,,,,,,,,25/Jan/12 19:26;kirktrue;CASSANDRA-3784.patch;https://issues.apache.org/jira/secure/attachment/12511870/CASSANDRA-3784.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-25 21:27:29.844,,,no_permission,,,,,,,,,,,,225309,,,Wed Jan 25 21:27:29 UTC 2012,,,,,,0|i0go6n:,95348,tjake,tjake,,,,,,,,,25/Jan/12 19:26;kirktrue;Patch that adds a type for T such that we can pass the compiler's generics checks.,25/Jan/12 21:27;tjake;committed thx!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect InetAddress equality test,CASSANDRA-3913,12542670,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,15/Feb/12 12:19,12/Mar/19 14:21,13/Mar/19 22:26,15/Feb/12 15:17,1.0.0,1.0.8,,,,,0,,,,,CASSANDRA-3485 introduced some InetAddress checks using == instead of .equals.,,,,,,,,,,,,,,,,15/Feb/12 12:20;brandon.williams;3913.txt;https://issues.apache.org/jira/secure/attachment/12514633/3913.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-15 13:47:50.188,,,no_permission,,,,,,,,,,,,227956,,,Wed Feb 15 15:17:43 UTC 2012,,,,,,0|i0gpq7:,95598,jbellis,jbellis,,,,,,,,,"15/Feb/12 13:47;jbellis;+1

I'd suggest committing to 0.8 as well just in case we do another release there",15/Feb/12 15:17;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra-cli returns 'command not found' instead of syntax error,CASSANDRA-3865,12541467,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,elubow,elubow,06/Feb/12 19:32,12/Mar/19 14:21,13/Mar/19 22:26,26/May/12 19:54,1.1.2,,,,,,0,cassandra-cli,,,,"When creating a column family from the output of 'show schema' with an index, there is a trailing comma after ""index_type: 0,""  The return from this is a 'command not found'  This is misleading because the command is found, there is just a syntax error.

'Command not found: `create column family $cfname ...`

",DSE 1.0.5,,,,,,,,,,,,,,,25/May/12 01:12;dbrosius@apache.org;3865_better_cli_ex_handling.txt;https://issues.apache.org/jira/secure/attachment/12529462/3865_better_cli_ex_handling.txt,24/May/12 02:09;dbrosius@apache.org;parse_doubles_better.txt;https://issues.apache.org/jira/secure/attachment/12528837/parse_doubles_better.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-07 14:33:20.617,,,no_permission,,,,,,,,,,,,226754,,,Sat May 26 22:37:50 UTC 2012,,,,,,0|i0gp67:,95508,xedin,xedin,,,,,,,,,07/Feb/12 14:33;yukim;This is fixed in 1.0.7 (CASSANDRA-3714).,"23/May/12 15:56;elubow;This doesn't appear to be fixed.

Welcome to Cassandra CLI version 1.0.8

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@linkcurrent] create column family social_poll_deltas
...	  with column_type = 'Standard'
...	  and comparator = 'CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.LongType)'
...	  and default_validation_class = 'UUIDType'
...	  and key_validation_class = 'UUIDType'
...	  and rows_cached = 0.0
...	  and row_cache_save_period = 0
...	  and row_cache_keys_to_save = 2147483647
...	  and keys_cached = 200000.0
...	  and key_cache_save_period = 14400
...	  and read_repair_chance = .25
...	  and gc_grace = 864000
...	  and min_compaction_threshold = 4
...	  and max_compaction_threshold = 32
...	  and replicate_on_write = true
...	  and row_cache_provider = 'SerializingCacheProvider'
...	  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
...	  and comment = 'Social poll totals and deltas ';
Command not found: `create column family social_poll_deltas with column_type = 'Standard' and comparator = 'CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.LongType)' and default_validation_class = 'UUIDType' and key_validation_class = 'UUIDType' and rows_cached = 0.0 and row_cache_save_period = 0 and row_cache_keys_to_save = 2147483647 and keys_cached = 200000.0 and key_cache_save_period = 14400 and read_repair_chance = .25 and gc_grace = 864000 and min_compaction_threshold = 4 and max_compaction_threshold = 32 and replicate_on_write = true and row_cache_provider = 'SerializingCacheProvider' and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy' and comment = 'Social poll totals and deltas ';`. Type 'help;' or '?' for help.","23/May/12 16:53;elubow;As an update, the problem with this (after MUCH trial and error), was that the read_repair_chance was .25 instead of 0.25.","24/May/12 02:09;dbrosius@apache.org;support decimal parsing as 0.5 and .5

against trunk","24/May/12 02:36;yukim;That't one way to fix parsing problem, but in my understanding, what Eric is saying is that it's hard to find the error from the message above.
If we can display more user friendly message like ""Parse error: read_repair_chance = .25"", instead of saying ""Command not found"", user can easily fix what's wrong.

I'm fine with current Double definition if we can fix the error message. What do you think, Pavel?","24/May/12 03:12;elubow;As Yuki said, I'd be more interested in the fix being a complaint of a syntax error (or parse error) when attempting to create a column family as opposed to ""command not found"" (which is clearly ambiguous).  This way I (as a user) know that the CLI is aware that I am attempting to create a column family and it's failing.  Now not only do I know that I am on the right track with the command I'm trying to execute, but then I know roughly where the problem is. Fixing the parse error is just a band-aid.","24/May/12 13:29;xedin;I think that Dave's patch is one part of it, another would be to change ""Command not found"" to ""Error in the command"" and add information from RecognitionException (which NoViableAltException extends) to were recognition error have actually happend.

Edit: Also, Dave, please don't forget that this is intended for inclusion into 1.0 so patches should be against cassandra-1.0 branch instead.","24/May/12 20:38;jbellis;My fault, I added 1.0 as a fix target after Dave's patch since I thought it was going to be a quick fix.  Let's target 1.1 instead.","25/May/12 01:12;dbrosius@apache.org;3865_better_cli_ex_handling.txt

has better decimal parsing as before.

Also has better error messaging when the exception is NoViableAltException.

For some reason the code special cased that exception and returned the non-useful ""Command not found"" message. I just removed the special casing and now the message is useful.

If there is some case that someone knows about that the message ""Command not found"" is useful, i can put it back in for that specific case, but i couldn't see it.

against cassandra-1.1",26/May/12 12:53;xedin;+1,26/May/12 15:59;dbrosius@apache.org;committed to branch cassandra-1.1 as commit 2d72056c14f9b97e67dd94e48691f3ec1a88d9d6,26/May/12 16:08;xedin;You also need to add line about that into CHANGES.txt and up-merge it into trunk.,"26/May/12 19:54;xedin;ok, I did CHANGES.txt update and merge for you this time :)","26/May/12 20:37;hudson;Integrated in Cassandra #1453 (See [https://builds.apache.org/job/Cassandra/1453/])
    add missing CHANGES.txt entry for CASSANDRA-3865 (Revision d16c4468e86fbdb82ed981ef6c6ced6f275e4f21)

     Result = FAILURE
xedin : 
Files : 
* CHANGES.txt
",26/May/12 20:59;xedin;I have fixed small bug and committed to 1.1 and up-merged into trunk.,"26/May/12 22:37;hudson;Integrated in Cassandra #1455 (See [https://builds.apache.org/job/Cassandra/1455/])
    fixes small CLI bug introduced by CASSANDRA-3865 (Revision 46722cc69e47e9b4bdcc5c28f426cf7f0a6a3a7d)

     Result = FAILURE
xedin : 
Files : 
* src/java/org/apache/cassandra/cli/Cli.g
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't include original exception class name in CQL message,CASSANDRA-3981,12544643,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,29/Feb/12 16:33,12/Mar/19 14:21,13/Mar/19 22:26,01/Mar/12 08:16,1.1.0,,,,,,0,cql,,,,"In CreateColumnFamilyStatement, we do
{noformat}
catch (ConfigurationException e)
{
    throw new InvalidRequestException(e.toString());
}
{noformat}

This result in having the exception message looking like:
{noformat}
java.sql.SQLSyntaxErrorException: org.apache.cassandra.config.ConfigurationException: Cf1 already exists in keyspace Keyspace1
{noformat}",,,,,,,,,,,,,,,,29/Feb/12 16:35;slebresne;3981.patch;https://issues.apache.org/jira/secure/attachment/12516579/3981.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-29 22:59:51.345,,,no_permission,,,,,,,,,,,,229829,,,Thu Mar 01 08:17:14 UTC 2012,,,,,,0|i0gqkv:,95736,jbellis,jbellis,,,,,,,,,"29/Feb/12 16:35;slebresne;Attaching trivial but actually only against CQL3. It does affect CQL2 too, but in a way, changing the error message could break clients code (if they weren't very careful) so I figured, why not leaving it the way it is for CQL2.",29/Feb/12 22:59;jbellis;+1,"01/Mar/12 08:17;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: double wide unicode chars cause incorrect padding in select output,CASSANDRA-4033,12545883,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,tpatterson,tpatterson,09/Mar/12 17:59,12/Mar/19 14:21,13/Mar/19 22:26,27/Mar/12 18:53,1.0.9,1.1.0,,Legacy/Tools,,,0,cql,cqlsh,,,"{code}
CREATE COLUMNFAMILY cf3 (KEY text primary key);
INSERT INTO cf3 (KEY, col1, col2) VALUES ('a', '1234 1234 1234 1234', 'abcd');
INSERT INTO cf3 (KEY, col1, col2) VALUES ('b', '愛愛愛愛 愛愛愛愛 愛愛愛愛 愛愛愛愛', 'abcd');
SELECT * FROM cf3 WHERE key in ('a', 'b');
{code}
produces this output:
{code}
 KEY | col1                                                | col2
-----+-----------------------------------------------------+------
   a |                                 1234 1234 1234 1234 | abcd
   b |                       愛愛愛愛 愛愛愛愛 愛愛愛愛 愛愛愛愛 | abcd
{code}
note the extra spaces before the ""love"" glyphs.",Using the patch pending/4003,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-03-10 01:23:01.143,,,no_permission,,,,,,,,,,,,231066,,,Tue Mar 27 18:53:14 UTC 2012,,,,,,0|i0gr6n:,95834,brandon.williams,brandon.williams,,,,,,,,,10/Mar/12 01:23;thepaul;good call. fix pushed to https://github.com/thepaul/cassandra/tree/pending/4033 (4033 branch and pending/4033 tag in my github repo),27/Mar/12 18:53;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL documentation missing INSERT syntax,CASSANDRA-2533,12504923,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,21/Apr/11 21:44,12/Mar/19 14:21,13/Mar/19 22:26,27/Apr/11 13:24,0.8.0 beta 2,,,Legacy/Documentation and Website,,,0,cql,,,,"Both documents are missing syntax for the INSERT statement:

https://github.com/apache/cassandra/blob/trunk/doc/cql/CQL.textile
https://github.com/apache/cassandra/raw/trunk/doc/cql/CQL.html",,,,,,,,,,,,,,,,26/Apr/11 17:30;xedin;CASSANDRA-2533.patch;https://issues.apache.org/jira/secure/attachment/12477427/CASSANDRA-2533.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-26 17:18:28.437,,,no_permission,,,,,,,,,,,,20679,,,Wed Apr 27 17:18:12 UTC 2011,,,,,,0|i0gbxb:,93362,jbellis,jbellis,,,,,,,,,"21/Apr/11 22:26;cdaw;The more I look at the code, docs and existing tests, it looks like we use UPDATE since it behaves like an upsert.  If that is the case, just let me know.","26/Apr/11 17:18;xedin;This is correct, Insert statement even generates UpdateStatement Insert is an alias for update.",26/Apr/11 17:30;xedin;I only update textile as html version going to be auto-generated from it on each Cassandra build.,27/Apr/11 13:24;jbellis;committed,"27/Apr/11 17:18;hudson;Integrated in Cassandra-0.8 #46 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/46/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make compaction type an enum,CASSANDRA-2482,12504311,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,14/Apr/11 19:33,12/Mar/19 14:21,13/Mar/19 22:26,14/Apr/11 21:37,0.8 beta 1,,,,,,0,,,,,"Compaction type should be an enum, half of the places we set it we use it as a message and include the keyspace/cf although that is already included in the compaction info object.

I realize this is minor and a pedantic but from the standpoint of someone writing a monitoring application its kind of annoying.",,,,,,,,,,,,,,,,14/Apr/11 21:11;nickmbailey;0001-Make-compactiontype-an-enum.patch;https://issues.apache.org/jira/secure/attachment/12476372/0001-Make-compactiontype-an-enum.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-14 21:33:37.059,,,no_permission,,,,,,,,,,,,20645,,,Thu Apr 14 22:19:45 UTC 2011,,,,,,0|i0gbmn:,93314,jbellis,jbellis,,,,,,,,,"14/Apr/11 21:33;jbellis;committed with a couple fixes:

 - row cache save uses correct enum
 - CompactionInfo.getTaskType returns enum instead of string so callers don't have to do string inspection anymore","14/Apr/11 22:19;hudson;Integrated in Cassandra-0.8 #7 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/7/])
    madeCompactionInfo.getTaskType return an enum
patch by nickmbailey; reviewed by jbellis for CASSANDRA-2482
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI doesn't handle inserting negative integers,CASSANDRA-2358,12501895,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,thobbs,thobbs,20/Mar/11 06:36,12/Mar/19 14:21,13/Mar/19 22:26,31/Mar/11 18:41,0.7.5,,,Legacy/Tools,,,0,,,,,"The CLI raises a syntax error when trying to insert negative integers:

{noformat}
[default@Keyspace1] set StandardInteger['key'][-12] = 'val';
Syntax error at position 28: mismatched character '1' expecting '-'
{noformat}",,1800,1800,,0%,1800,1800,,,,,,,,,29/Mar/11 10:51;xedin;CASSANDRA-2358-trunk.patch;https://issues.apache.org/jira/secure/attachment/12474866/CASSANDRA-2358-trunk.patch,20/Mar/11 15:12;xedin;CASSANDRA-2358.patch;https://issues.apache.org/jira/secure/attachment/12474116/CASSANDRA-2358.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-20 14:37:26.947,,,no_permission,,,,,,,,,,,,20578,,,Thu Mar 31 19:56:40 UTC 2011,,,,,,0|i0gawv:,93198,jbellis,jbellis,,,,,,,,,"20/Mar/11 14:37;xedin;I will wait until CASSANDRA-2341 is committed, it brings notation of the positive/negative integers into CLI grammar.",20/Mar/11 14:42;jbellis;We should fix negative ints in 0.7 unless it's a huge pain.  (Counters will stay 0.8 only.),"20/Mar/11 14:43;xedin;Gotcha! No, this won't be a pain at all.",23/Mar/11 18:56;jbellis;committed,"23/Mar/11 19:32;hudson;Integrated in Cassandra-0.7 #402 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/402/])
    allow negative numbers in the cli
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2358
",28/Mar/11 21:21;jbellis;merged to trunk but CliTest fails.  Can you fix?,28/Mar/11 21:29;xedin;Can you remove it from trunk for now? I will create a separate version of this patch for the trunk.,28/Mar/11 21:58;jbellis;removed,28/Mar/11 22:01;xedin;Thanks! ,29/Mar/11 10:51;xedin;branch: trunk (latest commit e6c5a28da940a086d0e786f1ad0288c0b0efa27d) ,31/Mar/11 18:41;jbellis;committed trunk patch,"31/Mar/11 19:56;hudson;Integrated in Cassandra #822 (See [https://hudson.apache.org/hudson/job/Cassandra/822/])
    add negative number support to cli, trunk version
patch by Pavel Yaskevich for CASSANDRA-2358
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes get ignored by dynamic snitch when read repair chance is zero,CASSANDRA-2662,12507590,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,doubleday,doubleday,18/May/11 10:55,12/Mar/19 14:21,13/Mar/19 22:26,28/Jul/11 23:43,0.7.9,0.8.5,,,,,0,,,,,"DynamicEndpointSnitch falls back to subsnitch when one of the scores of the endpoints being compared is missing.

This leads to a stable order of hosts until reads will lead to recorded scores. 
If setting read repair chance to 0 and reads are performed with quorum then (rf - # quorum nodes) will never get reads.",,,,,,,,,,,,,,,,31/May/11 18:05;brandon.williams;2662.txt;https://issues.apache.org/jira/secure/attachment/12480974/2662.txt,18/May/11 10:58;doubleday;dynsnitch.patch;https://issues.apache.org/jira/secure/attachment/12479566/dynsnitch.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-18 15:20:13.892,,,no_permission,,,,,,,,,,,,20759,,,Fri Aug 19 22:50:14 UTC 2011,,,,,,0|i0gcpj:,93489,thepaul,thepaul,,,,,,,,,"18/May/11 10:58;doubleday;One very simple fix is to initialize scores with 0 which forces at least one read. 

Dunno if thats a good idea when using multi dc snitches ...","18/May/11 15:20;brandon.williams;To clarify what this patch does: on the first round, behavior is unchanged, it will fall back to the subsnitch as usual.  On the second round however, the nodes will have scores so the dynamic snitch will determine the best host.  This seems like a good idea, but I think we should proceed cautiously with regard to CASSANDRA-1314.  This is certainly going to blow the cache on the second round, since the unpinned hosts at zero are guaranteed to be ranked higher than the pinned one.  However, in the current implementation, the badness threshold in CASSANDRA-1519 can never take effect at RR = 0 since the scores of the other replicas will always be null, and thus diversion to the subsnitch will always occur, returning the pinned replica.  With this patch, the second replica is likely to end up being the pinned one, which I think will be ok, as long as all coordinators agree, though this might not always be the case.","18/May/11 19:18;brandon.williams;Thinking this through a bit more:

Given coordinator A, and replicas X, Y, and Z (in subsnitch order), on the first round X will be chosen, and let's say it receives a score of 1.  With the patch, at this point Y and Z will be initialized with zero.  On the next round, Y will be chosen, and let's say it receives a score of or near 1, depending on network latency.  On the third round, Z will be chosen, and let's say it also receives a score similar to Y.  Now the cache is hot on all nodes, and subsequent reads have the possibility to oscillate between all three based on network latency variance.  This can be mitigated though with the badness threshold.  With the badness threshold on, the first round will occur as before, but subsequent rounds will continue to use X until it degrades past the threshold, at which point they will use Y, until the dynamic snitch reset()s, at which point everything will repeat.  I don't think this is harmful to CASSANDRA-1314 after all.

","18/May/11 19:47;brandon.williams;There is one problem with this patch, rather that initializing the score to zero, it needs to create an empty AdaptiveLatencyTracker for them before checking the scores, otherwise DES.reset() will never have an effect on those hosts.",31/May/11 18:05;brandon.williams;Updated patch ensures ALTs are created for the hosts by using receiveTiming,28/Jul/11 22:56;thepaul;+1,28/Jul/11 23:43;brandon.williams;Committed.,"29/Jul/11 00:24;hudson;Integrated in Cassandra-0.7 #537 (See [https://builds.apache.org/job/Cassandra-0.7/537/])
    DES shouldn't ignore nodes when the read repair chance is zero.
Patch by brandonwilliams reviewed by Paul Cannon for CASSANDRA-2662

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152038
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java
",19/Aug/11 22:50;jbellis;(This didn't get merged to 0.8 until just now.),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The class o.a.c.cql.jdbc.TypedColumn needs to be declared public,CASSANDRA-2672,12507821,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ardot,ardot,ardot,19/May/11 22:14,12/Mar/19 14:21,13/Mar/19 22:26,19/May/11 22:30,0.8.0,,,Legacy/CQL,,,0,,,,,"The implementation of {{ResultSet}} in the JDBC package provides a method: {{unwrap( Class<T> interfaceName)}} in order to allow some of the methods in the {{ResultSet}} implementation {{Class}} to be exposed.

The implementation currently restricts the access to only one acceptable interface: {{CassandraResultSet}}.

Two of the getters in that interface return {{TypedColumn}} which cleverly contains the ""Cassandra"" details of the desired column such as raw column details, and the {{AbstractType}} of the validator and the comparator among others. (Nice!) Unfortunately the {{TypedColumn}} class is not public so is it is not accessible to the callers code.


",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-05-19 22:30:55.799,,,no_permission,,,,,,,,,,,,20767,,,Thu May 19 22:30:55 UTC 2011,,,,,,0|i0gcrj:,93498,jbellis,jbellis,,,,,,,,,"19/May/11 22:30;jbellis;done in r1125144.

Note that CassandraResultSet in general and TypedColumn in particular expose implementation details that may change in the future; use with appropriate caution. :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra_cli: CREATE CF HELP should list option as key_cache_save_period instead of keys_cached_save_period,CASSANDRA-2572,12505258,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,27/Apr/11 00:36,12/Mar/19 14:21,13/Mar/19 22:26,27/Apr/11 15:33,0.8.0 beta 2,,,Legacy/Tools,,,0,,,,,"*cassandra-cli: output from create cf command*
{noformat}
[default@cqldb]  create column family supa_dupa2 with keys_cached_save_period = 124000;

No enum const class org.apache.cassandra.cli.CliClient$ColumnFamilyArgument.KEYS_CACHED_SAVE_PERIOD
{noformat}

*cassandra-cli: help create column family*
{noformat}
- keys_cached_save_period: Duration in seconds after which Cassandra should
  safe the keys cache. Caches are saved to saved_caches_directory as
  specified in conf/Cassandra.yaml. Default is 14400 or 4 hours.

  Saved caches greatly improve cold-start speeds, and is relatively cheap in
  terms of I/O for the key cache. Row cache saving is much more expensive and
  has limited use.
{noformat}

*cqlsh: documentation for create column family options*
{noformat}
key_cache_save_period_in_seconds	14400	Number of seconds between saving key caches.
{noformat}

*cqlsh: this actually works*
{noformat}
cqlsh>  CREATE COLUMNFAMILY cf1 (KEY varchar PRIMARY KEY) WITH key_cache_save_period_in_seconds=10000;
{noformat}

*cassandra-cli: CF definition via show keyspace*
{noformat}
    ColumnFamily: cf1
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/10000
      Memtable thresholds: 0.140625/30/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
{noformat}
",,,,,,,,,,,,,,,,27/Apr/11 02:31;dw;CASSANDRA-0.7-2572.patch;https://issues.apache.org/jira/secure/attachment/12477471/CASSANDRA-0.7-2572.patch,27/Apr/11 10:17;xedin;CASSANDRA-2572.patch;https://issues.apache.org/jira/secure/attachment/12477492/CASSANDRA-2572.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-27 01:34:43.255,,,no_permission,,,,,,,,,,,,20705,,,Wed Apr 27 17:18:12 UTC 2011,,,,,,0|i0gc5z:,93401,jbellis,jbellis,,,,,,,,,27/Apr/11 01:34;jbellis;may also be a bug in 0.7,"27/Apr/11 01:53;dw;I'm seeing the following in org.apache.cassandra.cli.CliClient:

 protected enum ColumnFamilyArgument
    {
    ....
    KEY_CACHE_SAVE_PERIOD,
    ....
    }

Instead of executing:

create column family supa_dupa2 with keys_cached_save_period = 124000;

one executes:

create column family supa_dupa2 with key_cache_save_period = 124000;

The second statement successfully executes.",27/Apr/11 03:33;cdaw;Thanks David.  Will make this a bug about help.,"27/Apr/11 10:17;xedin;changed doc for 0.8 (can be applied on both cassandra-0.8 and trunk without any problems), 0.7 is not affected by this bug.",27/Apr/11 15:33;jbellis;committed,"27/Apr/11 17:18;hudson;Integrated in Cassandra-0.8 #46 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/46/])
    fix cli help typo
patch by Pavel Yaskevich for CASSANDRA-2572
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: incorrect error message running truncate on CF that does not exist,CASSANDRA-2570,12505248,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,26/Apr/11 21:23,12/Mar/19 14:21,13/Mar/19 22:26,27/Apr/11 15:18,0.8.0 beta 2,,,Legacy/CQL,,,0,cql,,,,"Run truncate on a CF that does not exist. The error message is misleading.

*CQLSH*
{code}
cqlsh> truncate aaaa;
Unable to complete request: one or more nodes were unavailable.
{code}

*cassandra-cli*
{code}
[default@cqldb] truncate aaaaaaaaa;
aaaaaaaaa not found in current keyspace.
{code}",,,,,,,,,,,,,,,,27/Apr/11 14:46;xedin;CASSANDRA-2570-v2-0.8.patch;https://issues.apache.org/jira/secure/attachment/12477548/CASSANDRA-2570-v2-0.8.patch,27/Apr/11 15:03;xedin;CASSANDRA-2570-v2-trunk.patch;https://issues.apache.org/jira/secure/attachment/12477551/CASSANDRA-2570-v2-trunk.patch,27/Apr/11 10:47;xedin;CASSANDRA-2570.patch;https://issues.apache.org/jira/secure/attachment/12477494/CASSANDRA-2570.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-04-27 10:47:14.812,,,no_permission,,,,,,,,,,,,20703,,,Wed Apr 27 17:18:12 UTC 2011,,,,,,0|i0gc5j:,93399,jbellis,jbellis,,,,,,,,,"27/Apr/11 10:47;xedin;Working branch: trunk. QP now checks if the given CF exists in the current KS before executing ""truncate"" operation.",27/Apr/11 14:46;xedin;patch for version 0.8 instead of trunk + test for truncate validation.,"27/Apr/11 15:18;jbellis;committed to 0.8 and trunk w/ some changes:

 - test uses assert_raises, combined w/ existing test_truncate method
 - server uses validateColumnFamily to test existance","27/Apr/11 17:18;hudson;Integrated in Cassandra-0.8 #46 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/46/])
    validate cql TRUNCATE columnfamily before truncating
patch by Pavel Yaskevich and jbellis for CASSANDRA-2570
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in SSTableWriter when no ReplayPosition availible,CASSANDRA-2718,12508550,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,27/May/11 13:40,12/Mar/19 14:21,13/Mar/19 22:26,27/May/11 14:01,0.8.1,,,,,,0,,,,,"The following NPE occurs when durable_writes is set to false

{noformat}
ERROR 09:20:30,378 Fatal exception in thread Thread[FlushWriter:11,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.commitlog.ReplayPosition$ReplayPositionSerializer.serialize(ReplayPosition.java:127)
	at org.apache.cassandra.io.sstable.SSTableWriter.writeMetadata(SSTableWriter.java:209)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:187)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:173)
	at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:253)
	at org.apache.cassandra.db.Memtable.access$400(Memtable.java:49)
	at org.apache.cassandra.db.Memtable$3.runMayThrow(Memtable.java:270)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}",,,,,,,,,,,,,,,,27/May/11 13:42;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2718-avoide-NPE-when-bypassing-commitlog.txt;https://issues.apache.org/jira/secure/attachment/12480654/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2718-avoide-NPE-when-bypassing-commitlog.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-27 13:49:46.607,,,no_permission,,,,,,,,,,,,20787,,,Fri May 27 14:01:26 UTC 2011,,,,,,0|i0gd1b:,93542,slebresne,slebresne,,,,,,,,,27/May/11 13:49;slebresne;+1,27/May/11 14:01;tjake;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool statusthrift exception while node starts up,CASSANDRA-2721,12508683,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,30/May/11 07:16,12/Mar/19 14:21,13/Mar/19 22:26,30/May/11 08:37,0.8.1,,,,,,0,,,,,"We noticed when calling nodetool statusthrift, while a node is starting up, it throws an exception. I think the proper behavior should be just return false, instead of throwing an exception if RPC server hasn't started yet. That way this stack trace won't have to be thrown in nodetool:

Exception in thread ""main"" 

java.lang.IllegalStateException: No configured RPC daemon
",,,,,,,,,,,,,,,,30/May/11 07:19;lenn0x;0001-If-RPCServer-isn-t-started-just-return-false-instead.patch;https://issues.apache.org/jira/secure/attachment/12480824/0001-If-RPCServer-isn-t-started-just-return-false-instead.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-30 07:54:17.364,,,no_permission,,,,,,,,,,,,20789,,,Mon May 30 14:55:47 UTC 2011,,,,,,0|i0gd1z:,93545,slebresne,slebresne,,,,,,,,,"30/May/11 07:54;slebresne;I don't think nodetool statusthrift exists, but yeah, makes sense, +1. BUT, let's just put it in 0.8.1 however, for the sake of making Eric job's easier when he re-roll 0.8.0 (in the meantime, any hypothetical implementation of a nodetool statusthrift could just catch the IllegalStateException).",30/May/11 08:09;lenn0x;Ah! I forgot to commit that patch. It was put in our build. CASSANDRA-2722,"30/May/11 14:44;hudson;Integrated in Cassandra #912 (See [https://builds.apache.org/hudson/job/Cassandra/912/])
    ","30/May/11 14:55;hudson;Integrated in Cassandra-0.8 #146 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/146/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't include tmp files as sstable when create column families,CASSANDRA-2929,12514884,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,21/Jul/11 09:51,12/Mar/19 14:21,13/Mar/19 22:26,22/Jul/11 20:21,0.7.9,0.8.3,,,,,0,,,,,"When we open a column family and populate the SSTableReader, we happen to include -tmp files. This has no change to actually happen in a real life situation, but that is what was triggering a race in the unit tests triggering spurious assertion failure in estimateRowsFromIndex.",,,,,,,,,,,,,,,,21/Jul/11 09:52;slebresne;0001-Don-t-include-tmp-files-as-sstables-when-creating-CF.patch;https://issues.apache.org/jira/secure/attachment/12487274/0001-Don-t-include-tmp-files-as-sstables-when-creating-CF.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-21 14:24:37.106,,,no_permission,,,,,,,,,,,,20895,,,Fri Jul 22 21:36:18 UTC 2011,,,,,,0|i0gebj:,93750,jbellis,jbellis,,,,,,,,,21/Jul/11 09:52;slebresne;Patch is against 0.7,"21/Jul/11 14:24;jbellis;+1, although the flag proliferation on files() is starting to concern me","22/Jul/11 21:36;hudson;Integrated in Cassandra-0.7 #535 (See [https://builds.apache.org/job/Cassandra-0.7/535/])
    Don't include tmp file as sstable when creating cfs
patch by slebresne; reviewed by jbellis for CASSANDRA-2929

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1149716
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/io/sstable/SSTable.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException after upgrade to 0.8.0,CASSANDRA-2822,12511479,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,vilda,vilda,24/Jun/11 14:46,12/Mar/19 14:21,13/Mar/19 22:26,24/Jun/11 16:48,0.8.2,,,,,,0,nullpointerexception,,,,"I'm getting NullPointerException on a node upgraded from 0.7 to 0.8.0 (Debian package). The exception is thrown quickly several times after start. Then the Cassandra node is unresponsive. The Stack trace is:

ERROR 14:36:49,712 Fatal exception in thread Thread[WRITE-/10.228.243.191,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)

ERROR 14:36:49,758 Fatal exception in thread Thread[WRITE-/10.227.101.171,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)

ERROR 14:36:49,797 Fatal exception in thread Thread[WRITE-/10.228.243.191,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)

ERROR 14:36:50,756 Fatal exception in thread Thread[WRITE-/10.226.194.239,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)",Debian amd64,,,,,,,,,,,,,,,24/Jun/11 15:18;jbellis;2822.txt;https://issues.apache.org/jira/secure/attachment/12483719/2822.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-24 14:54:36.334,,,no_permission,,,,,,,,,,,,20849,,,Fri Jun 24 17:19:13 UTC 2011,,,,,,0|i0gdo7:,93645,slebresne,slebresne,,,,,,,,,24/Jun/11 14:54;jbellis;upgrade to the 0.8 cassandra.yaml,24/Jun/11 14:59;slebresne;I suppose we should add a nice error message though instead of NPE in outboundTcpConnection,24/Jun/11 15:02;jbellis;true,24/Jun/11 16:08;slebresne;+1,24/Jun/11 16:48;jbellis;committed,"24/Jun/11 17:19;hudson;Integrated in Cassandra-0.8 #192 (See [https://builds.apache.org/job/Cassandra-0.8/192/])
    tolerate missing encryption options
patch by jbellis; reviewed by slebresne for CASSANDRA-2822

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1139383
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra CLI - Import Keyspace Definitions from File - Comments do partitially interpret characters/commands,CASSANDRA-2852,12512674,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,jensmueller,jensmueller,03/Jul/11 18:23,12/Mar/19 14:21,13/Mar/19 22:26,04/Jul/11 16:22,0.7.7,0.8.2,,Legacy/Tools,,,0,,,,,"Hello, 

using: bin/cassandra-cli -host localhost --file conf/schema-sample.txt

with schema-sample.txt having contents like this:

/* here are a lot of comments,
like this sample create keyspace;
and so on
*/

Will result in an error: 
Line 1 => Syntax Error at Position 323: mismatched charackter '<EOF>' expecting '*'

The Cause is the keyspace; statement => the semicolon "";"" causes the error.

However:

Writing the word ""keyspace;"" with quotes, does NOT lead to the error.
so this works: 
/* here are a lot of comments,
like this sample create ""keyspace;""
and so on
*/

From my point of view this is an error. Everyting between the ""Start Comment"" => /* and ""End Comment"" => */ Should be treated as a comment and not be interpreted in any way. Thats the definition of a comment, to be not interpreted at all. 

Or this must be documented somewhere very prominently, otherwise this will lead to unnecessary wasting of time searching for this odd behavoiur. And it makes ""commenting out"" statements much more cumbersome.

Plattform: Windows Vista

thanks
",Win Vista ,,,,,,,,,,,,,,,04/Jul/11 10:39;xedin;CASSANDRA-2852.patch;https://issues.apache.org/jira/secure/attachment/12485130/CASSANDRA-2852.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-04 10:39:05.312,,,no_permission,,,,,,,,,,,,20866,,,Mon Jul 04 16:45:05 UTC 2011,,,,,,0|i0gdun:,93674,jbellis,jbellis,,,,,,,,,04/Jul/11 10:39;xedin;can be applied on both 0.7 and 0.8 branches.,04/Jul/11 16:22;jbellis;committed,"04/Jul/11 16:45;hudson;Integrated in Cassandra-0.7 #520 (See [https://builds.apache.org/job/Cassandra-0.7/520/])
    improve cli treatment of multiline comments
patch by pyaskevich; reviewed by jbellis for CASSANDRA-2852

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1142727
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/cli/CliMain.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
initialize log4j correctly in EmbeddedCassandraService,CASSANDRA-2857,12512883,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,05/Jul/11 19:51,12/Mar/19 14:21,13/Mar/19 22:26,29/Aug/11 15:24,0.8.5,,,,,,0,,,,,"Currently, ECS.cleanUpOldStuff calls CleanupHelper.cleanupAndLeaveDirs(), which initialized DatabaseDescriptor which does some logging.  When we go to initialize log4j later in AbstractCassandraService, it's too late.",,,,,,,,,,,,,,,,05/Jul/11 19:55;jbellis;2857-drivers.txt;https://issues.apache.org/jira/secure/attachment/12485319/2857-drivers.txt,05/Jul/11 19:57;jbellis;2857.txt;https://issues.apache.org/jira/secure/attachment/12485320/2857.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-29 14:59:18.375,,,no_permission,,,,,,,,,,,,19340,,,Mon Aug 29 16:23:13 UTC 2011,,,,,,0|i0gdvr:,93679,tjake,tjake,,,,,,,,,"05/Jul/11 19:52;jbellis;patch to move the log4j initialization into the ""real"" entry point classes",29/Aug/11 14:59;tjake;needs rebase but +1,29/Aug/11 15:24;jbellis;committed,"29/Aug/11 16:23;hudson;Integrated in Cassandra-0.8 #298 (See [https://builds.apache.org/job/Cassandra-0.8/298/])
    fix log4j initialization in EmbeddedCassandraService
patch by jbellis; reviewed by tjake for CASSANDRA-2857

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1162851
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AbstractCassandraDaemon.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraDaemon.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bulkload JMX call,CASSANDRA-2908,12514404,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,18/Jul/11 07:23,12/Mar/19 14:21,13/Mar/19 22:26,18/Jul/11 13:15,0.8.2,,,,,,0,bulkloader,,,,"The bulkload JMX call is supposed to simplify bulkloading when done from a Cassandra node (so you don't have to configure the bulkloading client to not conflict with the node itself), but that call doesn't work (it forgets to add the ranges to stream).",,,,,,,,,,,,,,,,18/Jul/11 07:23;slebresne;0001-Fix-JMX-call-bulkLoad.patch;https://issues.apache.org/jira/secure/attachment/12486799/0001-Fix-JMX-call-bulkLoad.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-18 11:49:31.416,,,no_permission,,,,,,,,,,,,20888,,,Mon Jul 18 13:15:11 UTC 2011,,,,,,0|i0ge6v:,93729,jbellis,jbellis,,,,,,,,,18/Jul/11 11:49;jbellis;+1,"18/Jul/11 13:09;hudson;Integrated in Cassandra-0.8 #220 (See [https://builds.apache.org/job/Cassandra-0.8/220/])
    Fix JMX bulkload call
patch by slebresne; reviewed by jbellis for CASSANDRA-2908

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147838
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/BulkLoader.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
","18/Jul/11 13:15;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliClient does not log root cause exception when catch it from executeCLIStatement,CASSANDRA-2746,12509492,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cywjackson,cywjackson,cywjackson,07/Jun/11 18:01,12/Mar/19 14:21,13/Mar/19 22:26,06/Jul/11 13:17,0.8.2,,,Legacy/Tools,,,0,,,,,"When executing a statement from the cassandra-cli (with --debug) , if an exception is thrown from one of the cases in side the executeCLIStatement method, the root cause is swallowed. For specific case such as the InvalidRequestException or the SchemaDisagreementException, just the message itself maybe enough, but for the general Exception case, without the root cause, it could be difficult to debug the issue. 

For example, we have seen exception like:
{noformat}
null
java.lang.RuntimeException
at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:223)
at org.apache.cassandra.cli.CliMain.main(CliMain.java:351)
{noformat}

the null there would most likely indicate this is a NPE (though it could still be any Exception with null message). By adding a initCause to the caught exception, we could see the root cause, eg:

{noformat}
null
java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:212)
        at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:223)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:351)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.cli.CliClient.describeKeySpace(CliClient.java:1336)
        at org.apache.cassandra.cli.CliClient.executeShowKeySpaces(CliClient.java:1166)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:170)
        ... 2 more
{noformat}

submitting a patch here that would add the initCause to all caught exceptions here. But the most important one is the general Exception case.",,,,,,,,,,,,,,,,07/Jun/11 18:02;cywjackson;patch2746.txt;https://issues.apache.org/jira/secure/attachment/12481727/patch2746.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-06 13:17:01.598,,,no_permission,,,,,,,,,,,,20803,,,Wed Jul 06 14:22:08 UTC 2011,,,,,,0|i0gd7j:,93570,jbellis,jbellis,,,,,,,,,07/Jun/11 18:02;cywjackson;add initCause to exceptions caught from CliClient.executeCLIStatement,"06/Jul/11 13:17;jbellis;committed, thanks!","06/Jul/11 14:22;hudson;Integrated in Cassandra-0.8 #206 (See [https://builds.apache.org/job/Cassandra-0.8/206/])
    add initCause to cliclient exceptions
patch by Jackson Chung; reviewed by jbellis for CASSANDRA-2746

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1143397
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool decommission should throw an error when there are extra params,CASSANDRA-2740,12509253,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,brandon.williams,brandon.williams,04/Jun/11 03:24,12/Mar/19 14:21,13/Mar/19 22:26,14/Jul/11 23:42,0.7.8,,,,,,0,,,,,"removetoken takes a token parameter, but decommission works against the node where the call is issued.  This allows confusion such as 'nodetool -h localhost decommission <ip or token>' actually decommissioning the local node, instead of whatever was passed to it.",,,,,,,,,,,,,,,,09/Jun/11 22:51;jhermes;2740.txt;https://issues.apache.org/jira/secure/attachment/12481992/2740.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-09 22:41:22.747,,,no_permission,,,,,,,,,,,,20799,,,Fri Jul 15 00:06:42 UTC 2011,,,,,,0|i0gd67:,93564,brandon.williams,brandon.williams,,,,,,,,,"09/Jun/11 22:38;brandon.williams;For 1.0 I'd like to see decom work like rt, and take a token parameter, but I don't want to break existing jmx tools for 0.8, so let's just error on extra params there.",09/Jun/11 22:41;jbellis;why add token param?,09/Jun/11 22:51;jhermes;All zero-arg commands now complain when there's extra args.,14/Jul/11 23:42;brandon.williams;Committed.,"15/Jul/11 00:06;hudson;Integrated in Cassandra-0.7 #528 (See [https://builds.apache.org/job/Cassandra-0.7/528/])
    Do not allow extra params to nodetool commands to prevent confusion.
Patch by Jon Hermes, reviewed by brandonwilliams for CASSANDRA-2740

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1146923
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/tools/NodeCmd.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
certain generic type causes compile error in eclipse,CASSANDRA-2937,12514978,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,yangyangyyy,yangyangyyy,21/Jul/11 23:22,12/Mar/19 14:21,13/Mar/19 22:26,22/Jul/11 01:50,,,,,,,0,,,,,"the code ColumnFamily and AbstractColumnContainer uses code similar to the following (substitute Blah with AbstractColumnContainer.DeletionInfo):



import java.util.concurrent.atomic.AtomicReference;
public class TestPrivateAtomicRef {
    protected final AtomicReference<Blah> b = new AtomicReference<Blah>(new Blah());
    // the following form would work for eclipse
//    protected final AtomicReference b = new AtomicReference(new Blah());

    private static class Blah {
    }
}


class Child extends TestPrivateAtomicRef {    
    public void aaa() {
        Child c = new Child();
        c.b.set(
        b.get()  //<==== eclipse shows error here
        );
    }
}


in eclipse, the above code generates compile error, but works fine under java command line. since many people use eclipse, it's better to 
make a temporary compromise and make DeletionInfo protected",,,,,,,,,,,,,,,,21/Jul/11 23:27;yangyangyyy;0002-avoid-eclipse-compile-error-for-generic-type-on-Atom.patch;https://issues.apache.org/jira/secure/attachment/12487373/0002-avoid-eclipse-compile-error-for-generic-type-on-Atom.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-22 01:50:33.892,,,no_permission,,,,,,,,,,,,20899,,,Fri Jul 22 01:50:33 UTC 2011,,,,,,0|i0gedj:,93759,,,,,,,,,,,21/Jul/11 23:27;yangyangyyy;minor fix to avoid Eclipse compile error,"22/Jul/11 01:50;jbellis;committed, although in general I'm against humoring broken tools",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in src/java/org/apache/cassandra/cli/CliClient  ,CASSANDRA-2873,12513260,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,e1n,e1n,08/Jul/11 13:25,12/Mar/19 14:21,13/Mar/19 22:26,09/Jul/11 21:23,0.8.2,,,Legacy/Tools,,,0,,,,,"I have read your documentation about syntax for creating column family and parameters that I can pass.
According to documentation i can use parameter :

"" - keys_cache_save_period: Duration in seconds after which Cassandra should
  safe the keys cache. Caches are saved to saved_caches_directory as
  specified in conf/Cassandra.yaml. Default is 14400 or 4 hours. ""

but then i was receiving error: ""No enum const class org.apache.cassandra.cli.CliClient$ColumnFamilyArgument.KEYS_CACHE_SAVE_PERIOD""


In class mentioned in title we have:

protected enum ColumnFamilyArgument
115 	{
116 	COLUMN_TYPE,
117 	COMPARATOR,
118 	SUBCOMPARATOR,
119 	COMMENT,
120 	ROWS_CACHED,
121 	ROW_CACHE_SAVE_PERIOD,
122 	KEYS_CACHED,
123 	KEY_CACHE_SAVE_PERIOD,   <---- TYPO !
124 	READ_REPAIR_CHANCE,
125 	GC_GRACE,
126 	COLUMN_METADATA,
127 	MEMTABLE_OPERATIONS,
128 	MEMTABLE_THROUGHPUT,
129 	MEMTABLE_FLUSH_AFTER,
130 	DEFAULT_VALIDATION_CLASS,
131 	MIN_COMPACTION_THRESHOLD,
132 	MAX_COMPACTION_THRESHOLD,
133 	REPLICATE_ON_WRITE,
134 	ROW_CACHE_PROVIDER,
135 	KEY_VALIDATION_CLASS
136 	} ",ubuntu linux 10.4,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-07-09 21:23:11.43,,,no_permission,,,,,,,,,,,,20876,,,Sat Jul 09 22:22:29 UTC 2011,,,,,,0|i0gdz3:,93694,,,,,,,,,,,"09/Jul/11 21:23;jbellis;""key_cache_save_period"" is correct.  updated CliHelp.yaml to this.","09/Jul/11 22:22;hudson;Integrated in Cassandra-0.8 #212 (See [https://builds.apache.org/job/Cassandra-0.8/212/])
    fix typo for CASSANDRA-2873
fix typo for CASSANDRA-2873

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1144749
Files : 
* /cassandra/branches/cassandra-0.8/src/resources/org/apache/cassandra/cli/CliHelp.yaml

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1144748
Files : 
* /cassandra/branches/cassandra-0.8/src/resources/org/apache/cassandra/cli/CliHelp.yaml
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive JDBC connections fail with InvalidUrlException when both the C* and Hive JDBC drivers are loaded,CASSANDRA-2842,12512391,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ardot,cdaw,cdaw,30/Jun/11 23:01,12/Mar/19 14:21,13/Mar/19 22:26,01/Jul/11 19:50,,,,,,,0,,,,,"Hive connections fail with InvalidUrlException when both the C* and Hive JDBC drivers are loaded, and it seems the URL is being interpreted as a C* url.

{code}
	Caused an ERROR
    [junit] Invalid connection url:jdbc:hive://127.0.0.1:10000/default. should start with jdbc:cassandra
    [junit] org.apache.cassandra.cql.jdbc.InvalidUrlException: Invalid connection url:jdbc:hive://127.0.0.1:10000/default. should start with jdbc:cassandra
    [junit] 	at org.apache.cassandra.cql.jdbc.CassandraDriver.connect(CassandraDriver.java:90)
    [junit] 	at java.sql.DriverManager.getConnection(DriverManager.java:582)
    [junit] 	at java.sql.DriverManager.getConnection(DriverManager.java:185)
    [junit] 	at com.datastax.bugRepros.repro_connection_error.test1_runHiveBeforeJdbc(repro_connection_error.java:34)

{code}

*Code Snippet: intended to illustrate the connection issues* 
* Copy file to test directory
* Change package declaration
* run:  ant test -Dtest.name=repro_conn_error
{code}

package com.datastax.bugRepros;

import java.sql.DriverManager;
import java.sql.Connection;
import java.sql.SQLException;

import java.util.Enumeration;

import org.junit.Test;

public class repro_conn_error
{
    @Test
    public void jdbcConnectionError() throws Exception 
    {  
        // Create Hive JDBC Connection - will succeed if      
        try 
        {
            // Uncomment loading C* driver to reproduce bug
            Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
            
            // Load Hive driver and connect
            Class.forName(""org.apache.hadoop.hive.jdbc.HiveDriver"");
            Connection hiveConn = DriverManager.getConnection(""jdbc:hive://127.0.0.1:10000/default"", """", """");
            hiveConn.close();  
            System.out.println(""successful hive connection"");

        } catch (SQLException e) {
            System.out.println(""unsuccessful hive connection"");
            e.printStackTrace();
        }
        
        // Create C* JDBC Connection
        try 
        {
            Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
            Connection jdbcConn = DriverManager.getConnection(""jdbc:cassandra:root/root@127.0.0.1:9160/default"");     
            jdbcConn.close();    
            System.out.println(""successful c* connection"");

        } catch (SQLException e) {
            System.out.println(""unsuccessful c* connection"");

            e.printStackTrace();
        }
        
        // Print out all loaded JDBC drivers.
        Enumeration d = java.sql.DriverManager.getDrivers();
        
        while (d.hasMoreElements()) {
            Object driverAsObject = d.nextElement();
            System.out.println(""JDBC driver="" + driverAsObject);
        }
    }
}

{code}",,,,,,,,,,,,,,,,01/Jul/11 14:19;ardot;pass-if-not-right-driver-v1.txt;https://issues.apache.org/jira/secure/attachment/12484870/pass-if-not-right-driver-v1.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-01 14:19:48.372,,,no_permission,,,,,,,,,,,,20860,,,Fri Jul 01 19:50:15 UTC 2011,,,,,,0|i0gdsf:,93664,jbellis,jbellis,,,,,,,,,"01/Jul/11 14:19;ardot;This test has been run against v1.0.3 of the driver. In that version the {{connect(...)}} method of {{CassandraDriver}} is called with an unsupported protocol:subprotocol in its URL. It recognizes it is not the proper protocol but erroneously throws an exception rather than returning a null to the caller stating that it can not handle it, so please move on. The patch is based on the current trunk of {{/drivers}} (v1.0.4).",01/Jul/11 17:12;ardot;I took a quick look at the Hive sources and I believe you will find the Hive Driver suffers from this defect as well. So if you reversed the order I think it will be the Hive driver that throws an exception rather than deferring to the next driver in the chain of loaded drivers(C*).,"01/Jul/11 19:50;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The link for ""Latest Builds"" in the download page is incorrect",CASSANDRA-2736,12509063,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,kunda,kunda,02/Jun/11 06:56,12/Mar/19 14:21,13/Mar/19 22:26,06/Jun/11 14:21,,,,Legacy/Documentation and Website,,,0,,,,,"The link in the download page (http://cassandra.apache.org/download/) is outdated, pointing to 
{noformat}http://hudson.zones.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/{noformat}
and should probably replaced by
{noformat}http://builds.apache.org/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/{noformat}

In addition, the wording ""Latest Builds"" is incorrect per the link, and should be changed to ""Latest Build"" or more precisely ""Latest Trunk Build"".
Alternatively, the link could instead point to either {noformat}https://builds.apache.org/job/Cassandra/changes{noformat} or {noformat}https://builds.apache.org/job/Cassandra{noformat}, both including a list of the latest trunk builds.

Furthermore, it might be sensible to have additional links to 0.6, 0.7 & 0.8 builds - as it's more probable users are running those rather than the trunk version.

Finally (I didn't think I'd right so much about a link...), I believe the text ""(Hudson)"" next to the link is not up-to-date, and should be replaced by ""(Jenkins)"" or removed altogether.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-06-06 14:21:40.059,,,no_permission,,,,,,,,,,,,20796,,,Mon Jun 06 14:21:40 UTC 2011,,,,,,0|i0gd5b:,93560,,,,,,,,,,,06/Jun/11 14:21;jbellis;updated to the jenkins latest-trunk-build url.  thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counters are not always hinted,CASSANDRA-3099,12520576,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,29/Aug/11 14:29,12/Mar/19 14:21,13/Mar/19 22:26,29/Aug/11 15:08,0.8.5,,,,,,0,,,,,"CASSANDRA-2892 mistakenly removed some hints for counters, namely the hints that were supposed to be stored on the local node (that is, instead of removing from the hintedEndpoints multimap only the local write (since it has been already applied), we were removing everything having the local node as destination).",,,,,,,,,,,,,,,,29/Aug/11 14:29;slebresne;0001-hint-counters.patch;https://issues.apache.org/jira/secure/attachment/12492076/0001-hint-counters.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-29 14:38:35.482,,,no_permission,,,,,,,,,,,,20969,,,Mon Aug 29 16:23:13 UTC 2011,,,,,,0|i0gfov:,93972,jbellis,jbellis,,,,,,,,,29/Aug/11 14:38;jbellis;+1,"29/Aug/11 15:08;slebresne;Committed, thanks","29/Aug/11 16:23;hudson;Integrated in Cassandra-0.8 #298 (See [https://builds.apache.org/job/Cassandra-0.8/298/])
    Always hint counters
patch by slebresne; reviewed by jbellis for CASSANDRA-3099

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1162844
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE during HH delivery when gossip turned off on target,CASSANDRA-3677,12536442,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,hsn,hsn,27/Dec/11 20:49,12/Mar/19 14:21,13/Mar/19 22:26,07/Feb/12 15:33,1.0.8,,,,,,0,,,,,"probably not important bug

ERROR [OptionalTasks:1] 2011-12-27 21:44:25,342 AbstractCassandraDaemon.java (line 138) Fatal exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
        at org.cliffc.high_scale_lib.NonBlockingHashMap.hash(NonBlockingHashMap.java:113)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:553)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:348)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfAbsent(NonBlockingHashMap.java:319)
        at org.cliffc.high_scale_lib.NonBlockingHashSet.add(NonBlockingHashSet.java:32)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleHintDelivery(HintedHandOffManager.java:371)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleAllDeliveries(HintedHandOffManager.java:356)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:119)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
",,,,,,,,,,,,,,,,07/Feb/12 15:09;soverton;3677-v1.patch;https://issues.apache.org/jira/secure/attachment/12513620/3677-v1.patch,28/Dec/11 20:33;brandon.williams;3677.txt;https://issues.apache.org/jira/secure/attachment/12508789/3677.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-28 18:25:15.704,,,no_permission,,,,,,,,,,,,222125,,,Thu Feb 23 20:59:34 UTC 2012,,,,,,0|i0gmvz:,95138,,,,,,,,,,,"28/Dec/11 18:25;brandon.williams;Can you confirm the version is 1.0.6?  This only seems possible with CASSANDRA-3554, because otherwise hint delivery only happens when a node comes up.",28/Dec/11 19:37;hsn;its 1.0.6 + patches from 1.0 branch,28/Dec/11 20:33;brandon.williams;The simplest thing to do is double up on the FD check.,"29/Dec/11 10:14;hsn;This error is not related to target node down, i got it today and no nodes down are reported.


INFO [HintedHandoff:1] 2011-12-29 10:49:38,993 HintedHandOffManager.java (line 
334) Finished hinted handoff of 0 rows to endpoint /216.17.99.40

 INFO [CompactionExecutor:5] 2011-12-29 10:54:52,465 CompactionTask.java (line 113) Compacting [SSTableReader(path='/usr/local/cassandra/data/system/HintsColumnFamily-hc-963-Data.db'), SSTableReader(path='/usr/local/cassandra/data/system/HintsColumnFamily-hc-962-Data.db')]
 INFO [CompactionExecutor:5] 2011-12-29 10:55:08,744 CompactionTask.java (line 221) Compacted to [/usr/local/cassandra/data/system/HintsColumnFamily-hc-964-Data.db,].  709,504,640 to 165,449,718 (~23% of original) bytes for 2 keys at 9.692558MB/s.  Time: 16,279ms.

ERROR [OptionalTasks:1] 2011-12-29 10:59:06,482 AbstractCassandraDaemon.java (line 138) Fatal exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
        at org.cliffc.high_scale_lib.NonBlockingHashMap.hash(NonBlockingHashMap.java:113)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:553)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:348)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfAbsent(NonBlockingHashMap.java:319)
        at org.cliffc.high_scale_lib.NonBlockingHashSet.add(NonBlockingHashSet.java:32)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleHintDelivery(HintedHandOffManager.java:371)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleAllDeliveries(HintedHandOffManager.java:356)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:119)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)","02/Jan/12 15:10;hsn;I suspect that this is happening when node is trying to deliver hints created for himself (to is null after all)

because i have some stuck hints, cant list them via CLI to look at keys because node will OOM.

INFO [CompactionExecutor:7] 2012-01-02 15:48:13,084 CompactionTask.java (line 221) Compacted to [/usr/local/cassandra/data/system/HintsColumnFamily-hc-1011-Data.db,].  168,568,056 to 163,770,679 (~97% of original) bytes for 1 keys at 29.619551MB/s.  Time: 5,273ms.

Its 2 node cluster and no hints were delivered to other node:

INFO [HintedHandoff:1] 2012-01-02 15:41:29,035 HintedHandOffManager.java (line 267) Started hinted handoff for token: 99070591730234615865843651857942052864
INFO [HintedHandoff:1] 2012-01-02 15:41:29,217 HintedHandOffManager.java (line 334) Finished hinted handoff of 0 rows to","06/Jan/12 19:17;brandon.williams;Sounds like this is just fallout from CASSANDRA-3440 then, you can delete the hints and see if it continues.","07/Feb/12 15:08;soverton;I've been able to reproduce this in 1.07 and in trunk as follows:
* create a cluster of 2 nodes
* stop one of the nodes
* insert some data at RF=1, CL=ANY - this causes hints to be stored 
* nodetool removetoken on the token of the dead node
* some time up to 10 minutes later the exception is logged:

ERROR [OptionalTasks:1] 2012-02-07 14:41:57,710 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
	at org.cliffc.high_scale_lib.NonBlockingHashMap.hash(NonBlockingHashMap.java:113)
	at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:553)
	at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:348)
	at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfAbsent(NonBlockingHashMap.java:319)
	at org.cliffc.high_scale_lib.NonBlockingHashSet.add(NonBlockingHashSet.java:32)
	at org.apache.cassandra.db.HintedHandOffManager.scheduleHintDelivery(HintedHandOffManager.java:410)
	at org.apache.cassandra.db.HintedHandOffManager.scheduleAllDeliveries(HintedHandOffManager.java:395)
	at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:84)
	at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:119)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

Adverse Effects:
* The 10min repeating scheduleAllDeliveries() introduced in CASSANDRA-3554 will no longer fire (since the exception was uncaught), so the node has now regressed to pre-CASSANDRA-3554 behaviour
* Even after the node is restarted, the exception will be thrown again next time the schedule is run. 
* This will continue until the row tombstone for the dead node's hints is garbage-collected (10 days)

Fix:
* don't try to schedule delivery of hints for tokens which are no longer valid endpoints (see attached).
",07/Feb/12 15:09;soverton;attached patch against trunk,"07/Feb/12 15:33;brandon.williams;Good catch, Sam.  Committed.","15/Feb/12 16:26;appodictic;Q: don't try to schedule delivery of hints for tokens which are no longer valid endpoints (see attached).

Does this mean the hints are just lost for good? Does anyone thing an attempt should be made to determine where the hints should go and possibly re-write them?","15/Feb/12 17:00;brandon.williams;It doesn't make a lot of sense.  a) we have no way to quickly find such hints, and b) if you removetoken the node, the data from existing replicas will be copied to restore the RF, so the hint isn't necessary (unless you wrote at ANY, in which case you've already lived dangerously.)","15/Feb/12 17:03;soverton;When a token is removed, hints intended for that endpoint are removed (see StorageService.excise(Token token, InetAddress endpoint)) so yes, they are lost for good. 

The removetoken process involves streaming from replicas to the new endpoint, so it should be up to date assuming writes were at CL > ANY. I can't think of a case where we would gain anything by delivering the hints for the removed endpoint (except where writes were at CL.ANY).
","15/Feb/12 17:31;appodictic;It seems wrong to abandon hints. Arguments like such as ""(unless you wrote at ANY, in which case you've already lived dangerously.)"" are  a slippery slope, and it says something about the contract of ANY.

According to Cassandra.thrift.
{quote}
 *   ANY          Ensure that the write has been written once somewhere, including possibly being hinted in a non-target node.
{quote}

It does not say :

{quote}
 *   ANY          Ensure that the write has been written once somewhere, including possibly being hinted in a non-target node, which probably wont get lost, unless .....
{quote}


Maybe there is some other harder to contrive scenario out there RF3, write ONE, two hints and one node failure then a move also causes an issue with lost hints.

It is an edge case, but I think it is important. Since writes are idempotent I would rather a hint gets delivered causing an extra write then it gets lost. 

1.0's made HH way more reliable, I would like to see Cassandra push that high standard and not have caveats associated around how ANY works.

 



","23/Feb/12 20:59;jbellis;If a node is removed while it's up (decommission), then removing hints should be a no-op, since they should be handed off before the stream-to-new-owners.  I'd be okay with a minor tweak to perform a check before decommission to refuse to continue if this is not the case.

But if a node is removed while dead (removeToken) then the hints for the old node are worthless, since we're streaming from a different copy, that by definition didn't need the hints for the dead one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java should not allow arbitrary arguments,CASSANDRA-2323,12501378,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,14/Mar/11 17:01,12/Mar/19 14:21,13/Mar/19 22:26,30/Mar/11 16:56,0.7.5,,,,,,0,,,,,"This doesn't seem like a big deal, until you accidentally insert a space between a dash and it's flag, and it's at the point where the line wraps in your terminal.",,,,,,,,,,,,,,,,28/Mar/11 16:28;xedin;CASSANDRA-2323.patch;https://issues.apache.org/jira/secure/attachment/12474787/CASSANDRA-2323.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-30 18:02:25.082,,,no_permission,,,,,,,,,,,,20561,,,Wed Mar 30 18:02:25 UTC 2011,,,,,,0|i0gapb:,93164,brandon.williams,brandon.williams,,,,,,,,,30/Mar/11 16:56;brandon.williams;Committed,"30/Mar/11 18:02;hudson;Integrated in Cassandra-0.7 #414 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/414/])
    stress.java rejects arbitrary arguments.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2323
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL Metadata has inconsistent schema nomenclature,CASSANDRA-3436,12529565,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,kreynolds,kreynolds,31/Oct/11 21:43,12/Mar/19 14:21,13/Mar/19 22:26,01/Nov/11 02:15,1.0.2,,,,,,0,cql,,,,"The dumped object below shows that the default_name_type and the default_value_type are referenced inconsistently .. default_name_type should probably use the shortened version like everything else.

--- !ruby/object:CassandraCQL::Thrift::CqlResult 
rows: 
- !ruby/object:CassandraCQL::Thrift::CqlRow 
  columns: 
  - !ruby/object:CassandraCQL::Thrift::Column 
    name: id
    timestamp: -1
    value: test string
  - !ruby/object:CassandraCQL::Thrift::Column 
    name: test_column
    timestamp: 1320097088551000
    value: test
  key: test string
schema: !ruby/object:CassandraCQL::Thrift::CqlMetadata 
  default_name_type: org.apache.cassandra.db.marshal.UTF8Type
  default_value_type: UTF8Type
  name_types: 
    id: AsciiType
  value_types: 
    id: AsciiType
    test_column: UTF8Type
type: 1",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-11-01 02:15:16.919,,,no_permission,,,,,,,,,,,,215425,,,Tue Nov 01 02:15:16 UTC 2011,,,,,,0|i0gjvr:,94651,,,,,,,,,,,01/Nov/11 02:15;jbellis;fixed in r1195768,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool printing classpath,CASSANDRA-3343,12526553,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,cdaw,cdaw,11/Oct/11 03:31,12/Mar/19 14:21,13/Mar/19 22:26,11/Oct/11 12:13,1.0.0,,,,,,0,,,,,"* Get file from: [https://repository.apache.org/content/repositories/orgapachecassandra-046/org/apache/cassandra/apache-cassandra/1.0.0/apache-cassandra-1.0.0-bin.tar.gz]
* Install C* and start server
* Run: nodetool -h localhost ring

{code}
Cathy-Daws-MacBook-Pro:bin cathy$ ./nodetool -h localhost ring

./../conf:./../build/classes/main:./../build/classes/thrift:./../lib/antlr-3.2.jar:./../lib/apache-cassandra-1.0.0.jar:./../lib/apache-cassandra-clientutil-1.0.0.jar:./../lib/apache-cassandra-thrift-1.0.0.jar:./../lib/avro-1.4.0-fixes.jar:./../lib/avro-1.4.0-sources-fixes.jar:./../lib/commons-cli-1.1.jar:./../lib/commons-codec-1.2.jar:./../lib/commons-lang-2.4.jar:./../lib/compress-lzf-0.8.4.jar:./../lib/concurrentlinkedhashmap-lru-1.2.jar:./../lib/guava-r08.jar:./../lib/high-scale-lib-1.1.2.jar:./../lib/jackson-core-asl-1.4.0.jar:./../lib/jackson-mapper-asl-1.4.0.jar:./../lib/jamm-0.2.5.jar:./../lib/jline-0.9.94.jar:./../lib/json-simple-1.1.jar:./../lib/libthrift-0.6.jar:./../lib/log4j-1.2.16.jar:./../lib/servlet-api-2.5-20081211.jar:./../lib/slf4j-api-1.6.1.jar:./../lib/slf4j-log4j12-1.6.1.jar:./../lib/snakeyaml-1.6.jar:./../lib/snappy-java-1.0.3.jar
Address         DC          Rack        Status State   Load            Owns    Token                                       
127.0.0.1       datacenter1 rack1       Up     Normal  8.91 KB         100.00% 10597065753338857570408052040129979696      
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-10-11 04:05:35.54,,,no_permission,,,,,,,,,,,,57031,,,Tue Oct 11 12:13:31 UTC 2011,,,,,,0|i0giqv:,94467,,,,,,,,,,,11/Oct/11 04:05;brandon.williams;Looks like this snuck into cassandra.in.sh through CASSANDRA-3311 (accidental?) which is the wrong place to do this if we do want to print the classpath.,11/Oct/11 12:13;slebresne;That's clearly a mistake. Corrected as r1181741.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memtables do not need to be flushed on the Table.apply() path anymore after 2449,CASSANDRA-3210,12523138,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yangyangyyy,yangyangyyy,yangyangyyy,15/Sep/11 01:24,12/Mar/19 14:21,13/Mar/19 22:26,15/Sep/11 03:39,1.0.0,,,,,,0,,,,,"2449 removes auto-flush from Table.apply(), but the data structure is still there, no harm, but better remove it:

in
https://github.com/apache/cassandra/blob/c7cdc317c9a14e29699f9842424388aee77d0e1a/src/java/org/apache/cassandra/db/Table.java

line 399 and 470",,,,,,,,,,,,,,,,15/Sep/11 01:24;yangyangyyy;0001-memtables-do-not-need-to-be-flushed-on-the-Table.app.patch;https://issues.apache.org/jira/secure/attachment/12494550/0001-memtables-do-not-need-to-be-flushed-on-the-Table.app.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-15 03:39:05.161,,,no_permission,,,,,,,,,,,,4018,,,Thu Sep 15 03:39:05 UTC 2011,,,,,,0|i0gh4v:,94206,jbellis,jbellis,,,,,,,,,"15/Sep/11 03:39;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't try to build secondary indexes when there is none,CASSANDRA-3123,12521174,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,02/Sep/11 12:01,12/Mar/19 14:21,13/Mar/19 22:26,02/Sep/11 17:20,0.8.5,,,Feature/2i Index,,,0,,,,,"buildSecondaryIndexes() is sometimes called without checking the cfs has secondary indexes. Has a result, it prints a useless message and will trigger a bunch of useless action (among which, a full scan of the indexed column family). This is not a huge problem in 0.8 because only the fairly new loadNewSSTables() call does this (which doesn't mean we should fix it). But in trunk, it does this after every streamIn session. ",,,,,,,,,,,,,,,,02/Sep/11 17:03;slebresne;3123-v2.patch;https://issues.apache.org/jira/secure/attachment/12492774/3123-v2.patch,02/Sep/11 12:04;slebresne;3123.patch;https://issues.apache.org/jira/secure/attachment/12492735/3123.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-02 13:24:07.884,,,no_permission,,,,,,,,,,,,4077,,,Fri Sep 02 18:15:32 UTC 2011,,,,,,0|i0gg2f:,94033,jbellis,jbellis,,,,,,,,,02/Sep/11 12:04;slebresne;Attaching patch against 0.8. It makes buildSecondaryIndexes just return if there is no indexed columns.,"02/Sep/11 13:24;jbellis;If we're going to change to relying on the build method to recognize it was asked to perform a no-op and bail early, we should call it ""maybeBuild..."" or similar and add javadoc so it's clear that it might be a no-op.",02/Sep/11 17:03;slebresne;Updated patch with proposed changes,02/Sep/11 17:08;jbellis;+11111111,"02/Sep/11 17:20;slebresne;Committed, thanks","02/Sep/11 18:15;hudson;Integrated in Cassandra-0.8 #312 (See [https://builds.apache.org/job/Cassandra-0.8/312/])
    Don't try to build secondary indexes when there is none
patch by slebresne; reviewed by jbellis for CASSANDRA-3123

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1164634
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/StreamInSession.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
example configuration of commitlog_sync: batch,CASSANDRA-2880,12513647,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,wmeler,wmeler,wmeler,11/Jul/11 09:17,12/Mar/19 14:21,13/Mar/19 22:26,12/Jul/11 19:20,0.8.2,,,,,,0,,,,,"There is no example of commitlog_sync: batch configuration in default config file, and one have to guess that commitlog_sync_batch_window_in_ms should be configured instead of CommitLogSyncBatchWindowInMS.",,,,,,,,,,,,,,,,11/Jul/11 09:19;wmeler;commitlog_batch.patch;https://issues.apache.org/jira/secure/attachment/12486033/commitlog_batch.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-12 19:20:34.944,,,no_permission,,,,,,,,,,,,20878,,,Tue Jul 12 19:20:34 UTC 2011,,,,,,0|i0ge07:,93699,jbellis,jbellis,,,,,,,,,11/Jul/11 09:19;wmeler;example config in comments,11/Jul/11 09:31;wmeler;patch for config file ready,"12/Jul/11 19:20;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: DELETE documentation uses UPDATE examples,CASSANDRA-2567,12505242,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,cdaw,cdaw,26/Apr/11 20:15,12/Mar/19 14:21,13/Mar/19 22:26,26/Apr/11 21:26,0.8.0 beta 2,,,,,,0,cql,,,,"
{panel}
h2. DELETE

_Synopsis:_

bc. 
DELETE [COLUMNS] FROM <COLUMN FAMILY> [USING <CONSISTENCY>] WHERE KEY = keyname1
DELETE [COLUMNS] FROM <COLUMN FAMILY> [USING <CONSISTENCY>] WHERE KEY IN (keyname1, keyname2);

A @DELETE@ is used to perform the removal of one or more columns from one or more rows.

h3. Specifying Columns

bc. 
DELETE [COLUMNS] ...

Following the @DELETE@ keyword is an optional comma-delimited list of column name terms. When no column names are specified, the remove applies to the entire row(s) matched by the ""WHERE clause"":#deleterows

h3. Column Family

bc. 
DELETE ... FROM <COLUMN FAMILY> ...

The column family name follows the list of column names.

h3. Consistency Level

bc. 
UPDATE ... [USING <CONSISTENCY>] ...

Following the column family identifier is an optional ""consistency level specification"":#consistency.

h3(#deleterows). Specifying Rows

bc. 
UPDATE ... WHERE KEY = keyname1
UPDATE ... WHERE KEY IN (keyname1, keyname2)

The @WHERE@ clause is used to determine which row(s) a @DELETE@ applies to.  The first form allows the specification of a single keyname using the @KEY@ keyword and the @=@ operator.  The second form allows a list of keyname terms to be specified using the @IN@ notation and a parenthesized list of comma-delimited keyname terms.
     
{panel}",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-04-26 21:26:32.033,,,no_permission,,,,,,,,,,,,20701,,,Tue Apr 26 21:26:32 UTC 2011,,,,,,0|i0gc4v:,93396,,,,,,,,,,,26/Apr/11 21:26;jbellis;fixed in r1096915,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mergeShardsChance deprecated; remove from thrift?,CASSANDRA-3940,12543576,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,thepaul,thepaul,21/Feb/12 22:00,12/Mar/19 14:21,13/Mar/19 22:26,22/Feb/12 15:32,1.1.0,,,Legacy/CQL,,,0,thrift_protocol,,,,Or at least it should be marked deprecated somehow.,,,,,,,,,,,,,,,,22/Feb/12 10:03;slebresne;0001-Remove-merge_shard_chance.txt;https://issues.apache.org/jira/secure/attachment/12515569/0001-Remove-merge_shard_chance.txt,22/Feb/12 10:03;slebresne;0002-Thrift-generated-files-updates.txt;https://issues.apache.org/jira/secure/attachment/12515570/0002-Thrift-generated-files-updates.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-22 10:03:12.698,,,no_permission,,,,,,,,,,,,228815,,,Wed Feb 22 15:32:28 UTC 2012,,,,,,0|i0gq1z:,95651,jbellis,jbellis,,,,,,,,,22/Feb/12 10:03;slebresne;It is indeed now useless. Attached patches to remove (against 1.1.0).,22/Feb/12 14:50;jbellis;+1,"22/Feb/12 15:32;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
help in schema-sample uses wrong file name,CASSANDRA-2360,12501914,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,21/Mar/11 02:42,12/Mar/19 14:21,13/Mar/19 22:26,21/Mar/11 14:21,0.7.5,,,,,,0,,,,,"As described in CASSANDRA-2007 

Wasn't sure about re-opening a resolved issue and wanted to make sure it was not lost. 
",,,,,,,,,,,,,,,,21/Mar/11 02:43;amorton;0001-change-help-to-use-correct-file-name-conf-sample-sch.patch;https://issues.apache.org/jira/secure/attachment/12474145/0001-change-help-to-use-correct-file-name-conf-sample-sch.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-21 14:21:01.848,,,no_permission,,,,,,,,,,,,20580,,,Mon Mar 21 14:21:01 UTC 2011,,,,,,0|i0gaxb:,93200,jbellis,jbellis,,,,,,,,,21/Mar/11 02:42;amorton;Modifies the help to use the correct name for the sample schema file. ,21/Mar/11 14:21;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tame excessive logging during repairs,CASSANDRA-2595,12506077,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,doubleday,doubleday,doubleday,03/May/11 17:02,12/Mar/19 14:20,13/Mar/19 22:26,04/May/11 08:43,0.7.6,,,,,,0,,,,,"PendingFile.toString is called from logging (i.e. StreamOut:173) which lists all sections in the pending file.

This leads to (in our case multi mb ) ... (59352638,59354005),(59373477,59379520),(59381952,59385368) ... in the log.",,,,,,,,,,,,,,,,03/May/11 17:04;doubleday;PendingFileToString.patch;https://issues.apache.org/jira/secure/attachment/12478069/PendingFileToString.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-03 17:22:50.069,,,no_permission,,,,,,,,,,,,20719,,,Wed May 04 09:28:57 UTC 2011,,,,,,0|i0gcb3:,93424,slebresne,slebresne,,,,,,,,,"03/May/11 17:22;slebresne;+1, repair is clearly too verbose (I'll commit that soonish)","04/May/11 08:39;hudson;Integrated in Cassandra-0.7 #467 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/467/])
    Tame excessive logging during repairs
patch by doubleday; reviewed by slebresne for CASSANDRA-2595
","04/May/11 08:43;slebresne;Committed, thanks","04/May/11 09:28;hudson;Integrated in Cassandra-0.8 #64 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/64/])
    merge CASSANDRA-2595 from 0.8
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC Unit test failed to run due to spurious character in text and bad YAML entry,CASSANDRA-2957,12515546,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ardot,ardot,ardot,27/Jul/11 16:08,12/Mar/19 14:20,13/Mar/19 22:26,27/Jul/11 17:28,,,,,,,0,,,,,"Problem #1:

Bad character in the text (line 294, col 1):
{code}
˜        PreparedStatement stmt = con.prepareStatement(""update JdbcInteger set ?=?, ?=? where key = ?"");
{code}

Problem #2:
Outdated YAML directive (line 17):

{code}
commitlog_rotation_threshold_in_mb: 128
{code}
",,,,,,,,,,,,,,,,27/Jul/11 16:09;ardot;jdbc-unittest-fix-v1.txt;https://issues.apache.org/jira/secure/attachment/12487991/jdbc-unittest-fix-v1.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-27 17:28:06.946,,,no_permission,,,,,,,,,,,,20912,,,Wed Jul 27 17:28:06 UTC 2011,,,,,,0|i0gei7:,93780,,,,,,,,,,,27/Jul/11 16:10;ardot;Patch is against trunk,"27/Jul/11 17:28;jbellis;+1, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL: cqlsh does shows Exception, but not error message when running truncate while a node is down.",CASSANDRA-2539,12504937,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,cdaw,cdaw,22/Apr/11 00:28,12/Mar/19 14:20,13/Mar/19 22:26,23/Apr/11 00:18,0.8.0 beta 2,,,,,,0,cql,,,,"This is really just a usability bug, but it would nice to bubble the error message that is printed in the log file up to the interface.

*cqlsh output*
{noformat}
cqlsh> truncate users;
Exception: UnavailableException()
{noformat}

*log file error*
{noformat}
 INFO [pool-2-thread-5] 2011-04-21 23:53:30,466 StorageProxy.java (line 1021) Cannot perform truncate, some hosts are down
{noformat}",,,,,,,,,,,,,,,,22/Apr/11 20:49;jbellis;2539-v3.txt;https://issues.apache.org/jira/secure/attachment/12477161/2539-v3.txt,22/Apr/11 19:47;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2539-print-friendlier-message-for-Unavailabl.txt;https://issues.apache.org/jira/secure/attachment/12477141/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2539-print-friendlier-message-for-Unavailabl.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-22 08:19:41.23,,,no_permission,,,,,,,,,,,,20684,,,Tue Apr 26 22:10:19 UTC 2011,,,,,,0|i0gbyn:,93368,jbellis,jbellis,,,,,,,,,"22/Apr/11 08:19;slebresne;UnavailableException does mean, by definition, ""some hosts are down"" (which means that we don't really have anything to bubble anything, we can just have cqlsh write it in plain english if we'd like).

But this made me think, maybe we could attach a String to the UnavailableException saying which nodes are down. Not sure what are the consequence in term of thrift compatibility though and if it has any, it's probably not worth the trouble.",22/Apr/11 19:48;urandom;patch attached,22/Apr/11 20:49;jbellis;v3 also adds a wrapper for TimedOutException,22/Apr/11 21:51;urandom;+1,23/Apr/11 00:18;jbellis;committed,"23/Apr/11 01:26;hudson;Integrated in Cassandra-0.8 #36 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/36/])
    more user-friendly messages for UE/TOE in cqlsh
patch by eevans and jbellis for CASSANDRA-2539
","26/Apr/11 22:10;cdaw;Re-tested in current build.  cqlsh now returns the message:

Unable to complete request: one or more nodes were unavailable.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We should refuse query for counters at CL.ANY,CASSANDRA-2990,12517821,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,03/Aug/11 18:10,12/Mar/19 14:20,13/Mar/19 22:26,09/Aug/11 20:27,0.8.4,,,,,,0,counters,,,,"We currently do not reject writes for counters at CL.ANY, even though this is not supported (and rightly so).",,,,,,,,,,,,,,,,09/Aug/11 17:40;slebresne;2990.patch;https://issues.apache.org/jira/secure/attachment/12489861/2990.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-09 18:40:17.612,,,no_permission,,,,,,,,,,,,20926,,,Tue Aug 09 21:36:14 UTC 2011,,,,,,0|i0gepb:,93812,jbellis,jbellis,,,,,,,,,"09/Aug/11 18:40;jbellis;A few days ago, you said, ""A counter mutation only live enough so that it is applied to the first replica. Once this is done, a *row* mutation is generated for the other replica. That second mutation can be hinted. But that is a row mutation, so there should be no special casing at all for that.""

Why can't we hint the first replica?","09/Aug/11 19:14;slebresne;bq. Why can't we hint the first replica?

Well, actually I think we could. Or at least if we cannot I forgot why. We would need to be sure we never replay an hint twice though, which I'm not sure is a guarantee right now. Also, we can only make this if what we store as a hint is the serialized mutation (in this case, the serialized CounterMutation): we can't apply the CounterMutation on a non-replica (partly because that would potentially increase the counter context too much, partly because counter remove suck, which would probably be a problem at some point).

So it should be doable, but it's a bit of work.","09/Aug/11 19:32;jbellis;Okay, +1 on making the validation match what is actually currently supported (no ANY for counters), although I'd change ""not supported"" to ""not yet supported.""

We can deal w/ adding ANY support if and when someone actually needs it.",09/Aug/11 20:27;slebresne;Committed with message tweak,"09/Aug/11 21:36;hudson;Integrated in Cassandra-0.8 #265 (See [https://builds.apache.org/job/Cassandra-0.8/265/])
    Refuse counter write at CL.ANY
patch by slebresne; reviewed by jbellis for CASSANDRA-2990

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1155548
Files : 
* /cassandra/branches/cassandra-0.8/test/system/test_cql.py
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/test/system/test_thrift_server.py
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/UpdateStatement.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaDoc fix for org.apache.cassandra.db.filter.QueryFilter,CASSANDRA-3993,12545084,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,darkdimius,darkdimius,darkdimius,04/Mar/12 10:21,12/Mar/19 14:20,13/Mar/19 22:26,06/Mar/12 19:16,1.1.1,,,Legacy/Documentation and Website,,,0,,,,,@param should be on separate line,,180,180,,0%,180,180,,,,,,,,,04/Mar/12 10:23;darkdimius;JavaDoc_fix_in_QueryFilter.patch;https://issues.apache.org/jira/secure/attachment/12516985/JavaDoc_fix_in_QueryFilter.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-06 19:16:59.307,,,no_permission,,,,,,,,,,,,230270,,,Tue Mar 06 19:16:59 UTC 2012,,,,,,0|i0gqpr:,95758,,,,,,,,,,,"04/Mar/12 10:23;darkdimius;Patch fixing the javaDoc

Created in wikimart.ru","06/Mar/12 19:16;jbellis;committed, but for future reference it would be a good idea to address multiple trivial changes like this in a single ticket",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix typo in nodetool help for repair,CASSANDRA-3905,12542446,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,13/Feb/12 21:44,12/Mar/19 14:20,13/Mar/19 22:26,14/Feb/12 10:46,1.0.8,,,,,,0,,,,,It says to use {{-rp}} instead of {{-pr}}.,,,,,,,,,,,,,,,,13/Feb/12 21:45;scode;CASSANDRA-3905.txt;https://issues.apache.org/jira/secure/attachment/12514401/CASSANDRA-3905.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-13 22:08:38.243,,,no_permission,,,,,,,,,,,,227732,,,Tue Feb 14 10:46:24 UTC 2012,,,,,,0|i0gpnb:,95585,jbellis,jbellis,,,,,,,,,13/Feb/12 22:08;jbellis;+1,14/Feb/12 10:46;slebresne;Committed (to 1.0 and upwards),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli shows org.apache.Cassandra.XXX in example help for replication strategy but it should be cassandra with a lowercase c,CASSANDRA-3509,12532000,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tommysdk,mdennis,mdennis,18/Nov/11 21:49,12/Mar/19 14:20,13/Mar/19 22:26,05/Mar/12 23:47,1.1.0,,,,,,0,,,,,"copying and pasting the example doesn't result in a working example and noticing the ""C"" -v- ""c"" is something easy to overlook ",,,,,,,,,,,,,,,,05/Mar/12 21:17;tommysdk;CASSANDRA-3509.PATCH;https://issues.apache.org/jira/secure/attachment/12517126/CASSANDRA-3509.PATCH,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-05 21:17:44.107,,,no_permission,,,,,,,,,,,,217736,,,Mon Mar 05 23:47:48 UTC 2012,,,,,,0|i0gks7:,94797,brandon.williams,brandon.williams,,,,,,,,,05/Mar/12 21:17;tommysdk;Submitted patch to cli help containing fix for incorrect package names (Cassandra should be lower-cased).,05/Mar/12 21:18;tommysdk;Apply patch on current trunk.,05/Mar/12 23:47;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] fix logging contexts,CASSANDRA-3404,12528815,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,dbrosius@apache.org,dbrosius@apache.org,26/Oct/11 03:40,12/Mar/19 14:20,13/Mar/19 22:26,26/Oct/11 08:26,,,,,,,0,,,,,"a couple of places the logging context doesn't match the class, probably due to copy/paste bug.
fixed.",,,,,,,,,,,,,,,,26/Oct/11 03:40;dbrosius@apache.org;log_context.diff;https://issues.apache.org/jira/secure/attachment/12500813/log_context.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-26 08:26:12.034,,,no_permission,,,,,,,,,,,,214675,,,Wed Oct 26 08:26:12 UTC 2011,,,,,,0|i0gjh3:,94585,,,,,,,,,,,"26/Oct/11 08:26;slebresne;Commmitted in r1189073, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoadBroadcaster never removes endpoints,CASSANDRA-3475,12530865,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,09/Nov/11 11:57,12/Mar/19 14:20,13/Mar/19 22:26,10/Nov/11 00:52,1.0.3,,,,,,0,lhf,,,,As the title says.,,,,,,,,,,,,,,,,09/Nov/11 11:59;brandon.williams;3475.txt;https://issues.apache.org/jira/secure/attachment/12503062/3475.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-09 20:20:10.681,,,no_permission,,,,,,,,,,,,216603,,,Thu May 16 23:30:27 UTC 2013,,,,,,0|i0gkd3:,94729,,,,,,,,,,,09/Nov/11 12:00;brandon.williams;Straightforward patch.,09/Nov/11 20:20;jbellis;+1,10/Nov/11 00:51;brandon.williams;Committed,16/May/13 23:30;codevally;Still this happen top me with 1.0.11 version. The JMX LoadMap shows already decommission nodes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metered Flusher log message confusing,CASSANDRA-3293,12525436,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,hsn,hsn,01/Oct/11 15:41,12/Mar/19 14:20,13/Mar/19 22:26,03/Oct/11 04:52,0.8.7,1.0.0,,,,,0,,,,," INFO [NonPeriodicTasks:1] 2011-10-01 17:34:32,652 MeteredFlusher.java (line 62) flushing high-traffic column family ColumnFamilyStore(+table='rapidshare',+ columnFamily='resultcache')

instead of table it should be *keyspace=*",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-10-03 04:52:52.519,,,no_permission,,,,,,,,,,,,42930,,,Mon Oct 03 04:52:52 UTC 2011,,,,,,0|i0gi5b:,94370,,,,,,,,,,,03/Oct/11 04:52;jbellis;fixed in r1178297.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MergeIterator assertion on sources != empty can be thrown,CASSANDRA-3260,12524658,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,26/Sep/11 16:11,12/Mar/19 14:20,13/Mar/19 22:26,27/Sep/11 13:16,1.0.0,,,,,,0,,,,,"MergeIterator.get assert that it don't get an empty list of sources. This seems to at least not be the case in the unit test for some of tests (this don't make any test fail however, but there is a few stack trace thrown). I think it's pretty unnatural to ""fail"" on an empty list of sources and would force every caller to first take the empty case into account, so I propose to just remove that assertion.",,,,,,,,,,,,,,,,26/Sep/11 16:12;slebresne;3260.patch;https://issues.apache.org/jira/secure/attachment/12496494/3260.patch,27/Sep/11 07:34;slebresne;3260_v2.patch;https://issues.apache.org/jira/secure/attachment/12496641/3260_v2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-26 16:50:05.436,,,no_permission,,,,,,,,,,,,3230,,,Tue Sep 27 13:16:31 UTC 2011,,,,,,0|i0ghr3:,94306,jbellis,jbellis,,,,,,,,,"26/Sep/11 16:50;jbellis;Should also then change the next line from

        if (sources.size() == 1)

to

        if (sources.size() <= 1)

+1 w/ that","27/Sep/11 07:34;slebresne;Hum, I don't think that works because both OneToOne and TrivialOneToOne constructors do sources.get(0), so that should stay a == 1.

The initial idea was to use the manyToOne that works fine will an empty list of sources, but attaching an alternative v2 that adds a simple EmptyIterator for the empty case.",27/Sep/11 12:49;jbellis;+1,"27/Sep/11 12:50;jbellis;TBH worrying about the empty case is probably premature optimization, so I'm fine w/ committing either one.","27/Sep/11 13:16;slebresne;I agree this is premature optimization, I attached the second one as an alternative to consider. Committed the first one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] fix bad comparison in hadoop cf recorder reader,CASSANDRA-3788,12539857,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,26/Jan/12 02:53,12/Mar/19 14:20,13/Mar/19 22:26,26/Jan/12 07:22,1.1.0,,,,,,0,hadoop,,,,"code does

rows.get(0).columns.get(0).column.equals(startColumn)

which is a Column against a ByteBuffer

changed to 

rows.get(0).columns.get(0).column.name.equals(startColumn)",,,,,,,,,,,,,,,,26/Jan/12 02:53;dbrosius@apache.org;bad_column_name_compare.diff;https://issues.apache.org/jira/secure/attachment/12511937/bad_column_name_compare.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-26 07:22:49.458,,,no_permission,,,,,,,,,,,,225360,,,Thu Jan 26 07:22:49 UTC 2012,,,,,,0|i0go8f:,95356,jbellis,jbellis,,,,,,,,,"26/Jan/12 07:22;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] fix bad validator lookup (bad key type),CASSANDRA-3789,12539858,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,26/Jan/12 02:59,12/Mar/19 14:20,13/Mar/19 22:26,26/Jan/12 07:26,1.0.8,,,,,,0,,,,,"code looks up an entry in a map by a byte[] even tho the map is keyed by ByteBuffer, add a ByteBuffer.wrap call to the key.",,,,,,,,,,,,,,,,26/Jan/12 02:59;dbrosius@apache.org;bad_contains_check_for_cdname.diff;https://issues.apache.org/jira/secure/attachment/12511938/bad_contains_check_for_cdname.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-26 07:26:33.987,,,no_permission,,,,,,,,,,,,225361,,,Thu Jan 26 07:26:33 UTC 2012,,,,,,0|i0go93:,95359,jbellis,jbellis,,,,,,,,,26/Jan/12 07:26;jbellis;committed (this one to 1.0.8 too),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] fix bad comparison of IColumn to ByteBuffer,CASSANDRA-3786,12539854,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,26/Jan/12 02:36,12/Mar/19 14:20,13/Mar/19 22:26,26/Jan/12 07:11,1.1.0,,,,,,0,,,,,"Code does

firstColumn.equals(startKey)

changed to 

firstColumn.name().equals(startKey)",,,,,,,,,,,,,,,,26/Jan/12 02:37;dbrosius@apache.org;bad_compare.diff;https://issues.apache.org/jira/secure/attachment/12511934/bad_compare.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-26 07:09:28.295,,,no_permission,,,,,,,,,,,,225357,,,Thu Jan 26 07:11:27 UTC 2012,,,,,,0|i0go7j:,95352,jbellis,jbellis,,,,,,,,,"26/Jan/12 07:09;jbellis;This is tagged affects 1.0.7 but patch only applies to trunk for me.  Do we need a different patch for 1.0 branch, or should this be affects-1.1?","26/Jan/12 07:11;jbellis;looks to me like the latter -- will commit on that basis, please reopen if I'm wrong",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageService.setMode() is used inconsistently,CASSANDRA-3388,12527840,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,thobbs,thobbs,19/Oct/11 22:45,12/Mar/19 14:20,13/Mar/19 22:26,28/Oct/11 17:23,1.0.2,,,,,,0,,,,,"{{StorageService.setMode()}}, which ends up setting the OperationMode attribute of the related mbean, is used inconsistently.  In most places, it's used like ""{{setMode(""MODE: details"")}}, but in a few places, it's used more like a normal log message.

To make this attribute more usable through JMX, {{setMode()}} should have a signature like {{setMode(mode, details)}}, where the mode parameter could be an enum (or even just a string, the main thing is just being consistent).  The OperationMode JMX attribute should definitely remain a string, though.",,,,,,,,,,,,,,,,25/Oct/11 16:17;slebresne;3388.patch;https://issues.apache.org/jira/secure/attachment/12500707/3388.patch,25/Oct/11 17:27;slebresne;3388_v2.patch;https://issues.apache.org/jira/secure/attachment/12500725/3388_v2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-19 22:50:40.016,,,no_permission,,,,,,,,,,,,92099,,,Fri Oct 28 17:23:57 UTC 2011,,,,,,0|i0gjaf:,94555,thobbs,thobbs,,,,,,,,,"19/Oct/11 22:50;jbellis;We're talking about this, right?

{code}
    private void setMode(String m, boolean log)
{code}

Why would we even expose this over JMX?  It's not intended to be public.","19/Oct/11 23:26;thobbs;Yes, that's the method.  Sorry, I forgot that there was also a ""log"" parameter.

It's a fairly reliable way to figure out if a node is joining, moving, decommissioning, or has finished decommissioning.  I'm open to alternative ways of discovering this reliably.  I assumed that's why this was exposed through JMX in the first place.","20/Oct/11 01:43;jbellis;I don't see setMode or getMode in StorageServiceMBean.  I'm lost, how is it exposed through JMX?",20/Oct/11 04:08;thobbs;It's named getOperationMode().,"20/Oct/11 04:25;jbellis;Ah, okay.

setMode isn't exposed though, which is as it should be since it's informational only.  Setting it wouldn't actually change any internal state to match.","20/Oct/11 04:33;thobbs;Right, I wasn't suggesting that setMode() be exposed externally at all.  I was merely saying that the way it's used _internally_ is inconsistent.  (And since the attribute is readable through JMX, being consistent matters.)","20/Oct/11 16:31;jbellis;Ah, I get it now.  I was confused by the description of ""making this attribute more usable through JMX.""","25/Oct/11 16:41;thobbs;A couple of comments on v1 of the patch:
* I think you accidentally lower-cased 'Leaving' on one line
* Everything is done consistently now, but I'm a little bit concerned about this being accidentally broken in the future.  Would you mind either splitting the mode (currently just the string prefix) into a separate parameter or at least documenting the setMode() method with a note about how it should be used?

Thanks, Sylvain.","25/Oct/11 17:27;slebresne;Yeah right. I was going with the 'smallest diff' approach but I agree we can make that cleaner. v2 does so by adding a Mode enumeration. A few details (that I think are ok but are worth mentioning):
* the patch changes the semantic of the getOperationMode JMX attribute to only return the ""mode"", not the message part. It can be changed back but I'm not sure the message is really useful outside of the log itself and it feels slightly cleaner that way.
* the modes are in caps. Again, feels more simple, natural but that can change.
* I've changed the last ""mode"" from Draining to Drained. Feels more useful/right that way.
* ","25/Oct/11 18:52;thobbs;Minor note: {{setMode(Mode.DECOMMISSIONED, null, true);}} could omit the {{null}} parameter.

Other than that, it looks perfect to me, and I agree with your thoughts.  Thanks!

+1",28/Oct/11 17:23;slebresne;Committed (with suggested minor fix),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add an escapeSQLString function and fix unescapeSQLString,CASSANDRA-2492,12504465,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,17/Apr/11 02:37,12/Mar/19 14:20,13/Mar/19 22:26,19/Apr/11 13:42,0.7.5,,,,,,0,,,,,"CliUtils.unescapeSqlString repeats the escape character e.g. 
{noformat}""my \\t tab"" becomes ""my \tt""{noformat}
because {{i}} is not bumped when an escape is processed.
 
Also for Cassandra-2221 I need a function to escape strings back so they will work if processed by the cli again. 

There are a number of non [standard escapes|http://java.sun.com/docs/books/jls/second_edition/html/lexical.doc.html#101089] which I assume is a hang over from is original source https://github.com/apache/cassandra/blob/1aeca2b6257b0ad6680080b1756edf7ee9acf8c8/src/java/org/apache/cassandra/cli/CliUtils.java

Will change to use the [StringEscapeUtils|http://commons.apache.org/lang/api-2.5/org/apache/commons/lang/StringEscapeUtils.html] class  ",,,,,,,,,,,,,,,,19/Apr/11 08:24;amorton;0001-use-StringEscapeUtils-to-escape-and-unescape.patch;https://issues.apache.org/jira/secure/attachment/12476699/0001-use-StringEscapeUtils-to-escape-and-unescape.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-19 13:42:27.045,,,no_permission,,,,,,,,,,,,20652,,,Tue Apr 19 13:42:27 UTC 2011,,,,,,0|i0gbon:,93323,jbellis,jbellis,,,,,,,,,"19/Apr/11 08:24;amorton;Attached patch to use StringEscapeUtils to escape and unescape cli strings, includes unit test.",19/Apr/11 13:42;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
If node fails to join a ring it will stay in joining state indefinately,CASSANDRA-3351,12526780,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,tuke,tuke,12/Oct/11 00:34,12/Mar/19 14:20,13/Mar/19 22:26,21/Oct/11 22:26,0.8.8,1.0.1,,,,,0,gossip,,,,"While attempting to add a new node to my ring something went wrong and I had to terminate the node on ec2. After this the node keeps appearing in the ring command in ""joining"" state and never goes away. Per driftx on the Cassandra channel if I do a whole cluster restart it should go away, but since this is a production system this is not really possible. Additionally if I could join a node with same IP again this should go away, but being on ec2 this is not always easy. So not sure if this truly qualifies as a bug or more like a feature request, but I feel there should be a way to remove a node in any state if I wish without joining a node with same ip or doing a whole cluster restart.","xlarge ec2, Ubuntu 11.04 Natty, JNA",,,,,,,,,,,,,,,20/Oct/11 17:41;brandon.williams;3351-trunk.txt;https://issues.apache.org/jira/secure/attachment/12499899/3351-trunk.txt,20/Oct/11 17:37;brandon.williams;3351.txt;https://issues.apache.org/jira/secure/attachment/12499898/3351.txt,12/Oct/11 00:37;tuke;cassandra.log.gz;https://issues.apache.org/jira/secure/attachment/12498689/cassandra.log.gz,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-10-20 17:37:13.16,,,no_permission,,,,,,,,,,,,72542,,,Sat Oct 22 01:00:20 UTC 2011,,,,,,0|i0gitz:,94481,thepaul,thepaul,,,,,,,,,12/Oct/11 00:37;tuke;Also per driftx I ran gms trace and grabbed about 1 minute worth of log entries that I have attached here. Problem node ip is 10.82.211.8,"20/Oct/11 17:37;brandon.williams;Two problems introduced in CASSANDRA-2496: the fatclient logic was changed a bit too much, and the isDeadState check was assuming fat clients had dead state, thus calling setHasToken to effectively not mark  them as fat clients.",21/Oct/11 22:13;thepaul;+1,21/Oct/11 22:26;brandon.williams;Committed.,"22/Oct/11 01:00;hudson;Integrated in Cassandra-0.8 #386 (See [https://builds.apache.org/job/Cassandra-0.8/386/])
    Prevent nodes that failed to join from being stuck in the joining state
indefinitely.
Patch by brandonwilliams, reviewed by Paul Cannon for CASSANDRA-3351

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1187578
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CsDef instead of CfDef in system_add_keyspace() function,CASSANDRA-3296,12525512,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,alexiswilke,alexiswilke,03/Oct/11 06:06,12/Mar/19 14:20,13/Mar/19 22:26,03/Oct/11 08:07,0.8.7,,,,,,0,,,,,"throw new InvalidRequestException(""CsDef ("" + cf.getName() +"") had a keyspace definition that did not match KsDef"");

The string starts with ""CsDef ("" when it should be ""CfDef ("".","In this file:
apache-cassandra-0.8.6-src/src/java/org/apache/cassandra/thrift/CassandraServer.java
Around line #893",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-10-03 08:07:43.626,,,no_permission,,,,,,,,,,,,43526,,,Mon Oct 03 08:07:43 UTC 2011,,,,,,0|i0gi6n:,94376,,,,,,,,,,,"03/Oct/11 08:07;slebresne;fixed in r1178325, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli has backwards index status message,CASSANDRA-2853,12512678,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stinkymatt,stinkymatt,stinkymatt,03/Jul/11 20:25,12/Mar/19 14:20,13/Mar/19 22:26,04/Jul/11 01:54,0.8.2,,,,,,0,,,,,"When a secondary index is building, the total bytes and processed bytes are swapped in the message.  Example:
Currently building index cf1, completed 12052040551 of 18047343 bytes.

The problem is a call to CompactionInfo constructor with swapped parameters.  Patch to follow.",,,,,,,,,,,,,,,,03/Jul/11 20:27;stinkymatt;fix_idx_msg.patch;https://issues.apache.org/jira/secure/attachment/12485106/fix_idx_msg.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-04 01:54:30.705,,,no_permission,,,,,,,,,,,,20867,,,Mon Jul 04 02:03:43 UTC 2011,,,,,,0|i0gduv:,93675,jbellis,jbellis,,,,,,,,,"04/Jul/11 01:54;jbellis;committed, thanks!","04/Jul/11 02:03;hudson;Integrated in Cassandra-0.8 #202 (See [https://builds.apache.org/job/Cassandra-0.8/202/])
    fix index-building status display
patch by Matt Kennedy; reviewed by jbellis for CASSANDRA-2853

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1142530
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilySerializer.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/SSTableIdentityIterator.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/Table.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/PendingFile.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/StreamInSession.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/IndexHelper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/StreamOut.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't allow {LOCAL|EACH}_QUORUM unless strategy is NTS,CASSANDRA-2627,12506574,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,09/May/11 08:48,12/Mar/19 14:20,13/Mar/19 22:26,09/May/11 15:13,0.7.6,0.8.0,,Legacy/CQL,,,0,,,,,"There is not check when {LOCAL|EACH}_QUORUM is used than we do use NTS, hence using such CL with simpleStrategy for instance result in
{noformat}
ERROR [pool-1-thread-1] 2011-05-09 10:44:29,728 Cassandra.java (line 2960) Internal error processing insert
java.lang.ClassCastException: org.apache.cassandra.locator.SimpleStrategy cannot be cast to org.apache.cassandra.locator.NetworkTopologyStrategy
...
{noformat}",,,,,,,,,,,,,,,,09/May/11 10:26;slebresne;0001-validate-CL-compatible-with-strategy.patch;https://issues.apache.org/jira/secure/attachment/12478582/0001-validate-CL-compatible-with-strategy.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-09 13:21:56.361,,,no_permission,,,,,,,,,,,,20739,,,Mon May 09 15:13:13 UTC 2011,,,,,,0|i0gchz:,93455,,,,,,,,,,,09/May/11 10:26;slebresne;Patch against 0.7,09/May/11 13:21;jbellis;+1 (it's possible to make these work w/ non-NTS but it's more work than it's worth),"09/May/11 13:44;hudson;Integrated in Cassandra-0.7 #475 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/475/])
    Don't allow {LOCAL|EACH}_QUORUM unless strategy is NTS
patch by slebresne; reviewed by jbellis for CASSANDRA-2627
",09/May/11 15:13;slebresne;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot read counter value from jdbc cql,CASSANDRA-3268,12525073,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,coreyhulen,coreyhulen,coreyhulen,28/Sep/11 16:02,12/Mar/19 14:20,13/Mar/19 22:26,29/Sep/11 01:02,1.0.0,,,Legacy/CQL,,,0,cql,,,,"it appears on line #36 in src/java/org/apache/cassandra/cql/jdbc/TypesMap.java  (notice it's in the portion of code that sits in the main src dir not the drivers)

map.put(""org.apache.cassandra.db.marshal.ColumnCounterType"", JdbcCounterColumn.instance);

should be 

map.put(""org.apache.cassandra.db.marshal.CounterColumnType"", JdbcCounterColumn.instance);

Notice CounterColumnType is reversed.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-09-29 01:02:35.382,,,no_permission,,,,,,,,,,,,36757,,,Thu Sep 29 01:02:35 UTC 2011,,,,,,0|i0ghun:,94322,jbellis,jbellis,,,,,,,,,"29/Sep/11 01:02;jbellis;fixed in r1177137, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bloom filter true positives not counted unless key cache is enabled,CASSANDRA-2637,12506865,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,11/May/11 15:42,12/Mar/19 14:20,13/Mar/19 22:26,11/May/11 16:07,0.7.6,0.8.1,,,,,0,,,,,,,,,,,,,,,,,,,,,11/May/11 15:43;jbellis;2637.txt;https://issues.apache.org/jira/secure/attachment/12478833/2637.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-11 15:49:38.177,,,no_permission,,,,,,,,,,,,20744,,,Wed May 11 19:44:04 UTC 2011,,,,,,0|i0gcjz:,93464,slebresne,slebresne,,,,,,,,,11/May/11 15:49;slebresne;+1,"11/May/11 19:44;hudson;Integrated in Cassandra-0.7 #480 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/480/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NPE in getRangeToRpcaddressMap,CASSANDRA-2996,12518032,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,05/Aug/11 12:49,12/Mar/19 14:20,13/Mar/19 22:26,09/Aug/11 16:56,0.8.4,,,,,,0,,,,,"DatabaseDescriptor.getRpcAddress() can be null, which getRangeToRpcaddressMap doesn't take into account",,,,,,,,,,,,,,,,05/Aug/11 13:17;slebresne;2996-v2.patch;https://issues.apache.org/jira/secure/attachment/12489472/2996-v2.patch,05/Aug/11 12:49;slebresne;2996.patch;https://issues.apache.org/jira/secure/attachment/12489469/2996.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-05 12:58:25.591,,,no_permission,,,,,,,,,,,,20930,,,Fri Aug 05 16:33:16 UTC 2011,,,,,,0|i0geqn:,93818,jbellis,jbellis,,,,,,,,,05/Aug/11 12:58;jbellis;Should we fix by making getRA return localAddress instead of null?,"05/Aug/11 13:17;slebresne;Yeah, I admit I went for lazy no that one, thinking that maybe there was a reason for having getRA returning null. But apparently there is none. Attaching v2 that sets rpcAddress to the right value upfront.",05/Aug/11 13:45;jbellis;+1,"05/Aug/11 16:33;hudson;Integrated in Cassandra-0.8 #257 (See [https://builds.apache.org/job/Cassandra-0.8/257/])
    Fix NPE in getRangeToRpcaddressMap
patch by slebresne; reviewed by jbellis for CASSANDRA-2996

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1154219
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AbstractCassandraDaemon.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java indexed range slicing is broken,CASSANDRA-2326,12501404,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,14/Mar/11 21:26,12/Mar/19 14:20,13/Mar/19 22:26,13/Apr/11 20:37,0.7.5,0.8 beta 1,,,,,0,,,,,"I probably broke it when I fixed the build that CASSANDRA-2312 broke.  Now it compiles, but never works.",,,,,,,,,,,,,,,,12/Apr/11 18:23;xedin;CASSANDRA-2326-trunk.patch;https://issues.apache.org/jira/secure/attachment/12476147/CASSANDRA-2326-trunk.patch,12/Apr/11 18:23;xedin;CASSANDRA-2326.patch;https://issues.apache.org/jira/secure/attachment/12476146/CASSANDRA-2326.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-14 22:11:46.802,,,no_permission,,,,,,,,,,,,20563,,,Tue Apr 19 00:04:18 UTC 2011,,,,,,0|i0gapz:,93167,brandon.williams,brandon.williams,,,,,,,,,14/Mar/11 22:11;jbellis;do you get back incomplete data or no data at all?,"14/Mar/11 22:15;brandon.williams;{noformat}
contrib/stress/bin/stress -n 300000 -d cassandra-1,cassandra-2,cassandra-3 -i 1 -t 300 -x KEYS -l2 -o INDEXED_RANGE_SLICE
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Operation [77] retried 10 times - error on calling get_indexed_slices for offset 0 
{noformat}

Not much further with keep-going:

{noformat}
contrib/stress/bin/stress -n 300000 -d cassandra-1,cassandra-2,cassandra-3 -i 1 -t 300 -x KEYS -l2 -o INDEXED_RANGE_SLICE -k
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Operation [99] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [178] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [173] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [54] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [190] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [35] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [9] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [104] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [139] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [195] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
{noformat}",14/Mar/11 22:21;jbellis;looks like stress.java could stand to print out InvalidRequestException.why,"27/Mar/11 21:29;xedin;It does print an exception message when it's not null. 

It seems to be imposible to currently implement IndexedRangeSlice operation because Operation.generateValues() now generates strings using randomizer (IndexedRangeSlice implies that we know an exact value we are looking keys for) so each of the runs can potentially give 0 results and we can loop infinitely...","11/Apr/11 10:45;xedin;We can maybe offer users to provide list of the values for indexes range slices, don't have any other solution right now...","12/Apr/11 15:49;jbellis;a --values [list of values] option sounds like a good solution to me.  doesn't have to be specific to index queries, but it's most useful there obviously.","12/Apr/11 15:52;xedin;I agree, I was also thinking about flag which will use old values generator instead of random as I possible solution.","12/Apr/11 16:07;brandon.williams;I like the idea of adding a flag for the old behavior, that Just Worked and didn't require more command line mess.  I'd even be onboard with making it the default and having random as an option, since random hasn't bought us much, if anything, yet.","12/Apr/11 16:09;xedin;I agree with Brandon on this, what do you think Jonathan?",12/Apr/11 16:12;jbellis;sgtm,"12/Apr/11 18:23;xedin;-V to generate randomized average size values, old behaviour by default",13/Apr/11 20:37;brandon.williams;Committed,19/Apr/11 00:04;jbellis;(rebased + committed trunk version),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GC can take 0 ms,CASSANDRA-3656,12535960,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,21/Dec/11 04:41,12/Mar/19 14:20,13/Mar/19 22:26,22/Dec/11 20:43,0.8.10,1.0.7,,,,,0,,,,,,,,,,,,,,,,,,,,,22/Dec/11 20:11;jbellis;3656.txt;https://issues.apache.org/jira/secure/attachment/12508431/3656.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-22 20:20:08.862,,,no_permission,,,,,,,,,,,,221643,,,Fri Dec 23 01:04:50 UTC 2011,,,,,,0|i0gmn3:,95098,brandon.williams,brandon.williams,,,,,,,,,"21/Dec/11 04:42;jbellis;As reported by Michael Vaknine on the mailing list,

{noformat}
STG-Cass4 ERROR [ScheduledTasks:1] 2011-12-12 22:55:26,554 java.lang.AssertionError
STG-Cass4 ERROR [ScheduledTasks:1] 2011-12-12 22:55:26,554 at org.apache.cassandra.service.GCInspector.logGCResults(GCInspector.java:103)
STG-Cass4 ERROR [ScheduledTasks:1] 2011-12-12 22:55:26,554 at org.apache.cassandra.service.GCInspector.access$000(GCInspector.java:41)
STG-Cass4 ERROR [ScheduledTasks:1] 2011-12-12 22:55:26,554 at org.apache.cassandra.service.GCInspector$1.run(GCInspector.java:85)
{noformat}",21/Dec/11 04:42;jbellis;(Michael's stacktrace is against 1.0.3.),22/Dec/11 20:20;brandon.williams;+1,22/Dec/11 20:43;jbellis;committed,"23/Dec/11 01:04;hudson;Integrated in Cassandra-0.8 #423 (See [https://builds.apache.org/job/Cassandra-0.8/423/])
    avoid logging (harmless) exception when GC takes < 1ms
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3656

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1222440
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/GCInspector.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] add missing break in nodecmd's command dispatching for SETSTREAMTHROUGHPUT,CASSANDRA-3824,12540681,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,01/Feb/12 06:05,12/Mar/19 14:20,13/Mar/19 22:26,01/Feb/12 07:47,1.1.0,,,Legacy/Tools,,,0,,,,,code falls thru SETSTREAMTHROUGHPUT into REBUILD case.,,,,,,,,,,,,,,,,01/Feb/12 06:06;dbrosius@apache.org;add_missing_break.diff;https://issues.apache.org/jira/secure/attachment/12512721/add_missing_break.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-01 07:47:31.162,,,no_permission,,,,,,,,,,,,226076,,,Wed Feb 01 07:47:31 UTC 2012,,,,,,0|i0gonj:,95424,slebresne,slebresne,,,,,,,,,"01/Feb/12 07:47;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[patch] use long math, if you want long results",CASSANDRA-3364,12527307,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,dbrosius@apache.org,dbrosius@apache.org,15/Oct/11 17:31,12/Mar/19 14:20,13/Mar/19 22:26,17/Oct/11 17:19,1.0.1,,,,,,0,,,,,"Code calculates long values, using integer intermediate input, which can cause truncation errors, safer just to use long input.",,,,,,,,,,,,,,,,15/Oct/11 17:31;dbrosius@apache.org;use_long_math.diff;https://issues.apache.org/jira/secure/attachment/12499153/use_long_math.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-17 17:19:26.688,,,no_permission,,,,,,,,,,,,86714,,,Mon Oct 17 17:19:26 UTC 2011,,,,,,0|i0gizb:,94505,brandon.williams,brandon.williams,,,,,,,,,"17/Oct/11 17:19;brandon.williams;Committed, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json on an index sstable failed with NPE,CASSANDRA-3059,12519354,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,cywjackson,cywjackson,19/Aug/11 00:15,12/Mar/19 14:04,13/Mar/19 22:26,24/Aug/11 14:54,0.8.5,,,Legacy/Tools,,,0,,,,,"$ ./bin/sstable2json /var/lib/cassandra-trunk/data/Keyspace1/Standard1.Idx1-h-1-Data.db 
{
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:74)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:69)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:64)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:147)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:87)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:71)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:177)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:142)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:134)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:304)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:335)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:348)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:406)


cfm is null for Index CF?",,,,,,,,,,,,,,,,19/Aug/11 02:37;jbellis;3059.txt;https://issues.apache.org/jira/secure/attachment/12490909/3059.txt,24/Aug/11 13:06;tjake;3059_v2.txt;https://issues.apache.org/jira/secure/attachment/12491477/3059_v2.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-19 02:37:02.206,,,no_permission,,,,,,,,,,,,19337,,,Wed Aug 24 14:54:53 UTC 2011,,,,,,0|i0ei3z:,82696,tjake,tjake,,,,,,,,,19/Aug/11 02:37;jbellis;patch against 0.8,24/Aug/11 13:06;tjake;v2 fixes off by one bug in substring. otherwise +1,24/Aug/11 14:54;jbellis;committed v2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
secondary index on a column that has a value of size > 64k will fail on flush,CASSANDRA-3057,12519349,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,cywjackson,cywjackson,18/Aug/11 22:02,12/Mar/19 14:04,13/Mar/19 22:26,19/Aug/11 01:54,0.8.5,,,Feature/2i Index,,,0,,,,,"exception seen on flush when an indexed column contain size > 64k:

granted that having a value > 64k possibly mean something that shouldn't be indexed as it most likely would have a high cardinality, but i think there would still be some valid use case for it.

test case:
simply run the stress test with 
-n 1 -u 0 -c 2  -y Standard  -o INSERT  -S 65536 -x KEYS

then call a flush

exception:
 INFO [FlushWriter:8] 2011-08-18 21:49:33,214 Memtable.java (line 218) Writing Memtable-Standard1.Idx1@1652462853(16/20 serialized/live bytes, 1 ops)
Standard1@980087547(196659/245823 serialized/live bytes, 3 ops)
ERROR [FlushWriter:8] 2011-08-18 21:49:33,230 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[FlushWriter:8,5,RMI Runtime]
java.lang.AssertionError: 65536
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithShortLength(ByteBufferUtil.java:330)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:245)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:49)
        at org.apache.cassandra.db.Memtable$3.runMayThrow(Memtable.java:270)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

",,,,,,,,,,,,,,,,19/Aug/11 01:17;jbellis;3057-v3.txt;https://issues.apache.org/jira/secure/attachment/12490904/3057-v3.txt,19/Aug/11 01:03;xedin;CASSANDRA-3057-v2.patch;https://issues.apache.org/jira/secure/attachment/12490903/CASSANDRA-3057-v2.patch,18/Aug/11 23:07;xedin;CASSANDRA-3057.patch;https://issues.apache.org/jira/secure/attachment/12490889/CASSANDRA-3057.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-08-18 22:05:04.95,,,no_permission,,,,,,,,,,,,20953,,,Fri Aug 19 02:25:19 UTC 2011,,,,,,0|i0gf6n:,93890,jbellis,jbellis,,,,,,,,,"18/Aug/11 22:05;jbellis;Let's add a ThriftValidation check that values inserted into an indexed column are less than the ""short"" length.","18/Aug/11 22:12;mdennis;+1 on validation approach (rejecting indexed values > max column name size) + useful error message ""can't index column value of size A for index B in CF C of KS D"" instead of ""assertion spew""

for durability of previous conversations: the problem is the value in a column ends up as the name in a the index column family.  So, if you try to index a value that is greater than the max column name size it is not handled well.","18/Aug/11 22:15;jbellis;(technically, the value becomes a row key, which happens to have the same size limit as column names.)","19/Aug/11 00:53;jbellis;We're already looping through ColumnDefinitions for getValueValidator, let's extract a getColumnDefinition from that so we only do the search once.","19/Aug/11 01:03;xedin;Sorry, I missed metadata.getColumnDefinition(ByteBuffer) method that is why I did a loop, fixed now.",19/Aug/11 01:17;jbellis;v3 attached to only do the definition lookup once.,19/Aug/11 01:24;xedin;Misunderstood what you meant... +1,19/Aug/11 01:54;jbellis;committed,"19/Aug/11 02:25;hudson;Integrated in Cassandra-0.8 #285 (See [https://builds.apache.org/job/Cassandra-0.8/285/])
    return an InvalidRequestException if an indexed column is assigned a value larger than 64K
patch by pyaskevich and jbellis for CASSANDRA-3057

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1159473
Files : 
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in MessagingService.java:420,CASSANDRA-3007,12518303,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,vilda,vilda,09/Aug/11 10:22,12/Mar/19 14:04,13/Mar/19 22:26,11/Aug/11 19:16,0.8.5,,,,,,0,nullpointerexception,streaming,,,"I'm getting large quantity of exceptions during streaming. It is always in MessagingService.java:420. The streaming appears to be blocked.

 INFO 10:11:14,734 Streaming to /10.235.77.27
ERROR 10:11:14,734 Fatal exception in thread Thread[StreamStage:2,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.MessagingService.stream(MessagingService.java:420)
        at org.apache.cassandra.streaming.StreamOutSession.begin(StreamOutSession.java:176)
        at org.apache.cassandra.streaming.StreamOut.transferRangesForRequest(StreamOut.java:148)
        at org.apache.cassandra.streaming.StreamRequestVerbHandler.doVerb(StreamRequestVerbHandler.java:54)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
","Linux w0 2.6.35-24-virtual #42-Ubuntu SMP Thu Dec 2 05:15:26 UTC 2010 x86_64 GNU/Linux
java version ""1.6.0_18""
OpenJDK Runtime Environment (IcedTea6 1.8.7) (6b18-1.8.7-2~squeeze1)
OpenJDK 64-Bit Server VM (build 14.0-b16, mixed mode)",,,,,,,,,,,,,,,09/Aug/11 13:03;jbellis;3007.txt;https://issues.apache.org/jira/secure/attachment/12489840/3007.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-09 13:00:27.041,,,no_permission,,,,,,,,,,,,19191,,,Thu Aug 11 20:21:40 UTC 2011,,,,,,0|i0get3:,93829,brandon.williams,brandon.williams,,,,,,,,,09/Aug/11 13:00;jbellis;What kind of streaming are you attempting?  ,"09/Aug/11 13:03;jbellis;Never mind, not relevant.  Looks like you upgraded from 0.7 without updating your configuration file?

Fix for missing encryption_options attached.","09/Aug/11 15:02;vilda;It's removetoken command.

Yes, I updated the node and forgot to specify encryption_options - thanks!",11/Aug/11 18:18;brandon.williams;+1,11/Aug/11 19:16;jbellis;committed,"11/Aug/11 20:21;hudson;Integrated in Cassandra-0.8 #272 (See [https://builds.apache.org/job/Cassandra-0.8/272/])
    fix NPE when encryption_options is unspecified
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3007

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156749
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should check for errors when calling /bin/ln,CASSANDRA-3101,12520615,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,thepaul,thepaul,29/Aug/11 18:39,12/Mar/19 14:04,13/Mar/19 22:26,14/Dec/11 21:44,1.0.7,,,,,,0,lhf,,,,"It looks like cassandra.utils.CLibrary.createHardLinkWithExec() does not check for any errors in the execution of the hard-link-making utility. This could be bad if, for example, the user has put the snapshot directory on a different filesystem from the data directory. The hard linking would fail and the sstable snapshots would not exist, but no error would be reported.

It does look like errors with the more direct JNA link() call are handled correctly- an exception is thrown. The WithExec version should probably do the same thing.

Definitely it would be enough to check the process exit value from /bin/ln for nonzero in the *nix case, but I don't know whether 'fsutil hardlink create' or 'cmd /c mklink /H' return nonzero on failure.

For bonus points, use any output from the Process's error stream in the text of the exception, to aid in debugging problems.",,,,,,,,,,,,,,,,12/Dec/11 18:54;vijay2win@yahoo.com;0001-0001-throw-IOE-while-calling-bin-ln-v2.patch;https://issues.apache.org/jira/secure/attachment/12507031/0001-0001-throw-IOE-while-calling-bin-ln-v2.patch,14/Dec/11 17:11;vijay2win@yahoo.com;0001-3101-throw-IOE-while-calling-bin-ln-v3.patch;https://issues.apache.org/jira/secure/attachment/12507379/0001-3101-throw-IOE-while-calling-bin-ln-v3.patch,31/Oct/11 21:50;vijay2win@yahoo.com;0001-3101-throw-IOE-while-calling-bin-ln.patch;https://issues.apache.org/jira/secure/attachment/12501687/0001-3101-throw-IOE-while-calling-bin-ln.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-10-31 21:50:50.741,,,no_permission,,,,,,,,,,,,1876,,,Thu Jan 12 22:30:36 UTC 2012,,,,,,0|i0gfpz:,93977,thepaul,thepaul,,,,,,,,,"31/Oct/11 21:50;vijay2win@yahoo.com;To test: 
link snapshots to a different device
remove jni lib
run nt snapshot","09/Dec/11 23:24;thepaul;The following imports appear to be unused, and should probably be removed:

{code}
import java.io.BufferedInputStream;
import java.io.DataInputStream;
import java.io.InputStream;
import sun.misc.IOUtils;
{code}

Also, why add the logger.error() call in createHardLink, but not in createHardLinkWithExec? seems like if we want to know about the error in the C* log in the first case, we'd also want to know about it in the second case.

As a less important point, I wonder if it isn't worth modifying the existing hard-linking code a bit: ""cmd /c mklink /H"" seems a much more clunky way to make a hard link, and I think the logic is backwards: ""fsutil hardlink create"" is supported in all Windows versions since Windows Server 2000, except XP. Maybe the osversion comparison was meant to use cmd /c mklink /H in the ""Windows XP or less"" case, not ""Windows Vista or later"".

Tested on Windows 7, though, after mounting a USB drive at the ""snapshots"" dir and trying to snapshot that keyspace. Error showed up correctly with output to nodetool, although no errors showed in the c* log.","11/Dec/11 17:06;vijay2win@yahoo.com;Hi Paul, i dont have windows machine where i can test the command on.... Do you mind giving me the command so i can just add it? I will fix the remaining...","12/Dec/11 16:41;thepaul;Vijay- I meant that the ""fsutil hardlink create"" command (which is already in the code) looks like the right one to use in all cases except Windows XP, but since we don't have a good way to verify, we might as well leave that part alone.

Let's just get rid of the unused imports and add the extra logger.error() call.","12/Dec/11 18:54;vijay2win@yahoo.com;Hi Paul, attached has the fix. Thanks!","13/Dec/11 18:50;thepaul;This works, except you've taken out a logger.error() call instead of adding another one. I think it's worth logging an error for the cassandra log in both cases.","14/Dec/11 17:11;vijay2win@yahoo.com;I miss read the previous message hence removed it sorry, plz find the updated. thanks!","14/Dec/11 18:25;thepaul;awesome.

+1",14/Dec/11 21:44;xedin;Committed.,"12/Jan/12 22:19;jbellis;bq. ""cmd /c mklink /H"" seems a much more clunky way to make a hard link, and I think the logic is backwards: ""fsutil hardlink create"" is supported in all Windows versions since Windows Server 2000, except XP

mklink does not require admin privileges, so that's the preferred method, but it was introduced in Vista.

fsutil is present on XP+ so we use it as a fallback: http://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/fsutil.mspx?mfr=true

","12/Jan/12 22:30;thepaul;aha, thanks for clarification.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operation with CL=EACH_QUORUM doesn't succeed when a replica is down (RF=3),CASSANDRA-3082,12520268,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,patricioe,patricioe,patricioe,26/Aug/11 16:17,12/Mar/19 14:04,13/Mar/19 22:26,26/Aug/11 21:06,0.7.9,0.8.5,,,,,0,consistency,,,,"{code}  DatacenterSyncWriteResponseHandler#assureSufficientLiveNodes()
     ...
     ...
        // Throw exception if any of the DC doesn't have livenodes to accept write.
        for (String dc: strategy.getDatacenters())
        {
        	if (dcEndpoints.get(dc).get() != responses.get(dc).get())
                throw new UnavailableException();
        }
{code}

should be:
 
{code}
      if (dcEndpoints.get(dc).get() < responses.get(dc).get())
{code}",,,,,,,,,,,,,,,,26/Aug/11 19:17;patricioe;CASSANDRA-0.7-3084.txt;https://issues.apache.org/jira/secure/attachment/12491823/CASSANDRA-0.7-3084.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-26 21:06:45.482,,,no_permission,,,,,,,,,,,,20960,,,Fri Aug 26 21:21:23 UTC 2011,,,,,,0|i0gfgf:,93934,jbellis,jbellis,,,,,,,,,"26/Aug/11 19:18;patricioe;It passed a manual test using CCM with 6 nodes.

DC1:3
DC2:3

Originally, with all replicas up and running, the insert with CL=EACH_QUORUM failed.

The patch addresses that issue.",26/Aug/11 21:06;jbellis;committed,"26/Aug/11 21:21;hudson;Integrated in Cassandra-0.7 #545 (See [https://builds.apache.org/job/Cassandra-0.7/545/])
    fix UnavailableException with writes at CL.EACH_QUORM
patch by Patricio Echague; reviewed by jbellis for CASSANDRA-3082

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1162255
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/DatacenterSyncWriteResponseHandler.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra CLI unable to use list command with INTEGER column names, resulting in syntax error",CASSANDRA-3075,12520071,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,renatodasil,renatodasil,25/Aug/11 08:05,12/Mar/19 14:04,13/Mar/19 22:26,28/Aug/11 11:26,0.8.5,,,Legacy/Tools,,,0,features,newbie,,,"I have a Column Family named 1105115.

I have inserted the CF with Hector, and it did not
throw any exception concerning the name of the
column.

If I am issuing the command

list 1105115;

I incur the following error:

[default@unknown] list 1105115;
Syntax error at position 5: mismatched input '1105115' expecting Identifier

I presume we are not to name CFs as integers?

 Or is there something I am missing from
the bellow help content:

[default@unknown] help list;
list <cf>;
list <cf>[<startKey>:];
list <cf>[<startKey>:<endKey>];
list <cf>[<startKey>:<endKey>] limit <limit>;

List a range of rows, and all of their columns, in the specified column
family.

The order of rows returned is dependant on the Partitioner in use.

Required Parameters:
- cf: Name of the column family to list rows from.

Optional Parameters:
- endKey: Key to end the range at. The end key will be included
in the result. Defaults to an empty byte array.

- limit: Number of rows to return. Default is 100.

- startKey: Key start the range from. The start key will be
included in the result. Defaults to an empty byte array.

Examples:
list Standard1;
list Super1[j:];
list Standard1[j:k] limit 40;

================================================

Column Family Info:

    ColumnFamily: 1105115
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.AsciiType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.5203125/111/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
","64 Bit Ubuntu 11.04(full update), AMD64 + 8GB RAM + 500GB Hdd, Java 1.6.0_26, Cassandra 0.8.0 + 4GB heap, Cassandra CLI",,,,,,,,,,,,,,,27/Aug/11 23:15;xedin;CASSANDRA-3075.patch;https://issues.apache.org/jira/secure/attachment/12491979/CASSANDRA-3075.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-25 14:18:41.388,,,no_permission,,,,,,,,,,,,20958,,,Sun Aug 28 12:16:15 UTC 2011,,,,,,0|i0a4f3:,56989,jbellis,jbellis,,,,,,,,,25/Aug/11 08:11;renatodasil;Column Family name edit on the extra info... sorry,"25/Aug/11 08:23;renatodasil;Issue with Column Families containing INTIGER names, where integer named column families cannot be listed due to Syntax error, withing Cassandra-cli, Cassandra ver 0.8.0.",25/Aug/11 14:18;jbellis;One possible solution would be to allow quoting CF names.,"27/Aug/11 23:15;xedin;Made column family (and keyspace) support integer representation in all statements (create/update/set/get/list/drop/drop index/truncate/assume), only one limitation in here - when you use ""drop index on <cf>.<column>"" command if <cf> is numeric it should be put in quotes. Tests are updated to check if everything works correctly.

Rebased with cassandra-0.8 branch (last commit d9091c5aa88927ee2daa31ec69d865946b975fd9)",28/Aug/11 04:35;jbellis;+1,28/Aug/11 11:26;xedin;Committed.,"28/Aug/11 12:16;hudson;Integrated in Cassandra-0.8 #297 (See [https://builds.apache.org/job/Cassandra-0.8/297/])
    Fix parsing of the Keyspace and ColumnFamily names in numeric and string representations in CLI
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3075

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1162495
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/Cli.g
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliCompiler.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating a keyspace SYSTEM cause issue,CASSANDRA-3066,12519538,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,wajam,wajam,21/Aug/11 16:33,12/Mar/19 14:04,13/Mar/19 22:26,30/Aug/11 16:54,0.8.5,,,,,,0,,,,,"It's possible to create a keyspace SYSTEM but impossible to do anything with it after.

I know naming a keyspace SYSTEM is probably not a good idea but I was testing something on a test cluster and found this bug. Step to reproduce:

connect localhost/9160;
create keyspace SYSTEM;
use SYSTEM;
create column family test
with comparator = UTF8Type and subcomparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata = [{column_name: title, validation_class: UTF8Type},
    {column_name: publisher, validation_class: UTF8Type}];

And you get:

system keyspace is not user-modifiable

Although SYSTEM keyspace have been created and is a different keyspace as system.",Windows,,,,,,,,,,,,,,,22/Aug/11 17:43;jbellis;3066-v2.txt;https://issues.apache.org/jira/secure/attachment/12491232/3066-v2.txt,21/Aug/11 20:45;wajam;CASSANDRA-3066-0.8-v1.patch;https://issues.apache.org/jira/secure/attachment/12491110/CASSANDRA-3066-0.8-v1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-22 17:40:57.738,,,no_permission,,,,,,,,,,,,20955,,,Tue Aug 30 17:21:58 UTC 2011,,,,,,0|i0gf9z:,93905,xedin,xedin,,,,,,,,,"21/Aug/11 20:46;wajam;If what we want is that the keyspace name is case-sensitive and system <> SYSTEM, patch attached! Otherwise, I have no clue :)","22/Aug/11 17:40;jbellis;The real problem is allowing the create in the first place.  v2 attached.  From the comments: 

{code}
        // keyspace names must be unique case-insensitively because the keyspace name beomes the directory
        // where we store CF sstables.  Names that differ only in case would thus cause problems on
        // case-insensitive filesystems (NTFS, most installations of HFS+).
{code}

(Also renamed the ListAccess methods to SchemaAccess.)",30/Aug/11 16:54;xedin;Committed.,"30/Aug/11 17:21;hudson;Integrated in Cassandra-0.8 #304 (See [https://builds.apache.org/job/Cassandra-0.8/304/])
    Add validation that Keyspace names are case-insensitively unique
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3066

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163289
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/ClientState.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CustomTThreadPoolServer should log TTransportException at DEBUG level,CASSANDRA-3142,12521408,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jancona,jancona,jancona,06/Sep/11 02:10,12/Mar/19 14:04,13/Mar/19 22:26,07/Sep/11 13:42,0.8.6,,,,,,0,,,,,"Currently CustomTThreadPoolServer, like the Thrift TThreadPoolServer, silently ignores TTransportException in its run() method. This is appropriate in most cases because TTransportException occurs fairly often when client connections die. However TTransportException is also thrown when TFramedTransport encounters a frame that is larger than thrift_framed_transport_size_in_mb. In that case, silently exiting the run loop leads to a SocketException on the client side which can be both difficult to diagnose, in part because nothing is logged by Cassandra, and high-impact, because the client may respond by marking the server node down and retrying the too-large request on another node, where it also fails. This process repeated leads to the entire cluster being marked down (see https://github.com/rantav/hector/issues/212). I've filed two Thrift issues (https://issues.apache.org/jira/browse/THRIFT-1323 and https://issues.apache.org/jira/browse/THRIFT-1324), but in the meantime, I suggest that CustomTThreadPoolServer log the exception at DEBUG level in order to support easier troubleshooting.
",,,,,,,,,,,,,,,,06/Sep/11 17:43;jancona;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3142-Add-debug-level-logging-of-TTransportEx.txt;https://issues.apache.org/jira/secure/attachment/12493184/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3142-Add-debug-level-logging-of-TTransportEx.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-07 13:42:14.481,,,no_permission,,,,,,,,,,,,4068,,,Wed Sep 07 14:22:33 UTC 2011,,,,,,0|i0ggb3:,94072,xedin,xedin,,,,,,,,,07/Sep/11 13:42;xedin;Committed.,"07/Sep/11 14:22;hudson;Integrated in Cassandra-0.8 #318 (See [https://builds.apache.org/job/Cassandra-0.8/318/])
    CustomTThreadPoolServer to log TTransportException at DEBUG level
patch by Jim Ancona; reviewed by Pavel Yaskevich for CASSANDRA-3142

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166173
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CustomTThreadPoolServer.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cli syntax for creating keyspace is inconsistent in 1.0,CASSANDRA-3119,12521061,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,slebresne,slebresne,01/Sep/11 13:46,12/Mar/19 14:04,13/Mar/19 22:26,07/Sep/11 20:09,1.0.0,,,,,,0,cli,,,,"In 0.8, to create a keyspace you could do:
{noformat}
create keyspace test with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = [{replication_factor:3}]
{noformat}

In current trunk, if you try that, you get back ""null"". Turns out this is because the syntax for strategy_options has changed and you should not use the brackets, i.e:
{noformat}
strategy_options = {replication_factor:3}
{noformat}
(and note that reversely, this syntax doesn't work in 0.8).

I'm not sure what motivated that change but this is very user unfriendly. The help does correctly mention the new syntax, but it is the kind of changes that takes you 5 minutes to notice. It will also break people scripts for no good reason that I can see.

We should either:
# revert to the old syntax
# support both the new and old syntax
# at least print a meaningful error message when the old syntax is used

Imho, the last solution is by far the worst solution.
",,,,,,,,,,,,,,,,07/Sep/11 19:18;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3119-warn-on-old-cli-syntax.txt;https://issues.apache.org/jira/secure/attachment/12493364/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3119-warn-on-old-cli-syntax.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-01 13:54:52.009,,,no_permission,,,,,,,,,,,,4080,,,Wed Sep 07 21:23:30 UTC 2011,,,,,,0|i0gg0v:,94026,xedin,xedin,,,,,,,,,"01/Sep/11 13:54;jbellis;I'm in favor of 3., because the old syntax was broken -- we never actually supported a list there, which is what it implies.","01/Sep/11 13:55;jbellis;As mentioned on the other ticket, this should also be described in NEWS.","01/Sep/11 14:22;xedin;it seems like the best move here will be to support both [{}] and {} in there because it's imposible to say what caused that ""null"" because it fails somewhere deep in ANTLR (in most cases when recognition was failed ANTLR throws standard exceptions which we handle correctly as ""Command not found"" or ""Syntax error"").","01/Sep/11 16:57;urandom;bq. ... the old syntax was broken – we never actually supported a list there, which is what it implies.

I agree completely, but a user whose script has broken is still going to be irritated.

These things are like paper cuts, they aren't a huge deal when looked at in isolation, but they add up to a negative experience pretty quickly.","07/Sep/11 19:19;tjake;The patch accepts both new and old syntax, also warns the user when they use [{}]


",07/Sep/11 20:09;xedin;Committed.,"07/Sep/11 21:23;hudson;Integrated in Cassandra #1085 (See [https://builds.apache.org/job/Cassandra/1085/])
    Fix inconsistency of the CLI syntax when {} should be used instead of [{}]
patch by Jake Luciani; reviewed by Pavel Yaskevich for CASSANDRA-3119

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166367
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/trunk/test/unit/org/apache/cassandra/cli/CliTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PropertyFileSnitch's ResourceWatcher fails because it uses FBUtilities.resourceToFile(..) while PropertyFileSnitch uses classloader.getResourceAsStream(..),CASSANDRA-3138,12521372,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,michaelsembwever,michaelsembwever,michaelsembwever,05/Sep/11 16:41,12/Mar/19 14:04,13/Mar/19 22:26,05/Sep/11 19:59,0.8.6,,,,,,0,,,,,"Resource files are not necessarily plain files. They could be inside a jar file. See CASSANDRA-2036

This will cause {noformat}RROR 24:15,806 ResourceWatcher$WatchedResource: Timed run of class org.apache.cassandra.locator.PropertyFileSnitch$1 failed.
org.apache.cassandra.config.ConfigurationException: unable to locate cassandra-topology.properties
	at org.apache.cassandra.utils.FBUtilities.resourceToFile(FBUtilities.java:467)
	at org.apache.cassandra.utils.ResourceWatcher$WatchedResource.run(ResourceWatcher.java:57)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662){noformat}",,,,,,,,,,,,,,,,05/Sep/11 16:52;michaelsembwever;CASSANDRA-3138.patch;https://issues.apache.org/jira/secure/attachment/12493046/CASSANDRA-3138.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-05 19:59:56.513,,,no_permission,,,,,,,,,,,,4070,,,Fri Sep 13 08:01:51 UTC 2013,,,,,,0|i0gg93:,94063,jbellis,jbellis,,,,,,,,,05/Sep/11 16:52;michaelsembwever;Simple solution that prevents the ResourceWatcher for PropertyFileSnitch from being submitted if the property file is not a plain file on disk.,"05/Sep/11 19:59;jbellis;committed (w/ logged message at debug), thanks!","05/Sep/11 20:14;hudson;Integrated in Cassandra-0.8 #314 (See [https://builds.apache.org/job/Cassandra-0.8/314/])
    avoid trying to watch cassandra-topology.properties when loaded from jar
patch by Mck SembWever; reviewed by jbellis for CASSANDRA-3138

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1165405
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/PropertyFileSnitch.java
","13/Sep/13 08:01;sumit.thakur@rancoretech.com;Hello All,

Same issue in apache cassandra 1.1.5

ERROR [ScheduledTasks:1] 2013-09-12 17:34:55,268 ResourceWatcher.java (line 67) Timed run of class org.apache.cassandra.locator.PropertyFileSnitch$1 failed.
org.apache.cassandra.config.ConfigurationException: unable to locate cassandra-topology.properties
	at org.apache.cassandra.utils.FBUtilities.resourceToFile(FBUtilities.java:327)
	at org.apache.cassandra.utils.ResourceWatcher$WatchedResource.run(ResourceWatcher.java:57)
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IntervalTree could miscalculate its max,CASSANDRA-3145,12521505,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,06/Sep/11 22:02,12/Mar/19 14:04,13/Mar/19 22:26,07/Sep/11 18:28,1.0.0,,,,,,0,,,,,"The implementation of IntervalTree in trunk expects an ordered list of Interval objects as the argument to its constructor. It uses the ordering (only) to determine its minimum and maximum endpoints out of all Intervals stored in it. However, no ordering should be able to guarantee the first element has the set-wide minimum and that the last element has the set-wide maximum; you have to order by minima or maxima or some combination.

I propose that the requirement for ordered input to the IntervalTree constructor be dropped, seeing as how the elements will be sorted as necessary inside the IntervalNode object anyway. The set-wide minimum and maximum could be more straightforwardly calculated inside IntervalNode, and just exposed via IntervalTree.",,,,,,,,,,,,,,,,06/Sep/11 22:13;thepaul;3145.patch.txt;https://issues.apache.org/jira/secure/attachment/12493224/3145.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-07 18:21:45.69,,,no_permission,,,,,,,,,,,,4066,,,Wed Sep 07 19:02:11 UTC 2011,,,,,,0|i0ggcf:,94078,bcoverston,bcoverston,,,,,,,,,"06/Sep/11 22:13;thepaul;fixes IntervalTree.max calculation.

also eliminates a small amount of overhead for the use of a ConcurrentSkipListSet, since there isn't a need for the efficient contains/remove operations or the concurrency support.",07/Sep/11 18:21;bcoverston;+1 the patch looks good,07/Sep/11 18:28;jbellis;committed,"07/Sep/11 19:02;hudson;Integrated in Cassandra #1084 (See [https://builds.apache.org/job/Cassandra/1084/])
    fix IntervalTree max calculation
patch by Paul Cannon; reviewed by Ben Coverston for CASSANDRA-3145

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166302
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/DataTracker.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/IntervalTree/IntervalNode.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/IntervalTree/IntervalTree.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logic of AbstractNetworkTopologySnitch.compareEndpoints is wrong,CASSANDRA-3152,12521828,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,07/Sep/11 22:22,12/Mar/19 14:04,13/Mar/19 22:26,08/Sep/11 00:54,0.8.6,,,,,,0,,,,,"Current logic in ANTS.cE is to compare the rack and then compare the DC's, the problem is when we have the same rack name but the racks are in a diffrent DC's this logic breaks...

Example: 
""us-east,1a"", InetAddress.getByName(""127.0.0.1"")
""us-east,1b"", InetAddress.getByName(""127.0.0.2"")
""us-east,1c"", InetAddress.getByName(""127.0.0.3"")
""us-west,1a"", InetAddress.getByName(""127.0.0.4"")
""us-west,1b"", InetAddress.getByName(""127.0.0.5"")
""us-west,1c"", InetAddress.getByName(""127.0.0.6"")

Expected:
/127.0.0.1,/127.0.0.3,/127.0.0.2,/127.0.0.4,/127.0.0.5,/127.0.0.6

Current:
/127.0.0.1,/127.0.0.4,/127.0.0.3,/127.0.0.2,/127.0.0.5,/127.0.0.6",JVM,,,,,,,,,,,,,,,07/Sep/11 23:41;vijay2win@yahoo.com;0001-fix-dc-rack-sorting-on-ANTS.patch;https://issues.apache.org/jira/secure/attachment/12493529/0001-fix-dc-rack-sorting-on-ANTS.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-08 00:54:26.361,,,no_permission,,,,,,,,,,,,4061,,,Thu Sep 08 01:20:19 UTC 2011,,,,,,0|i0ggfj:,94092,jbellis,jbellis,,,,,,,,,07/Sep/11 23:41;vijay2win@yahoo.com;Tested and passed basically moved the DC comparison logic up in ANTS,08/Sep/11 00:54;jbellis;committed,"08/Sep/11 01:20;hudson;Integrated in Cassandra-0.8 #320 (See [https://builds.apache.org/job/Cassandra-0.8/320/])
    allow topology sort to work with non-unique rack names between datacenters
patch by Vijay; reviewed by jbellis for CASSANDRA-3152

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166484
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/AbstractNetworkTopologySnitch.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad equality check in ColumnFamilyStore.isCompleteSSTables(),CASSANDRA-3154,12521834,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tupshin,tupshin,tupshin,07/Sep/11 23:21,12/Mar/19 14:04,13/Mar/19 22:26,08/Sep/11 17:36,1.0.0,,,,,,0,,,,,The equality check in isCompleteSSTables() always fails because it tries to call equals() with a Set and a List. This might result in failure to purge tombstones in some cases.,,,,,,,,,,,,,,,,08/Sep/11 01:31;jbellis;3154.txt;https://issues.apache.org/jira/secure/attachment/12493534/3154.txt,07/Sep/11 23:23;tupshin;CASSANDRA-3154.diff;https://issues.apache.org/jira/secure/attachment/12493527/CASSANDRA-3154.diff,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-07 23:30:14.419,,,no_permission,,,,,,,,,,,,4059,,,Fri Sep 09 11:19:45 UTC 2011,,,,,,0|i0gggf:,94096,bcoverston,bcoverston,,,,,,,,,"07/Sep/11 23:30;bcoverston;The cardinality restriction of .isEqualCollection is probably more restrictive than we need, but this does indeed fix the existing shallow equality problem.

+1","08/Sep/11 01:31;jbellis;I'd rather get rid of that code.  It's not useful because

- for leveled compactions, you are effectively guaranteed that once you have more than a couple sstables, you'll never compact all sstables at once
- for non-leveled compactions, you have a small enough number of sstables that isKeyInRemainingSSTables is fine without adding additional optimization for the ""major"" case

This patch gets rid of isMajor, and additionally renames CompactionType to OperationType to better reflect the ""compaction"" stage's role as generic background IO manager.",08/Sep/11 03:11;tupshin;+1 to getting rid of the code instead.,"08/Sep/11 17:22;bcoverston;Patch is good.
+1 Getting rid of isMajor makes the logic behind the determination of the compaction operation much cleaner.",08/Sep/11 17:36;jbellis;committed,"08/Sep/11 18:23;hudson;Integrated in Cassandra #1091 (See [https://builds.apache.org/job/Cassandra/1091/])
    remove isMajor compaction designation
patch by jbellis; reviewed by Tupshin Harper and Ben Coverston for CASSANDRA-3154

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166822
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cache/AutoSavingCache.java
* /cassandra/trunk/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/AbstractCompactionIterable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionController.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionInfo.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionIterable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionType.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/OperationType.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/ParallelCompactionIterable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndexBuilder.java
* /cassandra/trunk/test/unit/org/apache/cassandra/io/LazilyCompactedRowTest.java
","09/Sep/11 11:19;hudson;Integrated in Cassandra #1093 (See [https://builds.apache.org/job/Cassandra/1093/])
    Remove wrongly added import (by CASSANDRA-3154)

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1167078
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After a ""short read"", the wrong read command may be used",CASSANDRA-3157,12522259,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,08/Sep/11 13:47,12/Mar/19 14:04,13/Mar/19 22:26,08/Sep/11 14:58,1.0.0,,,,,,0,,,,,"In fetchRows, there is this code:
{noformat}
    for (int i = 0; i < commandsToSend.size(); i++)
    {
        ReadCallback<Row> handler = readCallbacks.get(i);
        ReadCommand command = commands.get(i);
{noformat}
On the first iteration of fetchRows, commands == commandsToSend so this is ok, but on a short read, commandsToSend will only contain the command to retry so we'll pick up the wrong command on the last line.",,,,,,,,,,,,,,,,08/Sep/11 13:48;slebresne;3157.patch;https://issues.apache.org/jira/secure/attachment/12493584/3157.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-08 14:09:18.903,,,no_permission,,,,,,,,,,,,4057,,,Thu Sep 08 15:17:19 UTC 2011,,,,,,0|i0gghr:,94102,jbellis,jbellis,,,,,,,,,08/Sep/11 14:09;jbellis;+1 on the fix.  (is while-on-newline really an improvement?),"08/Sep/11 14:16;slebresne;bq. is while-on-newline really an improvement?

Felt more coherent with the coding style to have the bracket on its own line. But I don't have to commit it.","08/Sep/11 14:58;slebresne;Committed, thanks","08/Sep/11 15:17;hudson;Integrated in Cassandra #1089 (See [https://builds.apache.org/job/Cassandra/1089/])
    After a ""short read"", the wrong read command may be used
patch by slebresne; reviewed by jbellis for CASSANDRA-3157

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166716
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent users from creating keyspaces with LocalStrategy replication,CASSANDRA-3139,12521380,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,05/Sep/11 19:03,12/Mar/19 14:04,13/Mar/19 22:26,05/Sep/11 22:21,0.8.6,,,Legacy/CQL,,,0,,,,,,,,,,,,,,,,,,,,,05/Sep/11 21:36;xedin;CASSANDRA-3139.patch;https://issues.apache.org/jira/secure/attachment/12493069/CASSANDRA-3139.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-05 21:36:20.283,,,no_permission,,,,,,,,,,,,4069,,,Mon Sep 05 23:19:59 UTC 2011,,,,,,0|i0gg9j:,94065,jbellis,jbellis,,,,,,,,,05/Sep/11 21:32;jbellis;LocalStrategy isn't deprecated; it's just reserved for internal use.,05/Sep/11 21:36;xedin;error message is fixed.,05/Sep/11 21:48;jbellis;+1,05/Sep/11 22:21;xedin;Committed.,"05/Sep/11 23:19;hudson;Integrated in Cassandra-0.8 #315 (See [https://builds.apache.org/job/Cassandra-0.8/315/])
    Prevent users from creating keyspaces with LocalStrategy replication
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3139

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1165438
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool snapshot does not handle keyspace arguments correctly,CASSANDRA-3038,12518904,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,zznate,zznate,zznate,15/Aug/11 17:40,12/Mar/19 14:04,13/Mar/19 22:26,22/Aug/11 15:43,0.8.5,,,Tool/nodetool,,,0,,,,,"Given the following invocation:
./bin/nodetool snapshot Keyspace1 -t keyspace1_snapshot -h localhost

Snapshots are made for all keyspaces

Given a multi-keyspace invocation:
./bin/nodetool snapshot Keyspace1 Keyspace2 Keyspac3 -t keyspace1_snapshot -h localhost

Snapshots will be made for Keyspace2 and Keyspace3 but not Keyspace1. 

It appears there is just some antiquated command argument noodling in NodeCmd#handleSnapshots",,,,,,,,,,,,,,,,15/Aug/11 17:42;zznate;3038.txt;https://issues.apache.org/jira/secure/attachment/12490452/3038.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-22 15:43:26.432,,,no_permission,,,,,,,,,,,,20945,,,Mon Aug 22 16:20:22 UTC 2011,,,,,,0|i0gezr:,93859,brandon.williams,brandon.williams,,,,,,,,,15/Aug/11 17:42;zznate;Change to just make a copy of the cmdArgs array without modifying position. ,"15/Aug/11 17:44;zznate;Works with single, multi or no keyspace arguments. Added additional print statements to reflect what keyspaces were requested.",22/Aug/11 15:43;brandon.williams;Committed.,"22/Aug/11 16:20;hudson;Integrated in Cassandra-0.8 #288 (See [https://builds.apache.org/job/Cassandra-0.8/288/])
    Handle snapshot arguments correctly.
Patch by Nate McCall, reviewed by brandonwilliams for CASSANDRA-3038

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1160311
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/NodeCmd.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary index should report it's memory consumption,CASSANDRA-3155,12521842,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,jasonrutherglen,jasonrutherglen,08/Sep/11 02:12,12/Mar/19 14:04,13/Mar/19 22:26,23/Dec/11 19:11,1.0.7,,,Feature/2i Index,,,0,,,,,Non-CFS backed secondary indexes will consume RAM which should be reported back to Cassandra to be factored into it's flush by RAM amount.,,,,,,,,,,,,,,,,22/Dec/11 22:59;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3155-report-all-live-index-memory.txt;https://issues.apache.org/jira/secure/attachment/12508461/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3155-report-all-live-index-memory.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-22 21:38:15.235,,,no_permission,,,,,,,,,,,,1859,,,Fri Dec 23 19:11:13 UTC 2011,,,,,,0|i0gggv:,94098,jbellis,jbellis,,,,,,,,,22/Dec/11 21:38;jbellis;This feels clunky to me.  Wouldn't it be better to have both CFS and non-CFS indexes provide the same API so we can do it polymorphically w/ a single loop?,"22/Dec/11 23:04;tjake;attached a different approach.  The polymorphic approach would still be clunky because it includes self, so you need to get self + indexes.  latest patch seems more readable.",23/Dec/11 04:13;jbellis;+1,23/Dec/11 19:11;tjake;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the versions that are referenced in the generated POMs so that they match the versions in svn's lib folder,CASSANDRA-3184,12522777,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stephenc,stephenc,stephenc,12/Sep/11 18:37,12/Mar/19 14:04,13/Mar/19 22:26,13/Sep/11 12:31,1.0.0,,,Packaging,,,0,,,,,Update the versions before the release so that the release uses the same dependencies for Maven downloaded dependencies,,,,,,,,,,,,,,,,13/Sep/11 11:38;stephenc;CASSANDRA-3184.patch;https://issues.apache.org/jira/secure/attachment/12494218/CASSANDRA-3184.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-13 12:31:18.156,,,no_permission,,,,,,,,,,,,4033,,,Tue Sep 13 12:31:18 UTC 2011,,,,,,0|i0ggtj:,94155,slebresne,slebresne,,,,,,,,,12/Sep/11 18:46;stephenc;specifically the dependencies for snappy-java ( http://search.maven.org/#artifactdetails%7Corg.xerial.snappy%7Csnappy-java%7C1.0.3.3%7Cbundle ) and compress-lzf ( http://search.maven.org/#artifactdetails%7Ccom.ning%7Ccompress-lzf%7C0.8.4%7Cbundle ) are missing,13/Sep/11 11:38;stephenc;Patch also fixes the version and scm paths in build.xml so that the generated POMs are correct,13/Sep/11 11:39;stephenc;Submitted patch will fix the poms,"13/Sep/11 11:40;stephenc;I've made the patch, up to a committer to take and apply","13/Sep/11 12:31;slebresne;+1, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GCInspector still not avoiding divide by zero,CASSANDRA-3164,12522322,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,08/Sep/11 20:59,12/Mar/19 14:04,13/Mar/19 22:26,13/Sep/11 20:17,0.7.10,0.8.6,,,,,0,thistimeforsure,,,,"This is because Long objects need to be compared with .equals, not ==.

CASSANDRA-3076 is the original issue but we should use a new ticket for this since 0.7.9 and 0.8.5 are both released already.",,,,,,,,,,,,,,,,09/Sep/11 18:17;jbellis;3164.txt;https://issues.apache.org/jira/secure/attachment/12493833/3164.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-08 23:38:19.485,,,no_permission,,,,,,,,,,,,4050,,,Wed Sep 14 14:49:04 UTC 2011,,,,,,0|i0ggkv:,94116,tjake,tjake,,,,,,,,,08/Sep/11 23:38;tjake;curse you autoboxing!,09/Sep/11 18:17;jbellis;obvious patch is obvious,13/Sep/11 18:01;slebresne;+1,13/Sep/11 20:17;jbellis;committed,"14/Sep/11 14:49;hudson;Integrated in Cassandra-0.7 #552 (See [https://builds.apache.org/job/Cassandra-0.7/552/])
    Fix divide by zero error in GCInspector
patch by jbellis; reviewed by slebresne for CASSANDRA-3164

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1170308
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/GCInspector.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool should not NPE when rack/dc info is not yet available,CASSANDRA-3186,12522798,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,12/Sep/11 20:15,12/Mar/19 14:04,13/Mar/19 22:26,16/Nov/11 21:46,0.8.8,,,,,,1,lhf,,,,"As the title says.  What happens is the persisted ring is loaded, but if those nodes are down and you're using a snitch like ec2 that gets rack/dc info from gossip, nodetool NPEs instead of showing that the node is down.",,,,,,,,,,,,CASSANDRA-3114,,,,16/Nov/11 21:38;alexaraujo;cassandra-0.8-3186.txt;https://issues.apache.org/jira/secure/attachment/12503955/cassandra-0.8-3186.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-12 20:19:18.405,,,no_permission,,,,,,,,,,,,1857,,,Thu Nov 17 00:01:54 UTC 2011,,,,,,0|i0ggun:,94160,brandon.williams,brandon.williams,,,,,,,,,12/Sep/11 20:19;jbellis;Is this related to CASSANDRA-3175?,12/Sep/11 20:25;brandon.williams;Yes.,16/Nov/11 21:38;alexaraujo;Merges 1.0's Ec2Snitch which does the right thing,"16/Nov/11 21:46;brandon.williams;Committed.  Note that I think this is the preferable way to solve this, since it allows you to change snitches for CASSANDRA-3114, though your consistency guarantees will likely be violated and require repair afterwards.","17/Nov/11 00:01;hudson;Integrated in Cassandra-0.8 #399 (See [https://builds.apache.org/job/Cassandra-0.8/399/])
    Set default rack/dc in ec2snitch to avoid NPEs.
Patch by Alex Araujo, reviewed by brandonwilliams for CASSANDRA-3186.

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1202892
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/Ec2Snitch.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unable to remove column metadata via CLI,CASSANDRA-3126,12521207,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,hsn,hsn,02/Sep/11 16:39,12/Mar/19 14:04,13/Mar/19 22:26,15/Oct/11 20:17,0.8.8,1.0.1,,Legacy/Tools,,,0,cassandra-cli,,,,"I cant find way how to remove all columns definitions without CF import/export.

[default@int4] update column family sipdb with column_metadata = [];
Syntax error at position 51: required (...)+ loop did not match anything at input ']'

[default@int4] update column family sipdb with column_metadata = [{}];
Command not found: `update column family sipdb with column_metadata = [{}];`. Type 'help;' or '?' for help.
[default@int4]
",,,,,,,,,,,,,,,,15/Oct/11 15:51;xedin;CASSANDRA-3126.patch;https://issues.apache.org/jira/secure/attachment/12499147/CASSANDRA-3126.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-15 15:51:27.793,,,no_permission,,,,,,,,,,,,1868,,,Sat Oct 15 21:22:05 UTC 2011,,,,,,0|i0gg3j:,94038,jbellis,jbellis,,,,,,,,,15/Oct/11 15:51;xedin;column_metadata = []; should be functional now.,15/Oct/11 16:06;jbellis;+1,15/Oct/11 20:17;xedin;Committed.,"15/Oct/11 21:22;hudson;Integrated in Cassandra-0.8 #375 (See [https://builds.apache.org/job/Cassandra-0.8/375/])
    Fix completely removing column metadata using CLI
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3126

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1183681
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/Cli.g
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra-CLI does not allow ""Config"" as column family name",CASSANDRA-3195,12522876,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,kalmatar,kalmatar,13/Sep/11 10:13,12/Mar/19 14:04,13/Mar/19 22:26,14/Sep/11 12:06,0.8.6,,,Legacy/Tools,,,0,,,,,"""create column family Config"" does not work, ""create column family Configg"" does.

I suppose the intent is that column families can be named freely, that they have a namespace completely of their own, and separate from, say, Cassandra-CLI commands.",,,,,,,,,,,,,,,,14/Sep/11 11:26;xedin;CASSANDRA-3195.patch;https://issues.apache.org/jira/secure/attachment/12494429/CASSANDRA-3195.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-13 12:43:46.067,,,no_permission,,,,,,,,,,,,4027,,,Wed Sep 14 15:15:48 UTC 2011,,,,,,0|i0ggyn:,94178,jbellis,jbellis,,,,,,,,,13/Sep/11 12:43;jbellis;Did you try quoting the desired name?,"13/Sep/11 13:01;kalmatar;create column family 'Config';

gives ""Invalid column family name: 'Config'"",

create column family ""Config"";

gives ""Syntax error at position 21: unexpected """""" for `create column family ""Config"";`.""

or did I miss another way of quoting?

",14/Sep/11 11:26;xedin;CLI supports only single-quotes. Patch to support quoting of the ColumnFamily name is attached (`update column family` statement already supports that) and test for that.,14/Sep/11 11:56;jbellis;+1,14/Sep/11 12:06;xedin;Committed.,"14/Sep/11 15:15;hudson;Integrated in Cassandra-0.8 #327 (See [https://builds.apache.org/job/Cassandra-0.8/327/])
    Allow quoting of the ColumnFamily name in CLI `create column family` statement
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3195

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1170555
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"increase file descriptor limit in deb, rpm packages",CASSANDRA-3206,12523066,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,jbellis,jbellis,14/Sep/11 16:04,12/Mar/19 14:04,13/Mar/19 22:26,23/Sep/11 21:23,0.8.7,,,Packaging,,,0,,,,,"We can use a lot of file descriptors (one per socket, 5? per sstable).  People hit this regularly on the user list and it will get worse with Leveled compaction, which limits sstable size to a relatively low size (currently 5MB).",,,,,,,,,,,,,,,,23/Sep/11 21:13;thepaul;3206.patch.txt;https://issues.apache.org/jira/secure/attachment/12496328/3206.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-14 21:19:01.049,,,no_permission,,,,,,,,,,,,4021,,,Fri Sep 23 22:47:32 UTC 2011,,,,,,0|i0gh3b:,94199,brandon.williams,brandon.williams,,,,,,,,,"14/Sep/11 16:06;jbellis;I'd suggest sizing this based on the number of sstables we need to support a ""sane"" amount of storage.  1TB?  2?","14/Sep/11 16:32;jbellis;bq. 5? per sstable

This is incorrect -- when using mmap'd I/O, we don't consume FD-per-sstable (we use one temporarily when setting up the mmap, then release it).  And even in buffered I/O mode we use one FD per sstable being read, per reading thread.  (Which will be much less than one FD per sstable in most cases, although range scans under LevelDB do not yet do a very good job of cutting back the number of SSTables consulted.)

So it sounds like we mostly need to make sure we have it high enough that we tolerate high numbers of unpooled connections (common in PHP environments, I'm told).","14/Sep/11 16:42;jbellis;Jeremy pointed out that we can also use a large number of FDs during compaction: either major compaction for tiered strategy, or a L0 compaction under Leveled, can open an effectively arbitrary number of sstables if compaction is behind.",14/Sep/11 16:43;jbellis;So...  64K?,"14/Sep/11 21:19;scode;My two cents: Just go wild. I really don't see the need to be conservative. On any modern system that you run Cassandra on, the resources consumed by file descriptors is going to be irrelevant and I don't see when you'd ever actually want Cassandra to hit the limit, unless it's *completely* run-away and buggy in which case the limit need not be low. Better a very high number so people don't run into it, than try to shave off.

64k seems reasonable, I'd be fine with 250k ;)
",14/Sep/11 22:49;jbellis;SGTM.,15/Sep/11 02:59;thepaul;I concur. I'll go with 100k unless someone has a good argument for having it be higher.,23/Sep/11 21:23;brandon.williams;Committed.,"23/Sep/11 22:47;hudson;Integrated in Cassandra-0.8 #339 (See [https://builds.apache.org/job/Cassandra-0.8/339/])
    Increase FD limit to 100k in packaging.
Patch by Paul Cannon, reviewed by brandonwilliams for CASSANDRA-3206

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1175027
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/debian/cassandra.conf
* /cassandra/branches/cassandra-0.8/debian/init
* /cassandra/branches/cassandra-0.8/redhat/cassandra.conf
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
USE <keyspace> doesn't work for numeric keyspaces,CASSANDRA-3208,12523104,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,liqweed,liqweed,14/Sep/11 21:03,12/Mar/19 14:04,13/Mar/19 22:26,16/Sep/11 21:22,0.8.7,,,,,,0,cli,,,,"In the CLI, {code}USE <keyspace>;{code} doesn't work for keyspaces' names that contain only digits.
The error I'm getting is:
{{Syntax error at position 4: mismatched input '20110914' expecting Identifier}}",,,,,,,,,,,,,,,,16/Sep/11 20:43;xedin;CASSANDRA-3208.patch;https://issues.apache.org/jira/secure/attachment/12494860/CASSANDRA-3208.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-14 22:48:00.643,,,no_permission,,,,,,,,,,,,4019,,,Fri Sep 16 22:22:54 UTC 2011,,,,,,0|i0gh47:,94203,jbellis,jbellis,,,,,,,,,14/Sep/11 22:48;jbellis;Did you try quoting the KS name?,16/Sep/11 08:21;liqweed;Quoting (neither single nor double quotes) didn't help.,"16/Sep/11 20:43;xedin;Numberic keyspace names do work for me on the latest cassandra-0.8 branch, the only thing what patch does is allows to use quotes in USE command.

Here is the CLI session

{noformat}
[git:cassandra-0.8] (~/work/java/cassandra) → ./bin/cassandra-cli --host localhost
Connected to: ""Test Cluster"" on localhost/9160
Welcome to the Cassandra CLI.

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] create keyspace 20110914;
afcddc30-e0a3-11e0-0000-242d50cf1ff5
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] use 20110914;
Authenticated to keyspace: 20110914
[default@20110914] use '20110914';
Authenticated to keyspace: 20110914
[default@20110914]
{noformat}",16/Sep/11 21:11;jbellis;+1,"16/Sep/11 22:22;hudson;Integrated in Cassandra-0.8 #333 (See [https://builds.apache.org/job/Cassandra-0.8/333/])
    Allow using quotes in ""USE <keyspace>;"" CLI command
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3208

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1171792
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log message at INFO when a global or keyspace level repair operation completes,CASSANDRA-3207,12523081,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,bcoverston,bcoverston,14/Sep/11 18:05,12/Mar/19 14:04,13/Mar/19 22:26,19/Sep/11 12:46,0.8.7,,,,,,0,logging,repair,,,"If JMX times out it's difficult to tell when repair completes.Right now we log at DEBUG for each column family but we need a way to tell when the repair operation completes as a whole.
",,,,,,,,,,,,,,,,16/Sep/11 15:30;slebresne;3207.patch;https://issues.apache.org/jira/secure/attachment/12494805/3207.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-16 15:30:59.624,,,no_permission,,,,,,,,,,,,4020,,,Mon Sep 19 13:20:03 UTC 2011,,,,,,0|i0gh3r:,94201,jbellis,jbellis,,,,,,,,,"16/Sep/11 15:30;slebresne;Attached patch adds two new log messages:
  * at the completion of a repair session. This part is already in 1.0.0 but not in 0.8.
  * at the beginning and total completion of a repair ""command"". A command being basically a 'nodetool repair' call, that usually consist of multiple session (one by token range).",16/Sep/11 19:08;jbellis;+1,"19/Sep/11 12:46;slebresne;Committed, thanks","19/Sep/11 13:20;hudson;Integrated in Cassandra-0.8 #335 (See [https://builds.apache.org/job/Cassandra-0.8/335/])
    Log message at INFO when a global or keyspace level repair operation completes
patch by slebresne; reviewed by jbellis for CASSANDRA-3207

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1172591
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamily.cloneMeShallow doesn't respect the insertionOrdered flag (for ArrayBackedSortedColumns),CASSANDRA-3205,12523056,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,14/Sep/11 14:18,12/Mar/19 14:04,13/Mar/19 22:26,14/Sep/11 14:32,1.0.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,14/Sep/11 14:21;slebresne;3205.patch;https://issues.apache.org/jira/secure/attachment/12494447/3205.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-14 14:24:54.502,,,no_permission,,,,,,,,,,,,4022,,,Wed Sep 14 14:32:31 UTC 2011,,,,,,0|i0gh2v:,94197,jbellis,jbellis,,,,,,,,,14/Sep/11 14:24;jbellis;+1,"14/Sep/11 14:32;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI does not support removing compression options from a ColumnFamily,CASSANDRA-3282,12525272,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,zznate,zznate,30/Sep/11 06:08,12/Mar/19 14:04,13/Mar/19 22:26,30/Sep/11 20:20,1.0.0,,,Legacy/Tools,,,0,,,,,This may be an issue with ThriftValidator as well - not accepting a null or empty compression properties map as a disable flag.,,,,,,,,,,,,,,,,30/Sep/11 20:10;xedin;CASSANDRA-3282-doc.patch;https://issues.apache.org/jira/secure/attachment/12497210/CASSANDRA-3282-doc.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-30 15:46:47.559,,,no_permission,,,,,,,,,,,,40960,,,Fri Sep 30 20:20:40 UTC 2011,,,,,,0|i0gi0f:,94348,brandon.williams,brandon.williams,,,,,,,,,"30/Sep/11 15:46;xedin;Works for me on the latest cassandra-1.0.0 branch:

{noformat}
[git:cassandra-1.0.0] (~/work/java/cassandra-trunk) → ./bin/cassandra-cli --host localhost
Connected to: ""Test Cluster"" on localhost/9160
Welcome to the Cassandra CLI.

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] create keyspace ks;
972385e0-eb7a-11e0-0000-242d50cf1fdd
Waiting for schema agreement...
... schemas agree across the cluster
[default@ks] create column family cf with comparator=UTF8Type and key_validation_class=UTF8Type and 
...	compression_options={sstable_compression:SnappyCompressor, chunk_length_kb:16};        
11870140-eb7b-11e0-0000-242d50cf1fdd
Waiting for schema agreement...
... schemas agree across the cluster
[default@ks] describe cf;                                                                           
    ColumnFamily: cf
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
      Compression Options:
        chunk_length_kb: 16
        sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor
[default@ks] update column family cf with comparator=UTF8Type and key_validation_class=UTF8Type and 
...	compression_options=null;                                                              
18177490-eb7b-11e0-0000-242d50cf1fdd
Waiting for schema agreement...
... schemas agree across the cluster
[default@ks] describe cf;                                                                           
    ColumnFamily: cf
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
[default@ks] quit;
{noformat}

Can you please provide at least your error message?",30/Sep/11 16:18;zznate;Thanks Pavel - I just verified this on the rc1 binary as well. Can we keep this open in the context of adding the above to the CLI help?,30/Sep/11 16:22;xedin;I will attach help update as a patch tonight.,30/Sep/11 20:14;brandon.williams;+1,30/Sep/11 20:20;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSLFactory should not enable cipher suites that aren't supported,CASSANDRA-3278,12525224,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,gcristea,gcristea,29/Sep/11 18:21,12/Mar/19 14:04,13/Mar/19 22:26,29/Nov/11 16:42,1.0.5,,,,,,0,,,,,"The socket creation (server or otherwise) in SSLFactory.java calls [setEnabledCipherSuites|http://download.oracle.com/javase/6/docs/api/javax/net/ssl/SSLServerSocket.html#setEnabledCipherSuites(java.lang.String\[\])] with the values specified in EncryptionOptions.java:

{code}
public String[] cipherSuites = {
    ""TLS_RSA_WITH_AES_128_CBC_SHA"", 
    ""TLS_RSA_WITH_AES_256_CBC_SHA""
};
{code}

The call to [setEnabledCipherSuites|http://download.oracle.com/javase/6/docs/api/javax/net/ssl/SSLServerSocket.html#setEnabledCipherSuites(java.lang.String\[\])] fails on systems that don't have [Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files 6|http://www.oracle.com/technetwork/java/javase/downloads/jce-6-download-429243.html] because AES256 is not supported.

To avoid installing the unlimited strength policy file the code in SSLFactory.java should call [getSupportedCipherSuites|http://download.oracle.com/javase/6/docs/api/javax/net/ssl/SSLServerSocket.html#getSupportedCipherSuites()] to find out which of the suites specified are supported.

Thanks,
George",OpenJDK on debian squeeze,,,,,,,,,,,,,,,29/Nov/11 04:30;vijay2win@yahoo.com;0001-commiting-changes-to-make-the-ks-ts-more-flexible-v2.patch;https://issues.apache.org/jira/secure/attachment/12505446/0001-commiting-changes-to-make-the-ks-ts-more-flexible-v2.patch,06/Oct/11 23:46;vijay2win@yahoo.com;0001-commiting-filter-for-supported-suits.patch;https://issues.apache.org/jira/secure/attachment/12498096/0001-commiting-filter-for-supported-suits.patch,06/Oct/11 23:46;vijay2win@yahoo.com;0002-commiting-changes-to-make-the-ks-ts-more-flexible.patch;https://issues.apache.org/jira/secure/attachment/12498095/0002-commiting-changes-to-make-the-ks-ts-more-flexible.patch,29/Nov/11 04:30;vijay2win@yahoo.com;0002-commiting-filter-for-supported-suits-v2.patch;https://issues.apache.org/jira/secure/attachment/12505447/0002-commiting-filter-for-supported-suits-v2.patch,29/Nov/11 04:30;vijay2win@yahoo.com;0003-expose-the-available-options-in-yaml-v2.patch;https://issues.apache.org/jira/secure/attachment/12505448/0003-expose-the-available-options-in-yaml-v2.patch,06/Oct/11 23:46;vijay2win@yahoo.com;0003-expose-the-available-options-in-yaml.patch;https://issues.apache.org/jira/secure/attachment/12498094/0003-expose-the-available-options-in-yaml.patch,04/Oct/11 00:24;gcristea;cassandra-3278-cache.txt;https://issues.apache.org/jira/secure/attachment/12497575/cassandra-3278-cache.txt,04/Oct/11 00:24;gcristea;cassandra-3278-nocache.txt;https://issues.apache.org/jira/secure/attachment/12497574/cassandra-3278-nocache.txt,,,,8.0,,,,,,,,,,,,,,,,,,,2011-09-29 18:44:00.553,,,no_permission,,,,,,,,,,,,40339,,,Tue Nov 29 16:42:05 UTC 2011,,,,,,0|i0ghyn:,94340,brandon.williams,brandon.williams,,,,,,,,,"29/Sep/11 18:44;jbellis;Thanks for the bug report, George. It sounds like you have a good handle on this, can you submit a patch?","30/Sep/11 00:34;gcristea;Sure, I'll try to get the patch to you by EOD tomorrow. 

Once question: do you see a problem with calling getSupportedCipherSuites and doing the filtering for every socket creation? 

Thanks,
George","04/Oct/11 00:24;gcristea;I wasn't happy with reading the keystore/trusstore files and doing the cipher suites' filtering for each socket creation so I ended up creating two patches:
* cassandra-3278-nocache.txt: Does the filtering, the down side that the filtering is done for each socket that's created.
* cassandra-3278-cache.txt: Caches the SSLContext along with the supported cipher suites for server and non-server sockets. The down side is that changing the keystore/truststore requires a restart of the node.

I don't have enough information to decide which version is preferable, I leave that to you.

Thanks,
George","05/Oct/11 20:43;vijay2win@yahoo.com;George,

Thanks for the patch,

The problem with the cached is that we need to restart the whole cluster when we change the KS/TS, instead we will have the flexibility if the new connections will just pick it up. We persist the connections untill disconnect hence the performance shouldn't be a concern. Also there can be variety of ssl client (example fat clients) which may have different sets of supported suits (caching one might not help).

1) cassandra-3278-nocache isn't a patch by itself (Can you rebase it?)
2) in the non cached one, If we can log a info on the filtered suit it will be great,

Just a side note... I would use Sets.intersection to reduce the amount of code :)
",06/Oct/11 23:46;vijay2win@yahoo.com;We can also expose some of the available options so the users can choose.,28/Oct/11 16:26;jbellis;Since Cassandra sockets are long-lived I'm fine with having filter-each-socket being the only option.,"28/Nov/11 21:22;brandon.williams;This looks good, but one minor nit: instead of cipherSuites in the yaml can we use the more idiomatic 'cipher_suites'?","29/Nov/11 04:30;vijay2win@yahoo.com;Done!, the only change between the v1 is change from cipherSuites to cipher_suites. Thanks!","29/Nov/11 16:42;brandon.williams;Thanks, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL does not throw an error when invalid hex is supplied,CASSANDRA-3231,12523646,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,20/Sep/11 01:06,12/Mar/19 14:04,13/Mar/19 22:26,27/Sep/11 20:14,0.8.7,,,,,,0,,,,,"As reported on irc, if you try to create an index on a CF with a default comparator of BytesType, but you supply invalid hex, weird things happen.  Namely if you try to create one on 'category' you instead get one on '\xca\xfe\xff\xff', which is 4 bytes that appears to coincide with attempting to interpret 'ca', 'te', 'go', 'ry' as hex.",,,,,,,,,,,,,,,,27/Sep/11 15:22;jbellis;3231-v2.txt;https://issues.apache.org/jira/secure/attachment/12496700/3231-v2.txt,27/Sep/11 14:16;xedin;CASSANDRA-3231.patch;https://issues.apache.org/jira/secure/attachment/12496686/CASSANDRA-3231.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-27 13:12:14.536,,,no_permission,,,,,,,,,,,,1846,,,Tue Sep 27 20:14:56 UTC 2011,,,,,,0|i0ghen:,94250,jbellis,jbellis,,,,,,,,,27/Sep/11 13:12;xedin;This is not a CQL/CLI problem but a problem in FBUtilities bytesToHex and hexToBytes methods. New issue would be opened to fix that (if needed).,27/Sep/11 13:20;jbellis;I don't think we need a new issue.,27/Sep/11 13:22;xedin;There is the question then - do we really need to change behavior of bytesToHex/hexToBytes methods of FBUtilities class?,"27/Sep/11 13:28;jbellis;we need to validate index targets one way or the other.

shouldn't this be done by calling AbstractType.validate?","27/Sep/11 13:32;xedin;This is not a question of validation because {from/get}String methods of any type should return valid data rather the question of BytesType string representation, I'm going to let Brandon comment on the latter.","27/Sep/11 13:46;jbellis;""public ByteBuffer fromString(String source) throws MarshalException""

if the source is not a valid encoding of the type (as ""category"" is not for BytesType) then this should throw","27/Sep/11 13:53;xedin;Exactly, but FBUtilities.hexToBytes does not validate the string so we need to way to validate was is string a correct hex or not","27/Sep/11 14:23;jbellis;what is this existing part of fromString checking for?

{code}
        catch (NumberFormatException e)
        {
            throw new MarshalException(String.format(""cannot parse '%s' as hex bytes"", source), e);
        }
{code}",27/Sep/11 14:26;xedin;I think that was expecting hexToBytes to validate string and throw a NumberFormatException but it never did before this patch.,"27/Sep/11 15:15;jbellis;looks like that's from the old hexToBytes implementation, where the inner loop used to be

{code}
            bytes[i] = (byte)Integer.parseInt(str.substring(i*2, i*2+2), 16);
{code}

So I think it's safe to remove now.",27/Sep/11 15:19;xedin;do you want to throw MarshalException directly from FBUtilities.hexToBytes to remove NumberFormatException from BytesType.fromString(String)?,27/Sep/11 15:22;jbellis;v2 attached that doesn't need an extra regexp pass.  (since we use bytestype for cql blob data it can be performance sensitive.),27/Sep/11 15:32;xedin;+1 just please add the test from my patch so we don't face such regressions in the future.,27/Sep/11 20:14;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Don't fail when numactl is installed, but NUMA policies are not supported",CASSANDRA-3245,12524371,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,thepaul,thepaul,23/Sep/11 02:56,12/Mar/19 14:04,13/Mar/19 22:26,26/Sep/11 20:47,1.0.0,,,Packaging,,,0,,,,,"When numactl is installed but NUMA policies are not supported, trying to run cassandra gives only:

{noformat}
numactl: This system does not support NUMA policy
{noformat}

..and the startup script fails there.

We should probably fail a little more gracefully. Possibly the best way to tell if numactl will work is by using:

{noformat}
numactl --hardware
{noformat}

but I don't have ready access to a machine with proper NUMA support at the moment so I can't check how easy it is to tell the difference in the output.

It looks just as reliable (if possibly a bit more brittle) to check for the existence of the directory {{/sys/devices/system/node}}. If that directory doesn't exist, we shouldn't even try to use or run numactl.","Any Linux system where a 'numactl' executable is available, but no NUMA policies are actually supported. EC2 nodes are easy examples of environments with no NUMA policy support.",,,,,,,,,,,,,,,25/Sep/11 19:39;scode;3245.txt;https://issues.apache.org/jira/secure/attachment/12496408/3245.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-23 08:52:21.433,,,no_permission,,,,,,,,,,,,776,,,Mon Sep 26 20:47:27 UTC 2011,,,,,,0|i0ghkn:,94277,thepaul,thepaul,,,,,,,,,"23/Sep/11 08:52;scode;Or we could do something seemingly ugly but probably very effective and safe: Try running ""ls -d / > /dev/null"" with numactl and if that fails, assume it is because numactl isn't working. That should hopefully work in pretty much any environment.

I'll submit a patch soonish.
","25/Sep/11 19:39;scode;Attaching trivial patch that does the ls test.

@paul Can you test whether numactl exits with a proper non-0 exit status on the system where it is not supported? I've tested the script as such, but I don't have a system available without the necessary support to test on.
","26/Sep/11 15:46;thepaul;bq. @paul Can you test whether numactl exits with a proper non-0 exit status on the system where it is not supported?

Indeed it does; using --interleave=all when there is no NUMA policy support causes an exit value of 1.

+1",26/Sep/11 20:47;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader ignores option doesn't work correctly,CASSANDRA-3247,12524396,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Sep/11 09:07,12/Mar/19 14:04,13/Mar/19 22:26,23/Sep/11 12:35,0.8.7,1.0.0,,,,,0,bulkloader,,,,The --ignores option is supposed to take an argument but it doesn't.,,,,,,,,,,,,,,,,23/Sep/11 09:09;slebresne;3247.patch;https://issues.apache.org/jira/secure/attachment/12496243/3247.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-23 12:20:14.861,,,no_permission,,,,,,,,,,,,3526,,,Fri Sep 23 13:21:27 UTC 2011,,,,,,0|i0ghlj:,94281,jbellis,jbellis,,,,,,,,,"23/Sep/11 12:20;jbellis;+1

(nit: ""Unknow"" should be ""Unknown"")","23/Sep/11 12:35;slebresne;Committed (with spelling fix), thanks","23/Sep/11 13:21;hudson;Integrated in Cassandra-0.8 #338 (See [https://builds.apache.org/job/Cassandra-0.8/338/])
    fix sstableloader --ignores option
patch by slebresne; reviewed by jbellis for CASSANDRA-3247

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1174701
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/BulkLoader.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Failure reading a erroneous/spurious AutoSavingCache file can result in a failed application of a migration, which can prevent a node from reaching schema agreement.",CASSANDRA-3218,12523356,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,eldondev,eldondev,eldondev,16/Sep/11 16:58,12/Mar/19 14:04,13/Mar/19 22:26,22/Sep/11 21:09,0.8.7,1.0.0,,,,,0,,,,,"Failure reading a erroneous/spurious AutoSavingCache file can result in a failed application of a migration, which can prevent a node from reaching schema agreement. This is distinctly possible when a machine loses it's data partition, and attempts to recover the schema upon restart, and so has to apply all the migrations. The initial stack traces look like this:

Add column family: org.apache.cassandra.config.CFMetaData@38bcfee6[cfId=1000,ksName=someks,cfName=somecf,cfType=Standard,comparat
or=org.apache.cassandra.db.marshal.UTF8Type,subcolumncomparator=<null>, ...

Followed by:

ERROR 00:56:47,974 Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.RuntimeException: java.lang.NegativeArraySizeException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NegativeArraySizeException
        at org.apache.cassandra.cache.AutoSavingCache.readSaved(AutoSavingCache.java:130)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:273)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:465)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:435)
        at org.apache.cassandra.db.Table.initCf(Table.java:369)
        at org.apache.cassandra.db.migration.AddColumnFamily.applyModels(AddColumnFamily.java:93)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:153)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:73)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more

Ultimately, attempted changes to this keyspace/cf will fail like this:

ERROR 13:07:51,006 Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown CF 1000
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException: Unknown CF 1000
        at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:155)
        at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:148)
        at org.apache.cassandra.db.migration.DropKeyspace.applyModels(DropKeyspace.java:63)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:153)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:73)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more",,,,,,,,,,,,,,,,19/Sep/11 17:49;eldondev;CASSANDRA-3218-autosave_exception_handling-v2.patch;https://issues.apache.org/jira/secure/attachment/12495115/CASSANDRA-3218-autosave_exception_handling-v2.patch,16/Sep/11 17:23;eldondev;CASSANDRA-3218-autosave_exception_handling.patch;https://issues.apache.org/jira/secure/attachment/12494825/CASSANDRA-3218-autosave_exception_handling.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-16 19:02:19.595,,,no_permission,,,,,,,,,,,,4014,,,Thu Sep 22 22:22:44 UTC 2011,,,,,,0|i0gh8n:,94223,jbellis,jbellis,,,,,,,,,16/Sep/11 17:23;eldondev;First pass at a patch that fixes this,"16/Sep/11 19:02;jbellis;Maybe we should just extend the existing IOException to Exception.  Basically our attitude should be ""I don't care what went wrong loading the cache, it's optional, let me continue.""",19/Sep/11 17:48;eldondev;Sounds good to me.,"22/Sep/11 21:09;jbellis;committed, thanks!","22/Sep/11 22:22;hudson;Integrated in Cassandra-0.8 #337 (See [https://builds.apache.org/job/Cassandra-0.8/337/])
    Don't allow any cache loading exceptions to halt startup
patch by Eldon Stegall; reviewed by jbellis for CASSANDRA-3218

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1174392
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cache/AutoSavingCache.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
probably don't need to do full copy to row cache after un-mmap() change,CASSANDRA-3223,12523412,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yangyangyyy,yangyangyyy,yangyangyyy,17/Sep/11 00:31,12/Mar/19 14:04,13/Mar/19 22:26,18/Sep/11 04:51,1.0.0,,,,,,0,,,,,"3179  changes from directly using the bytebuffer from mmap(), to copying that buffer,

CFS.cacheRow() https://github.com/apache/cassandra/blob/cassandra-1.0.0/src/java/org/apache/cassandra/db/ColumnFamilyStore.java   line 1126
says it makes a deep copy exactly to prevent issues from unmmap().

maybe this deep copy is not needed now given 3179


if so, maybe slightly better performance in both speed and memory",,,,,,,,,,,,,,,,18/Sep/11 04:19;yangyangyyy;0002-do-not-need-to-deep-copy-column-value-bytebuffer-now.patch;https://issues.apache.org/jira/secure/attachment/12494959/0002-do-not-need-to-deep-copy-column-value-bytebuffer-now.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-17 02:04:52.17,,,no_permission,,,,,,,,,,,,4010,,,Sun Sep 18 04:51:07 UTC 2011,,,,,,0|i0ghb3:,94234,jbellis,jbellis,,,,,,,,,17/Sep/11 02:04;jbellis;I think you're right.  Care to submit a patch to remove the extra copy?,"17/Sep/11 04:50;yangyangyyy;sure

","18/Sep/11 04:19;yangyangyyy;do not need to deep copy column value bytebuffer into row cache, now that we already do this copy in JIRA 3179","18/Sep/11 04:51;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Short reads protection results in returning more columns than asked for,CASSANDRA-3303,12525636,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,byronclark,slebresne,slebresne,04/Oct/11 09:04,12/Mar/19 14:04,13/Mar/19 22:26,04/Nov/11 08:37,1.0.2,,,,,,0,,,,,"When we detect a short read (in SP.fetchRows), we retry a new command created by:
{noformat}
logger.debug(""detected short read: expected {} columns, but only resolved {} columns"", sliceCommand.count, liveColumnsInRow);
int retryCount = sliceCommand.count + sliceCommand.count - liveColumnsInRow;
SliceFromReadCommand retryCommand = new SliceFromReadCommand(command.table,
                                                             command.key,
                                                             command.queryPath,
                                                             sliceCommand.start,
                                                             sliceCommand.finish,
                                                             sliceCommand.reversed,
                                                             retryCount);
{noformat}
That is, in that new command, the count is greater than what asked in the initial command. But we never cut back the result of that new retried query.",,,,,,,,,,,,,,,,19/Oct/11 02:46;byronclark;ASF.LICENSE.NOT.GRANTED--long_read.sh;https://issues.apache.org/jira/secure/attachment/12499627/ASF.LICENSE.NOT.GRANTED--long_read.sh,19/Oct/11 04:20;byronclark;cassandra-3303-1.patch;https://issues.apache.org/jira/secure/attachment/12499632/cassandra-3303-1.patch,19/Oct/11 04:24;byronclark;cassandra-3303-2.patch;https://issues.apache.org/jira/secure/attachment/12499634/cassandra-3303-2.patch,30/Oct/11 02:07;byronclark;cassandra-3303-3.patch;https://issues.apache.org/jira/secure/attachment/12501488/cassandra-3303-3.patch,01/Nov/11 19:48;byronclark;cassandra-3303-4.patch;https://issues.apache.org/jira/secure/attachment/12501816/cassandra-3303-4.patch,03/Nov/11 20:18;byronclark;cassandra-3303-5.patch;https://issues.apache.org/jira/secure/attachment/12502214/cassandra-3303-5.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2011-10-05 19:28:08.907,,,no_permission,,,,,,,,,,,,44229,,,Fri Nov 04 08:37:54 UTC 2011,,,,,,0|i0gi9r:,94390,slebresne,slebresne,,,,,,,,,"05/Oct/11 19:28;jbellis;Tagging this 1.0.1 since short reads being broken is nothing new (I would say returning too much data, is better than returning not enough)",19/Oct/11 02:46;byronclark;[^long_read.sh] is the script I'm using with ccm to reproduce the issue. It's the same script used for CASSANDRA-2643.,19/Oct/11 04:20;byronclark;[^cassandra-3303-1.patch] is a first attempt at fixing this.  I'm not sure it handles the situation correctly for a slice requested in reverse order.,19/Oct/11 04:24;byronclark;[^cassandra-3303-2.patch] doesn't reuse a counter variable on an inner loop.,"20/Oct/11 14:37;slebresne;This doesn't always work, because we cannot be sure that on the retry phase, we will have a digest mismatch again (some read repair could have kicked in). I.e, we should also trim also in the initial for loop, not only in the 'deal with repair response' one.

Also (and I'm nitpicking a bit) it would be nice to push the trim code into RetriedSliceFromReadCommand (and to make it handle reversed queries). For instance we could add a filter(ColumFamily) method or something like that. Same thing for the original count, rather than using instanceof, I'd rather add a originalCount() method to SliceFromReadCommand (that would return count) that the Retried variant would override.","30/Oct/11 02:07;byronclark;[^cassandra-3303-3.patch] should address all the concerns mentioned. 

I'm going to go ahead and pull some more of the logic out of StorageProxy and get rid of all the instanceof checks.","31/Oct/11 21:59;jbellis;So, we should wait before reviewing?","01/Nov/11 14:58;byronclark;Yes, please. I'll be attaching the improved patch later today.","01/Nov/11 17:20;jbellis;Belatedly and probably redundantly, I note that any time we hit RowRepairResolver we can have this count problem, not just w/ short read resolution.  (E.g., CASSANDRA-3395)","01/Nov/11 19:48;byronclark;[^cassandra-3303-4.patch] removes all of the ugly logic to handle this case (and the short read case) into the ReadCommands.

Jonathan - Does this already address CASSANDRA-3395 or is more work needed for that one?","02/Nov/11 14:51;slebresne;On the patch, a few minor remarks:
* There is a weird UnImplNode on top of ReadCommand.java (that prevents compilation)
* I think we could replace isLongRead and trimLongRead by a maybeTrim() method that would be a noop by default. Basically trimLongRead requires that isLongRead() has been called (otherwise it throws an unsupported exception), so in those case I think it's easier to use if it's just one method. Same for the pair (isShortRead, generateShortRetry) actually.
* nit: There is some imports rewrite (java.io.* -> multiple imports) in SFRC. It's a big deal but that kind of thing makes diffs bigger than necessary so it's nice to take the habit to no do that.

Now I don't think this patch addresses CASSANDRA-3395 as is. For that, we would basically need to move the trimLongRead to SliceFromReadCommand directly (the bug of CASSANDRA-3395 is that normal SliceFromReadCommand can have more that requested columns because of reconciliation of the result of different nodes). The easiest way here is probably to move the trim code to a SliceFromReadCommand.maybeTrim() and to have that method uses some getRequestedCount() method. For SFRC, the latter method would return count, for RSFRC, it would return originalCount.
","03/Nov/11 20:18;byronclark;[^cassandra-3303-5.patch] addresses all the comments in the most recent review. Additionally, it checks for a long read on SliceFromReadCommand instead of just RetriedSliceFromReadCommand.","04/Nov/11 08:37;slebresne;+1, committed
Thanks Byron.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing fields in show schema output,CASSANDRA-3304,12525659,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,hsn,hsn,04/Oct/11 12:52,12/Mar/19 14:04,13/Mar/19 22:26,04/Oct/11 15:58,1.0.0,,,Legacy/Tools,,,0,,,,,"if you compare output of these 2 commands:

*show keyspaces*
Keyspace: test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: sipdb
    ""phone calls routing information""
      Key Validation Class: org.apache.cassandra.db.marshal.IntegerType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.AsciiType
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Key cache size / save period in seconds: 0.0/0
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: kam
          Validation Class: org.apache.cassandra.db.marshal.AsciiType
      *Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompacti*

*show schema*
create column family sipdb
  with column_type = 'Standard'
  and comparator = 'AsciiType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'IntegerType'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and keys_cached = 0.0
  and key_cache_save_period = 0
  and read_repair_chance = 0.0
  and gc_grace = 0
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = false
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider'
  and comment = 'phone calls routing information'
  and column_metadata = [
    {column_name : 'kam',
    validation_class : AsciiType}];

You will discover that show schema is missing: 1. compaction strategy. 2. how many keys to save",,,,,,,,,,,,,,,,04/Oct/11 15:32;xedin;CASSANDRA-3304.patch;https://issues.apache.org/jira/secure/attachment/12497650/CASSANDRA-3304.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-04 13:22:46.186,,,no_permission,,,,,,,,,,,,44289,,,Wed Oct 05 20:38:09 UTC 2011,,,,,,0|i0gia7:,94392,jbellis,jbellis,,,,,,,,,04/Oct/11 13:22;jbellis;also durable writes,04/Oct/11 15:43;jbellis;+1,04/Oct/11 15:58;xedin;Committed.,"05/Oct/11 19:13;jbellis;Pavel, can you commit the relevant parts to 0.8 as well?  (My fault for not tagging this 0.8.7 originally.)","05/Oct/11 19:28;xedin;Done, that was only ""durable_writes"".","05/Oct/11 20:38;hudson;Integrated in Cassandra-0.8 #365 (See [https://builds.apache.org/job/Cassandra-0.8/365/])
    Fix missing fields in CLI `show schema` output
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3304

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1179395
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1.0 needs to clean out old-style hints,CASSANDRA-3291,12525391,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,01/Oct/11 02:28,12/Mar/19 14:04,13/Mar/19 22:26,01/Oct/11 05:41,1.0.0,,,,,,0,,,,,(Only marking this Minor because the manual workaround of deleting hint files is trivial.),,,,,,,,,,,,,,,,01/Oct/11 02:35;jbellis;3291.txt;https://issues.apache.org/jira/secure/attachment/12497264/3291.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-01 03:17:52.24,,,no_permission,,,,,,,,,,,,41760,,,Wed Oct 19 17:45:34 UTC 2011,,,,,,0|i0gi4f:,94366,patricioe,patricioe,,,,,,,,,"01/Oct/11 02:35;jbellis;Patch to update hint purging to check for a 1.0 marker instead of 0.7.

(Note that we do not need to check for both separately, since 1.0 doesn't care about 0.6-style hints any more than 0.7 did.)","01/Oct/11 03:17;patricioe;The patch looks good to me.

Just for the records, is there a need to check for 0.8 hints? (Don't remember if there was a major change in the structures between 0.8 and 1.0)","01/Oct/11 04:05;jbellis;Yes, the change was from 0.8 to 1.0.  So 1.0 just throws everything pre-1.0 away (after snapshot, just in case).",01/Oct/11 05:41;jbellis;committed,"19/Oct/11 16:45;yangyangyyy;I'm using the current HEAD of 1.0.0 github branch, and I'm still seeing this error, not sure if it's  this bug or another one.



 INFO [HintedHandoff:1] 2011-10-19 12:43:17,674 HintedHandOffManager.java (line 263) Started hinted handoff for token: 11342745564
0312821154458202477256070484 with IP: /10.39.85.140
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,885 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHan
doff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,886 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
","19/Oct/11 17:12;slebresne;Note sure what is the current HEAD of 1.0.0 given that branch as been removed in favor of the 1.0 one. But if it still points to the line
{noformat}
assert mutationColumn != null;
{noformat}
then I'm willing to bet it's another bug, since I believe the preceding assertions should have fired before this one if this was old-style hints. In which case, do you mind opening a new ticket?","19/Oct/11 17:45;yangyangyyy;created #3385, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
creating column family sets durable_writes to true,CASSANDRA-3292,12525396,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,01/Oct/11 07:00,12/Mar/19 14:04,13/Mar/19 22:26,19/Oct/11 13:22,0.8.8,1.0.1,,,,,0,schema,,,,"[default@rapidshare] describe keyspace rapidshare;
Keyspace: rapidshare:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: *false*
    Options: [datacenter1:1]
  Column Families:
[default@rapidshare] create column family t1;
1ba19300-ebfa-11e0-0000-34912694d0bf
Waiting for schema agreement...
... schemas agree across the cluster
[default@rapidshare] describe keyspace rapidshare;
Keyspace: rapidshare:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: *true*
    Options: [datacenter1:1]
  Column Families:
    ColumnFamily: t1
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.028124999999999997/1440/6 (millions of ops/minutes/MB)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []",,,,,,,,,,,,,,,,18/Oct/11 04:17;jbellis;0001-r-m-rename-migrations.patch;https://issues.apache.org/jira/secure/attachment/12499500/0001-r-m-rename-migrations.patch,18/Oct/11 04:17;jbellis;0002-clean-up-KSM.durableWrites.patch;https://issues.apache.org/jira/secure/attachment/12499501/0002-clean-up-KSM.durableWrites.patch,18/Oct/11 04:17;jbellis;0003-cloneWith.patch;https://issues.apache.org/jira/secure/attachment/12499502/0003-cloneWith.patch,18/Oct/11 04:17;jbellis;0004-update-tests-to-use-KSM.testMetadata.patch;https://issues.apache.org/jira/secure/attachment/12499503/0004-update-tests-to-use-KSM.testMetadata.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-10-01 07:52:28.942,,,no_permission,,,,,,,,,,,,41774,,,Wed Oct 19 13:22:50 UTC 2011,,,,,,0|i0gi4v:,94368,xedin,xedin,,,,,,,,,"01/Oct/11 07:52;satishbabu;I can re-pro in the following version too
 INFO 00:32:55,237 Cassandra version: 1.0.0-rc1-SNAPSHOT
 INFO 00:32:55,237 Thrift API version: 19.18.0
Satish","03/Oct/11 19:23;jbellis;01: removes the unused rename CF/KS code

02: adds KSM.cloneWith for migrations to use that will be futureproof against new constructors being added",03/Oct/11 19:25;jbellis;(patch is against trunk),17/Oct/11 16:51;jbellis;rebased 02,"17/Oct/11 20:28;xedin;you forgot to remove Table.renameCf method and some of the tests are failing
{noformat}
    [junit] Testsuite: org.apache.cassandra.db.ReadMessageTest
    [junit] Tests run: 3, Failures: 1, Errors: 0, Time elapsed: 0,599 sec
    [junit] 
    [junit] Testcase: testNoCommitLog(org.apache.cassandra.db.ReadMessageTest):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.cassandra.db.ReadMessageTest.testNoCommitLog(ReadMessageTest.java:135)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.ReadMessageTest FAILED
    [junit] Testsuite: org.apache.cassandra.db.RecoveryManager2Test
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0,669 sec
    [junit] 
    [junit] Testcase: testWithFlush(org.apache.cassandra.db.RecoveryManager2Test):	FAILED
    [junit] Expecting only 1 replayed mutation, got 0
    [junit] junit.framework.AssertionFailedError: Expecting only 1 replayed mutation, got 0
    [junit] 	at org.apache.cassandra.db.RecoveryManager2Test.testWithFlush(RecoveryManager2Test.java:68)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.RecoveryManager2Test FAILED
    [junit] Testsuite: org.apache.cassandra.db.RecoveryManager3Test
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0,621 sec
    [junit] 
    [junit] Testcase: testMissingHeader(org.apache.cassandra.db.RecoveryManager3Test):	FAILED
    [junit] Columns [(as string: )])] is not expected [col1]
    [junit] junit.framework.AssertionFailedError: Columns [(as string: )])] is not expected [col1]
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:553)
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:514)
    [junit] 	at org.apache.cassandra.db.RecoveryManager3Test.testMissingHeader(RecoveryManager3Test.java:76)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.RecoveryManager3Test FAILED
    [junit] Testsuite: org.apache.cassandra.db.RecoveryManagerTest
    [junit] Tests run: 3, Failures: 1, Errors: 1, Time elapsed: 0,611 sec
    [junit] 
    [junit] Testcase: testOne(org.apache.cassandra.db.RecoveryManagerTest):	FAILED
    [junit] Columns [(as string: )])] is not expected [col1]
    [junit] junit.framework.AssertionFailedError: Columns [(as string: )])] is not expected [col1]
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:553)
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:514)
    [junit] 	at org.apache.cassandra.db.RecoveryManagerTest.testOne(RecoveryManagerTest.java:70)
    [junit] 
    [junit] 
    [junit] Testcase: testRecoverCounter(org.apache.cassandra.db.RecoveryManagerTest):	Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.cassandra.db.RecoveryManagerTest.testRecoverCounter(RecoveryManagerTest.java:99)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.RecoveryManagerTest FAILED
{noformat}

Otherwise it seems to be the way to go :)",18/Oct/11 04:17;jbellis;updated w/ above fixes,"18/Oct/11 16:49;xedin;Please don't forget to remove Table.renameCf(...) method when you will be committing, +1.","18/Oct/11 17:17;jbellis;Yes, that's part of the new 01.","18/Oct/11 17:24;xedin;Sorry, seems like it didn't override 0001 for me, see it now, +1.","18/Oct/11 18:22;hudson;Integrated in Cassandra-0.8 #377 (See [https://builds.apache.org/job/Cassandra-0.8/377/])
    r/m obsolete CF/KS rename code
patch by jbellis; reviewed by pyaskevich for CASSANDRA-3292

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1185761
Files : 
* /cassandra/branches/cassandra-0.8/src/avro/internode.genavro
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/Table.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/RenameColumnFamily.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/RenameKeyspace.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/DefsTest.java
","19/Oct/11 07:01;hudson;Integrated in Cassandra-0.8 #380 (See [https://builds.apache.org/job/Cassandra-0.8/380/])
    finish fixing changing durable_writes keyspace option during CF creation
patch by jbellis; reviewed by pyaskevich for CASSANDRA-3292
add KSM.systemKeyspace and cloneWith methods
patch by jbellis; reviewed by pyaskevich for CASSANDRA-3292

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1185963
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/KSMetaData.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/SchemaLoader.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/config/DatabaseDescriptorTest.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/DefsTest.java

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1185961
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/KSMetaData.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/AddColumnFamily.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/DropColumnFamily.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/UpdateKeyspace.java
",19/Oct/11 13:22;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
help create column family refers to outdated fields,CASSANDRA-3284,12525288,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,hsn,hsn,hsn,30/Sep/11 10:25,12/Mar/19 14:04,13/Mar/19 22:26,30/Sep/11 20:46,1.0.0,,,Legacy/Tools,,,0,,,,,"help create column family in cassandra-cli refers to old, unsupported options.

- memtable_operations: Number of operations in millions before the memtable
  is flushed. Default is memtable_throughput / 64 * 0.3

- memtable_throughput: Maximum size in MB to let a memtable get to before
  it is flushed. Default is to use 1/16 the JVM heap size.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-09-30 20:46:09.281,,,no_permission,,,,,,,,,,,,40994,,,Fri Sep 30 20:46:09 UTC 2011,,,,,,0|i0gi13:,94351,jbellis,jbellis,,,,,,,,,"30/Sep/11 20:46;jbellis;fixed in r1177825, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disabling hinted handoff counterintuitively continues to log handoff messages,CASSANDRA-3176,12522664,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jeromatron,jeromatron,10/Sep/11 23:20,12/Mar/19 14:04,13/Mar/19 22:26,01/Oct/11 15:17,1.0.0,,,,,,0,HintedHandoff,,,,"In order to test a theory, we tried to disable hinted handoff on our cluster.  We updated all of the yaml files and then restarted all the nodes in our cluster.  However we continue to get messages such as this in the logs after restarting:
{quote}
INFO [HintedHandoff:1] 2011-09-10 22:41:40,813 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.1.2.3
INFO [HintedHandoff:1] 2011-09-10 22:41:40,813 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.1.2.3
INFO [HintedHandoff:1] 2011-09-10 22:41:45,025 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.2.3.4
INFO [HintedHandoff:1] 2011-09-10 22:41:45,026 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.2.3.4
INFO [HintedHandoff:1] 2011-09-10 22:42:10,017 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.3.4.5
INFO [HintedHandoff:1] 2011-09-10 22:42:10,017 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.3.4.5
{quote}

Also looking at the System.HintsColumnFamily in jmx there is activity there such as pending tasks that come and go.",,,,,,,,,,,,,,,,01/Oct/11 05:55;jbellis;3176.txt;https://issues.apache.org/jira/secure/attachment/12497268/3176.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-10 23:28:23.489,,,no_permission,,,,,,,,,,,,4039,,,Sat Oct 01 15:17:40 UTC 2011,,,,,,0|i0ggq7:,94140,slebresne,slebresne,,,,,,,,,"10/Sep/11 23:28;brandon.williams;Disabling HH means hints won't be written, not that existing hints won't be delivered.","10/Sep/11 23:53;jeromatron;Sounds like
1) the logging happens even when it's disabled.
2) I double checked in the StorageProxy.HintedHandoffEnabled and it is set to false
3) there were probably some residual hints that were never delivered before disabling HH.",01/Oct/11 05:55;jbellis;This isn't the first time the continued handoff messages post-disable have confused someone.  Patch to avoid these is attached.,"01/Oct/11 15:17;slebresne;+1, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to delete -Index files if index is currently building,CASSANDRA-3314,12525770,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,05/Oct/11 07:31,12/Mar/19 14:04,13/Mar/19 22:26,19/Oct/11 13:33,1.0.1,,,,,,0,compaction,indexing,,,"If there is index building in progress, following errors are thrown if cassandra is trying to delete *-Index.db files. There is no problem with deleting -Data or -Filter.. files. CF is using leveled compaction but it is probably not related.

ERROR [NonPeriodicTasks:1] 2011-10-05 09:13:03,702 AbstractCassandraDaemon.java
(line 133) Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.RuntimeException: java.io.IOException: Failed to delete C:\var\lib\cas
sandra\data\test\sipdb-h-772-Index.db
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:3
4)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.
access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.
run(ScheduledThreadPoolExecutor.java:206)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:662)",,,,,,,,,,,,,,,,18/Oct/11 11:34;jbellis;0001-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12499533/0001-cleanup.patch,18/Oct/11 11:34;jbellis;0002-acquire-references.patch;https://issues.apache.org/jira/secure/attachment/12499534/0002-acquire-references.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-05 18:52:20.331,,,no_permission,,,,,,,,,,,,46319,,,Wed Oct 19 13:33:49 UTC 2011,,,,,,0|i0gien:,94412,slebresne,slebresne,,,,,,,,,"05/Oct/11 18:52;jbellis;Not sure how this happens, because submitIndexBuild does grab the compaction lock, so schema changes should be blocked until it's done.","06/Oct/11 15:12;xedin;Can you please provider bigger exception or cassandra log and tell what disk access mode do you use for primary index: mmap or standard? 

My guess if that SSTableDeletingTask can't delete -Index.db component on Windows because it has open file descriptor somewhere...","06/Oct/11 16:23;hsn;access mode standard. i dont have that log, but it should be easy to reproduce.",06/Oct/11 19:18;xedin;Can you try with disk_access_mode: mmap to check if this is really a problem with descriptors leaking?,"07/Oct/11 13:18;hsn;with mmap mode, i dont see errors","07/Oct/11 17:49;xedin;Interesting thing that I can't reproduce this behavior and don't see in the code what can cause it, can you please provide steps to reproduce?","07/Oct/11 18:03;jbellis;Also, would be good to know if you can reproduce in latest 0.8, as I would expect.",07/Oct/11 18:07;xedin;Tried on both latest 0.8 and 1.0.0 with disk_access_mode: standard and with/without secondary indexes (used stress.java to populate system with data and nodetool to run different operations) and couldn't reproduce... ,"07/Oct/11 21:22;hsn;load table with data ( 2 m rows used). start index build, it will take a while. During index building run read/write test which will write enough data to trigger table compaction. i didnt tested it on 0.8","07/Oct/11 21:33;hsn;Here is larger segment of log, since index building started. I can reproduce it any time. Ignore bug _Nothing to compact in sipdb_, its not related. Its other leveldb compaction bug.

 INFO [MigrationStage:1] 2011-10-07 23:24:35,293 Migration.java (line 119) Applying migration c12c20d0-f12a-11e0-0000-23b38323f4da Update column family to org.apache.cassandra.config.CFMetaData@7c401b[cfId=1000,ksName=test,cfName=sipdb,cfType=Standard,comparator=org.apache.cassandra.db.marshal.AsciiType,subcolumncomparator=<null>,comment=phone calls routing information,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=0.0,replicateOnWrite=false,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.Int32Type,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@154c995,mergeShardsChance=0.1,keyAlias=java.nio.HeapByteBuffer[pos=478 lim=481 cap=483],column_metadata={java.nio.HeapByteBuffer[pos=0 lim=3 cap=3]=ColumnDefinition{name=6b616d, validator=org.apache.cassandra.db.marshal.AsciiType, index_type=KEYS, index_name='kam'}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO [MigrationStage:1] 2011-10-07 23:24:35,309 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Migrations@25069868(7870/9837 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2011-10-07 23:24:35,309 Memtable.java (line 237) Writing Memtable-Migrations@25069868(7870/9837 serialized/live bytes, 1 ops)
 INFO [MigrationStage:1] 2011-10-07 23:24:35,309 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Schema@3067216(3325/4156 serialized/live bytes, 3 ops)
 INFO [FlushWriter:4] 2011-10-07 23:24:35,356 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Migrations-h-17-Data.db (7934 bytes)
 INFO [FlushWriter:4] 2011-10-07 23:24:35,371 Memtable.java (line 237) Writing Memtable-Schema@3067216(3325/4156 serialized/live bytes, 3 ops)
 INFO [FlushWriter:4] 2011-10-07 23:24:35,418 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Schema-h-17-Data.db (3475 bytes)
 INFO [MigrationStage:1] 2011-10-07 23:24:35,418 SecondaryIndexManager.java (line 181) Creating new index : ColumnDefinition{name=6b616d, validator=org.apache.cassandra.db.marshal.AsciiType, index_type=KEYS, index_name='kam'}
 INFO [Creating index: sipdb.kam] 2011-10-07 23:24:35,449 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-sipdb@30231056(2284899/30490286 serialized/live bytes, 42052 ops)
 INFO [FlushWriter:4] 2011-10-07 23:24:35,449 Memtable.java (line 237) Writing Memtable-sipdb@30231056(2284899/30490286 serialized/live bytes, 42052 ops)
 INFO [FlushWriter:4] 2011-10-07 23:24:36,262 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\test\sipdb-h-1049-Data.db (4506422 bytes)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:36,262 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [Creating index: sipdb.kam] 2011-10-07 23:24:36,262 SecondaryIndex.java (line 145) Submitting index build of sipdb.kam for data in SSTableReader(path='\var\lib\cassandra\data\test\sipdb-h-1047-Data.db'), SSTableReader(path='\var\lib\cassandra\data\test\sipdb-h-1049-Data.db')
 INFO [CompactionExecutor:3] 2011-10-07 23:24:36,262 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [pool-1-thread-1] 2011-10-07 23:24:36,403 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.2837528604119 (just-counted was 36.2837528604119).  calculation took 16ms for 67 columns
 INFO [pool-1-thread-1] 2011-10-07 23:24:36,496 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.2837528604119 (just-counted was 35.62471395881007).  calculation took 15ms for 132 columns
 INFO [pool-1-thread-1] 2011-10-07 23:24:36,668 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.2837528604119 (just-counted was 35.51029748283753).  calculation took 31ms for 263 columns
 INFO [pool-1-thread-1] 2011-10-07 23:24:37,043 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.286453839516824 (just-counted was 36.286453839516824).  calculation took 62ms for 535 columns
 INFO [pool-1-thread-1] 2011-10-07 23:24:37,746 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.286453839516824 (just-counted was 36.11138755980861).  calculation took 109ms for 1065 columns
 INFO [pool-1-thread-1] 2011-10-07 23:24:39,137 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.286453839516824 (just-counted was 36.1418534947848).  calculation took 219ms for 2127 columns
 INFO [CompactionExecutor:3] 2011-10-07 23:24:39,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:39,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [pool-1-thread-1] 2011-10-07 23:24:42,012 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.286453839516824 (just-counted was 35.84278599835191).  calculation took 437ms for 4235 columns
 INFO [CompactionExecutor:3] 2011-10-07 23:24:44,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:44,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [pool-1-thread-1] 2011-10-07 23:24:47,403 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.286453839516824 (just-counted was 36.06701179177452).  calculation took 875ms for 8500 columns
 INFO [CompactionExecutor:3] 2011-10-07 23:24:49,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:49,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:54,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:54,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [pool-1-thread-1] 2011-10-07 23:24:58,512 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.286453839516824 (just-counted was 36.10818295381003).  calculation took 1750ms for 16940 columns
 INFO [CompactionExecutor:3] 2011-10-07 23:24:59,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:59,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:04,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:04,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:09,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:09,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:14,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:14,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:19,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:19,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:24,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:24,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:29,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:29,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [pool-1-thread-1] 2011-10-07 23:25:33,279 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 37.129730574542535 (just-counted was 37.129730574542535).  calculation took 3656ms for 32878 columns
 INFO [CompactionExecutor:3] 2011-10-07 23:25:34,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:34,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:39,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:39,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:44,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:44,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:49,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:49,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:54,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:54,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:59,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:59,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:04,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:04,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:09,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:09,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:14,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:14,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:19,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:19,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:24,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:24,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [OptionalTasks:1] 2011-10-07 23:26:25,483 MeteredFlusher.java (line 62) flushing high-traffic column family ColumnFamilyStore(table='test', columnFamily='sipdb') (estimated 66493149 bytes)
 INFO [OptionalTasks:1] 2011-10-07 23:26:25,483 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-sipdb.kam@22387768(1290429/59891601 serialized/live bytes, 65999 ops)
 INFO [OptionalTasks:1] 2011-10-07 23:26:25,483 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-sipdb@14468404(495177/6607770 serialized/live bytes, 9112 ops)
 INFO [FlushWriter:5] 2011-10-07 23:26:25,499 Memtable.java (line 237) Writing Memtable-sipdb.kam@22387768(1290429/59891601 serialized/live bytes, 65999 ops)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:29,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:29,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [FlushWriter:5] 2011-10-07 23:26:34,234 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\test\sipdb.kam-h-1-Data.db (6963969 bytes)
 INFO [FlushWriter:5] 2011-10-07 23:26:34,234 Memtable.java (line 237) Writing Memtable-sipdb@14468404(495177/6607770 serialized/live bytes, 9112 ops)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:34,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:34,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [FlushWriter:5] 2011-10-07 23:26:35,765 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\test\sipdb-h-1050-Data.db (984842 bytes)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:35,765 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:35,781 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\test\sipdb-h-1049-Data.db'), SSTableReader(path='\var\lib\cassandra\data\test\sipdb-h-1050-Data.db')]
 INFO [pool-1-thread-1] 2011-10-07 23:26:37,374 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 37.129730574542535 (just-counted was 36.844857020417244).  calculation took 12578ms for 65753 columns
 INFO [CompactionExecutor:3] 2011-10-07 23:26:42,078 CompactionTask.java (line 222) Compacted to [\var\lib\cassandra\data\test\sipdb-h-1051-Data.db,].  5 491 264 to 5 468 836 (~99% of original) bytes for 50 480 keys at 0,828250MBPS.  Time: 6 297ms.
 INFO [CompactionExecutor:3] 2011-10-07 23:26:42,078 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:42,078 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:42,078 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:42,093 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
ERROR [NonPeriodicTasks:1] 2011-10-07 23:26:42,140 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.RuntimeException: java.io.IOException: Failed to delete C:\var\lib\cassandra\data\test\sipdb-h-1049-Index.db
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Failed to delete C:\var\lib\cassandra\data\test\sipdb-h-1049-Index.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
	at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:138)
	at org.apache.cassandra.io.sstable.SSTableDeletingTask.runMayThrow(SSTableDeletingTask.java:81)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 8 more
","17/Oct/11 17:49;jbellis;01 does some cleanup around index building, including adding docstrings to make clear when references need to be acquired on sstables involved.

02 adds the missing reference acquisitions for new index creation and loadNewSSTables.","18/Oct/11 08:14;slebresne;There seems to be a mistake with the attached patch, or at least they don't correspond to the description. First patch don't really add any doscstrings and the second patch doesn't add any reference acquisitions.",18/Oct/11 11:34;jbellis;correct patches attached.,19/Oct/11 10:32;slebresne;The first patch removes the import of Lock in CompactionManager which prevents compilation. But otherwise +1.,19/Oct/11 13:33;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
need initClause when catch Exception and throw new Exception in cli,CASSANDRA-3312,12525758,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,satishbabu,cywjackson,cywjackson,05/Oct/11 02:22,12/Mar/19 14:04,13/Mar/19 22:26,05/Oct/11 11:14,0.8.7,,,,,,0,,,,,"through CASSANDRA-2746 , we added initCause to the Cli such that we could see more meaningful exception stacktrace when certain exception is thrown.

However, there are still some other area, eg:

executeGetWithConditions(Tree)
executeSet(Tree)
executeIncr(Tree, long)
etc etc...

basically any time you do a
{code}
            {
                throw new RuntimeException(e.getMessage());
            }
{code}

the real exception is lost. The right approach should be:

{code}
        catch (Exception e)
        {
            throw new RuntimeException(e.getMessage(), e);
        }
{code}

eg: i was getting this:
null
java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:310)
        at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:217)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:345)
Caused by: java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeGetWithConditions(CliClient.java:815)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:208)
        ... 2 more


but i have no idea what the problem is with just the above stack trace.

with the fix, this would tell us more:
null
java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:310)
        at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:217)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:345)
Caused by: java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeGetWithConditions(CliClient.java:815)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:208)
        ... 2 more
Caused by: UnavailableException()
        at org.apache.cassandra.thrift.Cassandra$get_indexed_slices_result.read(Cassandra.java:14065)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_indexed_slices(Cassandra.java:810)
        at org.apache.cassandra.thrift.Cassandra$Client.get_indexed_slices(Cassandra.java:782)
        at org.apache.cassandra.cli.CliClient.executeGetWithConditions(CliClient.java:806)
        ... 3 more",,,,,,,,,,,,,,,,05/Oct/11 07:01;satishbabu;3312.patch;https://issues.apache.org/jira/secure/attachment/12497764/3312.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-05 02:35:06.611,,,no_permission,,,,,,,,,,,,46284,,,Wed Oct 05 12:22:54 UTC 2011,,,,,,0|i0gidj:,94407,xedin,xedin,,,,,,,,,"05/Oct/11 02:35;jbellis;I think throw new RTE(e) would be fine, btw.",05/Oct/11 07:01;satishbabu;Added patch to support more exceptional details,05/Oct/11 07:23;jbellis;Tagging 0.8.7 because I *think* it's present there too.,"05/Oct/11 11:14;xedin;Committed, thanks!","05/Oct/11 12:22;hudson;Integrated in Cassandra-0.8 #362 (See [https://builds.apache.org/job/Cassandra-0.8/362/])
    Improved CLI exceptions
patch by satishbabu; reviewed by xedin for CASSANDRA-3312

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1179164
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to delete after running scrub,CASSANDRA-3318,12525837,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,05/Oct/11 14:21,12/Mar/19 14:04,13/Mar/19 22:26,06/Oct/11 15:22,1.0.0,,,,,,0,,,,,"Another problem with sstable deletions on 1.0. Running scrub produces lot of unable to delete messages on windows.

ERROR 16:16:37,562 Unable to delete \var\lib\cassandra\data\test\sipdb-h-711-Dat
a.db (it will be removed on server restart; we'll also retry after GC)
 INFO 16:16:37,577 Scrub of SSTableReader(path='\var\lib\cassandra\data\test\sip
db-h-711-Data.db') complete: 48396 rows in new sstable and 0 empty (tombstoned)
rows dropped",,,,,,,,,,,,,,,,05/Oct/11 21:50;jbellis;3318.txt;https://issues.apache.org/jira/secure/attachment/12497905/3318.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-05 18:41:17.526,,,no_permission,,,,,,,,,,,,46461,,,Thu Oct 06 15:22:05 UTC 2011,,,,,,0|i0gigf:,94420,slebresne,slebresne,,,,,,,,,05/Oct/11 18:41;jbellis;Is 711 a pre-scrub or post-scrub sstable?,05/Oct/11 18:49;jbellis;Suspect this also affects 0.7 and 0.8.,"05/Oct/11 21:08;hsn; INFO 23:05:39,845 Scrub of SSTableReader(path='\var\lib\cassandra\data\system\S
chema-h-10-Data.db') complete: 8 rows in new sstable and 0 empty (tombstoned) ro
ws dropped
ERROR 23:05:39,845 Unable to delete \var\lib\cassandra\data\system\Schema-h-10-D
ata.db (it will be removed on server restart; we'll also retry after GC)

its table after scrub is finished.","05/Oct/11 21:50;jbellis;that means it's the sstable from before scrub.

patch attached to close the readers before replacing the old sstable with the new one.

(the diff is intimidating but it's mostly indentation changes from combining two try/finally blocks.  recommend using a ""smart"" diff tool like intellij's, post-patch application.)",05/Oct/11 21:51;jbellis;(patch is against trunk),06/Oct/11 07:24;slebresne;+1,"06/Oct/11 15:20;jbellis;I'm only going to commit this to 1.0 after all, because before that our PhantomReference-based deleting is very unlikely to run into this problem, so I'm going to let the stable releases stay stable.",06/Oct/11 15:22;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dropping index causes some inflight mutations to fail,CASSANDRA-3334,12526275,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,07/Oct/11 21:20,12/Mar/19 14:04,13/Mar/19 22:26,20/Oct/11 21:13,1.0.1,,,,,,0,indexing,,,,"dropping index causes some inflight mutations to fail. hector on client side didnt throw any exception

 INFO [MigrationStage:1] 2011-10-07 23:11:53,742 Migration.java (line 119) Applying migration fb1a8540-f128-11e0-0000-23b38323f4da Update column family to org.apache.cassandra.config.CFMetaData@786669[cfId=1000,ksName=test,cfName=sipdb,cfType=Standard,comparator=org.apache.cassandra.db.marshal.AsciiType,subcolumncomparator=<null>,comment=phone calls routing information,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=0.0,replicateOnWrite=false,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.Int32Type,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@8bb33c,mergeShardsChance=0.1,keyAlias=java.nio.HeapByteBuffer[pos=461 lim=464 cap=466],column_metadata={java.nio.HeapByteBuffer[pos=0 lim=3 cap=3]=ColumnDefinition{name=6b616d, validator=org.apache.cassandra.db.marshal.AsciiType, index_type=null, index_name='null'}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO [MigrationStage:1] 2011-10-07 23:11:53,805 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Migrations@27537043(7860/9825 serialized/live bytes, 1 ops)
 INFO [MigrationStage:1] 2011-10-07 23:11:53,820 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Schema@8340427(3320/4150 serialized/live bytes, 3 ops)
 INFO [FlushWriter:3] 2011-10-07 23:11:53,820 Memtable.java (line 237) Writing Memtable-Migrations@27537043(7860/9825 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2011-10-07 23:11:55,008 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Migrations-h-14-Data.db (7924 bytes)
 INFO [FlushWriter:3] 2011-10-07 23:11:55,008 Memtable.java (line 237) Writing Memtable-Schema@8340427(3320/4150 serialized/live bytes, 3 ops)
 INFO [CompactionExecutor:4] 2011-10-07 23:11:55,008 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-13-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-14-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-11-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-12-Data.db')]
 INFO [FlushWriter:3] 2011-10-07 23:11:56,430 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Schema-h-14-Data.db (3470 bytes)
 INFO [CompactionExecutor:3] 2011-10-07 23:11:56,446 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-13-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-14-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-12-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-11-Data.db')]
ERROR [MutationStage:56] 2011-10-07 23:11:56,508 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:56,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:51] 2011-10-07 23:11:56,539 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:51,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:38] 2011-10-07 23:11:56,633 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:38,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:57] 2011-10-07 23:11:56,664 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:57,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",windows,,,,,,,,,,,,,,,15/Oct/11 03:16;jbellis;0001-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12499122/0001-cleanup.patch,15/Oct/11 03:16;jbellis;0002-fix.patch;https://issues.apache.org/jira/secure/attachment/12499123/0002-fix.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-15 03:16:44.969,,,no_permission,,,,,,,,,,,,50524,,,Thu Oct 20 21:03:56 UTC 2011,,,,,,0|i0gimv:,94449,slebresne,slebresne,,,,,,,,,"15/Oct/11 03:16;jbellis;Patch to fix attached, as well as patch to clean up the SecondaryIndexManager a bit.",19/Oct/11 16:50;slebresne;+1,20/Oct/11 21:03;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThreadPoolExecutor creates threads as non-daemon and will block on shutdown by default,CASSANDRA-3335,12526280,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,brandon.williams,brandon.williams,07/Oct/11 21:40,12/Mar/19 14:04,13/Mar/19 22:26,20/Dec/11 20:41,1.0.7,,,,,,0,,,,,"This is most obviously visible in OptionalTasks which should not block shutdown, but often does.",,,,,,,,,,,,,,,,14/Oct/11 22:31;jbellis;3335-v2.txt;https://issues.apache.org/jira/secure/attachment/12499105/3335-v2.txt,10/Dec/11 19:29;jbellis;3335-v3.txt;https://issues.apache.org/jira/secure/attachment/12506864/3335-v3.txt,16/Dec/11 04:47;jbellis;3335-v4.txt;https://issues.apache.org/jira/secure/attachment/12507648/3335-v4.txt,07/Oct/11 22:00;jbellis;3335.txt;https://issues.apache.org/jira/secure/attachment/12498247/3335.txt,12/Dec/11 19:41;brandon.williams;3335v3_jstack.txt;https://issues.apache.org/jira/secure/attachment/12507042/3335v3_jstack.txt,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-10-07 22:00:09.465,,,no_permission,,,,,,,,,,,,50529,,,Tue Dec 20 20:41:22 UTC 2011,,,,,,0|i0ginb:,94451,brandon.williams,brandon.williams,,,,,,,,,"07/Oct/11 22:00;jbellis;Patch that switches DTPE to daemon threads, and Memtable's jamm TPE to DTPE. The only other direct use of TPE is the thrift handler for sync mode in ACD, which doesn't seem to have a problem killing things off when we ask it to so I left it alone.","10/Oct/11 18:33;brandon.williams;+1 on this patch for being technically correct, but it actually does not help.  What I see is OptionalTasks jump to 100% on control-c, and then eventually it spins down and a few minutes later java finally exits.  During the last bit, I can see jamm still working.  I'm not sure why java is taking so long to exit, but it is responsive to nodetool the entire time, though tpstats shows nothing active.",14/Oct/11 22:31;jbellis;v2 adds a log line before running any scheduled task. Maybe that will help pinpoint the culprit.,14/Oct/11 22:32;jbellis;(all the task logging is done w/ the StorageService logger so it can be enabled separately from the reset of the package involved),"01/Dec/11 22:07;brandon.williams;Tracked down to the shutdown hook:

{noformat}
   java.lang.Thread.State: TIMED_WAITING (sleeping)
    at java.lang.Thread.sleep(Native Method)
    at org.apache.cassandra.utils.ExpiringMap.shutdown(ExpiringMap.java:103)
    at org.apache.cassandra.net.MessagingService.shutdown(MessagingService.java:495)
    at org.apache.cassandra.service.StorageService$2.runMayThrow(StorageService.java:426)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    at java.lang.Thread.run(Thread.java:662)
{noformat}

Appears to be the ExpiringMap added in CASSANDRA-2034.",10/Dec/11 19:28;jbellis;Patch to stop adding new messages to the callback map when MessagingService is shutdown.,12/Dec/11 19:41;brandon.williams;Doesn't seem to help.  Thread dump attached.,"16/Dec/11 04:47;jbellis;Okay, take four here.  The problem with v3 is that we were only blocking sendOneWay during shutdown, not addCallback, which is the source of the ExpiringMap entries we were waiting for.

As I commented,

{noformat}
    /**
     * There isn't a good way to shut down the MessagingService. One problem (but not the only one)
     * is that StorageProxy has no way to communicate back to clients, ""I'm nominally alive, but I can't
     * send that request to the nodes with your data.""  Neither TimedOut nor Unavailable is appropriate
     * to return in that situation.
     *
     * So instead of shutting down MS and letting StorageProxy/clients cope somehow, we shut down
     * the Thrift service and then wait for all the outstanding requests to finish or timeout.
     */
{noformat}

That part was straightforward.  I also had to make the Thrift shutdown actually work -- we were calling setSoTimeout to attempt to make accept() nonblocking, but ""0"" means ""wait indefinitely"" not ""don't wait at all"".  Then we needed to handle the timeout in the accept loop.

Finally, I did a bunch of cleanup to ExpiringMap and added trace-level logging in case we need to go at this *again*.


",20/Dec/11 18:56;brandon.williams;+1,20/Dec/11 20:41;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction throttling can be too slow,CASSANDRA-3344,12526640,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,frousseau,frousseau,11/Oct/11 10:29,12/Mar/19 14:04,13/Mar/19 22:26,14/Oct/11 08:55,0.8.8,1.0.1,,,,,0,,,,,"Compaction throttling needs to know how many active compactions are running (to divide bandwith for each active compaction).

The way active compaction is counted can be broken because it counts the number of active threads in the executor BUT the thread starts by acquiring a lock.
If the lock can't be acquired immediately : the thread is seen as ""active"" but does not participate in IO operations.
The case can happen when major compaction are triggered (major compaction acquire a write lock, while minor compactions acquire a read lock).

Having compaction througput to 16Mb/s, we observed is the following  (two times) :
 - only 1 active compaction (a long one for a few hours) starting at 16Mb/s, then after some time running at 2Mb/s, thus taking a very long time to complete
 - many pending compactions

Using JMX and monitoring the stack trace of the compaction threads showed that :
 - 1 thread was effectively compacting
 - 1 thread was waiting to acquire the write lock (due to a major compaction)
 - 6 threads were waiting to acquire the read lock (probably due to the thread above trying to acquire the write lock)

Attached is a proposed patch (very simple, not yet tested) which counts only active compactions.

",,,,,,,,,,,,,,,,11/Oct/11 10:31;frousseau;001-CASSANDRA-3344.patch;https://issues.apache.org/jira/secure/attachment/12498568/001-CASSANDRA-3344.patch,11/Oct/11 12:09;slebresne;3344.patch;https://issues.apache.org/jira/secure/attachment/12498586/3344.patch,11/Oct/11 12:27;slebresne;3344_v2.patch;https://issues.apache.org/jira/secure/attachment/12498587/3344_v2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-10-11 11:50:29.496,,,no_permission,,,,,,,,,,,,59005,,,Fri Oct 14 09:21:48 UTC 2011,,,,,,0|i0girb:,94469,jbellis,jbellis,,,,,,,,,"11/Oct/11 11:50;slebresne;Good diagnostic, I think you are right that this can happen. However the patch seems to replace the body of getActiveCompactions() by a recursive call to itself.",11/Oct/11 12:09;slebresne;Attaching a simple patch to add a counter that we only increment once we really do compaction.,"11/Oct/11 12:19;frousseau;Thanks for catching that.
The intent was to return the size of the CompactionExecutor.compactions list
","11/Oct/11 12:27;slebresne;Right, that's indeed even simpler. Attaching v2 to do that instead.",14/Oct/11 02:18;jbellis;+1 v2,"14/Oct/11 08:55;slebresne;Committed, thanks","14/Oct/11 09:21;hudson;Integrated in Cassandra-0.8 #372 (See [https://builds.apache.org/job/Cassandra-0.8/372/])
    Only count compaction as active (for throttling) once the compaction lock has been acquired.
patch by Fabien Rousseau and slebresne; reviewed by jbellis for CASSANDRA-3344

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1183241
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during nodetool repair,CASSANDRA-3400,12528521,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,scottfines,scottfines,24/Oct/11 16:41,12/Mar/19 14:04,13/Mar/19 22:26,25/Oct/11 15:52,0.8.8,1.0.1,,,,,0,,,,,"When running a nodetool repair, the following exception can be thrown:


ERROR [AntiEntropySessions:12] 2011-10-24 11:17:52,154 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[AntiEntropySessions:12,5,RMI Runtime]
java.lang.RuntimeException: java.util.ConcurrentModificationException
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.ConcurrentModificationException
at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
at java.util.HashMap$KeyIterator.next(HashMap.java:828)
at org.apache.cassandra.service.AntiEntropyService$RepairSession$RepairJob.sendTreeRequests(AntiEntropyService.java:784)
at org.apache.cassandra.service.AntiEntropyService$RepairSession.runMayThrow(AntiEntropyService.java:680)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
... 6 more
",Suse Enterprise linux 11.4,,,,,,,,,,,,,,,25/Oct/11 08:47;slebresne;3400.patch;https://issues.apache.org/jira/secure/attachment/12500632/3400.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-25 08:47:42.017,,,no_permission,,,,,,,,,,,,214381,,,Tue Oct 25 18:07:07 UTC 2011,,,,,,0|i0gjfj:,94578,jbellis,jbellis,,,,,,,,,"24/Oct/11 16:48;scottfines;A few notes as I discover them:

This error occurs every time the nodetool runs repair--repair effectively does not work on this node now.

","25/Oct/11 08:47;slebresne;There is a race between when we send requests and when start handling their result, i.e if a response comes before all the requests have been sent. It's fairly unlikely to happen imho but given than one request is local, if there is no and barely any data to build the merkle tree from, I suppose it's possible the local response comes quickly enough.

Patch attached to resolve the issue. ","25/Oct/11 14:05;jbellis;+1

nit: we never use thread.interrupt() so custom is to throw AssertionError for InterruptedException",25/Oct/11 15:52;slebresne;Committed (with the AssertionError change),"25/Oct/11 18:07;hudson;Integrated in Cassandra-0.8 #388 (See [https://builds.apache.org/job/Cassandra-0.8/388/])
    Fix race in AntiEntropyService
patch by slebresne; reviewed by jbellis for CASSANDRA-3400

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1188740
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inconsistent rejection of CL.ANY on reads,CASSANDRA-3410,12529076,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,27/Oct/11 16:32,12/Mar/19 14:04,13/Mar/19 22:26,07/Nov/11 17:31,1.1.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,27/Oct/11 16:32;jbellis;3410.txt;https://issues.apache.org/jira/secure/attachment/12501113/3410.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-07 16:55:40.554,,,no_permission,,,,,,,,,,,,214936,,,Mon Nov 07 18:24:30 UTC 2011,,,,,,0|i0gjjz:,94598,slebresne,slebresne,,,,,,,,,07/Nov/11 16:55;slebresne;+1,07/Nov/11 17:31;jbellis;committed,"07/Nov/11 18:24;hudson;Integrated in Cassandra #1191 (See [https://builds.apache.org/job/Cassandra/1191/])
    reject CL.ANY range scans as well as single/multi-row gets
patch by jbellis; reviewed by slebresne for CASSANDRA-3410

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1198828
Files : 
* /cassandra/trunk/NEWS.txt
* /cassandra/trunk/src/java/org/apache/cassandra/service/ReadCallback.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/ThriftValidation.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem markers don't show up in Eclipse,CASSANDRA-3397,12528394,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dallsopp,dallsopp,dallsopp,22/Oct/11 21:25,12/Mar/19 14:04,13/Mar/19 22:26,23/Dec/11 22:12,1.0.7,,,Packaging,,,0,ant,eclipse,ide,,"The generated Eclipse files install an Ant Builder to build Cassandra within Eclipse. This appears to mean that the default Java Builder is not present. This means that no problem markers show up in the Problem view or the Package Explorer etc when there are compiler errors or warnings  - you have to study the console output, then navigate manually to the sources of the problems, which is very tedious.

It seems to be possible to re-install the default Java Builder in parallel with the Ant Builder, getting the best of both worlds. I have documented this on the wiki at http://wiki.apache.org/cassandra/RunningCassandraInEclipse

I was wondering a) whether this can be done automatically by the generate-eclipse-files Ant target, and b) whether using both Builders will be problem if one is working on any of the generated code (Thrift, CQL etc). The Java Builder can be temporarily disabled if so by unticking it under Properties->Builders...

See also https://issues.apache.org/jira/browse/CASSANDRA-2854",Eclipse,,,,,,,,,,,,,,,07/Nov/11 09:39;dallsopp;Cassandra-3397.patch;https://issues.apache.org/jira/secure/attachment/12502740/Cassandra-3397.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-23 17:27:55.158,,,no_permission,,,,,,,,,,,,214254,,,Fri Dec 23 22:12:12 UTC 2011,,,,,,0|i0gjdz:,94571,urandom,urandom,,,,,,,,,"23/Oct/11 13:45;dallsopp;Looking at build.xml (generate-eclipse-files target), the .project file is generated from a verbatim listing, so adding a section for the Java Builder is trivial if that's the way we want to go...","23/Oct/11 17:27;urandom;This has annoyed me as well, so if there are no landmines associated with enabling both builders, I'd be in favor of it.",07/Nov/11 09:39;dallsopp;Small patch attached to add the Java Builder into the .project file when the generate-eclipse-files Ant target is run.,07/Nov/11 09:39;dallsopp;See attached Cassandra-3397.patch,"22/Dec/11 22:12;urandom;I wasn't aware of this issue when I submitted (and later committed) CASSANDRA-3632.  Have you had the chance to try this again since?

CASSANDRA-3632 restored the default Java builder (so problem marks do show now), but I left the Ant Builder out entirely.  I personally found it too heavy to be running, for example, on every file save (I mash CTRL+S compulsively).  Do you find the Ant Builder useful or were you primarily interested in restoring the error markers?","23/Dec/11 20:56;dallsopp;I was just interested in the error markers, thanks - I too found the Ant Builder too heavy!",23/Dec/11 22:12;urandom;Great!  Closing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
misc CQL doc fixes,CASSANDRA-3353,12526923,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,12/Oct/11 20:16,12/Mar/19 14:04,13/Mar/19 22:26,13/Oct/11 22:09,1.0.1,,,Legacy/Documentation and Website,,,0,cql,,,,See patch to follow for some minor documentation fixes (mostly formatting nits).,,,,,,,,,,,,,,,,12/Oct/11 20:17;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3353-minor-formatting-fixes.txt;https://issues.apache.org/jira/secure/attachment/12498811/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3353-minor-formatting-fixes.txt,12/Oct/11 20:17;urandom;ASF.LICENSE.NOT.GRANTED--v1-0002-bring-term-info-current-w-recent-changes.txt;https://issues.apache.org/jira/secure/attachment/12498812/ASF.LICENSE.NOT.GRANTED--v1-0002-bring-term-info-current-w-recent-changes.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-12 20:20:36.95,,,no_permission,,,,,,,,,,,,76186,,,Thu Oct 13 23:21:54 UTC 2011,,,,,,0|i0giuv:,94485,jbellis,jbellis,,,,,,,,,12/Oct/11 20:20;jbellis;+1,13/Oct/11 22:09;urandom;committed.,"13/Oct/11 23:21;hudson;Integrated in Cassandra #1157 (See [https://builds.apache.org/job/Cassandra/1157/])
    bring term info current w/ recent changes

Patch by eevans; reviewed by jbellis for CASSANDRA-3353
minor formatting fixes to doc

Patch by eevans; reviewed by jbellis for CASSANDRA-3353

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1183135
Files : 
* /cassandra/trunk/doc/cql/CQL.textile

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1183134
Files : 
* /cassandra/trunk/doc/cql/CQL.textile
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy does not log correctly when schema is not in agreement,CASSANDRA-3381,12527672,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tommysdk,cywjackson,cywjackson,18/Oct/11 22:52,12/Mar/19 14:04,13/Mar/19 22:26,24/Oct/11 23:10,1.0.1,,,,,,0,,,,,"""logger.debug(""%s disagrees (%s)"", host, entry.getKey());""

that would literally log: 

DEBUG [pool-2-thread-359] 2011-10-18 10:34:45,376 StorageProxy.java (line 821) %s disagrees (%s)


simple fix: replace with %s with {} ... may want to consider logging better comment?",,,,,,,,,,,,,,,,24/Oct/11 18:02;tommysdk;CASSANDRA-3381.patch;https://issues.apache.org/jira/secure/attachment/12500504/CASSANDRA-3381.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-24 18:02:29.61,,,no_permission,,,,,,,,,,,,88932,,,Mon Oct 24 23:10:09 UTC 2011,,,,,,0|i0gj73:,94540,jbellis,jbellis,,,,,,,,,"24/Oct/11 18:02;tommysdk;Fixed the erroneous debug logging statement by replacing %s with {}, as supported by SLF4J. Also made use of the {}-notation on some of the other debug logging statements in the class.","24/Oct/11 23:10;jbellis;committed.  Thanks, Jackson and Tommy!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFM.toAvro() incorrectly serialises key_validation_class defn,CASSANDRA-3391,12528043,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,20/Oct/11 22:26,12/Mar/19 14:04,13/Mar/19 22:26,21/Oct/11 13:43,0.8.8,1.0.1,,,,,0,,,,,"see http://www.mail-archive.com/user@cassandra.apache.org/msg18132.html

Repo with 

{code}
create keyspace Stats with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options={replication_factor:1};

use Stats;

create column family Sample_Stats with default_validation_class=CounterColumnType
    and key_validation_class='CompositeType(UTF8Type,UTF8Type)'
    and comparator='CompositeType(UTF8Type, UTF8Type)'
    and replicate_on_write=true;

[default@Stats] describe cluster;
Cluster Information:
   Snitch: org.apache.cassandra.locator.SimpleSnitch
   Partitioner: org.apache.cassandra.dht.RandomPartitioner
   Schema versions: 
	1d39bbf0-fb60-11e0-0000-242d50cf1ffd: [127.0.0.1]
{code}

Stop and restart the node

{code:java}
ERROR 10:12:22,729 Exception encountered during startup
java.lang.RuntimeException: Could not inflate CFMetaData for {""keyspace"": ""Stats"", ""name"": ""Sample_Stats"", ""column_type"": ""Standard"", ""comparator_type"": ""org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)"", ""subcomparator_type"": null, ""comment"": """", ""row_cache_size"": 0.0, ""key_cache_size"": 200000.0, ""read_repair_chance"": 1.0, ""replicate_on_write"": true, ""gc_grace_seconds"": 864000, ""default_validation_class"": ""org.apache.cassandra.db.marshal.CounterColumnType"", ""key_validation_class"": ""org.apache.cassandra.db.marshal.CompositeType"", ""min_compaction_threshold"": 4, ""max_compaction_threshold"": 32, ""row_cache_save_period_in_seconds"": 0, ""key_cache_save_period_in_seconds"": 14400, ""row_cache_keys_to_save"": 2147483647, ""merge_shards_chance"": 0.1, ""id"": 1000, ""column_metadata"": [], ""row_cache_provider"": ""org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider"", ""key_alias"": null, ""compaction_strategy"": ""org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy"", ""compaction_strategy_options"": {}, ""compression_options"": {}}
	at org.apache.cassandra.config.CFMetaData.fromAvro(CFMetaData.java:362)
	at org.apache.cassandra.config.KSMetaData.fromAvro(KSMetaData.java:193)
	at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:502)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:161)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Caused by: org.apache.cassandra.config.ConfigurationException: Invalid definition for comparator org.apache.cassandra.db.marshal.CompositeType.
	at org.apache.cassandra.db.marshal.TypeParser.getRawAbstractType(TypeParser.java:319)
	at org.apache.cassandra.db.marshal.TypeParser.getAbstractType(TypeParser.java:247)
	at org.apache.cassandra.db.marshal.TypeParser.parse(TypeParser.java:83)
	at org.apache.cassandra.db.marshal.TypeParser.parse(TypeParser.java:92)
	at org.apache.cassandra.config.CFMetaData.fromAvro(CFMetaData.java:358)
	... 6 more
Caused by: org.apache.cassandra.config.ConfigurationException: Nonsensical empty parameter list for CompositeType
	at org.apache.cassandra.db.marshal.CompositeType.getInstance(CompositeType.java:67)
	at org.apache.cassandra.db.marshal.CompositeType.getInstance(CompositeType.java:61)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.cassandra.db.marshal.TypeParser.getRawAbstractType(TypeParser.java:307)
	... 10 more
{code}

Will post the patch in a minute. ",,,,,,,,,,,,,,,,20/Oct/11 22:34;amorton;0001-3391-fix.patch;https://issues.apache.org/jira/secure/attachment/12499967/0001-3391-fix.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-21 13:34:34.228,,,no_permission,,,,,,,,,,,,96256,,,Fri Oct 21 16:00:27 UTC 2011,,,,,,0|i0gjbr:,94561,slebresne,slebresne,,,,,,,,,"20/Oct/11 22:34;amorton;Use AbstractType.toString() when serialising key_validator_class to Avro.

Checked other places to CFMD.toAvro() and CFMD.fromAvro() they seemed ok. ",21/Oct/11 13:34;slebresne;+1,"21/Oct/11 13:43;slebresne;Committed, thanks","21/Oct/11 16:00;hudson;Integrated in Cassandra-0.8 #384 (See [https://builds.apache.org/job/Cassandra-0.8/384/])
    Correctly serialize key_validation_class for avro
patch by amorton; reviewed by slebresne for CASSANDRA-3391

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1187339
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid clock drift on some Windows machines,CASSANDRA-3375,12527612,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,flavio,flavio,18/Oct/11 16:21,12/Mar/19 14:04,13/Mar/19 22:26,18/Oct/11 17:05,1.0.1,,,,,,0,windows,,,,"Performing Thread.sleep() with non-rounded values increases the frequency of interrupts on Windows machines; this can cause performance problems, and on some machines even clock drift problems for the duration of the sleep.
Fixing the issue is trivial: lower the degree of randomness by allowing only ""rounded"" sleep periods.
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6464007
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6435126",Windows,0,0,,0%,0,0,,,,,,,,,18/Oct/11 16:23;flavio;trunk-3375.txt;https://issues.apache.org/jira/secure/attachment/12499558/trunk-3375.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-18 17:05:12.569,,,no_permission,,,,,,,,,,,,88863,,,Tue Oct 18 17:05:12 UTC 2011,,,,,,0|i0gj4f:,94528,brandon.williams,brandon.williams,,,,,,,,,18/Oct/11 16:23;flavio;Simple fix as proposed in description. I forgot to mention that the problem is in HintedHandOffManager.,"18/Oct/11 17:05;brandon.williams;Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"get_slice, super column family with UUIDType as column comparator",CASSANDRA-3467,12530627,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,rbranson,alesl,alesl,07/Nov/11 20:33,12/Mar/19 14:04,13/Mar/19 22:26,08/Nov/11 18:03,1.0.3,,,Legacy/CQL,,,0,,,,,"get_slice with more than one column selected by predicate fails, when comparator is set to (Lexical|Time)UUIDType and more than one column is being selected.

{code}
// Sample data:
create column family A with column_type = Super and comparator = LexicalUUIDType and subcomparator = UTF8Type and default_validation_class = UTF8Type;
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['a'] = 'A';
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['b'] = 'B';
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['c'] = 'C';
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['d'] = 'D';
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['e'] = 'E';

// Failed call
$client->get_slice(
    'key', 
    new ColumnParent(array(
        'column_family'=>'A', 
        'super_column'=>base64_decode('sTkzfvttQeGoaB238qUqQg==')
    )), 
    new SlicePredicate(array(
        'column_names'=>array('a', 'b')
    )), 
    1
);

// Exception thrown
ERROR [ReadStage:302] 2011-11-07 21:29:30,339 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:302,5,main]
java.lang.RuntimeException: java.lang.IndexOutOfBoundsException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1269)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IndexOutOfBoundsException
	at java.nio.Buffer.checkIndex(Buffer.java:520)
	at java.nio.HeapByteBuffer.getLong(HeapByteBuffer.java:391)
	at org.apache.cassandra.utils.UUIDGen.getUUID(UUIDGen.java:67)
	at org.apache.cassandra.db.marshal.LexicalUUIDType.compare(LexicalUUIDType.java:58)
	at org.apache.cassandra.db.marshal.LexicalUUIDType.compare(LexicalUUIDType.java:31)
	at java.util.TreeMap.put(TreeMap.java:530)
	at java.util.TreeSet.add(TreeSet.java:238)
	at java.util.AbstractCollection.addAll(AbstractCollection.java:305)
	at java.util.TreeSet.addAll(TreeSet.java:295)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:98)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:61)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1278)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1164)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1131)
	at org.apache.cassandra.db.Table.getRow(Table.java:378)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:58)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:797)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1265)
	... 3 more

// This works though
$client->get_slice(
    'key', 
    new ColumnParent(array(
        'column_family'=>'A', 
        'super_column'=>base64_decode('sTkzfvttQeGoaB238qUqQg==')
    )), 
    new SlicePredicate(array(
        'column_names'=>array('a')
    )), 
    1
);

// This works too
$client->get_slice('key', 
	new ColumnParent(array(
		'column_family'=>'A', 
		'super_column'=>base64_decode('sTkzfvttQeGoaB238qUqQg==')
	)), 
	new SlicePredicate(array(
		'slice_range'=>new SliceRange(array(
			'start'=>'', 
			'finish'=>'',
			'reversed'=>false,
			'count'=>100
		))
	)), 1);
{code}

Regards
ales","ubuntu 11.04
java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)

php interface build with thrift 0.7.0",,,,,,,,,,,,,,,08/Nov/11 17:44;rbranson;3467-get-slice-on-scf-with-uuidtype-comparator.patch;https://issues.apache.org/jira/secure/attachment/12502938/3467-get-slice-on-scf-with-uuidtype-comparator.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-08 17:44:04.542,,,no_permission,,,,,,,,,,,,216365,,,Tue Nov 08 18:03:27 UTC 2011,,,,,,0|i0gk9j:,94713,jbellis,jbellis,,,,,,,,,08/Nov/11 17:44;rbranson;Attached is a test case and a patch for the issue.,"08/Nov/11 18:03;jbellis;CASSANDRA-3446 fixed this in passing, but I committed the test to prevent future regressions.  Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExpiringMap timer is not exception-proof,CASSANDRA-3537,12533074,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jonma,jonma,29/Nov/11 07:24,12/Mar/19 14:04,13/Mar/19 22:26,24/Feb/12 15:26,1.1.0,,,,,,0,,,,,"I have 4 cassandra nodes ,and I put about 30G data to db for every nodes . It's just 4 days before I start the cluster ,but now every 4 nodes have the same problem ,JVM heap is full  ,and  GC take no effect ,There must be some memory leak . Jmap the memory as follow:

Object Histogram:

num 	  #instances	#bytes	Class description
--------------------------------------------------------------------------
1:		15793606	758093088	java.nio.HeapByteBuffer
2:		2153811	320138208	java.lang.Object[]
3:		6163192	197222144	org.apache.cassandra.db.Column
4:		2543836	175890256	int[]
5:		2168816	155397192	long[]
6:		2078123	116374888	org.cliffc.high_scale_lib.ConcurrentAutoTable$CAT
7:		1847111	73884440	java.math.BigInteger
8:		1234243	59243664	java.util.Hashtable
9:		1770829	58233000	char[]
10:		1770627	56660064	java.lang.String
11:		1665886	39981264	org.apache.cassandra.db.DecoratedKey
12:		692706	38791536	org.cliffc.high_scale_lib.NonBlockingHashMap$CHM
13:		1234274	37172088	java.util.Hashtable$Entry[]
14:		1133541	36273312	java.net.Inet4Address
15:		738528	35449344	org.apache.cassandra.service.ReadCallback
16:		2078118	33249888	org.cliffc.high_scale_lib.Counter
17:		1373886	32973264	org.apache.cassandra.db.ReadResponse
18:		1234023	29616552	org.apache.cassandra.net.Message
19:		1234019	29616456	org.apache.cassandra.net.Header
20:		1846185	29538960	org.apache.cassandra.dht.BigIntegerToken
21:		891378	28524096	org.apache.cassandra.utils.ExpiringMap$CacheableObject
22:		692706	27708240	org.cliffc.high_scale_lib.NonBlockingHashMap
23:		1148252	27558048	java.util.Collections$SynchronizedSet
24:		541977	26014896	org.apache.cassandra.db.SliceFromReadCommand
25:		998001	23952024	java.util.concurrent.ConcurrentSkipListMap$Node
26:		928792	22291008	java.util.ArrayList
27:		692715	22166880	java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl
28:		891378	21393072	org.apache.cassandra.net.CallbackInfo
29:		1148247	18371952	java.util.Hashtable$KeySet
30:		731859	17564616	org.apache.cassandra.db.Row
31:		529991	16959712	org.apache.cassandra.db.ArrayBackedSortedColumns
32:		691425	16594200	org.apache.cassandra.db.AbstractColumnContainer$DeletionInfo
33:		648580	15565920	org.apache.cassandra.db.filter.QueryPath
34:		648338	15560112	org.apache.cassandra.service.RowDigestResolver
35:		971376	15542016	java.util.concurrent.atomic.AtomicInteger
36:		837418	13398688	org.apache.cassandra.utils.SimpleCondition
37:		535614	12854736	org.apache.cassandra.db.ColumnFamily
38:		725634	11610144	java.util.concurrent.atomic.AtomicReference
39:		195117	9365616	org.apache.cassandra.db.ThreadSafeSortedColumns
40:		281921	9021472	java.util.concurrent.ConcurrentSkipListMap$HeadIndex
41:		277679	8885728	java.util.concurrent.locks.ReentrantLock$NonfairSync
42:		314424	7546176	java.util.concurrent.ConcurrentSkipListMap$Index
43:		275186	6604464	java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject
44:		270280	6486720	java.util.concurrent.LinkedBlockingQueue$Node
45:		219553	5269272	org.apache.cassandra.io.sstable.IndexSummary$KeyPosition
46:		106436	5108928	java.util.TreeMap
47:		122185	4887400	org.apache.cassandra.db.ExpiringColumn
48:		189968	4559232	org.apache.cassandra.db.SuperColumn
49:		275659	4410544	java.util.concurrent.locks.ReentrantLock
50:		90213	4330224	java.util.concurrent.LinkedBlockingQueue
51:		107026	4281040	java.util.TreeMap$Entry
52:		30501	4222056	* ConstMethodKlass","当前堆大小: 
5,815,955 Kb
堆大小的最大值: 
6,045,696 Kb
分配的内存: 
6,045,696 Kb
暂挂结束操作: 
0 个对象
垃圾收集器: 
Name = 'ParNew', Collections = 3,294, Total time spent = 2 minutes
垃圾收集器: 
Name = 'ConcurrentMarkSweep', Collections = 5,909, Total time spent = 2 hours 17 minutes
 
操作系统: 
Linux 2.6.32.12-0.7-default
体系结构: 
amd64
处理器的数目: 
16
分配的虚拟内存: 
42,748,416 Kb
物理内存总量: 
24,568,836 Kb
可用物理内存: 
 7,136,380 Kb
交换空间总量: 
 2,104,472 Kb
可用交换空间: 
 1,970,800 Kb
 
VM 参数: 
-ea -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms6G -Xmx6G -Xmn2400M -XX:+HeapDumpOnOutOfMemoryError -Xss128k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Djava.net.preferIPv4Stack=true -Dcom.sun.management.jmxremote.port=9000 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dpasswd.properties=/opt/obs/cassandra/conf/passwd.properties -Dpasswd.mode=MD5 -Dlog4j.configuration=log4j-server.properties -Dlog4j.defaultInitOverride=true 
类路径: 
/opt/obs/cassandra/conf:/opt/obs/cassandra/build/classes/main:/opt/obs/cassandra/build/classes/thrift:/opt/obs/cassandra/lib/antlr-3.2.jar:/opt/obs/cassandra/lib/apache-cassandra-1.0.0.jar:/opt/obs/cassandra/lib/apache-cassandra-clientutil-1.0.0.jar:/opt/obs/cassandra/lib/apache-cassandra-thrift-1.0.0.jar:/opt/obs/cassandra/lib/avro-1.4.0-fixes.jar:/opt/obs/cassandra/lib/avro-1.4.0-sources-fixes.jar:/opt/obs/cassandra/lib/cassandra_simple_authentication.jar:/opt/obs/cassandra/lib/commons-cli-1.1.jar:/opt/obs/cassandra/lib/commons-codec-1.2.jar:/opt/obs/cassandra/lib/commons-lang-2.4.jar:/opt/obs/cassandra/lib/compress-lzf-0.8.4.jar:/opt/obs/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/opt/obs/cassandra/lib/guava-r08.jar:/opt/obs/cassandra/lib/high-scale-lib-1.1.2.jar:/opt/obs/cassandra/lib/jackson-core-asl-1.4.0.jar:/opt/obs/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/opt/obs/cassandra/lib/jamm-0.2.5.jar:/opt/obs/cassandra/lib/jline-0.9.94.jar:/opt/obs/cassandra/lib/json-simple-1.1.jar:/opt/obs/cassandra/lib/libthrift-0.6.jar:/opt/obs/cassandra/lib/log4j-1.2.16.jar:/opt/obs/cassandra/lib/servlet-api-2.5-20081211.jar:/opt/obs/cassandra/lib/slf4j-api-1.6.1.jar:/opt/obs/cassandra/lib/slf4j-log4j12-1.6.1.jar:/opt/obs/cassandra/lib/snakeyaml-1.6.jar:/opt/obs/cassandra/lib/snappy-java-1.0.3.jar",,,,,,,,,,,,,,,22/Feb/12 17:00;jbellis;3537-v2.txt;https://issues.apache.org/jira/secure/attachment/12515612/3537-v2.txt,23/Feb/12 17:46;jbellis;3537-v4.txt;https://issues.apache.org/jira/secure/attachment/12515768/3537-v4.txt,22/Feb/12 10:59;slebresne;3537.txt;https://issues.apache.org/jira/secure/attachment/12515573/3537.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-11-29 07:34:24.465,,,no_permission,,,,,,,,,,,,218802,,,Fri Feb 24 15:26:26 UTC 2012,,,,,,0|i0gl4n:,94853,slebresne,slebresne,,,,,,,,,29/Nov/11 07:34;jbellis;Reduce the memtable_total_space_in_mb setting.,"30/Nov/11 02:00;jonma;Thank u very much . But I have a test tonight  ,I changed my 4 cassandra nodes'  max heap size and memtable_total_space_in_mb setting as follow:
node A: heap size :6G  , memtable_total_space_in_mb = 1G (as you say ,I reduce the memtable_total_space_in_mb setting)
node B: 8G   ,  default ;
node C: 5G   ,  default ;
node D: 8G   ,  1G  ;
Then I restart the cluster, and begin to push data to it . This morning I have a frustrated test result ,  node A heap is full  ,node B OK , node C OK , node D heap is full . 

I don't know how to explain it .

","30/Nov/11 02:11;jbellis;what liveRatio is being logged for your workload?

can you reproduce using a synthetic Stress workload?  (stress tool available w/ source distribution)","30/Nov/11 03:50;jonma;In system.log ,I find this : 
<164> 4 2011-11-29T15:29:57.890714+00:00   WARN [MutationStage:10] 2011-11-29 15:29:57,890 Memtable.java (line 142) MemoryMeter uninitialized (jamm not specified as java agent); assuming liveRatio of 10.0.  Usually this means cassandra-env.sh disabled jamm because you are using a buggy JRE; upgrade to the Sun JRE instead .
 
I have no idea this will affect the test result ,or not . But all of my 4 cassandra nodes have the same log . I checked my system physical memory size : node A: 16 G ,ulimit -v :14G   node B:  32G , ulimit 28G ,  node C :48G ,ulimit -v 42G  node D:24G ulimit -v 21G , 

The ulimit -v setting will result the problem ?

ps : There is another java application run on my machine  . It result the log WARN .
 I haven't use Stress as workload ,I use a workload like YCSB based on Hector .
 There are 8 user CF in the system.




",30/Nov/11 05:02;jbellis;You should upgrade to the Sun/Oracle JVM as it suggests.,"30/Nov/11 05:17;jonma;In fact ,my jvm is Sun JVM, Java HotSpot(TM) 64-Bit Server VM  19.1-b02 .",30/Nov/11 05:32;jbellis;Then you'll have to come up with an alternative explanation for why jamm isn't being used.,"30/Nov/11 05:45;jonma;
If I terminate the another java application ,it will not have the log WARN .
I think I should terminate the another java application first , then do a new test again.  Thanks.
========================================
ps : There is another java application run on my machine . It result the log WARN .
I haven't use Stress as workload ,I use a workload like YCSB based on Hector .
.......","02/Dec/11 02:04;jonma;After  I set jamm correctly , I do a new test , and haven't find the memory problem in 20 hours  .But I could't confirm it's  ""jamm not specified as java agent""  result the heap problem . I analysised the dump memory info of the node which had the heap problem , they nearly have the same thing :  too many Message objects and ralated objects like below,although I have stoped reading and writing data for a few minutes and  empty all memtables  .
#instances	#bytes	Class description
---------------------------------------------
900654	21615696	org.apache.cassandra.net.Message
900653	21615672	org.apache.cassandra.net.Header
341351	16384848	org.apache.cassandra.service.ReadCallback
338502	13540080	org.apache.cassandra.db.RangeSliceCommand
338500	10832000	org.apache.cassandra.thrift.SliceRange
338502	8124048	org.apache.cassandra.thrift.SlicePredicate
343328	5493248	org.apache.cassandra.utils.SimpleCondition


Is there someone can tell me ,whether there are some relationship between ""jamm not specified as java agent"" and so many 
objects like above  ?
 ","02/Dec/11 06:45;jonma;Heap memory can not be GC again in my lasted test , although I have set jamm correctly . Jmap the memory ,find the same thing  ,too many Message objects and ralated objects  in the memory ! 
I have empty all memtables ,in normal condition , heap memory used not more than 0.5G ,but now more than 2.8G . I think only memory leak will result this .

{code:xml} 
Object Histogram:

num 	  #instances	#bytes	Class description
--------------------------------------------------------------------------
1:		9101138	927302160	byte[]
2:		9998059	479906832	java.nio.HeapByteBuffer
3:		875398	221844128	long[]
4:		4302698	137686336	org.apache.cassandra.db.Column
5:		869859	137476712	java.lang.Object[]
6:		1794458	101193096	int[]
7:		1515578	60623120	java.math.BigInteger
8:		834628	46739168	org.cliffc.high_scale_lib.ConcurrentAutoTable$CAT
9:		1644330	39463920	java.util.concurrent.ConcurrentSkipListMap$Node
10:		1461787	35082888	org.apache.cassandra.db.DecoratedKey
11:		513576	24651648	java.util.Hashtable
12:		741037	24559960	char[]
13:		1512396	24198336	org.apache.cassandra.dht.BigIntegerToken
14:		741097	23715104	java.lang.String
15:		732184	17572416	org.apache.cassandra.io.sstable.IndexSummary$KeyPosition
16:		513594	15637344	java.util.Hashtable$Entry[]
17:		278209	15579704	org.cliffc.high_scale_lib.NonBlockingHashMap$CHM
18:		462641	14804512	java.net.Inet4Address
19:		578801	13891224	java.util.concurrent.ConcurrentSkipListMap$Index
20:		834627	13354032	org.cliffc.high_scale_lib.Counter
21:		556415	13353960	org.apache.cassandra.db.ReadResponse
22:		267483	12839184	org.apache.cassandra.service.ReadCallback
23:		513250	12318000	org.apache.cassandra.net.Message
24:		513248	12317952	org.apache.cassandra.net.Header
25:		501596	12038304	org.apache.cassandra.db.AbstractColumnContainer$DeletionInfo
26:		244644	11742912	org.apache.cassandra.db.ThreadSafeSortedColumns
27:		365024	11680768	java.util.concurrent.ConcurrentSkipListMap$HeadIndex
28:		464800	11155200	java.util.Collections$SynchronizedSet
29:		278209	11128360	org.cliffc.high_scale_lib.NonBlockingHashMap
30:		342741	10967712	org.apache.cassandra.utils.ExpiringMap$CacheableObject
31:		199478	9574944	org.apache.cassandra.db.SliceFromReadCommand
32:		278218	8902976	java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl
33:		348179	8356296	org.apache.cassandra.db.ColumnFamily
34:		342741	8225784	org.apache.cassandra.net.CallbackInfo
35:		255241	8167712	org.apache.cassandra.db.ArrayBackedSortedColumns
36:		334724	8033376	java.util.ArrayList
37:		499963	7999408	java.util.concurrent.atomic.AtomicReference
38:		319779	7674696	org.apache.cassandra.db.Row
39:		187484	7499360	org.apache.cassandra.db.Memtable$6
40:		464794	7436704	java.util.Hashtable$KeySet
41:		187496	5999872	java.util.TreeMap$KeyIterator
42:		372748	5963968	java.util.concurrent.atomic.AtomicInteger
43:		236877	5685048	org.apache.cassandra.db.filter.QueryPath
44:		236733	5681592	org.apache.cassandra.service.RowDigestResolver
45:		300385	4806160	org.apache.cassandra.utils.SimpleCondition
46:		30726	4245920	* ConstMethodKlass
47:		30726	3696240	* MethodKlass
48:		151726	3641424	org.apache.cassandra.db.SuperColumn
49:		3134	3350536	* ConstantPoolKlass
50:		95622	3059904	java.util.concurrent.locks.ReentrantLock$NonfairSync
51:		45910	2497952	* SymbolKlass
52:		3134	2276040	* InstanceKlassKlass
53:		94544	2269056	java.util.concurrent.LinkedBlockingQueue$Node
54:		93130	2235120	java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject
55:		133398	2134368	java.util.concurrent.ConcurrentSkipListMap$Values
56:		2813	2099368	* ConstantPoolCacheKlass
57:		37698	1809504	java.util.TreeMap
58:		37783	1511320	java.util.TreeMap$Entry
59:		93602	1497632	java.util.concurrent.locks.ReentrantLock
60:		30768	1476864	java.util.concurrent.LinkedBlockingQueue
61:		85156	1362496	java.util.concurrent.ConcurrentSkipListMap$EntrySet
62:		41451	1326432	org.apache.cassandra.service.RowRepairResolver
63:		41141	1316512	java.util.concurrent.ConcurrentHashMap$HashEntry
64:		32885	1315400	org.apache.cassandra.service.WriteResponseHandler
65:		3110	1277672	* MethodDataKlass
66:		39780	1272960	com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node
67:		31556	1262240	org.apache.cassandra.net.AsyncResult
68:		30745	1229800	org.apache.cassandra.db.RangeSliceCommand
69:		37628	1204096	org.apache.cassandra.db.SliceByNamesReadCommand
70:		44528	1068672	com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue
71:		31974	1023168	java.util.concurrent.locks.AbstractQueuedSynchronizer$Node
72:		41435	994440	org.apache.cassandra.service.AsyncRepairCallback
73:		30821	986272	org.apache.cassandra.thrift.SliceRange
74:		30756	984192	java.util.RandomAccessSubList
75:		30745	983840	org.apache.cassandra.service.RangeSliceResponseResolver
76:		29115	931680	org.apache.cassandra.db.DeletedColumn
77:		21676	867040	org.apache.cassandra.db.ExpiringColumn
78:		35339	848136	java.lang.Long
79:		35204	844896	org.apache.cassandra.utils.Pair
80:		30924	742176	java.util.BitSet
81:		30821	739704	org.apache.cassandra.thrift.SlicePredicate
82:		30787	738888	org.apache.cassandra.dht.Bounds
83:		37673	602768	java.util.TreeSet
84:		37654	602464	java.util.TreeMap$KeySet
85:		31598	505568	java.util.concurrent.atomic.AtomicBoolean
86:		6575	420800	java.nio.DirectByteBufferR
87:		2004	416400	java.util.concurrent.ConcurrentHashMap$HashEntry[]
88:		3433	357032	java.lang.Class
89:		8976	287232	java.lang.ref.WeakReference
90:		4999	269008	short[]{code} 
","05/Dec/11 03:02;jonma;I spend a whole week analyse this problem , Now I find this problem is only caused by {color:red} 
the cache in ExpiringMap {color} 
 which is a NonBlockingHashMap . Why this cache can make the heap memory full and never be GC ?",05/Dec/11 05:55;jbellis;That is where callbacks for ongoing requests are stored.,"05/Dec/11 06:19;jonma;Yes. 
{code:title=ExpiringMap.java|borderStyle=solid}
/**
     *
     * @param expiration the TTL for objects in the cache in milliseconds
     */
    public ExpiringMap(long expiration, final Function<Pair<K,V>, ?> postExpireHook)
    {
        ....
        timer = new Timer(""EXPIRING-MAP-TIMER-"" + (++counter), true);
        TimerTask task = new TimerTask()
        {
            public void run()
            {
                long start = System.currentTimeMillis();
                for (Map.Entry<K, CacheableObject<V>> entry : cache.entrySet())
                {
                    if (entry.getValue().isReadyToDie(start))
                    {
                        cache.remove(entry.getKey());
                        if (postExpireHook != null)
                            postExpireHook.apply(new Pair<K, V>(entry.getKey(), entry.getValue().getValue()));
                    }
                }
            }
        };
        timer.schedule(task, expiration / 2, expiration / 2);
    }
{code} 

The code above exist a bug . If a exception occured in the run method ,the timer will be killed and no longer to expire the value in cache .
I find that if there is no ""EXPIRING-MAP-TIMER- x"" thread in the memory  when there is a  heap problem , and if there is a ""EXPIRING-MAP-TIMER- x"" thread in the memory   there is no heap problem .","05/Dec/11 14:10;jbellis;That's true, but unless you're seeing exceptions in the log it should be a non-issue.","06/Dec/11 11:36;jonma;{quote}
Jonathan Ellis added a comment - 05/Dec/11 14:10 
That's true, but unless you're seeing exceptions in the log it should be a non-issue.
{quote}
2011-12-06 06:50:23,383 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[EXPIRING-MAP-TIMER-1,5,main]
java.lang.AssertionError: /xxx.xxx.xx.xx
      at org.apache.cassandra.service.StorageProxy.scheduleLocalHint(StorageProxy.java:337)
      at org.apache.cassandra.net.MessagingService.scheduleMutationHint(MessagingService.java:199)
      at org.apache.cassandra.net.MessagingService.access$500(MessagingService.java:62)
      at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:173)
      at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:150)
      at org.apache.cassandra.utils.ExpiringMap$1.run(ExpiringMap.java:89)
      at java.util.TimerThread.mainLoop(Timer.java:512)
      at java.util.TimerThread.run(Timer.java:462)","06/Dec/11 13:35;jbellis;That's CASSANDRA-3440, you should upgrade.",07/Dec/11 03:38;jonma;Thanks very much . Helps a lot .,22/Feb/12 10:59;slebresne;Simple patch attached to make expiring map more exception proof.,"22/Feb/12 11:15;jonma;
{quote}
Simple patch attached to make expiring map more exception proof.
{quote}
Please catch Throwable  , not just only catch Exception. It may have Errors in try-catch block.",22/Feb/12 14:51;jbellis;catching Throwable or Error is generally Bad because most Errors are not recoverable. OutOfMemoryError is the most common. We have a global exception hook set up to shut down the process for those.,"22/Feb/12 17:00;jbellis;v2 attached that address this for DSTPE in general.  Also fixes a double-log of RuntimeExceptions from ThreadPoolExecutor in our DTPE afterExecute code.

The culprit for this last part is this fragment from TPE.runTask:
{code}
.               try {
                    task.run();
                    ran = true;
                    afterExecute(task, null);
                    ++completedTasks;
                } catch (RuntimeException ex) {
                    if (!ran)
                        afterExecute(task, ex);
                    throw ex;
                }
{code}

That is, for ""raw"" exceptions thrown by the Task (and not wrapped inside FutureTask), runTask will already re-throw the exception, allowing our default uncaught exception handler to log it.  So we don't want to double-log that in DTPE.","22/Feb/12 17:59;slebresne;I like the idea of using DSTPE. I'm not sure I understand why LoggingScheduledFuture is useful though. STPE already wraps runnables into FutureTask, which catches Throwable and turn them into ExecutionException. So DSTPE.afterExecute() should already get everything.

However, I think we may need the kind of wrapper LoggingScheduledFuture implements for DPTE, since if we use the TPE.execute() method (which we do), then the runnable is not wrapped by a FutureTask and thrown exception will be thrown in TPE.runTask. And since TPE.runTask only catches RuntimeException...

As a side note, I notice your patch is against 1.1 which I'm good with but we may want to change the fix version then.","22/Feb/12 19:24;jbellis;bq. I'm not sure I understand why LoggingScheduledFuture is useful though

The problem I'm trying to solve is, the core of STPE.ScheduledFutureTask:

{code}
.       private void runPeriodic() {
            boolean ok = ScheduledFutureTask.super.runAndReset();
            boolean down = isShutdown();
            // Reschedule if not cancelled and not shutdown or policy allows
            if (ok && (!down ||
                       (getContinueExistingPeriodicTasksAfterShutdownPolicy() &&
                        !isStopped()))) {
                long p = period;
                if (p > 0)
                    time += p;
                else
                    time = triggerTime(-p);
                ScheduledThreadPoolExecutor.super.getQueue().add(this);
            }
            // This might have been the final executed delayed
            // task.  Wake up threads to check.
            else if (down)
                interruptIdleWorkers();
        }
{code}

In other words, ""If any execution of the task encounters an exception, subsequent executions are suppressed.""

That said, you're right that a second wrapper doesn't solve this problem.  Not sure what to do here.  Use reflection to reach into the SFT in afterExecute, and re-schedule the original Runnable?

bq. if we use the TPE.execute() method (which we do), then the runnable is not wrapped by a FutureTask and thrown exception will be thrown in TPE.runTask

That's fine though, since TPE.execute is inherently non-repeating.","23/Feb/12 09:48;slebresne;bq. In other words, ""If any execution of the task encounters an exception, subsequent executions are suppressed.""

I see.

bq. Use reflection to reach into the SFT in afterExecute, and re-schedule the original Runnable?

That would be one option I suppose. Another would be to override all the schedule() method to wrap the runnable into a wrapper that would resubmit the task on exception (we would avoid reflection, not sure it's simpler though).

bq. That's fine though, since TPE.execute is inherently non-repeating.

True.","23/Feb/12 14:14;jbellis;bq. Another would be to override all the schedule() method to wrap the runnable 

That's going to require reflection as well since SFT is private. :)","23/Feb/12 14:23;slebresne;No I meant that we would wrap the initial Runnable into another Runnable that catches every exception of the first one and log them. In other words, as far as STPE is concerned, our runnable would never ever fail (and thus would never be suppressed).",23/Feb/12 17:46;jbellis;That makes sense.  v4 attached with this approach.,"23/Feb/12 17:47;jbellis;(And yes, I think this should target 1.1 instead of 1.0.9 since messing with executors scares me a bit.)",24/Feb/12 09:59;slebresne;+1 (nit: the overriden schedule method misses their @Override),24/Feb/12 15:26;jbellis;Added @Override and committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix crack-smoking in ConsistencyLevelTest,CASSANDRA-3531,12532705,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,25/Nov/11 11:37,12/Mar/19 14:04,13/Mar/19 22:26,04/Jan/12 03:17,1.0.7,,,Legacy/Testing,,,0,,,,,"First, let's note that this test fails in current 1.0 branch. It was ""broken"" (emphasis on the quotes) by CASSANDRA-3529. But it's not CASSANDRA-3529 fault, it's only that the use of NonBlockingHashMap changed the order of the tables returned by Schema.instance.getNonSystemTables(). *And*,  it turns out that ConsistencyLevelTest bails out as soon as it has found one keyspace with rf >= 2 due to a misplaced return. So it use to be that ConsistencyLevelTest was only ran for Keyspace5 (whose RF is 2) for which the test work. But for any RF > 2, the test fails.

The reason of this failing is that the test creates a 3 node cluster for whom only 1 node is alive as far as the failure detector is concerned. So for RF=3 and CL=QUORUM, the writes are unavailable (the failure detector is queried), while for reads we ""pretend"" two nodes are alive so we end up with a case where isWriteUnavailable != isReadUnavailable.
",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-11-25 17:47:24.854,,,no_permission,,,,,,,,,,,,218438,,,Wed Jan 04 03:17:20 UTC 2012,,,,,,0|i0gl1z:,94841,jbellis,jbellis,,,,,,,,,"25/Nov/11 17:47;jbellis;I did some minimal-necessary work to get CLT to pass in trunk, but it sounds like you have deeper changes in mind.","03/Jan/12 14:19;slebresne;The deeper change I have in mind consists roughly in removing that test. It's trying to tests the result of WriteHandler.assureSufficientLiveNodes() but that method depends on the result of the FailureDetector. The problem is that I don't think we really have a good way to create real multi-nodes cluster in the unit test. Maybe we can ""fake"" live nodes but I'm not sure how and in the end it makes me wonder what the tests is really testing if we're starting to fake too much stuff. It seems to me that the distributed tests are probably a better place to do that kind of thing.

In any case, It's really annoying to have unit tests failure, especially in the 1.0 branch. And as said in the description, that test never really worked anyway so any opposition to at least commenting it for now? ","04/Jan/12 03:17;jbellis;You're right: while it's reasonable to unit-test assureSufficientLiveNodes, the right place to do that is just with IWriteResponseHandler objects instead of mocking up a ring.  This test also pre-dated NTS and the different IWRH for that, so it's pretty fragile.

Went ahead and deleted it per your suggestion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE on startup when there are permissions issues with directories,CASSANDRA-3544,12533346,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,thobbs,thobbs,30/Nov/11 20:55,12/Mar/19 14:04,13/Mar/19 22:26,16/Dec/11 21:31,1.0.7,,,,,,0,,,,,"If the directories used by cassandra for data, commitlog, and saved caches aren't readable due to permissions, you get an NPE on startup.  In particular, if none of them are readable, you'll see something like this:

{noformat}
ERROR 14:50:11,945 Exception encountered during startup
java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:391)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:147)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:391)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:147)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
Exception encountered during startup: null
{noformat}

This traceback happens when the saved_caches directory isn't readable, but you can get different ones if only the data or commitlog directories aren't readable.

We should check the permissions of these directories before trying to list their contents.",,,,,,,,,,,,,,,,16/Dec/11 18:34;yukim;cassandra-1.0-3544.txt;https://issues.apache.org/jira/secure/attachment/12507714/cassandra-1.0-3544.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-07 17:02:29.723,,,no_permission,,,,,,,,,,,,219074,,,Fri Dec 16 21:31:51 UTC 2011,,,,,,0|i0gl7r:,94867,brandon.williams,brandon.williams,,,,,,,,,"07/Dec/11 17:02;yukim;Here is my first attempt. It checks all(data, commitlog, saved cache) directories' existence and permissions on startup.

If check fails, cassandra stops starting.","08/Dec/11 20:52;brandon.williams;The problem with this patch is it asserts the directories exist, where before we would try to create them if they didn't.",16/Dec/11 18:34;yukim;You're right. I changed to assert only when directory exists.,16/Dec/11 21:31;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
requiring --debug to see stack traces for failures in cassandra-cli is a terrible idea (aka silent failure is never a valid option),CASSANDRA-3508,12531999,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,mdennis,mdennis,18/Nov/11 21:47,12/Mar/19 14:04,13/Mar/19 22:26,21/Nov/11 18:26,1.0.4,,,Legacy/Tools,,,0,,,,,"this manifests itself in cassandra-cli by returning null to the user.  In order to see what the problem was (and in many cases, just to know there was a problem at all) requires running cassandra-cli with ""--debug""",,,,,,,,,,,,,,,,21/Nov/11 18:03;xedin;CASSANDRA-3508-v2.patch;https://issues.apache.org/jira/secure/attachment/12504525/CASSANDRA-3508-v2.patch,18/Nov/11 22:50;xedin;CASSANDRA-3508.patch;https://issues.apache.org/jira/secure/attachment/12504282/CASSANDRA-3508.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-18 22:50:05.708,,,no_permission,,,,,,,,,,,,217735,,,Mon Nov 21 18:26:48 UTC 2011,,,,,,0|i0gkrr:,94795,jbellis,jbellis,,,,,,,,,18/Nov/11 22:50;xedin;removes explicit --debug requirement and prints stack-traces in cases where it's necessary.,"21/Nov/11 15:21;jbellis;We need to be a little judicious here.  Simply printing stacktraces for *all* exceptions is not right either.  For instance, it looks to me like

{code}
             sessionState.err.println(e.getWhy());
 
-            if (sessionState.debug)
-                e.printStackTrace();
+            e.printStackTrace(sessionState.err);
{code}

{code}
-            if (sessionState.debug)
-                e.printStackTrace();
-            
             sessionState.err.println(""Login failure. Did you specify 'keyspace', 'username' and 'password'?"");
+            e.printStackTrace(sessionState.err);
{code}

should not be stacktraces by default.",21/Nov/11 15:47;xedin;I agree that we can remove it from the first one (describeRing) but I think we should keep stack-trace for login as it could be handy to have.,21/Nov/11 17:45;jbellis;I'd be okay with keeping --debug around for that.,21/Nov/11 18:03;xedin;this patch keeps the --debug option and shows exceptions in the places your mentioned only when that option is set.,21/Nov/11 18:10;jbellis;+1,21/Nov/11 18:26;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh ASSUME doesn't work when using SELECT keyspace.cfname syntax,CASSANDRA-3500,12531818,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,17/Nov/11 17:22,12/Mar/19 14:04,13/Mar/19 22:26,28/Nov/11 21:19,1.0.5,,,Legacy/Tools,,,0,cqlsh,,,,"After assigning an ASSUME type to some columnfamily CF in keyspace K, if a SELECT is subsequently done on CF while the session is using a different keyspace, the ASSUME does not take effect:

{noformat}
cqlsh> USE ks;
cqlsh:ks> CREATE COLUMNFAMILY cf (key int PRIMARY KEY, col int);
cqlsh:ks> INSERT INTO cf (key, col) VALUES (99, 1633837924);
cqlsh:ks> ASSUME cf(col) VALUES ARE ascii;
cqlsh:ks> SELECT * FROM cf;
 KEY |  col |
  99 | abcd |

cqlsh:ks> USE system;
cqlsh:system> SELECT * FROM ks.cf;
 KEY |        col |
  99 | 1633837924 |

{noformat}

the output from both {{SELECT}}s there should be the same.",,,,,,,,,,,,,,,,17/Nov/11 17:24;thepaul;3500.patch.txt;https://issues.apache.org/jira/secure/attachment/12504081/3500.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-28 21:19:22.866,,,no_permission,,,,,,,,,,,,217554,,,Mon Nov 28 21:19:22 UTC 2011,,,,,,0|i0gko7:,94779,brandon.williams,brandon.williams,,,,,,,,,17/Nov/11 17:24;thepaul;fixerated,28/Nov/11 21:19;brandon.williams;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh complains when you try to do UPDATE with counter columns,CASSANDRA-3493,12531370,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,14/Nov/11 21:09,12/Mar/19 14:04,13/Mar/19 22:26,14/Nov/11 21:31,1.0.3,,,Legacy/Tools,,,0,,,,,"trying to do a counter column UPDATE in cqlsh causes an ""Invalid syntax"" error:

{noformat}
cqlsh:foo> update brongo SET boo = boo+1 where key='hi';
Invalid syntax at line 1, char 28
  update brongo SET boo = boo+1 where key='hi';
                             ^
{noformat}

This is cause cqlsh's lexer doesn't know that + and - are valid operators in CQL. Don't worry, I'm not trying to make cqlsh be able to parse all CQL with exactness- it tries, in order to provide the best tab completion, but when it fails to parse it can still pass on CQL text to the server. This case is different because it's the lexer that can't understand the operators, before we even get to the parser. We do need a working and correct lexer, along with at least minimal parsing capability, in order to reliably split up statements, tell when the user is changing the keyspace, or SELECTing on a columnfamily with ASSUMEd types.

Also, the parser should be tweaked in a manner similar to CASSANDRA-3418.",,,,,,,,,,,,,,,,14/Nov/11 21:15;thepaul;3493.patch.txt;https://issues.apache.org/jira/secure/attachment/12503676/3493.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-14 21:29:19.834,,,no_permission,,,,,,,,,,,,217106,,,Mon Nov 14 21:31:47 UTC 2011,,,,,,0|i0gkl3:,94765,jbellis,jbellis,,,,,,,,,14/Nov/11 21:29;jbellis;+1,14/Nov/11 21:31;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't USE numeric keyspace names in CQL,CASSANDRA-3350,12526768,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,thepaul,thepaul,11/Oct/11 22:10,12/Mar/19 14:04,13/Mar/19 22:26,12/Oct/11 14:54,0.8.8,1.0.1,,,,,0,lhf,,,,"Cassandra allows keyspace names to start with a digit or an underscore (see o.a.c.db.migration.Migration.isLegalName), but CQL's {{USE}} statement only accepts a CQL identifier, which must start with a letter. So there's no way to use a keyspace named ""142"" or ""\_hi\_"" in CQL, for example.

The {{USE}} statement should accept string literals and integers as well as identifiers, and CQL identifiers ({{IDENT}}) should probably allow starting with the underscore.",,,,,,,,,,,,,,,,12/Oct/11 10:35;xedin;CASSANDRA-3350.patch;https://issues.apache.org/jira/secure/attachment/12498728/CASSANDRA-3350.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-12 13:09:17.196,,,no_permission,,,,,,,,,,,,72419,,,Wed Oct 12 15:22:45 UTC 2011,,,,,,0|i0gitj:,94479,jbellis,jbellis,,,,,,,,,12/Oct/11 13:09;jbellis;+1,12/Oct/11 14:54;jbellis;committed,"12/Oct/11 15:22;hudson;Integrated in Cassandra-0.8 #367 (See [https://builds.apache.org/job/Cassandra-0.8/367/])
    allow numeric keyspace names in USE statement
patch by pyaskevich; reviewed by jbellis for CASSANDRA-3350

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1182413
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/Cql.g
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in hinted handoff,CASSANDRA-3385,12527792,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,yangyangyyy,yangyangyyy,19/Oct/11 17:44,12/Mar/19 14:04,13/Mar/19 22:26,27/Oct/11 16:43,1.0.1,,,,,,1,hintedhandoff,,,,"I'm using the current HEAD of 1.0.0 github branch, and I'm still seeing this error, not sure if it's  this bug or another one.



 INFO [HintedHandoff:1] 2011-10-19 12:43:17,674 HintedHandOffManager.java (line 263) Started hinted handoff for token: 11342745564
0312821154458202477256070484 with IP: /10.39.85.140
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,885 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHan
doff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,886 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more


this could possibly be related to #3291
",,,,,,,,,,,,,,,,26/Oct/11 12:58;nicktelford;0001-Changed-assertions-in-delivery-of-hints-to-a-check-t.patch;https://issues.apache.org/jira/secure/attachment/12500876/0001-Changed-assertions-in-delivery-of-hints-to-a-check-t.patch,26/Oct/11 14:31;jbellis;3385.txt;https://issues.apache.org/jira/secure/attachment/12500888/3385.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-19 19:57:49.952,,,no_permission,,,,,,,,,,,,89290,,,Thu Oct 27 16:43:20 UTC 2011,,,,,,0|i0gj93:,94549,brandon.williams,brandon.williams,,,,,,,,,"19/Oct/11 17:49;yangyangyyy;enabled assertion



 INFO [HintedHandoff:1] 2011-10-19 13:44:08,346 HintedHandOffManager.java (line 263) Started hinted handoff for token: 0 with IP: 
/10.196.37.187
ERROR [HintedHandoff:1] 2011-10-19 13:44:08,513 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHan
doff:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:285)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)



line 285 is:
                assert versionColumn != null;
well the other following fields could also be null too,
                assert versionColumn != null;
                assert tableColumn != null;
                assert keyColumn != null;
                assert mutationColumn != null;

","19/Oct/11 19:48;yangyangyyy;I just found that this contributes to another symptom I'm seeing: for RF=3, and a ring of 3 nodes, if I bring down 1 box, the remaining 2 still work fine for Quorum access, but the latency is 20x high.

I can see from debugging that a lot of time is spent on storing hints into local system table on the coordinator. but this Table.apply is slow because a lot of time is spent on the lock, while it really should not happen since the lock is sharded into 4096 ones. it turns out that all the keys used in the hints writing are the same key, at least in the examples I looked at in the debugger, if I'm correct in this observation, this is a serious bug","19/Oct/11 19:57;yangyangyyy;I see, the key in hinted table is the ID of the dead box. given that this leads to lock contention, would it be better to change the storage layout of hints? ---- I have never tried hinted handoff before, not sure if lock contention was a problem before
","19/Oct/11 19:57;jbellis;So either new-style hints are being written without versionColumn by RowMutation.hintFor, or old style hints did not get cleaned out properly by SystemTable.purgeIncompatibleHints.  But both of those look fine to me.","19/Oct/11 20:03;yangyangyyy;
the hints code was from:

https://github.com/apache/cassandra/commit/3893f24098c3d82dc31571f0b6841e2d5821ea74#diff-12

#CASSANDRA-2034

maybe it should be better to NOT let the writer wait for hints to finish? right now the local hints write make the entire write slower in probably 2 ways : 1) the main write has to wait for hint write to finish, which is slow due to lock 2) hints writes are slow, which create a lot of jobs on MUTATION stage, so even if main write does not wait for them, the MUTATION stage could possibly be bogged down with hints writes, and not able to handle normal writes fast enough


",19/Oct/11 20:07;jbellis;IMO the right fix to the lock contention is to simply not synchronize when there are no indexes present.  But that's unrelated to the assertion failure here.,"19/Oct/11 20:18;yangyangyyy;that works for me too. but I guess you will finally have to handle cases where indexes are needed. in those cases, probably we need to change the hints format away from using IP as key",19/Oct/11 20:23;jbellis;If you're not going to use IPs as keys how are you going to replay hints efficiently?  You need to consider the read path as well as the write when modeling something. :),19/Oct/11 20:32;jbellis;Created CASSANDRA-3386 for the contention problem.,"25/Oct/11 15:21;danbarker85;I am using the RPM provided by datastax, I upgraded from 0.8.7 to 1.0.0, then performed a scrub and repair on each node as described in the documentation.
This worked fine for all nodes except one, which now throws exceptions whenever I start it up, and goes ""Down"" occassionly.  All other nodes have been fine.

This is from the log, I think it could be related, but I'm not sure:

INFO [HintedHandoff:4] 2011-10-25 15:30:08,867 HintedHandOffManager.java (line 263) Started hinted handoff for token: 0 with IP: /xx.xx.xx.xx
 INFO [HintedHandoff:4] 2011-10-25 15:30:08,868 HintedHandOffManager.java (line 318) Finished hinted handoff of 0 rows to endpoint /xx.xx.xx.xx
 INFO [HintedHandoff:4] 2011-10-25 15:30:09,998 HintedHandOffManager.java (line 263) Started hinted handoff for token: 148873535527910577765226390751398592512 with IP: /xx.xx.xx.xx
ERROR [HintedHandoff:4] 2011-10-25 15:30:09,999 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:4,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:285)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
 INFO [HintedHandoff:5] 2011-10-25 15:30:59,893 HintedHandOffManager.java (line 263) Started hinted handoff for token: 106338239662793269832304564822427566080 with IP: /xx.xx.xx.xx
ERROR [HintedHandoff:5] 2011-10-25 15:30:59,894 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:5,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:285)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
 INFO [HintedHandoff:6] 2011-10-25 15:31:41,194 HintedHandOffManager.java (line 263) Started hinted handoff for token: 42535295865117307932921825928971026432 with IP: /xx.xx.xx
 INFO [HintedHandoff:6] 2011-10-25 15:31:41,194 HintedHandOffManager.java (line 318) Finished hinted handoff of 0 rows to endpoint /xx.xx.xx.xx","26/Oct/11 12:58;nicktelford;As Jonathan said, it looks like either old hints that haven't been cleaned or some corruption to the hints that are stored.

While it's probably important to find the source of this problem, I think it's really bad that we use assertions to check for it. Hinted Handoff is an optimization, so for invalid hints shouldn't break things.

The attached patch removes the assertions and replaces them with a check to ensure the hint is valid. If it's not, a warning will be emitted (so you know something's not quite right) and everything will continue as normal.",26/Oct/11 13:48;jbellis;I'd say that reasoning makes this the perfect case for assertions -- it doesn't affect anything but hints for the assert to fail.,26/Oct/11 13:52;nicktelford;Wouldn't this disrupt the delivery of hints that aren't corrupt?,"26/Oct/11 14:08;jbellis;One possible avenue for this is that if startup takes long enough (due to CL replay + sstable index sampling, probably) that compactions start before the upgrade hint purge, compaction can generation ""new"" 0.8 hints after we try to delete them.  Patch to switch to truncate to avoid this, although CASSANDRA-3399 is open to make truncate more bulletproof in that situation itself.  In the meantime, removing the hints columnfamily manually before restarting should fix the problem.

Also added some debug logging to the hint purge in r1189221.","26/Oct/11 14:10;jbellis;bq. Wouldn't this disrupt the delivery of hints that aren't corrupt?

Sure.  Point is, that's not a Big Deal.  So it's worth having the big neon sign of an exception telling people ""this ain't right.""",26/Oct/11 14:31;jbellis;new version of patch adds a second check for 0.8 hints during hint delivery,26/Oct/11 15:31;nicktelford;I guess I can live with ignoring the known types of corruption (old hints) and leaving the assertions to flag up unknown forms of corruption.,26/Oct/11 21:50;brandon.williams;+1,27/Oct/11 16:43;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"it would be nice if ""describe keyspace"" in cli shows ""Cache Provider""",CASSANDRA-3384,12527782,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,vijay2win@yahoo.com,vijay2win@yahoo.com,19/Oct/11 16:55,12/Mar/19 14:04,13/Mar/19 22:26,19/Oct/11 19:45,0.8.8,1.0.1,,Legacy/Tools,,,0,lhf,,,,Describe keyspace in the cli doesn't show the cache provider it would be nice to show it to verify the settings.,JVM on Linux,,,,,,,,,,,,,,,19/Oct/11 19:38;xedin;CASSANDRA-3384.patch;https://issues.apache.org/jira/secure/attachment/12499741/CASSANDRA-3384.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-19 17:17:53.46,,,no_permission,,,,,,,,,,,,89279,,,Wed Oct 19 23:35:45 UTC 2011,,,,,,0|i0gj8n:,94547,brandon.williams,brandon.williams,,,,,,,,,"19/Oct/11 17:17;xedin;In the latest 0.8 branch I see ""row_cache_provider"" showing, can you clarify what else do you want to see there?",19/Oct/11 17:54;brandon.williams;I too see this in 0.8.,"19/Oct/11 18:13;vijay2win@yahoo.com;I dont see it in 0.8.6 may be in the later version?


[default@unknown] use Keyspace2;
Authenticated to keyspace: Keyspace2
[default@Keyspace2] update column family Standard2 with rows_cached=1000 and row_cache_provider=SerializingCacheProvider;
bf5faf90-fa7d-11e0-0000-90ba79db4aff
Waiting for schema agreement...
[default@Keyspace2] describe keyspace Keyspace2;                                WARNING: Could not connect to the JMX on 0.0.0.0:7199, information won't be shown.

Keyspace: Keyspace2:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [us-east:2]
  Column Families:
    ColumnFamily: Standard2
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 1000.0/0
      Key cache size / save period in seconds: 1.0/14400
      Memtable thresholds: 3.5390625/120/128 (millions of ops/minutes/MB)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
[default@Keyspace2] ",19/Oct/11 18:14;xedin;cassandra-0.8 branch has it.,"19/Oct/11 19:30;vijay2win@yahoo.com;Pavel, I dont see it... are you seeing it by doing ""show keyspaces/describe keyspace <name>""? i dont see in 1.0 or the latest 0.8 build.


[default@vj] update column family Standard1 with rows_cached=1000 and row_cache_provider=SerializingCacheProvider;
7ed92090-fa88-11e0-0000-242d50cf1ffe
Waiting for schema agreement...
... schemas agree across the cluster
[default@vj] describe keyspace vj;
Keyspace: vj:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [datacenter1:1]
  Column Families:
    ColumnFamily: Standard1
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 1000.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.29062499999999997/1440/62 (millions of ops/minutes/MB)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
[default@vj] 
","19/Oct/11 19:35;xedin;Take a look at the title, it states that command is ""show schema;"" and open CliClient.java:1673 under cassandra-0.8, let me check if ""describe keyspace"" shows it.","19/Oct/11 19:38;xedin;Here is one-line patch to show it in ""describe keyspace <keyspace>;"" and ""show keyspaces""",19/Oct/11 19:40;brandon.williams;+1,19/Oct/11 19:45;xedin;Committed.,19/Oct/11 19:48;vijay2win@yahoo.com;Sorry for the confusion... Thanks +1,"19/Oct/11 23:35;hudson;Integrated in Cassandra-0.8 #382 (See [https://builds.apache.org/job/Cassandra-0.8/382/])
    CLI `describe keyspace <ks>` to show ""Row Cache Provider""
patch by Pavel Yaskevich; reviewed by Brandon Williams for CASSANDRA-3384

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1186429
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
using an ant builder in Eclipse is painful,CASSANDRA-3632,12535199,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,14/Dec/11 20:09,12/Mar/19 14:04,13/Mar/19 22:26,19/Dec/11 15:59,,,,Legacy/Tools,Packaging,,2,,,,,"The {{generate-eclipse-files}} target creates project files that use an Ant builder.  Besides being painfully slow (I've had the runs stack up behind frequent saves), many of Eclipses errors and warnings do not show unless an internal builder is used.",,,,,,,,,,,,,,,,14/Dec/11 20:09;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3632-remove-ant-builder-restore-java-builder.txt;https://issues.apache.org/jira/secure/attachment/12507423/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3632-remove-ant-builder-restore-java-builder.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-19 15:23:58.766,,,no_permission,,,,,,,,,,,,220883,,,Mon Jun 18 23:45:23 UTC 2012,,,,,,0|i0gmbj:,95046,tjake,tjake,,,,,,,,,19/Dec/11 15:23;tjake;+1!,19/Dec/11 15:59;urandom;committed; closing,"19/Dec/11 17:27;hudson;Integrated in Cassandra #1260 (See [https://builds.apache.org/job/Cassandra/1260/])
    remove ant builder; restore java builder

Patch by eevans; reviewed by Rick Shaw for CASSANDRA-3632

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1220838
Files : 
* /cassandra/trunk/build.xml
","18/Jun/12 23:45;burkhardt;For more general usage of ant, here is a tutorial for review:
http://i-proving.com/2005/10/31/ant-tutorial/ ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL inserting blank key.,CASSANDRA-3612,12534669,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,samal_,samal_,11/Dec/11 18:01,12/Mar/19 14:04,13/Mar/19 22:26,28/Mar/12 17:06,1.0.9,1.1.0,,Legacy/CQL,,,0,cql,,,,"One of our application bug inserted blank key into cluster causing assertion error on key. After checking the root cause, I found it is the bug with CQL and reproducible. Client cassandra-node and cqlsh-1.0.6.
Blank key only work when one column provided.
 
{}
cqlsh> insert into login (KEY,email)values('','');
cqlsh> select * from login;
u'' | u'email',u'' 
cqlsh> insert into login (KEY,email,verified)values('','','');
Request did not complete within rpc_timeout.
cqlsh> insert into login (KEY,verified)values('','');
Request did not complete within rpc_timeout.
cqlsh> insert into login (KEY,email)values('','');
cqlsh> 
cqlsh> select * from login;
u'' | u'email',u'' | u'uid',None
cqlsh> select * from login;
u'' | u'email',u'' | u'uid',None
cqlsh> select * from login;
u'' | u'email',u'' | u'uid',None
cqlsh> 
cqlsh> select * from login;
u'' | u'email',u'' | u'uid',None
u'samalgorai@gmail.com' | u'email',u'samalgorai@gmail.com' | u'password',u'388ad1c312a488ee9e12998fe097f2258fa8d5ee' | u'uid',UUID('05ea41dc-241f-11e1-8521-3da59237b189') | u'verified',u'0'
cqlsh> quit;
{/}

http://pastebin.com/HJn5fHhH",Linux ubuntu 3.0.0-12-generic #20-Ubuntu SMP Fri Oct 7 14:56:25 UTC 2011 x86_64 x86_64 x86_64 GNU/Linux,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-12-11 19:01:38.457,,,no_permission,,,,,,,,,,,,220390,,,Wed Mar 28 17:06:08 UTC 2012,,,,,,0|i0gm1r:,95002,xedin,xedin,,,,,,,,,"11/Dec/11 19:01;brandon.williams;Adding the exception from pastebin for posterity.

{noformat}

s.  Time: 292ms.
ERROR 22:39:02,137 Fatal exception in thread Thread[MutationStage:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.ColumnFamilyStore.markReferenced(ColumnFamilyStore.java:1236)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:83)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1275)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1161)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1123)
	at org.apache.cassandra.db.Table.readCurrentIndexedColumns(Table.java:504)
	at org.apache.cassandra.db.Table.apply(Table.java:441)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:256)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:440)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1258)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR 22:42:02,786 Fatal exception in thread Thread[MutationStage:34,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.ColumnFamilyStore.markReferenced(ColumnFamilyStore.java:1236)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:83)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1275)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1161)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1123)
	at org.apache.cassandra.db.Table.readCurrentIndexedColumns(Table.java:504)
	at org.apache.cassandra.db.Table.apply(Table.java:441)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:256)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:440)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1258)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR 22:45:40,752 Fatal exception in thread Thread[MutationStage:36,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.ColumnFamilyStore.markReferenced(ColumnFamilyStore.java:1236)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:83)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1275)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1161)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1123)
	at org.apache.cassandra.db.Table.readCurrentIndexedColumns(Table.java:504)
	at org.apache.cassandra.db.Table.apply(Table.java:441)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:256)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:440)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1258)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR 22:46:02,674 Fatal exception in thread Thread[MutationStage:37,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.ColumnFamilyStore.markReferenced(ColumnFamilyStore.java:1236)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:83)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1275)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1161)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1123)
	at org.apache.cassandra.db.Table.readCurrentIndexedColumns(Table.java:504)
	at org.apache.cassandra.db.Table.apply(Table.java:441)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:256)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:440)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1258)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}","23/Feb/12 23:01;jbellis;We shouldn't allow blank keys at all, so there's a validation failure in QueryProcessor here.  Unsure why markReferenced doesn't fail the assert for the first insert though.

(If the fix is involved let's push to 1.1.)
","24/Feb/12 20:48;thepaul;Ok, finally reproduced.  This isn't because there is more than one column with the blank key, it's because your second column (""verified"") must have an index on it. It's the ""read mutated indexes"" code path here that invokes the assertion for non-empty keys. You'll also see a similar traceback when trying to DELETE the row with the empty key.

And yes, QueryProcessor is missing validation for these write operations here (INSERT, UPDATE, DELETE, and each of those inside BATCH statements (would take a different validation path). QueryProcessor does have validation for most everything else, though.","24/Feb/12 22:43;thepaul;This particular problem is fixed in my github fork, in the 3612-1.0 and 3612-1.1 branches.

https://github.com/thepaul/cassandra/commit/5dc73e765f - 3612-1.0
https://github.com/thepaul/cassandra/commit/eab19b0979 - 3612-1.1

However, it looks like problems of a similar nature still exist in cql3, in the 1.1 series. I'll fix that and include those changes along with this ticket.","27/Feb/12 17:43;thepaul;ok, 3612-1.1 branch updated (now at https://github.com/thepaul/cassandra/commit/d921a395ae).",28/Mar/12 16:49;xedin;+1,28/Mar/12 17:06;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: DESCRIBE output for a columnfamily does not work as input to same C* instance,CASSANDRA-3596,12534401,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,08/Dec/11 21:07,12/Mar/19 14:04,13/Mar/19 22:26,08/Dec/11 21:55,1.0.6,,,Legacy/Tools,,,0,cqlsh,,,,"The {{DESCRIBE COLUMNFAMILY}} cqlsh command produces output that is intended to be usable as valid CQL (at least, when given to another Cassandra instance of the same version). But the output yields errors when run:

{noformat}
cqlsh> USE blah;
cqlsh:blah> CREATE COLUMNFAMILY cf1 (c1 int PRIMARY KEY, c2 varchar);
cqlsh:blah> DESCRIBE COLUMNFAMILY cf1;

CREATE COLUMNFAMILY cf1 (
  c1 int PRIMARY KEY,
  c2 text
) WITH
  comment='' AND
  comparator=text AND
  row_cache_provider='ConcurrentLinkedHashCacheProvider' AND
  key_cache_size=200000.000000 AND
  row_cache_size=0.000000 AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  row_cache_save_period_in_seconds=0 AND
  key_cache_save_period_in_seconds=14400 AND
  replication_on_write=True;

cqlsh:blah> CREATE COLUMNFAMILY cf1 (
        ...   c1 int PRIMARY KEY,
        ...   c2 text
        ... ) WITH
        ...   comment='' AND
        ...   comparator=text AND
        ...   row_cache_provider='ConcurrentLinkedHashCacheProvider' AND
        ...   key_cache_size=200000.000000 AND
        ...   row_cache_size=0.000000 AND
        ...   read_repair_chance=0.100000 AND
        ...   gc_grace_seconds=864000 AND
        ...   default_validation=text AND
        ...   min_compaction_threshold=4 AND
        ...   max_compaction_threshold=32 AND
        ...   row_cache_save_period_in_seconds=0 AND
        ...   key_cache_save_period_in_seconds=14400 AND
        ...   replication_on_write=True;
Bad Request: replication_on_write is not a valid keyword argument for CREATE COLUMNFAMILY
{noformat}

So it needs to do a better job of determining which CF attributes are valid for which C* versions.",Cqlsh running against a recent Cassandra build,,,,,,,,,,,,,,,08/Dec/11 21:44;thepaul;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3596-spell-replicate_on_write-right.txt;https://issues.apache.org/jira/secure/attachment/12506661/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3596-spell-replicate_on_write-right.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-08 21:23:23.361,,,no_permission,,,,,,,,,,,,220123,,,Thu Dec 08 21:55:21 UTC 2011,,,,,,0|i0glv3:,94972,jbellis,jbellis,,,,,,,,,08/Dec/11 21:23;jbellis;It looks like this is just a bug of describe spitting out a setting that CQL doesn't know how to handle (in any version).,"08/Dec/11 21:24;jbellis;... So the fix is to add support for replicate_on_write to CQL, not to band-aid it in cqlsh.","08/Dec/11 21:38;thepaul;Yeah, actually, it looks like the fix is just to spell ""replicate_on_write"" correctly.",08/Dec/11 21:55;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index Scan's will span across multiple DC's,CASSANDRA-3598,12534412,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,08/Dec/11 22:47,12/Mar/19 14:04,13/Mar/19 22:26,09/Dec/11 18:02,0.7.4,0.8.9,1.0.6,,,,0,,,,,"Looks like we send requests to all the nodes provided by StorageService.instance.getLiveNaturalEndpoints(keyspace, range.right);
We dont filter it based on blockedFor (Consistency levels).

In a multi DC setup this will cause unnecessary load on the other DC. And even within a DC we might query more nodes than needed.

",,,,,,,,,,,,,,,,09/Dec/11 17:45;vijay2win@yahoo.com;0001-super-simple-patch-3598.patch;https://issues.apache.org/jira/secure/attachment/12506760/0001-super-simple-patch-3598.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-09 18:02:14.894,,,no_permission,,,,,,,,,,,,220134,,,Fri Dec 09 18:56:24 UTC 2011,,,,,,0|i0glvr:,94975,jbellis,jbellis,,,,,,,,,09/Dec/11 17:45;vijay2win@yahoo.com;one line change fot this ticket.,09/Dec/11 18:02;jbellis;Also fixed range scans similarly.  Note that neither will have any effect unless read_repair_chance < 1.,09/Dec/11 18:05;vijay2win@yahoo.com;Thanks! and we do...,"09/Dec/11 18:11;jbellis;(Checked 0.7 branch to follow up -- I fixed this there back in March, but screwed up the merge forward.)","09/Dec/11 18:56;hudson;Integrated in Cassandra-0.8 #414 (See [https://builds.apache.org/job/Cassandra-0.8/414/])
    range and index scans now only send requests to enough replicas to satisfy requested CL + RR
patch by Vijay and jbellis for CASSANDRA-3598

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1212552
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in FailureDetector,CASSANDRA-3519,12532284,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,22/Nov/11 07:38,12/Mar/19 14:04,13/Mar/19 22:26,22/Nov/11 10:11,0.8.8,1.0.4,,,,,0,,,,,"Noticed in a 2 DC cluster, error was on node in DC 2 streaming to a node in DC 1. 

{code:java}

INFO [GossipTasks:1] 2011-11-20 18:36:05,153 Gossiper.java (line 759) InetAddress /10.6.130.70 is now dead.
ERROR [GossipTasks:1] 2011-11-20 18:36:25,252 StreamOutSession.java (line 232) StreamOutSession /10.6.130.70 failed because {} died or was restarted/removed
ERROR [AntiEntropySessions:21] 2011-11-20 18:36:25,252 AntiEntropyService.java (line 688) [repair #7fb5b1b0-11f1-11e1-0000-baed0a2090fe] session completed with the following err
or
java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
ERROR [GossipTasks:1] 2011-11-20 18:36:25,256 Gossiper.java (line 172) Gossip error
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:190)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
ERROR [AntiEntropySessions:21] 2011-11-20 18:36:25,256 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[AntiEntropySessions:21,5,RMI Runtime]
java.lang.RuntimeException: java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        ... 3 more
ERROR [RMI TCP Connection(3634)-10.29.60.10] 2011-11-20 18:36:25,256 StorageService.java (line 1712) Repair session 7fb5b1b0-11f1-11e1-0000-baed0a2090fe failed.
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.IOException: Endpoint /10.6.130.70 died
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.service.StorageService.forceTableRepair(StorageService.java:1708)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1264)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1359)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        ... 3 more
 INFO [GossipStage:1] 2011-11-20 18:36:28,173 Gossiper.java (line 777) Node /10.6.130.70 has restarted, now UP
 INFO [GossipStage:1] 2011-11-20 18:36:28,175 Gossiper.java (line 745) InetAddress /10.6.130.70 is now UP
 INFO [GossipStage:1] 2011-11-20 18:36:28,175 StorageService.java (line 885) Node /10.6.130.70 state jump to normal
{code}

FailureDetector uses a normal ArrayList for the listeners.","Free BSD 8.2 
/java -version
java version ""1.6.0_07""
Diablo Java(TM) SE Runtime Environment (build 1.6.0_07-b02)
Diablo Java HotSpot(TM) 64-Bit Server VM (build 10.0-b23, mixed mode)

",,,,,,,,,,,,,,,22/Nov/11 07:54;amorton;3519.patch;https://issues.apache.org/jira/secure/attachment/12504713/3519.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-22 10:11:31.474,,,no_permission,,,,,,,,,,,,218017,,,Tue Nov 22 12:02:39 UTC 2011,,,,,,0|i0gkwn:,94817,slebresne,slebresne,,,,,,,,,"22/Nov/11 07:54;amorton;Use a CopyOnWriteArrayList in the FailureDetector to track listeners, like the Gossiper does. ","22/Nov/11 10:11;slebresne;+1, commmitted (to 0.8 and up)","22/Nov/11 12:02;hudson;Integrated in Cassandra-0.8 #400 (See [https://builds.apache.org/job/Cassandra-0.8/400/])
    Fix ConcurrentModificationException in FailureDetector
patch by amorton; reviewed by slebresne for CASSANDRA-3519

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1204884
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/FailureDetector.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UserInterruptedException is poorly encapsulated,CASSANDRA-3582,12534138,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,07/Dec/11 05:05,12/Mar/19 14:04,13/Mar/19 22:26,08/Dec/11 20:33,1.1.0,,,,,,0,compaction,,,,,,,,,,,,,,,,,,,,07/Dec/11 15:29;jbellis;3582-v2.txt;https://issues.apache.org/jira/secure/attachment/12506458/3582-v2.txt,07/Dec/11 05:06;jbellis;3582.txt;https://issues.apache.org/jira/secure/attachment/12506393/3582.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-08 16:09:50.928,,,no_permission,,,,,,,,,,,,219860,,,Thu Dec 08 21:51:18 UTC 2011,,,,,,0|i0glov:,94944,slebresne,slebresne,,,,,,,,,07/Dec/11 05:06;jbellis;patch renames UIE to CompactionInterruptedException and moves log special casing into CompactionExecutor,07/Dec/11 15:29;jbellis;v2 extracts DebuggableThreadPoolExecutor.handleOrLog as well,"08/Dec/11 16:09;slebresne;Is there a reason for having CompactionExecutor extend TheadPoolExecutor instead of extending DebuggableThreadPoolExecutor and overriding handleOrLog (which would allow to have it protected, and have extractThrowable private)?
","08/Dec/11 16:23;jbellis;CE doesn't need any of the other DTPE baggage (primarily the blocking submit), so it feels simpler to do a clean TPE extension and avoid having to compare to DTPE to tell ""what behavior exactly am I going to get from this?""",08/Dec/11 17:14;slebresne;Feels to me that DPTE is a very thin layer on top of TPE and I've mostly been more confused by TPE default behavior than by what DPTE adds on top of that. And it also does feel to me that just overriding handleOrLog. But it's a nit. +1 in any case.,08/Dec/11 20:33;jbellis;committed,"08/Dec/11 20:34;jbellis;(logExceptionsAfterExecute and the methods it calls need to be static since we use them from DSTPE too, which doesn't and can't share a common ancestor)","08/Dec/11 21:51;hudson;Integrated in Cassandra #1249 (See [https://builds.apache.org/job/Cassandra/1249/])
    improve UserInterruptedException encapsulation (and renamed to CompactionInterruptedException)
patch by jbellis; reviewed by slebresne for CASSANDRA-3582

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1212092
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionInterruptedException.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/UserInterruptedException.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndexBuilder.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rangeSlice may iterate the whole memtable while just query one row . This may seriously affect the  performance .,CASSANDRA-3638,12535297,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,jonma,jonma,15/Dec/11 13:12,12/Mar/19 14:04,13/Mar/19 22:26,23/Dec/11 16:25,1.1.0,,,,,,0,,,,,"RangeSliceVerbHandler may  just only query one row , but cassandra may iterate the whole memtable .
the problem is in ColumnFamilyStore.getRangeSlice() method .


{color:red} // this iterator may iterate the whole memtable!!{color}
{code:title=ColumnFamilyStore.java|borderStyle=solid}
 public List<Row> getRangeSlice(ByteBuffer superColumn, final AbstractBounds range, int maxResults, IFilter columnFilter)
    throws ExecutionException, InterruptedException
    {
    ...
        DecoratedKey startWith = new DecoratedKey(range.left, null);
        DecoratedKey stopAt = new DecoratedKey(range.right, null);

        QueryFilter filter = new QueryFilter(null, new QueryPath(columnFamily, superColumn, null), columnFilter);
        int gcBefore = (int)(System.currentTimeMillis() / 1000) - metadata.getGcGraceSeconds();

        List<Row> rows;
        ViewFragment view = markReferenced(startWith, stopAt);
        try
        {
            CloseableIterator<Row> iterator = RowIteratorFactory.getIterator(view.memtables, view.sstables, startWith, stopAt, filter, getComparator(), this);
            rows = new ArrayList<Row>();

            try
            {
                // pull rows out of the iterator
                boolean first = true;
                while (iterator.hasNext()) // this iterator may iterate the whole memtable!!               
               {
                    ....
                }
            }
          .....
        }
       .....
        return rows;
    }

{code} 

{color:red} // Just only query one row ,but returned a sublist of columnFamiles   {color}
{code:title=Memtable.java|borderStyle=solid}
// Just only query one row ,but returned a sublist of columnFamiles     
public Iterator<Map.Entry<DecoratedKey, ColumnFamily>> getEntryIterator(DecoratedKey startWith)
    {
        return columnFamilies.tailMap(startWith).entrySet().iterator();
    }
{code} 


{color:red} // entry.getKey() will never bigger or equal to startKey, and then iterate the whole sublist of memtable {color}             
{code:title=RowIteratorFactory.java|borderStyle=solid}
 public IColumnIterator computeNext()
        {
            while (iter.hasNext())
            {
                Map.Entry<DecoratedKey, ColumnFamily> entry = iter.next();
                IColumnIterator ici = filter.getMemtableColumnIterator(entry.getValue(), entry.getKey(), comparator);
                // entry.getKey() will never bigger or equal to startKey, and then iterate the whole sublist of memtable             
                if (pred.apply(ici))  
                    return ici;
            }
            return endOfData();
{code} ",,,,,,,,,,,,,,,,19/Dec/11 14:25;slebresne;3638.patch;https://issues.apache.org/jira/secure/attachment/12507907/3638.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-15 15:54:11.336,,,no_permission,,,,,,,,,,,,220981,,,Fri Dec 23 17:18:28 UTC 2011,,,,,,0|i0gme7:,95058,jbellis,jbellis,,,,,,,,,"15/Dec/11 15:54;jbellis;getRangeSlice is the ""scan a lot of rows"" method.  getColumnFamily is the ""scan a single row"" method.","16/Dec/11 01:10;jonma;I was cheated by the name also . I debug RangeSliceVerbHandler.java , I find the range  contains only one key . ","19/Dec/11 13:09;jonma;
{code:title=RowIteratorFactory.java|borderStyle=solid}
public IColumnIterator computeNext()
        {
            while (iter.hasNext())
            {
                Map.Entry<DecoratedKey, ColumnFamily> entry = iter.next();
                IColumnIterator ici = filter.getMemtableColumnIterator(entry.getValue(), entry.getKey(), comparator);
                if (pred.apply(ici))  
                    return ici;
            }
            return endOfData();
{code} 
The 'iter' is a submap of memtable's columnFamilies  . If pred.apply(ici) == false , 'iter' will iterate to the end .
Suppose 'iter' is  [ c ,d ,e ,f ] ,  pred.startKey is a ,and endKey is b , pred.apply(ici)  will always be false . 
In fact ,in this case , 'iter' never need to iterate , and should directly return endOfData() . This is the problem!
So , can anybody review the code too ?

","19/Dec/11 14:25;slebresne;I think you're right. And this go way back I believe, we've never used the stopAt bound to  stop iteration early in the range slice case.

Patch attached to fix this (against trunk).",19/Dec/11 15:36;jbellis;+1,"23/Dec/11 16:25;slebresne;Committed, thanks","23/Dec/11 17:18;hudson;Integrated in Cassandra #1267 (See [https://builds.apache.org/job/Cassandra/1267/])
    Optimize memtable iteration during range scan
patch by slebresne; reviewed by jbellis for CASSANDRA-3638

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1222728
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowIteratorFactory.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush non-cfs backed secondary indexes along with CF,CASSANDRA-3659,12536158,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,22/Dec/11 14:04,12/Mar/19 14:04,13/Mar/19 22:26,22/Dec/11 16:14,1.0.7,,,Feature/2i Index,,,0,secondary_index,,,,Non CFS backed secondary indexes currently don't get flushed alongside CF.  Only CFS backed ones do (i.e. KEYS),,,,,,,,,,,,,,,,22/Dec/11 14:06;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3659-flush-non-cfs-backed-indexes.txt;https://issues.apache.org/jira/secure/attachment/12508390/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3659-flush-non-cfs-backed-indexes.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-22 15:12:21.24,,,no_permission,,,,,,,,,,,,221841,,,Thu Dec 22 16:14:40 UTC 2011,,,,,,0|i0gmof:,95104,xedin,xedin,,,,,,,,,22/Dec/11 15:12;xedin;+1,22/Dec/11 16:14;tjake;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CASSANDRA-2335 was properly resolved - artifact:pom doesn't support the ""groupId"" attribute",CASSANDRA-3650,12535787,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,somerandomperson,somerandomperson,20/Dec/11 06:02,12/Mar/19 14:04,13/Mar/19 22:26,20/Dec/11 06:05,,,,,,,0,,,,,"CASSANDRA-2335 was about a build error that windows users will see saying ""artifact:pom doesn't support the ""groupId"" attribute"". The fix for this is described in http://wiki.apache.org/cassandra/RunningCassandraInEclipse#artifact:pom_error and was outlined in CASSANDRA-2335. This bug was originally filed in the mistaken belief that the issue wasn't noted, when it had been.",Windows,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,221470,,,Tue Dec 20 06:05:56 UTC 2011,,,,,,0|i0gmkf:,95086,ben.coverston@datastax.com,ben.coverston@datastax.com,,,,,,,,,20/Dec/11 06:05;somerandomperson;Next time read the build instructions correctly. This error is noted at the end.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
correct and improve stream protocol mismatch error,CASSANDRA-3652,12535802,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,20/Dec/11 08:38,12/Mar/19 14:04,13/Mar/19 22:26,22/Dec/11 20:32,1.0.7,,,,,,0,,,,,"The message (and code comment) claims it got a ""newer"" version despite the fact that the check only determines that it is non-equal.

Fix that, and also print the actual version gotten and expected.
",,,,,,,,,,,,,,,,20/Dec/11 08:41;scode;CASSANDRA-3652-1.0.txt;https://issues.apache.org/jira/secure/attachment/12508059/CASSANDRA-3652-1.0.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-22 20:32:44.451,,,no_permission,,,,,,,,,,,,221485,,,Thu Dec 22 20:32:44 UTC 2011,,,,,,0|i0gmlb:,95090,jbellis,jbellis,,,,,,,,,20/Dec/11 08:41;scode;Patch attached.,22/Dec/11 20:32;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
value validator in the cli does not pick up,CASSANDRA-3553,12533553,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,cywjackson,cywjackson,02/Dec/11 00:49,12/Mar/19 14:04,13/Mar/19 22:26,02/Dec/11 14:39,1.0.6,,,Legacy/Tools,,,0,cli,,,,"the summary is probably confusing, so here is an example:

{noformat}
[default@testks] describe testcf;
    ColumnFamily: testcf
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.LongType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType
{noformat}

notice both column and column value are under LongType

in the cli (without assume):

[default@testks] set testcf['foo'][1293843587]=30; 
null
InvalidRequestException(why:(Expected 8 or 0 byte long (2)) [testks][testcf][1293843587] failed validation)
        at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15198)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:858)
        at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:830)
        at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:902)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:216)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)

so the above so the value cannot be validated, so now lets try to change the column name also:

[default@testks] set testcf['foo'][30]=30;               
null
InvalidRequestException(why:(Expected 8 or 0 byte long (2)) [testks][testcf][30] failed validation)
        at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15198)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:858)
        at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:830)
        at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:902)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:216)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)

now lets set value with 8 characters:

[default@testks] set testcf['foo'][30]=12345678;  
Value inserted.

so that shows column is fine, only the value part is not fine. put it in assume or long() works:

[default@testks] assume testcf validator as long;
Assumption for column family 'testcf' added successfully.
[default@testks] set testcf['foo'][30]=30;       
Value inserted.


or (restart to a new session to un-assume):

[default@testks] set testcf['foo'][30]=long(30);         
Value inserted.",,,,,,,,,,,,,,,,02/Dec/11 14:26;xedin;CASSANDRA-3553-1-0-specific-changes.patch;https://issues.apache.org/jira/secure/attachment/12505895/CASSANDRA-3553-1-0-specific-changes.patch,02/Dec/11 12:04;xedin;CASSANDRA-3553.patch;https://issues.apache.org/jira/secure/attachment/12505876/CASSANDRA-3553.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-02 12:04:09.731,,,no_permission,,,,,,,,,,,,219281,,,Fri Dec 02 14:39:48 UTC 2011,,,,,,0|i0glbr:,94885,jbellis,jbellis,,,,,,,,,"02/Dec/11 12:04;xedin;it wasn't peaking correct default validation class, just converting everything to bytes...","02/Dec/11 14:13;jbellis;For consistency with the rest of the CLI we should use defaultValidator.fromString(columnValue) even for BytesType.  (Which means we should probably make this change only in 1.0, since 0.8 users may be relying on the old, broken behavior.)","02/Dec/11 14:26;xedin;I agree, here is patch with 1.0 specific changes.",02/Dec/11 14:31;jbellis;+1,02/Dec/11 14:39;xedin;Committed to 1.0 branch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A streamOutSession keeps sstables references forever if the remote end dies,CASSANDRA-3216,12523332,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,16/Sep/11 13:36,12/Mar/19 14:04,13/Mar/19 22:26,19/Sep/11 15:32,1.0.0,,,,,,0,streaming,,,,"A streamOutSession acquire a reference on the sstable it will stream and release them as soon as each sstable has been fully streamed. However, since a stream session has currently no means to know when it failed, we'll keep references indefinitely (meaning until next restart) if their is a failure. One way a stream session could very easily fail is if the remote end dies. We must make sure we correctly release sstable references when that happens.

Note that it won't be bulletproof, there is probably other means by which a streaming could fail: a bug in the code throwing an exception, no space left on the receiving end, etc... But those are unlikely enough that I propose to care only for the case of a node dying for now and leave the bullet-proofing to CASSANDRA-3112. ",,,,,,,,,,,,,,,,16/Sep/11 13:37;slebresne;3216.patch;https://issues.apache.org/jira/secure/attachment/12494794/3216.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-16 19:10:05.417,,,no_permission,,,,,,,,,,,,4015,,,Mon Sep 19 15:32:20 UTC 2011,,,,,,0|i0gh7r:,94219,jbellis,jbellis,,,,,,,,,"16/Sep/11 19:10;jbellis;Couldn't this cause false positives if a GC pause marks a node as ""down"" but really the stream would finish normally afterwards?","19/Sep/11 06:52;slebresne;It would, it's the same problem than with CASSANDRA-2433. But the patch the technique we've use there to wait to reach twice the normal phi convict threshold to hopefully make sure this won't happen. But we can increase the threshold even higher if we think that doubling doesn't put us enough on the safe side.",19/Sep/11 13:57;jbellis;+1,"19/Sep/11 15:32;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unsustainable Thread Accumulation in ParallelCompactionIterable.Reducer ThreadPoolExecutor,CASSANDRA-3711,12537700,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,maedhroz,maedhroz,08/Jan/12 23:46,12/Mar/19 14:04,13/Mar/19 22:26,10/Jan/12 08:29,1.0.7,,,,,,0,compaction,memory_leak,threading,threads,"With multithreaded compaction enabled, it looks like Reducer creates a new thread pool for every compaction.  These pools seem to just sit around - i.e. ""executor.shutdown()"" never gets called and the Threads live forever waiting for tasks that will never come.  For instance...


Name: CompactionReducer:1
State: TIMED_WAITING on java.util.concurrent.SynchronousQueue$TransferStack@72938aea
Total blocked: 0  Total waited: 1

Stack trace: 
 sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1043)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1103)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
java.lang.Thread.run(Thread.java:722)
","- Linux version 2.6.32-71.29.1.el6.x86_64 (mockbuild@c6b5.bsys.dev.centos.org) (gcc version 4.4.4 20100726 (Red Hat 4.4.4-13) (GCC) ) #1 SMP Mon Jun 27 19:49:27 BST 2011
- java version ""1.7.0_01"" / Java(TM) SE Runtime Environment (build 1.7.0_01-b08) / Java HotSpot(TM) 64-Bit Server VM (build 21.1-b02, mixed mode)",,,,,,,,,,,,,,,09/Jan/12 09:53;maedhroz;3711.2.txt;https://issues.apache.org/jira/secure/attachment/12509889/3711.2.txt,09/Jan/12 04:12;jbellis;3711.txt;https://issues.apache.org/jira/secure/attachment/12509875/3711.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-01-09 04:12:06.844,,,no_permission,,,,,,,,,,,,223206,,,Tue Jan 10 08:29:22 UTC 2012,,,,,,0|i0gnb3:,95206,slebresne,slebresne,,,,,,,,,09/Jan/12 04:12;jbellis;You're absolutely right.  Patch attached.,"09/Jan/12 09:50;maedhroz;Would it be useful to just create the executor once and reuse it for all compactions, then get rid of it in a shutdown hook or something similar?  I've attached a patch, 3711.2.txt, that does roughly that, if you want to take a look.","09/Jan/12 09:53;maedhroz;I guess I should grant the license, just in case ;)","09/Jan/12 17:37;jbellis;That changes the behavior, though, since now you have a single pool of AvailableProcessors across all compactions, rather than one per compaction.  Increasing the thread count proportionate to concurrent compactions setting isn't a silver bullet either, since you would still have the possibility of a large compaction ""starving"" out smaller ones, which was the problem that concurrent compactions were introduced to solve.","09/Jan/12 18:28;maedhroz;Yeah, I think you're right.  In retrospect, my idea would only have been useful (maybe) if the compactions were CPU-bound, which I guess is not the case.

I can't wait for this release, because I can stop doing rolling restarts every few days!","10/Jan/12 08:29;slebresne;+1 on v1, committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple threads can attempt hint handoff to the same target,CASSANDRA-3681,12536553,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,29/Dec/11 03:03,12/Mar/19 14:04,13/Mar/19 22:26,05/Jan/12 23:06,1.0.7,,,,,,0,hintedhandoff,,,,"HintedHandOffManager attempts to prevent multiple threads sending hints to the same target with the queuedDeliveries set, but the code is buggy.  If two handoffs *do* occur concurrently, the second thread can use an arbitrarily large amount of memory skipping tombstones when it starts paging from the beginning of the hint row, looking for the first live hint.  (This is not a problem with a single thread, since it always pages starting with the last-seen hint column name, effectively skipping the tombstones.  Then it compacts when it's done.)

Technically this bug is present in all older Cassandra releases, but it only causes problems in 1.0.x since the hint rows tend to be much larger (since there is one hint per write containing the entire mutation, instead of just one per row consisting of just the key).",,,,,,,,,,,,,,,,05/Jan/12 22:41;jbellis;3681-v3.txt;https://issues.apache.org/jira/secure/attachment/12509620/3681-v3.txt,29/Dec/11 03:05;jbellis;3681.txt;https://issues.apache.org/jira/secure/attachment/12508816/3681.txt,05/Jan/12 19:26;brandon.williams;3681v2.txt;https://issues.apache.org/jira/secure/attachment/12509596/3681v2.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-01-04 15:48:02.09,,,no_permission,,,,,,,,,,,,222236,,,Thu Jan 05 23:06:49 UTC 2012,,,,,,0|i0gmxj:,95145,brandon.williams,brandon.williams,,,,,,,,,29/Dec/11 03:05;jbellis;patch applies on top of the one for CASSANDRA-3624.,"04/Jan/12 15:48;brandon.williams;The problem here is that now the schema check comes before the FD check:

{noformat}
ERROR 15:45:37,154 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: Didin't receive gossiped schema from /10.179.64.227 in 60000ms
        at org.apache.cassandra.db.HintedHandOffManager.waitForSchemaAgreement(HintedHandOffManager.java:210)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:239)
        at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:385)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

I added another FD check for this in the patch on CASSANDRA-3677 (which isn't the problem there) but something similar should work.",05/Jan/12 19:26;brandon.williams;v2 does the FD check before and after the schema check.,"05/Jan/12 22:41;jbellis;v3 changes waitForSchemaAgreement to throw TimeoutException, which deliver can then catch and abort.  Also moves the queuedDeliveries removal back to a try/finally block, but covering the entire method.",05/Jan/12 23:01;brandon.williams;+1,05/Jan/12 23:06;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local range scans are not run on the read stage,CASSANDRA-3687,12536810,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Jan/12 14:53,12/Mar/19 14:04,13/Mar/19 22:26,16/Aug/12 20:30,1.2.0 beta 1,,,,,,0,,,,,"Running directly on the client request/StorageProxy thread means we're now allowing one range scan per thrift thread instead of one per read stage thread [which may be more, or less, depending on thrift server mode], and it bypasses the ""drop hopeless requests"" overcapacity protection built in there. ",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-18 00:54:00.945,,,no_permission,,,,,,,,,,,,222493,,,Thu Aug 16 20:30:28 UTC 2012,,,,,,0|i0gn0f:,95158,vijay2win@yahoo.com,vijay2win@yahoo.com,,,,,,,,,02/Jan/12 14:53;jbellis;Look at SP.LocalReadRunnable for how to do this safely. Simplest fix would be to just continue routing all range scans over MessagingService.,"14/May/12 22:53;jbellis;Patch pushed to https://github.com/jbellis/cassandra/branches/3687.  (Simple fix, but Jira attach file is broken.)

Generalizes AbstractRowResolver.injectPreProcessed into ReadCallback.response(TMessage) and adds LocalRangeSliceRunnable.","18/May/12 00:54;vijay2win@yahoo.com;+10, one thing which i noticed was will it be better to remove (coz the localnode might not be in position 0).

{code}
if (handler.endpoints.size() == 1 && handler.endpoints.get(0).equals(FBUtilities.getBroadcastAddress())
{code}

and add it to 

{code}
for (InetAddress endpoint : handler.endpoints)
{
    if (endpoint.equals(FBUtilities.getBroadcastAddress())
    {
        logger.debug(""reading data locally"");
        StageManager.getStage(Stage.READ).execute(new LocalRangeSliceRunnable(nodeCmd, handler));
    }
{code}

similar to fetchRows?","23/May/12 21:31;jbellis;We've already checked that size == 1, so position 0 is the only possible place for it to be. :)",12/Jul/12 03:58;vijay2win@yahoo.com;+1,16/Aug/12 20:30;jbellis;rebased and committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
strange values of pending tasks with compactionstats (below 0),CASSANDRA-3693,12537183,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,zenek_kraweznik0,zenek_kraweznik0,04/Jan/12 12:53,12/Mar/19 14:04,13/Mar/19 22:26,11/Jan/12 07:56,1.0.7,,,Legacy/Tools,,,0,compaction,,,,"during scrub:

Every 2.0s: for i in 1 2 3; do nodetool -h 192.168.2.$i compactionstats; done                                                                                                                                     Wed Jan  4 13:48:13 2012

pending tasks: 2147483646
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction         Archive        Messages     28034971475     72393139120    38.73%
pending tasks: -2147483647
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction         Archive        Messages     24575687282     72385305067    33.95%
pending tasks: 0

","linux, oracle java 1.6.26",,,,,,,,,,,,,,,10/Jan/12 23:58;jbellis;3693.txt;https://issues.apache.org/jira/secure/attachment/12510128/3693.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-05 23:12:49.963,,,no_permission,,,,,,,,,,,,222692,,,Wed Jan 11 10:34:01 UTC 2012,,,,,,0|i0gn33:,95170,slebresne,slebresne,,,,,,,,,05/Jan/12 09:10;zenek_kraweznik0;Also my cassandra is doing compactions on two nodes in loop. restart of cassandra service doesn't fix this. After restart compaction is also started.,"05/Jan/12 23:12;jbellis;Are you using LeveledCompaction?

How much data do you have on disk?  How many sstables?","08/Jan/12 21:13;zenek_kraweznik0;I'm using leveled compaction (with 4096mb limit, but with compression sstables are smaller).

There's a 80GB per node (3 nodes, replication factor=3).

# find /var/lib/cassandra/data -type f | wc -l
273
#

=> 273 sstables

compactionstats looks now like this:

pending tasks: -2147483648
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction         Archive        Messages     35050352366               0       n/a

I wonder why 0 bytes total is visible on this node (nodetool ring is reporting 37.18GB).

After every compactions bytes total is about 73xxxxxxxxx (i guess it is not compress data size), but this value isn't saved anywhere.",10/Jan/12 23:58;jbellis;Patch to keep compaction task count arithmetic as a long until after the division by sstable size has been done.,"11/Jan/12 07:56;slebresne;+1, committed","11/Jan/12 10:34;zenek_kraweznik0;ok, thanks. I'll chech it after ugprade to 1.0.7

And second question from my last comment: ""I wonder why 0 bytes total is visible on this node during compaction (nodetool ring is reporting 37.18GB).""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE on invalid CQL DELETE command,CASSANDRA-3755,12539021,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,thepaul,thepaul,19/Jan/12 19:31,12/Mar/19 14:04,13/Mar/19 22:26,29/Mar/12 14:33,1.0.9,,,,,,0,cql,,,,"The CQL command {{delete from k where key='bar';}} causes Cassandra to hit a NullPointerException when the ""k"" column family does not exist, and it subsequently closes the Thrift connection instead of reporting an IRE or whatever. This is probably wrong.",,,,,,,,,,,,,,,,20/Jan/12 06:48;dbrosius@apache.org;unknown_cf.diff;https://issues.apache.org/jira/secure/attachment/12511230/unknown_cf.diff,27/Mar/12 23:58;dbrosius@apache.org;unknown_cf_2.diff;https://issues.apache.org/jira/secure/attachment/12520215/unknown_cf_2.diff,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-01-20 06:48:17.399,,,no_permission,,,,,,,,,,,,224523,,,Thu Mar 29 14:33:22 UTC 2012,,,,,,0|i0gnu7:,95292,slebresne,slebresne,,,,,,,,,"20/Jan/12 06:48;dbrosius@apache.org;throw a IRE exception with understandable error message, rather than NPE.","24/Jan/12 07:22;dbrosius@apache.org;ah, that's much better, thanks...don't allow write actions from the client on system keyspace  -> attached.",24/Jan/12 16:12;jbellis;The second patch is for CASSANDRA-3759. I'll apply it there.  Did you mean to revise this one as well?,"24/Jan/12 22:33;dbrosius@apache.org;Arg, my apologies, posted to wrong bug.",25/Jan/12 00:25;jbellis;So the patch here is ready for review for fixing the NPE problem?,25/Jan/12 01:48;dbrosius@apache.org;yes,"08/Feb/12 17:12;slebresne;I believe the right fix would be to use ThriftValidation.validateColumnFamily that catches that kind of problems. Looking at DeleteStatement, it seems that it re-validate the column family for each key in mutationForKey(), but that's done later and thus the NPE is thrown first. We should probably move the validateColumnFamily up in prepareRowMutations() and then pass the resulting metadata as an argument of mutationForKey to avoid the multiple validation.","27/Mar/12 14:29;jbellis;Dave, did you want to take a stab at that approach?",27/Mar/12 23:35;dbrosius@apache.org;sure,"27/Mar/12 23:58;dbrosius@apache.org;pushed up Thrift.validateColumnFamily as Sylvain suggests.

(against trunk)

unknown_cf_2.diff","29/Mar/12 14:33;slebresne;+1, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh and cassandra-cli show keys differently for data created via stress tool,CASSANDRA-3726,12538024,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,cdaw,cdaw,11/Jan/12 07:39,12/Mar/19 14:04,13/Mar/19 22:26,20/Jan/12 19:25,1.0.8,,,,,,0,,,,,"{code}

// Run: stress --operation=INSERT --num-keys=5  --columns=2 --consistency-level=QUORUM --column-size=1 --threads=1 --replication-factor=1 --nodes=localhost

// cqlsh
cqlsh:Keyspace1> select * from Standard1;
 KEY,3 | C0,c | C1,c | 
 KEY,0 | 
 KEY,2 | C0,c | C1,c | 
 KEY,1 | C0,c | C1,c | 
 KEY,4 | C0,c | C1,c | 

cqlsh:Keyspace1> describe columnfamily Standard1;

CREATE COLUMNFAMILY Standard1 (
  KEY blob PRIMARY KEY
) WITH
  comment='' AND
  comparator=ascii AND
  row_cache_provider='ConcurrentLinkedHashCacheProvider' AND
  key_cache_size=200000.000000 AND
  row_cache_size=0.000000 AND
  read_repair_chance=1.000000 AND
  gc_grace_seconds=864000 AND
  default_validation=blob AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  row_cache_save_period_in_seconds=0 AND
  key_cache_save_period_in_seconds=14400 AND
  replicate_on_write=True;


// cassandra-cli
[default@Keyspace1] list Standard1;    
Using default limit of 100
-------------------
RowKey: 33
=> (column=C0, value=63, timestamp=1326259960705)
=> (column=C1, value=63, timestamp=1326259960705)
-------------------
RowKey: 30
-------------------
RowKey: 32
=> (column=C0, value=63, timestamp=1326259960704)
=> (column=C1, value=63, timestamp=1326259960704)
-------------------
RowKey: 31
=> (column=C0, value=63, timestamp=1326259960704)
=> (column=C1, value=63, timestamp=1326259960704)
-------------------
RowKey: 34
=> (column=C0, value=63, timestamp=1326259960705)
=> (column=C1, value=63, timestamp=1326259960705)

[default@Keyspace1] describe Standard1;
    ColumnFamily: Standard1
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.AsciiType
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Row Cache Provider: org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
{code}
",,,,,,,,,,,,,,,,20/Jan/12 16:54;thepaul;3726.patch.2.txt;https://issues.apache.org/jira/secure/attachment/12511273/3726.patch.2.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-19 16:52:40.245,,,no_permission,,,,,,,,,,,,223530,,,Fri Jan 20 19:25:01 UTC 2012,,,,,,0|i0gnhj:,95235,jbellis,jbellis,,,,,,,,,"19/Jan/12 16:52;thepaul;Added a bunch to cqlsh's output formatting so that certain types (in particular, blob) are shown in a more human-readable way. Used colors (when enabled) to help distinguish hex strings from text strings, and to distinguish control characters in text strings.

The attached patch contains the changes, or the 3726 branch in my github fork can be used:

https://github.com/thepaul/cassandra/commits/3726",20/Jan/12 00:01;jbellis;Can you rebase on top of 1.0?  I think 3586/3587 conflict.,"20/Jan/12 16:54;thepaul;So rebased, on top of cassandra-1.0 instead of trunk since that's where you seem to be putting it. New branch on my github is ""rebased/3726/1"", or new patch attached.

https://github.com/thepaul/cassandra/commits/rebased/3726/1","20/Jan/12 17:38;jbellis;1e6defcaba90b7313d87c2626cbb6ae418b40567 still doesn't apply cleanly to 1.0 for me.

Incidentally, why is switching ""out=sys.stdout"" to ""out=None"" and then special casing none in the method body an improvement?  Are you expecting stdout to change?","20/Jan/12 17:39;jbellis;... the new squashed patch applies fine to 1.0 though, so no need to rebase again.","20/Jan/12 17:52;thepaul;sys.stdout can change, yeah, and that might happen depending on how we decide to implement output capture vis à vis CASSANDRA-3479. Note that's different from just dup2-ing another stream onto fd 0.

bq. 1e6defcaba90b7313d87c2626cbb6ae418b40567 still doesn't apply cleanly to 1.0 for me.

Are you sure? Its only parent is fc6103966cbe90c96e75f4b52c10376b9df468d7, which is currently the head of cassandra-1.0 on the git-wip-us.apache.org repo.",20/Jan/12 19:25;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming CommitLog backup,CASSANDRA-3690,12536930,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,03/Jan/12 00:39,12/Mar/19 14:04,13/Mar/19 22:26,09/Apr/12 16:24,1.1.1,,,Legacy/Tools,,,0,,,,,"Problems with the current SST backups
1) The current backup doesn't allow us to restore point in time (within a SST)
2) Current SST implementation needs the backup to read from the filesystem and hence additional IO during the normal operational Disks
3) in 1.0 we have removed the flush interval and size when the flush will be triggered per CF, 
          For some use cases where there is less writes it becomes increasingly difficult to time it right.
4) Use cases which needs BI which are external (Non cassandra), needs the data in regular intervals than waiting for longer or unpredictable intervals.

Disadvantages of the new solution
1) Over head in processing the mutations during the recover phase.
2) More complicated solution than just copying the file to the archive.

Additional advantages:
Online and offline restore.
Close to live incremental backup.

Note: If the listener agent gets restarted, it is the agents responsibility to Stream the files missed or incomplete.

There are 3 Options in the initial implementation:
1) Backup -> Once a socket is connected we will switch the commit log and send new updates via the socket.
2) Stream -> will take the absolute path of the file and will read the file and send the updates via the socket.
3) Restore -> this will get the serialized bytes and apply's the mutation.

Side NOTE: (Not related to this patch as such) The agent which will take incremental backup is planned to be open sourced soon (Name: Priam).",,,,,,,,,,,,,,,,08/Mar/12 23:11;vijay2win@yahoo.com;0001-CASSANDRA-3690-v2.patch;https://issues.apache.org/jira/secure/attachment/12517636/0001-CASSANDRA-3690-v2.patch,07/Apr/12 01:00;vijay2win@yahoo.com;0001-CASSANDRA-3690-v4.patch;https://issues.apache.org/jira/secure/attachment/12521798/0001-CASSANDRA-3690-v4.patch,08/Apr/12 02:51;vijay2win@yahoo.com;0001-CASSANDRA-3690-v5.patch;https://issues.apache.org/jira/secure/attachment/12521871/0001-CASSANDRA-3690-v5.patch,24/Jan/12 05:42;vijay2win@yahoo.com;0001-Make-commitlog-recycle-configurable.patch;https://issues.apache.org/jira/secure/attachment/12511632/0001-Make-commitlog-recycle-configurable.patch,24/Jan/12 05:42;vijay2win@yahoo.com;0002-support-commit-log-listener.patch;https://issues.apache.org/jira/secure/attachment/12511633/0002-support-commit-log-listener.patch,24/Jan/12 05:42;vijay2win@yahoo.com;0003-helper-jmx-methods.patch;https://issues.apache.org/jira/secure/attachment/12511634/0003-helper-jmx-methods.patch,24/Jan/12 05:42;vijay2win@yahoo.com;0004-external-commitlog-with-sockets.patch;https://issues.apache.org/jira/secure/attachment/12511635/0004-external-commitlog-with-sockets.patch,24/Jan/12 05:42;vijay2win@yahoo.com;0005-cmmiting-comments-to-yaml.patch;https://issues.apache.org/jira/secure/attachment/12511636/0005-cmmiting-comments-to-yaml.patch,09/Apr/12 15:04;jbellis;3690-v6.txt;https://issues.apache.org/jira/secure/attachment/12521959/3690-v6.txt,,,9.0,,,,,,,,,,,,,,,,,,,2012-01-05 23:38:24.618,,,no_permission,,,,,,,,,,,,222525,,,Mon Apr 09 16:24:43 UTC 2012,,,,,,0|i0gn1r:,95164,jbellis,jbellis,,,,,,,,,"05/Jan/12 23:38;jbellis;The socket business sounds complicated.  CASSANDRA-1602 is a lot more straightforward, I'd recommend starting with that.","05/Jan/12 23:52;vijay2win@yahoo.com;Hi Jonathan, But there is additional IO which the server has to do to copy the archive logs to a different location (not locally)... 
While streaming the Commit log back to the server we have to copy it first and then read it back which is also a over head in recovery. 

Something like copying the data to S3 in amazon and copying right back for the node for recovery. (this backup will also be used for test cluster refresh for prod data and BI which is completely a different system)
Recovery in most case are loose of instance or the whole cluster (Virtual machines).","24/Jan/12 05:34;vijay2win@yahoo.com;0001 => Adds a configuration so we can avoid recycling in case some one wants to copy the files across to another location like a archive logs
0002 => Adds CommitLogListener, implementation can recive the updates to the commitlogs.
0003 => helper JMX in case the user wants to query the active CL's
0004 => this can go to the tools folder/we dont need to commit it to the core.","09/Feb/12 23:35;jbellis;bq. But there is additional IO which the server has to do to copy the archive logs to a different location (not locally)... While streaming the Commit log back to the server we have to copy it first and then read it back which is also a over head in recovery.

What if you mounted the archive logs via s3fs?","21/Feb/12 23:11;jasobrown;We spent some time looking into s3fs, and ran into problems between fuse (which s3fs depends on) and mmap. I put together a small java app to simulate writing to local disc vs. s3fs using mmapp'ed and non-mmap'ed files. (Note most of my java sample code is based on CommitLogSegment's implementation.) We found the non-mmap'ed files wrote to s3fs without a hitch, but writing to the mmap'ed S3 mount failed. The failure was different between OpenJDK 6 vs. Sun JDK, but they were both SIGBUS errors. Also, I ran the tests on my local ubuntu box running Linux 3.0.0-12, fuse version (fusermount --version) at 2.8.4. 

At this point, it seems like s3fs isn't as viable as one would hope.



","22/Feb/12 19:38;jbellis;That does rule out using s3fs in read/write mode, but I imagine that would be a pretty bad idea from a latency standpoint anyway.  But in the context of just mounting log files for replay/recover, CommitLog uses RandomAccessReader which is ordinary buffered i/o.",22/Feb/12 19:50;vijay2win@yahoo.com;Starting to think... What we really want is a Async Triggers CASSANDRA-1311 which listens for all the updates + a way to restore the data with mutation before starting the node. In someways thats what the original patch was trying to do will it make sense to merge these two efforts?,"22/Feb/12 20:46;jbellis;I'm skeptical of trying to do this on top of triggers.  First, CASSANDRA-1311 seems to lean towards coordinator-level triggers rather than replica-level.  Second, I don't think it makes sense for a Trigger-level API to deal with raw bytes, which would mean losing efficiency from having to re-serialize RowMutations.

I like the postgresql approach: http://www.postgresql.org/docs/9.1/static/continuous-archiving.html -- briefly, you configure an {{archive_command}} that tells it how you want it to copy full log segments off-server when full, and set up a recovery.conf file when you want to recover, which includes a {{restore_command}} that is the inverse of the archive command.

The main difference is that postgresql's default wal segment size is 16MB, which gives them a finer resolution than our 128MB.  I can't think of a reason we can't lower ours, though.","08/Mar/12 23:11;vijay2win@yahoo.com;Hi Jonathan,

Attached patch does exactly what we discussed here. Its almost the same as PostgreSQL :) 

In addition we can start the node with -Dcassandra.join_ring=false and then use JMX to restore files one by one via JMX.

Plz let me know.","14/Mar/12 04:29;jbellis;I don't see any uses of CommitLogRecover outside of CommitLog, which makes me think this is an unrelated refactoring.  Is that correct?  If so, let's split that out of this ticket.","14/Mar/12 04:37;vijay2win@yahoo.com;Hi Jonathan, The refactor is mainly for the following:

>>> In addition we can start the node with -Dcassandra.join_ring=false and then use JMX to restore files one by one via JMX.
This allows us to start the recovery process before all the files are downloaded from S3/External source. I can do that it in another ticket, let me know. Thanks!","05/Apr/12 21:45;jbellis;I've made a bunch of minor changes and pushed to https://github.com/jbellis/cassandra/branches/3690-v3.

I noticed that we need to wait for the archive to finish whether we end up recycling or not.  Seems to me it would be simpler to continue to always recycle, but (as we have here) wait for the archive first.  So archive can copy off to s3 or whatever directly, instead of ln somewhere else as an intermediate step.  Total i/o will be lower and commitlog will create extra segments if needed in the meantime.

Maybe we should also have a restore_list_segments command as well, so we can query s3 (again for instance) directly and have restore_command pull from there, rather than requiring a local directory?","05/Apr/12 21:53;jbellis;Also: would be nice to get rid of the new Thread / busywait archive dance.  If we used an ExecutorService instead, we could add the Future to the segment and just say segment.waitForArchive(), no looping.","07/Apr/12 00:58;vijay2win@yahoo.com;Hi Jonathan, Attached patch incorporates all the recommended changes.... except

>>> Maybe we should also have a restore_list_segments command as well, so we can query s3 (again for instance) directly and have restore_command pull from there, rather than requiring a local directory?
IMHO. It might be better if we have a streaming API to list and stream the data in... otherwise we need have to download to the local FS anyways, So it will be better to incrementally download and use the JMX to restore the files independently (example: A external agent), that may be a simple solution for now..... If the user has a NFS mount it will work even better all he needs to do is to ""ln -s"" location and he is done :)

Plz note that i also removed the requirement to turn off recycling for backup (as recommended), but i left that as configurable because it will good to have unique names in the backup sometimes so we dont overwrite :)","07/Apr/12 01:40;jbellis;bq. it will good to have unique names in the backup sometimes so we dont overwrite 

If you think about it, ""target filename"" is just a suggestion... you'd be free to ignore it and generate a different filename (incorporating timestamp for instance, or even a uuid) in the archive script.","07/Apr/12 01:51;vijay2win@yahoo.com;Hi Jonathan, Agree, would you like to remove the configuration to disable recycle (which this patch added)? let me know... Thanks!",07/Apr/12 20:43;jbellis;I do think we should make recycling always-on; it's a non-negligible performance win and so far we don't have a use case that requires disabling it.,"08/Apr/12 02:51;vijay2win@yahoo.com;Hi Jonathan, 
v5 removes the recycle related changes and
Added 2 JMX (getActiveSegmentNames and getArchivingSegmentNames)

(list all files) - (getActiveSegmentNames) will provide a view of orphan files which failed archiving...","09/Apr/12 15:04;jbellis;v6 attached.  The primary changes made are fixes to the Future logic: the only way you'll get a null Future back is if no archive tack was submitted; if it errors out, you'll get an ExecutionException when you call get(), but never a null.

Updated getArchivingSegmentNames javadoc to emphasize that it does NOT include failed archive attempts. Not sure if this is what was intended.",09/Apr/12 15:33;vijay2win@yahoo.com;+1,09/Apr/12 16:24;jbellis;committed w/ some final improvements to yaml comments,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
parsing of chunk_length_kb silently overflows,CASSANDRA-3644,12535605,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,18/Dec/11 01:32,12/Mar/19 14:04,13/Mar/19 22:26,22/Dec/11 20:40,1.0.7,,,,,,0,,,,,"Not likely to trigger for ""real"" values; I noticed because some other bug caused the chunk length setting to be corrupted somehow and take on some huge value having nothing to do with what I asked for in my schema update (not yet identified why; separate issue).",,,,,,,,,,,,,,,,20/Dec/11 06:10;scode;CASSANDRA-3644-1.0-v2.txt;https://issues.apache.org/jira/secure/attachment/12508052/CASSANDRA-3644-1.0-v2.txt,18/Dec/11 01:33;scode;CASSANDRA-3644-1.0.txt;https://issues.apache.org/jira/secure/attachment/12507823/CASSANDRA-3644-1.0.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-18 06:34:39.967,,,no_permission,,,,,,,,,,,,221289,,,Thu Dec 22 20:40:13 UTC 2011,,,,,,0|i0gmgv:,95070,jbellis,jbellis,,,,,,,,,18/Dec/11 06:34;jbellis;Patch does not apply to 1.0 branch (I assume from the name that's what it's from?),"20/Dec/11 06:10;scode;My apologies. I must have accidentally taken the wrong branch. v2 attached, against 1.0.",22/Dec/11 20:40;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
snapshot-before-compaction snapshots entire keyspace,CASSANDRA-3803,12540203,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,27/Jan/12 22:04,12/Mar/19 14:04,13/Mar/19 22:26,09/Feb/12 03:56,1.0.8,1.1.0,,,,,0,compaction,,,,Should only snapshot the CF being compacted,,,,,,,,,,,,,,,,05/Feb/12 20:24;jbellis;3803.txt;https://issues.apache.org/jira/secure/attachment/12513362/3803.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-04 17:27:49.769,,,no_permission,,,,,,,,,,,,225618,,,Thu Feb 09 03:56:46 UTC 2012,,,,,,0|i0gofb:,95387,slebresne,slebresne,,,,,,,,,"27/Jan/12 22:18;jbellis;I can't think of a good reason to snapshot the entire keyspace, and doing so can dramatically increase the space needed to enable snapshot_before_compaction.",04/Feb/12 17:27;slebresne;I think the attached patch is the wrong one.,05/Feb/12 20:24;jbellis;2nd try. Also switches to snapshot-without-flush.,"05/Feb/12 21:50;slebresne;+1, but I would actually suggest pushing this in 1.0.8. The fact we were flushing all CFs on each compaction is pretty bad (not sure anyone actually uses snapshot-before-compaction but still).","05/Feb/12 21:51;slebresne;As a side not, a nice alternative would be to snapshot only the files we're going to compact. But again, we probably don't care about that feature that much.",09/Feb/12 03:56;jbellis;committed to 1.0.8 + 1.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disabling m-a-t for fun and profit (and other ant stuff),CASSANDRA-3818,12540484,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dbrosius,urandom,urandom,31/Jan/12 02:32,12/Mar/19 14:04,13/Mar/19 22:26,01/Jan/13 19:00,2.0 beta 1,,,Packaging,,,0,build,,,,"It should be possible to disable maven-ant-tasks for environments with more rigid dependency control, or where network access isn't available.

Patches to follow.",,,,,,,,,,,,,,,,29/Dec/12 18:31;jbellis;3818-v2.txt;https://issues.apache.org/jira/secure/attachment/12562686/3818-v2.txt,31/Dec/12 06:27;dbrosius@apache.org;3818-v3.txt;https://issues.apache.org/jira/secure/attachment/12562761/3818-v3.txt,21/Nov/12 20:48;dbrosius;3818.txt;https://issues.apache.org/jira/secure/attachment/12554565/3818.txt,31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3818-keep-init-in-init-target.txt;https://issues.apache.org/jira/secure/attachment/12512516/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3818-keep-init-in-init-target.txt,31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0002-clean-up-avro-generation-dependencies-and-dependants.txt;https://issues.apache.org/jira/secure/attachment/12512517/ASF.LICENSE.NOT.GRANTED--v1-0002-clean-up-avro-generation-dependencies-and-dependants.txt,31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0003-remove-useless-build-subprojects-target.txt;https://issues.apache.org/jira/secure/attachment/12512518/ASF.LICENSE.NOT.GRANTED--v1-0003-remove-useless-build-subprojects-target.txt,31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0004-group-test-targets-under-test-all.txt;https://issues.apache.org/jira/secure/attachment/12512519/ASF.LICENSE.NOT.GRANTED--v1-0004-group-test-targets-under-test-all.txt,31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0005-add-property-to-disable-maven-junk.txt;https://issues.apache.org/jira/secure/attachment/12512520/ASF.LICENSE.NOT.GRANTED--v1-0005-add-property-to-disable-maven-junk.txt,31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0006-add-property-to-disable-rat-license-header-writing.txt;https://issues.apache.org/jira/secure/attachment/12512521/ASF.LICENSE.NOT.GRANTED--v1-0006-add-property-to-disable-rat-license-header-writing.txt,31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0007-don-t-needlessly-regenerate-thrift-code.txt;https://issues.apache.org/jira/secure/attachment/12512522/ASF.LICENSE.NOT.GRANTED--v1-0007-don-t-needlessly-regenerate-thrift-code.txt,,10.0,,,,,,,,,,,,,,,,,,,2012-04-04 22:05:39.068,,,no_permission,,,,,,,,,,,,225897,,,Tue Jan 01 19:00:13 UTC 2013,,,,,,0|i07hsn:,41650,jbellis,jbellis,,,,,,,,,"31/Jan/12 02:43;urandom;Changes attached as patches, and pushed to Github as: https://github.com/eevans/cassandra/tree/3818

0001 / 8ad3ad
0002 / 9e26a1
0003 / 294bd1

The first 3 changesets are basically cleanups.

0004 / fc8d65

Changeset #4 creates a new target called _test-all_, which runs test, test-compression, long-test, and test-clientutil.jar per the discussion on dev@

0005 / d110ae

Adds a property (-Dwithout.maven) that disables maven-base dependency resolution, (at least for the build and test targets).

0006 / e093d8

Adds a property (-Dwithout.rat) that disables the automatic prepending of license headers to the Thrift generated classes.

0007 / 1037a9

Adds a check that makes gen-thrift-java a noop if {{interface/cassandra.thrift}} has not been updated (useful in environments where the Thrift code is regenerated as part of the build).",04/Apr/12 22:05;stephenc;I will review in the next couple of days as soon as I get the bandwidth,"15/Nov/12 00:21;jbellis;Are you still planning to review, [~stephenc]?","21/Nov/12 11:07;jbellis;Dave, what can you make of this?","21/Nov/12 20:48;dbrosius;rebased the patch for trunk (1.3) as one file->3818.txt

I inserted bogus proxy server, cassandra still builds fine (and faster).

perhaps new targets should get description attributes so they show up in ant -p, but otherwise lgtm.","29/Dec/12 18:31;jbellis;Added descriptions to test-all and write-java-license-headers as v2.

But I'm getting the following error:

{noformat}
BUILD FAILED
/Users/johnathanellis/projects/cassandra/git/build.xml:1113: The following error occurred while executing this line:
/Users/johnathanellis/projects/cassandra/git/build.xml:1055: Reference cobertura.classpath not found.
{noformat}",30/Dec/12 20:30;dbrosius@apache.org;what target are you running?,31/Dec/12 00:23;jbellis;ant clean test,31/Dec/12 06:27;dbrosius@apache.org;fix cobertura.classpath issue in v3,01/Jan/13 14:43;jbellis;WFM.  Ship it!,01/Jan/13 19:00;dbrosius;pushed to trunk as commit 565f675b16747e7bfa827cb220cfce28f33c5b81,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy static initialization not triggered until thrift requests come in,CASSANDRA-3797,12540003,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,27/Jan/12 06:37,12/Mar/19 14:04,13/Mar/19 22:26,29/Feb/12 04:36,1.1.0,,,,,,0,,,,,"While plugging in the metrics library for CASSANDRA-3671 I realized (because the metrics library was trying to add a shutdown hook on metric creation) that starting cassandra and simply shutting it down, causes StorageProxy to not be initialized until the drain shutdown hook.

Effects:

* StorageProxy mbean missing in visualvm/jconsole after initial startup (seriously, I thought I was going nuts ;))
* And in general anything that makes assumptions about running early, or at least not during JVM shutdown, such as the metrics library, will be problematic
",,,,,,,,,,,,,,,,27/Feb/12 21:42;jbellis;3797-forname-this-time-for-sure.txt;https://issues.apache.org/jira/secure/attachment/12516218/3797-forname-this-time-for-sure.txt,27/Jan/12 17:22;jbellis;3797-forname.txt;https://issues.apache.org/jira/secure/attachment/12512192/3797-forname.txt,27/Jan/12 07:32;scode;CASSANDRA-3797-trunk-v1.txt;https://issues.apache.org/jira/secure/attachment/12512088/CASSANDRA-3797-trunk-v1.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-01-27 17:22:40.25,,,no_permission,,,,,,,,,,,,225506,,,Wed Feb 29 04:36:00 UTC 2012,,,,,,0|i0gocn:,95375,jbellis,jbellis,,,,,,,,,"27/Jan/12 07:32;scode;Attaching patch that has StorageService call a NOOP method on StorageProxy during start-up if not in client mode.

This feels unclean to me, but barring a ""bigger"" change to avoid the subtle static initialization order problem properly it was the easiest/cleanest fix I could think of.","27/Jan/12 08:04;scode;Maybe a Class.forName() is cleaner (a Class klass = StorageProxy.class does not (necessarily) cause it to be loaded).
",27/Jan/12 17:22;jbellis;Hmm.  Neither this nor the forName approach (attached) is sufficient to o.a.c.service.StorageProxy show up in jconsole for me.,"27/Jan/12 18:54;scode;Really? That's strange. I just re-tried again to double-check (ant clean and everything) and the patch seems to make a difference here.

In the case of Class.forName() I cannot believe that is not supposed to be guaranteed. Are you *sure* it's not working? I would have to assume static initialization must by definition happen for Class.forName() to fullfil its contract (even if I could possibly buy slightly more easily that the staticallyInitialize() NOOP didn't, but even that seems like a stretch).

I am just building and {{./bin/cassandra -f}} and immediately attaching with visualvm.","27/Feb/12 20:09;scode;Looks like {{3797-forname.txt}} is the same file as the original patch. In any case, suppose we just go for Class.forName() to avoid introducing that annoying method, and assuming it makes the metrics from CASSANDRA-3671 work, can I get a +1?",27/Feb/12 21:30;brandon.williams;I tested this with CASSANDRA-3671 and everything worked.,"27/Feb/12 21:42;jbellis;correct forname patch attached.

figured out my problem from last time: I was looking for StorageProxy under the old location in o.a.c.service, instead of o.a.c.db.  It appears correctly on startup in o.a.c.db with this patch.",29/Feb/12 04:36;scode;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hadoop word count example is unable to output to cassandra with default settings,CASSANDRA-3765,12539274,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,21/Jan/12 06:05,12/Mar/19 14:04,13/Mar/19 22:26,23/Jan/12 22:05,1.1.0,,,,,,0,hadoop,,,,"{noformat}
12/01/21 06:03:16 WARN mapred.LocalJobRunner: job_local_0001
java.lang.NullPointerException
        at org.apache.cassandra.utils.FBUtilities.newPartitioner(FBUtilities.java:407)
        at org.apache.cassandra.hadoop.ConfigHelper.getOutputPartitioner(ConfigHelper.java:384)
        at org.apache.cassandra.client.RingCache.<init>(RingCache.java:58)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter.<init>(ColumnFamilyRecordWriter.java:99)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter.<init>(ColumnFamilyRecordWriter.java:93)
        at org.apache.cassandra.hadoop.ColumnFamilyOutputFormat.getRecordWriter(ColumnFamilyOutputFormat.java:132)
        at org.apache.cassandra.hadoop.ColumnFamilyOutputFormat.getRecordWriter(ColumnFamilyOutputFormat.java:62)
        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:553)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
{noformat}

(Output to filesystem still works.)",,,,,,,,,,,,,,,,21/Jan/12 22:11;jbellis;3765.txt;https://issues.apache.org/jira/secure/attachment/12511410/3765.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-23 21:24:02.402,,,no_permission,,,,,,,,,,,,224776,,,Mon Jan 23 22:05:00 UTC 2012,,,,,,0|i0gnyf:,95311,brandon.williams,brandon.williams,,,,,,,,,"21/Jan/12 22:11;jbellis;Patch to fix defaults post-CASSANDRA-3197.

Still seeing (different) errors in record writer in the word count example; will address after CASSANDRA-2878 since that touches the same code.",23/Jan/12 21:24;brandon.williams;+1 on e66fc06 at https://github.com/jbellis/cassandra/tree/2878-rebased,23/Jan/12 22:05;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Once a host has been hinted to, log messages for it repeat every 10 mins even if no hints are delivered",CASSANDRA-3733,12538223,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,12/Jan/12 16:39,12/Mar/19 14:04,13/Mar/19 22:26,12/Jan/12 17:03,0.8.10,1.0.7,,,,,0,,,,,"{noformat}
 INFO 15:36:03,977 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 15:36:03,978 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
 INFO 15:46:31,248 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 15:46:31,249 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
 INFO 15:56:29,448 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 15:56:29,449 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
 INFO 16:06:09,949 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 16:06:09,950 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
 INFO 16:16:21,529 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 16:16:21,530 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
{noformat}

Introduced by CASSANDRA-3554.  The problem is that until a compaction on hints occurs, tombstones are present causing the isEmpty() check to be false.",,,,,,,,,,,,,,,,12/Jan/12 16:57;brandon.williams;3733.txt;https://issues.apache.org/jira/secure/attachment/12510388/3733.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-12 16:43:44.075,,,no_permission,,,,,,,,,,,,223729,,,Thu Jan 12 17:17:17 UTC 2012,,,,,,0|i0gnkn:,95249,jbellis,jbellis,,,,,,,,,"12/Jan/12 16:43;jbellis;Why doesn't this take care of it?

{code}
.       if (rowsReplayed > 0)
        {
            hintStore.forceFlush();
            try
            {
                CompactionManager.instance.submitMaximal(hintStore, Integer.MAX_VALUE).get();
            }
            catch (Exception e)
            {
                throw new RuntimeException(e);
            }
        }
{code}","12/Jan/12 16:51;brandon.williams;Because the flush isn't blocking:

{noformat}
 INFO 16:50:36,248 Started hinted handoff for token: 113427455640312821154458202477256070484 with IP: /10.179.64.227
 INFO 16:50:36,265 Enqueuing flush of Memtable-HintsColumnFamily@1721309039(0/0 serialized/live bytes, 1 ops)
 INFO 16:50:36,266 Writing Memtable-HintsColumnFamily@1721309039(0/0 serialized/live bytes, 1 ops)
 INFO 16:50:36,267 Nothing to compact in HintsColumnFamily.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO 16:50:36,268 Finished hinted handoff of 1 rows to endpoint /10.179.64.227
 INFO 16:50:36,272 Completed flushing /var/lib/cassandra/data/system/HintsColumnFamily-hc-2-Data.db (100 bytes)
{noformat}",12/Jan/12 16:57;brandon.williams;Patch to make flushes blocking.,12/Jan/12 17:00;jbellis;+1,"12/Jan/12 17:01;jbellis;the tombstone buffering on the next attempt could cause OOMs similar to CASSANDRA-3681, as well.",12/Jan/12 17:03;brandon.williams;Committed.,12/Jan/12 17:05;jbellis;Can you commit to 0.8 too?,12/Jan/12 17:17;brandon.williams;Done in aacbb1ca9c0e7a1992dfc92c096dd885ab149154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra 1.0.x breakes APT on debian OpenVZ,CASSANDRA-3636,12535263,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,zenek_kraweznik0,zenek_kraweznik0,15/Dec/11 07:51,12/Mar/19 14:04,13/Mar/19 22:26,23/Jul/12 17:07,1.0.11,1.1.3,,Packaging,,,0,,,,,"During upgrade from 1.0.6
{code}Setting up cassandra (1.0.6) ...
*error: permission denied on key 'vm.max_map_count'*
dpkg: error processing cassandra (--configure):
 subprocess installed post-installation script returned error exit status 255
Errors were encountered while processing:
 cassandra
{code}","Debian Linux (stable), OpenVZ container",,,,,,,,,,,,,,,18/May/12 02:35;thepaul;3636.patch.txt;https://issues.apache.org/jira/secure/attachment/12527987/3636.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-09 21:53:42.297,,,no_permission,,,,,,,,,,,,220947,,,Fri Aug 03 20:48:13 UTC 2012,,,,,,0|i0gmd3:,95053,brandon.williams,brandon.williams,,,,,,,,,"09/Jan/12 21:53;brandon.williams;What version of openvz is this?  As far as I can tell, there shouldn't be a problem changing this in a guest.","13/Jan/12 13:22;zenek_kraweznik0;This must be increasec on host, not on guest. This is kernel shared memory modifications, guests haven't their own kernel.

This problem apperears on debian squeeze openvz kernel and on newest openvz patches","19/Jan/12 09:11;saulius.grigaitis;I have exactly same version with cassandra 1.0.6 and 1.0.7 version (1.0.5 version works fine)

Installing from debian packanges on ubuntu 10.04 64bits:

Installing new version of config file /etc/cassandra/cassandra.yaml ...
error: permission denied on key 'vm.max_map_count'
dpkg: error processing cassandra (--configure):
 subprocess installed post-installation script returned error exit status 255

uname -a
Linux server 2.6.32-042stab044.11 #1 SMP Wed Dec 14 16:02:00 MSK 2011 x86_64 GNU/Linux","19/Jan/12 15:46;thepaul;Saulius- are you also running openvz? What version, if so?","20/Jan/12 09:13;saulius.grigaitis;Paul > yes, I'm running openvz. Openvz kernel version is 2.6.32-042stab044.11 .","21/Jan/12 00:24;zenek_kraweznik0;Cassandra is running on openvz, but post-install procedure is not completed, but changing this sysctl in openvz container is impossible.

This is not a bug with OpenVZ stricte, so I set components to packaging.

There is line in cassandra.postinst script:
sysctl -p /etc/sysctl.d/cassandra.conf

apt tries to run this script before install/remove any packages and stops on this step.

This script is breaking apt.

changing ""/etc/sysctl.d/cassandra.conf"" to ""sysctl -p /etc/sysctl.d/cassandra.conf || rm /etc/sysctl.d/cassandra.conf"" could solve this.","12/Apr/12 10:02;tiotempestade;Solved it! Just unpack the .deb and comment the sysctl -p /etc/sysctl.d/cassandra.conf line in postinst.

It is already set to vm.max_map_count = 1048575 as the script tries to do..


After that rebuild the .deb and proceed with Datastax's installation.","13/Apr/12 07:49;patrik.modesto;Hi, it's still a problem for cassandra 1.0.9 and openvz 3.0.23 on Debian Squeeze.",13/Apr/12 07:52;patrik.modesto;Added latest stable version of Cassandra.,14/May/12 17:04;jbellis;So...  should we just log a warning if the sysctl fails instead of failing entirely?,14/May/12 21:33;thepaul;Not much else we _can_ do.,"18/May/12 02:35;thepaul;This change should allow the deb install to succeed. Patrik, Tio, Zenek, Saulius, or someone else with this problem, could you try applying the patch and installing the deb?

Change also available on the 3636 branch on my github:

https://github.com/thepaul/cassandra/tree/3636",21/May/12 16:29;brandon.williams;Committed.,"23/Jul/12 09:00;joschi;The fix seems to have been lost during a merge.

The [trunk version of debian/cassandra.postinst|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blob;f=debian/cassandra.postinst] doesn't contain the changes from Paul's fix in commit [48438ffc61bce2396fa2eac2f88fa66a6e6a69cd|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commitdiff;h=48438ffc61bce2396fa2eac2f88fa66a6e6a69cd].","23/Jul/12 17:04;hudson;Integrated in Cassandra #1768 (See [https://builds.apache.org/job/Cassandra/1768/])
    merge CASSANDRA-3636 from 1.0 to 1.1 cassandra 1.0.x breakes APT on debian OpenVZ (Revision ff64c5d11e9e66dfa42a1ca72572a65fb7480f37)

     Result = ABORTED
dbrosius : 
Files : 
* debian/cassandra.postinst
","23/Jul/12 17:07;brandon.williams;bq. The trunk version of debian/cassandra.postinst doesn't contain the changes from Paul's fix in commit 48438ffc61bce2396fa2eac2f88fa66a6e6a69cd.

Yes it does.",03/Aug/12 14:15;mohitz;I am still facing this issue with cassandra 1.1.2. I downloaded the debian file and unpacked it to look at the postinst script. I don't see Paul's fix in there.,"03/Aug/12 17:20;brandon.williams;You're looking at the wrong branch, in that case, because it's not in 1.1.2.","03/Aug/12 20:48;mohitz;The fix version here said 1.1.1 incorrectly, so i checked 1.1.2. Thanks for correcting it.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: handle situation where data can't be deserialized as expected,CASSANDRA-3874,12541721,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,08/Feb/12 00:36,12/Mar/19 14:04,13/Mar/19 22:26,16/Feb/12 15:43,1.0.8,,,Legacy/Tools,,,0,cqlsh,,,,"When cqlsh tries to deserialize data which doesn't match the expected type (either because the validation type for the column/key alias was changed, or ASSUME has been used), it just fails completely and in most cases won't show any results at all. When there is only one misbehaving value out of a large number, this can be frustrating.

cqlsh should either show some failure marker in place of the bad value, or simply show the bytes along with some indicator of a failed deserialization.",,,,,,,,,,,,,,,,15/Feb/12 20:59;thepaul;3874-1.0.patch.txt;https://issues.apache.org/jira/secure/attachment/12514706/3874-1.0.patch.txt,15/Feb/12 20:59;thepaul;3874-1.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12514707/3874-1.1.patch.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-15 21:02:15.678,,,no_permission,,,,,,,,,,,,227008,,,Thu Feb 16 15:43:06 UTC 2012,,,,,,0|i0gp9z:,95525,brandon.williams,brandon.williams,,,,,,,,,"15/Feb/12 20:58;thepaul;Changes made in my 3874-1.0 branch at github: https://github.com/thepaul/cassandra/tree/3874-1.0

Since the merge forward is not completely trivial, my 3874-1.1 branch has those commits already merged to 1.1: https://github.com/thepaul/cassandra/tree/3874-1.1 , and it will be an easy merge from that to your updated cassandra-1.1, whatever it is.

I'll attach patch versions too, in case you still prefer those.",15/Feb/12 21:02;jbellis;Is this a big enough change that we should keep it to 1.1 only?,"15/Feb/12 21:09;thepaul;I think it's worth having in 1.0, especially if it's going to live for a few more releases. It's technically only a bugfix. But I don't mind much, whichever.","15/Feb/12 23:56;thepaul;updated the 3874-1.1 branch just now to add the 1.0.9 version of the python-cql library, since that is required for this fix to work. 1.0 doesn't have an embedded python-cql lib, so it's fine.",16/Feb/12 15:43;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FB.broadcastAddress fixes and Soft reset on Ec2MultiRegionSnitch.reconnect,CASSANDRA-3835,12540857,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,02/Feb/12 00:41,12/Mar/19 14:04,13/Mar/19 22:26,02/Feb/12 03:59,1.0.8,1.1.0,1.2.0 beta 1,,,,0,,,,,"looks like OutboundTcpConnectionPool.reset will clear the queue which might not be ideal for Ec2Multiregion snitch.
there is additional cleanup needed for FB.broadCastAddress.",,,,,,,,,,,,,,,,02/Feb/12 00:42;vijay2win@yahoo.com;0001-fix-fb-broadcastAddress.patch;https://issues.apache.org/jira/secure/attachment/12512881/0001-fix-fb-broadcastAddress.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-02 01:24:40.08,,,no_permission,,,,,,,,,,,,226211,,,Thu Feb 02 04:08:49 UTC 2012,,,,,,0|i0gosf:,95446,brandon.williams,brandon.williams,,,,,,,,,"02/Feb/12 01:24;brandon.williams;+1, looks like most of the BCA cleanup came from merges from 0.8 fixes.","02/Feb/12 03:59;vijay2win@yahoo.com;Fixed on 1.0.8, 1.1 and trunk, Thanks!","02/Feb/12 04:08;brandon.williams;Looks like this broke something in the cli, dtests are failing the chunk length test.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cqlsh can't show data under python2.5, python2.6",CASSANDRA-3846,12541134,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,03/Feb/12 20:52,12/Mar/19 14:04,13/Mar/19 22:26,06/Feb/12 19:42,1.0.8,,,Legacy/Tools,,,0,cqlsh,,,,"Kris Hahn discovered a python2.6-ism in recent cqlsh changes:

{code}
        bval = escapedval.encode(output_encoding, errors='backslashreplace')
{code}

before python2.7, str.encode() didn't accept a keyword argument for the second parameter. the semantics are the same without naming the parameter, though, so removing the ""errors="" bit should suffice to make it run right.

does not affect any released version, but does affect HEAD of cassandra-1.0, cassandra-1.1, and trunk.",,,,,,,,,,,,,,,,03/Feb/12 20:56;thepaul;3846.patch.txt;https://issues.apache.org/jira/secure/attachment/12513171/3846.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-06 19:42:17.359,,,no_permission,,,,,,,,,,,,226486,,,Mon Feb 06 19:42:17 UTC 2012,,,,,,0|i0goxb:,95468,brandon.williams,brandon.williams,,,,,,,,,"03/Feb/12 20:56;thepaul;..or https://github.com/thepaul/cassandra/commit/24cfab5ddb4dd1c619c9a14878dd31fd18a3cbe9 , found on my 3846 branch","03/Feb/12 21:13;thepaul;edit: this is also a problem on python2.6, not just python2.5. resolution is still the same.",06/Feb/12 19:42;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
long-test timing out,CASSANDRA-3841,12540963,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,mallen,mallen,02/Feb/12 18:05,12/Mar/19 14:04,13/Mar/19 22:26,18/Sep/12 14:58,1.1.6,,,Legacy/Testing,,,0,,,,,"    [junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.compaction.LongCompactionSpeedTest:BeforeFirstTest:	Caused an ERROR
    [junit] Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LongCompactionSpeedTest FAILED (timeout)
    [junit] Testsuite: org.apache.cassandra.utils.LongBloomFilterTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 64.536 sec
    [junit] 
    [junit] Testsuite: org.apache.cassandra.utils.LongLegacyBloomFilterTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 41.104 sec
    [junit] 

BUILD FAILED
/Users/mallen/dstax/repos/git/cassandra/build.xml:1113: The following error occurred while executing this line:
/Users/mallen/dstax/repos/git/cassandra/build.xml:1036: Some long test(s) failed.

Total time: 63 minutes 9 seconds
",,,,,,,,,,,,,,,,18/Sep/12 05:16;jasobrown;0001-CASSANDRA-3841-long-test-timing-out.patch;https://issues.apache.org/jira/secure/attachment/12545524/0001-CASSANDRA-3841-long-test-timing-out.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-03 00:12:22.918,,,no_permission,,,,,,,,,,,,226316,,,Tue Sep 18 14:58:25 UTC 2012,,,,,,0|i0gov3:,95458,jbellis,jbellis,,,,,,,,,"03/Feb/12 00:12;jbellis;I see LCST fail as well when run as part of the full long-test suite.  When I run just LCST I get this:

{noformat}
$ ant clean long-test -Dtest.name=LongCompactionSpeedTest
...
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 20.981 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 992 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: 4383 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 807 ms
    [junit] ------------- ---------------- ---------------

BUILD FAILED
/home/notroot/cassandra/build.xml:1075: Problem: failed to create task or type jvmarg
Cause: The name is undefined.
Action: Check the spelling.
Action: Check that any custom tasks/types have been declared.
Action: Check that any <presetdef>/<macrodef> declarations have taken place.
{noformat}

I think this means that
* LCST passes in isolation, but something isn't getting cleaned up from the prior test (MeteredFlusherTest) that causes it to timeout in the full suite
* There is some ant problem with -Dtest.name in conjunction with long-test","18/Sep/12 05:16;jasobrown;I was able to reproduce the originally reported error - it was just the junit test timing out. The previous time limit was 300000 ms (5 minutes), and I increased it to 600000 ms (10 minutes).

However, I was not able to reproduce Jonathan's reported problems, even after executing a run of all the individual tests:

{code}for i in LongTableTest MeteredFlusherTest LongCompactionSpeedTest LongBloomFilterTest LongLegacyBloomFilterTest; \
do ant clean long-test -Dtest.name=$i; done{code}",18/Sep/12 05:19;jasobrown;Patch is trivial - just a property value modification. Should be able to apply to both 1.1 and 1.2 branches.,"18/Sep/12 14:58;jbellis;I'm not seeing the ""failed to create task"" problem anymore, either.  Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix LazilyCompactedRowTest,CASSANDRA-3915,12542697,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,15/Feb/12 15:19,12/Mar/19 14:04,13/Mar/19 22:26,15/Feb/12 15:43,1.0.8,1.1.0,,Legacy/Testing,,,0,,,,,"LazilyCompactedRowTest.testTwoRowSuperColumn has never really worked. It uses LazilyCompactedRowTest.assertBytes() that assumes standard columns, even though that test is for super columns. For some reason, the deserialization of the super columns as columns was not breaking stuff and so the test was ""working"", but CASSANDRA-3872 changed that and LazilyCompactedRowTest.testTwoRowSuperColumn fails on current cassandra-1.1 branch (but it's not CASSANDRA-3872 fault, just the test that is buggy).",,,,,,,,,,,,,,,,15/Feb/12 15:21;slebresne;3915.patch;https://issues.apache.org/jira/secure/attachment/12514649/3915.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-15 15:33:45.26,,,no_permission,,,,,,,,,,,,227983,,,Wed Feb 15 15:43:00 UTC 2012,,,,,,0|i0gprb:,95603,brandon.williams,brandon.williams,,,,,,,,,15/Feb/12 15:21;slebresne;Attached patch to fix the tests. I believe the patch apply cleanly to both 1.0 (for which the test don't fail but is still broken) and 1.1 (where it does fix the test failure).,15/Feb/12 15:33;brandon.williams;+1,"15/Feb/12 15:43;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction doesn't clear out expired tombstones from SerializingCache,CASSANDRA-3921,12542796,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,15/Feb/12 23:56,12/Mar/19 14:04,13/Mar/19 22:26,16/Feb/12 15:28,1.1.0,,,,,,0,,,,,"Compaction calls removeDeletedInCache, which looks like this:

{code}
.   public void removeDeletedInCache(DecoratedKey key)
    {
        ColumnFamily cachedRow = cfs.getRawCachedRow(key);
        if (cachedRow != null)
            ColumnFamilyStore.removeDeleted(cachedRow, gcBefore);
    }
{code}

For the SerializingCache, this means it calls removeDeleted on a temporary, deserialized copy, which leaves the cache contents unaffected.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-02-16 01:30:40.205,,,no_permission,,,,,,,,,,,,228082,,,Thu Feb 16 15:28:14 UTC 2012,,,,,,0|i0gptz:,95615,slebresne,slebresne,,,,,,,,,"15/Feb/12 23:57;jbellis;I'm not sure what the right fix is.  We could invalidate the row, but that means a big compaction could wipe out the row cache entirely, which is worse.  Should we just wontfix this?","16/Feb/12 01:30;scode;With a typical workload, not evicting it would just fail to ""optimize"" - you have to wait for the data to get dropped out of cache.

But if you do have a use-case where you have a lot of requests to non-existent data, this could ""silently"" render the row caches almost useless in a way that is very hard to detect. Suppose you have rows with a hot set where you want caching, but the data is made up of ""churning"" columns (insertions/deletions; the set of columns constantly  But, the ""damaged"" is limited to the ratio of requests for empty rows vs. other requests, and is less prominent the bigger the average requested row in relation to the size of a tombstone.

The above should be true if the rows in cache only contain the tombstones.

But consider a use-case where you have rows (hot) that gets constantly updated with insertion/deletion (let's say it has 1-5 columns at any point in time, with columns constantly ""churned""). How you have something which is hot in cache due to reads (for the ones in hot set), data being over-written, and the per-row average size increasing over time. Cache locality becomes worse and worse and no one can see why, and a restart magically fixes it. This should hold true as long as row cache entries aren't entirely re-read on writes, which would in part defeat the purpose of caching (but I realize now I'm not sure what we do here).
","16/Feb/12 07:59;slebresne;Yeah, it's the serializing cache, we invalidate on each put, so the chance of tons of tombstone building up overtime is anecdotal imo. We could probably fix this by cloning the cachedRow after the get, apply removeDeleted on the clone and do a conditional replace (if it's still the value we got). This involve a bunch of serialization/deserialization so it's unclear that doing that is a better optimization than leaving the tombstone. So I'm good leaving it the way it is. Except that we may want to make removeDeletedInCache be a noop for copying caches just to avoid the useless deserialization. ","16/Feb/12 08:10;scode;If we invalidate on every put, I'm +1 on just ignoring the problem. Sure, it's possible to have a constant subset of a hotset repeatedly read, and someone making an attempt to make it take less space in cache by deleting data and waiting for tombstones... but that's so obscure/extreme that we can probably fix 500 other JIRA:s before this is a priority :) Definitely +1 on NOOP:ing the method though; or more importantly, documenting why it's a NOOP.

(Btw the incoherence of my previous comment is what happens when you split the posting of a comment in two pieces with a meeting in between...)","16/Feb/12 15:28;jbellis;bq. we may want to make removeDeletedInCache be a noop for copying caches just to avoid the useless deserialization

done in 94860c6c3713cda4f17dabb2ac2ce30cfe92f6e2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
all column validator options are not represented in cli help,CASSANDRA-3926,12542950,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,kirktrue,jeromatron,jeromatron,16/Feb/12 22:10,12/Mar/19 14:04,13/Mar/19 22:26,13/Mar/12 14:59,1.1.0,,,Legacy/Documentation and Website,,,0,cli,lhf,,,"The options added to column validators from CASSANDRA-2530 are not shown as options in the CLI help.  I was going to create a column family with a float validator and double checked the help and it wasn't shown.  So I just had to double check that I could.  Would be nice to have those added to those docs, even though CQL is the way forward.",,,,,,,,,,,,,,,,07/Mar/12 00:40;kirktrue;trunk-2530.txt;https://issues.apache.org/jira/secure/attachment/12517343/trunk-2530.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-07 00:40:48.554,,,no_permission,,,,,,,,,,,,228236,,,Tue Mar 13 14:59:17 UTC 2012,,,,,,0|i0gpw7:,95625,xedin,xedin,,,,,,,,,"16/Feb/12 22:11;jeromatron;fwiw, the docs are found in src/resources/org/apache/cassandra/cli/CliHelp.yaml",07/Mar/12 00:40;kirktrue;Patch to add appropriate lines to CliHelp.yaml.,"07/Mar/12 00:50;kirktrue;I noticed some other places in that file where types are mentioned, outside the context of column creation/modification. 

For example, in the {{NODE_THRIFT_GET}} section, the {{function}} can be specified to parse arguments. There are no entries for dates, et al. there. Should there be?","13/Mar/12 14:59;xedin;Functions are different thing so we don't need to update doc for them. Committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HintedHandoff won't compact a single sstable, resulting in repeated log messages",CASSANDRA-3955,12544002,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,24/Feb/12 15:33,12/Mar/19 14:04,13/Mar/19 22:26,24/Feb/12 16:47,1.0.9,1.1.0,,,,,0,,,,,"First introduced by CASSANDRA-3554, and then mostly solved in CASSANDRA-3733, there is still one special case where the HH log message will repeat every 10 mins for 0 rows: when there have previously been hints delivered to the node, but now only a single sstable exists.  Because the we refused to compact a single sstable, and it contains tombstones for the hints, the message repeats.",,,,,,,,,,,,,,,,24/Feb/12 16:26;jbellis;3955-v2.txt;https://issues.apache.org/jira/secure/attachment/12515937/3955-v2.txt,24/Feb/12 16:17;brandon.williams;3955.txt;https://issues.apache.org/jira/secure/attachment/12515936/3955.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-02-24 15:40:41.493,,,no_permission,,,,,,,,,,,,229240,,,Fri Feb 24 16:47:07 UTC 2012,,,,,,0|i0gq8v:,95682,jbellis,jbellis,,,,,,,,,"24/Feb/12 15:40;jbellis;CASSANDRA-3442 seems like the ""right"" way to fix this.  Is it an easy special case for 1.0.9?","24/Feb/12 16:17;brandon.williams;No need to special case, instead we'll just always force a user defined compaction.",24/Feb/12 16:26;jbellis;v2 attached to cover both places HHOM compacts,24/Feb/12 16:29;brandon.williams;+1,24/Feb/12 16:47;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test fixes for Windows,CASSANDRA-3967,12544364,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,28/Feb/12 04:42,12/Mar/19 14:04,13/Mar/19 22:26,16/Mar/12 16:59,1.1.0,,,Legacy/Testing,,,0,,,,,"- SchemaLoader initializes CommitLog, creating empty segments; CleanupHelper then errors out trying to delete them (because they are still open)
- CFS.clearUnsafe resets the sstable generation but does not remove the sstables involved, so Windows will error out trying to rename over the existing ones",,,,,,,,,,,,,,,,28/Feb/12 04:42;jbellis;3967.txt;https://issues.apache.org/jira/secure/attachment/12516274/3967.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-28 09:20:46.623,,,no_permission,,,,,,,,,,,,229601,,,Fri Mar 16 16:59:43 UTC 2012,,,,,,0|i0gqfj:,95712,slebresne,slebresne,,,,,,,,,"28/Feb/12 09:20;slebresne;The problem is that most tests rely on the behavior of CFS.clearUnsafe to not remove sstables. Typically, in SSTableReaderTest, there is 'persistence' tests that use clearUnsafe followed by a loadNewSSTable to check we correctly reload sstables. CASSANDRA-3970 is another example where we rely on the sstable not being removed. Also ColumnFamilyStoreTest and RecoveryManagerTest.

The patch also seem to be breaking a test in DatabaseDescriptorTest (testTransKsMigration that doesn't fail without the patch) but that test doesn't use clearUnsafe so I'm not sure what's going on.",08/Mar/12 22:41;jbellis;Posted a second attempt to https://github.com/jbellis/cassandra/branches/3967-2. This time I take the approach of changing load of new sstables to renumber when it loads to avoid conflicts.,"16/Mar/12 11:01;slebresne;There is a small bug in that after renaming we try to load the old (before renaming) descriptor. Added commit to https://github.com/pcmanus/cassandra/commits/3967-2 to fix that. With that, and while I don't have a windows and can't vouch that it fixes tests there, since that is anyway a more reasonable way to load new sstables, +1.","16/Mar/12 16:59;jbellis;committed fixed version, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
./bin/cqlsh `describe keyspace <keyspace>` command doesn't work when ColumnFamily row_cache_provider wasn't specified.,CASSANDRA-3933,12543356,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,xedin,xedin,20/Feb/12 15:08,12/Mar/19 14:04,13/Mar/19 22:26,22/Feb/12 10:34,1.0.8,,,,,,0,cqlsh,,,,"I have created Keyspace and ColumnFamily using CLI

{noformat}
/bin/cassandra-cli --host localhost
Connected to: ""Test Cluster"" on localhost/9160
Welcome to Cassandra CLI version 1.0.7-SNAPSHOT

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] create keyspace ks;
f89384f0-5bd3-11e1-0000-242d50cf1fff
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] use ks;
Authenticated to keyspace: ks
[default@ks] create column family cf;
fc807690-5bd3-11e1-0000-242d50cf1fff
Waiting for schema agreement...
... schemas agree across the cluster
[default@ks] quit;
{noformat}

and then I have tried to describe keyspace using CQLsh

{noformat}
./bin/cqlsh                     
Connected to Test Cluster at localhost:9160.
[cqlsh 2.0.0 | Cassandra 1.0.7-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.20.0]
Use HELP for help.
cqlsh> describe keyspace ks;

CREATE KEYSPACE ks WITH strategy_class = 'NetworkTopologyStrategy'
  AND strategy_options:datacenter1 = '1';

USE ks;

CREATE COLUMNFAMILY cf (
CfDef instance has no attribute 'row_cache_provider'
  KEY blob PRIMARY KEYcqlsh> 
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-02-21 22:23:51.916,,,no_permission,,,,,,,,,,,,228602,,,Wed Feb 22 10:34:59 UTC 2012,,,,,,0|i0gpz3:,95638,slebresne,slebresne,,,,,,,,,"21/Feb/12 22:23;thepaul;Ignore expected-but-missing CF options. Change is in my 3933-1.0 branch at https://github.com/thepaul/cassandra/commit/12bcc56422 .

Patch is pretty trivial, but since it needs a tiny extra change in 1.1, I have it pre-merged in my 3933-1.1 branch at https://github.com/thepaul/cassandra/commit/a401034f56 .","22/Feb/12 10:34;slebresne;+1, committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool ring output not sorted by token order,CASSANDRA-3863,12541457,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,mallen,mallen,06/Feb/12 18:38,12/Mar/19 14:04,13/Mar/19 22:26,07/Feb/12 17:32,1.1.0,,,Tool/nodetool,,,0,,,,,"Prior to 1.1 the output of nodetool ring was sorted in token order.  It looks like StorageService.getTokenToEndpointMap has been changed to return Map<String,String> in place of the previously used Map<Token, String>.",,,,,,,,,,,,,,,,07/Feb/12 17:15;yukim;cassandra-1.1-3863.txt;https://issues.apache.org/jira/secure/attachment/12513638/cassandra-1.1-3863.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-07 17:15:26.299,,,no_permission,,,,,,,,,,,,226744,,,Tue Feb 07 17:32:42 UTC 2012,,,,,,0|i0gp53:,95503,brandon.williams,brandon.williams,,,,,,,,,07/Feb/12 17:15;yukim;Patch attached to let {{getTokenToEndpointMap }} return Map with tokens in ascending order.,07/Feb/12 17:32;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure a directory is selected for Compaction,CASSANDRA-3985,12544758,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,amorton,amorton,01/Mar/12 10:38,12/Mar/19 14:04,13/Mar/19 22:26,03/May/12 23:02,1.0.10,1.1.1,,,,,0,,,,,"From http://www.mail-archive.com/user@cassandra.apache.org/msg20757.html

CompactionTask.execute() checks if there is a valid compactionFileLocation only if partialCompactionsAcceptable() . upgradesstables results in a CompactionTask with userdefined set, so the valid location check is not performed. 

The result is a NPE, partial stack 

{code:java}
$ nodetool -h localhost upgradesstables
Error occured while upgrading the sstables for keyspace MyKeySpace
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
        at org.apache.cassandra.db.compaction.CompactionManager.performSSTableRewrite(CompactionManager.java:219)
        at org.apache.cassandra.db.ColumnFamilyStore.sstablesRewrite(ColumnFamilyStore.java:995)
        at org.apache.cassandra.service.StorageService.upgradeSSTables(StorageService.java:1648)
<snip>
Caused by: java.lang.NullPointerException
        at java.io.File.<init>(File.java:222)
        at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:641)
        at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:652)
        at org.apache.cassandra.db.ColumnFamilyStore.createCompactionWriter(ColumnFamilyStore.java:1888)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:151)
        at org.apache.cassandra.db.compaction.CompactionManager$4.perform(CompactionManager.java:229)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
{code}

(night time here, will fix tomorrow, anyone else feel free to fix it.)",,,,,,,,,,,,,,,,03/May/12 22:33;jbellis;3985-2.txt;https://issues.apache.org/jira/secure/attachment/12525520/3985-2.txt,05/Mar/12 19:23;amorton;cassandra-1.0-3985.txt;https://issues.apache.org/jira/secure/attachment/12517111/cassandra-1.0-3985.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-03-07 16:35:42.763,,,no_permission,,,,,,,,,,,,229944,,,Thu May 03 23:02:50 UTC 2012,,,,,,0|i0gqmf:,95743,xedin,xedin,,,,,,,,,"05/Mar/12 19:23;amorton;Modified DatabaseDescriptor.getDataFileLocationForTable() to add ensureFreeSpace.

Reformatted getDataFileLocationForTable() as it was small. 
 
Removed unused DatabaseDescriptor.getNextAvailableDataLocation()

","07/Mar/12 16:35;xedin;Question - what is the reason why we only return ""true"" from ""ensureFreeSpace"" if action is not user defined?

Few styling issues:

{code}
public synchronized static String getDataFileLocationForTable(String table, long expectedCompactedFileSize,
                                                                  boolean ensureFreeSpace )
{code}

should be changed to 

{code}
public synchronized static String getDataFileLocationForTable(String table, 
                                                              long expectedCompactedFileSize,
                                                              boolean ensureFreeSpace)
{code}

or all arguments written on the same line.

Also we don't use spaces to delimit operands e.g.

{code}
for ( int i = 0 ; i < dataDirectoryForTable.length ; i++ )
{code}

I can see those styling problems inside of getDataFileLocationForTable(...) method.
","22/Mar/12 13:01;xedin;Figured out answer to my stupid question :)

Committed with code-style cleanup and removed ""currentIndex"" static variable from DatabaseDescriptor which is unused after getNextAvailableDataLocation() was dropped.","01/Apr/12 20:03;amorton;Sorry for not getting back.
Thanks.","03/May/12 22:30;jbellis;I don't understand this.  This will still return null for user defined compactions under the same conditions it would have before, with no log message, since ensureFreeSpace == !isUserDefined.",03/May/12 22:33;jbellis;It seems to me that the real fix we need is to turn the assert in CompactionTask into an if statement so it can't be turned off.  Patch attached.,03/May/12 22:35;jbellis;(All the other callers of {{getDataFileLocation}} and {{getDataFileLocationForTable}} already check for null correctly.),"03/May/12 22:51;xedin;+1, right now we won't try to return the directory with the biggest empty space and would abort compaction if there is no sufficient space left.",03/May/12 23:02;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool cleanup/scrub/upgradesstables promotes all sstables to next level (LeveledCompaction),CASSANDRA-3989,12544881,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,makiw,makiw,02/Mar/12 03:21,12/Mar/19 14:04,13/Mar/19 22:26,07/Mar/12 17:39,1.0.9,1.1.0,,,,,0,lcs,,,,"1.0.7 + LeveledCompactionStrategy
If you run nodetool cleanup, scrub, or upgradesstables, Cassandra execute compaction for each sstable. During the compaction, it put the new sstable to next level of the original sstable. If you run cleanup many times, sstables will reached to the highest level, and CASSANDRA-3608 will happens at next cleanup.

Reproduce procedure:
# create column family CF1 with compaction_strategy=LeveledCompactionStrategy and compaction_strategy_options={sstable_size_in_mb: 5};
# Insert some data into CF1.
# nodetool flush
# Verify the sstable is created at L1 in CF1.json
# nodetool cleanup
# Verify sstable in L1 is removed and new sstable is created at L2 in CF1.json
# repeat nodetool cleanup some times",RHEL6,,,,,,,,,,,,,,,07/Mar/12 13:38;slebresne;3989.txt;https://issues.apache.org/jira/secure/attachment/12517399/3989.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-02 18:36:26.496,,,no_permission,,,,,,,,,,,,230067,,,Thu Mar 15 04:05:15 UTC 2012,,,,,,0|i0gqnz:,95750,jbellis,jbellis,,,,,,,,,02/Mar/12 03:30;makiw;Fix LeveledManifest.promote will not promote sstables to next level if the number of source sstables (removed) is one.,"02/Mar/12 18:36;jbellis;Nice bug hunting.  Committed a lightly edited version of your fix: https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commitdiff;h=53fb52ac713e5471edd988b59cbd75f202a4f57b

Thanks!","06/Mar/12 05:05;makiw;Johathan, 
This fix has a problem. If you have only one Level 0 sstable, and there are no Level 1 sstables involved in the compaction, promote method will chooses 0 as newLevel, and it causes assertion.
Should I create a new ticket?","06/Mar/12 05:53;jbellis;No, let's reopen this one","07/Mar/12 02:47;makiw;Another problem with the fix.
In following condition, the background compaction task start looping.
- No L0 sstable
- Enough number of L1 sstables, exceed L1 capacity (compaction score > 1.1)

It seems the background task try to promote sstables to reduce compaction score, but it can't by this fix, then it will compact each L1 sstables forever.
","07/Mar/12 05:55;makiw;It is more complicated than I first thought.
getCandidatesFor(int level) returns next compaction candidates for the level by:
- sort the generations and find a sstable where we left off last time
- and then find overlappng sstables from next level

So if we add a new sstable into same level, getCompactionCandidates won't return empty, and then getNextBackgoundTask returns non-null task forever.
I think we should better to back out the fix and look for better strategy to resolve the issue.","07/Mar/12 13:28;slebresne;I agree, the committed fix does more harm than it helps so I reverted it.
I'm trying some other approach so will hopefully attach another version soon.","07/Mar/12 13:38;slebresne;Attaching another approach that makes the compaction type available to the leveled compaction that for cleanup, scrub and upgradeSSTables simply replace the old sstable by the new one (without changing level or anything else). The rational is those operation don't really ""change"" the sstable content and should simply ""replace"" sstables.",07/Mar/12 17:16;jbellis;+1,"07/Mar/12 17:39;slebresne;Committed, thanks","08/Mar/12 04:25;scode;Hmm, Does this address the case from CASSANDRA-3952?

{quote}
I think this might happen whenever there is exactly one sstable in L0 that is large enough for the score for L0 to be > 1, and if L1 is full (so that skipLevels doesn't promote).

It's possible the sstables I had lying around from sized-tiered compaction combined with my write load conspired to trigger this case.
{quote}","08/Mar/12 04:37;scode;Sorry, I think I just pulled before merge-to-trunk and was still looking at a trunk containing the original version of the patch.",15/Mar/12 04:05;makiw;Removed first patch file I attached here because it is harmful.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Short read protection is broken,CASSANDRA-3934,12543390,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,20/Feb/12 18:44,12/Mar/19 14:04,13/Mar/19 22:26,22/Feb/12 13:25,1.0.8,1.1.0,,,,,0,,,,,"When a read needs to do more than one retry (due to short reads), the originalCount is not preserved by the retry leading to returning more than the requested number of columns.
Moreover, when a retried read checks whether more retry is needed, it doesn't compare the number of live column retrieved against the original number of columns requested by the user, but against the number of columns requested during the retry, making it much more likely to actually do one more retry.

This catch by the two tests 'short_read_test' and 'short_read_reversed_test' at https://github.com/riptano/cassandra-dtest/blob/master/consistency_test.py that are failing intermittently.",,,,,,,,,,,,,,,,20/Feb/12 18:50;slebresne;3934.txt;https://issues.apache.org/jira/secure/attachment/12515269/3934.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-22 11:32:19.212,,,no_permission,,,,,,,,,,,,228636,,,Wed Feb 22 13:25:05 UTC 2012,,,,,,0|i0gpzj:,95640,brandon.williams,brandon.williams,,,,,,,,,"20/Feb/12 18:50;slebresne;Patch is against 1.0. The patch:
* don't retry if we have enough columns to satisfy the initial query
* keep the original count intact even if we do multiple retries.
* optimize the number of columns asked by a retry based on the ratio of number of column got/total asked on the initial query.",22/Feb/12 11:32;brandon.williams;+1,"22/Feb/12 13:25;slebresne;committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh still failing to handle decode errors in some column names,CASSANDRA-4003,12545279,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,06/Mar/12 04:55,12/Mar/19 14:04,13/Mar/19 22:26,27/Mar/12 19:31,1.0.9,1.1.0,,Legacy/Tools,,,0,cqlsh,,,,"Columns which are expected to be text, but which are not valid utf8, cause cqlsh to display an error and not show any output:

{noformat}
cqlsh:ks> CREATE COLUMNFAMILY test (a text PRIMARY KEY) WITH comparator = timestamp;
cqlsh:ks> INSERT INTO test (a, '2012-03-05') VALUES ('val1', 'val2');
cqlsh:ks> ASSUME test NAMES ARE text;
cqlsh:ks> select * from test;
'utf8' codec can't decode byte 0xe1 in position 4: invalid continuation byte
{noformat}

the traceback with cqlsh --debug:

{noformat}
Traceback (most recent call last):
  File ""bin/cqlsh"", line 581, in onecmd
    self.handle_statement(st)
  File ""bin/cqlsh"", line 606, in handle_statement
    return custom_handler(parsed)
  File ""bin/cqlsh"", line 663, in do_select
    self.perform_statement_as_tokens(parsed.matched, decoder=decoder)
  File ""bin/cqlsh"", line 666, in perform_statement_as_tokens
    return self.perform_statement(cqlhandling.cql_detokenize(tokens), decoder=decoder)
  File ""bin/cqlsh"", line 693, in perform_statement
    self.print_result(self.cursor)
  File ""bin/cqlsh"", line 728, in print_result
    self.print_static_result(cursor)
  File ""bin/cqlsh"", line 742, in print_static_result
    formatted_names = map(self.myformat_colname, colnames)
  File ""bin/cqlsh"", line 413, in myformat_colname
    wcwidth.wcswidth(name.decode(self.output_codec.name)))
  File ""/usr/local/Cellar/python/2.7.2/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0xe1 in position 4: invalid continuation byte
{noformat}",,,,,,,,,,,,CASSANDRA-7018,,,,27/Mar/12 16:05;thepaul;4003-2.txt;https://issues.apache.org/jira/secure/attachment/12520140/4003-2.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-07 17:24:35.923,,,no_permission,,,,,,,,,,,,230465,,,Tue Mar 31 21:47:14 UTC 2015,,,,,,0|i0gqu7:,95778,brandon.williams,brandon.williams,,,,,,,,,"07/Mar/12 00:45;thepaul;A fix is pushed to my 4003 github branch, also tagged at https://github.com/thepaul/cassandra/tree/pending/4003 .","07/Mar/12 17:24;jbellis;It looks like the core of the fix is this:

{noformat}
+    def get_nametype(self, cursor, num):
+        """"""
+        Determine the Cassandra type of a column name from the current row of
+        query results on the given cursor. The column in question is given by
+        its zero-based ordinal number within the row.
+
+        Pretty big hack, but necessary to differentiate some things like ascii
+        vs. blob hex. Probably this should be available from the driver
+        somehow, instead.
+        """"""
+
+        row = cursor.result[cursor.rs_idx - 1]
+        col = row.columns[num]
+        schema = cursor.decoder.schema
+        return schema.name_types.get(col.name, schema.default_name_type)
{noformat}

Can you elaborate as to what's going on?","07/Mar/12 17:51;thepaul;Sure. Since the CQL driver deserializes column names before the client software (cqlsh) can see them, and does not expose the Cassandra data type for the column names, it was not always possible to determine from returned column names how they were meant to be interpreted. For example, it was sometimes impossible to tell TimeUUIDType from UUIDType, or any of the various integer or counter types apart, or even BytesType from AsciiType.

Cqlsh makes an effort to display data in the most meaningful form, and secondarily to visually distinguish data that would otherwise be too ambiguous using colors. So it needs to know the original column name type.

The CQL driver does not expose that, so this code uses internals to get it. Clearly it would make more sense to expose the info from the driver side, and I plan to do that, but it takes some extra process and testing. This hack is backwards compatible with older CQL driver versions, but possibly not forwards-compat.

Maybe it would be best to do a runtime check against the driver to see if it supports exposing column types before making this call.","27/Mar/12 16:05;thepaul;The attached patch (also present in my updated 4003 branch) will check for column name-type support in the CQL driver before using the direct-inspection approach. python-cql version 1.0.10 supports this, but we don't need to require support for 1.0.10 yet.","27/Mar/12 16:06;thepaul;Since i linked the previous tag, the new one is at https://github.com/thepaul/cassandra/tree/pending/4003-2 .",27/Mar/12 19:31;brandon.williams;Committed.,"02/Jul/12 10:56;jeromatron;this should be fixed in python-cql version 1.0.10 and cassandra 1.0.9?  I'm using dse 2.1 which is based on cassandra 1.0.10 and I did apt-show-versions of python cql and it says ""python-cql/stable uptodate 1.0.10-1"".  I'm still getting this error:
{quote}
'ascii' codec can't decode byte 0xc3 in position 9: ordinal not in range(128)
{quote}

Is there still the possibility for this to come up?  I'm utilizing a solr_query in the where clause btw, if that makes any difference.","09/Jul/12 22:06;thepaul;That shouldn't make a difference. Can you tell me what data is in that cf/column and paste the full output of a minimal ""cqlsh --debug"" session with that query?",31/Mar/15 13:51;gadelkareem;I experience this bug while importing a file using SOURCE file,"31/Mar/15 21:47;thobbs;[~gadelkareem] if you're seeing this problem with Cassandra 2.0 or 2.1, can you open a new ticket with more details and a new stacktrace?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.util.ConcurrentModificationException in Gossiper,CASSANDRA-4019,12545635,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,tbritz,tbritz,08/Mar/12 10:46,12/Mar/19 14:04,13/Mar/19 22:26,08/Mar/12 23:49,1.0.0,1.0.9,1.1.0,,,,0,,,,,"I have never seen this one before. Might be triggered by a race condition under heavy load. This error was triggered on 0.8.9


ERROR [GossipTasks:1] 2012-03-05 04:16:55,263 Gossiper.java (line 162) Gossip error
java.util.ConcurrentModificationException
        at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:605)
        at org.apache.cassandra.utils.AbstractStatsDeque.sum(AbstractStatsDeque.java:37)
        at org.apache.cassandra.utils.AbstractStatsDeque.mean(AbstractStatsDeque.java:60)
        at org.apache.cassandra.gms.ArrivalWindow.mean(FailureDetector.java:259)
        at org.apache.cassandra.gms.ArrivalWindow.phi(FailureDetector.java:282)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:155)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:538)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:57)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:157)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [GossipStage:1] 2012-03-05 04:16:55,263 Gossiper.java (line 737) Node /192.168.3.18 has restarted, now UP again
 INFO [GossipStage:1] 2012-03-05 04:16:55,264 Gossiper.java (line 705) InetAddress /192.168.3.18 is now UP
",,,,,,,,,,,,,,,,08/Mar/12 17:55;brandon.williams;4019-trunk.txt;https://issues.apache.org/jira/secure/attachment/12517580/4019-trunk.txt,08/Mar/12 16:19;brandon.williams;4019.txt;https://issues.apache.org/jira/secure/attachment/12517572/4019.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-03-08 15:19:04.538,,,no_permission,,,,,,,,,,,,230818,,,Thu Mar 08 23:49:28 UTC 2012,,,,,,0|i0gr0f:,95806,jbellis,jbellis,,,,,,,,,"08/Mar/12 15:19;brandon.williams;It seems like the Right Way to solve this is to refactor BoundedStatsDeque to operate like the dsnitch's AdapativeLatencyTracker, which *is* threadsafe, and then have ALT extend BSD.  BSD isn't used anywhere else except the FD.","08/Mar/12 16:19;brandon.williams;For 0.8/1.0, do the simplest thing and switch BDS from ArrayDeque to LinkedBlockingDeque, imitating the ALT behavior (which is well tested.)  For trunk I'll do the aforementioned refator.",08/Mar/12 17:55;brandon.williams;Patch for trunk which removes ASD since all we actually use is sum and mean and consolidates it into BDS.  Dsnitch's ALT is also removed since BDS itself contains all the functionality it had before.,08/Mar/12 23:28;jbellis;+1 on both,08/Mar/12 23:49;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Races between schema changes and StorageService operations,CASSANDRA-2350,12501718,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,paladin8,paladin8,17/Mar/11 18:30,12/Mar/19 14:03,13/Mar/19 22:26,18/Mar/11 13:20,0.7.5,,,,,,0,,,,,"I only tested this on 0.7.0, but it judging by the 0.7.3 code (latest I've looked at) the same thing should happen.

The case in particular that I ran into is this: I force a compaction for all CFs in a keyspace, and while the compaction is happening I add another CF to the keyspace. I get the following exception because the underlying set of CFs has changed while being iterated over.

{noformat}
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(Unknown Source)
        at java.util.HashMap$ValueIterator.next(Unknown Source)
        at java.util.Collections$UnmodifiableCollection$1.next(Unknown Source)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1140)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Unknown Source)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
        at sun.rmi.transport.Transport$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source) 
{noformat}

The problem is a little more fundamental than that, though, as I believe any schema change of CFs in the keyspace during one of these operations (e.g. flush, compaction, etc) has the potential to cause a race. I'm not sure what would happen if the set of CFs to compact was acquired and one of them was dropped before it had been compacted.",,,,,,,,,,,,,,,,18/Mar/11 04:15;jbellis;2350.txt;https://issues.apache.org/jira/secure/attachment/12473969/2350.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-18 04:15:41.605,,,no_permission,,,,,,,,,,,,20573,,,Mon Mar 21 22:15:38 UTC 2011,,,,,,0|i0gavb:,93191,gdusbabek,gdusbabek,,,,,,,,,18/Mar/11 04:15;jbellis;make Table.columnFamilyStores a ConcurrentHashMap (and make it private),18/Mar/11 12:43;gdusbabek;+1,18/Mar/11 13:20;jbellis;committed,"21/Mar/11 22:15;hudson;Integrated in Cassandra-0.7 #397 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/397/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null CF comments should be allowed,CASSANDRA-2351,12501730,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,gdusbabek,gdusbabek,17/Mar/11 20:29,12/Mar/19 14:03,13/Mar/19 22:26,17/Mar/11 21:14,0.8 beta 1,,,,,,0,,,,,"Prior to 1906, cassandra tolerated null CF comments.  They were converted to empty quotes when the CFM was created.

",,,,,,,,,,,,,,,,17/Mar/11 20:57;jhermes;2351.txt;https://issues.apache.org/jira/secure/attachment/12473944/2351.txt,17/Mar/11 20:31;gdusbabek;null_cf_comment_test_case.patch;https://issues.apache.org/jira/secure/attachment/12473942/null_cf_comment_test_case.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-17 20:57:01.582,,,no_permission,,,,,,,,,,,,20574,,,Wed Mar 23 01:28:10 UTC 2011,,,,,,0|i0gavj:,93192,gdusbabek,gdusbabek,,,,,,,,,17/Mar/11 20:31;gdusbabek;Attached a test case that demonstrates the regression.,"17/Mar/11 20:57;jhermes;Enforces comment never null, also includes this test for further regression-catching.",17/Mar/11 21:14;gdusbabek;+1 committed.,"23/Mar/11 01:28;hudson;Integrated in Cassandra #797 (See [https://hudson.apache.org/hudson/job/Cassandra/797/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup conversions between bytes and strings,CASSANDRA-2367,12502184,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Mar/11 14:22,12/Mar/19 14:03,13/Mar/19 22:26,28/Mar/11 21:06,0.7.5,,,,,,0,,,,,"There is a bit of inconsistency in our conversions between ByteBuffers and Strings.
For instance, ByteBufferUtil.string() uses as a default the java default charset, while ByteBufferUtil.bytes(String) assumes UTF8. Moreover, a number of places in the code don't use those functions and uses getBytes() directly. There again, we often encode with the default charset but decode in UTF8 or the contrary.

Using the default charset is probably a bad idea anyway, since this depends on the actual system the node is running on and could lead to a stupid bug when running in heterogeneous systems.

This ticket proposes to always assume UTF8 all over the place (and tries to use the ByteBufferUtil as much as possible to help with that).",,7200,7200,,0%,7200,7200,,,,,,,,,23/Mar/11 14:23;slebresne;0001-Cleanup-bytes-string-conversions.patch;https://issues.apache.org/jira/secure/attachment/12474397/0001-Cleanup-bytes-string-conversions.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-23 17:11:13.833,,,no_permission,,,,,,,,,,,,20583,,,Mon Mar 28 21:06:22 UTC 2011,,,,,,0|i0gayv:,93207,jbellis,jbellis,,,,,,,,,23/Mar/11 14:23;slebresne;Attaching patch against 0.7.,"23/Mar/11 17:11;jbellis;I see two places this fixes bugs:

- HintedHandOffManger: post-delivery hint deletion is now done w/ UTF8 encoding, which matches old and new encoding of ip-address-as-string.
- SystemTable now encodes cluster name as UTF8; before it encoded as system encoding, decoded as UTF8.

Is that accurate?","23/Mar/11 17:27;slebresne;There is also:
  * the avro schema (DEFINITION_SCHEMA_COLUMN_NAME) for mutation. I was encoded in UTF8 (in Migration.java), but decoded using system encoding (in DefsTable.loadFromStorage(), since decoded by ByteBufferUtil.string() with default charset).
  * In HintedHandOffManager, the combined table and cfName is encoded as UTF8 but decoded with system encoding (once again through the use of BBUtil.string() with no specific charset.
",23/Mar/11 18:13;jbellis;committed to 0.7 w/ some build fixes for contrib/ (so you will want to base port to trunk on r1084660),"23/Mar/11 18:34;hudson;Integrated in Cassandra-0.7 #401 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/401/])
    fix encoding bugs in HintedHandoffManager, SystemTable when default charset is not UTF8
patch by slebresne; reviewed by jbellis for CASSANDRA-2367
","28/Mar/11 20:38;hudson;Integrated in Cassandra #812 (See [https://hudson.apache.org/hudson/job/Cassandra/812/])
    ",28/Mar/11 21:06;jbellis;(merged to trunk by Sylvain),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ByteBufferUtil.read(byte[]) returns 0 when the end of the stream has been reached.,CASSANDRA-2365,12502073,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,julie.zhang10,julie.zhang10,22/Mar/11 18:23,12/Mar/19 14:03,13/Mar/19 22:26,23/Mar/11 02:29,0.7.5,,,,,,0,,,,,"read(byte[], int, int) doesn't return -1 when the end of the stream is reached. Instead, it returns 0. 

len = Math.min(len, copy.remaining());
copy.get(bytes, off, len);

return len;

copy.remaining() returns 0 when the end of the stream is reached. ",,,,,,,,,,,,,,,,22/Mar/11 18:32;jbellis;2365.txt;https://issues.apache.org/jira/secure/attachment/12474319/2365.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-22 18:32:32.351,,,no_permission,,,,,,,,,,,,20582,,,Wed Mar 23 02:58:21 UTC 2011,,,,,,0|i0gayf:,93205,slebresne,slebresne,,,,,,,,,22/Mar/11 18:32;jbellis;patch to return -1 if no more data,22/Mar/11 18:56;slebresne;+1,23/Mar/11 02:29;jbellis;committed,"23/Mar/11 02:58;hudson;Integrated in Cassandra-0.7 #399 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/399/])
    fix potential infinite loop in ByteBufferUtil.inputStream
patch by jbellis; reviewed by slebresne for CASSANDRA-2365
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up thread pool and queue sizes,CASSANDRA-2333,12501491,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,15/Mar/11 18:11,12/Mar/19 14:03,13/Mar/19 22:26,23/Mar/11 16:22,0.7.5,,,,,,0,,,,,"Most of Cassandra assumes that ThreadPoolExecutor handles tasks by starting with Core threads, adding threads up to Max as tasks arrive, then queuing any additional.  This is not correct:

{noformat}
    If fewer than corePoolSize threads are running, the Executor always prefers adding a new thread rather than queuing.
    If corePoolSize or more threads are running, the Executor always prefers queuing a request rather than adding a new thread.
    If a request cannot be queued, a new thread is created unless this would exceed maximumPoolSize, in which case, the task will be rejected.
{noformat}

CASSANDRA-2178 fixed this in one place but made it worse by default since most people run with a single data dir, meaning as soon as you have multiple CFs flushing (or a single one with indexes) then you will start blocking writes.",,,,,,,,,,,,,,,,15/Mar/11 18:22;jbellis;2333.txt;https://issues.apache.org/jira/secure/attachment/12473712/2333.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-15 19:04:08.97,,,no_permission,,,,,,,,,,,,20566,,,Tue Mar 15 20:31:32 UTC 2011,,,,,,0|i0garj:,93174,brandon.williams,brandon.williams,,,,,,,,,"15/Mar/11 18:22;jbellis;Patch to remove use of maximumpoolsize in favor of core size + timing out core threads.  Also adds memtable_flush_queue_size configuration directive, defaulting to 4.","15/Mar/11 19:04;brandon.williams;+1, increased flush queue size is enough to prevent blocking writes at all with stress and an index.","15/Mar/11 20:31;hudson;Integrated in Cassandra-0.7 #386 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/386/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
row cache / streaming aren't aware of each other,CASSANDRA-2420,12503457,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,mdennis,mdennis,05/Apr/11 21:26,12/Mar/19 14:03,13/Mar/19 22:26,18/Apr/11 14:52,0.7.5,0.8 beta 1,,,,,0,,,,,"SSTableWriter.Builder.build() takes tables that resulted from streaming, repair, bootstrapping, et cetera and builds the indexes and bloom filters before ""adding"" it so the current node is aware of it.

However, if there is data present in the cache for a row that is also present in the streamed table the row cache can over shadow the data in the newly built table.  In other words, until the row in row cache is removed from the cache (e.g. because it's pushed out because of size, the node is restarted, the cache is manually cleared) the data in the newly built table will never be returned to clients.

The solution that seems most reasonable at this point is to have SSTableWriter.Builder.build() (or something below it) update the row cache if the row key in the table being built is also present in the cache.
",,,,,,,,,,,,,,,,13/Apr/11 16:03;slebresne;0001-Handle-the-row-cache-for-streamed-row-v2.patch;https://issues.apache.org/jira/secure/attachment/12476252/0001-Handle-the-row-cache-for-streamed-row-v2.patch,11/Apr/11 19:54;slebresne;0001-Handle-the-row-cache-for-streamed-row.patch;https://issues.apache.org/jira/secure/attachment/12476045/0001-Handle-the-row-cache-for-streamed-row.patch,18/Apr/11 14:32;slebresne;2420-for-0.7.patch;https://issues.apache.org/jira/secure/attachment/12476612/2420-for-0.7.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-04-11 19:54:56.767,,,no_permission,,,,,,,,,,,,20614,,,Mon Apr 18 15:06:45 UTC 2011,,,,,,0|i0gb9r:,93256,jbellis,jbellis,,,,,,,,,"11/Apr/11 19:54;slebresne;There is a very simple patch for this issue. It consists in invalidating the cache for each key we index. The downside is that this will invalidate all key that gets repaired, but updating the cache (instead of invalidating) implies reading on disk so doing this during the indexing or at the next read may not matter much. In any case, this is better that the current situation and after all .

I however attached a patch (against trunk for now) that 'do the right thing' and will update the cache in the case of repair instead of invalidating. I mentioned the first solution in case we consider that the 'right one' is too disruptive for 0.7 for instance (not that the patch is very complicated).

Note that the patch fixes a tiny unrelated issue: the writeStat are not updated during a write if the used cache has 'isPutCopying' (this could be fixed separately).
","11/Apr/11 20:49;jbellis;I would be more comfortable having LCR throw UnsupportedOperation if asked for full row, since You Shouldn't Do That.

Would prefer the updateCache case to be AES: ... default: invalidate and break; it's more obvious looking at it what the point is, and ""unnecessary"" invalidate calls will be harmless.","13/Apr/11 16:06;slebresne;bq. I would be more comfortable having LCR throw UnsupportedOperation if asked for full row, since You Shouldn't Do That.

Updated patch defines getFullColumnFamily() only for AbstractCompactedRow. However I think it would be a bad idea to fail in the Builder, so the Builder now simply invalidate the cache if he is facing a big row (hence not fitting it in memory) and log a warning since if that happens ""you're doing it wrong"".

I've also changed the switch case in updateCache.",14/Apr/11 20:30;jbellis;+1,14/Apr/11 20:32;jbellis;nit: s/higly/highly/ in the logged warning,"18/Apr/11 11:21;hudson;Integrated in Cassandra #854 (See [https://hudson.apache.org/hudson/job/Cassandra/854/])
    Merge CASSANDRA-2420 from 0.8
","18/Apr/11 11:34;hudson;Integrated in Cassandra-0.8 #13 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/13/])
    Update row cache post streaming
patch by slebresne; reviewed by jbellis for CASSANDRA-2420
","18/Apr/11 13:21;slebresne;Committed to 0.8 and trunk.
Was should we do about 0.7 ? I realized that we do not differentiate between the different reason for streaming in 0.7, so the simplest way to deal with this would probably be to just blindly invalidate the cache. Sounds reasonable ?",18/Apr/11 13:30;jbellis;Yes.,"18/Apr/11 14:32;slebresne;Attaching simple patch targeting 0.7. I put it for review individually because it's different enough from previous patch (but it's a one-liner, so should be too long to review anyway)",18/Apr/11 14:38;jbellis;+1,18/Apr/11 14:52;slebresne;Committed to 0.7 and 0.7/trunk,"18/Apr/11 15:06;hudson;Integrated in Cassandra-0.7 #437 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/437/])
    Invalidate cache for streamed rows
patch by slebresne; reviewed by jbellis for CASSANDRA-2420
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove loadbalance command,CASSANDRA-2448,12503953,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,11/Apr/11 17:00,12/Mar/19 14:03,13/Mar/19 22:26,11/Apr/11 17:35,0.8 beta 1,,,Legacy/Tools,,,0,,,,,"With the update to how the move command works, the loadbalance command is even less useful that it was previously.  The loadbalance command now calculates the token it is going to move to before it leaves which means it isn't considering the load it is giving away. Given that, I think we should just remove the loadbalance command entirely. Anyone who wants to do an old style loadbalance can just do decommission then bootstrap.

This is a minor change, and honestly I think it might count as a 'bug' so I think we should squeeze it into 0.8, post-freeze. ",,,,,,,,,,,,,,,,11/Apr/11 17:00;nickmbailey;0001-Remove-loadbalance-command-from-nodetool.patch;https://issues.apache.org/jira/secure/attachment/12476019/0001-Remove-loadbalance-command-from-nodetool.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-11 17:35:40.136,,,no_permission,,,,,,,,,,,,20630,,,Wed Apr 20 23:46:43 UTC 2011,,,,,,0|i0gbfj:,93282,jbellis,jbellis,,,,,,,,,"11/Apr/11 17:35;jbellis;lgtm, committed",11/Apr/11 19:11;nickmbailey;this should also be applied to trunk,11/Apr/11 19:36;jbellis;merging to trunk is asynchronous but will happen,"20/Apr/11 23:43;cdaw;Not sure how documentation gets updated, but we still mention using the loadbalance command on the wiki:

http://wiki.apache.org/cassandra/Operations#Moving_or_Removing_nodes",20/Apr/11 23:46;nickmbailey;Updated the wiki to indicate that the loadbalance command is only available in versions 0.7.* and lower.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cli divides read repair chance by 100,CASSANDRA-2458,12504040,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,12/Apr/11 11:40,12/Mar/19 14:03,13/Mar/19 22:26,12/Apr/11 15:18,0.8 beta 1,,,,,,0,cli,,,,cli incorrectly divides the read_repair chance by 100 when creating / updating CF's,,,,,,,,,,,,,,,,12/Apr/11 11:50;amorton;0001-do-not-divide-read_repair_chance-by-100.patch;https://issues.apache.org/jira/secure/attachment/12476112/0001-do-not-divide-read_repair_chance-by-100.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-12 15:18:52.065,,,no_permission,,,,,,,,,,,,20635,,,Tue Apr 12 16:42:13 UTC 2011,,,,,,0|i0gbhr:,93292,jbellis,jbellis,,,,,,,,,12/Apr/11 11:50;amorton;now expects read repair chance to be between 0 and 1 for create and update CF.,12/Apr/11 15:18;jbellis;committed,"12/Apr/11 16:42;hudson;Integrated in Cassandra-0.8 #2 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/2/])
    cli no longer divides read_repair_chance by 100
patch by Aaron Morton; reviewed by jbellis for CASSANDRA-2458
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in CacheWriter.saveCache(),CASSANDRA-2416,12503379,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,skamio,skamio,05/Apr/11 09:28,12/Mar/19 14:03,13/Mar/19 22:26,18/Apr/11 16:56,0.7.5,,,,,,0,,,,,"I've seen NullPointerException of CacheWriter in our cluster (replication 3).

ERROR [CompactionExecutor:1] 2011-04-05 09:57:42,968 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithLength(ByteBufferUtil.java:275)
        at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:84)
        at org.apache.cassandra.db.CompactionManager$10.runMayThrow(CompactionManager.java:960)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
",linux,,,,,,,,,,,,,,,18/Apr/11 16:20;jbellis;2416-v2.txt;https://issues.apache.org/jira/secure/attachment/12476624/2416-v2.txt,08/Apr/11 21:23;jbellis;2416.txt;https://issues.apache.org/jira/secure/attachment/12475841/2416.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-08 21:23:46.28,,,no_permission,,,,,,,,,,,,20610,,,Mon Apr 18 18:07:44 UTC 2011,,,,,,0|i0gb8v:,93252,slebresne,slebresne,,,,,,,,,08/Apr/11 21:23;jbellis;Patch to avoid storing DecoratedKeys with null key (which are generated by getRangeSlice) in the key cache.,08/Apr/11 21:41;slebresne;Why not do that in cacheKey directly to be sure we don't miss places ? ,18/Apr/11 16:20;jbellis;Because attempting to cache a DK that is really just a token is a semantic error that we shouldn't just paper over.  v2 adds an assert to make that clear.,18/Apr/11 16:46;slebresne;+1,18/Apr/11 16:56;jbellis;committed,"18/Apr/11 18:07;hudson;Integrated in Cassandra-0.7 #439 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/439/])
    avoid caching token-only decoratedkeys
patch by jbellis; reviewed by slebresne for CASSANDRA-2416
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up after failed compaction,CASSANDRA-2468,12504212,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,amorton,jbellis,jbellis,13/Apr/11 21:05,12/Mar/19 14:03,13/Mar/19 22:26,23/Jun/11 05:49,1.0.0,,,,,,1,,,,,(Started in CASSANDRA-2088.),,,,,,,,,,CASSANDRA-2629,CASSANDRA-2576,,,,,22/Jun/11 02:03;stuhood;0001-CASSANDRA-2468-clean-up-temp-files-after-failed-compac.txt;https://issues.apache.org/jira/secure/attachment/12483390/0001-CASSANDRA-2468-clean-up-temp-files-after-failed-compac.txt,06/Jun/11 01:59;amorton;0001-clean-up-temp-files-after-failed-compaction-v08-2.patch;https://issues.apache.org/jira/secure/attachment/12481527/0001-clean-up-temp-files-after-failed-compaction-v08-2.patch,08/Jun/11 10:11;amorton;0001-clean-up-temp-files-after-failed-compaction-v08-3.patch;https://issues.apache.org/jira/secure/attachment/12481796/0001-clean-up-temp-files-after-failed-compaction-v08-3.patch,06/May/11 02:57;amorton;0001-clean-up-temp-files-after-failed-compaction-v08.patch;https://issues.apache.org/jira/secure/attachment/12478361/0001-clean-up-temp-files-after-failed-compaction-v08.patch,06/May/11 02:57;amorton;0001-cleanup-temp-files-after-failed-compaction-v07.patch;https://issues.apache.org/jira/secure/attachment/12478360/0001-cleanup-temp-files-after-failed-compaction-v07.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-04-29 14:01:35.776,,,no_permission,,,,,,,,,,,,20642,,,Thu Jun 23 07:09:36 UTC 2011,,,,,,0|i0gbjz:,93302,stuhood,stuhood,,,,,,,,,29/Apr/11 14:01;slebresne;@Aaron: are you still working on this ? It would be nice to fix this quickly. And we should probably target 0.7 for that one.,29/Apr/11 21:51;amorton;should have time this weekend. ,"01/May/11 11:16;amorton;Attached patches for 0.7 and 0.8 rely on CASSANDRA-2588 to correctly detect sstables on disk. 

Added closeAndDeleteQuietly() to SSTableWriter and SSTableWriter.Builder to close open files and delete the SSTable files found on disk.

Checks for failures in CompactionManager doCompaction(), doScrub() and doCleanupCompaction() and submitSSTableBuild() 

Does not check in CompactionManager.submitIndexBuild() because the builder works against memtables and does not use an SSTableWriter. 

Checks for failures in Memtable.writeSortedContents()","03/May/11 09:59;slebresne;Comment:
    * I'm uncomfortable with having closeAndDeleteQuietly() delete the non tmp files (those really do not belong to the writer). Sure the calls are always conditioned so that we shouldn't messed up, but I'm not sure it's worth the risk of foot-shooting (it makes the function harder to use). I'd rather remove that part, rename closeAndDeleteQuietly() to something like cleanupIfNecessary() and call it every time in the finally block (which would remove the need to check if the reader was successfully created every damn time).
    * Nitpick: I'd prefer moving the code for SSTableWriter.Builder inside build() itself.
","03/May/11 11:50;amorton;The check to delete the non temp files was to cover the unlikely event that rename() threw an IOError part way through renaming the files. I thought about potentially deleting the wrong files but nothing else see the non temp files until the SSTR is returned. 

Happy to make the change tomorrow if you still want, it makes things a little safer. ","03/May/11 12:05;slebresne;bq. Happy to make the change tomorrow if you still want, it makes things a little safer.

Thanks, I prefer safer :) (I'm not too worried about rename failing)","04/May/11 19:42;amorton;Updated patches with suggested changes:
- only deletes temp files
- cleanupIfNecessary() used
- SSTW.Builder.build() cleans up self

NOTE: requires CASSANDRA-2602 to detect temp files correctly.","05/May/11 13:59;slebresne;Looks good but this needs rebasing (at least for 0.7, don't bother too much with 0.8, I'll try to merge it unless this is a pain)",06/May/11 02:57;amorton;rebased both the v07 and v08,02/Jun/11 19:18;stuhood;What's shakin' here?,"06/Jun/11 01:59;amorton;rebased the v08 version today, attached as 0001-clean-up-temp-files-after-failed-compaction-v08-2

Have not updated v07, let me know if you need it.  ","06/Jun/11 18:52;stuhood;* SSTable.delete will throw an IOError on IOException, which might kill cleanupIfNecessary... should we consider having delete throw IOException? I'd prefer not to catch IOError.
* {{SSTable.tempComponentsFor}} could probably be merged with componentsFor and an enum {{LIVE}}, {{TEMP}} or {{LIVE | TEMP}}? Not a blocker, just sugar.

After the IOError is fixed, +1 from me. Thanks Aaron!","06/Jun/11 19:00;jbellis;bq. SSTable.tempComponentsFor could probably be merged with componentsFor

I prefer the distinct, flag-less methods.","06/Jun/11 19:43;stuhood;bq. I prefer the distinct, flag-less methods.
componentsFor already has a boolean flag... my suggestion was to make it a ternary flag. Again, no big deal.","06/Jun/11 20:34;jbellis;Eh, you're right.  Then flags is probably better than flags AND multiple methods. :)","08/Jun/11 10:11;amorton;version 3 for v0.8 modified SSTable.delete() to raise an IOException so cleanupIfNecessary() can catch it. Also changes componentsFor to accept an enum. 

Do we want this in 0.7?","10/Jun/11 18:20;stuhood;+1 on the patch for 0.8/trunk
Thanks Aaron!",12/Jun/11 06:29;stuhood;Rebased for post-1610-trunk as v4.,22/Jun/11 02:03;stuhood;Rebased for trunk (assuming r1138084 is reverted).,"23/Jun/11 05:49;jbellis;committed, thanks all!","23/Jun/11 07:09;hudson;Integrated in Cassandra #936 (See [https://builds.apache.org/job/Cassandra/936/])
    clean up tmpfiles after failed compaction
patch by Aaron Morton; reviewed by slebresne and Stu Hood for CASSANDRA-2468

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1138740
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/Descriptor.java
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/io/util/FileUtils.java
* /cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableDeletingReference.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix build for distributed and stress tests,CASSANDRA-2462,12504104,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,12/Apr/11 20:46,12/Mar/19 14:03,13/Mar/19 22:26,14/Apr/11 18:38,0.8 beta 1,,,Legacy/Testing,Legacy/Tools,,0,,,,,Distributed and stress tests are not compiling for trunk.,,,,,,,,,,,,,,,,12/Apr/11 20:46;stuhood;0001-Update-stress-and-tests-for-trunk.txt;https://issues.apache.org/jira/secure/attachment/12476167/0001-Update-stress-and-tests-for-trunk.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-12 21:23:37.477,,,no_permission,,,,,,,,,,,,20637,,,Thu Apr 14 18:38:41 UTC 2011,,,,,,0|i0gbin:,93296,jbellis,jbellis,,,,,,,,,"12/Apr/11 21:23;jbellis;committed, minus the default value for RF in Stress.Session (which we want to omit for NTS)","12/Apr/11 23:33;stuhood;Not having a default will break standard stress runs unless people specify the RF flag, right?",14/Apr/11 18:38;jbellis;you're right. added a default for when RS=SimpleStrategy in r1092435,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig example script no longer working,CASSANDRA-2487,12504385,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,15/Apr/11 16:34,12/Mar/19 14:03,13/Mar/19 22:26,29/Apr/11 19:28,0.7.6,,,,,,0,hadoop,pig,,,"There is a strange error given when trying to run the example-script.pig.

java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableBytesWritable, recieved org.apache.pig.impl.io.NullableText
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:870)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:573)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:116)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:238)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:646)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:322)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)

Looks like it has to do with PIG-919 and PIG-1277.  For now we can just cast the var as a chararray and it works though.  Will attach a patch.",,,,,,,,,,,,,,,,15/Apr/11 16:36;jeromatron;2487.txt;https://issues.apache.org/jira/secure/attachment/12476461/2487.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-29 19:28:03.129,,,no_permission,,,,,,,,,,,,20649,,,Fri Apr 29 19:45:49 UTC 2011,,,,,,0|i0gbnj:,93318,brandon.williams,brandon.williams,,,,,,,,,15/Apr/11 16:36;jeromatron;Did the workaround they use in PIG-919 by casting as a chararray for now.  Also put the schema in the load.  Made the name of the keyspace and column family not like the old 0.6 stuff.  Also updated the readme a bit and included an example of setting env vars for running locally since a FAQ.,"29/Apr/11 19:28;brandon.williams;Thanks, committed.","29/Apr/11 19:45;hudson;Integrated in Cassandra-0.7 #465 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/465/])
    Update pig example script to work again.
Patch by Jeremy Hanna, reviewed by brandonwilliams for CASSANDRA-2487
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI: issue with keys being interpreted as hex and causing SET statement to fail,CASSANDRA-2497,12504586,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,18/Apr/11 19:40,12/Mar/19 14:03,13/Mar/19 22:26,20/Apr/11 16:01,0.8.0 beta 2,,,,,,0,,,,,"*Original Summary*: Issues with Update Column Family and adding a key_validation_class
_Changed summary because the issue repros on drop/create.  see comment._

*Reproduction Steps*
{code}
create column family users with comparator = UTF8Type 
and column_metadata = [{column_name: password, validation_class: UTF8Type}];

update column family users with key_validation_class=UTF8Type;

set users['jsmith']['password']='ch@ngem3';          
{code}


*EXPECTED RESULT:* After the UPDATE statement, the SET statement should go through successfully.


*ACTUAL RESULT:*  The SET statement gives the same error message, regardless of the UPDATE statement: 
{code}
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'jsmith' as hex bytes
{code}


*Output from describe keyspace*
{code}
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.29062499999999997/62/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type

{code}
","* Single Node instance on MacOSX

* Nightly Compiled Build from 4/18/2011

* Installed from: https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/apache-cassandra-2011-04-18_11-02-29-bin.tar.gz",,,,,,,,,,,,,,,18/Apr/11 22:55;xedin;CASSANDRA-2497.patch;https://issues.apache.org/jira/secure/attachment/12476663/CASSANDRA-2497.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-18 21:02:24.619,,,no_permission,,,,,,,,,,,,20656,,,Fri Jul 29 02:52:34 UTC 2011,,,,,,0|i0gbpj:,93327,jbellis,jbellis,,,,,,,,,"18/Apr/11 19:49;cdaw;This may not be related to the UPDATE.
I dropped and recreated the CF and still had the same issue.

{code}
[default@cathy] drop column family users;
72a86490-69f4-11e0-0000-242d50cf1fd4
Waiting for schema agreement...
... schemas agree across the cluster

[default@cathy] create column family users with comparator = UTF8Type and key_validation_class=UTF8Type and column_metadata = [{column_name: password, validation_class: UTF8Type}];
8a09a720-69f4-11e0-0000-242d50cf1fd4
Waiting for schema agreement...
... schemas agree across the cluster

[default@cathy] set users['jsmith']['password']='ch@ngem3';
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'jsmith' as hex bytes

{code}


*Output from describe keyspace after drop/create*
{code}
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.29062499999999997/62/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
{code}","18/Apr/11 21:02;jbellis;I think the cli is not reloading the CF metadata.  Try quitting and restarting the cli, after the recreate.

Pavel: we also want an ""assume"" for key types.","18/Apr/11 21:53;cdaw;Restarting the CLI did not fix the issue.
Adding an assume for the key type did.

{code}
[default@cathy] set users['jsmith']['password']='ch@ngem3';
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'jsmith' as hex bytes

[default@cathy] assume users keys as ascii;
Assumption for column family 'users' added successfully.

[default@cathy] set users['jsmith']['password']='ch@ngem3';
Value inserted.
{code}
","18/Apr/11 22:11;xedin;I figured out that problem is in the CLI, will attach a patch tomorrow morning! (sorry for previous misleading comment)","20/Apr/11 00:11;jbellis;Pavel, can you describe the problem + fix?","20/Apr/11 09:45;xedin;The problem was in the getKeyAsBytes method - it wasn't using information provided by cfdef.getKey_validation_class() (only comparator set by 'assume' statement or BytesType if it wasn't set). 

The fix was pretty trivial - make getKeyAsBytes use cfdef.getKey_validation_class() + printSliceList method was fixed to use getKeyComparatorForCF instead of just value from 'assume' statement.","20/Apr/11 16:01;jbellis;committed, w/ the addition of ""assert defaultValidationClass != null;""
","20/Apr/11 20:02;hudson;Integrated in Cassandra-0.8 #30 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/30/])
    ",26/Apr/11 22:24;cdaw;Retested and verified this is fixed in current build.,"10/May/11 08:24;bilal;I am getting the same issue in 0.8.0.beta2 version.

set Constructors['Ferrari']['principal'] = 'Stefano Domenicali';
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'Ferrari' as hex bytes

It worked after adding:
assume Constructors keys as Ascii;  ","10/May/11 12:47;jbellis;That is working as designed, if you don't want to use assume you need to add a key_validation_class.","10/May/11 16:51;bilal;Thanks Jonathan!

Regards,
Bilal

Sent from my iPhone


","17/Jun/11 01:07;dongfan;Thanks !

I Have been very helpful","05/Jul/11 06:52;mohctp;apache-cassandra-0.8.1

set Users['user1']['fname']='fname1';
rg.apache.cassandra.db.marshal.MarshalException: cannot parse 'fname' as hex bytes

assume Users keys as ascii;

no effect, set still gives the same error.",05/Jul/11 09:53;xedin;As you can see from your example this is not a problem with key but rather with column name. So use `assume <cf> comparator as <type>;` or re-create your CF with a valid comparator.,29/Jul/11 02:52;hollo08;Thanks !,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTables per read metric is incorrect,CASSANDRA-2422,12503468,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,05/Apr/11 22:25,12/Mar/19 14:03,13/Mar/19 22:26,05/Apr/11 22:47,0.8 beta 1,,,,,,0,,,,,"The sstables per read metric is currently recording the count of live sstables, rather than the number that are being read.",,,,,,,,,,,,,,,,05/Apr/11 22:43;stuhood;0001-Ugh.txt;https://issues.apache.org/jira/secure/attachment/12475538/0001-Ugh.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-05 22:33:02.321,,,no_permission,,,,,,,,,,,,20616,,,Wed Apr 06 00:03:16 UTC 2011,,,,,,0|i0gba7:,93258,jbellis,jbellis,,,,,,,,,"05/Apr/11 22:33;jbellis;Unless this is a trunk-only regression, let's fix for 0.7.5",05/Apr/11 22:43;stuhood;Patch for trunk: not a problem in the 0.7 branch.,"05/Apr/11 22:47;jbellis;committed, thanks!","06/Apr/11 00:03;hudson;Integrated in Cassandra #830 (See [https://hudson.apache.org/hudson/job/Cassandra/830/])
    fix sstable read count regression
patch by Stu Hood; reviewed by jbellis for CASSANDRA-2422
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
batch_mutate operations with CL=LOCAL_QUORUM throw TimeOutException when there aren't sufficient live nodes,CASSANDRA-2514,12504738,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,nar3ndra,nar3ndra,nar3ndra,20/Apr/11 00:29,12/Mar/19 14:03,13/Mar/19 22:26,20/Apr/11 18:16,0.7.5,,,,,,0,,,,,"We have a 2 DC setup with RF = 4. There are 2 nodes in each DC. Following is the keyspace definition:
<snip>
keyspaces:
    - name: KeyspaceMetadata
      replica_placement_strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
      strategy_options:
        DC1 : 2
        DC2 : 2
      replication_factor: 4
</snip>

I shutdown all except one node and waited for the live node to recognize that other nodes are dead. Following is the nodetool ring output on the live node:
Address         Status State   Load            Owns    Token                                       
                                                       169579575332184635438912517119426957796     
10.17.221.19    Down   Normal  ?               29.20%  49117425183422571410176530597442406739      
10.17.221.17    Up     Normal  81.64 KB        4.41%   56615248844645582918169246064691229930      
10.16.80.54     Down   Normal  ?               21.13%  92563519227261352488017033924602789201      
10.17.221.18    Down   Normal  ?               45.27%  169579575332184635438912517119426957796     

I expect UnavailableException when I send batch_mutate request to node that is up. However, it returned TimeOutException:
TimedOutException()
    at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:16493)
    at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:916)
    at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:890)

Following is the cassandra-topology.properties
# Cassandra Node IP=Data Center:Rack
10.17.221.17=DC1:RAC1
10.17.221.19=DC1:RAC2

10.17.221.18=DC2:RAC1
10.16.80.54=DC2:RAC2
","1. Cassandra 0.7.4 running on RHEL 5.5
2. 2 DC setup
3. RF = 4 (DC1 = 2, DC2 = 2)
4. CL = LOCAL_QUORUM",,,,,,,,,,,,,,,20/Apr/11 16:32;jbellis;2514-v2.txt;https://issues.apache.org/jira/secure/attachment/12476909/2514-v2.txt,20/Apr/11 00:41;nar3ndra;CASSANDRA-2514.patch;https://issues.apache.org/jira/secure/attachment/12476809/CASSANDRA-2514.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-20 16:32:18.163,,,no_permission,,,,,,,,,,,,20667,,,Wed Apr 20 19:46:08 UTC 2011,,,,,,0|i0gbtb:,93344,jbellis,jbellis,,,,,,,,,"20/Apr/11 00:37;nar3ndra;I think the issue is because DatacenterWriteResponseHandler.assureSufficientLiveNodes is not checking for live nodes.

DatacenterWriteResponseHandler.assureSufficientLiveNodes works on writeEndpoints. writeEndpoints contains list of the all the endpoints (may be more if there are nodes bootstrapping).

I think either writeEndpoints should ignore dead/unreachable nodes or DatacenterWriteResponseHandler.assureSufficientLiveNodes should use hintedEndpoints.keySet() as that contains the live endpoints.
I compared the implementation with WriteResponseHandler.assureSufficientLiveNodes and found that it uses hintedEndpoints.


I am attaching the patch that works for me.",20/Apr/11 00:41;nar3ndra;Use hintedEndpoints instead of writeEndpoints to work on live endpoints only.,20/Apr/11 00:52;nar3ndra;The code to reproduce this issue is a simple batch mutate operation. The operation I performed involved adding 2 columns to a SuperColumn. Let me know if it is not reproducible. I will provide the sample code.,"20/Apr/11 16:32;jbellis;Good catch, that is a bug.

v2 adds a couple improvements:

- only count the hinted endpoint towards live count if it's a normal write destination (hints can be sent elsewhere if all the write destinations are dead)
- similar fix for DSWRH (EACH_QUORUM)
- unrelated fix in WRH for CL.ANY not to continue through to the CL.Q/ALL code",20/Apr/11 16:32;jbellis;how does that look to you?,"20/Apr/11 17:22;nar3ndra;Looks good to me. 

Just one comment/question:
hintedEndpoints is subset of writeEndpoints. So is the additional check writeEndpoints.contains(destination), while we are iterating over hintedEndpoints, needed? I think assert would be better here.

","20/Apr/11 17:32;jbellis;That's the point, hintedEndpoints is *usually* but not always a subset of writeEndpoints. Here is the code from getHintedEndpoints:

{code}
        // assign dead endpoints to be hinted to the closest live one, or to the local node
        // (since it is trivially the closest) if none are alive.  This way, the cost of doing
        // a hint is only adding the hint header, rather than doing a full extra write, if any
        // destination nodes are alive.
        //
        // we do a 2nd pass on targets instead of using temporary storage,
        // to optimize for the common case (everything was alive).
        InetAddress localAddress = FBUtilities.getLocalAddress();
        for (InetAddress ep : targets)
        {
            if (map.containsKey(ep))
                continue;
            if (!StorageProxy.shouldHint(ep))
            {
                if (logger.isDebugEnabled())
                    logger.debug(""not hinting "" + ep + "" which has been down "" + Gossiper.instance.getEndpointDowntime(ep) + ""ms"");
                continue;
            }

            InetAddress destination = map.isEmpty()
                                    ? localAddress
                                    : snitch.getSortedListByProximity(localAddress, map.keySet()).get(0);
            map.put(destination, ep);
        }
{code}","20/Apr/11 17:33;jbellis;that is: our last-resort local hint storage may not be part of writeEndpoints (probably won't be, on a large cluster).","20/Apr/11 17:53;nar3ndra;Got it. In my setup I had HH disabled. So I overlooked the rest of the getHintedEndpoints.

The change looks good to me now. Thanks!

","20/Apr/11 18:16;jbellis;committed, thanks!","20/Apr/11 19:46;hudson;Integrated in Cassandra-0.7 #451 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/451/])
    fixes for verifying destinationavailability under hinted conditions
patch by Narendra Sharma and jbellis for CASSANDRA-2514
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh errors on comments that end with a semicolon,CASSANDRA-2488,12504418,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,angryparsley,angryparsley,15/Apr/11 22:25,12/Mar/19 14:03,13/Mar/19 22:26,19/Apr/11 17:54,0.8 beta 1,,,Legacy/Tools,,,0,cql,,,,"Commented-out lines that end in a semicolon cause an error.

Examples:

cqlsh> -- CREATE KEYSPACE ELE WITH replication_factor = 3 AND strategy_class = SimpleStrategy AND strategy_options:replication_factor=3;
Bad Request: line 0:-1 no viable alternative at input '<EOF>'
cqlsh> -- CREATE KEYSPACE ELE WITH replication_factor = 3 AND strategy_class = SimpleStrategy AND strategy_options:replication_factor=3
   ... 
   ... 
   ... ;
Bad Request: line 2:0 no viable alternative at input ';'
cqlsh> -- ;
Bad Request: line 0:-1 no viable alternative at input '<EOF>'
cqlsh> --;
Bad Request: line 0:-1 no viable alternative at input '<EOF>'

As long as there's a line with valid CQL before the semicolon, things work fine though.

I'm pretty sure the problem is on line 75 of cqlsh:
        if not line.endswith("";""):
            self.set_prompt(Shell.continue_prompt)
            return None

A quick workaround would be to kill the pretty continue prompt. A more involved fix would detect whether or not the semicolon was in a comment. This is harder than it sounds, since /* and */ allow multi-line comments.","OS X 10.6.7

$ java -version
java version ""1.6.0_24""
Java(TM) SE Runtime Environment (build 1.6.0_24-b07-334-10M3326)
Java HotSpot(TM) 64-Bit Server VM (build 19.1-b02-334, mixed mode)

(This stuff isn't really important. It's a bug in a Python script)",,,,,,,,,,,,,,,17/Apr/11 02:28;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2488-teach-cqlsh-to-ignore-comments.txt;https://issues.apache.org/jira/secure/attachment/12476545/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2488-teach-cqlsh-to-ignore-comments.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-17 02:19:56.77,,,no_permission,,,,,,,,,,,,20650,,,Tue Apr 19 19:05:56 UTC 2011,,,,,,0|i0gbnr:,93319,gdusbabek,gdusbabek,,,,,,,,,17/Apr/11 02:19;urandom;the attached patch should deal with any supported comment,19/Apr/11 17:41;gdusbabek;+1,19/Apr/11 17:54;urandom;committed,"19/Apr/11 19:05;hudson;Integrated in Cassandra-0.8 #22 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/22/])
    teach cqlsh to ignore comments

Patch by eevans; reviewed by gdusbabek for CASSANDRA-2488
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL does not preserve column order in select statement,CASSANDRA-2493,12504467,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,17/Apr/11 03:36,12/Mar/19 14:03,13/Mar/19 22:26,17/Apr/11 05:10,0.8 beta 1,,,Legacy/CQL,,,0,,,,,,,,,,,,,,,,,,,,,17/Apr/11 04:44;jbellis;2493.txt;https://issues.apache.org/jira/secure/attachment/12476548/2493.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-17 04:55:52.215,,,no_permission,,,,,,,,,,,,20653,,,Sun Apr 17 06:24:46 UTC 2011,,,,,,0|i0gbov:,93324,urandom,urandom,,,,,,,,,17/Apr/11 03:48;jbellis;patch to preserve column order.  columns that do not exist in a given row come back as null; this requires changing value and ts fields in thrift Column struct to optional. manual checking for set-ness is added to ThriftValidation to make up for this.,17/Apr/11 03:53;jbellis;updated patch fixes uses of Column constructor that no longer exists,17/Apr/11 04:44;jbellis;new patch also fixes bugs in CassandraResultSet & updates jdbc tests for new behavior,17/Apr/11 04:55;urandom;lgtm. +1,17/Apr/11 05:10;jbellis;committed,"17/Apr/11 06:24;hudson;Integrated in Cassandra-0.8 #12 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/12/])
    preserve column order in CQL result sets
patch by jbellis; reviewed by eevans for CASSANDRA-2493
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-env.sh pattern matching for OpenJDK broken in some cases,CASSANDRA-2499,12504595,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cywjackson,thobbs,thobbs,18/Apr/11 20:37,12/Mar/19 14:03,13/Mar/19 22:26,20/Apr/11 19:16,0.8.0 beta 2,,,Packaging,,,0,,,,,"With bash version 4.1.5, the section of cassandra-env that tries to match the JDK distribution seems to have some kind of syntax error.  I get the following message when running bin/cassandra:

{noformat}
bin/../conf/cassandra-env.sh: 99: [[: not found
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-04-20 01:11:02.181,,,no_permission,,,,,,,,,,,,20657,,,Wed Apr 20 20:02:24 UTC 2011,,,,,,0|i0gbpz:,93329,thobbs,thobbs,,,,,,,,,"20/Apr/11 01:11;cywjackson;chances are the default /bin/sh is linked to dash, please run a ls -al /bin/sh to confirm

a minor fix could be remove the [[ and add """" around the variables and values.

-if [[ $java_version != \*OpenJDK\* ]]
+if [ ""$java_version"" != ""\*OpenJDK\*"" ]

a more drastic fix is to update all the /bin/sh with /bin/bash (if desired to only support on bash)

see ref: https://wiki.ubuntu.com/DashAsBinSh

suggest to be reviewed to make a decision.","20/Apr/11 16:27;thobbs;/bin/sh was a link to dash, and the current script seems to work fine with bash.
{noformat}
[ ""$java_version"" != ""*OpenJDK*"" ]
{noformat} works for me in both dash and bash.",20/Apr/11 16:56;thobbs;Correction: the suggested replacement does *not* seem to detect OpenJDK.,"20/Apr/11 17:46;cywjackson;try this:

-java_version=`java -version 2>&1`
-if [[ $java_version != *OpenJDK* ]]
+check_openjdk=$(java -version 2>&1 | awk '{if (NR == 2) {print $1}}')
+if [ ""$check_openjdk"" != ""OpenJDK"" ]

{noformat}$ bash /tmp/testjdk java
version: Java(TM)
not OpenJDK
$ dash /tmp/testjdk /usr/bin/java
version: OpenJDK
yes OpenJDK
$ cat /tmp/testjdk
check_openjdk=$($1 -version 2>&1 | awk '{if (NR ==2) {print $1}}')
echo ""version: $check_openjdk""
if [ ""$check_openjdk"" != ""OpenJDK"" ]
then
    echo ""not OpenJDK""
    JVM_OPTS=""$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/jamm-0.2.1.jar""
else
    echo ""yes OpenJDK""
fi
{noformat}",20/Apr/11 19:16;jbellis;committed,"20/Apr/11 20:02;hudson;Integrated in Cassandra-0.8 #30 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/30/])
    fix jdk verison check for sh/dash
patch by Jackson Chung; reviewed by thobbs and jbellis for CASSANDRA-2499
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
default gc log settings overwrite previous log,CASSANDRA-2418,12503444,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,05/Apr/11 20:16,12/Mar/19 14:03,13/Mar/19 22:26,07/Apr/11 15:54,0.7.5,,,,,,0,,,,,"For those spoiled by nice rolling and appending syslogs log4js etc the JVM gc log can be jarring:

{noformat} 
# GC logging options -- uncomment to enable
# JVM_OPTS=""$JVM_OPTS -XX:+PrintGCDetails""
# JVM_OPTS=""$JVM_OPTS -XX:+PrintGCTimeStamps""
# JVM_OPTS=""$JVM_OPTS -XX:+PrintClassHistogram""
# JVM_OPTS=""$JVM_OPTS -XX:+PrintTenuringDistribution""
# JVM_OPTS=""$JVM_OPTS -XX:+PrintGCApplicationStoppedTime""
# JVM_OPTS=""$JVM_OPTS -Xloggc:/var/log/cassandra/gc.log""
{noformat} 

Will result in gc.log with days of data being overwritten on restart, which leads to sad faces.

The simplest change would be along these lines:
{noformat} 
GC_LOG_TS=`date +%s`
JVM_OPTS=""$JVM_OPTS -XX:+PrintGCDetails""
JVM_OPTS=""$JVM_OPTS -XX:+PrintGCTimeStamps""
JVM_OPTS=""$JVM_OPTS -XX:+PrintClassHistogram""
JVM_OPTS=""$JVM_OPTS -XX:+PrintTenuringDistribution""
JVM_OPTS=""$JVM_OPTS -XX:+PrintGCApplicationStoppedTime""
JVM_OPTS=""$JVM_OPTS -Xloggc:/var/log/cassandra/gc-$GC_LOG_TS.log""
{noformat} 

There are probably prettier approaches.",,,,,,,,,,,,,,,,05/Apr/11 22:43;jbellis;2418.txt;https://issues.apache.org/jira/secure/attachment/12475537/2418.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-05 22:43:12.979,,,no_permission,,,,,,,,,,,,20612,,,Thu Apr 07 16:30:06 UTC 2011,,,,,,0|i0gb9b:,93254,jbellis,jbellis,,,,,,,,,05/Apr/11 22:43;jbellis;patch based on Chris's suggestion,07/Apr/11 15:44;cburroughs;Patch looks good to me.,07/Apr/11 15:54;jbellis;committed,"07/Apr/11 16:30;hudson;Integrated in Cassandra-0.7 #424 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/424/])
    add date in seconds-since-epoch to default gc log filename
patch by Chris Burroughs and jbellis for CASSANDRA-2418
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL: Errors when running unqualified ""select column"" statement (no where clause)",CASSANDRA-2593,12505999,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,03/May/11 00:49,12/Mar/19 14:03,13/Mar/19 22:26,03/May/11 17:46,0.8.0,,,Legacy/CQL,,,0,cql,,,,"*Seed Data*
{code}
CREATE KEYSPACE cqldb with strategy_class = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=2;

USE cqldb;

CREATE COLUMNFAMILY users (KEY varchar PRIMARY KEY, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint);


INSERT INTO users (KEY, password) VALUES ('user0', 'ch@ngem3');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3a', 'f', 'TX', '1968');
INSERT INTO users (KEY, password) VALUES ('user2', 'ch@ngem3b');
INSERT INTO users (KEY, password) VALUES ('user3', 'ch@ngem3c');
{code}

*Query #1 - select varchar column*
{code}
cqlsh> select state from users;
u'user1' | u'state',u'TX'
Exception: 'NoneType' object has no attribute 'decode'

cqlsh> select state from users where KEY='user1';
u'user1' | u'state',u'TX'
{code}

*Query #2 - select bigint column*
{code}
cqlsh> select birth_year from users;
Exception: unpack requires a string argument of length 8

cqlsh> select birth_year from users where KEY='user1';
u'user1' | u'birth_year',1968
{code}

*A simple 'SELECT *' with no WHERE clause works fine*
{code}
cqlsh> select * from users;
u'user1' | u'birth_year',1968 | u'gender',u'f' | u'password',u'ch@ngem3a' | u'state',u'TX'
u'user0' | u'password',u'ch@ngem3'
u'user3' | u'password',u'ch@ngem3c'
u'user2' | u'password',u'ch@ngem3b'
{code}
",,,,,,,,,,,,,,,,03/May/11 16:58;xedin;CASSANDRA-2593-v2.patch;https://issues.apache.org/jira/secure/attachment/12478067/CASSANDRA-2593-v2.patch,03/May/11 15:35;xedin;CASSANDRA-2593.patch;https://issues.apache.org/jira/secure/attachment/12478060/CASSANDRA-2593.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-03 15:35:04.089,,,no_permission,,,,,,,,,,,,20718,,,Tue May 03 20:30:00 UTC 2011,,,,,,0|i0gcan:,93422,jbellis,jbellis,,,,,,,,,03/May/11 15:35;xedin;branch: cassandra-0.8 (can be applied to trunk also). In the cql/decoders.py we skip a column with an empty value instead of trying to decode it. ,"03/May/11 15:43;jbellis;{code}
+                if len(row) < 2:
+                    continue # if there are no columns, we skip
{code}

This looks unrelated to the empty column fix.  (It sort of looks like an attempt at addressing CASSANDRA-2548.)",03/May/11 15:47;xedin;If there are no columns to show why should we show an empty key?,"03/May/11 16:00;jbellis;Short answer, same reason Cathy linked over there: http://wiki.apache.org/cassandra/FAQ#range_ghosts.

Longer answer, I plan to address on 2548.  For now let's fix the empty column bug.

Can you add a test_cql.py test that catches the problem, to prevent a regression?","03/May/11 16:10;xedin;The thing is that when you do ""select name from users"" and only few of the rows have a ""name"" column, cqlsh will output all other keys as empty. If you want I can remove check from the CQLSH","03/May/11 16:20;jbellis;bq. The thing is that when you do ""select name from users"" and only few of the rows have a ""name"" column, cqlsh will output all other keys as empty.

Right. (And this part, at least, *is* intuitive to someone coming from SQL: if I have a null column, I still want to see the row included.)",03/May/11 16:58;xedin;removed check from CQLSH and added a test.,"03/May/11 17:46;jbellis;committed, thanks!","03/May/11 19:19;hudson;Integrated in Cassandra-0.8 #62 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/62/])
    fix returning null column values in the python cql driver
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2593
","03/May/11 20:30;cdaw;The patch works as expected:

{code}
cqlsh> select state from users;
u'user1' | u'state',u'TX'
u'user0'
u'user3'
u'user2'

cqlsh> select birth_year from users;
u'user1' | u'birth_year',1968
u'user0'
u'user3'
u'user2'
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incremental_backups and snapshot_before_compaction duplicate hard links,CASSANDRA-2598,12506094,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,michaelsembwever,michaelsembwever,03/May/11 20:19,12/Mar/19 14:03,13/Mar/19 22:26,05/May/11 15:42,0.8.0,,,,,,0,,,,,"See discussion @ http://thread.gmane.org/gmane.comp.db.cassandra.user/15933/

Enabling both incremental_backups and snapshot_before_compaction leads to the same hard links trying to be created.

This gives stacktraces like 

java.io.IOError: java.io.IOException: Unable to create hard link from
/cassandra-data/<keyspace>/<cf>-f-3875-Data.db
to
/cassandra-data/<keyspace>/snapshots/compact-<cf>/<cf>-f-3875-Data.db
(errno 17)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1629)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1654)
	at org.apache.cassandra.db.Table.snapshot(Table.java:198)
	at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:504)
	at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:146)
	at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:112)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to create hard link from
/cassandra-data/<keyspace>/<cf>-f-3875-Data.db
to
/cassandra-data/<keyspace>/snapshots/compact-<cf>/<cf>-f-3875-Data.db
(errno 17)
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:155)
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:713)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1622)
	... 10 more
",linux & jna,,,,,,,,,,,,,,,03/May/11 20:40;jbellis;2598.txt;https://issues.apache.org/jira/secure/attachment/12478091/2598.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-03 20:39:50.941,,,no_permission,,,,,,,,,,,,20722,,,Tue May 08 21:22:09 UTC 2012,,,,,,0|i0gcbr:,93427,michaelsembwever,michaelsembwever,,,,,,,,,"03/May/11 20:39;jbellis;It looks like I was wrong -- this isn't a problem w/ incremental_backups, but a regression in snapshot_before_compaction from CASSANDRA-1791.",03/May/11 20:40;jbellis;Patch to make snapshot_before_compaction directory names unique again,05/May/11 14:08;michaelsembwever;the patch fixes the problem,"05/May/11 15:42;jbellis;committed, thanks!","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ","08/May/12 11:08;edevil;I think I have a similar issue but I don't have incremental_backups or snapshot_before_compaction enabled, and I'm using 1.1.

Since I upgraded to Cassandra 1.1, I get the following error when trying to delete a CF. After this happens the CF is not accessible anymore, but I cannot create another one with the same name until I restart the server.

INFO [MigrationStage:1] 2012-05-07 18:10:12,682 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columnfamilies@1128094887(978/1222 serialized/live bytes, 21 ops)
INFO [FlushWriter:2] 2012-05-07 18:10:12,682 Memtable.java (line 266) Writing Memtable-schema_columnfamilies@1128094887(978/1222 serialized/live bytes, 21 ops)
INFO [FlushWriter:2] 2012-05-07 18:10:12,720 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-28-Data.db (1041 bytes)
INFO [MigrationStage:1] 2012-05-07 18:10:12,721 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columns@1599271050(392/490 serialized/live bytes, 8 ops)
INFO [FlushWriter:2] 2012-05-07 18:10:12,722 Memtable.java (line 266) Writing Memtable-schema_columns@1599271050(392/490 serialized/live bytes, 8 ops)
INFO [CompactionExecutor:8] 2012-05-07 18:10:12,722 CompactionTask.java (line 114) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-26-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-28-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfam
ilies-hc-27-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-25-Data.db')]
INFO [FlushWriter:2] 2012-05-07 18:10:12,806 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-23-Data.db (447 bytes)
INFO [CompactionExecutor:8] 2012-05-07 18:10:12,811 CompactionTask.java (line 225) Compacted to [/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-29-Data.db,].  24,797 to 21,431
(~86% of original) bytes for 2 keys at 0.232252MB/s.  Time: 88ms.
ERROR [MigrationStage:1] 2012-05-07 18:10:12,895 CLibrary.java (line 158) Unable to create hard link
com.sun.jna.LastErrorException: errno was 17
at org.apache.cassandra.utils.CLibrary.link(Native Method)
at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:150)
at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
ERROR [Thrift:17] 2012-05-07 18:10:12,898 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/
Disco/Client/snapshots/1336410612893-Client/Client.json (errno 17)
at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:372)
at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:191)
at org.apache.cassandra.service.MigrationManager.announceColumnFamilyDrop(MigrationManager.java:182)
at org.apache.cassandra.thrift.CassandraServer.system_drop_column_family(CassandraServer.java:948)
at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.getResult(Cassandra.java:3348)
at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.getResult(Cassandra.java:3336)
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336410612893-Client/Client.json (errno 17)
at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
at java.util.concurrent.FutureTask.get(FutureTask.java:83)
at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:368)
... 11 more
Caused by: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336410612893-Client/Client.json (errno 17)
at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1454)
at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
... 3 more
Caused by: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336410612893-Client/Client.json (errno 17)
at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:163)
at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
... 10 more
ERROR [MigrationStage:1] 2012-05-07 18:10:12,899 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336410612893-Client/Client.json (errno 17)
at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1454)
at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336410612893-Client/Client.json (errno 17)
at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:163)
at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
... 10 more


I've tried recreating the data dirs, to see if this was some kind of permissions problem, but the error happens every time and with any CF. I'm using the Cassandra Debian package on Debian squeeze and the Sun JVM (build 1.6.0_26-b03).",08/May/12 21:22;jbellis;Please create a new issue with steps to reproduce.  Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: Batch Updates: some consistency levels not working,CASSANDRA-2566,12505240,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,26/Apr/11 20:00,12/Mar/19 14:03,13/Mar/19 22:26,16/May/11 22:01,0.8.0,,,Legacy/CQL,,,0,cql,,,,"Testing the batch updates, and running into some issues with different consistency levels

+*Summary*+
* UNTESTED: CONSISTENCY ANY
* PASS: CONSISTENCY  ONE
* PASS: CONSISTENCY  QUORUM
* PASS: CONSISTENCY  ALL
* CQL ERROR: CONSISTENCY  LOCAL_QUORUM
* CQL ERROR: CONSISTENCY  EACH_QUORUM

 
+*Test Setup*+
{code}
CREATE KEYSPACE cqldb with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy'  
and strategy_options:replication_factor=1;

use cqldb;

CREATE COLUMNFAMILY users (KEY varchar PRIMARY KEY, password varchar, gender varchar, 
session_token varchar, state varchar, birth_year bigint);

INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');
{code}


+*Bug Details*+

*CONSISTENCY LOCAL_QUORUM*
{code}
BEGIN BATCH USING CONSISTENCY  LOCAL_QUORUM
UPDATE users SET state = 'UT' WHERE KEY = 'user1';
UPDATE users SET state = 'UT' WHERE KEY = 'user2';
UPDATE users SET state = 'UT' WHERE KEY = 'user3';
APPLY BATCH

cqlsh>  Bad Request: line 1:31 mismatched input 'LOCAL_QUORUM' expecting K_LEVEL
{code}

*CONSISTENCY EACH_QUORUM*
{code}
BEGIN BATCH USING CONSISTENCY  EACH_QUORUM
UPDATE users SET state = 'TX' WHERE KEY = 'user1';
UPDATE users SET state = 'TX' WHERE KEY = 'user2';
UPDATE users SET state = 'TX' WHERE KEY = 'user3';
APPLY BATCH

cqlsh> Bad Request: line 1:31 mismatched input 'EACH_QUORUM' expecting K_LEVEL
{code}
",,,,,,,,,,,,,,,,16/May/11 21:56;xedin;CASSANDRA-2566.patch;https://issues.apache.org/jira/secure/attachment/12479391/CASSANDRA-2566.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-16 20:58:00.208,,,no_permission,,,,,,,,,,,,20700,,,Mon May 16 22:30:27 UTC 2011,,,,,,0|i0gc4n:,93395,jbellis,jbellis,,,,,,,,,"16/May/11 20:58;jbellis;Updated CQL CL documentation and edited issue description.
",16/May/11 21:56;xedin;Added missing consistency levels to CQL grammar.,16/May/11 22:01;jbellis;committed (also removed obsolete DCQUORUM* values),"16/May/11 22:30;hudson;Integrated in Cassandra-0.8 #111 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/111/])
    update cql consistency levels
patch by pyaskevich; reviewed by jbellis for CASSANDRA-2566
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache-cassandra-cql-*.jar as a separate artifact,CASSANDRA-2579,12505738,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,28/Apr/11 23:52,12/Mar/19 14:03,13/Mar/19 22:26,06/May/11 15:14,0.8.0 beta 2,,,Legacy/Tools,Packaging,,0,cql,,,,"The CQL jar should not be stored in the -bin.tar.gz artifact, but created separately with its own checksum files.",,,,,,,,,,,,,,,,28/Apr/11 23:53;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2579-apache-cassandra-cql-.jar-as-a-separate.txt;https://issues.apache.org/jira/secure/attachment/12477714/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2579-apache-cassandra-cql-.jar-as-a-separate.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-02 15:23:30.817,,,no_permission,,,,,,,,,,,,19342,,,Fri May 06 15:14:50 UTC 2011,,,,,,0|i0gc7j:,93408,tjake,tjake,,,,,,,,,02/May/11 15:23;tjake;WFM +1,"02/May/11 19:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ",06/May/11 15:14;jbellis;(committed 2011-05-02),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema disagreements when using connections to multiple hosts,CASSANDRA-2536,12504930,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,21/Apr/11 22:55,12/Mar/19 14:03,13/Mar/19 22:26,29/Apr/11 15:38,0.7.6,0.8.0 beta 2,,,,,1,,,,,"If you have two thrift connections open to different nodes and you create a KS using the first, then a CF in that KS using the second, you wind up with a schema disagreement even if you wait/sleep after creating the KS.

The attached script reproduces the issue using pycassa (1.0.6 should work fine, although it has the 0.7 thrift-gen code).  It's also reproducible by hand with two cassandra-cli sessions.",Two node 0.8-beta1 cluster with one seed and JNA.,,,,,,,,,,,,,,,28/Apr/11 22:26;thobbs;2536-compare-timestamp.txt;https://issues.apache.org/jira/secure/attachment/12477706/2536-compare-timestamp.txt,21/Apr/11 22:56;thobbs;schema_disagree.py;https://issues.apache.org/jira/secure/attachment/12477050/schema_disagree.py,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-21 23:20:47.356,,,no_permission,,,,,,,,,,,,20682,,,Fri Apr 29 15:56:06 UTC 2011,,,,,,0|i0gbxz:,93365,jbellis,jbellis,,,,,,,,,"21/Apr/11 23:20;mbulman;I feel like a better, more critical sounding explanation, is:  create a keyspace on node1, create a cf in that keyspace on node2 = hang + schema disagreement.",22/Apr/11 01:37;jbellis;And this is fine in 0.7?,"22/Apr/11 16:03;thobbs;Actually, I can also reproduce this with a two node 0.7.4 cluster.  I'm pretty sure that this does not happen with 0.7.3, but I'll go ahead and verify that.",22/Apr/11 16:20;thobbs;Nevermind my thoughts that it doesn't happen in 0.7.3 -- it seems to happen there too.  It appears this is not a recent problem.,25/Apr/11 14:09;1eightt;Just bumped into this on a fresh 0.7.4 install on our test cluster. Does this only happen in a 2 node ring?,"27/Apr/11 18:38;ceocoder;encountered this on fresh 0.7.4 - 5 nodes - 100G+ per node. 

decommissioning the bad node and rejoin fixed the problem.","28/Apr/11 02:10;jbellis;Gary, any thoughts on where to start looking?","28/Apr/11 12:42;gdusbabek;bq. any thoughts...
I was going to add some jmx to get the last N schema versions (seems like it would be handy anyway and will be necessary if we ever get the rollback pony). Send schema to node A, verify that schema is propagated to B, send schema to B and watch the problem happen.  The code to start looking at are the Definitions*VerbHandlers.

Schema version is tracked in two places: gossip and in DatabaseDescriptor.defsVersion.  Make sure those are reasonably in sync (was the sourced of one bug in the past). ","28/Apr/11 20:58;thobbs;The issue is the clocks being out of sync between nodes.  Sometimes the v1 UUID generated by the second node has an earlier timestamp than the current schema UUID has.

There are a couple of things that could be fixed here:

1. A node shouldn't accept a schema change if the timestamp for the new schema would be earlier than its current schema.
2. Schema modification calls should accept an optional client-side timestamp that will be used for the v1 UUID.","28/Apr/11 21:08;gdusbabek;bq. Sometimes the v1 UUID generated by the second node has an earlier timestamp than the current schema UUID has.
Wouldn't that update be DOA then?  I thought we checked to make sure the new migration compared after the current migration (as well as making sure the new migration's previous version matches with the current version).

bq. A node shouldn't accept a schema change if the timestamp for the new schema would be earlier than its current schema.
If the clocks are *that* far off sync, I think the cluster has bigger problems (like writes not being applied). Plus, it would be easy for a node whose clock is way head to 'poison' schema updates from the rest of the cluster who are, in effect, behind the times.

bq. Schema modification calls should accept an optional client-side timestamp that will be used for the v1 UUID.
Seems like a better approach.

","28/Apr/11 21:21;jbellis;bq. A node shouldn't accept a schema change if the timestamp for the new schema would be earlier than its current schema.

You need this with or without the client-side timestamp, though; there's no sense in letting people blow their leg off.

And once you have that you don't need to add a client-side timestamp with all the PITA-ness that involves.

(And unlike with data modification, I can't think of a use for doing ""clever"" things w/ a client side timesamp.  So pushing it to the client doesn't really solve anything, just means you need to sync clocks across more machines.)","28/Apr/11 21:33;thobbs;{quote}
Wouldn't that update be DOA then? I thought we checked to make sure the new migration compared after the current migration (as well as making sure the new migration's previous version matches with the current version).
{quote}
We do check that the previous version matches, but the migration is applied locally without comparing the current and new uuids.

{quote}
If the clocks are that far off sync, I think the cluster has bigger problems (like writes not being applied).
{quote}
This can theoretically happen with clocks being off by only tens of milliseconds.

{quote}
And unlike with data modification, I can't think of a use for doing ""clever"" things w/ a client side timesamp. So pushing it to the client doesn't really solve anything, just means you need to sync clocks across more machines.
{quote}
Not for clever purposes -- it seems to me that clients making schema modifications are more likely to be centralized, so schema changes coming from a single client will (almost) always have increasing timestamps.",28/Apr/11 22:26;thobbs;Attached patch compares version timestamps before applying migration locally.,28/Apr/11 23:08;thobbs;I personally think the timestamp comparison is good enough for now.  Any interest in opening a new ticket for client-side timestamps?,"29/Apr/11 01:11;jbellis;bq. it seems to me that clients making schema modifications are more likely to be centralized

I would have also argued that they are likely to use the same connection (to the same server), and look where that got us. :)

bq. I personally think the timestamp comparison is good enough for now

I am okay with this. What do you think, Gary?

(Nit: the exception message says ""older"" but the comparison is ""older or equal."")","29/Apr/11 07:38;slebresne;I'll hijack this conversation by saying that I think we should start advertising that people should try to keep their server clocks in sync unless they have a good reason not too (which would legitimize the fact that ""timestamp comparison is good enough""). Counter removes for instance use server side timestamps and would be screwed up by diverging clocks (and by that I mean more screwed up than they already are by design). And really, is there any reason not to install a ntpd server in the first place anyway?",29/Apr/11 12:35;gdusbabek;I think timestamp comparisons will be fine.,"29/Apr/11 15:38;jbellis;committed, thanks!","29/Apr/11 15:56;hudson;Integrated in Cassandra-0.7 #462 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/462/])
    refuse to apply migrations with older timestamps than the current schema
patch by Tyler Hobbs; reviewed by jbellis and gdusbabek for CASSANDRA-2536
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: NPE running SELECT with an IN clause,CASSANDRA-2538,12504935,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,cdaw,cdaw,21/Apr/11 23:41,12/Mar/19 14:03,13/Mar/19 22:26,26/Apr/11 22:12,0.8.0 beta 2,,,,,,0,cql,,,,"*Test Case to Run*
{noformat}
cqlsh> select * from users where key in ('user2', 'user3');
Internal application error
{noformat}


*Test Setup*
{noformat}
CREATE COLUMNFAMILY users (
  KEY varchar PRIMARY KEY,
  password varchar);

INSERT INTO users (KEY, password) VALUES ('user1', 'ch@ngem3a');
{noformat}


*Log Files*
{noformat}
ERROR [RequestResponseStage:17] 2011-04-21 23:36:41,600 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[RequestResponseStage:17,5,main]
java.lang.AssertionError
	at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR [RequestResponseStage:17] 2011-04-21 23:36:41,600 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[RequestResponseStage:17,5,main]
java.lang.AssertionError
	at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR [pool-2-thread-5] 2011-04-21 23:37:12,026 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.NullPointerException
	at org.apache.cassandra.cql.WhereClause.and(WhereClause.java:59)
	at org.apache.cassandra.cql.WhereClause.<init>(WhereClause.java:44)
	at org.apache.cassandra.cql.CqlParser.whereClause(CqlParser.java:816)
	at org.apache.cassandra.cql.CqlParser.selectStatement(CqlParser.java:502)
	at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:191)
	at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:834)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:463)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1134)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{noformat}",,,,,,,,,,,,,,,,22/Apr/11 02:33;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2538-NPE-running-SELECT-with-an-IN-clause.txt;https://issues.apache.org/jira/secure/attachment/12477069/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2538-NPE-running-SELECT-with-an-IN-clause.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-22 01:42:21.754,,,no_permission,,,,,,,,,,,,20683,,,Tue Apr 26 22:56:43 UTC 2011,,,,,,0|i0gbyf:,93367,cdaw,cdaw,,,,,,,,,"22/Apr/11 01:42;jbellis;Eric, can you make it so CQL is more clear about not supporting IN, without internal errors?","22/Apr/11 02:36;urandom;This should result in a syntax error like any other mis-statement, but the NPE was beating it to the punch.  Patch attached.","22/Apr/11 22:20;cdaw;This is just a consistency thing, but the documentation is misleading.

The DELETE documentation shows you can use an 'IN' clause:
DELETE [COLUMNS] FROM <COLUMN FAMILY> [USING <CONSISTENCY>] WHERE KEY IN (keyname1, keyname2);

The example for DELETE uses UPDATE, so I assumed 'IN' would work for DELETE/UPDATE/SELECT.
UPDATE ... WHERE KEY IN (keyname1, keyname2)
","23/Apr/11 00:17;jbellis;Totally didn't know we support IN for DELETE. :)

Would prefer to fix by offering IN for for the other statements as well. The Antlr doesn't look too daunting w/ DELETE as an example (knock on wood). Can take a stab at that tomorrow.","23/Apr/11 03:12;urandom;{quote}
Totally didn't know we support IN for DELETE. 

Would prefer to fix by offering IN for for the other statements as well. The Antlr doesn't look too daunting w/ DELETE as an example (knock on wood). Can take a stab at that tomorrow.
{quote}

This is {{multiget_slice()}} / {{multiget_count()}} and was intentionally omitted (remember?).  Not saying this is decision that can't be revisited, just reminding.

Oh, and if we do move forward, it's post-0.8, we're in a feature-freeze. :)","25/Apr/11 13:43;jbellis;Okay, created CASSANDRA-2553 for adding IN support to 0.8.1.

+1 on the NPE fix.","26/Apr/11 21:46;cdaw;The patch generates this message:
{code}
cqlsh> select * from users where KEY in ('cathy', 'ed');
Bad Request: line 1:30 mismatched input 'in' expecting set null
{code}
","26/Apr/11 22:10;urandom;This says that it's a bad request, a problem at line 1, column 30, where it encountered mismatched input in the form of the word IN.  This is because `KEY IN ...' is not supported.",26/Apr/11 22:12;urandom;committed.,26/Apr/11 22:17;cdaw;Sorry if I wasn't clear. The patch passed in testing because it no longer generated an NPE and gave an ERROR message. I included the message just for info.,"26/Apr/11 22:56;hudson;Integrated in Cassandra-0.8 #43 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/43/])
    NPE running SELECT with an IN clause

Patch by eevans; reviewed by jbellis for CASSANDRA-2538
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Both name an index iterators cast block offset to int,CASSANDRA-2376,12502254,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,24/Mar/11 04:55,12/Mar/19 14:03,13/Mar/19 22:26,28/Mar/11 16:15,0.7.5,,,,,,0,,,,,This means that performing random access to the end of a large row will not work.,,,,,,,,,,,,,,,,27/Mar/11 05:22;jbellis;2376-v2.txt;https://issues.apache.org/jira/secure/attachment/12474720/2376-v2.txt,24/Mar/11 04:56;jbellis;2376.txt;https://issues.apache.org/jira/secure/attachment/12474468/2376.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-24 13:36:08.062,,,no_permission,,,,,,,,,,,,20589,,,Mon Mar 28 18:16:39 UTC 2011,,,,,,0|i0gb0v:,93216,slebresne,slebresne,,,,,,,,,24/Mar/11 04:56;jbellis;Patch also unifies skipBytes checking into skipBytesFully even where we really are only skipping an int's worth of bytes.,"24/Mar/11 13:36;slebresne;Small nitpick: it could be nice to add a message to the eof exception saying how many bytes out of how many have been skipped before reaching eof, but that'd require to do a catch and re-throw in the 'long' version.

+1",27/Mar/11 05:22;jbellis;v2 adds length information to EOFException,28/Mar/11 16:15;slebresne;Committed as rev1086290.,"28/Mar/11 18:16;hudson;Integrated in Cassandra-0.7 #410 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/410/])
    Don't cast block offsets to int
patch by jbellis; reviewed by slebresne for CASSANDRA-2376
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BRAF.sync() bug can cause massive commit log write magnification,CASSANDRA-2660,12507480,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,17/May/11 12:22,12/Mar/19 14:03,13/Mar/19 22:26,17/May/11 14:55,0.7.7,0.8.1,,,,,0,,,,,"This was discovered, fixed and tested on 0.7.5. Cursory examination shows it should still be an issue on trunk/0.8. If people otherwise agree with the patch I can rebase if necessary.

Problem:

BRAF.flush() is actually broken in the sense that it cannot be called without close co-operation with the caller. rebuffer() does the co-op by adjusting bufferOffset and validateBufferBytes appropriately, by sync() doesn't. This means sync() is broken, and sync() is used by the commit log.

The attached patch moves the bufferOffset/validateBufferBytes handling out into resetBuffer() and has both flush() and rebuffer() call that. This makes sync() safe.

What happened was that for batched commit log mode, every time sync() was called the data buffered so far would get written to the OS and fsync():ed. But until rebuffer() is called for other reasons as part of the write path, all subsequent sync():s would result in the very same data (plus whatever was written since last time) being re-written and fsync():ed again. So first you write+fsync N bytes, then N+N1, then N+N1+N2... (each N being a batch), until at some point you trigger a rebuffer() and it starts all over again.

The result is that you see *a lot* more writes to the commit log than are in fact written to the BRAF. And these writes translate into actual real writes to the underlying storage device due to fsync(). We had crazy numbers where we saw spikes upwards of 80 mb/second where the actual throughput was more like ~ 1 mb second of data to the commit log.

(One can make a possibly weak argument that it is also functionally incorrect as I can imagine implementations where re-writing the same blocks does copy-on-write in such a way that you're not necessarily guaranteed to see before-or-after data, particularly in case of partial page writes. However that's probably not a practical issue.)

Worthy of noting is that this probably causes added difficulties in fsync() latencies since the average fsync() will contain a lot more data. Depending on I/O scheduler and underlying device characteristics, the extra writes *may* not have a detrimental effect, but I think it's pretty easy to point to cases where it will be detrimental - in particular if the commit log is on a non-battery backed drive. Even with a nice battery backed RAID with the commit log on, the size of the writes probably contributes to difficulty in making the write requests propagate down without being starved by reads (but this is speculation, not tested, other than that I've observed commit log writer starvation that seemed excessive).

This isn't the first subtle BRAF bug. What are people's thoughts on creating separate abstractions for streaming I/O that can perhaps be a lot more simple, and use BRAF only for random reads in response to live traffic? (Not as part of this JIRA, just asking in general.)
",,,,,,,,,,,,,,,,17/May/11 12:23;scode;CASSANDRA-2660-075.txt;https://issues.apache.org/jira/secure/attachment/12479455/CASSANDRA-2660-075.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-17 14:07:31.176,,,no_permission,,,,,,,,,,,,20758,,,Tue May 17 16:18:38 UTC 2011,,,,,,0|i0gcp3:,93487,jbellis,jbellis,,,,,,,,,"17/May/11 14:07;jbellis;bq. What are people's thoughts on creating separate abstractions for streaming I/O that can perhaps be a lot more simple, and use BRAF only for random reads in response to live traffic? (Not as part of this JIRA, just asking in general.)

Every time I've looked at doing this I've put it aside because making all writes two-pass (first pass to compute size, so we don't have to seek back after serializing the row itself) is such a pain.",17/May/11 14:55;jbellis;Ideally we wouldn't wipe out the buffer for read purposes but since we are mixing rw in the same buffer (see comments above) this is the best option.  Committed.,"17/May/11 15:50;hudson;Integrated in Cassandra-0.7 #487 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/487/])
    mark BRAF buffer invalid post-flush so we don't re-flush partial buffers again
patch by Peter Schuller; reviewed by jbellis for CASSANDRA-2660
",17/May/11 16:18;jbellis;merged to 0.8 branch cleanly,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
auto bootstrapping a node into a cluster without a schema silently fails,CASSANDRA-2625,12506550,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,amorton,amorton,08/May/11 22:38,12/Mar/19 14:03,13/Mar/19 22:26,11/May/11 08:08,0.7.6,,,,,,0,,,,,"from http://www.mail-archive.com/user@cassandra.apache.org/msg13001.html

StorageService.joinRing() aborts the auto bootstrap process if the cluster does not have a schema defined. It looks like the node is left in the ""Joining"" mode and there is no logging. 

There could be a schema defined and no data loaded, so just having a schema does not make the token selection any better. And BootStrapper.bootstrap() handles their been no non system tables.

Can we let the bootstrap process continue ?",,,,,,,,,,,,,,,,10/May/11 19:32;slebresne;0001-2625.patch;https://issues.apache.org/jira/secure/attachment/12478723/0001-2625.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-10 19:32:24.403,,,no_permission,,,,,,,,,,,,20737,,,Wed May 11 08:08:49 UTC 2011,,,,,,0|i0gchj:,93453,jbellis,jbellis,,,,,,,,,10/May/11 19:32;slebresne;This is a regression from CASSANDRA-2435. Attaching fix.,10/May/11 19:46;jbellis;+1,"11/May/11 07:35;hudson;Integrated in Cassandra-0.7 #476 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/476/])
    Fix regression where boostrapping a node without schema defined fails
patch by slebresne; reviewed by jbellis for CASSANDRA-2625
",11/May/11 08:08;slebresne;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
work-around schema disagreements from cqlsh,CASSANDRA-2649,12507173,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,13/May/11 20:16,12/Mar/19 14:03,13/Mar/19 22:26,07/Jun/11 03:15,0.8.0,,,Legacy/Tools,,,0,cql,,,,"It is handy to be able to put CQL statements in a flat-file and load them by redirecting to {{cqlsh}} stdin, but this can fail on a cluster when executing statements that modify schema.

The attached patch works around this problem by retrying up to 3 times, with a progressive delay after each attempt.  A better solution would probably be to compare schema versions, but this seems to work well enough, and is better than _not_ handling it at all.",,,,,,,,,,,,,,,,13/May/11 20:17;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2649-try-to-work-around-schema-disagreement-.txt;https://issues.apache.org/jira/secure/attachment/12479162/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2649-try-to-work-around-schema-disagreement-.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-20 20:05:06.337,,,no_permission,,,,,,,,,,,,20752,,,Mon May 23 21:18:23 UTC 2011,,,,,,0|i0gcmn:,93476,jbellis,jbellis,,,,,,,,,"20/May/11 20:05;jbellis;bq. A better solution would probably be to compare schema versions

Agreed.  Why not just do that?  describe_schema_versions makes it extremely easy.","20/May/11 22:51;urandom;Well, because this currently employs no use of the thrift RPC API, (so it would be much more invasive to implement).

Also, after having some time to think about this, I'm not sure it would be that much better.  You'd still need to wait and retry between each disagreement.  The only advantage would be access to the actual versions for error reporting (and considering the nature of cqlsh, it's arguably not the right audience for that level of detail anyway).","23/May/11 15:43;jbellis;The right thing to do is wait for agreement after each schema change. Relying on integrityerror is broken since inserts will just fail.  using describe_schema_versions let's this be 100% correct instead of hoping you slept long enough before proceeding.

bq. this currently employs no use of the thrift RPC API

since cql is built on thrift, it wouldn't be difficult to use the thrift method until there is a ""native"" replacement.

","23/May/11 20:17;jbellis;Let's create a ticket to do this ""right"" for 0.8.1, but this patch is definitely a minimally inasive improvement on the existing multiple-schema-changes-are-guaranteed-to-hose-cqlsh status quo, so +1 for 0.8.0.","23/May/11 21:18;hudson;Integrated in Cassandra-0.8 #129 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/129/])
    work-around schema disagreements

Patch by eevans; review by jbellis for CASSANDRA-2649

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1126736
Files : 
* /cassandra/branches/cassandra-0.8/drivers/py/cqlsh
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted handoff needs to adjust page size for lage columns to avoid OOM,CASSANDRA-2652,12507223,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,14/May/11 14:52,12/Mar/19 14:03,13/Mar/19 22:26,16/May/11 21:16,0.7.7,,,,,,0,,,,,"Example OOM:
{noformat}
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:269)
	at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:356)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:318)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:99)
	at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:248)
	at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:268)
	at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:227)
	at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(ConcurrentSkipListMap.java:1493)
	at java.util.concurrent.ConcurrentSkipListMap.<init>(ConcurrentSkipListMap.java:1443)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:379)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:362)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:322)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.getNextBlock(IndexedSliceReader.java:179)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:121)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:49)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:116)
	at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1267)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1195)
	at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:138)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:331)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
{noformat}",,,,,,,,,,,,,,,,14/May/11 14:52;jbellis;2652.txt;https://issues.apache.org/jira/secure/attachment/12479227/2652.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-16 21:10:11.434,,,no_permission,,,,,,,,,,,,20754,,,Mon May 16 21:36:14 UTC 2011,,,,,,0|i0gcnb:,93479,brandon.williams,brandon.williams,,,,,,,,,14/May/11 14:52;jbellis;(also renames sendMessage to sendRow.),16/May/11 21:10;brandon.williams;+1,16/May/11 21:16;jbellis;committed,"16/May/11 21:36;hudson;Integrated in Cassandra-0.7 #486 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/486/])
    adjust hinted handoff page size to avoid OOM with large columns
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2652
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"in cassandra-cli, the help command output on validation types should be updated",CASSANDRA-2615,12506456,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,cywjackson,cywjackson,06/May/11 20:45,12/Mar/19 14:03,13/Mar/19 22:26,12/May/11 22:27,0.8.0,,,,,,0,,,,,"from cassandra-cli, say type ""help assume""

you will find:
  Supported values are:
    - AsciiType
    - BytesType
    - CounterColumnType (distributed counter column)
    - IntegerType (a generic variable-length integer type)
    - LexicalUUIDType
    - LongType
    - UTF8Type


ok now:
[default@cfs] assume inode comparator as UTF8Type;   
Type 'UTF8Type' was not found. Available: bytes, integer, long, lexicaluuid, timeuuid, utf8, ascii.


so looks like the ""supported type list should be update by taking away the ""Type"" post-fix..

however, on the other hand, you can't really use it:

[default@cfs] update column family inode;                         
Unable to find abstract-type class 'org.apache.cassandra.db.marshal.utf8'

looks like from the update, you still need the ""Type"" (case insensitive?)",,,,,,,,,,,,,,,,12/May/11 22:09;xedin;CASSANDRA-2615.patch;https://issues.apache.org/jira/secure/attachment/12479016/CASSANDRA-2615.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-11 11:00:32.819,,,no_permission,,,,,,,,,,,,20730,,,Thu May 12 22:27:35 UTC 2011,,,,,,0|i0gcfb:,93443,cywjackson,cywjackson,,,,,,,,,11/May/11 11:00;xedin;Added CounterColumnType to functions + fixed doc and assume function.,"12/May/11 22:21;cywjackson;+1

[default@testks] assume Super4 comparator as ascii
...     ;
Assumption for column family 'Super4' added successfully.
[default@testks] update column family Super4;     
 INFO 15:20:59,545 Applying migration 1d332bd0-7ce6-11e0-0000-fd7033aa10e7 Update column family to org.apache.cassandra.config.CFMetaData@6e72d873[cfId=1000,ksName=testks,cfName=Super4,cfType=Super,comparator=org.apache.cassandra.db.marshal.AsciiType,subcolumncomparator=org.apache.cassandra.db.marshal.BytesType,comment=,rowCacheSize=10000.0,keyCacheSize=200000.0,readRepairChance=1.0,replicateOnWrite=false,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.UTF8Type,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,memtableFlushAfterMins=1440,memtableThroughputInMb=242,memtableOperationsInMillions=1.134375,mergeShardsChance=0.1,keyAlias=java.nio.HeapByteBuffer[pos=475 lim=478 cap=480],column_metadata={}]
 INFO 15:20:59,546 Enqueuing flush of Memtable-Migrations@903913131(7137/8921 serialized/live bytes, 1 ops)
 INFO 15:20:59,547 Writing Memtable-Migrations@903913131(7137/8921 serialized/live bytes, 1 ops)
 INFO 15:20:59,547 Enqueuing flush of Memtable-Schema@1257515479(2960/3700 serialized/live bytes, 3 ops)
 INFO 15:20:59,737 Completed flushing /var/lib/cassandra/data/system/Migrations-g-3-Data.db (7201 bytes)
 INFO 15:20:59,739 Writing Memtable-Schema@1257515479(2960/3700 serialized/live bytes, 3 ops)
 INFO 15:20:59,912 Completed flushing /var/lib/cassandra/data/system/Schema-g-3-Data.db (3110 bytes)
1d332bd0-7ce6-11e0-0000-fd7033aa10e7
Waiting for schema agreement...
... schemas agree across the cluster
",12/May/11 22:27;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
row deletions do not add to memtable op count,CASSANDRA-2519,12504779,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,20/Apr/11 10:45,12/Mar/19 14:03,13/Mar/19 22:26,20/Apr/11 15:54,0.7.5,,,,,,0,,,,,"from discussion http://www.mail-archive.com/user@cassandra.apache.org/msg12531.html

Memtable.resolve() uses the count of columns in the CF to bump the op count however RowMutation.delete() does not add any columns to the CF when an entire row is deleted. If a super column or column is deleted it adds 1 towards the op count. Deleting many named columns will add many to the op count. ",,,,,,,,,,,,,,,,20/Apr/11 10:58;amorton;0001-include-row-deletions-in-memtable-op-count.patch;https://issues.apache.org/jira/secure/attachment/12476875/0001-include-row-deletions-in-memtable-op-count.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-20 15:54:22.238,,,no_permission,,,,,,,,,,,,20672,,,Wed Apr 20 16:26:11 UTC 2011,,,,,,0|i0gbuf:,93349,jbellis,jbellis,,,,,,,,,20/Apr/11 10:58;amorton;patch 0001 adds one to the memtable op count if the CF contains zero columns and cf.isMarkedForDeletion(),"20/Apr/11 15:54;jbellis;committed, thanks!","20/Apr/11 16:26;hudson;Integrated in Cassandra-0.7 #450 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/450/])
    count a row deletionas one operation towards memtable threshold
patch by Aaron Morton; reviewed by jbellis for CASSANDRA-2519
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrub does not close files,CASSANDRA-2669,12507784,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,doubleday,doubleday,19/May/11 17:14,12/Mar/19 14:03,13/Mar/19 22:26,15/Jun/11 12:06,0.7.7,0.8.1,,Legacy/Tools,,,0,,,,,"After scrubbing I find that cassandra process still holds file handles to the deleted sstables:

{noformat}
root@blnrzh047:/mnt/cassandra# jps
6932 Jps
32359 CassandraDaemon
32398 CassandraJmxHttpServer

root@blnrzh047:/mnt/cassandra# du -sh .
315G	.

root@blnrzh047:/mnt/cassandra# df -h .
Filesystem            Size  Used Avail Use% Mounted on
/dev/md0              1.1T  626G  420G  60% /mnt/cassandra


root@blnrzh047:/mnt/cassandra# lsof | grep /mnt
java      32359        root  356r      REG                9,0           24    4194599 /mnt/cassandra/data/system/Migrations-f-13-Index.db (deleted)
java      32359        root  357r      REG                9,0       329451    4194547 /mnt/cassandra/data/system/HintsColumnFamily-f-588-Data.db (deleted)
java      32359        root  358r      REG                9,0           22    4194546 /mnt/cassandra/data/system/HintsColumnFamily-f-588-Index.db (deleted)
java      32359        root  359r      REG                9,0       313225    4194534 /mnt/cassandra/data/system/HintsColumnFamily-f-587-Data.db (deleted)
java      32359        root  360r      REG                9,0           22    4194494 /mnt/cassandra/data/system/HintsColumnFamily-f-587-Index.db (deleted)
java      32359        root  361r      REG                9,0        30452    4194636 /mnt/cassandra/data/system/Schema-f-13-Data.db (deleted)
java      32359        root  362r      REG                9,0          484    4194635 /mnt/cassandra/data/system/Schema-f-13-Index.db (deleted)
{noformat}

I guess there's a missing dataFile.close() in CompactionManager:648
",,,,,,,,,,,,,,,,25/May/11 16:52;jbellis;2669.txt;https://issues.apache.org/jira/secure/attachment/12480429/2669.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-25 16:53:44.653,,,no_permission,,,,,,,,,,,,20764,,,Wed Jun 15 12:06:05 UTC 2011,,,,,,0|i0gcqv:,93495,slebresne,slebresne,,,,,,,,,25/May/11 16:53;jbellis;patch attached to add finally { close } block.,25/May/11 17:07;slebresne;+1,"25/May/11 18:21;hudson;Integrated in Cassandra-0.7 #497 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/497/])
    close scrub file handles
patch by jbellis; reviewed by slebresne for CASSANDRA-2669

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1127589
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/CompactionManager.java
","15/Jun/11 12:06;slebresne;Seems correctly committed, closing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.io.IOError: java.io.EOFException with version 0.7.6,CASSANDRA-2675,12507853,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,kochen,kochen,20/May/11 09:09,12/Mar/19 14:03,13/Mar/19 22:26,25/May/11 15:43,0.7.7,0.8.1,,,,,0,,,,,"I use the following data-model

column_metadata: []
name: Customers
column_type: Super
gc_grace_seconds: 60

I have a super-column-family with a single row.
Within this row I have a single super-column.
Within this super-column, I concurrently create, read and delete columns.

I have three threads:

- Do in a loop: add a column to the super-column.
- Do in a loop: delete a random column from the super-column.
- Do in a loop: read the super-column (with all columns).

After running the above threads concurrently, I always receive one of the following errors:

ERROR 17:09:57,036 Fatal exception in thread Thread[ReadStage:81,5,main]
java.io.IOError: java.io.EOFException
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:252)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:268)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:227)
        at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(Unknown Source)
        at java.util.concurrent.ConcurrentSkipListMap.<init>(Unknown Source)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:379)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:362)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:322)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:116)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1267)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1195)
        at org.apache.cassandra.db.Table.getRow(Table.java:324)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:451)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readByte(Unknown Source)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:324)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:335)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:71)
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:248)
        ... 30 more

java.io.IOError: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:252)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:268)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:227)
        at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(Unknown Source)
        at java.util.concurrent.ConcurrentSkipListMap.<init>(Unknown Source)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:379)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:362)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:322)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:116)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1385)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1262)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1190)
        at org.apache.cassandra.db.Table.getRow(Table.java:324)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:451)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:73)
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:248)
        ... 30 more 

ERROR 11:02:19,824 Fatal exception in thread Thread[ReadStage:3404,5,main]
java.io.IOError: java.io.IOException: mmap segment underflow; remaining is 660267 but 758592100 requested
	at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:252)
	at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:268)
	at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:227)
	at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(Unknown Source)
	at java.util.concurrent.ConcurrentSkipListMap.<init>(Unknown Source)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:379)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:362)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:322)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:116)
	at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1267)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1195)
	at org.apache.cassandra.db.Table.getRow(Table.java:324)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:451)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)","Reproduced on single Cassandra node (CentOS 5.5)
Reproduced on single Cassandra node (Windows Server 2008)",,,,,,,,,,,,,,,24/May/11 12:51;slebresne;0001-Don-t-remove-columns-from-super-columns-in-memtable.patch;https://issues.apache.org/jira/secure/attachment/12480236/0001-Don-t-remove-columns-from-super-columns-in-memtable.patch,24/May/11 15:17;slebresne;0002-Avoid-modifying-super-column-in-memtable-being-flush-v2.patch;https://issues.apache.org/jira/secure/attachment/12480266/0002-Avoid-modifying-super-column-in-memtable-being-flush-v2.patch,24/May/11 12:51;slebresne;0002-Avoid-modifying-super-column-in-memtable-being-flush.patch;https://issues.apache.org/jira/secure/attachment/12480237/0002-Avoid-modifying-super-column-in-memtable-being-flush.patch,20/May/11 09:11;kochen;CassandraIssue.zip;https://issues.apache.org/jira/secure/attachment/12479889/CassandraIssue.zip,23/May/11 13:37;kochen;CassandraIssueJava.zip;https://issues.apache.org/jira/secure/attachment/12480110/CassandraIssueJava.zip,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-05-24 12:51:09.701,,,no_permission,,,,,,,,,,,,20770,,,Thu Jun 16 09:34:40 UTC 2011,,,,,,0|i0gcs7:,93501,jbellis,jbellis,,,,,,,,,20/May/11 09:11;kochen;Test application (.NET 4) to reproduce problem,"23/May/11 13:37;kochen;Included the Java version of the test program.

Usage:

java -jar CassandraIssue.jar [<ip> [<keyspace> [<column-family> [<port>]]]]

","24/May/11 12:51;slebresne;I was able to reproduce, thanks for the java version.

I think the problem is that reads can remove subcolumns from a super-column that happens to be in a memtable being flushed. If a subcolumn become gc-able after when the super column count size was written on disk and the time the subcolumn itself is written we won't write it and will end up with short super columns (hence the EOFException). Note that this should not happen with a reasonable gc_grace value (one such that nothing that gets flushed will be gcable).

First attached patch fixes this by making reads copy the super-column before modifying it (0.7 patch).

I think there is a related second bug, in that when we reduce super columns (in QueryFilter), if we merge multiple super column with the same name, we'll ""merge"" them in the first super column. That is, we may end up adding subcolumns to a super column that is in an in-memory memtable. Most of the time this will be harmless, except some useless data duplication. But if that happens for a super column (in a memtable) being flushed and, as above, between the write of the number of column and the actual column writes, we may end up with too long super column. With could result in unreachable columns (i.e, data loss effectively) and quite probably some weird corruption during a compaction.

Second patch fixes this second problem.

I haven't been able to reproduce with the 2 attached patches and the thing is running since more than an hour.
","24/May/11 13:42;jbellis;does the copyOnWrite optimization in patch 1 make sense, given that patch 2 will do a copy anyway?  might be simpler to just force the copy in getCF so any higher-level callers don't have to worry about it.","24/May/11 14:07;slebresne;Yes I agree, patch 2 is actually enough.","24/May/11 14:35;jbellis;Is it?  Don't you still have the problem of a tombstone cleanup modifying things mid-flush?

I was thinking patch 1 modified to always-copy would be enough but there is actually an efficiency gain when you don't have multiple CF/SC merging, so +1 on both if I understand correctly.

Nit: would prefer to expose a CF.isEmpty method than make column size public -- CSLM.size is O(N) and while that doesn't matter here it would be easy to introduce inefficiency w/o realizing it.","24/May/11 15:17;slebresne;bq. Is it? Don't you still have the problem of a tombstone cleanup modifying things mid-flush?

Patch 2 make sure that the cf returned by a getTopLevelColumns() doesn't have any super column that is an alias of a super column in some memtable. So then we don't care what consumers of the result getTopLevelColumns() do. Even if they remove columns the 'being flushed' super column won't be affected.

The idea of not always copying in the first patch was to not incure the copy to all the part of the code that doesn't care (mainly compaction). But anyway, I do think that patch 2 is enough.

Attaching v2 of patch 2 to use isEmpty.",24/May/11 15:26;jbellis;+1,"24/May/11 19:13;hudson;Integrated in Cassandra-0.7 #496 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/496/])
    Clone super column to avoid modifying them mid-flush
patch by slebresne; reviewed by jbellis for CASSANDRA-2675

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1127130
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnFamily.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/filter/QueryFilter.java
","16/Jun/11 09:34;kochen;According to the changes of Cassandra 0.8.0, this is fixed in 0.8.0. The fixed versions in this issue says 0.7.7 and 0.8.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError post truncate,CASSANDRA-2673,12507836,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,ithkuil,ithkuil,20/May/11 01:06,12/Mar/19 14:03,13/Mar/19 22:26,01/Jun/11 15:39,0.7.7,0.8.1,,,,,0,,,,,"I had 3 nodes with about 100G in a CF. I run truncate on that CF from cassandra-cli. Then I run cleanup for that CF. I saw this exception shortly after.

 INFO [FlushWriter:5] 2011-05-20 02:56:42,699 Memtable.java (line 157) Writing Memtable-body@1278535630(26722 bytes, 1 operations)
 INFO [FlushWriter:5] 2011-05-20 02:56:42,706 Memtable.java (line 172) Completed flushing /var/lib/cassandra/data/dnet/body-f-1892-Data.db (26915 bytes)
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,981 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1892
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,982 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1889
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,983 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1890
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,983 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1888
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,984 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1887
 INFO [CompactionExecutor:1] 2011-05-20 03:02:08,724 CompactionManager.java (line 750) Cleaned up to /var/lib/cassandra/data/dnet/body-tmp-f-1891-Data.db.  25,629,365,173 to 25,629,365,173 (~100% of original) bytes for 884,546 keys.  Time: 1,165,900ms.
ERROR [CompactionExecutor:1] 2011-05-20 03:02:08,727 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
	at org.apache.cassandra.io.sstable.SSTableTracker.replace(SSTableTracker.java:108)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:1037)
	at org.apache.cassandra.db.CompactionManager.doCleanupCompaction(CompactionManager.java:769)
	at org.apache.cassandra.db.CompactionManager.access$500(CompactionManager.java:56)
	at org.apache.cassandra.db.CompactionManager$2.call(CompactionManager.java:173)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",linux 64-bit ubuntu. deb package (datastax). (Random partitioner),,,,,,,,,,,,,,,25/May/11 16:33;jbellis;2673.txt;https://issues.apache.org/jira/secure/attachment/12480427/2673.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-20 20:10:16.485,,,no_permission,,,,,,,,,,,,20768,,,Wed Jun 01 17:47:22 UTC 2011,,,,,,0|i0gcrr:,93499,slebresne,slebresne,,,,,,,,,"20/May/11 20:10;jbellis;Looks like we need to move truncate to the compaction executor so they don't race.

In the meantime, retrying the truncate until it works should be fine.","25/May/11 16:33;jbellis;patch to move the truncation operation to CompactionManager instead of postFlushExecutor. (Not sure what the thinking was there, to be honest. Which may mean it's something subtle I'm missing now, or maybe just that I was smoking crack when I wrote that code originally. CASSANDRA-531, the original truncate ticket, doesn't shed any light here.)",01/Jun/11 14:38;slebresne;+1,01/Jun/11 15:39;jbellis;committed,"01/Jun/11 17:47;hudson;Integrated in Cassandra-0.7 #501 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/501/])
    fix truncate/compaction race
patch by jbellis; reviewed by slebresne for CASSANDRA-2673

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1130191
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/Table.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/CompactionManager.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/TruncateVerbHandler.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect NetworkTopolgyStrategy Options on upgrade from 0.7.5,CASSANDRA-2678,12507930,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,ctrahman,ctrahman,20/May/11 20:57,12/Mar/19 14:03,13/Mar/19 22:26,23/May/11 19:12,0.8.0,,,,,,0,,,,,"After an upgrade from 0.7.5 to 0.8.0-rc1 on a 10 node, single DC ring configured with NTS, operations fail due to an inability to reach replicas in the 'second datacenter':

ERROR [pool-2-thread-8] 2011-05-17 12:15:23,145 Cassandra.java (line 3294) Internal error processing insert
java.lang.IllegalStateException: datacenter (replication_factor) has no more endpoints, (3) replicas still needed
        at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:118)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:100)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1611)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1599)
        at org.apache.cassandra.service.StorageProxy.getWriteEndpoints(StorageProxy.java:217)
        at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:202)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:154)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:557)
        at org.apache.cassandra.thrift.CassandraServer.internal_insert(CassandraServer.java:434)
        at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:442)
        at org.apache.cassandra.thrift.Cassandra$Processor$insert.process(Cassandra.java:3286)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
DEBUG [ScheduledTasks:1] 2011-05-17 12:15:33,975 StorageLoadBalancer.java (line 334) Disseminating load info ...

On checking the keyspace definition with cassandra-cli, it appears that 0.8.0-rc1 considered the 'replication_factor:3' configuration in the older version as a DC name in part of the DC replication strategy:

  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [replication_factor:3, DC1:3]

I attempted to remove replication_factor as a DC using the 'update keyspace' command, but it would persist.  I was able to remove the DC1:3 and use:

update keyspace MyKeyspace with strategy_options=[{replication_factor:3}];

then changed the topology properties file, renamed DC1 to replication_factor, and it worked - so there is a workaround. ","10 node cluster, RHELL6.0
0.7.5, Single DC using NTS and RF=3 (DC1:3) -> upgraded to 0.8.0-rc1. ",,,,,,,,,,,,,,,23/May/11 16:20;jbellis;2678-v2.txt;https://issues.apache.org/jira/secure/attachment/12480122/2678-v2.txt,20/May/11 21:49;jbellis;2678.txt;https://issues.apache.org/jira/secure/attachment/12479961/2678.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-20 21:49:06.628,,,no_permission,,,,,,,,,,,,20771,,,Mon May 23 19:12:48 UTC 2011,,,,,,0|i0gcsv:,93504,jhermes,jhermes,,,,,,,,,20/May/11 21:49;jbellis;patch to only add r_f to strategy_options for simplestrategy and oldnetworktopologystrategy,"23/May/11 16:03;jhermes;KSMetaData:55
{noformat}
maybeAddReplicationFactor(options, ks_def.strategy_class, ks_def.isSetReplication_factor() ? ks_def.replication_factor : null);
{noformat}
...needs to be
{noformat}
maybeAddReplicationFactor(options, ks_def.strategy_class.toString(), ks_def.replication_factor);
{noformat}

It's much clearer to not ternary on the value (since you're not going to use it in the `ne SS || ONTS` case), and more importantly that ks_def.strategy_class is a CharSequence, so you'll hit an error if you expect it to be a String in maybe...().

Otherwise straightforward.","23/May/11 16:20;jbellis;bq. It's much clearer to not ternary on the value

you need to though, to convert int -> Integer.  Otherwise it will autobox the 0 default value, instead of the correct null.  (maybeAdd wasn't checking for null though -- v2 corrects this.)

bq. more importantly that ks_def.strategy_class is a CharSequence

no, this is a Thrift object so it's a String.","23/May/11 16:44;jhermes;If the def has a null value for RF (as it correctly should post-RF change), should we verify that 'replication_factor' already exists in the map inside of maybe() and do nothing?

Line 55 is unnecessary/duped, and line 142 looks like it might also need to pass null, otherwise it's just avro/thriftisms.","23/May/11 18:35;jbellis;bq. If the def has a null value for RF (as it correctly should post-RF change)

As above, it can't (since it is an int not an Integer), but it can be ""unset.""

bq. should we verify that 'replication_factor' already exists in the map inside of maybe() and do nothing?

No, the strategy class is responsible for actually validating its options.  We're just trying to stick a thin compatibility layer in here so people can continue using 0.7 clients w/ 0.8 server temporarily.","23/May/11 18:48;jhermes;bq. As above, it can't (since it is an int not an Integer), but it can be ""unset.""
I meant the null that we pass in line 55 because it was 'unset'.

I agree with the latter choice. We could doubly validate here and catch the error just a _little_ bit faster than waiting for the class to validate, but it's not a big deal. Assuming the duplicate code was removed, +1.",23/May/11 19:12;jbellis;committed minus the bogus maybeAdd call.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
generate-eclipse-files ant target throws StackOverflowError in eclipse,CASSANDRA-2687,12508074,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,23/May/11 16:08,12/Mar/19 14:03,13/Mar/19 22:26,23/May/11 19:04,0.8.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,23/May/11 16:09;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2687-fix-eclipse-stackoverflow-issue.txt;https://issues.apache.org/jira/secure/attachment/12480121/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2687-fix-eclipse-stackoverflow-issue.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-23 16:31:43.149,,,no_permission,,,,,,,,,,,,20776,,,Mon May 23 19:04:10 UTC 2011,,,,,,0|i0gcun:,93512,urandom,urandom,,,,,,,,,23/May/11 16:31;urandom;Nice. +1,23/May/11 19:04;tjake;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
range scan doesn't repair missing rows,CASSANDRA-2680,12507952,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,21/May/11 02:08,12/Mar/19 14:03,13/Mar/19 22:26,23/May/11 14:23,0.7.7,0.8.1,,,,,0,,,,,"Range scans do not do digest queries but they do compare all the replicas they receive and repair any discrepancies in the background.  (Thus, to get comparable behavior to normal read repair, CL.ALL must be used.)

The bug is that currently, replicas that omit a row entirely will be ignored and that row will not be sent to them.  ",,,,,,,,,,,,,,,,21/May/11 02:22;jbellis;2680.txt;https://issues.apache.org/jira/secure/attachment/12479978/2680.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-23 14:06:39.762,,,no_permission,,,,,,,,,,,,20772,,,Tue May 24 19:13:31 UTC 2011,,,,,,0|i0gctb:,93506,slebresne,slebresne,,,,,,,,,21/May/11 02:22;jbellis;patch to add placeholders for missing rows,23/May/11 14:06;slebresne;+1,23/May/11 14:23;jbellis;committed,"24/May/11 19:13;hudson;Integrated in Cassandra-0.7 #496 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/496/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in Table.createReplicationStrategy during sends from HintedHandOffManager,CASSANDRA-2685,12508047,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,ithkuil,ithkuil,23/May/11 11:59,12/Mar/19 14:03,13/Mar/19 22:26,23/May/11 14:21,0.7.7,0.8.1,,,,,0,,,,,"After about 800k inserts in a column family with RF=1, I get this exception:

{code}
ERROR [HintedHandoff:2] 2011-05-20 18:38:25,089 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[HintedHandoff:2,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:266)
	at org.apache.cassandra.db.Table.<init>(Table.java:212)
	at org.apache.cassandra.db.Table.open(Table.java:106)
	at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:331)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
	at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:409)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{code}","2.6.32-5-xen-amd64 #1 SMP Wed Jan 12 05:46:49 UTC 2011 x86_64 GNU/Linux
",,,,,,,,,,,,,,,23/May/11 13:40;jbellis;2685.txt;https://issues.apache.org/jira/secure/attachment/12480111/2685.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-23 13:40:58.86,,,no_permission,,,,,,,,,,,,20775,,,Tue May 24 19:13:31 UTC 2011,,,,,,0|i0gcuf:,93511,gdusbabek,gdusbabek,,,,,,,,,23/May/11 13:40;jbellis;patch to skip replay of hints for dropped KS/CF,23/May/11 13:47;gdusbabek;+1.,23/May/11 14:21;jbellis;committed,"24/May/11 19:13;hudson;Integrated in Cassandra-0.7 #496 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/496/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI escaped single quote parsing gives errors,CASSANDRA-2623,12506513,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,rday,rday,07/May/11 21:21,12/Mar/19 14:03,13/Mar/19 22:26,12/May/11 15:31,0.7.6,0.8.0,,Legacy/Tools,,,0,cli,,,,"Escaping quotes in cli commands causes parsing errors.


some examples::::
No need to create columns etc, it doesn't get through parsing the expression::

cassandra-cli

1. 
set column['KEY+vals'][VALUE] = 'VAL\'' ;
Syntax error at position 41: mismatched character '<EOF>' expecting '''

2.
set column['KEY+val\'s'][VALUE] = 'VAL' ;
Syntax error at position 41: mismatched character '<EOF>' expecting '''

3.
set column['KEY+vals\''][VALUE] = 'VAL\'' ;
Syntax error at position 38: unexpected ""\"" for `set column['KEY+vals\''][VALUE] = 'VAL\'' ;`.
","windows vista, linux",,,,,,,,,,,,,,,10/May/11 12:24;xedin;CASSANDRA-2623-0.7.patch;https://issues.apache.org/jira/secure/attachment/12478692/CASSANDRA-2623-0.7.patch,10/May/11 12:24;xedin;CASSANDRA-2623-trunk.patch;https://issues.apache.org/jira/secure/attachment/12478693/CASSANDRA-2623-trunk.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-09 05:24:39.999,,,no_permission,,,,,,,,,,,,20735,,,Thu May 12 20:56:28 UTC 2011,,,,,,0|i0gch3:,93451,jbellis,jbellis,,,,,,,,,"09/May/11 05:19;rday;In the Cli.g file the string literal does not allow for escaped single quote.

http://svn.apache.org/repos/asf/cassandra/trunk/src/java/org/apache/cassandra/cli/Cli.g


StringLiteral
    :
    '\'' (~'\'')* '\'' ( '\'' (~'\'')* '\'' )*
    ;

I'm not sure yet what is the purpose of the repeating last half of the expression.
First Attempt to allow escaped characters seemed to cause other problems, still investigating that.


","09/May/11 05:24;jbellis;single quotes are escaped by doubling them, as in SQL:

{noformat}
'aren''t you glad you can escape quotes this way'
{noformat}
","09/May/11 14:27;rday;Unforuntately using 2 single quotes causes insertions of both quotes in to the data set,

create keyspace KEYS with
   placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and
   replication_factor = 1;
use KEYS;

create column family CF with
  comparator=UTF8Type and
  rows_cached=1.0 and
  memtable_throughput=128 and
  min_compaction_threshold=10 and
  column_metadata =
  [
    {column_name:VERSION, validation_class:LongType},
    {column_name:VALUE, validation_class:UTF8Type}
  ];
  
  
  set CF['key''1'][VERSION] = 'aren''t happy' ; 
  set CF['key''1'][VALUE] = 'aren''t happy' ; 

  list CF

RowKey: key''1
=> (column=VALUE, value=aren''t happy, timestamp=1304952103638000)
=> (column=VERSION, value=1, timestamp=1304952057641000)


Through a thrift java client insert we are able to insert and find single quote keys,
But if data is inserted via CLI,  the java thrift single quote lookups do not match.",09/May/11 14:34;jbellis;You're right. That's a bug.,"09/May/11 15:47;rday;Possibly the unescapeSQLString in CliUtils.java
can do something to get rid of the single quotes.
The unit test doesn't cover escaping via 2 single quotes.

    public static String unescapeSQLString(String b)
    {
        if (b.charAt(0) == '\'' && b.charAt(b.length()-1) == '\'')
            b = b.substring(1, b.length()-1);
        return StringEscapeUtils.unescapeJava(b);
    }

also method escapeSQLString which 
(only used in the unit test but public ) 
is assuming a backslash to escape single quotes.

    public static String escapeSQLString(String b)
    {
        // single quotes are not escaped in java, need to be for cli
        return StringEscapeUtils.escapeJava(b).replace(""\'"", ""\\'"");
    }


The StringLiteral Definition makes sense for using 2 single quotes escaping since it should always have an even number of quotes so just the backing code needs to be enlightened.
","09/May/11 16:44;rday;Just replacing the 2 quotes with one seems to work for the simple case in 7.5.

{code:title=CliUtils.java|borderStyle=solid}
    public static String unescapeSQLString(String b) 
    { 
        if (b.charAt(0) == '\'' && b.charAt(b.length()-1) == '\'') 
            b = b.substring(1, b.length()-1); 
        b = b.replaceAll(""''"",""'""); 
        return StringEscapeUtils.unescapeJava(b); 
    } 
{code}",10/May/11 12:24;xedin;fix for CLI grammar to support escaped quotes + tests.,10/May/11 14:18;jbellis;you really like the backslash approach better than the SQL quote-doubling one?,"10/May/11 14:41;xedin;Yes, I personally don't like SQL way of dealing with string interpolation.",12/May/11 15:31;jbellis;committed,"12/May/11 20:56;hudson;Integrated in Cassandra-0.7 #483 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/483/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception adding validators to non-string columns,CASSANDRA-2696,12508130,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,24/May/11 03:47,12/Mar/19 14:03,13/Mar/19 22:26,24/May/11 05:29,0.8.0,,,,,,0,,,,,"Adding column metadata to a column with a non-string name causes an Exception to be raised:

{noformat}
org.apache.cassandra.db.marshal.MarshalException: Expected 8 or 0 byte long (3)
	at org.apache.cassandra.db.marshal.LongType.validate(LongType.java:106)
	at org.apache.cassandra.config.CFMetaData.validateAliasCompares(CFMetaData.java:977)
	at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:699)
	at org.apache.cassandra.db.migration.UpdateColumnFamily.<init>(UpdateColumnFamily.java:59)
	at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:968)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:4032)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

For example, if the comparator type is LongType, adding a validator or index to column int(3) will cause this.",,,,,,,,,,,,,,,,24/May/11 03:49;thobbs;2696-test.txt;https://issues.apache.org/jira/secure/attachment/12480200/2696-test.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-24 05:05:14.947,,,no_permission,,,,,,,,,,,,20778,,,Tue May 24 05:29:58 UTC 2011,,,,,,0|i0gcwn:,93521,jbellis,jbellis,,,,,,,,,24/May/11 03:49;thobbs;Attached patch reproduces using a thrift system test.,24/May/11 03:50;thobbs;It's also worth noting that this does not affect 0.7.x.,24/May/11 05:05;jbellis;I could have sworn we fixed this for CASSANDRA-2622. :(,"24/May/11 05:09;jbellis;It's totally in my patch!

{code}
-    public static void validateAliasCompares(org.apache.cassandra.thrift.CfDef cf_def) throws ConfigurationException
-    {
-        AbstractType comparator = DatabaseDescriptor.getComparator(cf_def.comparator_type);
-        if (cf_def.key_alias != null)
-            comparator.validate(cf_def.key_alias);
-    }
{code}

Either I mis-applied the patch on commit or there was a bad merge to reintroduce it...","24/May/11 05:29;jbellis;re-patched CFMetaData and committed with the new test in r1126874, r1126875",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stop JDBC driver from needing access to cassandra.yaml,CASSANDRA-2694,12508119,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,23/May/11 23:31,12/Mar/19 14:03,13/Mar/19 22:26,24/May/11 13:34,0.8.0,,,,,,0,,,,,"The JDBC driver uses CFMetaData.fromThrift(), it was calling validateMemtableSettings() which used static methods on  DatabaseDescriptor. This causes cassandra.yaml to be loaded and means the client side needs access to the file. 

I think this needs to be fixed for 0.8, I have the patch. 

**Updated** changed title from ""remove references to DatabaseDescriptor in CFMetaData""",,,,,,,,,,,,,,,,24/May/11 10:13;amorton;0001-2694-v2.patch;https://issues.apache.org/jira/secure/attachment/12480229/0001-2694-v2.patch,23/May/11 23:34;amorton;0001-2694.patch;https://issues.apache.org/jira/secure/attachment/12480185/0001-2694.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-24 01:50:51.586,,,no_permission,,,,,,,,,,,,20777,,,Tue May 24 13:34:41 UTC 2011,,,,,,0|i0gcw7:,93519,jbellis,jbellis,,,,,,,,,23/May/11 23:34;amorton;remove references to DatabaseDescriptor in CFMetaData,"24/May/11 01:50;jbellis;I think it would be cleaner if JDBC just accepted the CfDef as given by the server. It would be weird if we changed the validation code in a newer version, causing the JDBC code when run against an old server to say ""that can't be right!""

I'd move the validate code to ThriftValidation with the rest of the ""is this struct I got from the user reasonable"" checks, and have the fromThrift calls in CassandraServer call that method manually, so that fromThrift just does what it advertises.","24/May/11 03:12;amorton;sounds good, will take a look soon.","24/May/11 10:13;amorton;v2 patch moves thrift validation to ThriftValidation and calls it from ThriftValidation.validateCfDef()

I left the avro validation used by CFMetaData.apply() in CFMetaData. ","24/May/11 13:34;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool setcompactionthroughput requiring wrong number of arguments?,CASSANDRA-2550,12505058,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,terjem,terjem,terjem,23/Apr/11 17:58,12/Mar/19 14:03,13/Mar/19 22:26,25/Apr/11 13:39,0.8.0 beta 2,,,Tool/nodetool,,,0,nodetool,,,,"---
            case SETCOMPACTIONTHROUGHPUT :
                if (arguments.length != 2) { badUse(""Missing value argument.""); }
                probe.setCompactionThroughput(Integer.valueOf(arguments[1]));
                break;
---

I would think arguments.length should be just 1?

",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-04-25 13:39:27.097,,,no_permission,,,,,,,,,,,,20692,,,Mon Apr 25 14:19:26 UTC 2011,,,,,,0|i0gc13:,93379,jbellis,jbellis,,,,,,,,,"25/Apr/11 13:39;jbellis;fixed in r1096479, thanks!","25/Apr/11 14:19;hudson;Integrated in Cassandra-0.8 #37 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/37/])
    fix nodetool setcompactionthroughput
patch by Terje Marthinussen; reviewed by jbellis for CASSANDRA-2550
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatacenterReadResolver not triggering repair,CASSANDRA-2556,12505139,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,stuhood,stuhood,25/Apr/11 21:39,12/Mar/19 14:03,13/Mar/19 22:26,28/Apr/11 13:37,0.7.6,0.8.0 beta 2,,,,,0,,,,,DatacenterReadResolver only calls maybeResolveForRepair for local reads.,,,,,,,,,,,,,,,,27/Apr/11 20:46;jbellis;2556.txt;https://issues.apache.org/jira/secure/attachment/12477580/2556.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-27 20:46:00.654,,,no_permission,,,,,,,,,,,,20695,,,Thu May 12 14:42:59 UTC 2011,,,,,,0|i0gc2f:,93385,slebresne,slebresne,,,,,,,,,"27/Apr/11 20:46;jbellis;patch that extracts the ""is this message one that should count towards blockfor"" logic into the waitingFor method; DatacenterReadCallback now only overrides those instead of all of the response methods, which fixes the bug and reduces the surface for introducing similar bugs in the future.  (applies on top of CASSANDRA-2552 v2.)","28/Apr/11 11:50;slebresne;+1
The patch applies on top of 0.8 (that is on top of CASSANDRA-2552 v2 for 0.8). Should be backported to 0.7 first I think.",28/Apr/11 13:37;jbellis;committed,"28/Apr/11 13:59;hudson;Integrated in Cassandra-0.7 #460 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/460/])
    trigger read repair correctly forLOCAL_QUORUM reads
patch by jbellis; reviewed by slebresne for CASSANDRA-2556
","12/May/11 14:42;jbellis;This also fixed a second bug: the old code checked for n == blockFor on LOCAL_QUORUM, so there was a race where if we exceeded blockFor with two responses coming in at nearly the same time, it would never test at the moment of equality and the coordinator would throw TOE even though enough responses were actually received.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.jar is not executable,CASSANDRA-2744,12509377,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,thobbs,thobbs,06/Jun/11 18:43,12/Mar/19 14:03,13/Mar/19 22:26,09/Jun/11 22:10,0.8.1,,,Legacy/Tools,,,0,,,,,"If you build stress.jar by running 'ant jar' from tools/stress/ and try to execute it with 'java -jar stress.jar', you get the following error:

{noformat}
Failed to load Main-Class manifest attribute from
stress.jar
{noformat}",,,,,,,,,,,,,,,,09/Jun/11 22:07;xedin;CASSANDRA-2744.patch;https://issues.apache.org/jira/secure/attachment/12481986/CASSANDRA-2744.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-06 21:04:39.8,,,no_permission,,,,,,,,,,,,20801,,,Thu Jun 09 23:41:07 UTC 2011,,,,,,0|i0gd73:,93568,brandon.williams,brandon.williams,,,,,,,,,"06/Jun/11 21:04;xedin;work branch: cassandra-0.8, the latest commit  504fb537fb15526a4558a94cfec04b8b7f2dc58e",09/Jun/11 22:07;xedin;jar is default task now.,09/Jun/11 22:10;brandon.williams;Committed.,"09/Jun/11 23:41;hudson;Integrated in Cassandra-0.8 #162 (See [https://builds.apache.org/job/Cassandra-0.8/162/])
    Stress.java creates a jar by default.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2744

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1134108
Files : 
* /cassandra/branches/cassandra-0.8/tools/stress/build.xml
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memory leak in CompactionManager's estimatedCompactions,CASSANDRA-2708,12508349,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dalaro,dalaro,dalaro,25/May/11 20:51,12/Mar/19 14:03,13/Mar/19 22:26,22/Aug/11 21:49,0.7.9,0.8.5,,,,,0,,,,,"CompactionManager's estimatedCompactions map seems to hold all or most ColumnFamilyStores in the system as keys.  Keys are never removed from estimatedCompactions.

I have a project that embeds Cassandra as a storage backend.  Some of my integration tests create and drop a single keyspace and pair of column families a hundred or 150 times in one JVM.  These tests always OOM'd.  Loading some near-death heapdumps in mat suggested CompactionManager's estimatedCompactions held over 80% of total heap via its ColumnFamilyStore keys.  estimatedCompactions had the only inbound reference to these CFSs, and the CFSs themselves had invalid = true.

As a workaround, I changed estimatedCompactions to a WeakReference-keyed map (using Guava MapMaker).  My integration tests no longer OOM.

I'm generally unfamiliar with Cassandra's guts.  I don't know whether weak referencing the keys of estimatedCompactions is correct (or ideal).  But, that did seem to confirm my guess that retained references to dead CFSs in estimatedCompactions were swamping my heap after lots of Keyspace+ColumnFamily drops.",,,,,,,,,,,,,,,,25/May/11 20:57;dalaro;cassandra-0.7-2708.txt;https://issues.apache.org/jira/secure/attachment/12480452/cassandra-0.7-2708.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-22 21:09:09.77,,,no_permission,,,,,,,,,,,,20781,,,Mon Aug 22 21:28:17 UTC 2011,,,,,,0|i0gcz3:,93532,jbellis,jbellis,,,,,,,,,25/May/11 20:57;dalaro;Attaching 3-line patch against cassandra-0.7 r1127675,"22/Aug/11 21:09;jbellis;Sorry Dan, I missed this when you first submitted it.  Committed for 0.7.9.  Thanks for the help!","22/Aug/11 21:28;hudson;Integrated in Cassandra-0.7 #540 (See [https://builds.apache.org/job/Cassandra-0.7/540/])
    avoid retaining references to dropped CFS objects in CompactionManager.estimatedCompactions
patch by Dan LaRocque; reviewed by jbellis for CASSANDRA-2708

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1160436
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/CompactionManager.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
duplicate rows returned from SELECT where KEY term is duplicated,CASSANDRA-2717,12508504,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jancona,amorton,amorton,27/May/11 02:48,12/Mar/19 14:03,13/Mar/19 22:26,27/Jul/11 20:04,0.8.3,,,,,,0,cql,lhf,,,"Noticed while working on CASSANDRA-2268 when random keys generated during a mutli_get test contain duplicate keys. 

The thrift multiget_slice() returns only the unique rows because of the map generated for the result. 

CQL will return a row for each KEY term in the SELECT. 

I could make QueryProcessor.getSlice() only create commands for the unique keys if we wanted to. 

Not sure it's a bug and it's definitely not something that should come up to often, reporting it because it's different to the thrift mutli_get operation. 

Happy to close if it's by design. 

",,,,,,,,,,,,,,,,09/Jul/11 13:19;jancona;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2717-Prevent-multiple-KEY-terms-properly-han.txt;https://issues.apache.org/jira/secure/attachment/12485815/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2717-Prevent-multiple-KEY-terms-properly-han.txt,19/Jul/11 02:01;jancona;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2717-Prevent-multiple-KEY-terms-properly-han.txt;https://issues.apache.org/jira/secure/attachment/12486957/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2717-Prevent-multiple-KEY-terms-properly-han.txt,19/Jul/11 02:01;jancona;ASF.LICENSE.NOT.GRANTED--v2-0002-Set-multiKey-state-from-within-grammar.txt;https://issues.apache.org/jira/secure/attachment/12486958/ASF.LICENSE.NOT.GRANTED--v2-0002-Set-multiKey-state-from-within-grammar.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-05-27 02:54:14.032,,,no_permission,,,,,,,,,,,,20786,,,Wed Jul 27 20:04:01 UTC 2011,,,,,,0|i0gd13:,93541,xedin,xedin,,,,,,,,,"27/May/11 02:54;jbellis;You mean when using OR?

A little confused b/c OR support was added to SELECT for 0.8.1 but this is tagged 0.8b2.","27/May/11 03:53;amorton;query was ""SELECT foo FROM my-cf WHERE KEY = 'bar' and KEY = 'bar';""

Nothing a sane person would do, I was just using the randomly generated keys for the stress test the same way the thrift based one does. 
","27/May/11 04:47;jbellis;Definitely a bug.

(KEY=X and KEY=Y should result in zero rows, unless X=Y in which case it should be one, assuming X exists.)","27/May/11 22:32;amorton;Ah, I sent a stupid query and it worked so I continued along. 

I'll try to dig into it this weekend. ","09/Jul/11 13:34;jancona;I added some tests to test/system/test_cql.py. I found that not only did ""WHERE KEY = 'bar' and KEY = 'bar'"" return two rows, so did ""WHERE KEY = 'bar' and KEY = 'baz'"" and ""WHERE KEY IN ('bar', 'bar')""

The attached patch makes having more than one ""KEY ="" clause be an error, and changes the List of keys in WhereClause to a Set. 

Jonathan mentioned that OR support was added, but I didn't see that in cassandra-0.8. Am I looking at the wrong branch? If so, this patch will have to be reworked, along with the logic in WhereClause.",18/Jul/11 21:27;xedin;I think we should set isMultiKey flag in the grammar (in whereClause statement) instead of depending on andKeyEquals(...) method.,19/Jul/11 02:02;jancona;Done.,19/Jul/11 11:00;xedin;+1,"19/Jul/11 16:14;jbellis;bq. Jonathan mentioned that OR support was added

I was probably thinking of CASSANDRA-2553.

Committed this fix, thanks Jim and Pavel!","19/Jul/11 18:04;hudson;Integrated in Cassandra-0.8 #224 (See [https://builds.apache.org/job/Cassandra-0.8/224/])
    CQL: include only one row per unique keyfor IN queries
patch by Jim Ancona; reviewed by pyaskevich for CASSANDRA-2717

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148425
Files : 
* /cassandra/branches/cassandra-0.8/test/system/test_cql.py
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/Term.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/Cql.g
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/WhereClause.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/SelectStatement.java
",27/Jul/11 20:04;jbellis;belatedly marking resolved,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyRecordWriter fails to throw a write exception encountered after the user begins to close the writer,CASSANDRA-2755,12509754,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,michaelsembwever,gregkatz,gregkatz,09/Jun/11 19:40,12/Mar/19 14:03,13/Mar/19 22:26,28/Jun/11 12:27,0.7.7,0.8.2,,,,,0,,,,,"There appears to be a race condition in {{ColumnFamilyRecordWriter}} that can result in the loss of an exception. Here is how it can happen (W stands for the {{RangeClient}}'s worker thread; U stands for the {{ColumnFamilyRecordWriter}} user's thread):

# W: {{RangeClient}}'s {{run}} method catches an exception originating in the Thrift client/socket, but doesn't get a chance to set it on the {{lastException}} field before it the thread is preempted.
# U: The user calls {{close}} which calls {{stopNicely}}. Because the {{lastException}} field is null, {{stopNicely}} does not throw anything. {{close}} then joins on the worker thread.
# W: The {{RangeClient}}'s {{run}} method sets the {{lastException}} field and exits.
# U: Although the thread in {{close}} is waiting for the worker thread to exit, it has already checked the {{lastException}} field so it doesn't detect the presence of the last exception. Instead, {{close}} returns without throwing anything.

This race condition means that intermittently write failures will go undetected.",,,,,,,,,,,,,,,,13/Jun/11 15:12;jbellis;2755-v2.txt;https://issues.apache.org/jira/secure/attachment/12482343/2755-v2.txt,12/Jun/11 10:28;michaelsembwever;CASSANDRA-2755.patch;https://issues.apache.org/jira/secure/attachment/12482295/CASSANDRA-2755.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-12 10:28:36.679,,,no_permission,,,,,,,,,,,,20807,,,Tue Jun 28 13:24:34 UTC 2011,,,,,,0|i0gd9b:,93578,jbellis,jbellis,,,,,,,,,"12/Jun/11 10:28;michaelsembwever;In RangeClient i cannot see why close() needs to be called before lastException is assigned. The following patch should work: I have tested it against various jobs but i have no reproducible testcase to confirm this bug against.

Also in the patch is a slight cleanup to ColumnFamilyRecordWriter's close() methods: keeping implementation out of deprecated methods.","13/Jun/11 15:12;jbellis;It looks to me that as long as we check for the exception before calling join, there will be a window to miss one.

v2 encapsulates RangeClient.close better to avoid this.","13/Jun/11 15:46;michaelsembwever;The check for the exception also occurs in ColumnFamilyRecordWriter.write(buf, value) -> RangeClient.put(pair)
Isn't it possible the put(..) is being called while the RangeClient thread is inside close() ?
(isn't write(..) called more often than close() ?)

For this reason inside RangeClient.run() i assigned lastException before calling close()","13/Jun/11 15:54;jbellis;bq. Isn't it possible the put(..) is being called while the RangeClient thread is inside close?

old close, new closeInternal?

Yes, but I don't see how that changes things.  I.e., it's always possible that the last put() will happen before an exception is set; hence, the extra check on close.","13/Jun/11 18:08;michaelsembwever;bq. it's always possible that the last put() will happen before an exception is set; hence, the extra check on close.
Quite right. ",28/Jun/11 11:01;michaelsembwever;Jonathan: Is your patch being applied?,"28/Jun/11 12:01;jbellis;waiting for a +1, wasn't clear if your last comment was intended that way.",28/Jun/11 12:10;michaelsembwever;Yes it was a +1,"28/Jun/11 12:27;jbellis;committed, thanks!","28/Jun/11 13:24;hudson;Integrated in Cassandra-0.7 #515 (See [https://builds.apache.org/job/Cassandra-0.7/515/])
    fix race that could result in Hadoopwriter failing to throw exception for encountered error
patch by Mck SembWever and jbellis for CASSANDRA-2755

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1140565
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool repair never finishes. Loops forever through merkle trees?,CASSANDRA-2758,12509829,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,terjem,terjem,10/Jun/11 13:39,12/Mar/19 14:03,13/Mar/19 22:26,15/Jun/11 11:55,0.8.1,,,,,,0,,,,,"I am not sure all steps here is needed, but as part of testing something else, I set up
node1: initial_token: 1
node2: initial_token: 5

Then:
{noformat}
create keyspace myks 
 with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
 with strategy_options = [{ replication_factor:2 }];

use myks;

create column family test with comparator = AsciiType and column_metadata=[ {column_name: 'up_', validation_class: LongType, index_type: 0}, {column_name: 'del_', validation_class: LongType, index_type: 0} ]
 and keys_cached = 100000 and rows_cached = 10000 and min_compaction_threshold = 2;
quit;
{noformat}

Doing nodetool repair after this gets both nodes busy looping forever.

A quick look at one node in eclipse makes me guess its having fun spinning through  merkle trees, but I have to admit I have not look at it for a long time.





",,,,,,,,,,,,,,,,14/Jun/11 09:58;slebresne;0001-Fix-MerkleTree.init-to-not-create-non-sensical-trees.patch;https://issues.apache.org/jira/secure/attachment/12482537/0001-Fix-MerkleTree.init-to-not-create-non-sensical-trees.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-14 09:58:30.031,,,no_permission,,,,,,,,,,,,20809,,,Fri Jun 17 02:20:45 UTC 2011,,,,,,0|i0gd9z:,93581,stuhood,stuhood,,,,,,,,,"14/Jun/11 09:58;slebresne;MerkleTree.init(), which is used to create the merkle tree in case there is no data, was creating a nonsensical tree by stopping it's iteration too late.

Attached patch to fix (and dumping priority to minor because it has very little chance to hit anyone in any real-life situation).","14/Jun/11 20:15;stuhood;+1, thanks!","15/Jun/11 11:55;slebresne;Committed, thanks","17/Jun/11 02:20;hudson;Integrated in Cassandra-0.8 #173 (See [https://builds.apache.org/job/Cassandra-0.8/173/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in sstable2json,CASSANDRA-2760,12509854,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,daningaddr,daningaddr,10/Jun/11 17:48,12/Mar/19 14:03,13/Mar/19 22:26,27/Jun/12 00:53,1.1.2,,,Legacy/Tools,,,0,,,,,"./sstable2json /var/lib/cassandra/data/test/snapshots/1307649033076/User-g-4-Data.db 
{
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamily.<init>(ColumnFamily.java:82)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:70)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:142)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:90)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:74)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:179)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:144)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:136)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:313)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:344)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:357)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:415)
",,,,,,,,,,,,,,,,23/Jun/12 00:06;jasobrown;0001-cassandra-2760.patch;https://issues.apache.org/jira/secure/attachment/12533139/0001-cassandra-2760.patch,10/Jun/11 17:51;daningaddr;User-g-4-Data.db;https://issues.apache.org/jira/secure/attachment/12482083/User-g-4-Data.db,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-10 17:57:45.174,,,no_permission,,,,,,,,,,,,20811,,,Wed Jun 27 00:53:39 UTC 2012,,,,,,0|i0gdaf:,93583,jbellis,jbellis,,,,,,,,,10/Jun/11 17:57;jbellis;Sounds like you're trying to run sstable2json on a columnfamily that isn't present in your schema.,"23/Jun/12 00:06;jasobrown;Instead of trying to process an unknown column family (and exiting with an exception), load the Descriptor at the top of the export process and check to see if the keyspace/column family exists inside the current cassandra setup. If not, print a warning message and exit.","23/Jun/12 00:07;jasobrown;Note: I worked on this patch against the current (1.2) trunk. I can backport it to earlier versions, if we need.","27/Jun/12 00:53;jbellis;patch applies cleanly to 1.1, so committed there as well.  thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Token cannot contain comma (possibly non-alpha/non-numeric too?) in OrderPreservingPartitioner,CASSANDRA-2762,12509893,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,cywjackson,cywjackson,11/Jun/11 01:54,12/Mar/19 14:03,13/Mar/19 22:26,06/Jul/11 17:11,0.7.7,0.8.2,,,,,0,,,,,"It'd appear that when the token contain comma in the OrderPreservingPartitioner case, C* will fail with assert error:

ERROR [GossipStage:1] 2011-06-09 16:01:05,063 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[GossipStage:1,5,main]
java.lang.AssertionError
    at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:685)
    at org.apache.cassandra.service.StorageService.onChange(StorageService.java:648)
    at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:772)
    at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:737)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:679)
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:60)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
",,,,,,,,,,,,,,,,23/Jun/11 15:51;jbellis;2762-v2.txt;https://issues.apache.org/jira/secure/attachment/12483612/2762-v2.txt,20/Jun/11 21:40;jbellis;2762.txt;https://issues.apache.org/jira/secure/attachment/12483212/2762.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-11 12:52:02.139,,,no_permission,,,,,,,,,,,,20813,,,Wed Jul 06 16:14:36 UTC 2011,,,,,,0|i0gdav:,93585,slebresne,slebresne,,,,,,,,,"11/Jun/11 12:52;jbellis;{code}
    // this must be a char that cannot be present in any token
    public final static char DELIMITER = ',';
    public final static String DELIMITER_STR = new String(new char[] { DELIMITER });
{code}

Changing to something more obscure would be a backwards-incompatible change. Not worth it.","16/Jun/11 03:54;jbellis;However we should make sure that we fail-fast invalid initial_token setting, as well as do not hand out bad tokens during auto bootstrap token selection","20/Jun/11 21:40;jbellis;patch adds TokenFactory.validate, calls it on initial_token or move input, and adds code to prevent commas in autogenerated tokens","23/Jun/11 15:51;jbellis;v2 does token hacking post-midpoint instead of in midpoint itself, which is confusing (and unnecessary for merkle tree uses of midpoint which is the main one).","06/Jul/11 15:15;slebresne;nit: the comment "" // Hack to prevent giving nodes tokens with strings in them"" should probably read "" // Hack to prevent giving nodes tokens with DELIMITER_STR in them""?

+1","06/Jul/11 15:57;jbellis;committed w/ that change.

also added a final check post-replaceall:

{code}
            if (tokenMetadata_.getTokenToEndpointMap().containsKey(token))
                throw new RuntimeException(""Unable to compute unique token for new node -- specify one manually with initial_token"");
{code}","06/Jul/11 15:59;jbellis;I also note for the record that if you must use an ordered partitioner, you should almost certainly be using ByteOrderedPartitioner instead of the obsolete OPP.","06/Jul/11 16:14;hudson;Integrated in Cassandra-0.7 #524 (See [https://builds.apache.org/job/Cassandra-0.7/524/])
    ensure that string tokens do not contain commas
patch by jbellis; reviewed by slebresne for CASSANDRA-2762

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1143476
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/tools/NodeProbe.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/dht/Token.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/StorageServiceMBean.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/dht/RandomPartitioner.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/dht/AbstractByteOrderedPartitioner.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java agent option missing in cassandra.bat file,CASSANDRA-2787,12510656,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,kochen,kochen,kochen,17/Jun/11 12:49,12/Mar/19 14:03,13/Mar/19 22:26,20/Jun/11 16:42,0.8.1,,,Packaging,,,0,,,,,"This option must be included in cassandra.bat:

-javaagent:%CASSANDRA_HOME%\lib\jamm-0.2.2.jar

Otherwise you see the following warnings in cassandra log:

WARN 12:02:32,478 MemoryMeter uninitialized (jamm not specified as java agent); assuming liveRatio of 10.0. Usually this means cassandra-env.sh disabled jamm because you are using a buggy JRE; upgrade to the Sun JRE instead
",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-06-18 19:15:37.315,,,no_permission,,,,,,,,,,,,20829,,,Mon Jun 20 19:20:28 UTC 2011,,,,,,0|i0gdgf:,93610,jbellis,jbellis,,,,,,,,,"18/Jun/11 19:15;jbellis;Really, forward slashes?",20/Jun/11 09:12;kochen;Changed them to backslashes,"20/Jun/11 16:42;jbellis;committed, thanks!","20/Jun/11 19:20;hudson;Integrated in Cassandra-0.8 #178 (See [https://builds.apache.org/job/Cassandra-0.8/178/])
    add jamm agent to cassandra.bat
patch by rene kochen; reviewed by jbellis for CASSANDRA-2787

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1137695
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/bin/cassandra.bat
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OPP#describeOwnership reports incorrect ownership,CASSANDRA-2800,12511003,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,20/Jun/11 21:07,12/Mar/19 14:03,13/Mar/19 22:26,06/Jul/11 14:40,0.7.7,0.8.2,,,,,0,,,,,"OPP#describeOwnership relies on StorageService#getSplits and counts the received tokens as its basis of ownership.

When the number of result keys is less than the number of splits, the full count is omitted (to save work?). However, we don't care if a split would end up fractional in this case, we just need the full count.

The logic here is:
{code}
int splits = keycount * DatabaseDescriptor.getIndexInterval() / keysPerSplit;
if (keycount >= splits) { ... add count to result set }
{code}
We were passing in 1 key per split (since we just care about the count), but splits=keycount*IndexInterval is guaranteed to be > keycount, so the result set is not completely formed.
The better ""unit keysPerSplit"" to use is IndexInterval itself, which gives splits=keycount*II/II=keycount, so the logic runs correctly.",,,,,,,,,,,,,,,,20/Jun/11 21:08;jhermes;2800.txt;https://issues.apache.org/jira/secure/attachment/12483208/2800.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-06 14:40:05.383,,,no_permission,,,,,,,,,,,,20838,,,Wed Jul 06 14:53:13 UTC 2011,,,,,,0|i0gdjb:,93623,slebresne,slebresne,,,,,,,,,"06/Jul/11 14:40;slebresne;+1
Committed, thanks","06/Jul/11 14:53;hudson;Integrated in Cassandra-0.7 #522 (See [https://builds.apache.org/job/Cassandra-0.7/522/])
    Fix describeOwnership for OPP
patch by jhermes; reviewed by slebresne for CASSANDRA-2800

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1143437
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up mbeans that return Internal Cassandra types,CASSANDRA-2805,12511159,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,21/Jun/11 20:46,12/Mar/19 14:03,13/Mar/19 22:26,31/Dec/11 16:19,1.1.0,,,,,,0,lhf,,,,"We need to clean up wherever we return internal cassandra objects over jmx. Namely CompactionInfo objects as well as Tokens. There may be a few other examples.

This is bad for two reasons

1. You have to load the cassandra jar when querying these mbeans, which sucks.
2. Stuff breaks between versions when things are moved. For example, CASSANDRA-1610 moves the compaction related classes around. Any code querying those jmx mbeans in 0.8.0 is now broken in 0.8.2. (assuming those moves stay in the 0.8 branch)

For things like CompactionInfo we should just expose more mbean methods or serialize to something standard like json.

I'd like to target this for 0.8.2. Since we've already broken compatibility between 0.8.0 and 0.8.1, I'd say just fix this everywhere now.",,,,,,,,,,,,,,,,30/Dec/11 21:06;nickmbailey;0001-Don-t-return-internal-types-over-jmx.patch;https://issues.apache.org/jira/secure/attachment/12508954/0001-Don-t-return-internal-types-over-jmx.patch,29/Dec/11 19:48;nickmbailey;0001-Don-t-return-internal-types-over-jmx.patch;https://issues.apache.org/jira/secure/attachment/12508879/0001-Don-t-return-internal-types-over-jmx.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-02 23:20:52.444,,,no_permission,,,,,,,,,,,,1929,,,Sat Dec 31 16:19:08 UTC 2011,,,,,,0|i0gdkf:,93628,yukim,yukim,,,,,,,,,"02/Jul/11 23:20;jbellis;If you can list which methods to clean up, this would be a good ticket for the surprisingly frequently asked, ""What's a good place to get my feet wet in the Cassandra code?""","05/Jul/11 13:37;nickmbailey;Off the top of my head:

 * A CompactionInfo object is returned when getting compactions from the CompactionManager mbean.
 * Sub-types of Token objects are returned when calling getTokenToEndpointMap (StorageService mbean i think).

I'll see if i can't look around for any more.",05/Jul/11 15:38;appodictic;Maybe the JMX CompositeType could be returned instead of JSON? ,"05/Jul/11 15:49;nickmbailey;Didn't know that existed. I guess it is specifically designed to address this kind of problem?

A quick look makes it seem overly complicated and verbose. If we want to avoid json we can still allow for more complicated types like Map<String> and whatnot. The main problem is just having to include the cassandra jar when querying jmx. From what I can tell it's basically impossible to do jmx-type stuff on a non-jvm language anyway, so maybe we don't really need json.","05/Jul/11 17:53;appodictic;CompositeData is a JMX type that holds a key value map of other JMX types and can be nested. 

http://docs.jboss.org/jbossas/javadoc/4.0.1-sp1/jmx/javax/management/openmbean/CompositeDataSupport.html

This should allows us to return complex objects over JMX without having to resort to JSON serializing things.
","06/Jul/11 22:38;gauravsh;Unless someone else has already started, I am planning to take this one up.","07/Jul/11 16:23;nickmbailey;Ed,

I'm not sure I see the advantage of a JMX Composite type as opposed to using something like a Map<String, Int> approach in any place that needs 'complex' types. ","15/Jul/11 16:28;gauravsh;Based on my research so far scanning the MBean's and their internal users (NodeProbe, NodeCmd and CliClient), there are 4 Cassandra-type dependencies: CompactionInfo, CompactionType, Token, Range. Addressing them individually and discussing my plan:

1. CompactionInfo/CompactionType
Now, CompactionInfo/CompactionType are manageable with a Map as suggested but Range and Token are a bit tightly coupled and more involved.

2. Range
Since Range already has the partitioner (either injected or implicit from StorageService), I believe I can add 2 new constructors that look like:
    public Range(String left, String right)
    public Range(String left, String right, IPartitioner partitioner)

and use the partioner.getTokenFactory().fromString() to curate the left and right Token's.

Also, to replace the StorageServiceMBean's:
    public Map<Range, List<String>> getRangeToEndpointMap(String keyspace);
    public Map<Range, List<String>> getPendingRangeToEndpointMap(String keyspace);

based on their usages, the Range in StorageService can be safely copied to something like a Pair/Tuple.

I noticed that the getRangeToAddressMap() is not exposed on the StorageServiceMBean interface - is that by design (not that I am complaining because right now, it is 1 less dependency to decouple but if it is an omission, I need to account for it)?

3. Token
I can change all MBean interfaces that need a Token to the corresponding String representation using partitioner.getTokenFactory().toString() and then reconstruct back using the fromString()","17/Jul/11 18:32;nickmbailey;I'm not sure you need to add any constructors to Range. How about just an asPair() method or something similar that returns the tokens that make up the range converted to strings?

Everything else looks fine. getRangeToAddressMap() isn't needed because getRangeToEndpointMap() is exposed.

Another thing that might be nice to fix here is the getNaturalEndpoints() method. It currently only takes a byte array or byteBuffer object which makes it impossible to call from something like jconsole. It would be nice to overload that with another method that takes a string as the key so you can call it from jconsole.","26/Aug/11 22:01;jbellis;Guarav, are you still working on this?",02/Sep/11 21:30;nickmbailey;I'm probably going to take this and try and get it done quickly before the 1.0 freeze. I'd really like to solve this problem before 1.0 and then be strict on preventing this in future mbean methods.,"11/Oct/11 19:52;nickmbailey;Obviously I didn't get this done before the 1.0 freeze. And of course, this breaks between 0.8 and 1.0 since the serial version of CompactionInfo changes.

Can we target this for 1.0.1?",12/Oct/11 01:51;jbellis;I don't think we should break compatibility in a minor release.,"12/Oct/11 02:01;nickmbailey;I agree my main argument though is that we have a history of doing it unknowingly. Especially something like this where any update the the CompactionInfo requires updating the serialization version and then breaks compatibility.

I suppose I can live with this in 1.1 and a 'let's try really really really hard not to break compatibility in the 1.0.x releases.",29/Dec/11 19:48;nickmbailey;Should take care of any instances where internal types are returned over jmx.,"30/Dec/11 20:10;yukim;Nick,

- I think double brace initialization should be avoided at CompactionInfo#asMap. (yeah, I know Java syntax is sucks.)
- I prefer {{Integer/Long.toString(val)}} over {{new Integer/Long(val).toString()}}.

but, otherwise +1.","30/Dec/11 21:06;nickmbailey;Alright removed the double brace :(, and updated to use static toString.",30/Dec/11 22:22;yukim;+1,31/Dec/11 16:19;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json needs to escape quotes,CASSANDRA-2780,12510538,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,tcn,tcn,16/Jun/11 07:49,12/Mar/19 14:03,13/Mar/19 22:26,16/Jun/11 20:04,0.8.2,,,,,,1,,,,,"[default@foo] set transactions[test][data]='{""foo"":""bar""}'; 

$ cat /tmp/json
{
""74657374"": [[""data"", ""{""foo"":""bar""}"", 1308209845388000]]
}

$ ./json2sstable -s -c transactions -K foo /tmp/json /tmp/ss-g-1-Data.db
Counting keys to import, please wait... (NOTE: to skip this use -n <num_keys>)
org.codehaus.jackson.JsonParseException: Unexpected character ('f' (code 102)): was expecting comma to separate ARRAY entries
 at [Source: /tmp/json2; line: 2, column: 27]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:929)
	at org.codehaus.jackson.impl.JsonParserBase._reportError(JsonParserBase.java:632)
	at org.codehaus.jackson.impl.JsonParserBase._reportUnexpectedChar(JsonParserBase.java:565)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:128)
	at org.codehaus.jackson.impl.JsonParserBase.skipChildren(JsonParserBase.java:263)
	at org.apache.cassandra.tools.SSTableImport.importSorted(SSTableImport.java:328)
	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:252)
	at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:476)
ERROR: Unexpected character ('f' (code 102)): was expecting comma to separate ARRAY entries
 at [Source: /tmp/json2; line: 2, column: 27]

http://www.mail-archive.com/user@cassandra.apache.org/msg14257.html
",,,,,,,,,,,,,,,,17/Jun/11 18:48;xedin;CASSANDRA-2780-v2.patch;https://issues.apache.org/jira/secure/attachment/12482970/CASSANDRA-2780-v2.patch,16/Jun/11 16:16;xedin;CASSANDRA-2780.patch;https://issues.apache.org/jira/secure/attachment/12482812/CASSANDRA-2780.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-16 20:04:03.049,,,no_permission,,,,,,,,,,,,20824,,,Mon Jun 20 10:08:41 UTC 2011,,,,,,0|i0gdev:,93603,brandon.williams,brandon.williams,,,,,,,,,"16/Jun/11 08:09;tcn;Fix is easy:

$ cat /tmp/json 
{
""74657374"": [[""data"", ""{\""foo\"":\""bar\""}"", 1308209845388000]]
}

$ ./json2sstable -s -c transactions -K foo /tmp/json /var/lib/cassandra/data/foo/transactions-g-1-Data.db
Counting keys to import, please wait... (NOTE: to skip this use -n <num_keys>)
Importing 1 keys...
1 keys imported successfully.

[default@foo] get transactions[test][data];
=> (column=data, value={""foo"":""bar""}, timestamp=1308209845388000)",16/Jun/11 20:04;brandon.williams;Committed.,"16/Jun/11 23:38;jbellis;Tatu's feedback:

{quote}
The best way to handle escaping/quoting is to use tools that deal with the format -- this patch only handles escaping of double-quotes, but similar problems would occur with linefeeds, or backslashes.

So using a JSON generator / writer would make sense, if project uses one already? If not, I would recommend adding handling of backslashes and white space other than space (char codes below 32), as those must be escaped in String values and keys.

Also: if unquoted keys were required (not valid JSON, but there's lots of data like that...), Jackson can be configured to accept unquoted field names. That has its own potential issues (whether escaping is allowed, which characters are legal in unquotes names), but appears to work well enough that users haven't complained.
{quote}","16/Jun/11 23:42;brandon.williams;{quote}
So using a JSON generator / writer would make sense, if project uses one already? If not, I would recommend adding handling of backslashes and white space other than space (char codes below 32), as those must be escaped in String values and keys.
{quote}

We don't use one.  I believe the historical reason is that one couldn't be found that would handle things incrementally, but I think that may have changed by now (sst2j is pretty long in the tooth) and we should probably switch to one if possible.","17/Jun/11 02:20;hudson;Integrated in Cassandra-0.8 #173 (See [https://builds.apache.org/job/Cassandra-0.8/173/])
    sstable2json escapes quotes.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2780

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1136637
Files : 
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/SchemaLoader.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/SSTableExport.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
","17/Jun/11 08:03;tcn;The patch will replace existing \"" with \\"" (backslash backslash quote) which may not be the desired behaviour.

Alternative (regex should be precompiled, of course):
String.format(""\""%s\"""", val.replaceAll(""(?<!\\\\)\"""", ""\\\\\""""));
 ",17/Jun/11 09:47;xedin;changed replace with pattern/matcher with your regex in the escapeQuotes method. thanks!,"17/Jun/11 14:51;tcn;D'oh, actually an existing \"" must be replaced with \\\"" :-\",17/Jun/11 14:56;xedin;Can you please post a correct regex so i can include it to patch?,17/Jun/11 15:08;jbellis;Why are we talking about regexes instead of taking Tatu's advice?,"17/Jun/11 15:13;xedin;Agreed, I see now that would be the best option. Will be working at that direction.","17/Jun/11 18:48;xedin;uses ObjectMapper.writeValue(PrintStream, Object) to serialize keys and columns to JSON instead of using regexs.","17/Jun/11 19:20;brandon.williams;Committed v2, thanks!","17/Jun/11 20:27;hudson;Integrated in Cassandra-0.8 #175 (See [https://builds.apache.org/job/Cassandra-0.8/175/])
    Use jackson's ObjectMapper instead of hand-crafting json in sstable2json.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2780

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1136991
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/SSTableExport.java
","20/Jun/11 10:08;tcn;Sure, using a real serializer is always better (but usually somewhat slower). Works nicely. Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix build for removal of commons-collections,CASSANDRA-2784,12510617,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,16/Jun/11 20:42,12/Mar/19 14:03,13/Mar/19 22:26,16/Jun/11 21:04,1.0.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,16/Jun/11 20:53;stuhood;0001-Remove-references-to-the-commons.collections-package.txt;https://issues.apache.org/jira/secure/attachment/12482858/0001-Remove-references-to-the-commons.collections-package.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-16 21:04:13.738,,,no_permission,,,,,,,,,,,,20826,,,Thu Jun 16 21:04:13 UTC 2011,,,,,,0|i0gdfr:,93607,jbellis,jbellis,,,,,,,,,16/Jun/11 20:53;stuhood;0001 Removes the last references to the commons.collections package.,"16/Jun/11 21:04;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI remove ascii column,CASSANDRA-2821,12511478,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,sdolgy,sdolgy,24/Jun/11 14:44,12/Mar/19 14:03,13/Mar/19 22:26,15/Jul/11 17:34,0.7.8,0.8.2,,Legacy/Tools,,,0,,,,,"[default@sdo] incr counters[ascii('EU')][ascii('null')];
Value incremented.
[default@sdo] list counters;
Using default limit of 100
-------------------
RowKey: 4555
=> (counter=6e756c6c, value=1)

1 Row Returned.
[default@sdo] del counters[ascii('EU')][ascii('null')];
org.apache.cassandra.db.marshal.MarshalException: cannot parse
'FUNCTION_CALL' as hex bytes
[default@sdo]

Suggested workaround, although not tested:

assume counters comparator as bytes;
del counters['EU'][0];",,,,,,,,,,,,,,,,15/Jul/11 14:19;xedin;CASSANDRA-2821-0.7.patch;https://issues.apache.org/jira/secure/attachment/12486622/CASSANDRA-2821-0.7.patch,15/Jul/11 14:19;xedin;CASSANDRA-2821-0.8.patch;https://issues.apache.org/jira/secure/attachment/12486621/CASSANDRA-2821-0.8.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-24 14:49:15.369,,,no_permission,,,,,,,,,,,,20848,,,Fri Jul 15 21:38:25 UTC 2011,,,,,,0|i0gdnz:,93644,brandon.williams,brandon.williams,,,,,,,,,"24/Jun/11 14:49;jbellis;Edit to clarify there is nothing magic about string 'null'.

Probably affects 0.7 too?","15/Jul/11 14:19;xedin;Proper function support and key validation (0.8 only) for DEL command.

Both rebased with latest versions of their branches.

",15/Jul/11 17:34;brandon.williams;Committed.,"15/Jul/11 20:32;hudson;Integrated in Cassandra-0.7 #529 (See [https://builds.apache.org/job/Cassandra-0.7/529/])
    Fix column deletion in the cli.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2821

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147258
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/branches/cassandra-0.7/test/unit/org/apache/cassandra/cli/CliTest.java
","15/Jul/11 21:38;hudson;Integrated in Cassandra-0.8 #216 (See [https://builds.apache.org/job/Cassandra-0.8/216/])
    Proper function support and key validation for cli deletes.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2821

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147259
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose number of threads blocked on submitting a memtable for flush,CASSANDRA-2817,12511377,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Jun/11 16:11,12/Mar/19 14:03,13/Mar/19 22:26,23/Jun/11 17:17,0.7.7,0.8.2,,,,,0,,,,,"Writes can be blocked by a thread trying to submit a memtable while the flush queue is full. While this is the expected behavior (the goal being to prevent OOMing), it is worth exposing when that happens so that people can monitor it and modify settings accordingly if that happens too often.",,,,,,,,,,,,,,,,23/Jun/11 17:01;slebresne;0001-Expose-threads-blocked-on-submission-to-executor-v2.patch;https://issues.apache.org/jira/secure/attachment/12483620/0001-Expose-threads-blocked-on-submission-to-executor-v2.patch,23/Jun/11 16:12;slebresne;0001-Expose-threads-blocked-on-submission-to-executor.patch;https://issues.apache.org/jira/secure/attachment/12483615/0001-Expose-threads-blocked-on-submission-to-executor.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-23 16:18:29.524,,,no_permission,,,,,,,,,,,,20846,,,Thu Jun 23 17:28:45 UTC 2011,,,,,,0|i0gdn3:,93640,jbellis,jbellis,,,,,,,,,23/Jun/11 16:18;kingryan;+1,"23/Jun/11 16:18;jbellis;Very clever solution, I like it.

Can you add ""currently blocked"" to statuslogger and nodetool tpstats?",23/Jun/11 17:01;slebresne;Attaching v2 with support in StatusLogger and tpstats.,23/Jun/11 17:03;jbellis;+1,"23/Jun/11 17:17;slebresne;Committed, thanks","23/Jun/11 17:28;hudson;Integrated in Cassandra-0.7 #510 (See [https://builds.apache.org/job/Cassandra-0.7/510/])
    Expose number of threads blocked on submitting a memtable for flush
patch by slebresne; reviewed by jbellis for CASSANDRA-2817

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1138996
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/concurrent/JMXEnabledThreadPoolExecutorMBean.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/utils/StatusLogger.java
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/concurrent/JMXEnabledThreadPoolExecutor.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/tools/NodeProbe.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/tools/NodeCmd.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In the Cli, update column family <cf> with comparator; create Column metadata",CASSANDRA-2809,12511233,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,silvere,silvere,22/Jun/11 12:54,12/Mar/19 14:03,13/Mar/19 22:26,17/Jul/11 14:13,0.7.8,0.8.2,,Legacy/Tools,,,0,,,,,"Using cassandra-cli, I can't update the comparator of a column family with the type I want and when I did it with BytesType, Column metadata appear for each of my existing columns.
Step to reproduce:
{code}
[default@unknown] create keyspace Test
    with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
    and strategy_options = [{replication_factor:1}];

[default@unknown] use Test;
Authenticated to keyspace: Test

[default@Test] create column family test;

[default@Test] describe keyspace;
...
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
...

[default@Test] update column family test with comparator = 'LongType';
comparators do not match.
{code}
why?? the CF is empty
{code}
[default@Test] update column family test with comparator = 'BytesType';
f8e4dcb0-9cca-11e0-0000-d0583497e7ff
Waiting for schema agreement...
... schemas agree across the cluster

[default@Test] describe keyspace;
...
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
...

[default@Test] set test[ascii('row1')][long(1)]=integer(35);
set test[ascii('row1')][long(2)]=integer(36);
set test[ascii('row1')][long(3)]=integer(38);
set test[ascii('row2')][long(1)]=integer(45);
set test[ascii('row2')][long(2)]=integer(42);
set test[ascii('row2')][long(3)]=integer(33);

[default@Test] list test;
Using default limit of 100
-------------------
RowKey: 726f7731
=> (column=0000000000000001, value=35, timestamp=1308744931122000)
=> (column=0000000000000002, value=36, timestamp=1308744931124000)
=> (column=0000000000000003, value=38, timestamp=1308744931125000)
-------------------
RowKey: 726f7732
=> (column=0000000000000001, value=45, timestamp=1308744931127000)
=> (column=0000000000000002, value=42, timestamp=1308744931128000)
=> (column=0000000000000003, value=33, timestamp=1308744932722000)

2 Rows Returned.

[default@Test] update column family test with comparator = 'LongType';
comparators do not match.
{code}
same question than before, my columns contains only long, why I can't?

{code}
[default@Test] update column family test with comparator = 'BytesType';

[default@Test] describe keyspace;                                      
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name:  (0000000000000001)
          Validation Class: org.apache.cassandra.db.marshal.IntegerType
        Column Name:  (0000000000000003)
          Validation Class: org.apache.cassandra.db.marshal.IntegerType
        Column Name:  (0000000000000002)
          Validation Class: org.apache.cassandra.db.marshal.IntegerType
{code}
Column Metadata appear from nowhere. I don't think that it's expected.
","Ubuntu 10.10, 32bit
java version ""1.6.0_24""
installed from Debian packages of Brisk-beta2",,,,,,,,,,,,,,,22/Jun/11 19:43;jbellis;2809-validate.txt;https://issues.apache.org/jira/secure/attachment/12483492/2809-validate.txt,15/Jul/11 22:13;xedin;CASSANDRA-2809.patch;https://issues.apache.org/jira/secure/attachment/12486692/CASSANDRA-2809.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-22 19:42:48.961,,,no_permission,,,,,,,,,,,,20840,,,Sun Jul 17 14:30:59 UTC 2011,,,,,,0|i0gdlb:,93632,jbellis,jbellis,,,,,,,,,"22/Jun/11 19:42;jbellis;Changing comparators is not allowed, since the point of a comparator is that data within a row will be sorted on disk by the comparator's ordering.  Changing the comparator without rewriting the data would corrupt the sstable.

Not sure where that column_metadata comes from though.  Looks like a bug.",22/Jun/11 19:43;jbellis;Noticed that update_cf doesn't validate the given CFMetaData.  patch to add this.  (Does not address the column_metadata problem.),"15/Jul/11 12:55;xedin;That column metadata is a feature: when you use a function call in SET statement it is storing information about validation class for individual columns locally (without sync with server) that was added in context of CASSANDRA-1635. 

+1 of validate patch.",15/Jul/11 17:42;brandon.williams;Committed.,"15/Jul/11 21:19;jbellis;bq. That column metadata is a feature: when you use a function call in SET statement it is storing information about validation class for individual columns locally

Ah, I remember that.  But this shouldn't show up in a describe keyspace or we cause the above confusion. :)",15/Jul/11 21:26;xedin;How should it be stored on your opinion?,"15/Jul/11 21:29;jbellis;I'd probably keep a ""local"" CFMetadata around and check that first.","15/Jul/11 21:38;hudson;Integrated in Cassandra-0.8 #216 (See [https://builds.apache.org/job/Cassandra-0.8/216/])
    cli validates CFMetaData on update.
Patch by jbellis, reviewed by Pavel Yaskevich for CASSANDRA-2809

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147261
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
","15/Jul/11 21:46;xedin;I took a look the code now and what I see is that describe keyspace always calling `thrift.describe_keyspace()` but in the example after adding rows you did a successful ""update column family test with comparator = 'BytesType';"" which persisted changes in column metadata. I think this is actually a good thing to have we just need to document that, what do you think, Jonathan?","15/Jul/11 21:53;jbellis;To make sure we're on the same page, here's what I'm talking about:

{noformat}
      Column Metadata:
        Column Name:  (0000000000000001)
          Validation Class: org.apache.cassandra.db.marshal.IntegerType
        Column Name:  (0000000000000003)
{noformat}

We shouldn't be persisting that or showing it to the user as part of describe keyspace.","15/Jul/11 21:54;xedin;Ok, I will attach a patch fixing this in a few.",15/Jul/11 22:13;xedin;patch for version 0.7 but can be applied on 0.8 also.,"17/Jul/11 14:03;jbellis;does this fix the describe problem too, or just the update one?",17/Jul/11 14:05;xedin;Describe too.,17/Jul/11 14:13;jbellis;committed,"17/Jul/11 14:30;hudson;Integrated in Cassandra-0.7 #530 (See [https://builds.apache.org/job/Cassandra-0.7/530/])
    avoid including inferred types in CFupdate
patch by pyaskevich; reviewed by jbellis for CASSANDRA-2809

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147621
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/cli/CliClient.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0.8.0 is unable to participate with nodes using a _newer_ protocol version,CASSANDRA-2818,12511410,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,mallen,mallen,23/Jun/11 21:22,12/Mar/19 14:03,13/Mar/19 22:26,28/Jun/11 17:50,0.8.2,,,,,,0,,,,,"When a 0.8.1 node tries to join a 0.8.0 ring, we see an endless supply of these in system.log:

INFO [Thread-4] 2011-06-23 21:14:04,149 IncomingTcpConnection.java (line 103) Received connection from newer protocol version. Ignorning message.

and the node never joins the ring.",,,,,,,,,,,,,,,,24/Jun/11 16:42;jbellis;2818-disconnect.txt;https://issues.apache.org/jira/secure/attachment/12483733/2818-disconnect.txt,24/Jun/11 20:20;jbellis;2818-v2.txt;https://issues.apache.org/jira/secure/attachment/12483764/2818-v2.txt,27/Jun/11 20:40;brandon.williams;2818-v3.txt;https://issues.apache.org/jira/secure/attachment/12484006/2818-v3.txt,27/Jun/11 22:32;jbellis;2818-v4.txt;https://issues.apache.org/jira/secure/attachment/12484061/2818-v4.txt,24/Jun/11 19:23;brandon.williams;2818.txt;https://issues.apache.org/jira/secure/attachment/12483759/2818.txt,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-06-24 13:53:00.743,,,no_permission,,,,,,,,,,,,20847,,,Tue Jun 28 21:27:07 UTC 2011,,,,,,0|i0gdnb:,93641,jbellis,jbellis,,,,,,,,,"24/Jun/11 13:53;jbellis;Not sure what's going on. Here's what's supposed to happen:

If new node N is contacted first by old node M, N records the version of M and generates messages at that version. M never knows that N is actually newer.

If M is contacted first, we expect to see the above message a few times, but M adds N to its gossip list after the first time. Once N gets a gossip from M, it will know to use M's version when creating messages.

I don't see anything obviously wrong with this code. :(","24/Jun/11 16:34;jbellis;It looks like this is a problem in 0.7 too, but you can avoid it if you happen to upgrade a seed node first.","24/Jun/11 16:42;jbellis;Patch for part one of the problem: actually disconnect when we can't handle a new version, so the other end will retry.","24/Jun/11 16:45;jbellis;The breakdown in how it's supposed to work is, Gossiper.setVersion does not actually add it to the set of nodes-to-contact (liveEndpoints and unreachableEndpoints).  We can't fix it directly by simply adding to liveEndpoints either, because Gossiper assumes that if we know about the node, we also know about its state (e.g. rack and DC information).","24/Jun/11 19:23;brandon.williams;In 0.7, we did actually add the node to the endpoint state map by calling addSavedEndpoint.  I removed this in CASSANDRA-2092, probably because it makes the log message somewhat incorrect (""XXX has restarted, now UP again"") but if it was good enough for 0.7, I think it's good enough for 0.8. Note that even without the disconnect 0.7->0.8 works, but the disconnect is an optimization.  Protection from DC/RACK NPEs is guaranteed by addSavedEndpoint initially marking the node as down, so there's no reason to query the state information (other things that utilize getNaturalEndpoints may NPE like nodetool ring, but it's a short window to exploit and non-critical.)  Patch to restore the previous behavior to 0.8.",24/Jun/11 20:20;jbellis;v2 incorporates the disconnect patch for 0.8 and removes a redundant endpointstate lookup.,"27/Jun/11 20:40;brandon.williams;v2 has two problems:

* It shuts the connection down slightly too aggressively, causing an exception on the remote side before setVersion gets called.

* It stores the remote's version even when it is greater, causing the lower version node to always report itself as the newer version to the newer node.

v3 address the first problem by sleeping for a half second before closing, and addresses the second by only calling setVersion if the remote side is compatible, otherwise it calls addSavedEndpoint before disconnecting so that it will reconnect.","27/Jun/11 21:34;jbellis;bq. It shuts the connection down slightly too aggressively, causing an exception on the remote side before setVersion gets called

I can see that the initiating side could get pissed that the target closes the socket uncleanly -- what I don't get is how a sleep could make a difference.  Is it on the reconnect?  In which case the sleep is going to be fragile with a bigger cluster, since we depend on gossip to spread the version info.

Do you have a sample stacktrace?","27/Jun/11 22:03;brandon.williams;This repeats infinitely:

{noformat}
TRACE 22:02:38,187 cassandra-2/10.179.64.227 sending GOSSIP_DIGEST_SYN to 9@/10.179.65.102
DEBUG 22:02:38,188 error writing to /10.179.65.102
java.net.SocketException: Connection reset
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:114)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:90)
{noformat}","27/Jun/11 22:32;jbellis;That makes sense for v2, yeah.

I realized that we don't actually need to reconnect to send old-version messages -- version is per-Message, the connection itself is basically just a queue around a socket.

v4 attached that doesn't drop (non-streaming) connections at all.  (This is part of the ""how did it possibly work on 0.7?"" answer, I think.)","27/Jun/11 22:33;jbellis;v4 also changes the current-version to 3, so we don't create a version exhaustion problem for ourselves (see comment in MS).",27/Jun/11 23:19;brandon.williams;+1,28/Jun/11 17:50;jbellis;committed,"28/Jun/11 21:27;hudson;Integrated in Cassandra-0.8 #197 (See [https://builds.apache.org/job/Cassandra-0.8/197/])
    fix Message version propagation fromold nodes to new ones
patch by brandonwilliams and jbellis for CASSANDRA-2818

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1140751
Files : 
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
* /cassandra/branches/cassandra-0.8/contrib
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/IncomingTcpConnection.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
* /cassandra/branches/cassandra-0.8
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
regression: exposing cache size through MBean,CASSANDRA-2781,12510580,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,16/Jun/11 14:39,12/Mar/19 14:03,13/Mar/19 22:26,16/Jun/11 16:23,0.8.1,,,,,,0,,,,,"Looks like it was part of CASSANDRA-1969.  A method called size, as opposed to getSize, won't be exposed through jmx.",,,,,,,,,,,,,,,,16/Jun/11 14:43;cburroughs;2781-v1.txt;https://issues.apache.org/jira/secure/attachment/12482803/2781-v1.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-16 16:14:34.904,,,no_permission,,,,,,,,,,,,20825,,,Fri Jun 17 02:20:45 UTC 2011,,,,,,0|i0gdf3:,93604,kingryan,kingryan,,,,,,,,,16/Jun/11 14:43;cburroughs;Just size --> getSize in InstrumentingCacheMBean,"16/Jun/11 16:14;kingryan;It would be nice if we had some tests around these things, but I'm +1 on this patch.","16/Jun/11 16:23;jbellis;committed, thanks!","17/Jun/11 02:20;hudson;Integrated in Cassandra-0.8 #173 (See [https://builds.apache.org/job/Cassandra-0.8/173/])
    fix cache mbean getSize
patch by Chris Burroughs; reviewed by Ryan King for CASSANDRA-2781

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1136529
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cache/InstrumentingCacheMBean.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/NodeCmd.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cache/InstrumentingCache.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index predicate values used in get_indexed_slice() are not validated,CASSANDRA-2328,12501429,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,15/Mar/11 06:53,12/Mar/19 14:03,13/Mar/19 22:26,17/Mar/11 02:55,0.7.5,,,,,,0,,,,,"If a client makes a get_indexed_slice() request with malformed predicate values we get an assertion failing rather than InvalidRequestException.

{noformat}
ERROR 14:47:56,842 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: 6
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVer
bHandler.java:51)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.
java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IndexOutOfBoundsException: 6
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(Ti
meUUIDType.java:56)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.jav
a:45)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.jav
a:29)
        at org.apache.cassandra.db.ColumnFamilyStore.satisfies(ColumnFamilyStore
.java:1608)
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java
:1552)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVer
bHandler.java:42)
        ... 4 more
{noformat}",,,,,,,,,,,,,,,,16/Mar/11 00:12;amorton;0001-validate-index-predicate-name-and-value.patch;https://issues.apache.org/jira/secure/attachment/12473753/0001-validate-index-predicate-name-and-value.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-17 02:55:43.93,,,no_permission,,,,,,,,,,,,20565,,,Fri Mar 18 04:06:55 UTC 2011,,,,,,0|i0gaqf:,93169,jbellis,jbellis,,,,,,,,,"16/Mar/11 00:12;amorton;Attached patch validates the expression column name and value for get_indexed_slice(). 

Also adds a regression test in the (thrift) system tests.","17/Mar/11 02:55;jbellis;committed, thanks!","18/Mar/11 04:06;hudson;Integrated in Cassandra-0.7 #391 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/391/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Changing replication_factor using ""update keyspace"" not working",CASSANDRA-2846,12512469,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jborgstrom,jborgstrom,01/Jul/11 14:15,12/Mar/19 14:03,13/Mar/19 22:26,04/Jul/11 16:20,0.8.2,,,,,,0,,,,,"Unless I've misunderstood the new way to do this with 0.8 I think ""update keyspace"" is broken:

{code}
[default@unknown] create keyspace Test with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = [{replication_factor:1}];
37f70d40-a3e9-11e0-0000-242d50cf1fbf
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] describe keyspace Test;
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
[default@unknown] update keyspace Test with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = [{replication_factor:2}];
489fe220-a3e9-11e0-0000-242d50cf1fbf
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] describe keyspace Test;                                                                                                                   Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
{code}

Isn't the second ""describe keyspace"" supposed to to say ""replication_factor:2""?

Relevant bits from system.log:
{code}
Migration.java (line 116) Applying migration 489fe220-a3e9-11e0-0000-242d50cf1fbf Update keyspace Testrep strategy:SimpleStrategy{}durable_writes: true to Testrep strategy:SimpleStrategy{}durable_writes: true
UpdateKeyspace.java (line 74) Keyspace updated. Please perform any manual operations
{code}
",A clean 0.8.1 install using the default configuration,,,,,,,,,,,,,,,01/Jul/11 15:36;jbellis;2846.txt;https://issues.apache.org/jira/secure/attachment/12484892/2846.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-01 15:36:53.231,,,no_permission,,,,,,,,,,,,20862,,,Mon Jul 04 17:21:16 UTC 2011,,,,,,0|i0gdtb:,93668,jborgstrom,jborgstrom,,,,,,,,,"01/Jul/11 15:36;jbellis;        // server helpfully sets deprecated replication factor when it sends a KsDef back, for older clients.
        // we need to unset that on the new KsDef we create to avoid being treated as a legacy client in return.
","01/Jul/11 23:07;jhermes;--1, doesn't update strategy_options for KS's that already have SimpleStrategy.-

+1, it's good.",01/Jul/11 23:09;jbellis;Jonas's test case works for me.,"04/Jul/11 09:54;jborgstrom;Jonathan, thanks for your fast response. Your patch works for me.",04/Jul/11 16:20;jbellis;committed,"04/Jul/11 17:21;hudson;Integrated in Cassandra-0.8 #204 (See [https://builds.apache.org/job/Cassandra-0.8/204/])
    fix CLI perpetuating obsolete KsDef.replication_factor
patch by jbellis; tested by Jonas Borgström for CASSANDRA-2846

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1142725
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java Build Path is broken in Eclipse after running generate-eclipse-files Ant target,CASSANDRA-2854,12512681,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dallsopp,dallsopp,dallsopp,03/Jul/11 22:37,12/Mar/19 14:03,13/Mar/19 22:26,27/Oct/11 18:23,1.0.1,,,Packaging,,,1,,,,,"Following the instructions in http://wiki.apache.org/cassandra/RunningCassandraInEclipse, but checking out v0.8.1:

After running the 'generate-eclipse-files' Ant target, the build path is set up to include all the libraries in build/lib/jars, but each library has ;C (semicolon, letter C) appended to it, so Eclipse can't find the libraries - there are about 55 errors like:

Project 'cassandra-0.8.1' is missing required library: '\Users\David\eclipse_workspace\cassandra-0.8.1\build\lib\jars\commons-cli-1.2.jar;C'
","Windows 7 64-bit, Eclipse Helios SR1, Subclipse",,,,,,,,,,,,,,,22/Oct/11 11:18;dallsopp;cassandra-2854.patch;https://issues.apache.org/jira/secure/attachment/12500320/cassandra-2854.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-22 16:54:59.782,,,no_permission,,,,,,,,,,,,1921,,,Thu Oct 27 18:23:44 UTC 2011,,,,,,0|i0gdv3:,93676,urandom,urandom,,,,,,,,,03/Jul/11 22:44;dallsopp;Manually editing all the build path entries (and removing the one called just ';C' ) seems to resolve all the Eclipse project errors.,"03/Jul/11 23:46;dallsopp;The Ant target has the following (javascript:

{noformat}
jars = project.getProperty(""eclipse-project-libs"").split("":"");  
{noformat}

which is probably picking up on the colon in windows paths such as 
{noformat}
C:\Users\Foo\eclipse_workspace\cassandra-0.8.1\build\lib\foo.jar
{noformat}","22/Jul/11 16:54;patricioe;In my Case, Mac OS X 10.6.8 and Eclipse Helios 3.6.2

Even though the source folder is src/java I get this error (around 9 thousand of these)

{code}
The declared package ""org.apache.cassandra.utils.obs"" does not match the expected package ""java.org.apache.cassandra.utils.obs""	BitUtil.java	/cassandra-trunk/src/java/org/apache/cassandra/utils/obs	line 18	Java Problem
{code}
",22/Jul/11 18:01;patricioe;I had this problem with Eclipse 3.6.2 always. Eclipse 3.7 works just fine.,"25/Jul/11 16:14;dallsopp;Patricio - I think that's a separate problem, but thanks for the tip, as I was getting that problem as well! I couldn't figure out why Eclipse acted as if that src was on the build path, but showed only src/java in the settings!","13/Oct/11 22:17;jbellis;David, can you submit a patch to address the : problem?","14/Oct/11 08:46;dallsopp;Will try to take a look. Probably needs to detect the OS in Ant to modify the regex passed to split(), e.g.:

<condition property=""path.sep"" value="";"">
   <os family=""windows""/>
</condition>
<condition property=""path.sep"" value="":"">
   <os family=""unix""/>
</condition>

<fail unless=""foo.path"">No path.sep set for this OS!</fail>

Or (possibly) use unix-style paths in the first place if Ant handles them OK on Windows (but not sure where they originate from; this may not be possible)...","22/Oct/11 10:28;dallsopp;Wasn't having much success getting Eclipse happy on Windows, so tried on Linux but similar problems: after checking out trunk, running build.xml (runs OK) and then the generate-eclipse-files target (runs OK), I refresh the project and get:

{noformat}
Errors occurred during the build.
Errors running builder 'Integrated External Tool Builder' on project 'Cassandra-trunk'.
Variable references non-existent resource : ${workspace_loc:/cassandra-trunk/build.xml}
Variable references non-existent resource : ${workspace_loc:/cassandra-trunk/build.xml}
{noformat}

This is easy to fix - the project name I chose in Eclipse (Cassandra-trunk) is capitalized but the directory (cassandra-trunk) isn't. Go to Project->Properties->Builders, select Cassandra-Ant-Builder, Edit, and fix the Build File and Base Directory to match the name in Eclipse. This is probably caused by me checking out the code using SVN in the console rather than using Subversive within Eclipse...

*However, Eclipse report 2153 errors in the project at this point.*

If someone has this working nicely, could they please update the wiki page (http://wiki.apache.org/cassandra/RunningCassandraInEclipse), as it's got very outdated and confusing. Alternatively, if most Cassandra folks are using an environment other than Eclipse that works better, could you describe it on the wiki?","22/Oct/11 10:40;dallsopp;After running generate-eclipse-files on either OS, some of the jars are duplicated, i.e. there are two copies on the build path, sometimes of different versions.

e.g commons-lang-2.1 and commons-lang-2.4
and servlet-api-2.5.20081211 (one copy from /lib and one from build/lib/jars)","22/Oct/11 11:18;dallsopp;Fix turns out to be trivial - in the javascript split() call, replace the hardcoded UNIX path separator "":"" with 

{code}project.getProperty(""path.separator""){code}

to acquire the correct OS-specific separator.",22/Oct/11 11:18;dallsopp;One-liner to fix path separator,"22/Oct/11 13:09;dallsopp;I think I have found a workable way to get Eclipse set up reliably, so will nuke most of the wiki page (http://wiki.apache.org/cassandra/RunningCassandraInEclipse) and add a new version. There may well be a neater way, as I'm not yet sure _why_ things break when following the current instructions. I suspect it may be due to Eclipse compiling things into /bin until it is (later) reconfigured to user /build/classes/main.
",27/Oct/11 18:23;urandom;committed; thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hex-to-bytes conversion accepts invalid inputs silently,CASSANDRA-2851,12512673,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,dallsopp,dallsopp,03/Jul/11 18:04,12/Mar/19 14:03,13/Mar/19 22:26,10/Feb/12 19:15,1.1.0,,,,,,0,,,,,"FBUtilities.hexToBytes() has a minor bug - it copes with single-character inputs by prepending ""0"", which is OK - but it does this for any input with an odd number of characters, which is probably incorrect.

{noformat}
if (str.length() % 2 == 1)
    str = ""0"" + str;
{noformat}

Given 'fff' as an input, can we really assume that this should be '0fff'? Isn't this just an error?

Add the following to FBUtilitiesTest to demonstrate:

{noformat}
String[] badvalues = new String[]{""000"", ""fff""};
       
for (int i = 0; i < badvalues.length; i++)
    try
    {
        FBUtilities.hexToBytes(badvalues[i]);
        fail(""Invalid hex value accepted""+badvalues[i]);
    } catch (Exception e){}
{noformat}",,,,,,,,,,,,,,,,10/Feb/12 11:30;slebresne;2851.patch;https://issues.apache.org/jira/secure/attachment/12514100/2851.patch,03/Jul/11 19:12;dallsopp;cassandra-2851.diff;https://issues.apache.org/jira/secure/attachment/12485099/cassandra-2851.diff,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-04 08:57:13.686,,,no_permission,,,,,,,,,,,,20865,,,Fri Feb 10 19:15:15 UTC 2012,,,,,,0|i0gduf:,93673,jbellis,jbellis,,,,,,,,,"03/Jul/11 18:12;dallsopp;In addition to the invalid values, should probably also add a check in FBUtilitiesTest.testHexToBytesStringConversion() that """" is converted to byte[0], and that a null input is handled appropriately.

","03/Jul/11 19:12;dallsopp;If there's agreement that odd-sized inputs should throw an exception, the attached patch does this and updates the unit test.","04/Jul/11 08:57;slebresne;Why would it be ok for single-character inputs and not other odd-sized inputs ? An odd-sized input doesn't (ever) correspond to a valid byte array, so I'd say either we always silently add a 0 to ""make it fit"" or we never do it. I do actually am in favor of throwing an exception rather then coping with it silently since it's more likely to indicate a user error than to be helpful (but maybe that addition of a '0' in front was there for a reason?).
I'll note that even though I can't imagine why people would generate odd-sized hex input, since it is allowed so far, there is a chance someone out there does it, and it would be a ""regression"" for that guy. So maybe we should target 1.0 for the sake of making minor upgrade as smooth for everybody as can be.

On the patch side, we must make sure every consumer of hexToBytes() handles the new exception (or make it a NumberFormatException but I don't think this is a good idea). For instance, at least BytesType.fromString() should catch the IllegalArgumentException and rethrow a MarshalException, otherwise CQL will crap his pants on odd-sized inputs.","04/Jul/11 12:35;jbellis;bq. maybe that addition of a '0' in front was there for a reason

I think it's there b/c of Integer.toHexString: ""This value is converted to a string of ASCII digits in hexadecimal (base 16) with no extra leading 0s.""

Our bytesToHex does pad... but only for single-digit results.  So if we fix hexToBytes we'll introduce an incompatibility. (Granted, a minor one.)","04/Jul/11 15:35;slebresne;bq. Our bytesToHex does pad... but only for single-digit results. So if we fix hexToBytes we'll introduce an incompatibility. (Granted, a minor one.)

I don't understand. There is no such thing as padding when you convert a byte array to hex (Integer.toHexString does return only the right number of hexadecimal digits because it has no reason to do otherwise, but it's an implementation detail of bytesToHex). A byte is always 8 bit, never 4, and the output of bytesToHex will *always* have a even number of characters (as it should). Our hexToBytes just happen to semi-randomly add 0 in front to transform a buggy input with an odd number of character to a even one, in the off chance that a client used the (stupid) ""optimization"" of removing at most 1 leading 0 to win some space or something. In my opinion, it would be better to simply refuse odd sized input because this is more likely a truncated input (and people using stupid clients should fix them, though I'm ok with saying that we'll force them to fix it only on a major upgrade).","04/Jul/11 16:16;jbellis;You're right, I was misreading how we were using Integer.toHexString.","04/Jul/11 17:51;dallsopp;The origin of the current behaviour is CASSANDRA-1411 https://issues.apache.org/jira/browse/CASSANDRA-1411 if that helps...

","04/Jul/11 19:18;jbellis;Good point, David.

Sounds like the problem is thinking of this as a generic hex conversion function, rather than as ""hex that specifically represents bytes.""","08/Sep/11 09:48;slebresne;bq. Sounds like the problem is thinking of this as a generic hex conversion function, rather than as ""hex that specifically represents bytes.""

Well, in a way those methods never pretended being generic hex to *bits* conversion functions since they are called hex2bytes and bytes2hex. I still think that the correct behavior for hex2bytes is to throw an exception on odd sized input. And that CASSANDRA-1411 was a mistake: yes I understand that it's annoying the first time you use '0' as token to get an exception and to have to write '00', but if you had written '0', there is a good chance you had missed the meaning of the input, i.e that it is  an hex string representing a byte array.

Now changing that behavior would not be backward compatible, so I wonder if we really should do anything here. I would be in favor however to refactor the code slightly so that hex2bytes do throw an exception on odd sized input (it would be fine if the method was named hex2bits, but it's fishy with hex2bytes) and to move the code that pads the input into ByteOrderedPartitioner, since that is what it was meant to: simplify the input of tokens.",08/Sep/11 13:03;jbellis;That sounds reasonable.,"13/Sep/11 10:32;dallsopp;Sounds good to me - I agree the underlying conversion should be strict to
avoid inadvertent misuse and hidden bugs in future - with a more lenient
wrapper for specific cases and backward compatibility.


","10/Feb/12 11:30;slebresne;Attaching patch that implements the idea above, i.e. hexToBytes throw an exception on odd number of input characters, but silently adds a '0' in front when parsing a token for an ordered partitioner.

This does mean BytesType will throw a MarshalException when given a string of odd length. I think it is a good thing, it was a but that we were potentially silently adding some zeroes to column names or values given by the user. However, this does kind of break some form of backward compatibility so I suppose we should either push it for 1.1.0 or report to 1.2. I personally would be good for 1.1.0 (I consider the current behavior a bug) but I won't argue if someone prefers 1.2.",10/Feb/12 13:44;jbellis;+1 for 1.1.0 from me,"10/Feb/12 19:15;slebresne;Ok, committed to 1.1.0. We'll revert if someone feels strongly this is a violation of the freeze.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting RR Chance via CliClient results in chance being too low,CASSANDRA-2837,12512038,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,dehora,dehora,28/Jun/11 20:04,12/Mar/19 14:03,13/Mar/19 22:26,29/Jun/11 01:15,0.7.7,,,,,,0,,,,,"running a command like 

{noformat}
""update column family shorturls with read_repair_chance=0.4;""
{noformat}

results in the value being set to 0.0040. Was expecting it to be 0.4.

Affects 0.7.6.-2; seems to be fixed on trunk/0.8.",,,,,,,,,,,,,,,,28/Jun/11 20:06;jhermes;2837.txt;https://issues.apache.org/jira/secure/attachment/12484494/2837.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-28 20:06:19.353,,,no_permission,,,,,,,,,,,,20857,,,Wed Jun 29 01:50:01 UTC 2011,,,,,,0|i0gdrb:,93659,jbellis,jbellis,,,,,,,,,"28/Jun/11 20:06;jhermes;Straightforward patch.
If the help says it expects a double from 0.0 to 1.0, it shouldn't then divide by 100 as if expecting a percentage.","29/Jun/11 01:15;jbellis;committed, thanks!","29/Jun/11 01:50;hudson;Integrated in Cassandra-0.7 #516 (See [https://builds.apache.org/job/Cassandra-0.7/516/])
    fix CLI parsing of read_repair_chance
patch by Jon Hermes; reviewed by jbellis for CASSANDRA-2837

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1140928
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/cli/CliClient.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InvalidRequestException when validating column data includes entire column value,CASSANDRA-2849,12512664,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dallsopp,dallsopp,dallsopp,03/Jul/11 11:41,12/Mar/19 14:03,13/Mar/19 22:26,11/Aug/11 19:19,0.8.5,,,,,,0,,,,,"If the column value fails to validate, then ThriftValidation.validateColumnData() calls bytesToHex() on the entire column value and puts this string in the Exception. Since the value may be up to 2GB, this is potentially a lot of extra memory. The value is likely to be logged (and presumably returned to the thrift client over the network?). This could cause a lot of slowdown or an unnecessary OOM crash, and is unlikely to be useful (the client has access to the full value anyway if required for debugging).

Also, the reason for the exception (extracted from the MarshalException) is printed _after_ the data, so if there's any truncation in the logging system at any point, the reason will be lost. 

The reason should be displayed before the column value, and the column value should be truncated in the Exception message.",,,,,,,,,,,,,,,,05/Jul/11 16:52;jbellis;2849-v2.txt;https://issues.apache.org/jira/secure/attachment/12485302/2849-v2.txt,03/Jul/11 12:08;dallsopp;cassandra-2849.diff;https://issues.apache.org/jira/secure/attachment/12485085/cassandra-2849.diff,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-05 16:52:46.664,,,no_permission,,,,,,,,,,,,20864,,,Thu Aug 11 20:21:40 UTC 2011,,,,,,0|i0gdtz:,93671,jbellis,jbellis,,,,,,,,,"03/Jul/11 12:08;dallsopp;Attached diff moves the reason for the exception before the data, and truncates the column value data in the exception to 64K.

Unable to test this properly today since working on a fresh Windows box over the weekend and unable to get build working in Eclipse 8-(

","03/Jul/11 15:22;dallsopp;A lot of classes use ByteBufferUtil.bytesToHex(), often for logging and exceptions, so may make more sense to add a method there for getting a truncated hex string.","03/Jul/11 16:13;dallsopp;On a related note, I just noticed that ByteBufferUtil.bytesToHex() is unnecessarily slow - will raise a new issue for this.",05/Jul/11 16:52;jbellis;v2 adds column name decoding and moves the value to a debug log (presumably the client  knows what value it sent...),"11/Aug/11 14:28;jbellis;David, does v2 look ok to you?","11/Aug/11 15:35;dallsopp;Yes, that looks good to me, thanks.",11/Aug/11 19:19;jbellis;committed,"11/Aug/11 20:21;hudson;Integrated in Cassandra-0.8 #272 (See [https://builds.apache.org/job/Cassandra-0.8/272/])
    include column name in validation failure exceptions
patch by jbellis; reviewed by David Allsopp for CASSANDRA-2849

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156753
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming estimatedKey calculation should never be 0,CASSANDRA-2916,12514491,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,18/Jul/11 19:41,12/Mar/19 14:03,13/Mar/19 22:26,18/Jul/11 19:56,1.0.0,,,,,,0,,,,,"The new in-streaming SSTable rebuild uses the sender's estimated key calculation to determine which codepath to take: in some cases, samples can result in an estimated key count of 0.",,,,,,,,,,,,,,,,18/Jul/11 19:44;stuhood;0001-CASSANDRA-2916-Assume-at-least-one-estimated-key-for-s.txt;https://issues.apache.org/jira/secure/attachment/12486911/0001-CASSANDRA-2916-Assume-at-least-one-estimated-key-for-s.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-18 19:56:54.528,,,no_permission,,,,,,,,,,,,20890,,,Mon Jul 18 20:28:10 UTC 2011,,,,,,0|i0ge8n:,93737,jbellis,jbellis,,,,,,,,,"18/Jul/11 19:44;stuhood;Patch to ensure that we never estimate 0 keys for non-empty ranges. Also, properly handle the case where a range captures exactly one key.","18/Jul/11 19:56;jbellis;committed, thanks!","18/Jul/11 20:28;hudson;Integrated in Cassandra #960 (See [https://builds.apache.org/job/Cassandra/960/])
    ensure that we never estimate 0 keys when streaming non-empty ranges
patch by Stu Hood; reviewed by jbellis for CASSANDRA-2916

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148029
Files : 
* /cassandra/trunk/test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Native Memory Leak,CASSANDRA-2868,12513152,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,doubleday,doubleday,07/Jul/11 14:05,12/Mar/19 14:03,13/Mar/19 22:26,23/Aug/11 20:06,0.7.9,0.8.5,,,,,4,,,,,"We have memory issues with long running servers. These have been confirmed by several users in the user list. That's why I report.

The memory consumption of the cassandra java process increases steadily until it's killed by the os because of oom (with no swap)

Our server is started with -Xmx3000M and running for around 23 days.

pmap -x shows

Total SST: 1961616 (mem mapped data and index files)
Anon  RSS: 6499640
Total RSS: 8478376

This shows that > 3G are 'overallocated'.

We will use BRAF on one of our less important nodes to check wether it is related to mmap and report back.",,,,,,,,,,,,,,,,13/Jul/11 01:13;cburroughs;2868-v1.txt;https://issues.apache.org/jira/secure/attachment/12486249/2868-v1.txt,29/Jul/11 21:10;brandon.williams;2868-v2.txt;https://issues.apache.org/jira/secure/attachment/12488246/2868-v2.txt,16/Aug/11 16:59;jbellis;2868-v3.txt;https://issues.apache.org/jira/secure/attachment/12490554/2868-v3.txt,27/Jul/11 17:49;cburroughs;48hour_RES.png;https://issues.apache.org/jira/secure/attachment/12488003/48hour_RES.png,14/Jul/11 13:16;cburroughs;low-load-36-hours-initial-results.png;https://issues.apache.org/jira/secure/attachment/12486435/low-load-36-hours-initial-results.png,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2011-07-11 18:31:36.862,,,no_permission,,,,,,,,,,,,20872,,,Tue Aug 23 20:20:06 UTC 2011,,,,,,0|i0gdxz:,93689,jbellis,jbellis,,,,,,,,,"11/Jul/11 10:45;doubleday;Hm after 3 days checking a node that does not use mmaped files it looks like this:

nativelib: 14128
locale-archive: 1492
ffiSwFShY(deleted): 8
javajar: 2292
[anon]: 3609388
[stack]: 132
java: 44
7008: 32
jna534482390478104336.tmp: 92

Total RSS: 3627608
Total SST: 0


Compared to start RSS increased by ~400MB. So it seems that this is not related to mem mapping.

We will deploy CASSANDRA-2654 this week. Will see if that changes anything but I suspect not ...","11/Jul/11 18:31;cburroughs;At one point I was convinced this was a JVM bug and opened http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7037080  After seeing how totally broken NIO is after CASSANDRA-2654 I'm no longer sure of anything.

I was going to start a survey on the user list after the summit to see if any OS/jvm level pattern could be found, since clearly it doesn't happen to everyone in all cases.",11/Jul/11 20:33;jbellis;Is your data size constant?  If not you are probably seeing growth in the index samples and bloom filters.,"12/Jul/11 08:30;doubleday;Next: [anon]: 3675224 (+47616KB in 1 day)

bq. Is your data size constant? If not you are probably seeing growth in the index samples and bloom filters.

Well no - the data size is increasing. But I thought that index and bf is good old plain java heap no? JVM heap stats are really relaxed. Yet I think that doesn't really matter because what we are seeing is an ever increasing rss mem consumption even though we have -Xmx3G and -Xms3G and mlockall (pmap shows these 3G as one block). So something seems to be constantly allocating native mem that has nothing to do with java heap.","12/Jul/11 20:43;jbellis;We call getLastGcInfo several times a second.  http://twitter.com/#!/kimchy/status/90861039930970113

You could try turning GCInspector methods into a no-op and see if that makes it go away.","13/Jul/11 00:11;cburroughs;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7066129 will be the id when bugs.sun.com gets around to doing it's thing.

I confirmed that -XX:MaxDirectMemorySize does not protect you from this (ie it's a native native leak, not some DirectByteBuffer thing).  I'll be able to test this but not until the end of this week at the earliest (and it will then take at least another week to be sure).",13/Jul/11 01:13;cburroughs;In case it is useful to anyone else this is what I intend to test with.,14/Jul/11 13:16;cburroughs;Initial results.  Graph of VmRSS from /proc/PID/status at 10 second intervals from my last comment to now.  Box on the left has GCInspector disabled.  These are on two test boxes under trivial load so this is all still *very* tentative.  Will start testing under real load by early next week.,14/Jul/11 13:21;jbellis;Promising!,15/Jul/11 11:31;doubleday;It's indeed promising. We have been running this in production for 3 days now and rss increased only insignificantly by ~5MB a day. ,"15/Jul/11 15:33;hanzhu;{quote}
We have been running this in production for 3 days now and rss increased only insignificantly by ~5MB a day
{quote}

Do you mean it is very helpful to control RSS increasing by removing getLastGcInfo()? 

I have no idea why just some of us meets the problem.","15/Jul/11 16:24;cburroughs;I interpreted Daniel's ""this"" to be the 2868-v1.txt patch (or something equivalent) with cassandra.enable_gc_inspector=false.  I did not find -XX:MaxDirectMemorySize to be helpful.",16/Jul/11 06:45;doubleday;Yes - we did disable the GCInspector.,"16/Jul/11 07:04;hanzhu;Got it!

Do you have any idea why only some of us reports the problem?","16/Jul/11 10:29;doubleday;Well either it's environment specific or (more likely) others didn't notice / care because they have enough memory and/or restart the nodes often enough.

We have 16GB of RAM and run Cassandra with 3GB. Within one month we loose ~3GB (13GB -> 10GB) files system cache because of the mem leak. Looking at our graphs I can't really tell a difference performance wise. So I guess only people with weaker servers (less memory headroom) will really notice. We noticed only because we got the system oom on a cluster that's not critical and which we didn't really monitor.","18/Jul/11 07:59;doubleday;Looks good to me. Guess cassandra should just disable the inspector for now (probably make it jmx'able to start it manually)

Thu Jul 14 09:39:26 CEST 2011: [anon]: 3234068
Thu Jul 14 17:22:45 CEST 2011: [anon]: 3266888
Fri Jul 15 09:33:53 CEST 2011: [anon]: 3269160
Mon Jul 18 09:54:29 CEST 2011: [anon]: 3270188","18/Jul/11 20:36;jbellis;We could try switching to what JConsole does, which is just log the total number and time spent for each compaction type.  This uses a different API which hopefully does not leak: http://www.java2s.com/Open-Source/Java-Document/6.0-JDK-Modules-sun/tools/sun/tools/jconsole/MemoryTab.java.htm

Logging the lifetime totals there with StatusLogger similar to what we do now for dropped messages would be better than nothing.","27/Jul/11 17:49;cburroughs;48 hours under production load after C* had already been running for a few days.  Two on the left have GCInspector enabled.  The two on the right do not.  (Note that the scale on the lower right one reflects a change of only 10s of bytes.)

So it looks like victory to me.","27/Jul/11 18:09;jbellis;Thanks, Chris.  We'll work on rewriting GCInspector to use the java.lang.management api instead, unless you have time to take a stab at that.","29/Jul/11 20:15;jjordan;Depending how long the rewrite is going to take, can we get the config file option to disable gc inspector into a new 0.7.X and 0.8.X release?",29/Jul/11 21:10;brandon.williams;v2 switches the GCInspector to us java.lang.managment.  I don't know if it too leaks or not yet.,"01/Aug/11 00:56;brandon.williams;I created three isolated nodes, all with a hack of setting the inspector interval to 1ms applied (not the tightest loop, but good enough and easy.)  One of the nodes had the inspector disabled entirely (the control), one was vanilla, and one had v2 applied.  After starting them up with a 128M heap and letting them run for a few minutes, here are the results:

||version||resident||
|control|72M|
|patched|72M|
|vanilla|540M|

I think it's safe to say java.lang.management doesn't share the leak.","01/Aug/11 18:05;slebresne;Comments on v2:
* Couldn't we estimate the reclaimed size by recording the last memory used (that would need to be the first thing we do in logGCResults so that we record it each time) ?
* Wouldn't it be worth indicating that how many collection have been done since last log message if it's > 1, since it can (be > 1).
* Nit: especially if we decide to keep the last memory used, it may be more efficient (in cleaner imho) to have just one HashMap of string -> GCInfo where GCInfo would be a small struct with times, counts and usedMemory. Not that it is very performance sensitive... ","01/Aug/11 18:55;jbellis;bq. Couldn't we estimate the reclaimed size

Well, not really, what we'd have is ""difference in size between last time it was called, and now"" which isn't all that close to ""amount reclaimed by a specific GC.""

bq. Wouldn't it be worth indicating that how many collection have been done since last log message

IMO the duration-based thresholds are hard to reason about here, where we're dealing w/ summaries and not individual GC results.  I think I'd rather have something like the dropped messages logger, where every N seconds we log the summary we get from the mbean.

The flushLargestMemtables/reduceCacheSizes stuff should probably be removed. :(","09/Aug/11 18:43;brandon.williams;bq. Wouldn't it be worth indicating that how many collection have been done since last log message if it's > 1, since it can (be > 1).

The only reason I added count tracking was to prevent it from firing when there were no GCs (the api is flakey.)  I've never actually been able to get > 1 to happen, but we can add it to the logging.

bq. IMO the duration-based thresholds are hard to reason about here, where we're dealing w/ summaries and not individual GC results.

We are dealing with individual GCs at least 99% of the time in practice.  The worst case is >1 GC inflates the gctime enough that we errantly log when it's not needed, but I imagine to trigger that you would have to be in a gc pressure situation already.

bq. I think I'd rather have something like the dropped messages logger, where every N seconds we log the summary we get from the mbean.

That seems like it could be a lot of noise since GC is constantly happening.

bq. The flushLargestMemtables/reduceCacheSizes stuff should probably be removed. 

I think the logic there is still sound (""Did we just do a CMS? Is the heap still 80% full?"") and it seems to work as well as it always has.

","16/Aug/11 16:59;jbellis;bq. I've never actually been able to get > 1 to happen, but we can add it to the logging

I'm sure it's possible w/ a small enough heap, especially since GCInspector is paused along w/ everything else for STW collections (including new gen).

v3 attached to accomodate this and add durationPerCollection.",16/Aug/11 17:05;brandon.williams;Why is v3 touching compaction?,16/Aug/11 17:09;jbellis;dirty working directory.  GCI is the only relevant file.,"16/Aug/11 23:36;brandon.williams;+1 to GCI changes.  Also, it is indeed possible to get >1 with a tiny heap.",17/Aug/11 02:20;jbellis;committed,"17/Aug/11 03:20;hudson;Integrated in Cassandra-0.8 #282 (See [https://builds.apache.org/job/Cassandra-0.8/282/])
    work around native memory leak in com.sun.management.GarbageCollectorMXBean
patch by brandonwilliams and jbellis for CASSANDRA-2868

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1158490
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/GCInspector.java
",17/Aug/11 03:28;jjordan;Can we get this in 0.7.X as well?,23/Aug/11 19:18;brandon.williams;Reopening to backport to 0.7,23/Aug/11 20:06;brandon.williams;Committed to 0.7 in r1160879,"23/Aug/11 20:20;hudson;Integrated in Cassandra-0.7 #543 (See [https://builds.apache.org/job/Cassandra-0.7/543/])
    work around native memory leak in com.sun.management.GarbageCollectorMXBean
patch by brandonwilliams and jbellis for CASSANDRA-2868

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1160879
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/GCInspector.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"While dropping and recreating an index, incremental snapshotting can hang",CASSANDRA-2872,12513235,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,slebresne,slebresne,08/Jul/11 08:59,12/Mar/19 14:03,13/Mar/19 22:26,19/Jul/11 18:08,0.7.8,0.8.2,,,,,0,,,,,"When creating a hard link (at list with JNA), link() hang if the target of the
link already exists. In theory though, we should not hit that situation
because we use a new directory for each manual snapshot and the generation
number of the sstables should prevent this from hapenning with increment
snapshot.

However, when you drop, then recreate a secondary index, if the sstables are
deleted after the drop and before we recreate the index, the recreated index
sstables will start with a generation to 0. Thus, when we start backuping them
incrementally, it will conflict with the sstables of the previously dropped
index.

First, we should check for the target existance because calling link() to at
least avoid hanging. But then we must make sure that when we drop, then
recreate an index, we will either not name the sstables the same way or the
incremental snapshot use a different directory.
",,,,,,,,,,,,,,,,19/Jul/11 17:06;jbellis;2872-v2.txt;https://issues.apache.org/jira/secure/attachment/12487033/2872-v2.txt,19/Jul/11 16:08;jbellis;2872.txt;https://issues.apache.org/jira/secure/attachment/12487019/2872.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-12 23:49:10.739,,,no_permission,,,,,,,,,,,,20875,,,Tue Jul 19 19:41:05 UTC 2011,,,,,,0|i0gdyv:,93693,slebresne,slebresne,,,,,,,,,"12/Jul/11 23:43;slebresne;One option could be to have some kind of ""index"" generation that we would persist in the system tables. What I mean here is that if you create a index on say column 'birthdate' for some CF test, the index would start being called test.birthdate.1 (or just test.birthdate for compatibility sake), then if you drop it and recreate it, it would be called test.birthdate.2. That way, we would avoid the name clash during the incremental backup.",12/Jul/11 23:49;jbellis;I really don't like messing with the name.  That feels like the implementation is leaking too much into something user-visible.,"19/Jul/11 15:06;jbellis;Remind me why simply making sstable generation global doesn't fix this?

It's not like there's so much contention on that AtomicInteger that we need to partition it.","19/Jul/11 15:22;slebresne;bq. Remind me why simply making sstable generation global doesn't fix this?

If you drop an index, then shutdown the node, then restart and recreate the index. Upon restart and crawling of the existing data files, it could be that the first available generation is the one of a sstable of the dropped index.

I guess there is two reasonably simple solutions:
# scan the (incremental) snapshot directories for the generation number too. If we do that, I guess we don't even have to make the generation global as long as we do this scanning each time a ColumnFamilyStore is created.
# make the generation number persistent in the system tables (again, no need to make the number global for that).

I think I prefer the second solution because it's more general and feels more elegant. But we would still have to scan the data dir and take the max(what we found during scan, what's in the system table) in case people force-feed data files that weren't created on that node (or the system tables are wiped).

That being said, I totally agree that the generation number don't have to be partitioned if we don't want to. But not sure it's a big deal that way either.","19/Jul/11 16:07;jbellis;bq. we would still have to scan the data dir and take the max(what we found during scan, what's in the system table) in case people force-feed data files that weren't created on that node (or the system tables are wiped).

That's why I prefer the scan approach -- I'd rather have a single reliable source of truth (the contents of the fs) than an unreliable one that we have to supplement with the reliable one anyway.

Patch attached.","19/Jul/11 16:54;slebresne;bq. That's why I prefer the scan approach

Yeah, I kind of wrote my comment before as things were coming to me, in particular I wrote that I was preferring the second solution before realizing we would still need to scan. Agreeing that the scan approach is cleaner/easier.

On the patch, shouldn't we only include the sstables for the column family we're creating. I know it's ok to take the max for all cfs, but it feels a bit random unless we have only one global generation. And could be worst avoiding 2-3 mails on the mailing of people wondering why that has changed (I mean, I'm sure someone will remark it) :). ",19/Jul/11 17:06;jbellis;v2 restricts to the CF in question.,"19/Jul/11 17:13;slebresne;lgtm +1

And I can confirm that this fix the failing test of CASSANDRA-2521. ",19/Jul/11 18:08;jbellis;committed,"19/Jul/11 19:41;hudson;Integrated in Cassandra-0.7 #531 (See [https://builds.apache.org/job/Cassandra-0.7/531/])
    fix re-using index CF sstable names after drop/recreate
patch by jbellis; reviewed by slebresne for CASSANDRA-2872

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148466
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dynamic snitch + read repair off can cause LOCAL_QUORUM reads to return spurious UnavailableException,CASSANDRA-2870,12513176,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,07/Jul/11 19:22,12/Mar/19 14:03,13/Mar/19 22:26,08/Jul/11 16:38,0.7.8,0.8.2,,,,,0,,,,,"When Read Repair is off, we want to avoid doing requests to more nodes than necessary to satisfy the ConsistencyLevel.  ReadCallback does this here:

{code}
        this.endpoints = repair || resolver instanceof RowRepairResolver
                       ? endpoints
                       : endpoints.subList(0, Math.min(endpoints.size(), blockfor)); // min so as to not throw exception until assureSufficient is called
{code}

You can see that it is assuming that the ""endpoints"" list is sorted in order of preferred-ness for the read.

Then the LOCAL_QUORUM code in DatacenterReadCallback checks to see if we have enough nodes to do the read:

{code}
        int localEndpoints = 0;
        for (InetAddress endpoint : endpoints)
        {
            if (localdc.equals(snitch.getDatacenter(endpoint)))
                localEndpoints++;
        }

        if (localEndpoints < blockfor)
            throw new UnavailableException();
{code}

So if repair is off (so we truncate our endpoints list) AND dynamic snitch has decided that nodes in another DC are to be preferred over local ones, we'll throw UE even if all the replicas are healthy.",,,,,,,,,,,,,,,,07/Jul/11 19:34;jbellis;2870.txt;https://issues.apache.org/jira/secure/attachment/12485644/2870.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-08 08:40:03.722,,,no_permission,,,,,,,,,,,,20874,,,Wed Jul 13 14:59:02 UTC 2011,,,,,,0|i0gdyf:,93691,slebresne,slebresne,,,,,,,,,"07/Jul/11 19:34;jbellis;patch extracts the ""give me the minimum set of endpoints necessary"" code into preferredEndpoints(), and makes it DC-aware for LOCAL_QUORUM reads.",08/Jul/11 08:40;slebresne;+1,08/Jul/11 16:38;jbellis;committed,"08/Jul/11 16:52;hudson;Integrated in Cassandra-0.7 #526 (See [https://builds.apache.org/job/Cassandra-0.7/526/])
    fix possibility of spuriousUnavailableException for LOCAL_QUORUM reads with dynamic snitch and read repair disabled
patch by jbellis; reviewed by slebresne for CASSANDRA-2870

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1144380
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/ReadCallback.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/DatacenterReadCallback.java
","08/Jul/11 23:51;jeromatron;This also appears to affect 0.7.6 and when read repair is not off.  I didn't set read repair on my CFs (defaults to 100%) and tried a simple rowcount pig script using read consistency LOCAL_QUORUM and it fails with UE.  I would think if that's the case, the priority should be higher and it should go in 0.7.7.  Any thoughts?","09/Jul/11 03:51;jbellis;This has been present since LOCAL_QUORUM was introduced, so it's not a new regression.  And a reasonable workaround exists (disable dynamic snitch).  So no, I don't think we should hold up 0.7.7 for this.",09/Jul/11 05:38;jeromatron;Okay - it just seemed like a higher priority issue with the scope expanded.  We'll probably just disable dynamic snitch until the fix is in a release then.,"13/Jul/11 14:40;patrickubs;In the default configuration of 0.7.6-2 (and other versions) LOCAL_QUORUM reads dont work. This is not a minor bug and should be fixed in the next release.
By default configuration I mean the tar ball that is distributed by the cassandra website.
The fact that it is not a regression just shows that this functionality was never properly tested.
",13/Jul/11 14:59;jbellis;It will be fixed in 0.7.8; 0.7.7 entered the release process before this was reported.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming SSTable build does not use cleanupIfNecessary,CASSANDRA-2906,12514348,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,stuhood,stuhood,17/Jul/11 06:22,12/Mar/19 14:03,13/Mar/19 22:26,20/Jul/11 15:07,1.0.0,,,,,,0,,,,,"The new streaming sstable rebuilding in IncomingStreamReader needs to wrap things in with {{try, finally, cleanupIfNecessary}} to ensure that the writer is cleaned up properly.",,,,,,,,,,,,,,,,20/Jul/11 12:48;yukim;ASF.LICENSE.NOT.GRANTED--trunk-2906-v2.txt;https://issues.apache.org/jira/secure/attachment/12487148/ASF.LICENSE.NOT.GRANTED--trunk-2906-v2.txt,18/Jul/11 21:10;yukim;ASF.LICENSE.NOT.GRANTED--trunk-2906.txt;https://issues.apache.org/jira/secure/attachment/12486923/ASF.LICENSE.NOT.GRANTED--trunk-2906.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-18 21:10:51.51,,,no_permission,,,,,,,,,,,,20886,,,Wed Jul 20 16:16:48 UTC 2011,,,,,,0|i0ge6f:,93727,stuhood,stuhood,,,,,,,,,18/Jul/11 21:10;yukim;Thank you for pointing it out. Patch attached to do perform closeIfNecessary.,"18/Jul/11 23:31;stuhood;You can actually return the reader object from inside the {{try, finally}}: the finally block isn't executed until after the return statement. See the uses of cleanupIfNecessary in SSTableWriter.Builder for an example.",20/Jul/11 12:48;yukim;Attached v2. Just return SSTR instead of storing to local var.,20/Jul/11 15:07;jbellis;committed,"20/Jul/11 16:16;hudson;Integrated in Cassandra #965 (See [https://builds.apache.org/job/Cassandra/965/])
    add cleanupIfNecessary for single-pass streaming SSTable build
patch by Yuki Morishita; reviewed by stuhood and jbellis for CASSANDRA-2906

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148811
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
durable_writes flag cannot be changed via the CLI (system does not process KsDef.durable_writes option properly).,CASSANDRA-2907,12514395,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,benschrauwen,benschrauwen,18/Jul/11 03:53,12/Mar/19 14:03,13/Mar/19 22:26,18/Jul/11 17:13,0.8.2,,,,,,0,,,,,"I am unable to change the durable_writes option in the CLI. Here are the commands to replicate the problem on a clean install:

create keyspace test;
update keyspace test with durable_writes=false;
show keyspaces;

It will still say:

Keyspace: test:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [datacenter1:1]
  Column Families:


PS: I looked in the tests of the CLI code of CASSANDRA-2683 and saw that the feature actually is not properly tested: the flag is set, but never tested.",,,,,,,,,,,,,,,,18/Jul/11 13:34;xedin;CASSANDRA-2907.patch;https://issues.apache.org/jira/secure/attachment/12486866/CASSANDRA-2907.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-18 12:55:05.278,,,no_permission,,,,,,,,,,,,20887,,,Mon Jul 18 18:23:54 UTC 2011,,,,,,0|i0ge6n:,93728,jbellis,jbellis,,,,,,,,,"18/Jul/11 03:54;benschrauwen;BTW, the logs show this:

INFO 23:44:02,549 Applying migration 2da39610-b0f0-11e0-0000-242d50cf1fbf Update keyspace testrep strategy:NetworkTopologyStrategy{}durable_writes: true to testrep strategy:NetworkTopologyStrategy{}durable_writes: true
","18/Jul/11 12:55;xedin;I have discovered that this is not a CLI but rather a Core problem, I will change a title to reflect that.",18/Jul/11 13:34;xedin;rebased with cassandra-0.8 (the latest commit 5616b0235aa4e32e49b58d707de2c98310cf0b9d) ,"18/Jul/11 14:08;benschrauwen;+1

I can confirm that this patch now allows to pass the test I mentioned in the initial post. Thanks for the super fast fix!",18/Jul/11 17:13;jbellis;committed,"18/Jul/11 18:23;hudson;Integrated in Cassandra-0.8 #222 (See [https://builds.apache.org/job/Cassandra-0.8/222/])
    fix updating KS with durable_writes=false
patch by pyaskevich; reviewed by jbellis for CASSANDRA-2907

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147974
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/KSMetaData.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/UpdateKeyspace.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dropped columnfamilies can leave orphaned data files that do not get cleared on restart,CASSANDRA-2942,12515146,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,cdaw,cdaw,23/Jul/11 00:57,12/Mar/19 14:03,13/Mar/19 22:26,29/Aug/11 15:12,1.0.0,,,,,,0,,,,,"* Bring up 3 node cluster
* From node1: Run Stress Tool
{code} stress --num-keys=10 --columns=10 --consistency-level=ALL --average-size-values --replication-factor=3 --nodes=node1,node2 {code}
* Shutdown node3
* From node1: drop the Standard1 CF in Keyspace1
* Shutdown node2 and node3
* Bring up node1 and node2. Check that the Standard1 files are gone.
{code}
ls -al /var/lib/cassandra/data/Keyspace1/
{code}
* Bring up node3. The log file shows the drop column family occurs
{code}
 INFO 00:51:25,742 Applying migration 9a76f880-b4c5-11e0-0000-8901a7c5c9ce Drop column family: Keyspace1.Standard1
{code}
* Restart node3 to clear out dropped tables from the filesystem
{code}
root@cathy3:~/cass-0.8/bin# ls -al /var/lib/cassandra/data/Keyspace1/
total 36
drwxr-xr-x 3 root root 4096 Jul 23 00:51 .
drwxr-xr-x 6 root root 4096 Jul 23 00:48 ..
-rw-r--r-- 1 root root    0 Jul 23 00:51 Standard1-g-1-Compacted
-rw-r--r-- 2 root root 5770 Jul 23 00:51 Standard1-g-1-Data.db
-rw-r--r-- 2 root root   32 Jul 23 00:51 Standard1-g-1-Filter.db
-rw-r--r-- 2 root root  120 Jul 23 00:51 Standard1-g-1-Index.db
-rw-r--r-- 2 root root 4276 Jul 23 00:51 Standard1-g-1-Statistics.db
drwxr-xr-x 3 root root 4096 Jul 23 00:51 snapshots
{code}
*Bug:  The files for Standard1 are orphaned on node3*

",,,,,,,,,,,,,,,,24/Aug/11 02:38;jbellis;2942.txt;https://issues.apache.org/jira/secure/attachment/12491434/2942.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-23 02:22:36.098,,,no_permission,,,,,,,,,,,,20902,,,Mon Aug 29 16:23:17 UTC 2011,,,,,,0|i0geen:,93764,slebresne,slebresne,,,,,,,,,"23/Jul/11 02:22;jbellis;You should be able to reproduce this even on a single node -- just drop a CF, then restart.  It only cleans out marked-for-delete files from known CFs.

May be fixed by CASSANDRA-2521.  Otherwise we can add ""go ahead and clear out marked-for-delete files, even if they don't belong to an active CF"" logic.","23/Aug/11 03:00;jbellis;bq. May be fixed by CASSANDRA-2521

2521 makes it substantially better, but there's still a window where you can miss deletes.

The ""real"" fix is to commitlog-ify schema changes, but that's outside our scope for the forseeable future.

Adding ""wait for outstanding SSTableDeletingTasks"" to our jvm shutdown hook would be almost as good.",24/Aug/11 02:38;jbellis;patch to wait for StorageService.tasks on shutdown.  Also moves CL segment deletion there.,"29/Aug/11 14:56;slebresne;nit: we could log an info message when awaitTermination returns false. It also look like DeletionService could just go away with with this.

But otherwise, +1. I agree it is good enough and not worth going for more complicated.",29/Aug/11 15:12;jbellis;committed,"29/Aug/11 16:23;hudson;Integrated in Cassandra #1054 (See [https://builds.apache.org/job/Cassandra/1054/])
    reduce window where dropped CF sstables may not be deleted
patch by jbellis; reviewed by slebresne for CASSANDRA-2942

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1162849
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/DeletionService.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/util/FileUtils.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
replication_factor > 1 always causes cassandra to return null,CASSANDRA-2960,12515563,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,stevecorona,stevecorona,27/Jul/11 19:37,12/Mar/19 14:03,13/Mar/19 22:26,01/Aug/11 18:42,0.8.3,,,,,,0,,,,,"On a brand new cluster:
	
[default@SimpleTest] create keyspace SimpleTest2 with strategy_options = [{replication_factor:3}];              
16babc60-b886-11e0-0000-c9ff69cb2dfb
Waiting for schema agreement...
... schemas agree across the cluster

[default@SimpleTest] use SimpleTest2;
Authenticated to keyspace: SimpleTest2

[default@SimpleTest2] create column family CFTest with comparator=UTF8Type and default_validation_class=UTF8Type;
1f108660-b886-11e0-0000-c9ff69cb2dfb
Waiting for schema agreement...
... schemas agree across the cluster

[default@SimpleTest2] set CFTest['1']['text'] = 'test';
null

[default@SimpleTest2] get CFTest['1'];
null

[default@SimpleTest2] list CFTest;
Using default limit of 100
null

[default@SimpleTest2] describe cluster;
Cluster Information:
   Snitch: org.apache.cassandra.locator.SimpleSnitch
   Partitioner: org.apache.cassandra.dht.RandomPartitioner
   Schema versions: 
	1f108660-b886-11e0-0000-c9ff69cb2dfb: [10.60.98.20, 10.60.98.24, 10.60.98.26]",Ubuntu 11.04,,,,,,,,,,,,,,,31/Jul/11 03:57;jbellis;2960.txt;https://issues.apache.org/jira/secure/attachment/12488338/2960.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-27 19:58:44.178,,,no_permission,,,,,,,,,,,,20914,,,Tue Nov 29 14:14:06 UTC 2011,,,,,,0|i0geiv:,93783,slebresne,slebresne,,,,,,,,,27/Jul/11 19:38;stevecorona;No errors are being reported in the logs and all nodes are up w/ the same schema version.,"27/Jul/11 19:58;jbellis;i assume you do actually have >= 3 nodes?

what is the full exception from the cli when you run it with --debug?","27/Jul/11 20:03;stevecorona;Yes, running with 3 nodes. Same exact thing happens if I set replication_factor to 2 nodes. replication_factor=1 works as expected, however.

Here is the exception:

[default@SimpleTest2] get CFTest['1'];
null
java.lang.RuntimeException
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:302)
	at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:217)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:345)
Caused by: UnavailableException()
	at org.apache.cassandra.thrift.Cassandra$get_slice_result.read(Cassandra.java:7652)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_get_slice(Cassandra.java:570)
	at org.apache.cassandra.thrift.Cassandra$Client.get_slice(Cassandra.java:542)
	at org.apache.cassandra.cli.CliClient.doSlice(CliClient.java:468)
	at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:603)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:200)
	... 2 more","27/Jul/11 20:06;jbellis;do all the nodes' ""nodetool ring"" show everyone up?

what does the server log at debug level?  (should include which hosts it thinks are unavailable)","27/Jul/11 20:11;stevecorona;root@cassandra01:~# nodetool ring -h 10.60.98.26
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               113427455640312814857969558651062452224     
10.60.98.26     datacenter1 rack1       Up     Normal  59.21 KB        33.33%  0                                           
10.60.98.24     datacenter1 rack1       Up     Normal  67.87 KB        33.33%  56713727820156407428984779325531226112      
10.60.98.20     datacenter1 rack1       Up     Normal  67.87 KB        33.33%  113427455640312814857969558651062452224

Will turn on debug server logs now and follow up. ","27/Jul/11 20:30;stevecorona;Okay- this is what I am seeing:

When I do a read:

==> system.log <==
DEBUG [pool-2-thread-11] 2011-07-27 15:26:28,641 CassandraServer.java (line 303) get_slice

==> output.log <==
DEBUG 15:26:28,646 Command/ConsistencyLevel is SliceFromReadCommand(table='SimpleTest2', key='01', column_parent='QueryPath(columnFamilyName='CFTest', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=1000000)/ONE

==> system.log <==
DEBUG [pool-2-thread-11] 2011-07-27 15:26:28,646 StorageProxy.java (line 509) Command/ConsistencyLevel is SliceFromReadCommand(table='SimpleTest2', key='01', column_parent='QueryPath(columnFamilyName='CFTest', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=1000000)/ONE

==> output.log <==
DEBUG 15:26:28,650 Blockfor/repair is 1/true; setting up requests to 

==> system.log <==
DEBUG [pool-2-thread-11] 2011-07-27 15:26:28,650 ReadCallback.java (line 84) Blockfor/repair is 1/true; setting up requests to 

==> output.log <==
DEBUG 15:26:28,650 Live nodes  do not satisfy ConsistencyLevel (1 required)

==> system.log <==
DEBUG [pool-2-thread-11] 2011-07-27 15:26:28,650 ReadCallback.java (line 211) Live nodes  do not satisfy ConsistencyLevel (1 required)",27/Jul/11 20:44;jbellis;Is it possible it defaulted to NTS?,"27/Jul/11 21:12;stevecorona;Bingo, that's it- looks like on creating a new keyspace with replication_factor it's defaulting to NTS.

[default@unknown] create keyspace SimpleTest12 with strategy_options = [{'replication_factor':'2'}];
[default@SimpleTest12] describe keyspace;
Keyspace: SimpleTest12:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
","27/Jul/11 22:39;jbellis;ah, so then it's treating your options as ""I want 2 replicas in datacenter 'replication_factor'"" but all your nodes are in 'datacenter1'.

I thought we had logic to disallow naming a dc that to prevent this confusion but apparently not.  I'll look into that.",31/Jul/11 03:57;jbellis;patch to raise an exception instead of ignoring replication_factor option in NTS.,01/Aug/11 17:36;slebresne;+1,01/Aug/11 18:42;jbellis;committed,"01/Aug/11 19:14;hudson;Integrated in Cassandra-0.8 #250 (See [https://builds.apache.org/job/Cassandra-0.8/250/])
    throw exception when NTS is given replication_factoras an option
patch by jbellis; reviewed by slebresne for CASSANDRA-2960

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152890
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/NetworkTopologyStrategy.java
","29/Nov/11 08:49;guizhang;we are running 0.84, looks still have this problem.

[default@unknown] create keyspace Gui3;
9deaddb0-1a4b-11e1-0000-21e4216050ff
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] use Gui3;                      
Authenticated to keyspace: Gui3
[default@Gui3] create column family gui_test3;
a997c1f0-1a4b-11e1-0000-21e4216050ff
Waiting for schema agreement...
... schemas agree across the cluster
[default@Gui3] set gui_test3['jsmith']['first'] = 'John';
Null

if we use below script
 create keyspace Gui2 with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = [{replication_factor:3}];
it works fine.

[guzhang@slcdbx5001-01 bin]$ /opt/cassandra/bin/nodetool -host localhost version
ReleaseVersion: 0.8.4
","29/Nov/11 14:14;jbellis;""on creating a new keyspace ... it's defaulting to [NetworkTopologyStrategy].""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL regex to match column family in query,CASSANDRA-2939,12515057,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,bcvisin,bcvisin,bcvisin,22/Jul/11 19:33,12/Mar/19 14:03,13/Mar/19 22:26,08/Aug/11 18:47,,,,,,,0,CQL,python,,,"In the file cursor.py (https://svn.apache.org/repos/asf/cassandra/drivers/py/cql/cursor.py)

{code}Line 37: _cfamily_re = re.compile(""\s*SELECT\s+.+\s+FROM\s+[\']?(\w+)"", re.I | re.M){code}

The Regex will improperly match anything after the word  'from' even if it is in the WHERE clause

I believe the fix is:

{code}_cfamily_re = re.compile(""\s*SELECT\s+.+?\s+FROM\s+[\']?(\w+)"", re.I | re.M){code}

Added the ? so the regex is not so greedy

use this query to reproduce the results:

SELECT key FROM column_family WHERE key = 'break from chores'""",Python CQL,,,,,,,,,,,,,,,08/Aug/11 17:29;thobbs;2939-test.txt;https://issues.apache.org/jira/secure/attachment/12489714/2939-test.txt,08/Aug/11 17:29;thobbs;2939.txt;https://issues.apache.org/jira/secure/attachment/12489713/2939.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-08 17:29:24.55,,,no_permission,,,,,,,,,,,,20901,,,Mon Aug 08 18:47:43 UTC 2011,,,,,,0|i0gedz:,93761,thobbs,thobbs,,,,,,,,,"08/Aug/11 17:29;thobbs;2939.txt updates the regex to the one provided by Blake. (In case you're not familiar with the syntax, ""\+?"" is a non-greedy version of ""+"".)

2939-test.txt adds a small unit test to exercise the Cursor._cfamily_re regex.",08/Aug/11 18:47;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assert err on SystemTable.getCurrentLocalNodeId during a cleanup,CASSANDRA-2824,12511491,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,cywjackson,cywjackson,24/Jun/11 16:34,12/Mar/19 14:03,13/Mar/19 22:26,28/Jun/11 07:59,0.8.2,,,,,,0,,,,,"when running nodetool cleanup the following happened:

$ ./bin/nodetool cleanup --host localhost
Exception in thread ""main"" java.lang.AssertionError
at org.apache.cassandra.db.SystemTable.getCurrentLocalNodeId(SystemTable.java:383)
at org.apache.cassandra.utils.NodeId$LocalNodeIdHistory.<init>(NodeId.java:179)
at org.apache.cassandra.utils.NodeId.<clinit>(NodeId.java:38)
at org.apache.cassandra.utils.NodeId$OneShotRenewer.<init>(NodeId.java:159)
at org.apache.cassandra.service.StorageService.forceTableCleanup(StorageService.java:1317)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
at sun.rmi.transport.Transport$1.run(Transport.java:159)
at java.security.AccessController.doPrivileged(Native Method)
at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662) 

",,,,,,,,,,,,,,,,27/Jun/11 14:30;slebresne;2824.patch;https://issues.apache.org/jira/secure/attachment/12483958/2824.patch,27/Jun/11 17:39;slebresne;2824_v2.patch;https://issues.apache.org/jira/secure/attachment/12483980/2824_v2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-27 14:30:30.668,,,no_permission,,,,,,,,,,,,20851,,,Tue Jun 28 08:23:06 UTC 2011,,,,,,0|i0gdon:,93647,jbellis,jbellis,,,,,,,,,"27/Jun/11 14:30;slebresne;There is possibly a race here that triggers the assert. The code is relying on the fact that system tables have a gc_grace of 0 to assume it cannot get tombstone back, but given that gcbefore is at the precision of a second, and given than that on tie with the tombstone timestamp we do include the tombstone, we could get tombstone back.

Attaching patch that takes a safer road.",27/Jun/11 15:03;jbellis;why not just call removeDeleted?,"27/Jun/11 17:39;slebresne;You're right, I skipped the fact that gcBefore==0 is different from gc_grace==0. Patch v2 attached.",27/Jun/11 19:31;jbellis;+1 (can you include a link here in the comment when you commit?),"28/Jun/11 07:59;slebresne;Committed (with link to the issue in comment), thanks","28/Jun/11 08:23;hudson;Integrated in Cassandra-0.8 #195 (See [https://builds.apache.org/job/Cassandra-0.8/195/])
    Avoids race in SystemTable.getCurrentLocalNodeId
patch by slebresne; reviewed by jbellis for CASSANDRA-2824

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1140472
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool hangs (doesn't return prompt) if you specify a table that doesn't exist or a KS that has no CF's,CASSANDRA-2933,12514940,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,cdaw,cdaw,21/Jul/11 16:02,12/Mar/19 14:03,13/Mar/19 22:26,27/Jul/11 15:25,0.8.3,,,Tool/nodetool,,,0,lhf,,,,"Invalid CF
{code}
ERROR 02:18:18,904 Fatal exception in thread Thread[AntiEntropyStage:3,5,main]
java.lang.IllegalArgumentException: Unknown table/cf pair (StressKeyspace.StressStandard)
	at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:147)
	at org.apache.cassandra.service.AntiEntropyService$TreeRequestVerbHandler.doVerb(AntiEntropyService.java:601)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}


Empty KS
{code}
 INFO 02:19:21,483 Waiting for repair requests: []
 INFO 02:19:21,484 Waiting for repair requests: []
 INFO 02:19:21,484 Waiting for repair requests: []
{code}",,,,,,,,,,,,,,,,27/Jul/11 12:30;yukim;ASF.LICENSE.NOT.GRANTED--2933.txt;https://issues.apache.org/jira/secure/attachment/12487966/ASF.LICENSE.NOT.GRANTED--2933.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-27 12:30:34.307,,,no_permission,,,,,,,,,,,,20897,,,Wed Jul 27 16:12:39 UTC 2011,,,,,,0|i0gecn:,93755,slebresne,slebresne,,,,,,,,,27/Jul/11 12:30;yukim;Patch attached to check validity of given column families.,"27/Jul/11 15:25;slebresne;+1 (Committed, thanks)","27/Jul/11 16:12;hudson;Integrated in Cassandra-0.8 #240 (See [https://builds.apache.org/job/Cassandra-0.8/240/])
    check column family input validity in nodetool repair
patch by yukim; reviewed by slebresne for CASSANDRA-2933

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1151497
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra doesn't start on Red Hat Linux due to hardcoded JAVA_HOME,CASSANDRA-2992,12517928,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,tarasp,tarasp,04/Aug/11 14:26,12/Mar/19 14:03,13/Mar/19 22:26,08/Aug/11 13:11,0.8.4,,,Packaging,,,0,,,,,"On CentOS /etc/init.d/cassandra has
bq. export JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0/

While there is no such a directory on our server it was ok for 0.8.2, because /usr/sbin/cassandra checked the executable
{quote}
if [ -x $JAVA_HOME/bin/java ]; then
    JAVA=$JAVA_HOME/bin/java
else
    JAVA=`which java`
fi
{quote}

But 0.8.3 builds replaced the above code with one that doesn't check if JAVA_HOME is set correctly.
{quote}
if [ -n ""$JAVA_HOME"" ]; then
    JAVA=""$JAVA_HOME/bin/java""
else
    JAVA=java
fi
{quote}

That's why cassandra doesn't start anymore.


The correct fix would be to remove ""export JAVA_HOME"" from /etc/init.d/cassandra or set it only to correct path and only if it hasn't already been set.

It would also be nice to revert to ""[ -x $JAVA_HOME/bin/java ]"" in /usr/sbin/cassandra
",CentOS release 5.6,,,,,,,,,,,,,,,06/Aug/11 22:53;thepaul;0001-don-t-hardcode-JAVA_HOME-in-redhat-initscript.patch.txt;https://issues.apache.org/jira/secure/attachment/12489600/0001-don-t-hardcode-JAVA_HOME-in-redhat-initscript.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-04 14:37:45.04,,,no_permission,,,,,,,,,,,,20927,,,Mon Aug 08 14:14:42 UTC 2011,,,,,,0|i0gepr:,93814,tarasp,tarasp,,,,,,,,,04/Aug/11 14:37;jbellis;Is this fixed by CASSANDRA-2785 ?,"04/Aug/11 14:51;tarasp;No, CASSANDRA-2785 actually caused this bug, since now invalid JAVA_HOME is not ignored:
http://svn.apache.org/viewvc/cassandra/branches/cassandra-0.8/bin/cassandra?r1=1126728&r2=1152241&pathrev=1152241

But the real bug is setting JAVA_HOME to a fixed value in redhat specific scripts.","06/Aug/11 20:26;thepaul;I don't want to get into the business of second-guessing the admin when a configuration parameter is set, so I vote we fix the hard-coded JAVA_HOME in redhat/cassandra. Probably by borrowing the jvm search stuff from debian/init and adjusting the directory list appropriately.","06/Aug/11 21:02;jbellis;so, use existing JAVA_HOME if set, else search for it?  sounds good to me.","06/Aug/11 22:53;thepaul;0001: add search for appropriate JAVA_HOME in redhat initscript, now that cassandra-env.sh expects the right $JAVA to be preconfigured (see #2785).","06/Aug/11 22:57;thepaul;Taras, would you mind verifying that the list of JVM search dirs I included matches what you expect to find? I'm not as familiar with the range of java deployment practices on RH-based OSes.",08/Aug/11 12:29;tarasp;Our system administrators say the patch is perfect.,"08/Aug/11 13:11;jbellis;Committed.

","08/Aug/11 13:11;jbellis;Note that CASSANDRA-2785 was reverted for the second 0.8.3 vote, so I'm updating both affects and fix-for versions to 0.8.4.","08/Aug/11 14:14;hudson;Integrated in Cassandra-0.8 #260 (See [https://builds.apache.org/job/Cassandra-0.8/260/])
    don't hardcode JAVA_HOME in RH init script
patch by Paul Cannon; reviewed by Taras Puchko for CASSANDRA-2992

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1154950
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/redhat/cassandra
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
row delete breaks read repair,CASSANDRA-2590,12505899,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,02/May/11 03:44,12/Mar/19 14:03,13/Mar/19 22:26,09/Jun/11 13:51,0.7.7,0.8.1,,,,,0,,,,,"related to CASSANDRA-2589 

Working at CL ALL can get inconsistent reads after row deletion. Reproduced on the 0.7 and 0.8 source. 

Steps to reproduce:

# two node cluster with rf 2 and HH turned off
# insert rows via cli 
# flush both nodes 
# shutdown node 1
# connect to node 2 via cli and delete one row
# bring up node 1
# connect to node 1 via cli and issue get with CL ALL 
# first get returns the deleted row, second get returns zero rows.

RowRepairResolver.resolveSuperSet() resolves a local CF with the old row columns, and the remote CF which is marked for deletion. CF.resolve() does not pay attention to the deletion flags and the resolved CF has both markedForDeletion set and a column with a lower timestamp. The return from resolveSuperSet() is used as the return for the read without checking if the cols are relevant. 

Also when RowRepairResolver.mabeScheduleRepairs() runs it sends two mutations. Node 1 is given the row level deletation, and Node 2 is given a mutation to write the old (and now deleted) column from node 2. I have some log traces for this if needed. 

A quick fix is to check for relevant columns in the RowRepairResolver, will attach shortly.    ",,,,,,,,,,,,,,,,09/May/11 10:04;amorton;0001-2590-v3.patch;https://issues.apache.org/jira/secure/attachment/12478579/0001-2590-v3.patch,02/May/11 04:06;amorton;0001-cf-resolve-test-and-possible-solution-for-read-repai.patch;https://issues.apache.org/jira/secure/attachment/12477928/0001-cf-resolve-test-and-possible-solution-for-read-repai.patch,03/May/11 22:01;jbellis;2590-v2.txt;https://issues.apache.org/jira/secure/attachment/12478099/2590-v2.txt,09/Jun/11 00:45;jbellis;2590-v4-0.7.txt;https://issues.apache.org/jira/secure/attachment/12481882/2590-v4-0.7.txt,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-05-03 21:20:30.807,,,no_permission,,,,,,,,,,,,20715,,,Thu Jun 09 14:08:59 UTC 2011,,,,,,0|i0gc9z:,93419,jbellis,jbellis,,,,,,,,,02/May/11 04:06;amorton;unit test to show columns in a deleted CF after calling resolve() and a hack fix for the use case described above.,"03/May/11 21:20;jbellis;So the problem is that something in repair isn't calling removeDeleted?  That's the approach we normally take; it's like your ensureRelevant, but it doesn't mutate the original copy (which is important since the original might be part of a cache).  Here's your test modified to use that:

{code}
    @Test
    public void testEnsureRelevant()
    {
        ColumnFamily cf1 = ColumnFamily.create(""Keyspace1"", ""Standard1"");
        cf1.addColumn(column(""one"", ""A"", 0));

        ColumnFamily cf2 = ColumnFamily.create(""Keyspace1"", ""Standard1"");
        cf2.delete((int) (System.currentTimeMillis() / 1000), 1);

        cf2.resolve(cf1);
        assert cf2.getColumnCount() == 1;
        
        ColumnFamily cleaned = ColumnFamilyStore.removeDeleted(cf2, Integer.MAX_VALUE);
        assert cleaned == null;
    }
{code}","03/May/11 22:01;jbellis;... but that's not what we want for RowRepairResolver. (I freely admit that dealing with tombstones is subtle and tricky. :)

removeDeleted will give you back a version of the row with any GC-able tombstones removed. That's not what we want for read repair; we want to preserve tombstones, but we want a ""canonical"" representation of only the minimum tombstones necessary. (Technically, this doesn't matter for the repair per se, because repairing obsolete data is harmless. What we're concerned with is getting the right result back to the client, and thriftifyColumns & friends in CassandraServer assume that canonicalization has been performed previously.)

So we do want to do what you were doing with ensureRelevant, but it's a little more complex than that because we have the same problem at the supercolumn level, as at the row level.

QueryFilter.collectCollatedColumns is responsible for doing this when merging different versions from memtables and sstables, so we just need to wire it up in RRR. Here's a patch that uses an IdentityQueryFilter to do this.","07/May/11 03:08;amorton;Thanks, am doing some more tests on super columns","09/May/11 10:04;amorton;2590-v3 uses removeDeleted() in RowRepairResolver.resolveSuperset() and includes tests in RowResolverTest.Patch is for 0.8.

CASSANDRA-2621 shows that QueryFilter.collectCollatedColumns() returns a CF with deleted columns and the caller should use removeDeleted. 

Continuing to use CF.resolve() seemed like the minimum change. Let me know if you think we should still use QueryFilter to resolve the differences. ","09/Jun/11 00:45;jbellis;I think we're almost there.

The trick is you actually need _both_ collectCollatedColumns and removeDeleted, since rD assumes cCC has already been called (which it is, when we're merging versions from different sstables...  but not when we're merging versions from different replicas, as in RRR).

Added a test (testResolveDeletedSuper) to illustrate this.  Fails against v3 (rD but no cCC) but passes w/ v4 (cCC and rD).","09/Jun/11 08:37;slebresne;+1 on v4, we do need both calls.

That being said, we should probably refactor that part of the code someday because it is not the cleanest thing ever. And there is probably ways to avoid those two phases (which does do some duplicate works I believe).",09/Jun/11 13:51;jbellis;committed.  thanks Aaron!,"09/Jun/11 14:08;hudson;Integrated in Cassandra-0.7 #504 (See [https://builds.apache.org/job/Cassandra-0.7/504/])
    fix removing columns and subcolumns that are supressed by a row orsupercolumn tombstone during replica resolution
patch by Aaron Morton and jbellis; reviewed by slebresne for CASSANDRA-2590

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1133873
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/test/unit/org/apache/cassandra/service/RowResolverTest.java
* /cassandra/branches/cassandra-0.7/test/unit/org/apache/cassandra/Util.java
* /cassandra/branches/cassandra-0.7/test/unit/org/apache/cassandra/db/TableTest.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/RowRepairResolver.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removed/Dead Node keeps reappearing,CASSANDRA-2371,12502232,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,acctech,acctech,23/Mar/11 22:03,12/Mar/19 14:03,13/Mar/19 22:26,18/Apr/11 18:59,0.7.5,,,Legacy/Tools,,,2,,,,,"The removetoken option does not seem to work. The original node 10.240.50.63 comes back into the ring, even after the EC2 instance is no longer in existence. Originally I tried to add a new node 10.214.103.224 with the same token, but there were some complications with that. I have pasted below all the INFO log entries found with greping the system log files.

Seems to be a similar issue seen with http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Ghost-node-showing-up-in-the-ring-td6198180.html 

INFO [GossipStage:1] 2011-03-16 00:54:31,590 StorageService.java (line 745) Nodes /10.214.103.224 and /10.240.50.63 have the same token 95704415696513900000000000000000000000.  /10.214.103.224 is the new owner
 INFO [GossipStage:1] 2011-03-16 17:26:51,083 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:27:24,767 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:29:30,191 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:31:35,609 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:33:39,440 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-23 17:22:55,520 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.240.50.63


 INFO [GossipStage:1] 2011-03-10 03:52:37,299 Gossiper.java (line 608) Node /10.240.50.63 is now part of the cluster
 INFO [GossipStage:1] 2011-03-10 03:52:37,545 Gossiper.java (line 600) InetAddress /10.240.50.63 is now UP
 INFO [HintedHandoff:1] 2011-03-10 03:53:36,168 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.240.50.63
 INFO [HintedHandoff:1] 2011-03-10 03:53:36,169 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.240.50.63
 INFO [GossipStage:1] 2011-03-15 23:23:43,770 Gossiper.java (line 623) Node /10.240.50.63 has restarted, now UP again
 INFO [GossipStage:1] 2011-03-15 23:23:43,771 StorageService.java (line 726) Node /10.240.50.63 state jump to normal
 INFO [HintedHandoff:1] 2011-03-15 23:28:48,957 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.240.50.63
 INFO [HintedHandoff:1] 2011-03-15 23:28:48,958 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.240.50.63
 INFO [ScheduledTasks:1] 2011-03-15 23:37:25,071 Gossiper.java (line 226) InetAddress /10.240.50.63 is now dead.
 INFO [GossipStage:1] 2011-03-16 00:54:31,590 StorageService.java (line 745) Nodes /10.214.103.224 and /10.240.50.63 have the same token 95704415696513900000000000000000000000.  /10.214.103.224 is the new owner
 WARN [GossipStage:1] 2011-03-16 00:54:31,590 TokenMetadata.java (line 115) Token 95704415696513900000000000000000000000 changing ownership from /10.240.50.63 to /10.214.103.224
 INFO [GossipStage:1] 2011-03-18 23:37:09,158 Gossiper.java (line 610) Node /10.240.50.63 is now part of the cluster
 INFO [GossipStage:1] 2011-03-21 23:37:10,421 Gossiper.java (line 610) Node /10.240.50.63 is now part of the cluster
 INFO [GossipStage:1] 2011-03-21 23:37:10,421 StorageService.java (line 726) Node /10.240.50.63 state jump to normal
 INFO [GossipStage:1] 2011-03-23 17:22:55,520 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.240.50.63
 INFO [ScheduledTasks:1] 2011-03-23 17:22:55,521 HintedHandOffManager.java (line 210) Deleting any stored hints for 10.240.50.63
",Large Amazon EC2 instances. Ubuntu 10.04.2 ,,,,,,,,,,,,,,,23/Mar/11 22:43;brandon.williams;2371.txt;https://issues.apache.org/jira/secure/attachment/12474449/2371.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-23 22:43:23.439,,,no_permission,,,,,,,,,,,,20586,,,Mon Apr 18 19:43:59 UTC 2011,,,,,,0|i0gazr:,93211,,,,,,,,,,,"23/Mar/11 22:43;brandon.williams;My guess is what is happening is that after aVeryLongTime when the state is evicted, another gossip round occurs and the state is repopulated.  Patch to re-quarantine on eviction to avoid this.",23/Mar/11 23:30;acctech;This looks like it's changing the source code. Can we deploy this on a live cluster? ,01/Apr/11 04:09;jbellis;Did you get a chance to try a patched build?,"01/Apr/11 16:48;acctech;Actually the issue was resolved when we did a restart of the cluster, and ran the nodetool removetoken command when only a portion of the nodes had started up. The main reason I didn't apply the patch yet, is merely because I need to learn how to patch the code and integrate the new build first... Thank you!","07/Apr/11 00:05;brandon.williams;There is a second problem here.  We don't populate the gossiper's application state to LEFT for removetoken, only for decommission.  The problem with adding it, however, is that SS.handleStateLeft gets called every gossip round and runs through the hint removal process.  One option may be to check if the node is locally persisted and if not, just ignore the message since we never knew about it anyway.  Another is to just not remove hints when we see the LEFT state, because they'll expire anyway and large unaccessed rows aren't a problem, so this seems like a throwback from the <=0.6 days.","10/Apr/11 09:03;jeromatron;I applied the patch and within a few days even with no restarting of any of the Cassandra nodes, the removed token came back. Just FYI.","11/Apr/11 16:11;jbellis;bq. The problem with adding it, however, is that SS.handleStateLeft gets called every gossip round and runs through the hint removal process

But it only runs hint removal once per node, right?  Or is ""onChange"" not an accurate method name?

bq. One option may be to check if the node is locally persisted and if not, just ignore the message since we never knew about it anyway.

Locally persisted... with a token in SystemTable?  Ignore the ... hint message?

bq. Another is to just not remove hints when we see the LEFT state

We should issue a delete to the hints row, but we should not force a major compaction. The rule of thumb is, avoiding inflicting a performance hit on the cluster trumps immediate disk space cleanup.","14/Apr/11 22:55;brandon.williams;It looks like the bad news here is that gossip makes many assumptions about a node being alive when it hears about it, and has no provisions for keeping removed node state.  This is bad, but highly exacerbated by keeping the removed node state for 3 days instead of 30s.  I think the best solution right now is to revert CASSANDRA-2115 until we can overhaul gossip to handle this.  The bug will still exist as it always has, but the window to trigger it is far shorter.",15/Apr/11 00:50;jbellis;Sounds reasonable.,18/Apr/11 18:59;brandon.williams;Reverted CASSANDRA-2115 and created CASSANDRA-2496 to fix this correctly.,"18/Apr/11 19:43;hudson;Integrated in Cassandra-0.7 #440 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/440/])
    Revert ""Keep endpoint state until aVeryLongTime when not a fat client""

This reverts commit db9164ffd96ebc7752fc5789c90c7211ba323ad2.

For CASSANDRA-2371.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ColumnFamilyRecordReader fails for a given split because a host is down, even if records could reasonably be read from other replica.",CASSANDRA-2388,12502417,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,pauloricardomg,eldondev,eldondev,25/Mar/11 20:40,12/Mar/19 14:03,13/Mar/19 22:26,20/Nov/15 13:54,2.1.12,2.2.4,,Legacy/Tools,,,6,hadoop,inputformat,,,ColumnFamilyRecordReader only tries the first location for a given split. We should try multiple locations for a given split.,,,,,,,,,,,,,,,,26/Mar/11 01:32;eldondev;0002_On_TException_try_next_split.patch;https://issues.apache.org/jira/secure/attachment/12474683/0002_On_TException_try_next_split.patch,25/Jun/14 19:22;pauloricardomg;1.2-CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12652488/1.2-CASSANDRA-2388.patch,25/Jun/14 19:22;pauloricardomg;2.0-CASSANDRA-2388-v2.patch;https://issues.apache.org/jira/secure/attachment/12652489/2.0-CASSANDRA-2388-v2.patch,09/Jun/14 14:11;pauloricardomg;2.0-CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12649379/2.0-CASSANDRA-2388.patch,28/Jun/11 15:02;michaelsembwever;CASSANDRA-2388-addition1.patch;https://issues.apache.org/jira/secure/attachment/12484458/CASSANDRA-2388-addition1.patch,04/Jul/11 14:01;michaelsembwever;CASSANDRA-2388-extended.patch;https://issues.apache.org/jira/secure/attachment/12485146/CASSANDRA-2388-extended.patch,02/Jul/11 22:14;michaelsembwever;CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12485071/CASSANDRA-2388.patch,22/Jun/11 13:46;michaelsembwever;CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12483441/CASSANDRA-2388.patch,11/Jun/11 09:15;michaelsembwever;CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12482140/CASSANDRA-2388.patch,09/Jun/11 06:29;michaelsembwever;CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12481890/CASSANDRA-2388.patch,,10.0,,,,,,,,,,,,,,,,,,,2011-03-25 22:45:55.124,,,no_permission,,,,,,,,,,,,20597,,,Fri Nov 20 13:54:28 UTC 2015,,,,,,0|i07hqv:,41642,pkolaczk,pkolaczk,,,,,,,,,25/Mar/11 22:45;brandon.williams;I'm not sure special casing NoRouteToHostException to be blacklisted is the best thing to do.  I don't think connections are being setup so often that maintaining a blacklist for any reason is needed.,"26/Mar/11 18:24;brandon.williams;Unfortunately, I thought of another problem here.  If we go over the entire replica set, we're potentially going outside of the DC, which is bad since a lot of installations have a DC dedicated to analytics so it doesn't affect their app. It seems that the local address is preferred though, are your task trackers not on the same machines as Cassandra?","17/May/11 21:07;jbellis;Eldon, are you planning to take another stab at this?",17/May/11 21:09;tjake;We need to return the list of replicas in the same DC,"23/May/11 14:45;jbellis;Mck, do you want to take a stab at this?",23/May/11 16:27;michaelsembwever;I'm having a go currently at CASSANDRA-1125 so i might as well look at this too. (but you've caught me on a holiday-week...),"07/Jun/11 14:11;michaelsembwever;How do i obtain the DataCenter name for a given address?

IEndpointSnitch.getDataCenter(inetAddress) would work nicely for me but how do i get the snitch client-side?","07/Jun/11 20:41;michaelsembwever;Initial attempt at solution. Although I'm a little apprehensive to the additions to cassandra.thrift
(describe_rack(..) isn't used anywhere, it just made sense to add describe_datacenter(..) and describe_rack(..) at the same time).

I've tested that existing hadoop jobs work but the new functionality hasn't been tested (as i currently don't have any RF=2 data setup).

This patch does not include the required re-generated Cassandra.java","09/Jun/11 06:29;michaelsembwever;Second attempt. (god only knows what i was trying to test last patch ;)
this patch:
 - adds describe_datacenter and describe_rack to cassandra.thrift
 - adds locations in ColumnFamilyRecordReader from the split's alternative endpoints if dc is the same

This patch does not include the required re-generated Cassandra.java
","09/Jun/11 10:22;michaelsembwever;I have tested this now on data w/ RF=2.
Seems to work ~ok as far as i can see.

One side-effect of this patch is where once one could configure ConfigHelper.setInitialAddress(conf, ""localhost"") this will no longer work for tasks trying to run on the down node.
ColumnFamilyRecordReader.getLocations() will ConnectException trying to call describe_datacenter(..). This will lead to the task failing. Hadoop re-runs the task then on another node and eventually the job will complete. But the fall back to replica never is used.

If the initialAddress is hardcoded to one node then we no longer have a decentralised job.

I would like to allow a comma-separated in initialAddress, for example it could be ""localhost, node01, node02, node03"". This would give preference to localhost and avoid any centralisation.

I would also like to make ColumnFamilyRecordReader.getLocations() return an iterator instead of an array.
The createConnection(..) and client.describe_datacenter(..) calls are an unnecessary overhead when all nodes (or first endpoint location) are up, and could be avoided by lazy-loading the list.","11/Jun/11 09:15;michaelsembwever;New patch. I think i'm at last happy with it.

getLocations() returns an iterator so client.describe_datacenter() is only called when necessary.

Rather than provide a list in initialAddress it was possible to use either the initialAddress OR the endpoint. This gave the benefit in not listing a location that can't actually be connected to.

The ""only use replica from same DC"" is an option now in ConfigHelper. By default it is true.

Again the re-generated Cassandra.java is not included in the patch.

I have tested this on normal jobs, and RF=2 jobs with a node down.","13/Jun/11 15:23;tjake;The get_rack seems unused so it should be removed.

Also, it might be better to pass all locations in the get_datacenter thrift call since you can get the results in one shot and sort them by the dynamic snitch, filtering out the dead nodes:

{noformat}
  DatabaseDescriptor.getEndpointSnitch().sortByProximity(FBUtilities.getLocalAddress(), endpoints); 
{noformat}

{noformat}
  FailureDetector.instance.isAlive(endpoint)
{noformat}

","13/Jun/11 16:15;michaelsembwever;Then (if i understand you correctly) i would need in cassandra.thrift
{noformat}
      /** returns alive endpoints, sorted by proximity, that belong in the same datacenter as the given endpoint */
  list<string> get_endpoints_in_same_datacenter(1: string endpoint, 2: required list<string> endpoints)
    throws (1:InvalidRequestException ire)
{noformat}

Then the API becomes quite specific to this usecase. Is the performance gain worth it? What's the cost of each client.describe_datacenter(..) call, and probably more important what is the lost performance of writing to the furthest node that's within the same datacenter?","13/Jun/11 16:40;michaelsembwever;Just make sure i understand you T Jake, you would rather something like this in CassandraServer.java?
(I've renamed from the previous comment get_endpoints_in_same_datacenter(..) to sort_endpoints_by_proximity(..))
{noformat}
    public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC) 
        throws TException, InvalidRequestException
    {
        try
        {
            List<String> results = new ArrayList<String>();
            InetAddress address = InetAddress.getByName(endpoint);
            String datacenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(address);
            List<InetAddress> addresses = new ArrayList<InetAddress>();
            for(String ep : endpoints)
            {
                addresses.add(InetAddress.getByName(ep));
            }
            DatabaseDescriptor.getEndpointSnitch().sortByProximity(address, addresses);
            for(InetAddress ep : addresses)
            {
                String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(ep);
                if(FailureDetector.instance.isAlive(ep) && (!restrictToSameDC || datacenter.equals(dc)))
                {
                    results.add(ep.getHostName());
                }
            }
            return results.toArray(new String[results.size()]);
        }
        catch (UnknownHostException e)
        {
            throw new InvalidRequestException(e.getMessage());
        }
    }
{noformat}","13/Jun/11 16:59;tjake;bq. what is the lost performance of writing to the furthest node that's within the same datacenter?

The benefit is really the DynamicSnitch. if a node it slow due to compaction then this would avoid sending requests there... 

bq. public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC)

I don't think it makes sense to send the client endpoint to this call since the endpoint might not be a cassandra node.  It's a reasonable assumption that the endpoint it's talking to is local enough to the client to use that.

","13/Jun/11 17:52;michaelsembwever;{quote}
bq.   public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC)
I don't think it makes sense to send the client endpoint to this call since the endpoint might not be a cassandra node. It's a reasonable assumption that the endpoint it's talking to is local enough to the client to use that.
{quote}
For the test set i was running against, RF=2, each split's has two endpoints always in different datacenters.

If the ""local"" endpoint is down then getLocations() will then call client.sort_endpoints_by_proximity(..) and this will fail (being the same endpoint).
It then makes a client connection through the ""other"" endpoint. \[see CFRR.describeDatacenter(..)].
This will presume the wrong datacenter and return itself as a valid endpoint. 
I need some way to know what the original datacenter is, even when it is down.","13/Jun/11 18:19;tjake;ok but why not change the response to map<string,list<string>>  where key is DC and value are proximity sorted endpoints?","13/Jun/11 19:19;michaelsembwever;Won't the sorting still be wrong?
For the use-case above it will solve restricting to the correct datacenter, but the sorting will still be based on proximity to the wrong node?

bq. I don't think it makes sense to send the client endpoint to this call since the endpoint might not be a cassandra node. 
It might not be an alive cassandra node, but it should be a cassandra node. It comes from the split's list of endpoints. At least in this use-case, or are you referring to general usage for this new api?
bq. It's a reasonable assumption that the endpoint it's talking to is local enough to the client to use that.
I don't think so... The endpoint that it talks to is a completely random (just the next endpoint listed in the split's list). This is why i think that such sorting won't just be wrong but not even close. Does this make sense?","14/Jun/11 13:07;tjake;I think the core issue is you can't assume the hadoop node is running on a cassandra node...

If it is then the logic is straight forward, if not then it's possible the connection could cross DC boundaries. One possibility is to use the ip octets like the RackInferringSnitch.  

How's this proposal then?  keep the sort_endpoints_by_proximity signature as is and pass the client endpoint along with the list of data endpoints and add the following logic:

1) sort the endpoints using the endpoint_snitch.
2) if client endpoint *is* a valid cassandra node get the nodes DC and prune nodes outside of this DC
3) if client endpoint *is not* a valid cassandra node try to infer the DC from its ip and prune dataendpoint nodes in a different DC. If no cassandra nodes are in the DC list goto 3).
4) all else fails return the sorted endpoint list
","15/Jun/11 11:23;michaelsembwever;bq. [snip] One possibility is to use the ip octets like the RackInferringSnitch. 

In our usecase we have three nodes defined via PropertyFileSnitch:{noformat}152.90.241.22=DC1:RAC1 #node1
152.90.241.23=DC2:RAC1 #node2
152.90.241.24=DC1:RAC1 #node3{noformat}
The only way to infer here is even addresses belong to one dc, odd to the other. This is not how RackInferringSnithc works.

When we make the connection through the ""other"" (node2) endpoint taking the rack inferring approach ""152.90."" will say it's in DC2. (again) this is the wrong DC and will return itself as a valid endpoint....

Step (3) seems to me to be too specific to be included here.
If i go only with steps (1),(2),and (4) we get this code:{noformat}    public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC) 
            throws TException, InvalidRequestException
    {
        try
        {
            List<String> results = new ArrayList<String>();
            InetAddress address = InetAddress.getByName(endpoint);
            boolean endpointValid = null != Gossiper.instance.getEndpointStateForEndpoint(address);
            String datacenter = DatabaseDescriptor
                    .getEndpointSnitch().getDatacenter(endpointValid ? address : FBUtilities.getLocalAddress());
            List<InetAddress> addresses = new ArrayList<InetAddress>();
            for(String ep : endpoints)
            {
                addresses.add(InetAddress.getByName(endpoint));
            }
            DatabaseDescriptor.getEndpointSnitch().sortByProximity(address, addresses);
            for(InetAddress ep : addresses)
            {
                String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(ep);
                if(FailureDetector.instance.isAlive(ep) && (!restrictToSameDC || datacenter.equals(dc)))
                {
                    results.add(ep.getHostName());
                }
            }
            return results.toArray(new String[results.size()]);
        }
        catch (UnknownHostException e)
        {
            throw new InvalidRequestException(e.getMessage());
        }
    }{noformat}

I'm happy with this (except that {{Gossiper.instance.getEndpointStateForEndpoint(address)}} is only my guess on how to tell if an endpoint is valid as such).","22/Jun/11 13:04;michaelsembwever;Problem with the suggested approach is that sortByProximity(..) *only* works when address is the local address. See assert statement DynamicEndpointSnitch:134

I could hack this and rewrite the line to
{noformat}IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
snitch = snitch instanceof DynamicEndpointSnitch ? ((DynamicEndpointSnitch)snitch).subsnitch : snitch;
snitch.sortByProximity(address, addresses);{noformat}
But this of course means that we always bypass DynamicEndpointSnitch's ""scores"".","22/Jun/11 13:46;michaelsembwever;Up to date patch.
Follows T Jake's points (1),(2), and (4).
And bypasses DynamicEndpointSnitch when sorting by proximity.",24/Jun/11 15:43;tjake;committed with a change to use the dynamic snitch id the passed endpoint is valid.,"24/Jun/11 16:43;hudson;Integrated in Cassandra-0.8 #191 (See [https://builds.apache.org/job/Cassandra-0.8/191/])
    Change ColumnFamilyRecordReader to read split from replicas if primary is down

Patch by Mck SembWever; reviewed by tjake for CASSANDRA-2388

jake : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1139358
Files : 
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
* /cassandra/branches/cassandra-0.8/interface/cassandra.thrift
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
",25/Jun/11 00:50;jeromatron;This patch applies to the current 0.7-branch with minimal problems - just some imports on CassandraServer that it couldn't resolve properly.  Can this be committed against 0.7-branch for inclusion in 0.7.7?,25/Jun/11 01:05;jeromatron;I've done basic testing with the word count and pig examples to make sure that the basic hadoop integration isn't negatively affected by this.  I'll also try it against our dev cluster before and after the patch - killing one node to see if it fails over to another replica - to make sure it does what it should that way.,25/Jun/11 01:15;jeromatron;Reopening for testing against 0.7.6.,"25/Jun/11 02:44;jbellis;Took a look at this belatedly.  I don't understand the contortions at all.  It looks like there's a ton of effort put in to avoiding making sortByProximity work w/ non-local nodes.  Why not just make that work instead?
","25/Jun/11 02:48;jbellis;also: running hadoop on a non-cassandra node is dumb.  i don't see a point in supporting that really.  (yes, my fault it was written that way to begin with, mea culpa.)",25/Jun/11 02:58;jeromatron;Jonathan - is it possible to attach an updated patch based on your changes to 0.8 branch?  Not sure if that would be simple to extract.,"25/Jun/11 03:07;jbellis;with svn: svn diff -r 1139323:1139483 and hack out the OutboundTcpConnection change in the middle manually from the output.

with git: create a branch, rip out the offending OTC change and squash the other two",25/Jun/11 03:07;jbellis;I think there's deep surgery to be done here still though.  Backporting is probably premature.,"25/Jun/11 03:12;jbellis;bq. It looks like there's a ton of effort put in to avoiding making sortByProximity work w/ non-local nodes

Wait, why do we even care?  ""local node"" IS the right host to sort against -- we want the split that is closest to the node running the job, this is not the same as some other C* node we contact.","28/Jun/11 14:13;michaelsembwever;bq. It looks like there's a ton of effort put in to avoiding making sortByProximity work w/ non-local nodes
Because it's only when that local node is down that we actually need to sort...
When/if DynamicEndpointSnitch's limitation is fixed (and it can sort by non-local nodes) then CassandraServer.java need not bypass it. But this won't simplify the code in CFRR. Now that CFIF supports multiple initialAddresses the method CFRR.sortEndpointsByProximity(..) can be rewritten (ie any connection to any initialAddress is all we need, no need to mess around with trying to connect through replica's to find information about replicas...)
bq. Wait, why do we even care? ""local node"" IS the right host to sort against
Depends on this is CFRR's ""local node"" or CassandraServer's ""local node""... 
CFRR's local node is the right and only node worth sorting against, it being the ""task tracker node"". 
But when c* on the ""task tracker node"" is down, then we randomly connect to another c* node so to find out of the replica we know about which are 1) up, 2) closest, and 3) in the same dc. Then it is a random c* node that becomes the ""local node"" and the call needs to be {{snitch.sortByProximity(initialAddress, addresses)}}.
But yes... the CFRR code is contorted. In many ways i prefer the simplicity of the first patch (both in api and in implementation) despite it not being ""as correct"". i thought of this ""fallback to replica"" as a last resort to keep the m/r job running, rather than an actively used feature where DynamicEndpointSnitch's scores will maximise performance. But then i'm only thinking in terms of a small c* cluster and i certainly am naive about what performance gains these scores can give...","28/Jun/11 14:51;jbellis;bq. CFRR's local node is the right and only node worth sorting against, it being the ""task tracker node"". 

Right.

bq. Then it is a random c* node that becomes the ""local node""

We still want to sort by proxmity-to-TT, because CFRR connects directly to the split owner to do the reads.  initialAddress isn't involved post-split-discovery.

Again, all the complexity goes away if we just embed the snitch into CFIF/TT.

One wrinkle: ec2snitch requires gossip, so TT would need a separate local ip to participate in the gossip ring.  We could make that optional (and fall back to old ""recognize local data, otherwise you get a random replica"" behavior otherwise).
","28/Jun/11 15:01;jbellis;Taking a step back: aren't we optimizing for (1) a corner case with (2) the wrong solution?

Here's what I mean:

1) CFRR already prioritizes the local replica.  So if you have >= one TT for each replica, this only helps if the local C* node dies, BUT the TT does not.  This doesn't happen often.

2) If we ARE in that situation, the ""right"" solution would be to send the job to a TT whose local replica IS live, not to read the data from a nonlocal replica.  How can we signal that?  ",28/Jun/11 15:02;michaelsembwever;CASSANDRA-2388-addition1.patch: Simplify CFRR now that multiple initialAddresses are supported.,"28/Jun/11 21:11;brandon.williams;bq. If we ARE in that situation, the ""right"" solution would be to send the job to a TT whose local replica IS live, not to read the data from a nonlocal replica. How can we signal that?

ISTM the right thing to do in that situation is just fail and let the JT reschedule somewhere else.","29/Jun/11 04:58;michaelsembwever; - This does happen already (i've seen it while testing initial patches that were no good).
Problem is that the TT is blacklisted, reducing hadoop's throughput for all jobs running.
I bet too that a fallback to a replica is faster than a fallback to another TT.

 - There is no guarantee that any given TT will have its split accessible via a local c* node - this is only a preference in CFRR. A failed task may just as likely go to a random c* node. At least now we can actually properly limit to the one DC and sort by proximity. 

 - One thing we're not doing here is applying this same DC limit and sort by proximity in the case when there isn't a localhost preference. See CFRR.initialize(..)
It would make sense to rewrite CFRR.getLocations(..) to
{noformat}    private Iterator<String> getLocations(final Configuration conf) throws IOException
    {
        return new SplitEndpointIterator(conf);
    }{noformat} and then to move the finding-a-preference-to-localhost code into SplitEndpointIterator...

 - A bug i can see in the patch that did get accepted already is in CassandraServer.java:763 when endpointValid is false and restrictToSameDC is true we end up restricting to a random DC. I could fix this so restrictToSameDC is disabled in such situations but this actually invalidates the previous point: we can't restrict to DC anymore and we can only sortByProximity to a random node... I think this supports Jonathan's point that it's overall a poor approach. I'm more and more in preference of my original approach using just client.getDatacenter(..) and not worrying about proximity within the datacenter.

 - Another bug is that, contray to my patch, the code committed
bq. committed with a change to use the dynamic snitch id the passed endpoint is valid.
 can call {{DynamicEndpointSnitch.sortByProximity(..)}} with an address that is not localhost and this breaks the assertion in the method. ","29/Jun/11 18:40;brandon.williams;{quote}
This does happen already (i've seen it while testing initial patches that were no good).
Problem is that the TT is blacklisted, reducing hadoop's throughput for all jobs running.
{quote}

If the cassandra node where the TT resides isn't working, then throughput is reduced regardless.


bq. I bet too that a fallback to a replica is faster than a fallback to another TT.

I doubt that for any significant job.  Locality is important.  Move the job to the data, not the data to the job.

{quote}
There is no guarantee that any given TT will have its split accessible via a local c* node - this is only a preference in CFRR. A failed task may just as likely go to a random c* node. At least now we can actually properly limit to the one DC and sort by proximity.
{quote}

This sounds like the thing we need to fix, then.  Ensuring that the TT assigned to the map has a local replica.","29/Jun/11 19:25;jbellis;bq. If the cassandra node where the TT resides isn't working, then throughput is reduced regardless.

Right: we _want_ it to be blacklisted in that scenario.","29/Jun/11 19:45;jbellis;bq. This sounds like the thing we need to fix, then. Ensuring that the TT assigned to the map has a local replica.

reverted 1139358, 1139483 to make a fresh start for this.

how do we ""ensure"" this?  isn't that the JT's job, to send jobs to the splits we gave it from CFIF?  (which does make sure that only nodes with the data, are included in the split source list.)","29/Jun/11 19:49;michaelsembwever;{quote}If the cassandra node where the TT resides isn't working, then throughput is reduced regardless.
bq. Right: we want it to be blacklisted in that scenario.{quote}
This is making the presumption that the hadoop cluster is only used with CFIF.
The TT could still be useful for other jobs submitted.
Furthermore a blacklisted TT does't automatically come back - it needs to be manually restarted. Isn't this creating more headache for operations?","29/Jun/11 19:59;tjake;I dont think we should require the TT to be running locally. The whole idea is to support access to Cassandra data from hadoop even if it's just an import. 

This patch does spend a lot of time dealing with non local data for that reason. ","29/Jun/11 20:47;brandon.williams;{quote}
This is making the presumption that the hadoop cluster is only used with CFIF.
The TT could still be useful for other jobs submitted.
{quote}

I'm fine with that assumption.  If you want to run other jobs, use a different cluster.  Cassandra's JVM is eating wasteful memory at that point.

{quote}
Furthermore a blacklisted TT does't automatically come back - it needs to be manually restarted. Isn't this creating more headache for operations?
{quote}

I don't think this is actually the case, see HADOOP-4305


{quote}
I dont think we should require the TT to be running locally. The whole idea is to support access to Cassandra data from hadoop even if it's just an import.

This patch does spend a lot of time dealing with non local data for that reason.
{quote}

I'm fine with dropping support for non-colocated TTs, or at least saying there's no DC-specific support.  Because frankly, that is a very suboptimal thing to do, transfer the data across the network all the time, and flies in the face of Hadoop's core principles.","29/Jun/11 21:00;jbellis;bq. a blacklisted TT does't automatically come back

tlipcon says it comes back after 24h, fwiw.  In any case it's still the case that we DO want to blacklist it while it's down.  (Brisk could perhaps add a ""clear my tasktracker on restart"" operation as a further enhancement.)

bq. I'm fine with dropping support for non-colocated TTs

+1, it was a bad idea and I'm sorry I wrote it. :)","29/Jun/11 21:16;michaelsembwever;bq. tlipcon says it comes back after 24h
just to be clear about my concerns. 
this means a dead c* node will bring down a TT. In a hadoop cluster with 3 nodes this means for 24hrs you're lost 33% throughput. (If less than 10% of hadoop jobs used CFIF i could well imagine some pissed users). (What if you have a temporarily problem with flapping c* nodes and you end up with a handful of blacklisted TTs? etc etc etc).

All this when using a replica, any replica, could have kept things going smoothly, the only slowdown being some of the data into CFIF had to go over the network instead...
","30/Jun/11 00:32;jbellis;bq. this means a dead c* node will bring down a TT

Again: _this is what you want to happen_.  As long as the C* process on the same node is down, you want the TT to be blacklisted and the jobs to go elsewhere.

bq. In a hadoop cluster with 3 nodes this means for 24hrs you're lost 33% throughput

Right, but the real cause is because the C* process is dead, not b/c the TT is blacklisted.  Making the TT read from other nodes will only hurt your network, not fix the throughput problem, b/c i/o is the bottleneck.","30/Jun/11 07:50;michaelsembwever;Then i would hope for two separate InputFormats. One optimised for local node connection, where cassandra is deemed the more important system over hadoop, and another where data can be read in from anywhere. I think the latter should be supported in some manner  since users may not always have the possibility to install hadoop and cassandra on the same servers, or they might not think it to be so critical part (eg if CFIF is reading using a IndexClause the input data set might be quite small and the remaining code in the m/r be the bulk of the processing...)","30/Jun/11 13:17;jbellis;bq. another where data can be read in from anywhere

This is totally antithetical to how hadoop is designed to work.  I don't think it's worth supporting in-tree.","30/Jun/11 21:43;michaelsembwever;Is CASSANDRA-2388-local-nodes-only-rough-sketch the direction we want then?

This is very initial code, i can't get {{new JobClient(JobTracker.getAddress(conf), conf).getClusterStatus().getActiveTrackerNames()}} to work, need a little help here.
(Also CFRR.getLocations() can be drastically reduced).","01/Jul/11 20:05;jbellis;+1 to CFRR changes

wasn't immediately clear to me what CFIF changes are doing, can you elaborate?","02/Jul/11 22:07;michaelsembwever;The idea is to setup splits to have only endpoints that are valid trackers. But now i see this is just a brainfart :-) Ofc the jobTracker will apply this match for us. And that CFIF was always 'restricted' to running on endpoints. Although the documentation on inputSplit.getLocations() is a little thin as to whether this restricts which trackers it should run on or whether is just a preference... I guess it doesn't matter, as you point out Jonathan all that's required here is the one line changed in CFRR.

","02/Jul/11 22:16;michaelsembwever;the new ""one-liner"" CASSANDRA-2388 attached. i'll ""submit patch"" once i've tested it some...","02/Jul/11 22:53;jbellis;Sounds good, thanks!","04/Jul/11 06:39;michaelsembwever;{quote}2) If we ARE in that situation, the ""right"" solution would be to send the job to a TT whose local replica IS live, not to read the data from a nonlocal replica. How can we signal that?{quote}To /really/ solve this issue can we do the following? 
In CFIF.getRangeMap() take out of each range any endpoints that are not alive. A client connection already exists in this method. This filtering out of dead endpoints wouldn't be difficult, and would move tasks *to* the data making use of replica. This approach does need a new method in cassandra.thrift, eg {{list<string> describe_alive_nodes()}}","15/Aug/11 18:47;jbellis;Does that really fix things though?  Because you could have a data node be reachable from the coordinator answering describe_alive_nodes, but unreachable from the client.  So the client still needs to be able to skip unreachable endpoints itself, so describe_alive seems like gratuitous complexity.","19/Aug/11 19:39;patrik.modesto;I'd like to point out the situation in which no node for a given range of keys is available. It can happen for example with keyspace set to RF=1 and a node goes down. I created a patch that gives a user a chance to ignore missing range/node and continue runnig the MapReduce job. The patch is here: http://pastebin.com/hhrr8m9P

Jonathan already replied to the ML with ""ignoring unavailable ranges is a misfeature, imo"".

In our case it's very usefull, although there may be another/smarter solution. We have a keyspace with RF=1 and the nature of our data allows us to ignore temporarily missing node. The current ColumnFamilyInputFormat fails with RuntimeException and AFAIK there is no way around.","19/Aug/11 20:59;brandon.williams;bq. Does that really fix things though? Because you could have a data node be reachable from the coordinator answering describe_alive_nodes, but unreachable from the client. So the client still needs to be able to skip unreachable endpoints itself, so describe_alive seems like gratuitous complexity.

I agree, since the view is from the coordinator, describe_alive_nodes isn't very helpful, and also has to wait on the failure detector to mark the node down anyway.","19/Aug/11 22:44;jbellis;bq. I agree, since the view is from the coordinator, describe_alive_nodes isn't very helpful

Committed Mck's most recent patch.

bq. We have a keyspace with RF=1 and the nature of our data allows us to ignore temporarily missing node

The ""right"" fix is to increase RF.  Ignoring missing data is not a scenario we want to support.","22/Aug/11 08:27;hudson;Integrated in Cassandra-0.7 #539 (See [https://builds.apache.org/job/Cassandra-0.7/539/])
    fail jobs when Cassandra node has failed but TaskTracker has not
patch by Mck SembWever; reviewed by jbellis and brandonwilliams for CASSANDRA-2388

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1159807
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
","30/Aug/11 20:55;michaelsembwever;This approach isn't really working for me and was committed too quickly i believe.

bq. Although the documentation on inputSplit.getLocations() is a little thin as to whether this restricts which trackers it should run on or whether is just a preference

Tasks are still being evenly distributed around the ring regardless of what the ColumnFamilySplit.locations is.

The chance of a task actually working is RF/N. Therefore the chances of a blacklisted node are high. Worse is that the whole ring can quickly become blacklisted.

http://abel-perez.com/hadoop-task-assignment has an interesting section in it explaining how the task assignment is supposed to work (and that data locality is preferred but not a requirement). Could ColumnFamilySplit.locations be in the wrong format? (eg they should ip not hostname?).","31/Aug/11 16:23;michaelsembwever;see last comment. (say if this should be a separate bug...)

Maybe hadoop's task allocation isn't working properly because i've an unbalanced ring (i'm working in parallel to fix that).
If this is the case i think it's an unfortunate limitation (the ring must be balanced to get any decent hadoop performance).
It's also probably likely when using {{ConfigHelper.setInputRange(..)}} that the number of nodes involved is small (approaching RF).
With the default hadoop scheduler your hadoop cluster is occupied while just a few taskTrackers are busy. Of course switching to FairScheduler will help some here.

I'll take a look into hadoop's task allocation code as well...","08/Sep/11 06:01;michaelsembwever;In the meantime could we make this behavior configurable.
eg replace CFRR:176 with something like
{noformat}
    if(ConfigHelper.isDataLocalityDisabled())
    {
        return split.getLocations()[0];
    }
    else
    {
        throw new UnsupportedOperationException(""no local connection available"");
    }{noformat}
",08/Sep/11 12:55;jbellis;Should we just revert the change for now?,"08/Sep/11 21:36;michaelsembwever;Well that would work for me, was only thinking you want to push a ""default behavior"" (especially for those using a RP). 
But I think a better understanding (at least from me) of hadoop's task scheduling is required before enforcing data locality, as as-is it certainly doesn't work for all.","14/Sep/11 02:25;tjake;I just want to confirm what this ticket is about.

The JT has a list of endpoints for a given split.
When a task runs it may or may not be on one of those nodes 
If other tasks are running on all those replicas the JT may put them on a remote node.

So we need to decide which endpoint to connect to given the chance that nodes are down.

1. Check if the node running CFRR is one of the replicas (we have this) this means JT has assigned a data-local task (good)
2. If none of these nodes are local then pick another.
3. If connection fails try the one other nodes.
4. Try to avoid endpoints in a different DC.

The biggest problem is 4.  Maybe the way todo this is change getSplits logic to never return replicas in another DC.  I think this would require adding DC info to the describe_ring call.  Then we only need to worry about 1-3.






","14/Sep/11 14:49;hudson;Integrated in Cassandra-0.7 #552 (See [https://builds.apache.org/job/Cassandra-0.7/552/])
    revert CASSANDRA-2388 (again)

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1170333
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
","08/Mar/12 22:38;jbellis;Marking as minor since the job should get re-submitted, and it's very difficult to reproduce when the tasktrackers are colocated with cassandra nodes (the recommended configuration).","31/Oct/12 16:33;lannyripple;Would very much like a fix to this.  We have a 40 node ring running 2x hadoop clusters on 20 nodes each.  One cluster is on systems that are more flaky than the other (bad batch of memory).  When building a split on the first cluster if a ring node is down in the area of the second cluster we get timeouts with no way to blacklist the offending node even though we have replicas local to the first cluster.

The ring is partitioned into DC1:2, DC2:2 with a hadoop cluster over each DC.","21/Nov/12 11:06;jbellis;Jake's plan above seems like a reasonable approach, but let me back up a step.  I'm just not convinced that the problem we're trying to solve is a real one.  Why do we want to suck a split's worth of data off-node?  If it's because you don't have TackTrackers running on your Cassandra nodes, well, go fix that.

If it's because Hadoop has created too many tasks and all the local replicas have their task queue full, won't assigning it to a non-local TT just cause more contention, than waiting for a local slot to free up?","21/Nov/12 14:42;scottfines;I have two distinct use-cases where running TaskTrackers alongside Cassandra nodes does not accomplish our goals:

1. Joining data. We have a large data set in cassandra, true, but we have a *much* larger data set held in Hadoop itself (around 4 orders of magnitude larger in hadoop than in cassandra). We need to join the two datasets together, and use the output from that join to feed multiple systems, none of which are cassandra. Since the data in Hadoop is so much larger than that in Cassandra, we have to bring the Cassandra data to hadoop, not the other way around. Because of security concerns, we can't spread our hadoop data onto our cassandra nodes (even if that didn't screw with our capacity planning), so we have no other choice but to move the Cassandra data (in small chunks) onto Hadoop. Why not use HBase, you say? We needed Cassandra for its write performance for other problems than this one. 

1. Offline, incremental backups. We have a large volume of time-series data held in Cassandra, and taking nightly snapshots and moving them to our archival center is prohibitively slow--it turns out that moving RF copies of our entire dataset over a leased line every night is a pretty bad idea. Instead, I use MapReduce to take an incremental backup of a much smaller subset of the data, then move that. That way, we not only are not moving the entire data set, but we are also using Cassandra's consistency mechanisms to resolve all the replicas. The only efficient way I've found to do this is via MapReduce (we use the Random Partitioner), and since it's an offline backup, we need to move it over the network anyway--may as well use the optimized network connecting Hadoop and Cassandra instead of the tiny pipe connecting cassandra to our archival center. 

Both of these reasons dictate that we *not* run a TT alongside our Cassandra nodes, no matter what the *recommended* approach is. In this case, we need a strong, fault-tolerant CFIF to serve our purposes.

","18/May/13 21:50;michaelsembwever;Jonathan,
 I can't say i'm in favour of enforcing data locality.
Because  data locality in hadoop doesn't work this way… when a tasktracker through the next heartbeat announces that it has a task slot free the jobtracker will do its best to assign a task with data locality to it but failing this will assign it a random task. the number of these random tasks can be quite high, just like i mentioned above
{quote} Tasks are still being evenly distributed around the ring regardless of what the ColumnFamilySplit.locations is. {quote}

This can be almost solved by upgrading to hadoop-0.21+, using the fair scheduler and setting the property {code}<property>
        <name>mapred.fairscheduler.locality.delay</name>
        <value>360000000</value>
<property>{code}.

At the end of the day while hadoop encourages data locality it does not enforce it.
The ideal approach would be to sort all locations by proximity.
The feasible approach hopefully is still [~tjake]'s above. In addition i'd be in favour of a setting in the job's configuration as to whether a location from another datacenter can be used.

references:
 - http://www.infoq.com/articles/HadoopInputFormat
 - http://www.mentby.com/matei-zaharia/running-only-node-local-jobs.html
 - https://groups.google.com/a/cloudera.org/forum/?fromgroups#!topic/cdh-user/3ggnE5hV0PY
 - http://www.cs.berkeley.edu/~matei/papers/2010/eurosys_delay_scheduling.pdf","27/May/13 16:02;jbellis;bq. The feasible approach hopefully is still T Jake Luciani's above

Okay.  Referring back to Jake's comments,

bq. The biggest problem is [avoiding endpoints in a different DC]. Maybe the way todo this is change getSplits logic to never return replicas in another DC. I think this would require adding DC info to the describe_ring call

I note that we expose node snitch location in system.peers.  So at worst we could ""join"" against that manually.","27/May/13 18:41;michaelsembwever;{quote}The biggest problem is [avoiding endpoints in a different DC]. Maybe the way todo this is change getSplits logic to never return replicas in another DC. I think this would require adding DC info to the describe_ring call{quote}

Tasktrackers may have access to a set of datacenters, so this DC info needs contain a list of DCs.

For example, our setup separates datacenters by physical datacenter and hadoop-usage, like:{noformat}DC1 ""Production + Hadoop""
  c*01 c*03
DC2 ""Production + Hadoop""
  c*02 c*04
DC3 ""Production""
  c*05
DC4 ""Production""
  c*06{noformat}

So here we'd pass to getSplits() a DC info like ""DC1,DC2"".
But the problem remain, given a task executing on c*01 that fails to connect to localhost, although we can now prevent a connection to DC3 or DC4, we can't favour a connection to any other split in DC1 over anything in DC2. Is this solvable? ","09/Jun/14 14:19;pauloricardomg;Attached [2.0-CASSANDRA-2388.patch|https://issues.apache.org/jira/secure/attachment/12649379/2.0-CASSANDRA-2388.patch] (against 2.0), where CASSANDRA-6302 is ported to ColumnFamilyRecordReader, so the next replicas are tried when the first one fails.

Reading from a non-local-DC replica is not a problem anymore due to the introduction of describe_local_ring (CASSANDRA-6268), that limits the input splits to the local DC.

I would like to acknowledge my colleague Danilo Penna Queiroz who paired with me on this patch. ",09/Jun/14 14:41;jbellis;[~pkolaczk] to review,22/Jun/14 21:05;pauloricardomg;[~pkolaczk] any update on this? cheers! :),"23/Jun/14 07:44;pkolaczk;[~pauloricardomg] I can't promise, but I try to do that at the end of this week. ","25/Jun/14 19:22;pauloricardomg;Attaching patch based on 1.2.16 and fixed patch (v2) for 2.0.

Maybe the 1.2 patch can still make it to 1.2.17...","07/Aug/14 19:19;pauloricardomg;[~pkolaczk] any update on this? this review has been roaming for quite some time now...
sorry for bothering but would be nice to see this integrated. cheers!",06/Oct/14 20:27;pauloricardomg;http://mail-archives.apache.org/mod_mbox/cassandra-dev/201410.mbox/%3CCALdd-zjmvp7JOtguZ_k951RQHDtFt1cthX=RnHQ332C=gAZbjw@mail.gmail.com%3E,"10/Oct/14 11:16;michaelsembwever; [~pkolaczk] + [~pauloricardomg],  AFAIK everything thrift related is frozen, so i presume the patch isn't going to be applied to master.
Otherwise it's +1 on the patch from me.",01/Dec/14 14:02;pkolaczk;+1,"24/Aug/15 16:18;jbellis;Paulo, can you rebase to 2.1?","25/Aug/15 13:30;pauloricardomg;Rebased 2.1 patch available [here|https://github.com/pauloricardomg/cassandra/tree/2388-2.1].

2.1 tests:
* [testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.1-testall/lastCompletedBuild/testReport/]
* [dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.1-dtest/lastCompletedBuild/testReport/]

2.2 tests (don't know if {{ColumnFamilyInputFormat}} and {{ColumnFamilyRecordReader}} should be deprecated by then, but the classes are still present there):
* [testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.2-testall/lastCompletedBuild/testReport/]
* [dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.2-dtest/lastCompletedBuild/testReport/]","20/Nov/15 13:54;slebresne;Committed, thanks."
Quorum reads are not monotonically consistent,CASSANDRA-2494,12504486,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,sbridges,sbridges,17/Apr/11 14:29,12/Mar/19 14:02,13/Mar/19 22:26,11/Aug/11 19:29,1.0.0,,,,,,2,,,,,"As discussed in this thread,

http://www.mail-archive.com/user@cassandra.apache.org/msg12421.html

Quorum reads should be consistent.  Assume we have a cluster of 3 nodes (X,Y,Z) and a replication factor of 3. If a write of N is committed to X, but not Y and Z, then a read from X should not return N unless the read is committed to at  least two nodes.  To ensure this, a read from X should wait for an ack of the read repair write from either Y or Z before returning.

Are there system tests for cassandra?  If so, there should be a test similar to the original post in the email thread.  One thread should write 1,2,3... at consistency level ONE.  Another thread should read at consistency level QUORUM from a random host, and verify that each read is >= the last read.",,,,,,,,,,,,,,,,03/Aug/11 16:34;jbellis;2494-v2.txt;https://issues.apache.org/jira/secure/attachment/12489205/2494-v2.txt,27/Jul/11 01:46;jbellis;2494.txt;https://issues.apache.org/jira/secure/attachment/12487930/2494.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-17 21:01:09.146,,,no_permission,,,,,,,,,,,,20654,,,Thu Sep 24 20:45:58 UTC 2015,,,,,,0|i0gbp3:,93325,slebresne,slebresne,,,,,,,,,"17/Apr/11 21:01;scode;As far as I can tell the consistency being asked for was never promised by Cassandra is in fact not expected.

The expected behavior of writes is that they propagate; the difference between ONE and QUORUM is just how many are required to receive a write prior to a return to the client with a successful error code. For reads, that means you may get lucky at ONE or you may get lucky at QUORUM; the positive guarantee is in the case of a *completing* QUORUM write followed by a QUORUM read.

So just to be clear, although I don't think this is what is being asked for: As far as I know, it has never been the case, nor the intent to promise, that a write which fails is guaranteed not to eventually complete. Simply ""fixing"" reads is not enough; by design the data will be replicated during read-repair and AES - this is how consistency is achieved in Cassandra.

However, it sounds like what is being asked for is not that they don't propagate in the event of a write ""failure"", but just that reads don't see the writes until they are sufficiently propagated to guarantee that any future QUORUM read will also see the data. I can understand that is desirable, in the sense of achieving monotonically forward-moving data as the benchmark/test from the e-mail thread does. Another way to look at is that maybe you never want to read data successfully prior to achieving a certain level of replication, in order to avoid a client ever seeing data that may suddenly go away due to e.g. a node failure in spite of said failure not exceeding the number of failures the cluster was designed to survive.

So the key point would be the bit about guaranteeing that any ""future QUORUM read will see the data or data subsequently overwritten"", and actively read-repairing and waiting for it to happen would take care of that. It would be important to ensure that the act of ensuring a quorum of nodes have seen the data is the important part; one should not await for a quorum to agree on the *current* version of the data as that would create potentially unbounded round-trips on hotly contended data.

Thing to consider: One might think about cases where read-repair is currently not done, like range slices, and how an implementation that requires read repair for consistency affects that.

","18/Apr/11 04:20;sbridges;Peter Shuller wrote,

""However, it sounds like what is being asked for is not that they don't propagate in the event of a write ""failure"", but just that reads don't see the writes until they are sufficiently propagated to guarantee that any future QUORUM read will also see the data.""

Yes, that is the issue.  The comment in the bug about writing at ONE and reading at QUORUM is just a way of testing this new guarantee in a distributed test, if Cassandra has those.","18/Apr/11 20:41;jjordan;I would think that reads at QUORUM should never go backwards.  Even if the Write was at ZERO.  If there were writes to the cluster of a=1 time=5, a=2 time=10, a=3 time=15, and I do a read at QUORUM which tells me a=3 time=15, I should not be able to do another read at QUORUM and get a=2 time=10.","22/Apr/11 05:17;stuhood;W plus R must be _greater than_ N for consistency.

EDIT: And adding a blocking implicit write step to QUORUM reads by waiting for read repair is not reasonable.","22/Apr/11 08:26;scode;I don't think anyone is claiming otherwise, unless I'm misunderstanding. The problem is that while the ""if sucessfully written to quorum, subsequent quorum reads will see it"" guarantee is indeed maintained, it is possible for quorum reads to see data go backwards (on a timeline) in the event of a *failed* attempted quorum write. This includes the possibility of reads seeing data that then permanently vanishes, even though you only lost say 1 node that you designed your cluster for surviving (RF >= 3, QUORUM). (""lost 1 node"" can be substituted with ""killed 1 node in periodic commit mode"")

I still don't think this is a violation of what was promised, but I can see how making the further guarantee would make for more useful consistency semantics in some cases.

With respect to implicit write: An alternative is to adjust reconciliation logic when applied as part of reads (as opposed to AES,  hinted hand-off, writes) to take consistency level into account and only consider columns whose timestamp is >= the greatest timestamp that has quorum (off the top of my head I think that should be correct in call cases, but I didn't think this through terribly).
","22/Apr/11 08:34;scode;Ok, so my last suggestion is in fact broken. A counter example is:

 A: column @ t1
 B: column @ t2
 C: column @ t3

If A + B is participating, A's column @ t1 has timestamp quorum and would be selected. If B + C is participating, B's column is picked. Thus, a read where B + C participates will see data that will be reverted once A + B happens to be picked.

Note to self: Think before posting.
","22/Apr/11 14:45;sbridges;I think the guarantee of quorum reads not seeing old writes once a quorum read sees a new write is  very useful.  I suspect most people already think that this guarantee occurs, including, it seems, Jonathan Ellis whose quote can be found in the email thread linked to in the bug,

""The important guarantee this gives you is that once one quorum read sees the new value, all others will too.   You can't see the newest version, then see an older version on a subsequent write [sic, I
assume he meant read], which is the characteristic of non-strong consistency""



","22/Apr/11 15:01;slebresne;The problem is you are considering the consistency of reads but not write. The guarantee is: ""quorum reads will not see old quorum write once a quorum read sees a new quorum"". Period. I you don't consider the consistency of a write, consider the case of a CL.ANY write. In this case, the update may not be at all on any replica. How can we ensure the quorum read property that you want ? We query all nodes for quorum reads in case there is an hint somewhere ?

If you look at the Consistency part of http://wiki.apache.org/cassandra/ArchitectureOverview, it seems to me that it is pretty clear that the consistency of reads *and* writes is involved to achieve strong consistency. So I would hope 'most people' are aware of that.","22/Apr/11 15:11;scode;The issue is that of *failed* QUORUM writes. I.e., you design your system to use QUORUM writes and QUORUM reads, and expect that once a QUORUM read sees a given piece of data a subsequent QUORUM read will also see it (or a later data). A *failed* QUORUM write that was replicated to less than a QUORUM would be visible as part of QUORUM reads that happen to touch one of those replicas, but there is no guarantee that subsequent reads see it.

I was under the impression this was never an intended guarantee. Apparently I may be wrong about that given the jbellis quote above. In either case, it is certainly not an *actual* guarantee given by the current implementation.

The guarantee that a *successful* QUORUM write is seen by a subsequent QUORUM read is, as far as I can tell, not in question here.

","22/Apr/11 15:22;sbridges;To be clear, this is a new guarantee.  The current guarantee is R+W>N gives you consistency.  This bug is asking that a successful quorum read of A means that A has been committed to a quorum of nodes.

""How can we ensure the quorum read property that you want ?""

If when reading at quorum, and no quorum can be found which agrees on a particular value, then the coordinator ( ? ) will wait for acks of read repair writes (or perhaps just do normal writes) to be returned from a sufficient number of nodes to ensure that the value has been committed to a quorum of nodes.

Without this new guarantee it is hard for readers to function correctly.  The reader does not know that the quorum write failed, or is still in progress, so without reading at ALL, the R+W>N guarantee does not help the reader.



","27/Jul/11 01:46;jbellis;I don't see any reason not to guarantee that the replicas we read from provide monotonic read consistency.

Patch attached to do this.
","27/Jul/11 11:08;slebresne;Ok, I now see what you mean :)
Makes perfect sense.

Comments on the patch:
* There is a number of case where scheduleRepairs may not have been called (if the read for repair timeout and/or we had no or only 1 response or we have the situation were removeDeleted removes everything), so repairResults will be null in those cases. In SP, we should check for it.
* That new wait can extend the rpc timeout to almost twice what it should be. I agree that it is not a huge deal, but by exposing the 'startTime' stored in RepairCallback we can make it so we don't extend it that way.
* Shouldn't we give the same love to range requests, now that we do repairs there too ?","03/Aug/11 16:34;jbellis;bq. There is a number of case where scheduleRepairs may not have been called

defaulted repairResults to emptyList.

bq. That new wait can extend the rpc timeout to almost twice what it should be

Well, sort of -- rpctimeout is working exactly as intended, i.e., to prevent waiting indefinitely for a node that died after we sent it a request.  Treating it as ""max time to respond to client"" has never really been correct.  (E.g., in the CL > ONE case we can already wait up to rpctimeout twice, one for the original digest read set, and again for the data read after mismatch.)  So I don't think we should try to be clever with that here.

bq. Shouldn't we give the same love to range requests, now that we do repairs there too

done.","11/Aug/11 15:34;slebresne;bq. Well, sort of – rpctimeout is working exactly as intended, i.e., to prevent waiting indefinitely for a node that died after we sent it a request. Treating it as ""max time to respond to client"" has never really been correct. (E.g., in the CL > ONE case we can already wait up to rpctimeout twice, one for the original digest read set, and again for the data read after mismatch.) So I don't think we should try to be clever with that here.

Fair enough. It would probably be useful to make rpctimeout meaning closer to ""max time to respond to client"". Created CASSANDRA-3018 for that though.

+1 on v2.",11/Aug/11 19:29;jbellis;committed,"11/Aug/11 20:14;hudson;Integrated in Cassandra #1017 (See [https://builds.apache.org/job/Cassandra/1017/])
    provide monotonic read consistency
patch by jbellis; reviewed by slebresne for CASSANDRA-2494

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156758
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RepairCallback.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RowRepairResolver.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RangeSliceResponseResolver.java
",24/Sep/15 20:45;aaaron;The relevant code in the patch has changed significantly. Is the monotonic read consistency guarantee still provided?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Command Line Client has Memtable thresholds: (millions of ops/minutes/MB) last two parameters reversed,CASSANDRA-2599,12506103,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,joelastpass,joelastpass,03/May/11 21:31,12/Mar/19 14:02,13/Mar/19 22:26,03/May/11 21:36,0.8.0,,,,,,0,cli,,,,"Here's a patch

Index: src/java/org/apache/cassandra/cli/CliClient.java
===================================================================
--- src/java/org/apache/cassandra/cli/CliClient.java	(revision 1099262)
+++ src/java/org/apache/cassandra/cli/CliClient.java	(working copy)
@@ -1325,7 +1325,7 @@
                 sessionState.out.printf(""      Row cache size / save period in seconds: %s/%s%n"", cf_def.row_cache_size, cf_def.row_cache_save_period_in_seconds);
                 sessionState.out.printf(""      Key cache size / save period in seconds: %s/%s%n"", cf_def.key_cache_size, cf_def.key_cache_save_period_in_seconds);
                 sessionState.out.printf(""      Memtable thresholds: %s/%s/%s (millions of ops/minutes/MB)%n"",
-                                cf_def.memtable_operations_in_millions, cf_def.memtable_throughput_in_mb, cf_def.memtable_flush_after_mins);
+                                cf_def.memtable_operations_in_millions, cf_def.memtable_flush_after_mins, cf_def.memtable_throughput_in_mb);
                 sessionState.out.printf(""      GC grace seconds: %s%n"", cf_def.gc_grace_seconds);
                 sessionState.out.printf(""      Compaction min/max thresholds: %s/%s%n"", cf_def.min_compaction_threshold, cf_def.max_compaction_threshold);
                 sessionState.out.printf(""      Read repair chance: %s%n"", cf_def.read_repair_chance);
",All,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,20723,,,Tue May 03 21:36:29 UTC 2011,,,,,,0|i0gcbz:,93428,,,,,,,,,,,03/May/11 21:36;joelastpass;jbellis fixed this in April,"03/May/11 21:36;joelastpass;jbellis fixed this in April
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enabling SSL on a fairly light cluster leaks Open files.,CASSANDRA-3257,12524571,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,25/Sep/11 17:25,12/Mar/19 14:02,13/Mar/19 22:26,26/Sep/11 17:14,0.8.7,1.0.0,,,,,0,,,,,"To reproduce:

Enable SSL encryption and let the server be idle for a day or so you will see the below....

[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ /usr/sbin/lsof |grep -i cassandra-app.jks |wc -l ;date
16333
Sun Sep 25 17:23:29 UTC 2011
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ java -jar cmdline-jmxclient-0.10.3.jar - localhost:7501 java.lang:type=Memory gc
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ /usr/sbin/lsof |grep -i cassandra-app.jks |wc -l ;date
64
Sun Sep 25 17:23:53 UTC 2011
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ 

After running GC manually the issue goes away.",JVM on CentOS,,,,,,,,,,,,,,,26/Sep/11 16:54;vijay2win@yahoo.com;0001-ssl-open-file-issue.patch;https://issues.apache.org/jira/secure/attachment/12496503/0001-ssl-open-file-issue.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-26 17:14:25.003,,,no_permission,,,,,,,,,,,,3220,,,Tue Sep 27 06:26:04 UTC 2011,,,,,,0|i0ghpr:,94300,,,,,,,,,,,26/Sep/11 16:08;vijay2win@yahoo.com;This issue elevates when a node is down.,"26/Sep/11 16:16;vijay2win@yahoo.com;Seems like the issue may be because we are opening the FIS and not closing it.... in SSLFactory.createSSLContext, testing it... patch soon.",26/Sep/11 16:54;vijay2win@yahoo.com;Closing the FIS fixes the issue (in SSLFactory).,"26/Sep/11 17:14;brandon.williams;Committed, thanks!","26/Sep/11 23:08;jbellis;should use FileUtils.closeQuietly so if one close throws, the other still happens",27/Sep/11 05:33;jbellis;done in r1176204.  also cleaned up formatting.,"27/Sep/11 06:26;vijay2win@yahoo.com;Thanks Jonathan!, was just about to attach it...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
long-test fails to build,CASSANDRA-2959,12515557,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,mallen,mallen,27/Jul/11 18:30,12/Mar/19 14:02,13/Mar/19 22:26,27/Jul/11 18:34,,,,Legacy/Testing,,,0,,,,,"build-test:
    [javac] /var/lib/jenkins/jobs/Cassandra/workspace/build.xml:910: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 125 source files to /var/lib/jenkins/jobs/Cassandra/workspace/build/test/classes
    [javac] /var/lib/jenkins/jobs/Cassandra/workspace/test/unit/org/apache/cassandra/service/RemoveTest.java:172: removingNonlocal(org.apache.cassandra.dht.Token) in org.apache.cassandra.gms.VersionedValue.VersionedValueFactory cannot be applied to (org.apache.cassandra.dht.Token,org.apache.cassandra.dht.Token)
    [javac]                     valueFactory.removingNonlocal(endpointTokens.get(1), removaltoken));
    [javac]                                 ^
    [javac] /var/lib/jenkins/jobs/Cassandra/workspace/test/unit/org/apache/cassandra/service/RemoveTest.java:189: removedNonlocal(org.apache.cassandra.dht.Token) in org.apache.cassandra.gms.VersionedValue.VersionedValueFactory cannot be applied to (org.apache.cassandra.dht.Token,org.apache.cassandra.dht.Token)
    [javac]                     valueFactory.removedNonlocal(endpointTokens.get(1), removaltoken));
    [javac]                                 ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 2 errors

BUILD FAILED
/var/lib/jenkins/jobs/Cassandra/workspace/build.xml:910: Compile failed; see the compiler error output for details.
",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-07-27 18:32:28.341,,,no_permission,,,,,,,,,,,,20913,,,Wed Jul 27 18:46:35 UTC 2011,,,,,,0|i0gein:,93782,,,,,,,,,,,"27/Jul/11 18:32;cdaw;The check-ins for the build where this error started are:
{code}
Revision: 1150847

Changes

Gossip handles dead states, token removal actually works, gossip states
are held for aVeryLongTime.
Patch by brandonwilliams and Paul Cannon, reviewed by Paul Cannon for
CASSANDRA-2496. (detail)

add ability to drop local reads/writes that are going to timeout
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2943 (detail)
{code}",27/Jul/11 18:46;brandon.williams;Removed obsolete tests in r1151587,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rebuffer called excessively during seeks,CASSANDRA-2581,12505751,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,29/Apr/11 04:34,12/Mar/19 14:02,13/Mar/19 22:26,03/May/11 15:50,0.7.6,0.8.0,,,,,0,,,,,"When doing an strace tonight, I noticed during memtable flushes that we were only writing 1KB per every write() system call...After diving more into it, it's because of a bug in the seek() code. 

if (newPosition >= bufferOffset + validBufferBytes || newPosition < bufferOffset)

vs.

if (newPosition > (bufferOffset + validBufferBytes) || newPosition < bufferOffset)

Two things I noticed, we shouldn't need to rebuffer if newPosition is equal to bufferOffset + validBufferBytes, second the evaluation was doing (newPosition >= bufferOffset) + validBufferBytes which always seemed to be true.
",,,,,,,,,,,,,,,,29/Apr/11 04:37;lenn0x;0001-Rebuffer-called-excessively-during-seeks.patch;https://issues.apache.org/jira/secure/attachment/12477725/0001-Rebuffer-called-excessively-during-seeks.patch,29/Apr/11 15:53;jbellis;2581.txt;https://issues.apache.org/jira/secure/attachment/12477807/2581.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-29 15:53:17.989,,,no_permission,,,,,,,,,,,,20711,,,Tue May 03 16:04:04 UTC 2011,,,,,,0|i0gc7z:,93410,xedin,xedin,,,,,,,,,"29/Apr/11 15:53;jbellis;bq. we shouldn't need to rebuffer if newPosition is equal to bufferOffset + validBufferBytes

I think that is correct, patch attached. I will ask Pavel to also review since this is extremely important not to break.

bq. second the evaluation was doing (newPosition >= bufferOffset) + validBufferBytes

No, addition is higher precedence than comparison (or the logical operations). In fact if you force the grouping you suggest, javac will reject it since you cannot add a boolean and an int.","03/May/11 10:49;xedin;+1, we can just change ""current"" if we point on the last byte of the the buffer.","03/May/11 13:57;jbellis;bq. we can just change ""current"" if we point on the last byte of the the buffer

Meaning you suggest an additional code change?","03/May/11 14:01;xedin;No, I mean that the patch is correct, sorry :)",03/May/11 15:50;jbellis;committed goffinet's patch with the additional parentheses for clarity,"03/May/11 16:04;hudson;Integrated in Cassandra-0.7 #466 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/466/])
    fix excessively pessimistic rebuffering in BRAF writes
patch by goffinet; reviewed by jbellis and Pavel Yaskevich for CASSANDRA-2581
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError when repairing a node,CASSANDRA-3256,12524504,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,alienth,alienth,24/Sep/11 06:10,12/Mar/19 14:02,13/Mar/19 22:26,26/Sep/11 14:32,0.8.7,,,,,,0,repair,,,,"When repairing a node, the following exception was thrown two times:

{code}
ERROR [AntiEntropyStage:2] 2011-09-23 23:00:24,016 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[AntiEntropyStage:2,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.AntiEntropyService.rendezvous(AntiEntropyService.java:170)
        at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:90)
        at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.doVerb(AntiEntropyService.java:518)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

No other errors occurred on the node. From peeking at the code, this assertion appears to simply check if an existing repair session could be found. Interestingly, the repair did continue to run after this as evidenced by several other AntiEntropyService entires in the log.

8 node ring with an RF of 3, if that matters at all. No other nodes in the ring threw exceptions.",,,,,,,,,,,,,,,,26/Sep/11 12:53;slebresne;3256.patch;https://issues.apache.org/jira/secure/attachment/12496471/3256.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-26 12:53:08.822,,,no_permission,,,,,,,,,,,,3269,,,Mon Sep 26 15:17:40 UTC 2011,,,,,,0|i0ghpb:,94298,jbellis,jbellis,,,,,,,,,"24/Sep/11 06:13;alienth;I should note, this same node has been repaired several times in the past with no issues.",25/Sep/11 02:18;alienth;The repair which triggered this assertion error did eventually return with no further errors.,"26/Sep/11 12:53;slebresne;My guess is that this is probably harmless. Basically the node received a merkle tree for a session that it doesn't know about. This means that the said repair session has been interrupted. _A priori_, I see only two things that can cause this:
  * The repair thread on the host has been interrupted. But 1) in that case you should have found a exception earlier on saying ""Interrupted while waiting for repair: repair will continue in the background."" and 2) I don't see what could interrupt that thread.
  * The node restarted and only now receives a response for request made before the restart. This is imho the more likely scenario. And if so, the previous repair (the one started before the restart) won't succeed, but there is no more consequence than that.

That being throwing an assertion error is probably such a great idea given that it's a scenario that can happen. Attaching a patch that simply log an hopefully more explicit message.",26/Sep/11 13:09;jbellis;+1,26/Sep/11 14:32;slebresne;Committed. Thanks.,"26/Sep/11 15:17;hudson;Integrated in Cassandra-0.8 #341 (See [https://builds.apache.org/job/Cassandra-0.8/341/])
    Log a miningfull warning when a node receive a message for a repair session that don't exist anymore
patch by slebresne; reviewed by jbellis for CASSANDRA-3256

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1175880
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in org.apache.cassandra.service.AntiEntropyService when repair finds a keyspace with no CFs,CASSANDRA-3988,12544851,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,wdhathaway,wdhathaway,01/Mar/12 22:32,12/Mar/19 14:02,13/Mar/19 22:26,04/Mar/12 10:17,1.0.9,,,,,,0,,,,,"2012-03-01 21:38:09,039 [RMI TCP Connection(142)-10.253.106.21] INFO  StorageService - Starting repair command #15, repairing 3 ranges.
2012-03-01 21:38:09,039 [AntiEntropySessions:14] INFO  AntiEntropyService - [repair #d68369f0-63e6-11e1-0000-8add8b9398fd] new session: will sync /10.253.106.21, /10.253.106.248, /10.253.106.247 on range (85070591730234615865843651857942052864,106338239662793269832304564822427566080] for PersonalizationDataService2.[]
2012-03-01 21:38:09,039 [AntiEntropySessions:14] ERROR AbstractCassandraDaemon - Fatal exception in thread Thread[AntiEntropySessions:14,5,RMI Runtime]
java.lang.NullPointerException
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.runMayThrow(AntiEntropyService.java:691)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)",,,,,,,,,,,,,,,,02/Mar/12 17:59;slebresne;3988-v2.txt;https://issues.apache.org/jira/secure/attachment/12516864/3988-v2.txt,02/Mar/12 08:06;slebresne;3988.txt;https://issues.apache.org/jira/secure/attachment/12516800/3988.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-03-02 08:06:08.644,,,no_permission,,,,,,,,,,,,230037,,,Sun Mar 04 10:17:46 UTC 2012,,,,,,0|i0gqnj:,95748,jbellis,jbellis,,,,,,,,,02/Mar/12 08:06;slebresne;Attached patch to skip the repair when there is no CFs.,"02/Mar/12 17:20;jbellis;don't we use length == 0 CFs in forceTableRepair to mean ""load a list of all of them?""  iow i think the check needs to be on the result of getValidColumnFamilies.","02/Mar/12 17:43;brandon.williams;We assert this can't happen:

{code}
java.lang.AssertionError: Repairing no column families seems pointless, doesn't it
{code}

So I'm not sure how we get into this situation, but apparently we can.","02/Mar/12 17:59;slebresne;You are right, I was a bit too quick. v2 attached that do that after the call to getValidColumnFamilies.","02/Mar/12 18:39;jbellis;bq. We assert this can't happen

That's a good point -- how is this getting by the RepairSession constructor?  Do we have two bugs?",03/Mar/12 11:46;slebresne;@Bill do you run with assertion disabled?,"03/Mar/12 15:44;wdhathaway;@Sylvain - Correct, we are not enabling assertions (no -ea flag).","03/Mar/12 16:29;jbellis;+1 on v2, then","04/Mar/12 10:17;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicSnitch race in adding latencies,CASSANDRA-2618,12506465,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,06/May/11 22:27,12/Mar/19 14:02,13/Mar/19 22:26,07/May/11 01:43,0.7.6,,,,,,0,,,,,"ERROR 15:33:48,614 Fatal exception in thread Thread[ReadStage:264,5,main]
java.lang.RuntimeException: java.util.NoSuchElementException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.NoSuchElementException
	at java.util.concurrent.LinkedBlockingDeque.removeFirst(LinkedBlockingDeque.java:401)
	at java.util.concurrent.LinkedBlockingDeque.remove(LinkedBlockingDeque.java:621)
	at org.apache.cassandra.locator.AdaptiveLatencyTracker.add(DynamicEndpointSnitch.java:288)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.receiveTiming(DynamicEndpointSnitch.java:202)
	at org.apache.cassandra.net.MessagingService.addLatency(MessagingService.java:152)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:642)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
ERROR 15:33:48,615 Fatal exception in thread Thread[ReadStage:264,5,main]
java.lang.RuntimeException: java.util.NoSuchElementException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.NoSuchElementException
	at java.util.concurrent.LinkedBlockingDeque.removeFirst(LinkedBlockingDeque.java:401)
	at java.util.concurrent.LinkedBlockingDeque.remove(LinkedBlockingDeque.java:621)
	at org.apache.cassandra.locator.AdaptiveLatencyTracker.add(DynamicEndpointSnitch.java:288)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.receiveTiming(DynamicEndpointSnitch.java:202)
	at org.apache.cassandra.net.MessagingService.addLatency(MessagingService.java:152)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:642)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more

What is happening that AdaptiveLatencyTracker.add is trying to add a latency, but the deque is full, so it makes a second effort to remove an entry from the deque and then try to add again.  However, when it tries to remove, the deque has already been emptied by DES.reset call clear() on all the ALTs.  This bug has existed for a long time, but it's very rare and difficult to trigger.",,,,,,,,,,,,,,,,06/May/11 22:37;brandon.williams;2618.txt;https://issues.apache.org/jira/secure/attachment/12478475/2618.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-07 00:10:10.322,,,no_permission,,,,,,,,,,,,20731,,,Sat May 07 05:20:30 UTC 2011,,,,,,0|i0gcfz:,93446,,,,,,,,,,,06/May/11 22:37;brandon.williams;Patch to avoid the race between add() and clear().,07/May/11 00:10;tjake;+1,07/May/11 01:43;brandon.williams;Committed.,"07/May/11 02:10;stuhood;Are we sure this fixes the race? ArrayDeque isn't a concurrent datastructure, so it's possible that its internals could race in such a way that it gets into a bad state. The size field not matching the index, for example.",07/May/11 02:16;brandon.williams;I can't say for certain since this is terribly difficult to repro (it's been there since DES inception and I've only seen it once) but the patch definitely addressed a likely cause and is certainly needed either way.,"07/May/11 02:17;brandon.williams;Also, we're using LinkedBlockingDeque which is a concurrent structure.","07/May/11 05:20;hudson;Integrated in Cassandra-0.7 #473 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/473/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read repair: NEWS does not match actual behavior,CASSANDRA-3169,12522441,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,09/Sep/11 15:42,12/Mar/19 14:02,13/Mar/19 22:26,12/Sep/11 15:43,1.0.0,,,,,,0,,,,,"NEWS says:

{noformat}
    - Hinted Handoff is substantially more robust, with the result that
      when HH is enabled, repair only needs to be run if a node crashes.
    - Because of this, read repair is disabled now by default on newly
      created ColumnFamilies.
{noformat}

But default RR value is still 1.0.",,,,,,,,,,,,,,,,09/Sep/11 15:44;jbellis;3169.txt;https://issues.apache.org/jira/secure/attachment/12493791/3169.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-09 15:46:32.706,,,no_permission,,,,,,,,,,,,4045,,,Mon Sep 12 15:43:04 UTC 2011,,,,,,0|i0ggnb:,94127,slebresne,slebresne,,,,,,,,,09/Sep/11 15:45;jbellis;Patch reduces default RR to 0.1 and updates NEWS to match -- seems premature to default it to all the way off.,09/Sep/11 15:46;slebresne;+1,12/Sep/11 15:43;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Return both listen_address and rpc_address through describe_ring,CASSANDRA-3187,12522800,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,12/Sep/11 20:22,12/Mar/19 14:02,13/Mar/19 22:26,13/Sep/11 18:30,0.8.6,,,,,,0,,,,,"CASSANDRA-1777 changed describe_ring to return the rpc address associated with a node instead of the listen_address. This allows using different interfaces for listen_address and rpc_address, but breaks when rpc_address is set to something like 0.0.0.0.

I think the describe_ring should just return both interfaces. We can add an optional field to the TokenRange struct that is 'listen_endpoints' or something similar and populate that with the listen addresses of nodes.",,,,,,,,,,,,,,,,12/Sep/11 22:45;nickmbailey;0001-0.8-Return-both-rpc-address-and-listen-address-with-the-.patch;https://issues.apache.org/jira/secure/attachment/12494143/0001-0.8-Return-both-rpc-address-and-listen-address-with-the-.patch,13/Sep/11 03:15;nickmbailey;0001-0.8-Return-both-rpc-address-and-listen-address-with-v2.patch;https://issues.apache.org/jira/secure/attachment/12494167/0001-0.8-Return-both-rpc-address-and-listen-address-with-v2.patch,12/Sep/11 22:45;nickmbailey;0001-1.0-Return-both-rpc-address-and-listen-address-with-the-.patch;https://issues.apache.org/jira/secure/attachment/12494144/0001-1.0-Return-both-rpc-address-and-listen-address-with-the-.patch,13/Sep/11 03:15;nickmbailey;0001-1.0-Return-both-rpc-address-and-listen-address-with-v2.patch;https://issues.apache.org/jira/secure/attachment/12494168/0001-1.0-Return-both-rpc-address-and-listen-address-with-v2.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-09-12 22:47:08.083,,,no_permission,,,,,,,,,,,,656,,,Wed Sep 14 15:15:48 UTC 2011,,,,,,0|i0ggv3:,94162,brandon.williams,brandon.williams,,,,,,,,,12/Sep/11 21:36;nickmbailey;Applies to 1.0.0 branch and 0.8 branch,"12/Sep/11 22:45;nickmbailey;Better version of the patch, separate patches for 0.8 and 1.0.","12/Sep/11 22:47;jbellis;The question in my mind is, do we want to ""unbreak"" 0.8.4/0.8.5, and make ""endpoints"" the same as it was in earlier versions (i.e. listen_address) and add a new field for rpc_address?","12/Sep/11 22:49;brandon.williams;bq. The question in my mind is, do we want to ""unbreak"" 0.8.4/0.8.5, and make ""endpoints"" the same as it was in earlier versions (i.e. listen_address) and add a new field for rpc_address?

If I could do it all over again, I wouldn't have put CASSANDRA-1777 in 0.8, but I think at this point changing it back in a minor is bad and will end up causing a lot of confusion.","12/Sep/11 22:53;jbellis;Does make it simpler to write a client that works against both latest-0.7 and latest-0.8, though (for 0.8 >= 0.8.6).

Especially if we push 0.8.6 out quickly.  For whatever reason it seems like many 0.8 deployments haven't moved off of 0.8.1 yet.","12/Sep/11 22:56;brandon.williams;bq. Especially if we push 0.8.6 out quickly. For whatever reason it seems like many 0.8 deployments haven't moved off of 0.8.1 yet.

True, and now that I think about I've heard nothing but complaints about CASSANDRA-1777, and no one clamored too hard for it to begin with.",13/Sep/11 03:15;nickmbailey;Rebased to make the required parameter listen_address again and add an optional rpc_address.,13/Sep/11 18:30;brandon.williams;Committed,"14/Sep/11 15:15;hudson;Integrated in Cassandra-0.8 #327 (See [https://builds.apache.org/job/Cassandra-0.8/327/])
    Return both listen_address and rpc_address through describe_ring.
Patch by Nick Bailey, reviewed by brandonwilliams for CASSANDRA-3187

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1170284
Files : 
* /cassandra/branches/cassandra-0.8/NEWS.txt
* /cassandra/branches/cassandra-0.8/interface/cassandra.thrift
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/AuthenticationException.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/AuthenticationRequest.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/AuthorizationException.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/CfDef.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/ColumnDef.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/ColumnOrSuperColumn.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/ColumnParent.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/ColumnPath.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/CounterColumn.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/CounterSuperColumn.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/CqlResult.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/CqlRow.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Deletion.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/IndexClause.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/IndexExpression.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/KeyCount.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/KeyRange.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/KeySlice.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/KsDef.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Mutation.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SchemaDisagreementException.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SlicePredicate.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SliceRange.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/TimedOutException.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/TokenRange.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/UnavailableException.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
orphaned data files may be created during migration race,CASSANDRA-2381,12502323,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,24/Mar/11 20:55,12/Mar/19 14:02,13/Mar/19 22:26,28/Mar/11 19:07,0.7.5,,,,,,0,,,,,"We try to prevent creating orphans by locking Table.flusherLock in maybeSwitchMemtable and the Migration process, but since the actual writing is done asynchronously in Memtable.writeSortedContents there is a race window, where we acquire lock in maybeSwitch, we're not dropped so we queue the flush and release the lock, Migration does the drop, then Memtable writes itself out.",,,,,,,,,,,,,,,,24/Mar/11 20:59;jbellis;2381-0.8.txt;https://issues.apache.org/jira/secure/attachment/12474557/2381-0.8.txt,28/Mar/11 16:01;jbellis;2381-v3.txt;https://issues.apache.org/jira/secure/attachment/12474782/2381-v3.txt,28/Mar/11 14:10;jbellis;2831-v2.txt;https://issues.apache.org/jira/secure/attachment/12474778/2831-v2.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-03-28 13:21:41.173,,,no_permission,,,,,,,,,,,,20593,,,Mon Mar 28 20:16:26 UTC 2011,,,,,,0|i0gb1z:,93221,gdusbabek,gdusbabek,,,,,,,,,"24/Mar/11 20:59;jbellis;Patch against 0.8:

- moves Table.flusherLock to Memtable.flushLock; acquire during memtable writing
- replaces flushLock use in Table.maybeSwitchMemtables with a synchronized block
- removes beforeApplyModel; snapshotting is moved into applyModel for drops and removed entirely for updates",28/Mar/11 13:21;gdusbabek;Is it necessary to make the MT lock static?  I do not think there would be a problem with concurrently flushing two memtables of different column families (the CL serialization is still preserved by the synchronization in CFS).,28/Mar/11 14:10;jbellis;You're right. v2 attached and rebased. Had to make lock/unlock responsibility of individual Migration classes since lock is no longer global.,28/Mar/11 14:54;gdusbabek;I'm seeing unit test failures in CliTest and DefsTest with v2 applied (never ran them on v1).,28/Mar/11 16:01;jbellis;v3 grabs CF reference before removing it from CFMetadata map.  tests pass.,28/Mar/11 17:49;gdusbabek;+1.,28/Mar/11 19:07;jbellis;committed,"28/Mar/11 20:16;hudson;Integrated in Cassandra-0.7 #411 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/411/])
    r/m unnecessary isDropped check from maybeSwitchMemtable (the one in Memtable.writeSortedContents it the important one)
patch by jbellis for CASSANDRA-2381
add actual dropped check to Memtable.flush for CASSANDRA-2381
patch by jbellis
fix migration race vs flush
patch by jbellis; reviewed by gdusbabek for CASSANDRA-2381
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: Range query throws errors when run thru cqlsh but passes in system test,CASSANDRA-2600,12506117,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,03/May/11 22:54,12/Mar/19 14:02,13/Mar/19 22:26,09/May/11 07:14,0.8.0,,,,,,0,cql,,,,"*It appears the following nose test breaks when run via cqlsh*
{code}
CREATE COLUMNFAMILY StandardLongA (KEY text PRIMARY KEY) WITH comparator = bigint AND default_validation = ascii;

UPDATE StandardLongA SET 1='1', 2='2', 3='3', 4='4' WHERE KEY='aa';
UPDATE StandardLongA SET 5='5', 6='6', 7='8', 9='9' WHERE KEY='ab';
UPDATE StandardLongA SET 9='9', 8='8', 7='7', 6='6' WHERE KEY='ac';
UPDATE StandardLongA SET 5='5', 4='4', 3='3', 2='2' WHERE KEY='ad';
UPDATE StandardLongA SET 1='1', 2='2', 3='3', 4='4' WHERE KEY='ae';
UPDATE StandardLongA SET 1='1', 2='2', 3='3', 4='4' WHERE KEY='af';
UPDATE StandardLongA SET 5='5', 6='6', 7='8', 9='9' WHERE KEY='ag';

cqlsh> SELECT 4 FROM StandardLongA WHERE KEY > 'ad' AND KEY < 'ag';
Internal application error

cqlsh> SELECT * FROM StandardLongA WHERE KEY > 'ad' AND KEY < 'ag';
Internal application error
{code}


{code}

ERROR 21:43:16,880 Internal error processing execute_cql_query
java.lang.AssertionError: [109302822465993666080409141220504733189,104027502549504462599318918375258179002]
	at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:40)
	at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:33)
	at org.apache.cassandra.cql.QueryProcessor.multiRangeSlice(QueryProcessor.java:142)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:507)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1127)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{code}

*This test case runs nightly in the system tests and passes*
[http://173.203.89.16:8080/job/CassandraSystem/]
{code}
jenkins@mallen2:~/jobs/Cassandra/workspace$ nosetests test/system/test_cql.py
..................................
----------------------------------------------------------------------
Ran 34 tests in 147.040s

OK

{code}
",,,,,,,,,,,,,,,,05/May/11 10:39;xedin;CASSANDRA-2600.patch;https://issues.apache.org/jira/secure/attachment/12478264/CASSANDRA-2600.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-04 17:44:04.038,,,no_permission,,,,,,,,,,,,20724,,,Tue May 10 22:30:30 UTC 2011,,,,,,0|i0gcc7:,93429,jbellis,jbellis,,,,,,,,,04/May/11 17:44;xedin;Can you post your environment/cluster settings here please?...,04/May/11 19:47;xedin;Does select work from you with ByteOrderedPartitioner?,"04/May/11 23:24;brandon.williams;If you are using RandomPartioner, what is happening is it's comparing the md5 value of 'ad' against 'ag' and it turns out 'ad' is higher when hashed.  This isn't a useful query to perform with RP (it will never do what you want) so CQL should probably just throw a better error here to let the user know that.","05/May/11 06:01;cdaw;The test now passes after changing cassandra.yaml to use 
partitioner: org.apache.cassandra.dht.ByteOrderedPartitioner

There is a comment in the test_cql.py file that says:
{code}
# FIXME: The above is woefully inadequate, but the test config uses
# CollatingOrderPreservingPartitioner which only supports UTF8.
{code}

","05/May/11 09:39;xedin;It seems though that that partitioner is needed for some of the tests, so let it be for now. I will throw a prettier error if user tries to do a key range slice with RandomPartitioner, will attach file asap.","05/May/11 10:19;jbellis;Here's what we do in classic thrift if end < start:

{code}
                if (p instanceof RandomPartitioner)
                    throw new InvalidRequestException(""start key's md5 sorts after end key's md5.  this is not allowed; you probably should not specify end key at all, under RandomPartitioner"");
                else
                    throw new InvalidRequestException(""start key must sort before (or equal to) finish key in your partitioner!"");
{code}",05/May/11 10:39;xedin;Please apply after CASSANDRA-2592,09/May/11 07:14;jbellis;committed,"10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Actually uses efficient cross DC writes,CASSANDRA-3472,12530704,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,08/Nov/11 10:36,12/Mar/19 14:02,13/Mar/19 22:26,08/Nov/11 15:29,0.8.8,1.0.3,,,,,0,,,,,"CASSANDRA-2138 introduced the following code:
{noformat}
if (dataCenter.equals(localDataCenter) || StorageService.instance.useEfficientCrossDCWrites())
{
    // direct writes to local DC or old Cassadra versions
    for (InetAddress destination : messages.getValue())
        MessagingService.instance().sendRR(message, destination, handler);
}
else
{
    // Non-local DC. First endpoint in list is the destination for this group
{noformat}
A 'not' is missing on that useEfficientCrossDCWrites call (which does return true for any version >= 0.7.1).

A simple fix would be to add the missing !, but as said a comment, all this code should have been removed in 0.8 since it was detecting nodes before 0.7.1, but direct upgrade from pre-0.7.1 to 0.8+ is not supported. So let's just completely remove that code now.",,,,,,,,,,,,,,,,08/Nov/11 10:37;slebresne;3472.patch;https://issues.apache.org/jira/secure/attachment/12502905/3472.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-08 15:10:57.189,,,no_permission,,,,,,,,,,,,216442,,,Tue Nov 08 18:09:07 UTC 2011,,,,,,0|i0gkbr:,94723,jbellis,jbellis,,,,,,,,,08/Nov/11 10:37;slebresne;Attached patch is against 0.8,08/Nov/11 15:10;jbellis;+1,08/Nov/11 15:29;slebresne;Committed,"08/Nov/11 18:09;hudson;Integrated in Cassandra-0.8 #396 (See [https://builds.apache.org/job/Cassandra-0.8/396/])
    Fix bug preventing the use of efficient cross-DC writes
patch by slebresne; reviewed by jbellis for CASSANDRA-3472

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1199284
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
occasional failure of CliTest,CASSANDRA-3939,12543558,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,21/Feb/12 20:25,12/Mar/19 14:02,13/Mar/19 22:26,21/Feb/12 22:48,1.0.8,,,Legacy/Testing,,,0,,,,,"{{CliTest}} will occasionally fail with an NPE.

{noformat}
[junit] Testcase: testCli(org.apache.cassandra.cli.CliTest):	Caused an ERROR
[junit] java.lang.NullPointerException
[junit] java.lang.RuntimeException: java.lang.NullPointerException
[junit] 	at org.apache.cassandra.cli.CliClient.executeAddColumnFamily(CliClient.java:1039)
[junit] 	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:228)
[junit] 	at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:213)
[junit] 	at org.apache.cassandra.cli.CliTest.testCli(CliTest.java:241)
[junit] Caused by: java.lang.NullPointerException
[junit] 	at org.apache.cassandra.cli.CliClient.validateSchemaIsSettled(CliClient.java:2855)
[junit] 	at org.apache.cassandra.cli.CliClient.executeAddColumnFamily(CliClient.java:1030)
{noformat}

This occurs because no default for {{schema_mwt}} is applied unless {{main()}} is invoked.

(Trivial )patch to follow.",,,,,,,,,,,,,,,,21/Feb/12 20:29;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3939-properly-initialize-CliSessionState.sch.txt;https://issues.apache.org/jira/secure/attachment/12515483/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3939-properly-initialize-CliSessionState.sch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-21 20:51:15.827,,,no_permission,,,,,,,,,,,,228797,,,Tue Feb 21 22:48:48 UTC 2012,,,,,,0|i0gq1j:,95649,brandon.williams,brandon.williams,,,,,,,,,21/Feb/12 20:51;brandon.williams;+1,21/Feb/12 22:48;urandom;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When Snappy compression is not available on the platform, trying to enable it introduces problems",CASSANDRA-3573,12533891,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,tivv,tivv,05/Dec/11 15:20,12/Mar/19 14:02,13/Mar/19 22:26,05/Dec/11 20:30,1.0.6,,,,,,0,,,,,"I've tried to enable compression for some column families in my cluster using Snappy compression.
It does not work and I am having problems with schema updates to remove it (a lot of UNREACHABLE nodes during scema update).

In log I have the next:


ERROR [FlushWriter:961] 2011-12-05 17:16:33,383 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Flu
shWriter:961,5,main]
java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy
        at org.apache.cassandra.io.compress.SnappyCompressor.initialCompressedBufferLength(SnappyCompressor.java:39)
        at org.apache.cassandra.io.compress.CompressedSequentialWriter.<init>(CompressedSequentialWriter.java:63)
        at org.apache.cassandra.io.compress.CompressedSequentialWriter.open(CompressedSequentialWriter.java:34)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:91)
        at org.apache.cassandra.db.ColumnFamilyStore.createFlushWriter(ColumnFamilyStore.java:1850)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:250)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:47)
        at org.apache.cassandra.db.Memtable$4.runMayThrow(Memtable.java:291)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)

It looks like Snappy can't initialize because it does not have native library for my platform. It would be great if:
1) A check be done on schema update if Snappy can be used
2) If it is enabled and can't be used it would still work without compression writes (but may be outputting some errors to indicate the situation)
 ","FreeBSD
",,,,,,,,,,,,,,,05/Dec/11 19:34;xedin;CASSANDRA-3573.patch;https://issues.apache.org/jira/secure/attachment/12506148/CASSANDRA-3573.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-05 15:40:37.905,,,no_permission,,,,,,,,,,,,219617,,,Mon Dec 05 20:30:01 UTC 2011,,,,,,0|i0glkn:,94925,brandon.williams,brandon.williams,,,,,,,,,"05/Dec/11 15:40;tjake;If you build snappy-java directly on those machines you can tell snappy to use that library with the following:

cassandra -Djava.library.path=(path to the installed snappyjava lib) -Dorg.xerial.snappy.use.systemlib=true","05/Dec/11 16:24;tivv;In case when someone get int othis trap, I could replace Snappy compression class with NOOP one and restart: 

public class SnappyCompressor implements ICompressor
{
    public static final SnappyCompressor instance = new SnappyCompressor();

    public static SnappyCompressor create(Map<String, String> compressionOptions)
    {
        // no specific options supported so far
        return instance;
    }

    public int initialCompressedBufferLength(int chunkLength)
    {
        return chunkLength;
    }

    public int compress(byte[] input, int inputOffset, int inputLength, ICompressor.WrappedArray output, int outputOffset) throws IOException
    {
        System.arraycopy(input, inputOffset, output.buffer, outputOffset, inputLength);
        return inputLength;
    }

    public int uncompress(byte[] input, int inputOffset, int inputLength, byte[] output, int outputOffset) throws IOException
    {
        System.arraycopy(input, inputOffset, output, outputOffset, inputLength);
        return inputLength;
    }
}
",05/Dec/11 17:01;jbellis;Sounds like we should try to instantiate the compressor during validateCfDef to make sure it's not going to cause problems later.,05/Dec/11 18:21;tivv;I'm afraid the patch won't help because Snappy compressor don't initialize snappy on init. This must be changed too.,"05/Dec/11 18:25;xedin;you are right, I will add validation to SnappyCompressor create method to ensure that Snappy lib is in place.","05/Dec/11 20:06;brandon.williams;We probably need to bump the thrift rev too, but +1 otherwise.","05/Dec/11 20:08;brandon.williams;bq. We probably need to bump the thrift rev too

Nevermind.",05/Dec/11 20:30;xedin;Committed with grammar change thrown/threw.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node which was decommissioned and shut-down reappears on a single node,CASSANDRA-3243,12524242,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,alienth,alienth,22/Sep/11 07:19,12/Mar/19 14:02,13/Mar/19 22:26,30/Sep/11 20:15,0.8.7,,,,,,0,,,,,"I decommissioned a node several days ago. It was no longer in the ring list on any node in the ring. However, it was in the dead gossip list.

In an attempt to clean it out of the dead gossip list so I could truncate, I shut down the entire ring and bought it back up. Once the ring came back up, one node showed the decommissioned node as still in the ring in a state of 'Down'. No other node in the ring shows this info.

I successfully ran removetoken on the node to get that phantom node out. However, it is back in the dead gossip list, preventing me from truncating.

Where might the info on this decommissioned node be being stored? Is HH possibly trying to deliver to the removed node, thus putting it back in the ring on one node?

I find it extremely curious that none of the other nodes in the ring showed the phantom node. Shouldn't gossip have propagated the node everywhere, even if it was down?",,,,,,,,,,,,,,,,23/Sep/11 23:16;brandon.williams;3243.txt;https://issues.apache.org/jira/secure/attachment/12496337/3243.txt,23/Sep/11 06:00;alienth;locationinfo_0919.tgz;https://issues.apache.org/jira/secure/attachment/12496228/locationinfo_0919.tgz,23/Sep/11 06:00;alienth;locationinfo_0922.tgz;https://issues.apache.org/jira/secure/attachment/12496229/locationinfo_0922.tgz,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-09-22 10:32:45.468,,,no_permission,,,,,,,,,,,,3528,,,Fri Sep 30 21:49:04 UTC 2011,,,,,,0|i0ghjr:,94273,bcoverston,bcoverston,,,,,,,,,"22/Sep/11 07:27;alienth;Looking through the logs, the node which saw the decommissioned node didn't print anything about discovering it via gossip. The very first log line I have regarding the phantom node is when I forced a removetoken.","22/Sep/11 10:32;brandon.williams;Can you explain what you mean by ""dead gossip list"" and how this prevents truncate?

bq. Where might the info on this decommissioned node be being stored?

After CASSANDRA-2496, we store dead gossip states for 3 days, so that any other nodes that were down at the time of removal can know later not to repopulate the ring with the removed node, but this isn't persisted anywhere, so since you did a full ring restart, the only candidate left is the persisted endpoints, though all nodes should have removed it from there after the decommission/removetoken.

bq. Is HH possibly trying to deliver to the removed node, thus putting it back in the ring on one node?

No, HH will only attempt delivery on an onAlive event, and it doesn't inject any gossip states.","22/Sep/11 19:39;alienth;bq. Can you explain what you mean by ""dead gossip list"" and how this prevents truncate?

The decommissioned node is showing up in the 'UNREACHABLE' list when calling 'describe cluster'. When I attempt to run truncate, the command returns that truncate cannot occur due to a node being down.

bq. After CASSANDRA-2496, we store dead gossip states for 3 days, so that any other nodes that were down at the time of removal can know later not to repopulate the ring with the removed node, but this isn't persisted anywhere, so since you did a full ring restart, the only candidate left is the persisted endpoints, though all nodes should have removed it from there after the decommission/removetoken.

Is there a way I can get a list of endpoints to see how this node showed back up?


Also, any thoughts on why this node only re-appeared on a single node?

Thanks!
Jason","22/Sep/11 22:29;brandon.williams;bq. Is there a way I can get a list of endpoints to see how this node showed back up?

It must be a saved endpoint, if you can attached the latest LocationInfo sstable from that machine (and tell me the IP and/or token) I can take a look.

bq. Also, any thoughts on why this node only re-appeared on a single node?

I'm not sure, let's see if it was still persisted first.","23/Sep/11 06:00;alienth;LocationInfo from the node which re-added the dead node back to the ring.

This LocationInfo is from *after* the decommission of the phantom node, but before the restart which resulted in the node re-adding the phantom node.

The phantom node's token was 120000000000000000000000000000000000000.","23/Sep/11 06:00;alienth;LocationInfo from the node which re-added the dead node back to the ring.

This LocationInfo is from *after* the restart which resulted in the phantom node re-appearing. It is also after the forced token removal of the phantom node.

The phantom node's token was 120000000000000000000000000000000000000.","23/Sep/11 20:02;brandon.williams;0919 is missing the LOCATION_KEY (the node's own token) which is odd, because cassandra will refuse to startup with this table since it should not exist without this key. It does show itself in the saved endpoints, but no other nodes.

0922 is complete in that it contains LOCATION_KEY and cassandra starts right up with it, and I can see the removed token in the saved endpoints with an ip address of 10.34.22.201.  However the strange thing is the timestamp on that column is approximately 2 days _older_ than the one for the local node itself, which should be impossible.  Is there any chance this node's clock was way off or changed?","23/Sep/11 22:28;alienth;Clock on these boxes seems fine. We keep ntpd running at all times. I've also verified via logging that it has been consistent.

How do I go about getting that endpoint *out* of the LocationInfo?","23/Sep/11 23:03;alienth;For outside reference:

Brandon recommended I delete the 'Ring' key from the LocationInfo on this node and then restart to resolve the weirdness.","23/Sep/11 23:16;brandon.williams;The best explanation I have for how you could get here is that SystemTable only forces a flush on the updateToken(Token token) signature.  removeToken and updateToken(InetAddress ep, Token token) do not, so if the machine is restarted before the commitlog is synced, the update/removal can be lost.  Patch to address this.",30/Sep/11 20:05;bcoverston;+1 on the patch,30/Sep/11 20:15;brandon.williams;Committed.  Please reopen if you see this again after 0.8.7.,"30/Sep/11 21:49;hudson;Integrated in Cassandra-0.8 #351 (See [https://builds.apache.org/job/Cassandra-0.8/351/])
    Flush system table after updating or removing tokens.
Patch by brandonwilliams, reviewed by Ben Coverston for CASSANDRA-3243

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177810
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cancelling index build throws assert error,CASSANDRA-3313,12525766,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,05/Oct/11 06:58,12/Mar/19 14:02,13/Mar/19 22:26,20/Oct/11 21:14,1.0.1,,,,,,0,indexing,,,,"Canceling index build throws this, but checking log there was no compaction running in background.

INFO 08:46:41,343 Writing Memtable-IndexInfo@9480253(34/42 serialized/live byte
s, 1 ops)
ERROR 08:46:41,343 Fatal exception in thread Thread[CompactionExecutor:3,5,main]

java.lang.AssertionError
        at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates
(SecondaryIndexManager.java:397)
        at org.apache.cassandra.db.Table.indexRow(Table.java:534)
        at org.apache.cassandra.db.index.SecondaryIndexBuilder.build(SecondaryIn
dexBuilder.java:64)
        at org.apache.cassandra.db.compaction.CompactionManager$7.run(Compaction
Manager.java:856)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 08:46:41,531 Completed flushing \var\lib\cassandra\data\system\IndexInfo-h
-1-Data.db (88 bytes)",,,,,,,,,,,,,,,,17/Oct/11 17:58;jbellis;3313.txt;https://issues.apache.org/jira/secure/attachment/12499408/3313.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-05 07:22:07.703,,,no_permission,,,,,,,,,,,,46313,,,Thu Oct 20 21:14:14 UTC 2011,,,,,,0|i0gie7:,94410,slebresne,slebresne,,,,,,,,,05/Oct/11 07:22;jbellis;I'm not aware of a 'cancel index build' command so I'm curious what you mean by that.,05/Oct/11 11:57;hsn;redefine CF to column_metadata without index.,17/Oct/11 17:58;jbellis;patch to accommodate index disappearing mid-update.  applies on top of CASSANDRA-3314.,"19/Oct/11 16:44;slebresne;+1

nitpick: to be uber pedantic, we could use the row key validator to print the row key in the log messages.",20/Oct/11 21:09;jbellis;same as CASSANDRA-3334,"20/Oct/11 21:14;jbellis;actually it's not quite the same, got confused by merge conflicts",20/Oct/11 21:14;jbellis;rebased + committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""show schema"" in CLI outputs invalid text structure that cannot be replayed (easily tweakable though)",CASSANDRA-3129,12521215,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,udontknow,udontknow,02/Sep/11 18:23,12/Mar/19 14:02,13/Mar/19 22:26,06/Sep/11 16:50,0.8.6,,,Legacy/Tools,,,0,,,,,"Log explaining the problem. Trouble happens at the ""and replication_factor = 1"" string

[default@unknown] connect cassandra2/9160;                                       
Connected to: ""Lab1"" on cassandra2/9160

* Create a keyspace with a pretty simple definition
[default@unknown] create keyspace foo
...	  with placement_strategy = 'SimpleStrategy'
...	  and strategy_options = [{replication_factor : 1}];
f9e13340-d58f-11e0-0000-e3f60146f2df
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] use foo;
Authenticated to keyspace: foo

* Copy the schema so we can paste it later
[default@foo] show schema; 
create keyspace foo
  and replication_factor = 1
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = [{replication_factor : 1}];

use foo;


* Remove the keyspace, so we can paste the exact same text above
[default@foo] drop keyspace foo;
07c93a70-d590-11e0-0000-e3f60146f2df
Waiting for schema agreement...
... schemas agree across the cluster

* Paste the schema shown above as result of the 'show schema' command
[default@unknown] create keyspace foo
...	  and replication_factor = 1
...	  with placement_strategy = 'SimpleStrategy'
...	  and strategy_options = [{replication_factor : 1}];
No enum constant org.apache.cassandra.cli.CliClient.AddKeyspaceArgument.REPLICATION_FACTOR
* Presented an error that should not occur if show schema generated valid text
","CentOS 6.0 
Linux cassandra2.local 2.6.32-71.29.1.el6.x86_64 #1 SMP Mon Jun 27 19:49:27 BST 2011 x86_64 x86_64 x86_64 GNU/Linux
java version ""1.7.0""
Java(TM) SE Runtime Environment (build 1.7.0-b147)
Java HotSpot(TM) 64-Bit Server VM (build 21.0-b17, mixed mode)
Cassandra 0.8.4 from the official tarball, also reproducible on the datastax 0.8.4-1 rpm.",,,,,,,,,,,,,,,06/Sep/11 16:05;xedin;CASSANDRA-3129.patch;https://issues.apache.org/jira/secure/attachment/12493170/CASSANDRA-3129.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-02 19:45:43.296,,,no_permission,,,,,,,,,,,,4075,,,Tue Sep 06 17:29:16 UTC 2011,,,,,,0|i0gg53:,94045,jbellis,jbellis,,,,,,,,,"02/Sep/11 19:45;xedin;Evaldo, please confirm that this fixed the problem!",06/Sep/11 15:27;jbellis;Does this handle an upgraded 0.7 schema that didn't have replication_factor set in strategy_options?,"06/Sep/11 15:47;xedin;makes sure to include ""replication_factor"" if set as an attribute to the strategy_options.","06/Sep/11 15:57;jbellis;The existing backwards compatibility code in KSMetadata doesn't cover this?

(No, I don't know the answer to these questions.)","06/Sep/11 16:05;xedin;Oh, I overlooked that - it actually does add replication_factor to the strategy_options.","06/Sep/11 16:23;jbellis;+1

I also note that the original report shows ""... and ... with ... and ..."" in the create statement.  Is there a second bug?  The first option should always be ""with.""","06/Sep/11 16:40;xedin;bq. I also note that the original report shows ""... and ... with ... and ..."" in the create statement. Is there a second bug? The first option should always be ""with.""

It was fixed with removal of replication_strategy - writeAttr(...) call there had a wrong second argument.",06/Sep/11 16:50;xedin;Committed.,"06/Sep/11 17:29;hudson;Integrated in Cassandra-0.8 #316 (See [https://builds.apache.org/job/Cassandra-0.8/316/])
    Fix CLI `show schema;` to output correct keyspace definition statement
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3129

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1165758
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IntergerType uses Thrift method that attempts to unsafely access backing array of ByteBuffer and fails,CASSANDRA-2684,12508014,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,edanuff,edanuff,edanuff,22/May/11 20:43,12/Mar/19 14:02,13/Mar/19 22:26,23/May/11 01:34,0.7.7,0.8.0,,,,,0,,,,,"I get the following exception:

{noformat}
ERROR 13:27:38,153 Fatal exception in thread Thread[ReadStage:36,5,main]
java.lang.RuntimeException: java.lang.UnsupportedOperationException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:940)
	at org.apache.thrift.TBaseHelper.byteBufferToByteArray(TBaseHelper.java:264)
	at org.apache.thrift.TBaseHelper.byteBufferToByteArray(TBaseHelper.java:251)
	at org.apache.cassandra.db.marshal.IntegerType.getString(IntegerType.java:136)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getString(AbstractCompositeType.java:131)
	at org.apache.cassandra.db.Column.getString(Column.java:228)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:123)
	at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1303)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1188)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1145)
	at org.apache.cassandra.db.Table.getRow(Table.java:385)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:61)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:641)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}

Tracing it down, I find that IntegerType's getString method() looks like this:

{code:title=IntegerType.java|borderStyle=solid}
    public String getString(ByteBuffer bytes)
    {
        if (bytes == null)
            return ""null"";
        if (bytes.remaining() == 0)
            return ""empty"";

        return new java.math.BigInteger(TBaseHelper.byteBufferToByteArray(bytes)).toString(10);
    }
{code} 
    
TBaseHelper.byteBufferToByteArray() looks like this:

{code:title=TBaseHelper.java|borderStyle=solid}
  public static byte[] byteBufferToByteArray(ByteBuffer byteBuffer) {
    if (wrapsFullArray(byteBuffer)) {
      return byteBuffer.array();
    }
    byte[] target = new byte[byteBuffer.remaining()];
    byteBufferToByteArray(byteBuffer, target, 0);
    return target;
  }

  public static boolean wrapsFullArray(ByteBuffer byteBuffer) {
    return byteBuffer.hasArray()
      && byteBuffer.position() == 0
      && byteBuffer.arrayOffset() == 0
      && byteBuffer.remaining() == byteBuffer.capacity();
  }

  public static int byteBufferToByteArray(ByteBuffer byteBuffer, byte[] target, int offset) {
    int remaining = byteBuffer.remaining();
    System.arraycopy(byteBuffer.array(),
        byteBuffer.arrayOffset() + byteBuffer.position(),
        target,
        offset,
        remaining);
    return remaining;
  }
{code} 

The second overloaded implementation of byteBufferToByteArray is calling the bytebuffer's array() method.

Suggested fixes:

1) Don't use TBaseHelper in IntegerType.getString(), use ByteBufferUtil.getArray()

2) Report problem upstream to Thrift.

3) Find a better way to deserialize BigIntegers that doesn't require an array copy.






",,,,,,,,,,,,,,,,22/May/11 20:54;edanuff;2684.txt;https://issues.apache.org/jira/secure/attachment/12480058/2684.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-23 01:34:31.98,,,no_permission,,,,,,,,,,,,20774,,,Mon May 23 01:56:47 UTC 2011,,,,,,0|i0gcu7:,93510,,,,,,,,,,,23/May/11 01:34;jbellis;committed,"23/May/11 01:56;hudson;Integrated in Cassandra-0.7 #492 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/492/])
    fix IntegerType.getString with direct buffers
patch by Ed Anuff; reviewed by jbellis for CASSANDRA-2684

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1126290
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/marshal/IntegerType.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair hangs if a neighbor has nothing to send ,CASSANDRA-2797,12510951,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,20/Jun/11 12:33,12/Mar/19 14:02,13/Mar/19 22:26,20/Jun/11 17:29,0.8.1,,,,,,0,repair,streaming,,,"This is actually a streaming problem. If a StreamOutSession has nothing to transfer (i.e, no sstables have the requested ranges), it will not even initiate the transfer and simply close the session right away. The problem is that if the session was initiated by a remote end (through a StreamRequestMessage), the remote end will never be notified and never run his callback.
",,,,,,,,,,,,,,,,20/Jun/11 17:04;slebresne;0001-Always-initiate-streaming-transfer-to-notify-remote-.patch;https://issues.apache.org/jira/secure/attachment/12483174/0001-Always-initiate-streaming-transfer-to-notify-remote-.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-20 15:51:10.766,,,no_permission,,,,,,,,,,,,20836,,,Mon Jun 20 19:20:28 UTC 2011,,,,,,0|i0gdin:,93620,,,,,,,,,,,"20/Jun/11 12:35;slebresne;Apparently the code was already wired to handle initiating a session with nothing to transfer, but a if prevented that code path. Patch attached against 0.7.","20/Jun/11 15:51;jbellis;I thought transferSSTables is supposed to be only called for locally-initiated transfers, and transferRangesForRequest is for ones initiated remotely.  (Confusing, I know.  This got cleaned up in 0.8.)","20/Jun/11 17:04;slebresne;You are right. I hit this on 0.8 and wrongly assumed 0.7 was impacted too while this is actually a problem due to the mentioned refactor. Attaching patch against 0.8, 0.7 is not impacted.",20/Jun/11 17:08;jbellis;+1,"20/Jun/11 17:29;slebresne;Committed, thanks","20/Jun/11 19:20;hudson;Integrated in Cassandra-0.8 #178 (See [https://builds.apache.org/job/Cassandra-0.8/178/])
    fix repair hanging if a neighbor has nothing to send
patch by slebresne; reviewed by jbellis for CASSANDRA-2797

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1137711
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/StreamOut.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add key_validation_class support to cli,CASSANDRA-2432,12503630,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,erny1803,erny1803,07/Apr/11 12:42,12/Mar/19 14:02,13/Mar/19 22:26,09/Apr/11 18:20,0.8 beta 1,,,Legacy/Documentation and Website,,,0,,,,,Also update README to include utf8type key validator.,"svn: 1089671
Ubuntu 10.10 / amd64
Java(TM) SE Runtime Environment (build 1.6.0_24-b07)
Java HotSpot(TM) 64-Bit Server VM (build 19.1-b02, mixed mode)",1200,1200,,0%,1200,1200,,,,,,,,,09/Apr/11 13:40;xedin;CASSANDRA-2432.patch;https://issues.apache.org/jira/secure/attachment/12475888/CASSANDRA-2432.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-09 18:20:49.55,,,no_permission,,,,,,,,,,,,20621,,,Sat Apr 09 19:17:18 UTC 2011,,,,,,0|i0gbcf:,93268,jbellis,jbellis,,,,,,,,,09/Apr/11 18:20;jbellis;committed,"09/Apr/11 19:17;hudson;Integrated in Cassandra #839 (See [https://hudson.apache.org/hudson/job/Cassandra/839/])
    add key_validation_class support to cli
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2432
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix ""Unable to create hard link"" SSTableReaderTest error messages",CASSANDRA-3735,12538275,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,12/Jan/12 23:11,12/Mar/19 14:02,13/Mar/19 22:26,06/Feb/12 16:10,1.1.0,,,,,,0,,,,,"Sample failure (on Windows):

{noformat}
    [junit] java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\Users\Jonathan\projects\cassandra\git\build\test\cassandra\data\Keyspace1\backups\Standard1-hc-1-Index.db c:\Users\Jonathan\projects\cassandra\git\build\test\cassandra\data\Keyspace1\Standard1-hc-1-Index.db,command error Code: 1, command output: Cannot create a file when that file already exists.
    [junit]
    [junit]     at org.apache.cassandra.utils.CLibrary.exec(CLibrary.java:213)
    [junit]     at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188)
    [junit]     at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)

    [junit]     at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 17:10:17,111 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
{noformat}",,,,,,,,,,,,,,,,12/Jan/12 23:26;jbellis;0001-fix-generation-update-in-loadNewSSTables.patch;https://issues.apache.org/jira/secure/attachment/12510437/0001-fix-generation-update-in-loadNewSSTables.patch,05/Feb/12 02:52;scode;0002-remove-incremental-backups-before-reloading-sstables-v2.patch;https://issues.apache.org/jira/secure/attachment/12513276/0002-remove-incremental-backups-before-reloading-sstables-v2.patch,12/Jan/12 23:26;jbellis;0002-remove-incremental-backups-before-reloading-sstables.patch;https://issues.apache.org/jira/secure/attachment/12510438/0002-remove-incremental-backups-before-reloading-sstables.patch,05/Feb/12 06:17;scode;0003-reset-file-index-generator-on-reset.patch;https://issues.apache.org/jira/secure/attachment/12513285/0003-reset-file-index-generator-on-reset.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-01-23 10:23:26.094,,,no_permission,,,,,,,,,,,,223781,,,Mon Feb 06 16:10:21 UTC 2012,,,,,,0|i0gnlj:,95253,slebresne,slebresne,,,,,,,,,"12/Jan/12 23:26;jbellis;Two parts:

first makes it so loadNewSSTables won't reset sstable generation to a lower value than it currently is.

second removes the (hard linked) sstables from the incremental backups directory, so that when loadNewSSTables links them back in they won't conflict.

patch 2 doesn't actually work on Windows because something is keeping the Data component open.  Java6 just says ""delete failed;"" java7's Files.delete says ""java.nio.file.FileSystemException: build\test\cassandra\data\Keyspace1\backups\Standard1-hc-1-Data.db: The process cannot access the file because it is being used by another process.""  (Even after I prototyped CASSANDRA-3734 to use Files.createLink instead of ProcessBuilder, so that's not the ""other process."")

I'm guessing it's counting something (what?) that opened the original Data, towards the link entry as well.  But, this should be enough to make the messages go away on Linux, which takes a more permissive view of deleting files opened elsewhere.","23/Jan/12 10:23;slebresne;This seems to handle most of the warnings. However (on 1.0 branch):
* SSTableReaderTest doesn't pass on linux either. It ends up with the following trace:
{noformat}
    [junit] Testcase: testPersistentStatisticsFromOlderIndexedSSTable(org.apache.cassandra.io.sstable.SSTableReaderTest):	Caused an ERROR
    [junit] Failed to delete /home/mcmanus/Git/cassandra/build/test/cassandra/data/Keyspace1/backups
    [junit] java.io.IOException: Failed to delete /home/mcmanus/Git/cassandra/build/test/cassandra/data/Keyspace1/backups
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:237)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.clearAndLoad(SSTableReaderTest.java:167)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.assertIndexQueryWorks(SSTableReaderTest.java:260)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.testPersistentStatisticsFromOlderIndexedSSTable(SSTableReaderTest.java:251)
{noformat}
* I get a ""Largest generation seen in loaded sstables was 8, which may overlap with native sstable files (generation 8)"" warning during ColumnFamilyStoreTest, followed by  a (not surpising) 'Unable to create hard link', in testSliceByNamesCommandOldMetatada. I believe a quick fix could be to have clearUnsafe reset the generation for the CF. Another solution would be to make loadNewSSTables smarter/less fragile by having it 'reserve' a generation for each of the new file to load from the current generation and rename the loaded files accordingly.",05/Feb/12 02:52;scode;Attaching new version of 0002* that works (but still with the left-overs already mentioned by jbellis/sylvain) post CASSANDRA-2794.,"05/Feb/12 06:17;scode;Correction, the conversion to post-2794 did remove the failure of the SSTableReaderTest. Or at least it's no longer happening for me on trunk with the attached patch (v2). Likely because it's only removing specific sstable components given by the iterator, rather than trying to recursively delete backups, but I don't pretend to understand exactly what the history of changes is that caused it to start failing to delete it to begin with.

With repsect to the 'Largest generation seen...' warning, I get that too, but I don't see any subsequent hard link creation failures, nor do I understand why I would if the files are created without using the counter and the quick fix just suppresses the warning? But I'm probably missing something. I do have failing hard linking in ThriftValidationTest though. Maybe this is a side-effect that you're referring to?

In any case, attaching the trivial (if I understood the suggestion correctly) reset patch that supresses the warning.


","06/Feb/12 16:10;slebresne;I still had one remaining link error with v2 in CFSTest, but that was because tests runs with incremental backup and 2 tests were reusing the same CF with just a clearUnsafe in between, so the backup were kept around. I've moved the removal of backups from the SSTableReader.clearAndLoad() method introduced by the patches to CFS.clearUnsafe() directly to avoid this. I took the liberty to commit with that last change. I don't get any link related exceptions anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log4j unable to load properties file from classpath,CASSANDRA-2383,12502347,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,dallsopp,iecanfly,iecanfly,25/Mar/11 04:07,12/Mar/19 14:02,13/Mar/19 22:26,13/Jul/11 03:19,0.7.8,,,Legacy/Tools,,,0,,,,,"when cassandra home folder is placed inside a folder which has space characters in its name,
log4j settings are not properly loaded and warning messages are shown.","OS : windows
java : 1.6.0.23",,,,,,,,,,,,,,,02/Jul/11 11:05;dallsopp;cassandra-0.7.6-2-2383.diff;https://issues.apache.org/jira/secure/attachment/12484961/cassandra-0.7.6-2-2383.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-27 20:16:28.407,,,no_permission,,,,,,,,,,,,20595,,,Wed Jul 13 03:30:35 UTC 2011,,,,,,0|i0gb2f:,93223,jbellis,jbellis,,,,,,,,,"27/Jun/11 20:16;dallsopp;In cassandra.bat, change:

 -Dlog4j.configuration=log4j-server.properties^

to:

 -Dlog4j.configuration=""file:///%CASSANDRA_HOME%/conf/log4j-server.properties""^

I'm hopeful there's a less ugly way, but this seems to work.","27/Jun/11 20:28;dallsopp;OK, simpler solution is:

 -Dlog4j.configuration=file:conf/log4j-server.properties^

But note that this only works if the current directory is CASSANDRA_HOME, so double-clicking on the batch file won't work, whereas the previous solution will work from the 'bin' directory, so double-clicking is OK.","27/Jun/11 21:42;jbellis;log4j should already find it because of

set CLASSPATH=""%CASSANDRA_HOME%\conf""
","27/Jun/11 21:45;dallsopp;Another solution that seems to work regardless of working directory is leave the original log4j.configuration line, but remove the line:

-Dlog4j.defaultInitOverride=true^

This gets the classloader to find the config file as a resource, rather than supplying a file reference directly.

However, I'm unsure why the defaultInitOverride was there in the first place...","27/Jun/11 21:55;dallsopp;@Jonathan - yes, I thought so too, but it doesn't. The default init process for log4j is described at http://logging.apache.org/log4j/1.2/manual.html, but it doesn't really explain what happens if defaultInitOverride is set!

With  -Dlog4j.debug=true and the original batch file, i.e.

{quote}
 -Dlog4j.configuration=log4j-server.properties^
 -Dlog4j.defaultInitOverride=true^}}
{quote}
I see:
{quote}
Starting Cassandra Server
log4j: [/C:/Users/David/Key%20Value/apache-cassandra-0.7.6-2/conf/log4j-server.properties] does not exist.
log4j: Default initialization of overridden by log4j.defaultInitOverrideproperty.
log4j:WARN No appenders could be found for logger (org.apache.cassandra.service.AbstractCassandraDaemon).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{quote}
If I remove the defaultInitOverride, I get:
{quote}
Starting Cassandra Server
log4j: [/C:/Users/David/Key%20Value/apache-cassandra-0.7.6-2/conf/log4j-server.properties] does not exist.
log4j: Trying to find [log4j-server.properties] using context classloader sun.misc.Launcher$AppClassLoader@1a45a877.
log4j: Using URL [file:/C:/Users/David/Key%20Value/apache-cassandra-0.7.6-2/conf/log4j-server.properties] for automatic log4j configuration.
log4j: Reading configuration from URL file:/C:/Users/David/Key%20Value/apache-cassandra-0.7.6-2/conf/log4j-server.properties
[etc...]
{quote}
Finally, with:
{quote}
 -Dlog4j.configuration=file:conf/log4j-server.properties^
 -Dlog4j.defaultInitOverride=true^
{quote}
I get this, if current directory is CASSANDRA_HOME:
{quote}
Starting Cassandra Server
log4j: Default initialization of overridden by log4j.defaultInitOverrideproperty.
[etc...]
{quote}
But this, if current directory is CASSANDRA_HOME/bin (e.g. if double-clicking the batch file):
{quote}
Starting Cassandra Server
log4j: [conf/log4j-server.properties] does not exist.
log4j: Default initialization of overridden by log4j.defaultInitOverrideproperty.
log4j:WARN No appenders could be found for logger (org.apache.cassandra.service.AbstractCassandraDaemon).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{quote}","27/Jun/11 22:04;jbellis;Weird.  Could you dig into the code in AbstractCassandraDaemon a little?  Here's what's trying to do load by classpath:

{code}
            // load from the classpath.
            configLocation = AbstractCassandraDaemon.class.getClassLoader().getResource(config);
            if (configLocation == null)
                throw new RuntimeException(""Couldn't figure out log4j configuration."");
{code}
","27/Jun/11 22:10;dallsopp;http://www.vipan.com/htdocs/log4jhelp.html has the following:

log4j.configuration=app_config.properties: 

First call to Category.getRoot() or Category.getInstance(...) method makes Log4j go through an initialization process. (You can watch that happening by setting ""log4j.debug=true"".) 

During this process, Log4j looks in the application's classpath for a ""log4j.properties"" file *or the properties file you specify* via the this property key. 

However, you need to set this as a system property, for example by running your program with java -Dlog4j.configuration=app_config.properties .... This is because, if you set it in the configuration file, it is too late. Log4j would have already started to read the log4j.properties file by default, if available!",27/Jun/11 22:22;dallsopp;@Jonathan - will try to take a look soon. The getResource() stuff might perhaps be affected by the classes being within a jarfile.,"28/Jun/11 21:22;dallsopp;OK, have gone around in circles a bit on this!

-Dlog4j.defaultInitOverride enables AbstractCassandraDaemon to take charge of the log4j configuration in order to make it dynamic (you can change the log4j config file, and it should be updated using the log4j PropertyConfigurator every 10 seconds).

The default value of log4j.configuration in the code and in the batch file is ""log4j-server.properties"", which is not a valid URL, so we drop into:

{noformat} configLocation = AbstractCassandraDaemon.class.getClassLoader().getResource(config);{noformat} 

as you said before. This *does* detect the correct file from CASSANDRA_HOME/conf, since log4j logs the *full path* even though we only supply the filename ""log4j-server.properties"":

{noformat} log4j: [/C:/Users/David/Key%20Value/apache-cassandra-0.7.6-2/conf/log4j-server.properties] does not exist.{noformat} 

getResource() returns a URL. Converting this to a file using getFile() works fine when there are no spaces; one can verify that the file exists (File.exists() == true). If there are spaces, then this conversion produces a filename that includes the %20 encoding for spaces - this is an incorrect filename.

We need instead to convert using:

{noformat} new File(configLocation.toURI());{noformat} 

(with appropriate exception handling for URISyntaxException) which produces a filename with spaces rather than %20.
","28/Jun/11 22:31;dallsopp;Suggested fix for AbstractCassandraDaemon static initializer (apologies - haven't got a suitable version of diff on this windows box yet). Untested on linux as yet.

{noformat}
    //Initialize logging in such a way that it checks for config changes every 10 seconds.
    static
    {
        String config = System.getProperty(""log4j.configuration"", ""log4j-server.properties"");
        URL configLocation = null;
        try 
        {
            // try loading from a physical location first.
            configLocation = new URL(config);
        }
        catch (MalformedURLException ex) 
        {
            // then try loading from the classpath.
            configLocation = AbstractCassandraDaemon.class.getClassLoader().getResource(config);
        }
        
        if (configLocation == null)
            throw new RuntimeException(""Couldn't figure out log4j configuration: ""+config);
        
        // Now convert URL to a filename
        String configFileName = null;
		try
		{
			// first try URL.getFile() which works for opaque URLs (file:foo) and paths without spaces
			configFileName = configLocation.getFile();
			File configFile = new File(configFileName);
			// then try alternative approach which works for all hierarchical URLs with or without spaces
			if(!configFile.exists())
			{
				configFileName = new File(configLocation.toURI()).getCanonicalPath();
			}
		} 
		catch (Exception e)
		{
			throw new RuntimeException(""Couldn't convert log4j configuration location to a valid file."", e);
		} 
        PropertyConfigurator.configureAndWatch(configFileName, 10000);
        org.apache.log4j.Logger.getLogger(AbstractCassandraDaemon.class).info(""Logging initialized"");
    }
{noformat}","29/Jun/11 01:09;jbellis;Looks like this breaks ""configuration parameter is a well-formed url?""  configLocation is never used if ""new URL"" succeeds.","29/Jun/11 13:34;dallsopp;Yes, that'll teach me to post code late at night :-(","02/Jul/11 09:39;dallsopp;OK, have edited the code snippet above to hopefully fix the obvious broken-ness! 

Still struggling to get Cassandra building properly on Windows/Eclipse so haven't yet been able to test properly though (need to work through http://wiki.apache.org/cassandra/RunningCassandraInEclipse again from scratch as it didn't seem to work first time round...)","02/Jul/11 10:06;dallsopp;The above code seems to work for full hierarchical URIs:

-Dlog4j.configuration=""file:///C:/conf%20space/log4j-server.properties""

and for classpath locations:

-Dlog4j.configuration=log4j-server.properties

(on windows, with a space in the file path)

It does not work for opaque URIs such as file:log4j-server.properties because you can't construct a File directly from these (you get java.lang.IllegalArgumentException: URI is not hierarchical)","02/Jul/11 10:26;dallsopp;OK, third time lucky I hope. The edited version above now tries the original getFile() approach, then falls back on the new File(url.toURI()) approach if the file doesn't exist.

This seems to work with classpath names, opaque URIs *and* hierarchical URIs","02/Jul/11 13:11;dallsopp;Tries to get URL of log4j configuration as before (as a URL or from the classpath). 

Attempts two different methods of converting the URL to a valid File in order to cope with spaces in file paths on Windows, as well as various flavours of URL (opaque or hierarchical).","13/Jul/11 03:19;jbellis;looks good to me.  committed, thanks!","13/Jul/11 03:30;hudson;Integrated in Cassandra-0.7 #527 (See [https://builds.apache.org/job/Cassandra-0.7/527/])
    support spaces in path to log4j configuration
patch by David Allsopp; reviewed by jbellis for CASSANDRA-2383

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1145849
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/AbstractCassandraDaemon.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
statistics component not fsynced,CASSANDRA-2382,12502344,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,25/Mar/11 03:28,12/Mar/19 14:02,13/Mar/19 22:26,07/Apr/11 15:36,0.7.5,,,,,,0,,,,,The statistics file is prone to getting lost during a hard reset since it is not fsynced like the other sstable components.,,,,,,,,,,,,,,,,25/Mar/11 03:33;jbellis;2382.txt;https://issues.apache.org/jira/secure/attachment/12474587/2382.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-25 04:05:05.875,,,no_permission,,,,,,,,,,,,20594,,,Sun Mar 27 05:44:34 UTC 2011,,,,,,0|i0gb27:,93222,brandon.williams,brandon.williams,,,,,,,,,25/Mar/11 03:33;jbellis;Patch switches to BRAF (which fsyncs on close) and also enables fadvise to avoid caching the written stats.,25/Mar/11 04:05;brandon.williams;+1,25/Mar/11 04:47;jbellis;Not sure what to do about CompactSerializerTest -- it doesn't seem to know about ICompactSerializer2.  (I'm kind of surprised we still have so many old-style serializers...),"25/Mar/11 04:50;brandon.williams;We should probably get rid of one of them if we can, I've never really understood why there are two.","25/Mar/11 04:57;jbellis;Some serializers did (do?) depend on Stream-specific methods.  Those may be gone now, it's been a while since I checked.","25/Mar/11 13:58;jbellis;bq. Not sure what to do about CompactSerializerTest

Gary says the point of CST was to test message serialization rather than on disk and EHS got lumped in because it was using ICS instead of ISC2 like the other disk serializers (ColumnFamily and childeren).  So removing it from the CST list is the right fix here.

committed w/ that change.","27/Mar/11 05:44;hudson;Integrated in Cassandra-0.7 #409 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/409/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not possible to change row_cache_provider on existing cf,CASSANDRA-3414,12529198,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,krummas,krummas,28/Oct/11 05:53,12/Mar/19 14:02,13/Mar/19 22:26,28/Oct/11 12:48,0.8.8,,,,,,0,,,,,"row_cache_provider is not possible to change using update column family xyz with row_cache_provider='something' in 0.8

It does work in 1.0.0

Reason is that the field is not added to the avro record, patch attached fixes that",,,,,,,,,,,,,,,,28/Oct/11 05:55;krummas;3414.patch;https://issues.apache.org/jira/secure/attachment/12501241/3414.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-28 12:48:43.358,,,no_permission,,,,,,,,,,,,215058,,,Fri Oct 28 15:23:40 UTC 2011,,,,,,0|i0gjlr:,94606,jbellis,jbellis,,,,,,,,,28/Oct/11 05:55;krummas;add the row_cache_provider to the avro record,"28/Oct/11 12:48;jbellis;committed, thanks!","28/Oct/11 15:23;hudson;Integrated in Cassandra-0.8 #389 (See [https://builds.apache.org/job/Cassandra-0.8/389/])
    fix updating CF row_cache_provider
patch by Marcus Eriksson; reviewed by jbellis for CASSANDRA-3414

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1190282
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFS.scrubDataDirectories tries to delete nonexistent orphans,CASSANDRA-4021,12545682,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yulinyen,brandon.williams,brandon.williams,08/Mar/12 17:22,12/Mar/19 14:02,13/Mar/19 22:26,11/Mar/13 23:16,1.1.11,1.2.3,,,,,0,datastax_qa,,,,"The check only looks for a missing data file, then deletes all other components, however it's possible for the data file and another component to be missing, causing an error:

{noformat}

 WARN 17:19:28,765 Removing orphans for /var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-24492: [Index.db, Filter.db, Digest.sha1, Statistics.db, Data.db]
ERROR 17:19:28,766 Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file system-HintsColumnFamily-hd-24492-Index.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:357)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:352)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:105)
java.lang.AssertionError: attempted to delete non-existing file system-HintsColumnFamily-hd-24492-Index.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:357)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:352)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:105)
Exception encountered during startup: attempted to delete non-existing file system-HintsColumnFamily-hd-24492-Index.db
{noformat}",,,,,,,,,,,,,,,,08/Mar/12 20:59;brandon.williams;4021.txt;https://issues.apache.org/jira/secure/attachment/12517607/4021.txt,06/Nov/12 14:19;brandon.williams;node1.log;https://issues.apache.org/jira/secure/attachment/12552274/node1.log,11/Mar/13 23:15;jbellis;patch_4021.txt;https://issues.apache.org/jira/secure/attachment/12573208/patch_4021.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-03-08 23:25:58.089,,,no_permission,,,,,,,,,,,,230865,,,Mon Mar 11 23:15:33 UTC 2013,,,,,,0|i0dvbb:,79002,jbellis,jbellis,,,,,,,,,08/Mar/12 20:59;brandon.williams;Apparently this goes all the way back to CASSANDRA-1471 but occurs very rarely.  Patch to attempt deletion without caring about success.,"08/Mar/12 23:25;jbellis;The reasoning is that since the data component is written (renamed) last, the others should be there too if the data is.","08/Mar/12 23:29;brandon.williams;Aren't we actually firing this when the data does not exist, though?

{code}
            if (components.contains(Component.DATA) && dataFile.length() > 0)
                // everything appears to be in order... moving on.
                continue;

            // missing the DATA file! all components are orphaned
{code}","11/Mar/12 23:59;jbellis;Not sure what you mean -- since data file is renamed last, it's normal (if the process was killed mid-flush) for components to exist without a data file.  But I can't think of a situation that would cause the inverse.  Are we perhaps deleting things asynchronously somewhere?","12/Mar/12 00:11;brandon.williams;I'm not sure how this happened, I tried to repro artificially and wasn't able to.  Originally what happened is I was testing a patch that threw a TON of errors (all time was spent in logging), and after a ctrl-c and restart this happened.

Is it really important to confirm the deletion here?  Being unable to start rather sucks.","30/Mar/12 09:08;slebresne;What's weird is that the INDEX component was clearly found at the beginning of scrubDataDirectories, but didn't existed anymore when we tried the deletion. However, this happens during AbstractDaemon.setup() so I don't think there can be any concurrent process deleting the file. Anyway, I don't thing the delete confirmation is very important but I'm not fond of changing code when we don't understand what's going on. At least as long as nobody have reproduced without using a buggy patch initially.","30/Mar/12 16:00;brandon.williams;FWIW, there has been another case of this reported, but it was on Windows.",02/Apr/12 21:49;jbellis;I don't suppose you have a log file for that case?,"30/Jul/12 21:46;rcoli;Trival repro case for something similar :

1) create file named something-that-looks-like-a-sstable-but-isn't, for example the tablesnap-created filename ""ObfuscatedCF-hd-8813-Statistics.db-listdir.json"", which users of tablesnap are relatively likely to accidentally have in their datadir.

2) stop node.

3) start.

This case seems like an issue with the sstable identifying code, real sstables names don't end with random strings like ""-listdir.json"". :)",30/Jul/12 21:49;jbellis;My mind boggles that you appear to expect Cassandra to cope with arbitrary user-created files in the directories it is supposed to have control over.,"30/Jul/12 22:48;rcoli;I don't especially ""expect"" Cassandra to cope with arbitrary user-created files in the directories it is supposed to have control over. The purpose of my comment was primarily to assist any other operator who might have accidentally created such a file, who would then google the exception and be confused because this ticket was marked no-repro.

However.. the comment for scrubDataDirectories says the following :
""
     * Removes unnecessary files from the cf directory at startup: these include temp files, orphans, zero-length files
     * and compacted sstables. Files that cannot be recognized will be ignored.
""

So it is a goal to ""recognize"" files properly, and to ""ignore"" files that are not ""recognized"" properly.

Further in the code we see..
""
if (!""snapshots"".equals(name) && !""backups"".equals(name) && !name.contains("".json""))
""

Which suggests that had my file not happened to have had suffix .json, it would have been ""recognized"" and at least logged an error about being an invalid file, even if it were not ""ignored,"" I would have had a chance of reading a relevant log message ...

I agree that the practice of creating arbitrary files named like sstables, but with an additional ""-"" in them should be considered hazardous!

But as we can easily ""recognize"" that any file with more ""-"" delimited elements in them than possible are not sstables, I continue to suggest that the user might prefer to discover this before Cassandra has tried and failed to treat such a file as a sstable, and refused to start as a result of trying to scrub the broken sstable. :D","01/Nov/12 06:35;yulinyen;We also see similar error message, but it is not about the 'index'. We are using 1.0.10.

WARN [main] 2012-09-05 08:32:01,804 ColumnFamilyStore.java (line 383) Removing orphans for /opt/ruckuswireless/wsg/db/data/wsg/inventorySummary-hd-91: [Statistics.db, Filter.db, Digest.sha1, Data.db, Index.db]
ERROR [main] 2012-09-05 08:32:01,805 AbstractCassandraDaemon.java (line 373) Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file inventorySummary-hd-91-Digest.sha1
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:388)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:193)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:356)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)

I was wondering if anyone could confirm that this is the same issue.","01/Nov/12 21:51;brandon.williams;It looks similar, what kind of environment was this in?","02/Nov/12 00:50;yulinyen;OS:
Linux TEST1 2.6.32.24 #1 SMP Wed Oct 3 14:51:26 CST 2012 x86_64 GNU/Linux

Hardware:
24 cores, 48G memory, 12G for heap. 

The first time we saw this was after the server was shutdown abruptly (due to the hardware issue). We have not seen the same issue for a while. However, it happened again just a few days ago. Unfortunately, we could not find a consistent way to reproduce it.","06/Nov/12 14:19;brandon.williams;dtestbot managed to randomly reproduce this morning.  It looks like a race between compaction cleanup and forcible shutdown, then startup.  Log attached.","07/Nov/12 01:12;yulinyen;I might be wrong. From the log, it looks like C* is started twice in a row. Is this a possible cause? Anyway to workaround this?","12/Nov/12 21:49;jbellis;That's odd, because here's the code causing that assertion:

{code}
.           File dataFile = new File(desc.filenameFor(Component.DATA));
            if (components.contains(Component.DATA) && dataFile.length() > 0)
                // everything appears to be in order... moving on.
                continue;

            // missing the DATA file! all components are orphaned
            logger.warn(""Removing orphans for {}: {}"", desc, components);
            for (Component component : components)
            {
                FileUtils.deleteWithConfirm(desc.filenameFor(component));
            }
{code}

I must be missing something because these are the possibilities I see:

# .db exists and is non-empty.  we don't try to delete it.
# .db exists and is empty.  we delete it, and do not get a ""file does not exist"" failure
# .db does not exist (is not part of components), so we do not try to delete it
","22/Jan/13 16:47;brandon.williams;Saw this in the wild again.  It was encountered multiple times on the same component set preventing startup, in the following order: CompressionInfo, CompressionInfo, Index, CompressionInfo, CompressionInfo.  After that, it was finally able to startup so we weren't able to see which components existed or did not, but it looks like it does make incremental progress on the deletions with each restart.  I don't want to paper over something without understanding what's going on here, but it's highly annoying and visible when this error is fatal.  We aren't making progress here too quickly, so maybe in the interim we should add more logging around this and log something at ERROR instead of dying.","24/Jan/13 19:06;sgpope;Saw this today on one of our test machine. Windows box with Cassandra 1.1.5. It would be nice to have it log and continue instead of puking. :) On the bright side, it does make progress every time upon startup and eventually starts fine. Haven't been able to reproduce it since. Log:

WARN [main] 2013-01-24 13:01:50,480 ColumnFamilyStore.java (line 393) Removing orphans for C:\Program Files\Quest Software\MessageStats Business Insights\Storage\data\Doradus\MS_1_MessageParticipantPair_Terms\Doradus-MS_1_MessageParticipantPair_Terms-he-31: [Statistics.db, Filter.db, Digest.sha1, Data.db, Index.db]
ERROR [main] 2013-01-24 13:01:50,480 AbstractCassandraDaemon.java (line 406) Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file Doradus-MS_1_MessageParticipantPair_Terms-he-31-Filter.db
                at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:68)
                at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:63)
                at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:398)
                at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:196)
                at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:389)
                at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
WARN [main] 2013-01-24 13:35:32,656 ColumnFamilyStore.java (line 393) Removing orphans for C:\Program Files\Quest Software\MessageStats Business Insights\Storage\data\Doradus\MS_1_MessageParticipantPair_Terms\Doradus-MS_1_MessageParticipantPair_Terms-he-31: [Data.db, Digest.sha1, Filter.db, Index.db]
ERROR [main] 2013-01-24 13:35:32,656 AbstractCassandraDaemon.java (line 406) Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file Doradus-MS_1_MessageParticipantPair_Terms-he-31-Data.db
                at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:68)
                at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:63)
                at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:398)
                at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:196)
                at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:389)
                at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
WARN [main] 2013-01-24 13:35:44,559 ColumnFamilyStore.java (line 393) Removing orphans for C:\Program Files\Quest Software\MessageStats Business Insights\Storage\data\Doradus\MS_1_MessageParticipantPair_Terms\Doradus-MS_1_MessageParticipantPair_Terms-he-31: [Index.db, Digest.sha1, Data.db, Filter.db]
ERROR [main] 2013-01-24 13:35:44,559 AbstractCassandraDaemon.java (line 406) Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file Doradus-MS_1_MessageParticipantPair_Terms-he-31-Data.db
                at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:68)
                at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:63)
                at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:398)
                at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:196)
                at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:389)
                at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
","06/Mar/13 03:50;yulinyen;We saw this issue again on 1.0.10. It seems this happens when the file-name-change after compaction is not completed. The following are the sstables on our file system and the error we saw (the -ea option is removed):

-rw-r--r--   1 byan admin 66305 Mar  5 10:07 indexBytes-hd-111-Data.db
-rw-r--r--   1 byan admin    67 Mar  5 10:07 indexBytes-hd-111-Digest.sha1
-rw-r--r--   1 byan admin  1936 Mar  5 10:07 indexBytes-hd-111-Filter.db
-rw-r--r--   1 byan admin   333 Mar  5 10:07 indexBytes-hd-111-Index.db
-rw-r--r--   1 byan admin  4336 Mar  5 10:07 indexBytes-hd-111-Statistics.db
-rw-r--r--   1 byan admin   240 Mar  5 12:50 indexBytes-hd-112-Data.db
-rw-r--r--   1 byan admin    67 Mar  5 12:50 indexBytes-hd-112-Digest.sha1
-rw-r--r--   1 byan admin    16 Mar  5 12:50 indexBytes-hd-112-Filter.db
-rw-r--r--   1 byan admin    32 Mar  5 12:50 indexBytes-hd-112-Index.db
-rw-r--r--   1 byan admin  4336 Mar  5 12:50 indexBytes-hd-112-Statistics.db
-rw-r--r--   1 byan admin   240 Mar  5 15:18 indexBytes-hd-113-Data.db
-rw-r--r--   1 byan admin    67 Mar  5 15:18 indexBytes-hd-113-Digest.sha1
-rw-r--r--   1 byan admin    16 Mar  5 15:18 indexBytes-hd-113-Filter.db
-rw-r--r--   1 byan admin    32 Mar  5 15:18 indexBytes-hd-113-Index.db
-rw-r--r--   1 byan admin  4336 Mar  5 15:18 indexBytes-hd-113-Statistics.db
-rw-r--r--   1 byan admin   120 Mar  5 15:26 indexBytes-hd-114-Data.db
-rw-r--r--   1 byan admin    67 Mar  5 15:26 indexBytes-hd-114-Digest.sha1
-rw-r--r--   1 byan admin    16 Mar  5 15:26 indexBytes-hd-114-Filter.db
-rw-r--r--   1 byan admin    16 Mar  5 15:26 indexBytes-hd-114-Index.db
-rw-r--r--   1 byan admin  4336 Mar  5 15:26 indexBytes-hd-114-Statistics.db
-rw-r--r--   1 byan admin   333 Mar  5 15:26 indexBytes-hd-115-Index.db
-rw-r--r--   1 byan admin 66305 Mar  5 15:26 indexBytes-tmp-hd-115-Data.db
-rw-r--r--   1 byan admin    67 Mar  5 15:26 indexBytes-tmp-hd-115-Digest.sha1
-rw-r--r--   1 byan admin  1936 Mar  5 15:26 indexBytes-tmp-hd-115-Filter.db
 
WARN [main] 2013-03-05 10:43:22,494 ColumnFamilyStore.java (line 383) Removing orphans for /opt/test/tt/indexBytes-hd-115: [Digest.sha1, Index.db, Filter.db, Data.db]
ERROR [main] 2013-03-05 10:43:22,494 AbstractCassandraDaemon.java (line 373) Exception encountered during startup
java.io.IOError: java.io.IOException: Failed to delete /opt/test/tt/indexBytes-hd-115-Digest.sha1
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:392)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:193)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:356)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
Caused by: java.io.IOException: Failed to delete /opt/ruckuswireless/wsg/db/data/wsg/indexBytes-hd-115-Digest.sha1
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:388)
	... 3 more


It seems the ""Descriptor"" does not take the ""temporary"" flag into account in the ""hash"" and ""equals"" function. I was wondering if anyone could confirm this.",11/Mar/13 23:15;jbellis;Committed Boris's patch from the dev list (attached).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig_cassandra script errors when running against pig 0.9.1 tar ball because there are multiple jars.,CASSANDRA-3320,12525954,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,boneill,boneill,boneill,05/Oct/11 20:07,12/Mar/19 14:02,13/Mar/19 22:26,12/Oct/11 16:12,0.8.8,,,,,,0,,,,,"The pig_cassandra script in contrib/pig/bin assumes there is only one pig jar file in $PIG_HOME.  However, the latest release of pig 0.9.1 has two jar files: one for hadoop and one without hadoop.  See below:

bone@zen:~/tools/pig-0.9.1-> ls -al *.jar
-rw-r--r--  1 bone  staff   5130595 Sep 29 18:55 pig-0.9.1-withouthadoop.jar
-rw-r--r--  1 bone  staff  12430153 Sep 29 18:55 pig-0.9.1.jar


This breaks the shell script with:
bin/pig_cassandra: line 42: [: /Users/bone/tools/pig/pig-0.9.1-withouthadoop.jar: binary operator expected
Unrecognized option: -x

Attached is a patch for the shell script that takes the last jar file listed in the directory. This fixes the problem.  I also add an ""echo"" to notify the user which jar file they are using. 
",Running on mac os x.  PIG_HOME set to a fresh download of pig 0.9.1.,,,,,,,,,,,,,,,05/Oct/11 20:09;boneill;trunk-3320.txt;https://issues.apache.org/jira/secure/attachment/12497880/trunk-3320.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-12 16:12:55.843,,,no_permission,,,,,,,,,,,,46578,,,Wed Oct 12 17:19:43 UTC 2011,,,,,,0|i0gihb:,94424,brandon.williams,brandon.williams,,,,,,,,,05/Oct/11 20:09;boneill;Patch,"10/Oct/11 16:58;boneill;Also, I just noticed when running on my linux under bash I get the following error:
[boneill@boneill-lin] pig $ bin/pig_cassandra -x local example-script.pig 
Unrecognized option: -x
Could not create the Java virtual machine.

When I echo the $PIG_JAR variable, I get:
$PIG_JAR = /home/boneill/tools/pig/pig*.jar

Thus, the jar file resolution isn't working.  This same patch fixes this on linux/bash as well.
","12/Oct/11 16:12;brandon.williams;Committed, thanks.","12/Oct/11 17:19;hudson;Integrated in Cassandra-0.8 #368 (See [https://builds.apache.org/job/Cassandra-0.8/368/])
    Fix pig script to only pick up a single pig jar.
Patch by Brian Oneill, reviewed by brandonwilliams for CASSANDRA-3320

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1182456
Files : 
* /cassandra/branches/cassandra-0.8/contrib/pig/bin/pig_cassandra
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction degrades key cache stats,CASSANDRA-3325,12526023,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,frousseau,frousseau,frousseau,06/Oct/11 10:29,12/Mar/19 14:02,13/Mar/19 22:26,06/Oct/11 16:05,1.0.0,,,,,,0,compaction,,,,"When ""compaction_preheat_key_cache"" is set to true, then during compaction, it keep tracks of cached keys to to re-cache their new position.
It does this by calling  the following method on every key of the compacted sstable :
sstable.getCachedPosition(row.key)
which also update cache stats, thus lowering hit rate

Below is an attached patch allowing to know if the key is cached, but without updating the stats.",,,,,,,,,,,,,,,,06/Oct/11 10:33;frousseau;001-CASSANDRA-3325.patch;https://issues.apache.org/jira/secure/attachment/12497972/001-CASSANDRA-3325.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-06 16:05:04.557,,,no_permission,,,,,,,,,,,,49580,,,Thu Oct 06 16:05:04 UTC 2011,,,,,,0|i0gijb:,94433,jbellis,jbellis,,,,,,,,,"06/Oct/11 16:05;jbellis;committed to 1.0.0, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"can't use RackInferringSnitch and CQL JDBC's ""CREATE KEYSPACE"" with NetworkTopologyStrategy",CASSANDRA-3239,12524230,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,machenmusik@comcast.net,machenmusik@comcast.net,22/Sep/11 03:24,12/Mar/19 14:02,13/Mar/19 22:26,27/Sep/11 22:15,0.8.7,,,Legacy/CQL,,,0,,,,,"If using the CQL JDBC driver, there's a problem with using RackInferringSnitch

1. With RackInferringSnitch, the datacenter names are numeric
2. With CQL and NetworkTopologyStrategy, the data center replicas are specified as strategy_options:<dc-name>=<#-of-replicas>
3. Using a number for <dc-name> fails
4. Using a quoted number for <dc-name> fails
",,,,,,,,,,,,,,,,27/Sep/11 16:28;xedin;CASSANDRA-3239.patch;https://issues.apache.org/jira/secure/attachment/12496761/CASSANDRA-3239.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-22 03:54:10.558,,,no_permission,,,,,,,,,,,,3529,,,Tue Sep 27 22:15:43 UTC 2011,,,,,,0|i0ghhz:,94265,urandom,urandom,,,,,,,,,22/Sep/11 03:54;jbellis;Note: if the fix is complicated let's target it for 1.0.0,"22/Sep/11 17:57;jbellis;What is ""compident?"" I'm pretty sure we don't want to allow int-as-ident in most places.","22/Sep/11 18:41;xedin;If I understand everything correctly it means ""composite identifier"" and it is only used by ""CREATE SCHEMA"" statement.",27/Sep/11 16:28;xedin;rebased with the latest cassandra-0.8 branch and COMPIDENT changed to support integers only at the second part.,27/Sep/11 20:01;urandom;+1,27/Sep/11 22:15;urandom;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleSnitch.compareEndpoints doesn't respect the intent of the snitch,CASSANDRA-3262,12524827,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,27/Sep/11 09:48,12/Mar/19 14:02,13/Mar/19 22:26,28/Sep/11 04:17,0.8.7,1.0.0,,,,,0,,,,,"SimpleSnitch is supposed to not sort the input addresses, thus respecting the order of the partitioner. However, it's compareEndpoints instead uses IP addresses comparison. Note that this matter when the dynamicSnitch fall back to the wrapped snitch since it uses the compareEndpoint method then.",,,,,,,,,,,,,,,,27/Sep/11 10:10;slebresne;3262.patch;https://issues.apache.org/jira/secure/attachment/12496657/3262.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-27 12:55:47.359,,,no_permission,,,,,,,,,,,,17916,,,Wed Sep 28 05:15:25 UTC 2011,,,,,,0|i0ghrz:,94310,jbellis,jbellis,,,,,,,,,27/Sep/11 10:10;slebresne;Note that I refactor a bit the code because it felt weird to have compareEndpoint being concrete in AbstractEndpointSnitch but sortByProximity being abstract. It seems more natural to have compareEndpoint being the abstract method and sortByProximity being defined from it in AES.,"27/Sep/11 12:55;jbellis;The reason for this behavior (CASSANDRA-1314) is because for best performance, normal reads want exactly the opposite behavior of counters: we want to direct reads to the same replica so that the cache stays hot.  Put another way, if each replica is serving a distinct range then you get (replica count) times as much cache memory [with CL.ONE and RR off) than if each is getting the full range of requests from different coordinators.","27/Sep/11 13:56;slebresne;I think there is a misunderstanding. I agree with the pinning of replicas. The problem is that the current implementation of AbstractEndpointSnitch.compareEndpoints is:
{noformat}
public int compareEndpoints(InetAddress target, InetAddress a1, InetAddress a2)
{
    return a1.getHostAddress().compareTo(a2.getHostAddress());
}
{noformat}
If you sort a list of hosts using that, you will always return the host that have the ""smallest"" IP. In other words, in a 3 nodes cluster with RF=3, all and every read will hit the exact same node.

What makes it kind of work today is that this compareEndpoints() method is barely used. It's used only in the case where the dynamic snitch have no scores for the endpoints. Otherwise, it's sortByProximity that is used (which doesn't rely on compareEndpoints -- this is confusing and my patch corrects it). And sortByProximity does *the right thing*, i.e, it doesn't sort the input list since it is supposed to be in token order (which effectively pin one range to every replica).

So the patch here proposes two things:
  * If fixes the compareEndpoints method: comparing IP addresses is not a good idea.
  * It refactors the code to make sortByProximity use compareEndpoint, to having getting in that situation again.",28/Sep/11 04:17;jbellis;lgtm.  made some minor updates to comments and committed.,"28/Sep/11 05:15;hudson;Integrated in Cassandra-0.8 #346 (See [https://builds.apache.org/job/Cassandra-0.8/346/])
    Keep SimpleSnitch proximity ordering unchanged from what the Strategy generates, as intended
patch by slebresne; reviewed by jbellis for CASSANDRA-3262

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1176712
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/AbstractEndpointSnitch.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/AbstractNetworkTopologySnitch.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/SimpleSnitch.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EncryptionOptions should be instantiated,CASSANDRA-3489,12531262,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,13/Nov/11 16:32,12/Mar/19 14:02,13/Mar/19 22:26,01/Dec/11 20:37,1.0.6,,,,,,0,,,,,"As the title says, otherwise you get an NPE when the options are missing from the yaml.  It's included in my second patch on CASSANDRA-3045 and is a one line fix.",,,,,,,,,,,,,,,,28/Nov/11 22:10;brandon.williams;3489.txt;https://issues.apache.org/jira/secure/attachment/12505406/3489.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-13 16:41:30.58,,,no_permission,,,,,,,,,,,,216999,,,Thu Dec 01 20:37:56 UTC 2011,,,,,,0|i0gkjb:,94757,,,,,,,,,,,"13/Nov/11 16:41;jbellis;There's a bunch of ""if encryption options is null then ignore it"" special cases already, if you're going to instantiate a default instead then let's get rid of those.

May also need to be applied to 0.8 unless aforesaid special cases cover everything.",28/Nov/11 22:10;brandon.williams;I could only find the special case added the first time we fixed this back in 0.8 for CASSANDRA-3007.  Attached patch removes that and instantiates the default instead.,"01/Dec/11 20:33;jbellis;Hmm.  I thought the other place was OTC, but that's going to NPE in the current code base.  So +1 for this patch.","01/Dec/11 20:34;jbellis;(Checked, and 0.8 OTC does have the null check.  So we're good there.)",01/Dec/11 20:37;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counter decrements require a space around the minus sign but not around the plus sign,CASSANDRA-3418,12529307,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,kreynolds,kreynolds,28/Oct/11 19:45,12/Mar/19 14:02,13/Mar/19 22:26,11/Nov/11 16:50,1.0.3,,,,,,0,cql,,,,"UPDATE validation_cf_counter SET test=test+1 WHERE id='test_key' => Success
UPDATE validation_cf_counter SET test=test + 1 WHERE id='test_key' => Success
UPDATE validation_cf_counter SET test=test - 1 WHERE id='test_key' => Success
UPDATE validation_cf_counter SET test=test-1 WHERE id='test_key' => Failure (line 1:38 no viable alternative at input 'test')

",,,,,,,,,,,,,,,,15/Nov/11 20:29;xedin;CASSANDRA-3418-fix.patch;https://issues.apache.org/jira/secure/attachment/12503788/CASSANDRA-3418-fix.patch,11/Nov/11 14:54;xedin;CASSANDRA-3418.patch;https://issues.apache.org/jira/secure/attachment/12503375/CASSANDRA-3418.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-11 16:25:16.693,,,no_permission,,,,,,,,,,,,215167,,,Wed Nov 16 18:50:51 UTC 2011,,,,,,0|i0gjnj:,94614,jbellis,jbellis,,,,,,,,,"11/Nov/11 16:25;jbellis;+1

time to add QueryProcessorTest?",11/Nov/11 16:50;xedin;Committed. It's not related to QueryProcessor tho,"14/Nov/11 20:12;thepaul;This doesn't do the right thing when minusing a negative value, like:

{noformat}
UPDATE validation_cf_counter SET test = test - -1 WHERE id='test_key';
{noformat}

It subtracts 1, instead of adding 1.","14/Nov/11 20:27;jbellis;bq. It's not related to QueryProcessor tho

But that's the simplest way to add CQL tests I would think?  Open to other ideas.","15/Nov/11 20:29;xedin;here is the patch that fixes problem that Paul found. 

@Jonathan: I agree, we probably should do the same thing for CQL as we did for CLI.",16/Nov/11 18:02;thepaul;+1 for the fix,16/Nov/11 18:50;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
examples/hadoop_word_count reducer to cassandra doesn't output into the output_words cf,CASSANDRA-2727,12508874,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,jeromatron,jeromatron,31/May/11 21:41,12/Mar/19 14:02,13/Mar/19 22:26,17/Jun/11 15:26,0.8.2,,,,,,0,hadoop,,,,"I tried the examples/hadoop_word_count example and could output to the filesystem but when I output to cassandra (the default), nothing shows up in output_words.  I can output to cassandra using pig so I think the problem is isolated to this example.",,,,,,,,,,,,,,,,17/Jun/11 15:12;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2727-fix-for-word-count-reducer.txt;https://issues.apache.org/jira/secure/attachment/12482942/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2727-fix-for-word-count-reducer.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-17 15:14:11.531,,,no_permission,,,,,,,,,,,,20793,,,Fri Jun 17 16:20:10 UTC 2011,,,,,,0|i0gd3b:,93551,,,,,,,,,,,17/Jun/11 15:14;jbellis;+1,"17/Jun/11 16:20;hudson;Integrated in Cassandra-0.8 #174 (See [https://builds.apache.org/job/Cassandra-0.8/174/])
    fix cassandra reducer example

Patch by tjake; reviewed by jbellis for CASSANDRA-2727

jake : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1136910
Files : 
* /cassandra/branches/cassandra-0.8/examples/hadoop_word_count/src/WordCount.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing partitioner causes interval tree build failure before the change can be detected,CASSANDRA-3407,12529058,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,apache.zli,apache.zli,27/Oct/11 14:33,12/Mar/19 14:02,13/Mar/19 22:26,16/Nov/11 22:09,1.0.4,,,,,,0,,,,,"After installed 1.0.0 and changed config file cassandra.yaml, restart cassandra and got exception,

INFO 22:25:37,727 Opening /srv/opt/cassandra8/data/system/IndexInfo-g-121 (5428 bytes)
ERROR 22:25:37,753 Exception encountered during startup_type: 0},
java.lang.StackOverflowError, validation_class: UTF8Type, index_type: 0},
       at java.math.BigInteger.compareMagnitude(BigInteger.java:2477)
       at java.math.BigInteger.compareTo(BigInteger.java:2463)type: 0},
       at org.apache.cassandra.dht.BigIntegerToken.compareTo(BigIntegerToken.java:39)
       at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:83)
       at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)
       at java.util.Arrays.mergeSort(Arrays.java:1144)dex_type: 0},
       at java.util.Arrays.sort(Arrays.java:1079)dex_type: 0},
       at java.util.Collections.sort(Collections.java:117)},
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.findMinMedianMax(IntervalNode.java:102)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:43)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
.....

       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:38)
       at org.apache.cassandra.db.DataTracker$View.buildIntervalTree(DataTracker.java:522)
       at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:547)
       at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:268)
       at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
       at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:216)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:315)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:285)
       at org.apache.cassandra.db.Table.initCf(Table.java:372)
       at org.apache.cassandra.db.Table.<init>(Table.java:320)
       at org.apache.cassandra.db.Table.open(Table.java:121)
       at org.apache.cassandra.db.Table.open(Table.java:104)
       at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:215)
       at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:150)
       at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
       at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Exception encountered during startup: null
",Linux  2.6.18,,,,,,,,,,,,,,CASSANDRA-3470,28/Oct/11 21:46;thepaul;3407-assert-intervals.patch;https://issues.apache.org/jira/secure/attachment/12501376/3407-assert-intervals.patch,11/Nov/11 01:52;yukim;3407-v2.txt;https://issues.apache.org/jira/secure/attachment/12503322/3407-v2.txt,08/Nov/11 03:21;yukim;3407.txt;https://issues.apache.org/jira/secure/attachment/12502876/3407.txt,27/Oct/11 14:34;apache.zli;exception1.txt;https://issues.apache.org/jira/secure/attachment/12501096/exception1.txt,28/Oct/11 22:09;apache.zli;system.log;https://issues.apache.org/jira/secure/attachment/12501383/system.log,27/Oct/11 19:38;apache.zli;system.log;https://issues.apache.org/jira/secure/attachment/12501141/system.log,29/Oct/11 00:54;apache.zli;system.tar.gz;https://issues.apache.org/jira/secure/attachment/12501416/system.tar.gz,,,,,7.0,,,,,,,,,,,,,,,,,,,2011-10-27 15:00:19.841,,,no_permission,,,,,,,,,,,,214918,,,Wed Nov 16 22:09:01 UTC 2011,,,,,,0|i0gjif:,94591,jbellis,jbellis,,,,,,,,,27/Oct/11 14:34;apache.zli;Full logs,27/Oct/11 15:00;jbellis;Superficially it looks like we might want to change that IntervalTree method to a loop instead of recursion,"27/Oct/11 16:16;slebresne;On my machine, I'm able to do 1156 recursive calls with -Xss128k (which we use by default) before getting a StackOverflowException. Is that expected that we create such huge intervalTree ?","27/Oct/11 16:23;jbellis;Does our intervaltree self-balance or will it degenerate if nodes are added in the ""wrong"" order, e.g. sorted order in a standard binary tree?","27/Oct/11 17:03;thepaul;They're immutable and recreated as necessary with new DataTracker.View object, so ongoing balancing isn't an issue. It looks like the worst case depth (with the right arrangement of intervals; insertion order doesn't matter) is equal to the number of SSTableReaders in a View.",27/Oct/11 17:06;jbellis;Hmm.  We could have way more than 1156 SSTables in a view very easily.,"27/Oct/11 17:18;thepaul;To get that, you'd need all the SSTable intervals to be wholly contained by the next larger one, and none intersecting the center point of any larger intervals. It seems unlikely, but maybe there are patterns of usage which could have that effect.

Either way, it probably is worth changing both the tree creation and search methods to use iteration instead of recursion.","27/Oct/11 17:23;jbellis;In the meantime, Zhong should be able to just use a larger stack as a workaround.","27/Oct/11 17:38;slebresne;bq. It seems unlikely, but maybe there are patterns of usage which could have that effect

It sound to me like we don't understand what is going on.

Besides, if you look at the full log, the exception is triggered on the IndexInfo cf. It also happens at the very startup after he has loaded only 3 sstables (hence DataTracker likely have 3 sstables, not a shit load). I'm pretty sure it's a bug leading to infinite recursive calls, not a stack size problem.",27/Oct/11 18:13;jbellis;Good point.,"27/Oct/11 18:24;jbellis;I added debug logging for the intervals in the tree to the 1.0 branch in r1189921; if Zhong can build with that and give us the log, that would help a lot.","27/Oct/11 19:08;apache.zli;
I will try to build it and test it.

 		 	   		  
",27/Oct/11 19:38;apache.zli;I turn on debug and this is full logs,"28/Oct/11 21:02;thepaul;So here's the culprit:

bq. Interval(DecoratedKey(145657158669597754039818762792796113368, 6b657973706163654c42534441544150524f445553), DecoratedKey(130897858884062407407634012854175581156, 6b65797370616365544553544441544150524f445553))

Notice that the min of that Interval (~1.45e38) is greater than the max (~1.30e38). The IntervalTree logic is wholly incapable of dealing with that, so we end up in an infinite loop. The only way I can find that this could happen is from an invalid SSTableReader object. I'll make a patch with some extra asserts; if you can run with that, Zhong, maybe we can at least see where that invalid data is coming from.","28/Oct/11 21:46;thepaul;Zhong, please try building and running that same setup with this patch.",28/Oct/11 22:09;apache.zli;patch applied and log attached.,"28/Oct/11 22:15;thepaul;Hmm, the patch doesn't seem to have taken effect. Are you running with java's -ea option to enable assertions?","30/Oct/11 03:24;thepaul;I haven't been able to reproduce this still, even with the IndexInfo files. I guess let's figure out where things are going differently. You're using the ByteOrderedPartitioner, it looks like? And can you confirm, you're running a single node of Cassandra 1.0.0, with all its data/commitlog/saved_caches directories empty except for data/system/IndexInfo*.db, and you get this exception?","31/Oct/11 15:38;apache.zli;Oh. Forgot to change the partitioner setting, it is default ""RandomPartitioner"". I should use OrderPreservingPartitioner.

After I changed to OrderPreservingPartitioner, everything works fine.

Sorry about this.
","31/Oct/11 15:56;jbellis;Actually we do have a problem, since we're supposed to check the partitioner setting in system table and raise an error there.  But it looks like we run into this problem first...

If we only built the interval tree for LCS then that would take care of it for the size-tiered case.  Any better ideas?",31/Oct/11 15:57;jbellis;What if we stored the partitioner in the metadata/statistics component so we don't depend on that not changing to be able to read the data files?,31/Oct/11 16:02;jbellis;That approach would also make our partitioner-change detection more robust against manually copying data files into a cluster with a different partitioner.,"08/Nov/11 02:55;yukim;Patch attached for 1.0 branch.
It writes partitioner class name to sstable metadata and when opening sstable, checks recorded partitioner against node's.

Since opening sstable is done in parallel, C* just skips failed ones regardless of whether its system keyspace or not.

And note that I did not modify the descriptor version(""h"") here. If updating version is desirable, I'll update the patch.","08/Nov/11 03:21;jbellis;bq. I did not modify the descriptor version(""h"") here

We probably should, but see CASSANDRA-3470.",11/Nov/11 01:52;yukim;Rebased and modified original patch to use minor versioning of sstable metadata.,"16/Nov/11 22:09;jbellis;Committed w/ update to throw RuntimeException instead of using assert, and remove of the old SystemTable-based partitioner checking.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unnecessary locking in hint path due to InetAddress.getLocalhost(),CASSANDRA-3387,12527830,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yangyangyyy,yangyangyyy,yangyangyyy,19/Oct/11 20:59,12/Mar/19 14:02,13/Mar/19 22:26,20/Oct/11 16:36,1.0.1,,,,,,0,,,,,"in tests we found locking contention also due to the following




Stacks at 01:49:19 PM (uptime 10:54)


MutationStage:62 [BLOCKED] CPU time: 0:06
java.net.InetAddress.getLocalHost()
org.apache.cassandra.utils.UUIDGen.getClockSeqAndNode()
org.apache.cassandra.utils.UUIDGen.createTimeUUIDBytes(long)
org.apache.cassandra.utils.UUIDGen.getTimeUUIDBytes()
org.apache.cassandra.db.RowMutation.hintFor(RowMutation, ByteBuffer)
org.apache.cassandra.service.StorageProxy$4.run()
java.lang.Thread.run()





we can easily change every getLocalHost() call to use a cached value",,,,,,,,,,,,,,,,19/Oct/11 21:03;yangyangyyy;3387.diff;https://issues.apache.org/jira/secure/attachment/12499757/3387.diff,19/Oct/11 21:25;yangyangyyy;3387_v2.diff;https://issues.apache.org/jira/secure/attachment/12499764/3387_v2.diff,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-19 21:12:42.193,,,no_permission,,,,,,,,,,,,92085,,,Thu Oct 20 16:36:50 UTC 2011,,,,,,0|i0gj9z:,94553,jbellis,jbellis,,,,,,,,,"19/Oct/11 21:03;yangyangyyy;use cached value , do not call getLocalhost() every time when we try to construct a hint
",19/Oct/11 21:12;jbellis;Why not just use FBUtilities.getLocalAddress?,"19/Oct/11 21:24;yangyangyyy;oh, I didn't know that one.

yes, FBUtilities.getLocalAddress() is better","19/Oct/11 21:25;yangyangyyy;version 2 , using fb utils","20/Oct/11 16:36;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageServiceMBean is missing a getCompactionThroughputMbPerSec() method,CASSANDRA-3117,12521008,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,01/Sep/11 02:47,12/Mar/19 14:02,13/Mar/19 22:26,01/Sep/11 16:25,0.8.5,,,Legacy/Tools,,,0,,,,,"Without a getter, you can assign a new value but not query the existing one (which is strange).",,,,,,,,,,,,,,,,01/Sep/11 15:26;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3117-add-missing-mbean-method.txt;https://issues.apache.org/jira/secure/attachment/12492612/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3117-add-missing-mbean-method.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-01 15:29:02.708,,,no_permission,,,,,,,,,,,,4082,,,Fri Sep 02 07:22:59 UTC 2011,,,,,,0|i0gfzr:,94021,,,,,,,,,,,01/Sep/11 15:29;jbellis;+1,01/Sep/11 16:25;urandom;committed.,"02/Sep/11 07:22;hudson;Integrated in Cassandra-0.8 #310 (See [https://builds.apache.org/job/Cassandra-0.8/310/])
    add missing mbean method

Patch by eevans; reviewed by jbellis for CASSANDRA-3117

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1164127
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageServiceMBean.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair still streams unnecessary sstables,CASSANDRA-3345,12526654,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,11/Oct/11 13:07,12/Mar/19 14:02,13/Mar/19 22:26,11/Oct/11 15:12,1.0.0,,,,,,0,repair,,,,"Through rebases, CASSANDRA-2610 unfortunately got committed with the use of the wrong streaming method, the one that stream all the sstables of the keyspace.",,,,,,,,,,,,,,,,11/Oct/11 13:08;slebresne;3345.patch;https://issues.apache.org/jira/secure/attachment/12498590/3345.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-11 14:56:59.091,,,no_permission,,,,,,,,,,,,59123,,,Tue Oct 11 15:12:04 UTC 2011,,,,,,0|i0girr:,94471,brandon.williams,brandon.williams,,,,,,,,,"11/Oct/11 13:08;slebresne;That sucks, sorry. Patch attached.",11/Oct/11 14:56;brandon.williams;+1,11/Oct/11 15:12;slebresne;Committed for the 1.0.0 reroll,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra can't find jamm on startup,CASSANDRA-2647,12507147,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,synchrom,synchrom,13/May/11 16:47,12/Mar/19 14:02,13/Mar/19 22:26,19/May/11 18:21,0.8.0,,,Packaging,,,0,,,,,"I installed the Debian package (from http://www.apache.org/dist/cassandra/debian unstable) of Cassandra 0.8beta2 on Ubuntu 10.04 with the sun jdk over a working copy of 0.7.2. It broke on restart.
On startup it gives this:
{code}
Error occurred during initialization of VM
agent library failed to init: instrument
Error opening zip file or JAR manifest missing : /lib/jamm-0.2.2.jar
{code}
/etc/cassandra/cassandra-env.sh contains this:
{code}
# add the jamm javaagent
check_openjdk=$(java -version 2>&1 | awk '{if (NR == 2) {print $1}}')
if [ ""$check_openjdk"" != ""OpenJDK"" ]
then
    JVM_OPTS=""$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/jamm-0.2.2.jar""
fi
{code}
By default, CASSANDRA_HOME is not set, so it's looking in /lib for this jar. It seems CASSANDRA_HOME should be set to /usr/share/cassandra, since that's where jamm-0.2.2.jar is installed, but that means the path is still wrong.

I set CASSANDRA_HOME to /usr/share/cassandra and changed the JVM_OPTS line to this:
{code}
    JVM_OPTS=""$JVM_OPTS -javaagent:$CASSANDRA_HOME/jamm-0.2.2.jar""
{code}

and then cassandra started ok.

Is this a bug or did I miss something?

I also noticed that this line appears to be the only use of CASSANDRA_HOME.",Ubuntu 10.04 Lucid,,,,,,,,,,,,,,,13/May/11 22:13;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2647-move-jars-under-lib-to-satisfy-hard-cod.txt;https://issues.apache.org/jira/secure/attachment/12479180/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2647-move-jars-under-lib-to-satisfy-hard-cod.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-13 19:51:18.747,,,no_permission,,,,,,,,,,,,20750,,,Tue May 24 19:54:21 UTC 2011,,,,,,0|i0gcm7:,93474,thepaul,thepaul,,,,,,,,,13/May/11 19:51;brandon.williams;CASSANDRA_HOME should be getting set in cassandra.in.sh which is the first thing sourced by bin/cassandra and /etc/init.d/cassandra,"13/May/11 21:01;synchrom;Thanks for that. I think I'd normally be looking for /etc/defaults/cassandra to set that kind of thing. Also, the supplied cassandra.in.sh does not set CASSANDRA_HOME, not even commented-out. It might be nice to mention that file in a comment in /etc/cassandra/cassandra.yaml",13/May/11 22:15;urandom;the attached (untested )patch should deal with this (it's one way of dealing with it anyway).,"16/May/11 08:39;synchrom;Not tested that either, but it looks like it would make cassandra work 'out of the box' on Debian/Ubuntu, fixing both my issues. Thanks.","16/May/11 17:29;thepaul;Tested and verified Eric's patch, both via initscript and invocation through /usr/sbin/cassandra; the right path to jamm (and all the other lib jars) gets set correctly. +1 for the patch.

I think ideally the rest of the initscript would also make use of $CASSANDRA_HOME (like when specifying the location of the jars to add to the classpath), but since the deb isn't expected to support a change to $CASSANDRA_HOME, it's fine as is.","16/May/11 19:27;jtrav;Forgive the awfully dumb question - I am a new user trying to get Cassandra running (for the first time) and ran into this issue as well.

What's the preferred method for applying this patch to the code base?  Running Natty x86_64 server.

Thanks in advance.","16/May/11 19:43;thepaul;bq. What's the preferred method for applying this patch to the code base? Running Natty x86_64 server.

Joe- I would say your options are:

1. Apply the patch to a source checkout from the 0.8.0-beta2 tag (using ""patch -p1 < patch.txt"" from the toplevel directory), then build your own custom .debs (using, maybe, ""dpkg-buildpackage -us -uc"").

2. Just change this line in /etc/cassandra/cassandra-env.sh:

{code}
    JVM_OPTS=""$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/jamm-0.2.2.jar""
{code}

to

{code}
    JVM_OPTS=""$JVM_OPTS -javaagent:/usr/share/cassandra/jamm-0.2.2.jar""
{code}

That solves the same problem in a different way. So when you upgrade to an official package with the real fix, you will want to change /etc/cassandra/cassandra-env.sh back to the way it was.

3. Just wait for 0.8.0 beta3 to be released and use that","16/May/11 19:46;jtrav;Thanks for the quick response, Paul.  I've taken approach #2 for now, and will loop back when I upgrade.  Much appreciated.",19/May/11 18:21;jbellis;committed,"24/May/11 11:19;synchrom;I just noticed this [on the wiki|http://wiki.apache.org/cassandra/RunningCassandra]:
{quote}
Startup options should be configured in /etc/default/cassandra (not /usr/share/cassandra/cassandra.in.sh) on Debian systems.
{quote}","24/May/11 17:34;thepaul;bq. Startup options should be configured in /etc/default/cassandra (not /usr/share/cassandra/cassandra.in.sh) on Debian systems.

That's correct, dpkg won't track changes to config files outside of /etc without special treatment. Is something wrong with that?",24/May/11 19:20;synchrom;It was just that this bug's current patch does the opposite. I mentioned earlier that I'd expect things like CASSANDRA_HOME to be set in /etc/defaults/cassandra rather than somewhere in /usr.,"24/May/11 19:54;thepaul;bq. It was just that this bug's current patch does the opposite. I mentioned earlier that I'd expect things like CASSANDRA_HOME to be set in /etc/defaults/cassandra rather than somewhere in /usr.

CASSANDRA_HOME can be overridden by the user in /etc/cassandra/cassandra-env.sh; it's sourced after CASSANDRA_HOME is set to a default in both /etc/init.d/cassandra and /usr/sbin/cassandra .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make cqlsh look for a suitable python version,CASSANDRA-3457,12530367,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,04/Nov/11 19:39,12/Mar/19 14:02,13/Mar/19 22:26,02/Dec/11 03:10,1.0.6,,,Legacy/Tools,,,0,cqlsh,,,,"On RHEL 5, which I guess we still want to support, the default ""python"" in the path is still 2.4. cqlsh does use a fair number of python features introduced in 2.5, like collections.defaultdict, functools.partial, generators. We can require RHEL 5 users to install a later python from EPEL, but we'd have to call it as 'python2.5', or 'python2.6', etc.

So rather than take the time to vet everything against python2.4, we may want to make a wrapper script for cqlsh that checks for the existence of python2.7, 2.6, and 2.5, and calls the appropriate one to run the real cqlsh.",,,,,,,,,,,,,,,,28/Nov/11 18:37;thepaul;3457.patch.txt;https://issues.apache.org/jira/secure/attachment/12505366/3457.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-28 18:46:17.818,,,no_permission,,,,,,,,,,,,216105,,,Fri Dec 02 03:10:31 UTC 2011,,,,,,0|i0gk53:,94693,jbellis,jbellis,,,,,,,,,17/Nov/11 17:35;thepaul;Just a note for future reference- the opinion has been expressed that requiring EPEL for cassandra rpms may not be an option. Might need to remove the 2.5 features from cqlsh and test it along with the python driver on 2.4.,"28/Nov/11 18:46;jbellis;{noformat}
for pyver in 2.6 2.7 2.5
{noformat}

Why that particular ordering?","28/Nov/11 19:03;thepaul;bq. Why that particular ordering?

No strong reason. 2.6 is just a little more likely to be found than 2.7, over our expected set of install targets, and both 2.6 and 2.7 are little bits faster than 2.5.

Fine with changing to something else.","28/Nov/11 19:11;tjake;tested on centos 6 which used 2.4 by default, with this patch it finds the right version so +1","28/Nov/11 19:20;thepaul;One minor edit: if we change the shebang from {{\#\!/bin/bash}} to {{\#\!/bin/sh}}, it will work for Solaris too. none of the code actually needs bash.","02/Dec/11 03:10;jbellis;committed, with bin/sh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkRecordWriter shouldn't stream any empty data/index files that might be created at end of flush,CASSANDRA-3946,12543638,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,lenn0x,lenn0x,22/Feb/12 08:15,12/Mar/19 14:02,13/Mar/19 22:26,17/Apr/12 21:27,1.0.10,1.1.0,,,,,0,,,,,"If by chance, we flush sstables during BulkRecordWriter (we have seen it happen), I want to make sure we don't try to stream them.",,,,,,,,,,,,,,,,17/Apr/12 19:45;yukim;0001-Abort-SSTableWriter-when-exception-occured.patch;https://issues.apache.org/jira/secure/attachment/12523009/0001-Abort-SSTableWriter-when-exception-occured.patch,22/Feb/12 08:24;lenn0x;0001-CASSANDRA-3946-BulkRecordWriter-shouldn-t-stream-any.patch;https://issues.apache.org/jira/secure/attachment/12515552/0001-CASSANDRA-3946-BulkRecordWriter-shouldn-t-stream-any.patch,17/Apr/12 20:24;yukim;3946-1.0.txt;https://issues.apache.org/jira/secure/attachment/12523016/3946-1.0.txt,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-02-22 14:49:11.267,,,no_permission,,,,,,,,,,,,228877,,,Tue Apr 17 21:27:20 UTC 2012,,,,,,0|i0gq4n:,95663,jbellis,jbellis,,,,,,,,,22/Feb/12 14:49;jbellis;I'm not sure I understand -- is it trying to stream a sstable that's in the process of being written?  Or did you actually get zero-length files created?  The latter is Not Supposed To Happen.,04/Apr/12 18:19;jbellis;Sounds like the right fix here is to make BRW not output zero-length sstables.,04/Apr/12 18:22;brandon.williams;I'm not convinced that having the loader skip empty files is right yet.  Why are empty files being created?,"13/Apr/12 21:08;jbellis;Yuki, does anything look suspicious in BulkRecordWriter for zero-length file creation?",13/Apr/12 21:20;brandon.williams;I suspect if there is a problem here it's actually in UnsortedSimpleSSTableWriter.,"17/Apr/12 19:45;yukim;The only way bulk writer writes 0 size sstable is when bad thing happens during writing.
Attached patch makes sure writer get aborted so that incomplete files are removed when exception happend during writes. In order to do this, I have to let SSTableWriter create ""temp"" file, but it should be OK since closeAndOpenReader renames file.","17/Apr/12 20:01;jbellis;LGTM.  Could you also post a version against 1.0, Yuki?",17/Apr/12 20:24;yukim;Patch for 1.0 branch attached.,"17/Apr/12 21:27;jbellis;committed, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cassandra.bat fails when CASSANDRA_HOME contains a whitespace, again",CASSANDRA-2952,12515482,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,miau,miau,miau,27/Jul/11 07:24,12/Mar/19 14:01,13/Mar/19 22:26,12/Aug/11 22:05,0.8.5,,,Packaging,,,0,,,,,"I installed cassandra into C:\Program Files\apache-cassandra and tried to start cassandra. But cassandra.bat fails with following error.

{code} 
C:\Program Files\apache-cassandra>bin\cassandra.bat
Starting Cassandra Server
Error opening zip file or JAR manifest missing : C:\Program
Error occurred during initialization of VM
agent library failed to init: instrument
{code}

This problem is similar to CASSANDRA-2237. I'll post a patch to fix the problem later.",Windows7 32bit,,,,,,,,,,,,,,,27/Jul/11 07:30;miau;CASSANDRA-2952.diff;https://issues.apache.org/jira/secure/attachment/12487945/CASSANDRA-2952.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-12 21:12:46.091,,,no_permission,,,,,,,,,,,,20909,,,Sat Aug 13 04:59:01 UTC 2011,,,,,,0|i0gegv:,93774,bcoverston,bcoverston,,,,,,,,,"12/Aug/11 21:12;bcoverston;+1, adding quotes around the string will work.
Also checked for other places where we may not have been adding quotes, and they are fine.","12/Aug/11 22:05;jbellis;committed, thanks!","12/Aug/11 23:21;hudson;Integrated in Cassandra-0.8 #276 (See [https://builds.apache.org/job/Cassandra-0.8/276/])
    fix cassandra.bat when CASSANDRA_HOME contains spaces
patch by Koji Ando; reviewed by bcoverston for CASSANDRA-2952

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1157268
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/bin/cassandra.bat
",13/Aug/11 04:59;miau;I've verified that Cassandra-0.8 #276 works fine. Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC ResultSet improperly handles null column values,CASSANDRA-2956,12515544,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,ardot,ardot,ardot,27/Jul/11 15:56,12/Mar/19 14:01,13/Mar/19 22:26,27/Jul/11 17:26,,,,,,,0,JDBC,,,,"JDBC {{ResultSet}} getters return built-in datatypes such as {{int, long, short, byte, boolean}} that are not capable of handling a null value. As a consequence, it provides a method: {{wasNull}} which returns true if the value was null. The spec requires a zero numeric value (or false in the case of {{boolean}} ) is returned by the getter. This was being mis-handled and a null value was being cast (boxed) to the return value. An NPE would result.
",,,,,,,,,,,,,,,,27/Jul/11 15:57;ardot;jdbc-bugfix-handling-null-values.txt;https://issues.apache.org/jira/secure/attachment/12487988/jdbc-bugfix-handling-null-values.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-27 17:26:30.842,,,no_permission,,,,,,,,,,,,20911,,,Wed Jul 27 17:26:30 UTC 2011,,,,,,0|i0gehz:,93779,,,,,,,,,,,"27/Jul/11 17:26;jbellis;+1, committed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix smallish problems find by FindBugs,CASSANDRA-3658,12536150,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,22/Dec/11 11:28,12/Mar/19 14:01,13/Mar/19 22:26,22/Dec/11 21:56,1.1.0,,,,,,0,fingbugs,,,,"I've just run (the newly released) FindBugs 2 out of curiosity. Attaching a number of patches related to issue raised by it. There is nothing major at all so all patches are against trunk.

I've tried keep each issue to it's own patch with a self describing title. It far from covers all FindBugs alerts, but it's a picky tool so I've tried to address only what felt at least vaguely useful. Those are still mostly nits (only patch 2 is probably an actual bug).",,,,,,,,,,,,,,,,22/Dec/11 11:29;slebresne;0001-Respect-Future-semantic.patch;https://issues.apache.org/jira/secure/attachment/12508370/0001-Respect-Future-semantic.patch,22/Dec/11 11:29;slebresne;0002-Avoid-race-when-reloading-snitch-file.patch;https://issues.apache.org/jira/secure/attachment/12508371/0002-Avoid-race-when-reloading-snitch-file.patch,22/Dec/11 11:29;slebresne;0003-use-static-inner-class-when-possible.patch;https://issues.apache.org/jira/secure/attachment/12508372/0003-use-static-inner-class-when-possible.patch,22/Dec/11 11:29;slebresne;0004-Remove-dead-code.patch;https://issues.apache.org/jira/secure/attachment/12508373/0004-Remove-dead-code.patch,22/Dec/11 11:29;slebresne;0005-Protect-against-signed-byte-extension.patch;https://issues.apache.org/jira/secure/attachment/12508374/0005-Protect-against-signed-byte-extension.patch,22/Dec/11 11:29;slebresne;0006-Add-hashCode-method-when-equals-is-overriden.patch;https://issues.apache.org/jira/secure/attachment/12508375/0006-Add-hashCode-method-when-equals-is-overriden.patch,22/Dec/11 11:29;slebresne;0007-Inverse-argument-of-compare-instead-of-negating-to-a.patch;https://issues.apache.org/jira/secure/attachment/12508376/0007-Inverse-argument-of-compare-instead-of-negating-to-a.patch,22/Dec/11 11:29;slebresne;0008-stop-pretending-Token-is-Serializable-LocalToken-is-.patch;https://issues.apache.org/jira/secure/attachment/12508377/0008-stop-pretending-Token-is-Serializable-LocalToken-is-.patch,22/Dec/11 11:29;slebresne;0009-remove-useless-assert-that-is-always-true.patch;https://issues.apache.org/jira/secure/attachment/12508378/0009-remove-useless-assert-that-is-always-true.patch,22/Dec/11 11:29;slebresne;0010-Add-equals-and-hashCode-to-Expiring-column.patch;https://issues.apache.org/jira/secure/attachment/12508379/0010-Add-equals-and-hashCode-to-Expiring-column.patch,,10.0,,,,,,,,,,,,,,,,,,,2011-12-22 17:08:08.01,,,no_permission,,,,,,,,,,,,221833,,,Tue Dec 27 21:01:48 UTC 2011,,,,,,0|i0gmnz:,95102,jbellis,jbellis,,,,,,,,,22/Dec/11 17:08;jbellis;+1 on all but 09; I'd rather leave that assert alone since its point is to catch a bug if we change the signature of scanner,22/Dec/11 21:56;slebresne;I'm good with keeping the assert. Committed all except 09. Thanks,"23/Dec/11 00:47;hudson;Integrated in Cassandra #1266 (See [https://builds.apache.org/job/Cassandra/1266/])
    Fix minor issues reported by FindBugs
patch by slebresne; reviewed by jbellis for CASSANDRA-3658

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1222476
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/trunk/src/java/org/apache/cassandra/config/ReplicationStrategy.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ArrayBackedSortedColumns.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamily.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ExpiringColumn.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowIteratorFactory.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/SizeTieredCompactionStrategy.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/filter/QueryFilter.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/marshal/AbstractType.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/marshal/DynamicCompositeType.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/marshal/ReversedType.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/LocalToken.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/Token.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/compress/CompressionMetadata.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java
* /cassandra/trunk/src/java/org/apache/cassandra/locator/PropertyFileSnitch.java
* /cassandra/trunk/src/java/org/apache/cassandra/net/ProtocolHeader.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/AntiEntropyService.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/FileStreamTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/EstimatedHistogram.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/NodeId.java
","27/Dec/11 20:52;nickmbailey;This breaks a bunch of jmx stuff. A fair amount of jmx methods return Token objects so they need to be serializable. I plan on doing CASSANDRA-2805 for 1.1, but jmx will be broken in trunk until I get that done unless that specific patch is reverted.",27/Dec/11 21:01;jbellis;reverted 0008 for now,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
should export JAVA variable in the bin/cassandra and use that in the cassandra-env.sh when check for the java version,CASSANDRA-2785,12510628,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,cywjackson,cywjackson,17/Jun/11 00:59,12/Mar/19 14:01,13/Mar/19 22:26,07/Aug/11 20:12,0.8.4,,,Packaging,,,0,,,,,"I forgot which jira we add this java -version check in the cassandra-env.sh (for adding jamm to the javaagent), but we should probably use the variable JAVA set in bin/cassandra (will need export) and use $JAVA instead of ""java"" in the cassandra-env.sh

In a situation where JAVA_HOME may have been properly set as the Sun's java but the PATH still have the OpenJDK's java in front, the check will fail to add the jamm.jar, even though the cassandra jvm is properly started via the Sun's java.",,,,,,,,,,,,,,,,27/Jul/11 21:22;thepaul;0001-use-JAVA-in-cassandra-env.sh.patch.txt;https://issues.apache.org/jira/secure/attachment/12488029/0001-use-JAVA-in-cassandra-env.sh.patch.txt,28/Jul/11 16:10;thepaul;0002-fix-usage-of-bash-n-tests.patch.txt;https://issues.apache.org/jira/secure/attachment/12488112/0002-fix-usage-of-bash-n-tests.patch.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-27 21:22:01.492,,,no_permission,,,,,,,,,,,,20827,,,Sat Aug 06 23:04:49 UTC 2011,,,,,,0|i0gdfz:,93608,,,,,,,,,,,"27/Jul/11 21:22;thepaul;patch 0001: Uses $JAVA in cassandra-env.sh instead of relying on $PATH.

Makes sure that JAVA is appropriately set from bin/cassandra and debian/init.

Changes debian initscript so that /etc/default/cassandra is sourced _before_ /etc/cassandra/cassandra-env.sh. I don't know of anyone using /etc/default/cassandra in such a way that this would be a problem. The change gives users a place to specify their own $JAVA_HOME if they so desire.","28/Jul/11 07:15;urandom;The -n eval in bin/cassandra needs to be quoted or it'll read non-zero even when it's not.

{noformat}
--- a/bin/cassandra
+++ b/bin/cassandra
@@ -80,7 +80,7 @@ elif [ -r $CASSANDRA_INCLUDE ]; then
 fi
 
 # Use JAVA_HOME if set, otherwise look for java in PATH
-if [ -n $JAVA_HOME ]; then
+if [ -n ""$JAVA_HOME"" ]; then
     JAVA=$JAVA_HOME/bin/java
 else
     JAVA=java
{noformat}

Aside from that, +1","28/Jul/11 16:10;thepaul;Doh, I never knew that one. I normally quote everything, but was trying to match style.

I found a couple other places where we need to quote the argument to test -n.",29/Jul/11 10:30;urandom;+1,29/Jul/11 14:40;urandom;committed; thanks!,"29/Jul/11 15:29;hudson;Integrated in Cassandra-0.8 #243 (See [https://builds.apache.org/job/Cassandra-0.8/243/])
    honor path to java when JAVA_HOME set

Patch by Paul Cannon; reviewed by eevans for CASSANDRA-2785

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152241
Files : 
* /cassandra/branches/cassandra-0.8/conf/cassandra-env.sh
* /cassandra/branches/cassandra-0.8/debian/cassandra.postinst
* /cassandra/branches/cassandra-0.8/debian/init
* /cassandra/branches/cassandra-0.8/bin/cassandra
","04/Aug/11 16:31;slebresne;This is causing problems, see CASSANDRA-2992.

I have only reverted this patch on a specific branch so that it doesn't block 0.8.3 release. However, I have not reverted yet on 0.8 branch (or trunk), so that it could be fixed incrementally.

As a side note, it would be nice to set the correct fix version and reviewer when marking an issue resolved.","06/Aug/11 23:04;thepaul;I think this patch is still correct, and that the bug causing the regression was the invalid hard-coding of JAVA_HOME in redhat/cassandra. If the fix for CASSANDRA-2992 is acceptable, this should probably be re-closed as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in ReversedType comparator,CASSANDRA-3111,12520907,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,frousseau,frousseau,31/Aug/11 10:35,12/Mar/19 14:01,13/Mar/19 22:26,31/Aug/11 16:57,0.8.5,,,,,,0,,,,,"Scenario :
 * create a cf with a reversed comparator
 * insert a few columns in a row
 * try to read data with : SliceRange(start='', finish='', reversed=true)
 ** no data is returned, but some columns are expected
 * try to read data with : SliceRange(start='', finish='', reversed=false)
 ** if not flushed on disk : no data is returned


",,,,,,,,,,,,,,,,31/Aug/11 10:40;frousseau;001-CASSANDRA-3111.patch;https://issues.apache.org/jira/secure/attachment/12492441/001-CASSANDRA-3111.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-31 16:57:19.476,,,no_permission,,,,,,,,,,,,4085,,,Wed Aug 31 17:16:02 UTC 2011,,,,,,0|i0gfxj:,94011,slebresne,slebresne,,,,,,,,,"31/Aug/11 16:57;slebresne;+1

Committed with an added unit test. Thanks","31/Aug/11 17:16;hudson;Integrated in Cassandra-0.8 #306 (See [https://builds.apache.org/job/Cassandra-0.8/306/])
    Fix handling of the empty byte buffer by ReversedType
patch by frousseau; reviewed by slebresne for CASSANDRA-3111

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163695
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/marshal/ReversedType.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh doesn't work on windows (no readline),CASSANDRA-3131,12521228,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,urandom,urandom,02/Sep/11 20:40,12/Mar/19 14:01,13/Mar/19 22:26,11/Nov/11 22:32,1.0.3,,,,,,0,cql,,,,"Saulius Menkevicius reports in CASSANDRA-3010 that {{cqlsh}} doesn't start on Windows because the readline module is not present.

{{cqlsh}} should be fixed to only use readline if it is present.",,,,,,,,,,,,,,,,11/Nov/11 22:24;thepaul;3131.patch.txt;https://issues.apache.org/jira/secure/attachment/12503432/3131.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-08 17:11:55.871,,,no_permission,,,,,,,,,,,,1865,,,Fri Nov 11 22:32:28 UTC 2011,,,,,,0|i0gg5z:,94049,jbellis,jbellis,,,,,,,,,08/Nov/11 17:11;thepaul;Addressed in CASSANDRA-3188.,"11/Nov/11 22:24;thepaul;Although cqlsh works on windows as is (both with cygwin and under the Command Prompt application), it mistakenly tries to use ANSI escape codes for color under Command Prompt, which show up as messy {{];34;40m}}-ish garbage.

The improvements here make cqlsh smarter about choosing whether to use color by default.",11/Nov/11 22:32;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FreeableMemory can be accessed after it is invalid,CASSANDRA-2951,12515459,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,26/Jul/11 22:02,12/Mar/19 14:01,13/Mar/19 22:26,27/Jul/11 20:58,0.8.3,,,,,,0,,,,,"SerializingCache.get looks like this:

{code}
    public V get(Object key)
    {
        FreeableMemory mem = map.get(key);
        if (mem == null)
            return null;
        return deserialize(mem);
    }
{code}

If a cache object is evicted or replaced after the get happens, but before deserialize completes, we will trigger an assertion failure (if asserts are enabled) or segfault (if they are not).",,,,,,,,,,,,,,,,26/Jul/11 22:03;jbellis;2951.txt;https://issues.apache.org/jira/secure/attachment/12487902/2951.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-27 15:51:48.975,,,no_permission,,,,,,,,,,,,20908,,,Thu Jul 28 07:25:21 UTC 2011,,,,,,0|i0gegn:,93773,slebresne,slebresne,,,,,,,,,26/Jul/11 22:03;jbellis;patch adds reference counting to FreeableMemory,"27/Jul/11 15:51;slebresne;I think there is a race between eviction and remove (or two removes even), so that the references can be < 0. So I think the '== 0' in reference() and finalize() (but not unreference() obviously) should become '<= 0'.

With that corrected, looks good. +1.",27/Jul/11 20:58;jbellis;quite right.  committed with that change.,"27/Jul/11 21:16;hudson;Integrated in Cassandra #978 (See [https://builds.apache.org/job/Cassandra/978/])
    fix potential use of free'd native memory interface/SerializingCache
patch by jbellis; reviewed by slebresne for CASSANDRA-2951

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1151625
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/cache/FreeableMemory.java
* /cassandra/trunk/src/java/org/apache/cassandra/cache/SerializingCache.java
* /cassandra/trunk/CHANGES.txt
","28/Jul/11 07:25;slebresne;You forgot to use <= 0 in finalize in you commit, I took the liberty to change it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool netstats progress does not update on receiving side,CASSANDRA-2972,12515826,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,wmeler,wmeler,30/Jul/11 03:42,12/Mar/19 14:01,13/Mar/19 22:26,03/Aug/11 21:14,0.8.4,,,,,,0,,,,,"when you add/remove node to cluster, nodetool netstats show correct results only on sending side - on receiving side you can see only 0% progress",,,,,,,,,,,,,,,,01/Aug/11 17:45;yukim;ASF.LICENSE.NOT.GRANTED--cassandra-0.8-2972-v2.txt;https://issues.apache.org/jira/secure/attachment/12488424/ASF.LICENSE.NOT.GRANTED--cassandra-0.8-2972-v2.txt,01/Aug/11 17:26;yukim;ASF.LICENSE.NOT.GRANTED--cassandra-0.8-2972.txt;https://issues.apache.org/jira/secure/attachment/12488423/ASF.LICENSE.NOT.GRANTED--cassandra-0.8-2972.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-30 04:14:38.149,,,no_permission,,,,,,,,,,,,20920,,,Wed Aug 03 22:19:59 UTC 2011,,,,,,0|i0gelb:,93794,jbellis,jbellis,,,,,,,,,"30/Jul/11 04:14;jbellis;What do you think, Yuki?","01/Aug/11 17:26;yukim;Patch attached for 0.8 branch.
StreamInSession holds references to all pending files(files attribute) and current processing file(current attribute). The former include the latter but those are different objects after deserialized, and only the latter gets update on progress.
When returning incoming streaming status via JMX, only the former pending files described above are returned due to the nature of HashSet#add.",01/Aug/11 17:45;yukim;Slightly modified version.,"03/Aug/11 17:23;jbellis;I think we also need to update remoteFile.progress in IncomingStreamReader, at least in the 0.8 branch","03/Aug/11 21:03;yukim;IncomingStreamReader#readnwrite (or SSLIncomingStreamReader#readwrite) in the 0.8 branch does update remoteFile.progress, so the issue is just a matter of dispaying through JMX.","03/Aug/11 21:08;jbellis;ah, you're right.  i was thrown off by the whacky indentation there for ISR.",03/Aug/11 21:14;jbellis;committed v2,"03/Aug/11 22:19;hudson;Integrated in Cassandra-0.8 #254 (See [https://builds.apache.org/job/Cassandra-0.8/254/])
    include files-to-be-streamed in StreamInSession.getSources
patch by Yuki Morishita; reviewed by jbellis for CASSANDRA-2972

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1153668
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/StreamInSession.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C* .deb installs C* init.d scripts such that C* comes up before mdadm and related,CASSANDRA-2481,12504307,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,mdennis,mdennis,14/Apr/11 18:51,12/Mar/19 14:01,13/Mar/19 22:26,23/May/11 15:45,0.7.6,0.8.0,,Packaging,,,0,,,,,the C* .deb packages install the init.d scripts at S20 which is before mdadm and various other services.  This means that when a node reboots that C* is started before the RAID sets are up and mounted causing C* to think it has no data and attempt bootstrapping again.,,,,,,,,,,,,,,,,18/May/11 16:44;thepaul;2481-fix.txt;https://issues.apache.org/jira/secure/attachment/12479618/2481-fix.txt,12/May/11 18:26;thepaul;2481.txt;https://issues.apache.org/jira/secure/attachment/12479001/2481.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-18 16:26:15.407,,,no_permission,,,,,,,,,,,,20644,,,Fri May 20 16:51:36 UTC 2011,,,,,,0|i0gbmf:,93313,brandon.williams,brandon.williams,,,,,,,,,"18/Apr/11 16:26;hudson;Integrated in Cassandra-0.8 #14 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/14/])
    add optional replication_factor fields to KsDef to make supporting both 0.8 and 0.7 easier for client devs
patch by jbellis; reviewed by Nate McCall for CASSANDRA-2481
","12/May/11 18:27;thepaul;patch solverizes this problem. uses priority 50, or requires mdadm to be started first if using dependency-based init ordering.","12/May/11 19:27;jbellis;Out of curiosity, why aren't start/stop symmetrical?

{noformat}
+	dh_installinit -u'start 50 2 3 4 5 . stop 50 0 1 6'
{noformat}","12/May/11 20:05;thepaul;bq. Out of curiosity, why aren't start/stop symmetrical?

they are: ((start = 50) == 100 - (end = 50))

If you were referring to the ""2 3 4 5"" vs ""0 1 6"", those are the standard ""starting stuff"" and ""stopping stuff"" runlevels for SysV init.",12/May/11 20:46;brandon.williams;Committed.,"12/May/11 21:10;hudson;Integrated in Cassandra-0.7 #484 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/484/])
    Start/stop cassandra after more important services such as mdadm in
debian packaging.
Patch by Paul Cannon, reviewed by brandonwilliams for CASSANDRA-2481
","18/May/11 10:05;slebresne;When installing the debian package for 0.7.6 and 0.8.0-rc1 on ubuntu 11.04 (natty), I get
{noformat}
Installing new version of config file /etc/init.d/cassandra ...
update-rc.d: error: start|stop arguments not terminated by "".""
usage: update-rc.d [-n] [-f] <basename> remove
       update-rc.d [-n] <basename> defaults [NN | SS KK]
       update-rc.d [-n] <basename> start|stop NN runlvl [runlvl] [...] .
       update-rc.d [-n] <basename> disable|enable [S|2|3|4|5]
		-n: not really
		-f: force
{noformat}

Given that it works like a charm with 0.7.5, I strongly suspect this is this patch doing.","18/May/11 15:32;gasolwu;{quote}
Installing new version of config file /etc/init.d/cassandra ...
update-rc.d: error: start|stop arguments not terminated by "".""
{quote}

find dh_installinit in debian/rules and edit like following line.

dh_installinit -u'start 50 2 3 4 5 . stop 50 0 1 6 .'

it works for me.",18/May/11 16:44;thepaul;fix for previous,18/May/11 17:10;brandon.williams;Fix committed.,"18/May/11 17:48;hudson;Integrated in Cassandra-0.7 #488 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/488/])
    Fix for dh_installinit syntax for CASSANDRA-2481
Patch by Paul Cannon, reviewed by brandonwilliams

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1124338
Files : 
* /cassandra/branches/cassandra-0.7/debian/rules
","20/May/11 15:24;sword2;This is still broken for 0.8.0~rc1.  The last commit to it shows a merge with 0.7 but this fix is not in it.

quick workaround is to modify the cassandra.postinst file.  You can get that from dpkg-query -c cassandra, mine is under /var/lib/dpkg/info/:
{noformat}
sudo cp /var/lib/dpkg/info/cassandra.postinst /var/lib/dpkg/info/cassandra.postinst.original
sudo sed -i 's/50 2 3 4 5 \. stop 50 0 1 6 >/50 2 3 4 5 \. stop 50 0 1 6 \.>/' /var/lib/dpkg/info/cassandra.postinst
#make sure it's correct then run the below line.
#sudo dpkg --configure cassandra
{noformat}

","20/May/11 16:51;jbellis;the fix is in the 0.8.0 branch for -final, but you are right that it is not in rc1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce default memtable size,CASSANDRA-2413,12503152,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,01/Apr/11 16:51,12/Mar/19 14:01,13/Mar/19 22:26,05/Apr/11 15:30,0.7.5,,,,,,0,,,,,"I'm going to wimp out on targeting CASSANDRA-2006 for 0.7.5 so to mitigate OOMing by newcomers let's reduce the default memtable size -- what we have now predates indexes, which can dramatically increase memory requirements.",,,,,,,,,,,,,,,,01/Apr/11 17:01;jbellis;2413.txt;https://issues.apache.org/jira/secure/attachment/12475240/2413.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-05 15:05:28.646,,,no_permission,,,,,,,,,,,,20608,,,Tue Apr 05 16:06:41 UTC 2011,,,,,,0|i0gb87:,93249,bcoverston,bcoverston,,,,,,,,,"05/Apr/11 15:05;bcoverston;+1, patch is good.",05/Apr/11 15:30;jbellis;committed,"05/Apr/11 16:06;hudson;Integrated in Cassandra-0.7 #420 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/420/])
    halve default memtable thresholds
patch by jbellis; reviewed by bcoverston for CASSANDRA-2413
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL ignores client timestamp for full row deletion,CASSANDRA-2912,12514451,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,18/Jul/11 14:46,12/Mar/19 14:01,13/Mar/19 22:26,20/Jul/11 15:54,0.8.2,,,,,,0,cql,,,,,,,,,,,,,,,,,,,,19/Jul/11 08:22;slebresne;0001-CQL-timestamp-row-deletion.patch;https://issues.apache.org/jira/secure/attachment/12486979/0001-CQL-timestamp-row-deletion.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-18 16:02:16.464,,,no_permission,,,,,,,,,,,,20889,,,Tue Jul 19 22:16:48 UTC 2011,,,,,,0|i0ge7r:,93733,xedin,xedin,,,,,,,,,18/Jul/11 16:02;jbellis;can you add a test for this?,19/Jul/11 08:22;slebresne;Turns out setting timestamp for DELETE wasn't working at all anyway because the parser wasn't allowing it. New attached patch fixes this and add tests for all of that.,"19/Jul/11 19:49;xedin;{noformat}
usingClauseDelete[Attributes attrs]
    : K_USING usingClauseDeleteObjective[attrs] ( K_AND? usingClauseDeleteObjective[attrs] )*
    ;
{noformat}

This means that we allow user to use multiple number of usingClauseDeleteObjective statements, any motivation behind that I'm not getting?","19/Jul/11 20:11;slebresne;A usingClauseDeleteObject is either TIMESTAMP or CONSISTENCY, so you at least want to allow setting both of them. Now if the question was why * instead of ? at the end, then it's mostly because I copied/pasted/modified usingClause where the * didn't seem to be a problem. I kind of think that having a * even if more than 2 doesn't make much sense is no big deal and will avoid, when we add a third possible usingClauseDeleteObjective, to be limited to 2 clauses just because we forgot that it was limited to 2 previously. But I don't really feel strongly about that.","19/Jul/11 20:20;xedin;I'm asking because that also means that user will be able to specify as many timestamps and consistency levels as he wants, we will need to split usingClauseDeleteObjective into two statements - one for consistency level and one for timestamp and use them in usingClauseDelete like (K_USING (usingConsistencyLevel[attrs] (K_AND usingTimestamp[attrs])? | usingTimestamp[attrs] (K_AND usingConsistencyLevel[attrs])?) to support language consistency. ","19/Jul/11 21:10;slebresne;bq. I'm asking because that also means that user will be able to specify as many timestamps and consistency levels as he wants

That's what I said. To rephrase:
* it's the case also for the UPDATE statement: we should do something for both or none.
* I personally don't think it's a problem, the last statement would be picked each time, which feels a reasonable behavior in that case. More precisely I don't think it's worth caring about.
* if we do want to add a restriction though, I think it would be easier to have Attributes throw an exception when one of its set method is called twice. Because your proposed fix will become a bit ugly for the UPDATE statement and it would be a pain to extend to future new clauses.","19/Jul/11 21:23;xedin;Ok, lets leave it for now, +1.","19/Jul/11 22:16;hudson;Integrated in Cassandra-0.8 #226 (See [https://builds.apache.org/job/Cassandra-0.8/226/])
    respect client timestamp on full row deletions
patch by slebresne; reviewed by pyaskevich for CASSANDRA-2912

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148547
Files : 
* /cassandra/branches/cassandra-0.8/test/system/test_cql.py
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/Cql.g
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/DeleteStatement.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debian Package for 0.8 is missing,CASSANDRA-2826,12511523,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,smitchell360,smitchell360,25/Jun/11 04:30,12/Mar/19 14:01,13/Mar/19 22:26,12/Aug/11 21:45,0.8.5,,,Packaging,,,1,,,,,"In file

http://www.apache.org/dist/cassandra/debian/dists/08x/InRelease

Codename: sid

Should be changed to

Codename: 08x",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-06-25 14:28:29.722,,,no_permission,,,,,,,,,,,,20853,,,Fri Aug 12 21:45:01 UTC 2011,,,,,,0|i0gdp3:,93649,,,,,,,,,,,25/Jun/11 14:28;thepaul;I think Eric has to take care of this.,12/Aug/11 21:45;urandom;This is no longer relevant as the unstable suite has been eliminated.  Closing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make forceUserDefinedCompaction actually do what it says,CASSANDRA-2575,12505353,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,27/Apr/11 21:07,12/Mar/19 14:01,13/Mar/19 22:26,28/Apr/11 16:16,0.8.0 beta 2,,,,,,0,,,,,See http://www.mail-archive.com/user@cassandra.apache.org/msg12621.html for motivation,,,,,,,,,,,,,,,,28/Apr/11 15:39;jbellis;2575-v2.txt;https://issues.apache.org/jira/secure/attachment/12477663/2575-v2.txt,28/Apr/11 03:43;jbellis;2575.txt;https://issues.apache.org/jira/secure/attachment/12477607/2575.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-04-28 02:31:46.028,,,no_permission,,,,,,,,,,,,20708,,,Mon May 02 19:54:50 UTC 2011,,,,,,0|i0gc6n:,93404,slebresne,slebresne,,,,,,,,,27/Apr/11 21:17;jbellis;patch splits out doCompactionWithoutSizeEstimates and calls that from submitUserDefined.,28/Apr/11 02:31;stuhood;The patch for 2552 is attached.,28/Apr/11 03:43;jbellis;corrected patch attached.,"28/Apr/11 10:32;slebresne;I suppose we could backport to 0.7 too.
Also wondering if it couldn't make sense to not compact if we end up with only 1 sstable in doCompaction, now that you still can do a userDefinedCompaction on only 1 sstable. Not being able to compact the 2 smallest sstables seems pathological enough and it would be nice to avoid the 'same sstable compacted forever' problem seen in the mail thread above.

Anyway, +1 on the patch. ","28/Apr/11 15:39;jbellis;bq. Also wondering if it couldn't make sense to not compact if we end up with only 1 sstable in doCompaction, now that you still can do a userDefinedCompaction on only 1 sstable

Good idea. v2 attached with a rewrite of the retry loop to do this.",28/Apr/11 15:47;slebresne;+1 on v2,28/Apr/11 16:16;jbellis;committed,"02/May/11 19:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
key validator not getting set when adding a keyspace,CASSANDRA-2467,12504200,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,13/Apr/11 19:05,12/Mar/19 14:01,13/Mar/19 22:26,15/Apr/11 12:54,0.8 beta 1,,,,,,0,,,,,needs to be applied CassandraServer.convertToCFMetaData(),,,,,,,,,,,,CASSANDRA-2311,,,,13/Apr/11 19:19;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-set-requested-key-validator-when-creating-a-keyspace.txt;https://issues.apache.org/jira/secure/attachment/12476274/ASF.LICENSE.NOT.GRANTED--v1-0001-set-requested-key-validator-when-creating-a-keyspace.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-14 20:30:15.709,,,no_permission,,,,,,,,,,,,20641,,,Fri Apr 15 12:54:19 UTC 2011,,,,,,0|i0gbjr:,93301,urandom,urandom,,,,,,,,,13/Apr/11 19:32;gdusbabek;todo: we could use a system test to verify that non-default CF and KS options are actually applied.,14/Apr/11 20:30;jbellis;+1,15/Apr/11 12:54;gdusbabek;committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove checkAllColumnFamilies on startup,CASSANDRA-2444,12503814,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,09/Apr/11 03:50,12/Mar/19 14:01,13/Mar/19 22:26,22/Apr/11 22:21,0.8.0 beta 2,,,,,,0,compaction,,,,"We've ran into many times where we do not want compaction to run right away against CFs when booting up a node. If the node needs to compact, it will do so at the first flush",,,,,,,,,,,,,,,,22/Apr/11 22:13;lenn0x;0001-CASSANDRA-2444-Remove-checking-all-column-families-o.patch;https://issues.apache.org/jira/secure/attachment/12477170/0001-CASSANDRA-2444-Remove-checking-all-column-families-o.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-21 23:30:24.939,,,no_permission,,,,,,,,,,,,20628,,,Fri Apr 22 23:31:41 UTC 2011,,,,,,0|i0gben:,93278,stuhood,stuhood,,,,,,,,,"21/Apr/11 23:30;stuhood;Rather than adding an option for this, I think we should remove the check entirely. If the node needs to compact, it will do so at the first flush, which is more likely to be staggered across the cluster (and if this flush is forced by commitlogs, then AOK).","22/Apr/11 04:47;jbellis;bq. Rather than adding an option for this, I think we should remove the check entirely. If the node needs to compact, it will do so at the first flush

+1","22/Apr/11 23:31;hudson;Integrated in Cassandra-0.8 #35 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/35/])
    Remove checking all column families on startup for compaction candidates
patch by goffinet; reviewed by stuhood for CASSANDRA-2444
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null pointer dereference of m in org.apache.cassandra.db.commitlog.CommitLogSegment.dirtyString(),CASSANDRA-3021,12518719,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,fantayeneh,fantayeneh,fantayeneh,12/Aug/11 16:24,12/Mar/19 14:01,13/Mar/19 22:26,12/Aug/11 19:33,0.8.5,,,,,,0,,,,,,,,,,,,,,,,,,,,,12/Aug/11 16:27;fantayeneh;trunk-fixed-nulldereference.patch;https://issues.apache.org/jira/secure/attachment/12490268/trunk-fixed-nulldereference.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-12 19:33:10.427,,,no_permission,,,,,,,,,,,,20939,,,Fri Aug 12 20:21:47 UTC 2011,,,,,,0|i0gevz:,93842,jbellis,jbellis,,,,,,,,,12/Aug/11 16:27;fantayeneh;Fixes the NP Dereference,"12/Aug/11 19:33;jbellis;committed to 0.8.5 and trunk, thanks!","12/Aug/11 20:21;hudson;Integrated in Cassandra-0.8 #275 (See [https://builds.apache.org/job/Cassandra-0.8/275/])
    fix NPE when debug logging is enabled and dropped CF is present
patch by fantayeneh gizaw; reviewed by jbellis for CASSANDRA-3021

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1157225
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve caching of same-version Messages on digest and repair paths,CASSANDRA-3158,12522263,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,08/Sep/11 14:03,12/Mar/19 14:01,13/Mar/19 22:26,08/Sep/11 16:04,0.8.6,,,,,,0,,,,,,,,,,,,,,,,,,,,,08/Sep/11 14:48;jbellis;3158-v2.txt;https://issues.apache.org/jira/secure/attachment/12493597/3158-v2.txt,08/Sep/11 14:03;jbellis;3158.txt;https://issues.apache.org/jira/secure/attachment/12493591/3158.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-08 14:57:36.443,,,no_permission,,,,,,,,,,,,4056,,,Thu Sep 08 17:33:01 UTC 2011,,,,,,0|i0ggif:,94105,slebresne,slebresne,,,,,,,,,"08/Sep/11 14:48;jbellis;v2 also lazy-initializes the caching producer for the initial digest reads, and updates comments","08/Sep/11 14:57;slebresne;For the initial digest reads, it would be nice to use the CachingMessageProducer for the local digest read too.",08/Sep/11 15:04;jbellis;no Messages are created for local reads.,"08/Sep/11 15:07;slebresne;Oh right, my bad. Anyways, lgtm, +1.",08/Sep/11 16:04;jbellis;committed,08/Sep/11 16:36;jbellis;Note that on merge to trunk I also added a readCallbacks.clear() statement at the beginning of the short-read do/while loop.,"08/Sep/11 17:33;hudson;Integrated in Cassandra-0.8 #321 (See [https://builds.apache.org/job/Cassandra-0.8/321/])
    Improve caching of same-version Messages on digest and repair paths
patch by jbellis; reviewed by slebresne for CASSANDRA-3158

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166774
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyRecordReader can report progress > 100%,CASSANDRA-3942,12543611,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,tjake,tjake,22/Feb/12 02:53,12/Mar/19 14:01,13/Mar/19 22:26,21/Jun/12 19:47,1.1.2,,,,,,0,,,,,CFRR.getProgress() can return a value > 1.0 since the totalRowCount is a estimate.,,,,,,,,,,,,,,,,21/Jun/12 17:24;brandon.williams;3942.txt;https://issues.apache.org/jira/secure/attachment/12532908/3942.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-18 16:11:16.42,,,no_permission,,,,,,,,,,,,228850,,,Thu Jun 21 19:47:37 UTC 2012,,,,,,0|i0gq2v:,95655,jbellis,jbellis,,,,,,,,,"18/Jun/12 16:11;brandon.williams;So, is the idea simply to clamp it at 1.0?  Since all we have is an estimate we can't really get any more accurate.",21/Jun/12 17:24;brandon.williams;Patch to clamp getProgress to 1.0 or less.,21/Jun/12 18:58;jbellis;+1,21/Jun/12 19:47;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
post-2392 trunk does not build with java 7,CASSANDRA-3796,12540000,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,27/Jan/12 05:31,12/Mar/19 14:01,13/Mar/19 22:26,28/Jan/12 05:17,,,,,,,0,,,,,"See below, on a fresh clone. Builds w/ java 6.

{code}
    [javac] /tmp/c2/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableReader.java:419: error: no suitable method found for binarySearch(List<DecoratedKey>,RowPosition)
    [javac]         int index = Collections.binarySearch(indexSummary.getKeys(), key);
    [javac]                                ^
    [javac]     method Collections.<T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>) is not applicable
    [javac]       (cannot instantiate from arguments because actual and formal argument lists differ in length)
    [javac]     method Collections.<T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2) is not applicable
    [javac]       (no instance(s) of type variable(s) T#2 exist so that argument type List<DecoratedKey> conforms to formal parameter type List<? extends Comparable<? super T#2>>)
    [javac]   where T#1,T#2 are type-variables:
    [javac]     T#1 extends Object declared in method <T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>)
    [javac]     T#2 extends Object declared in method <T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2)
    [javac] /tmp/c2/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableReader.java:509: error: no suitable method found for binarySearch(List<DecoratedKey>,RowPosition)
    [javac]             int left = Collections.binarySearch(samples, leftPosition);
    [javac]                                   ^
    [javac]     method Collections.<T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>) is not applicable
    [javac]       (cannot instantiate from arguments because actual and formal argument lists differ in length)
    [javac]     method Collections.<T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2) is not applicable
    [javac]       (no instance(s) of type variable(s) T#2 exist so that argument type List<DecoratedKey> conforms to formal parameter type List<? extends Comparable<? super T#2>>)
    [javac]   where T#1,T#2 are type-variables:
    [javac]     T#1 extends Object declared in method <T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>)
    [javac]     T#2 extends Object declared in method <T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2)
    [javac] /tmp/c2/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableReader.java:521: error: no suitable method found for binarySearch(List<DecoratedKey>,RowPosition)
    [javac]                       : Collections.binarySearch(samples, rightPosition);
    [javac]                                    ^
    [javac]     method Collections.<T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>) is not applicable
    [javac]       (cannot instantiate from arguments because actual and formal argument lists differ in length)
    [javac]     method Collections.<T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2) is not applicable
    [javac]       (no instance(s) of type variable(s) T#2 exist so that argument type List<DecoratedKey> conforms to formal parameter type List<? extends Comparable<? super T#2>>)
    [javac]   where T#1,T#2 are type-variables:
    [javac]     T#1 extends Object declared in method <T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>)
    [javac]     T#2 extends Object declared in method <T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2)
{code}
",,,,,,,,,,,,,,,,27/Jan/12 06:58;scode;CASSANDRA-3796-trunk-v1.txt;https://issues.apache.org/jira/secure/attachment/12512084/CASSANDRA-3796-trunk-v1.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-27 10:04:01.566,,,no_permission,,,,,,,,,,,,225503,,,Sat Jan 28 05:17:32 UTC 2012,,,,,,0|i0goc7:,95373,,,,,,,,,,,27/Jan/12 06:58;scode;Attaching patch. It would be great if someone from CASSANDRA-2392 could review and make sure I am not introducing a subtle bug by implementing the Comparator at the RowPosition level.,"27/Jan/12 10:04;slebresne;This is just a generics problem (for some reason Java 7 became more picky about this, one could almost call it a bug but anyway) so I'd prefer not adding real code to fix (though I don't think there was anything wrong with the patch per se). Committed http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=00613729 that adds enough generic info for the compiler to stop complaining. I'll let you check this fixes it for you too.

As a side note, the generics missing info was for DecoratedKey. DK has been generic for as long as I can remember but I don't really think this is of any use in that case, so it could be worth removing the type parameter from DK altogether, which would avoid adding a <?> every time. This would affect a high number of lines though (all those where we have DK<?>) so I've sticked to just adding the generic info for now.","28/Jan/12 05:17;scode;+1, works for me (and sounds good).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CounterColumn and CounterContext use a log4j logger instead of using slf4j like the rest of the code base,CASSANDRA-3603,12534529,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,09/Dec/11 19:29,12/Mar/19 14:01,13/Mar/19 22:26,17/Dec/11 04:52,1.0.7,,,,,,0,,,,,"(Will submit patch but not now, no time.)",,,,,,,,,,,,,,,,17/Dec/11 00:48;scode;CASSANDRA-3603-trunk.txt;https://issues.apache.org/jira/secure/attachment/12507748/CASSANDRA-3603-trunk.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-17 04:52:28.947,,,no_permission,,,,,,,,,,,,220251,,,Sat Dec 24 03:12:51 UTC 2011,,,,,,0|i0glxz:,94985,jbellis,jbellis,,,,,,,,,17/Dec/11 00:48;scode;Trivial patch applied.,17/Dec/11 04:52;jbellis;re-organized imports to match coding style and committed,24/Dec/11 03:12;scode;My apologies. Looks like I accidentally nuked projectCodeStyle.xml in the wc without realizing it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check for 0.0.0.0 is incorrect,CASSANDRA-3584,12534221,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,07/Dec/11 17:36,12/Mar/19 14:01,13/Mar/19 22:26,07/Dec/11 20:55,1.0.6,,,,,,0,,,,,"As noted by Jake in the comments to CASSANDRA-3214, we are using == for a String comparison.",,,,,,,,,,,,,,,,07/Dec/11 17:38;jbellis;3584.txt;https://issues.apache.org/jira/secure/attachment/12506487/3584.txt,07/Dec/11 18:46;tjake;3584_v2.txt;https://issues.apache.org/jira/secure/attachment/12506495/3584_v2.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-07 18:46:33.379,,,no_permission,,,,,,,,,,,,219943,,,Wed Dec 07 20:55:05 UTC 2011,,,,,,0|i0glpr:,94948,tjake,tjake,,,,,,,,,07/Dec/11 17:38;jbellis;fix attached,07/Dec/11 18:46;tjake;Added similar check to getSubSplits otherwise 0.0.0.0 can not be used.,07/Dec/11 19:24;jbellis;+1 after fixing whitespace in your operators :),07/Dec/11 20:55;tjake;committed with space fix,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
show schema fails,CASSANDRA-3415,12529245,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,28/Oct/11 13:13,12/Mar/19 14:01,13/Mar/19 22:26,20/Dec/11 10:58,0.8.10,1.0.7,,Legacy/Tools,,,0,,,,,"following command breaks ""show schema"" cli command with error ""A long is exactly 8 bytes: 5""

create column family resultcache with column_type = 'Super' and comparator = 'LongType' and  key_validation_class = 'UTF8Type' and subcomparator = 'AsciiType' and replicate_on_write = false and rows_cached = 700 and keys_cached = 30000 and key_cache_save_period = 0 and column_metadata = [ {column_name: id, validation_class: LongType}, {column_name: name, validation_class: 'AsciiType'}, {column_name: crc32, validation_class: LongType}, {column_name: size, validation_class: LongType} ];",freebsd,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-10-28 14:28:36.236,,,no_permission,,,,,,,,,,,,215105,,,Tue Dec 20 09:38:58 UTC 2011,,,,,,0|i0gjm7:,94608,,,,,,,,,,,28/Oct/11 13:16;hsn;1.0.0 has same bug,28/Oct/11 14:28;jbellis;please test 1.0.1 artifacts: http://people.apache.org/~slebresne/,28/Oct/11 16:53;hsn;bug in 1.0.1 too,03/Nov/11 06:26;jbellis;fixed in r1196956.  Thanks for the reproducible test case!,"03/Nov/11 06:26;jbellis;(note that the problem is only with the cli, the schema itself is fine.)","03/Nov/11 09:04;hudson;Integrated in Cassandra-0.8 #392 (See [https://builds.apache.org/job/Cassandra-0.8/392/])
    fix displaying cfdef entries for super columnfamilies
patch by jbellis for CASSANDRA-3415

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1196956
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
",20/Dec/11 09:38;hsn;This is fixed in 0.8 and 1.0 branches. Close ticket,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bloom filters should avoid huge array allocations to avoid fragmentation concerns,CASSANDRA-2466,12504130,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,m0nstermind,scode,scode,13/Apr/11 03:45,12/Mar/19 14:01,13/Mar/19 22:26,26/Oct/11 19:34,1.0.1,,,,,,0,,,,,"The fact that bloom filters are backed by single large arrays of longs is expected to interact badly with promotion of objects into old gen with CMS, due to fragmentation concerns (as discussed in CASSANDRA-2463).

It should be less of an issue than CASSANDRA-2463 in the sense that you need to have a lot of rows before the array sizes become truly huge. For comparison, the ~ 143 million row key limit implied by the use of 'int' in BitSet prior to the switch to OpenBitSet translates roughly to 238 MB (assuming the limitation factor there was the addressability of the bits with a 32 bit int, which is my understanding).

Having a preliminary look at OpenBitSet with an eye towards replacing the single long[] with multiple arrays, it seems that if we're willing to drop some of the functionality that is not used for bloom filter purposes, the bits[i] indexing should be pretty easy to augment with modulo to address an appropriate smaller array. Locality is not an issue since the bloom filter case is the worst possible case for locality anyway, and it doesn't matter whether it's one huge array or a number of ~ 64k arrays.

Callers may be affected like BloomFilterSerializer which cares about the underlying bit array.

If the full functionality of OpenBitSet is to be maintained (e.g., xorCount) some additional acrobatics would be necessary and presumably at a noticable performance cost if such operations were to be used in performance critical places.

An argument against touching OpenBitSet is that it seems to be pretty carefully written and tested and has some non-trivial details and people have seemingly benchmarked it quite carefully. On the other hand, the improvement would then apply to other things as well, such as the bitsets used to keep track of in-core pages (off the cuff for scale, a 64 gig sstable should imply a 2 mb bit set, with one bit per 4k page).


",,,,,,,,,,,,,,,CASSANDRA-2521,16/Oct/11 16:03;m0nstermind;2466v1.patch;https://issues.apache.org/jira/secure/attachment/12499202/2466v1.patch,17/Oct/11 19:34;m0nstermind;2466v2.patch;https://issues.apache.org/jira/secure/attachment/12499421/2466v2.patch,14/Oct/11 17:44;m0nstermind;BloomFilterSerializer.java;https://issues.apache.org/jira/secure/attachment/12499069/BloomFilterSerializer.java,14/Oct/11 06:52;m0nstermind;OpenBitSet.java;https://issues.apache.org/jira/secure/attachment/12498988/OpenBitSet.java,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2011-04-13 06:14:39.869,,,no_permission,,,,,,,,,,,,20640,,,Wed Oct 26 20:06:23 UTC 2011,,,,,,0|i0gbjj:,93300,jbellis,jbellis,,,,,,,,,"13/Apr/11 06:14;cscotta;Peter, it's interesting that you mention this. During the first run of the memory profiling I ran while investigating CASSANDRA-2463 on vanilla 0.7.4, I saw a tremendous amount of allocation of long[]/longs (comprising ~70% of heap allocations at that specific moment), but was not sure of the source. I doubt I'll have time to look into this at the office this week, but if I'm able to outside of work I may check into the allocation a bit.

If you're on it, by all means go for it. Thanks for mentioning this!","13/Apr/11 17:48;kingryan;Moving to smaller arrays would make the allocation easier, but wouldn't reduce the raw amount of memory needed for a large bloom filter.

Would it be worth moving these off-heap completely?","13/Apr/11 18:55;scode;Yes, it's not decreasing the total amount. The idea is that with small values, at least you're now left with a tweakable situation where a suitable heap size and a suitable initial occupancy trigger should be sufficient to avoid concurrent mode failures, as long as you're avoiding the fragmentation induced promotion failures (pragmatically, even if theoretically unclean, one can also help CMS along by inserting some minor application level pauses during allocations of many chunks as part of a larger allocation). But the memory pressure remains.

Regarding off-heap: I didn't suggest it because I'd personally like to avoid JNI/JNA if possible for ""safety"" reasons, and I would also like to avoid adding further dependencies on CMS sweeps for external resources (files, off-heap mem, etc) for the usual reasons. But maybe I'm more paranoid about that than most.

It would be really nice to just have an explicitly managed pool of fixed-size off-heap buffers of a reasonable size (say, a meg a piece, mmap():ed). I'm thinking maybe the bloom filters can be explicitly managed more easily than the mmaps/brafs for data reading; for example by accepting that for the few requests that race with the removal of an sstable, the bloom filter would just pretend all bits are set.

Hmmm...

",21/Apr/11 07:02;scode;If/once the bullet is bitten in CASSANDRA-2521 this should be much simpler to accomplish without the GC tradeoffs.,13/Oct/11 22:41;jbellis;The main problem with moving BF off-heap is that single-item gets from direct buffers are about 1/2 the speed of accessing an on-heap byte[].  That's a pretty big hit to take.,"14/Oct/11 06:19;scode;Good point. What's your feeling on the approach of modifying the bitset to use a number of small byte arrays? (Probably at some fixed size to help fragmentation.)

It does mean an additional level of indirection in an array of arrays, and it's not clear (to me at least) that it is expected to usually reside in CPU cache.
",14/Oct/11 06:52;m0nstermind;I made it already for our cassandra deployment (because we have huge bloom filters). Attaching it as is (will rebase it to head if you'll find it useful).,"14/Oct/11 12:47;jbellis;What effect did you observe when deploying this, Oleg?","14/Oct/11 14:44;m0nstermind;Promotion failures caused by bloom filters gone away completely.
 
I did not measured performance overhead specifically on bloom filters, so all I can tell that it was not introduced measureable performance degradation on production cluster operation from client perspective. 

(currently we have ~3300 thrift single column read reqs/sec per node with ~1ms avg call duration measured from client; 98% reads are bounced by bloom filters; size of single bloom filter is up to 250Mb)
","14/Oct/11 17:32;jbellis;Is serialization the same, or at least backwards-compatible w/ existing OBS BFs?","14/Oct/11 17:44;m0nstermind;You mean on-disk format ? Yes. Specifics are handled by BloomFilterSerializer. Attached serializer as implemented for 0.6.
(as far as i see no changes are required in BloomFilter itself)",14/Oct/11 17:58;jbellis;Are there any other moving parts to this? Could we ask you to submit a patch against trunk?,14/Oct/11 18:14;m0nstermind;Of course. I'll try to prepare it till monday.,14/Oct/11 21:27;jbellis;Great!,16/Oct/11 16:03;m0nstermind;Rebased to trunk. ant test passed.,"17/Oct/11 13:11;jbellis;Some comments:

{code}
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
{code}

Isn't one of those bounds checks redundant?  It looks like wlen/bitLength is the right one to use.

* lots of commented-out methods in OBS. Let's remove them entirely if we're not going to implement.
* OBS.setPage is unnecessary","17/Oct/11 19:41;m0nstermind;Attached v2:
Removed there commented out code as well as OBS.setPage.

regarding  ""i < pageSize && bitLength-- > 0"":
both bound checks are neccessary:
* ""i<pageSize"" ensures there are no out of bounds for every single page, while
* ""bitLength-- > 0"" ensures there are no more than neccessary bytes are written from the last page (otherwise extra zeroes will appear in file, which will break backwards compatibility).","25/Oct/11 18:52;jbellis;Patch looks good to me, just waiting for performance testing now.","26/Oct/11 19:34;jbellis;Tested (by Brandon) and committed. Thanks, Oleg!",26/Oct/11 19:49;scode;Awesome!,26/Oct/11 20:06;m0nstermind;Youre welcome ;-),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh missing help for INSERT,CASSANDRA-3718,12537933,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,10/Jan/12 18:03,12/Mar/19 14:01,13/Mar/19 22:26,10/Jan/12 20:23,1.0.7,,,Legacy/Tools,,,0,cqlsh,,,,this must have been overlooked.,,,,,,,,,,,,,,,,10/Jan/12 19:10;thepaul;3718.patch.txt;https://issues.apache.org/jira/secure/attachment/12510081/3718.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-10 20:23:07.233,,,no_permission,,,,,,,,,,,,223439,,,Tue Jan 10 20:23:07 UTC 2012,,,,,,0|i0gnef:,95221,,,,,,,,,,,10/Jan/12 19:10;thepaul;Fixed in https://github.com/thepaul/cassandra/commit/25f5a63dc3f0a34266d041bcb66a0e2a5ab7c7d9 ; patch attached here.,10/Jan/12 20:23;brandon.williams;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update POM generation after migration to git,CASSANDRA-3732,12538192,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,stephenc,slebresne,slebresne,12/Jan/12 13:26,12/Mar/19 14:01,13/Mar/19 22:26,03/Apr/12 16:41,1.0.10,1.1.0,,Packaging,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-04-03 08:32:28.564,,,no_permission,,,,,,,,,,,,223698,,,Thu Feb 20 15:46:11 UTC 2014,,,,,,0|i0gnjz:,95246,slebresne,slebresne,,,,,,,,,03/Apr/12 08:32;stephenc;https://github.com/apache/cassandra/pull/9,"03/Apr/12 15:32;slebresne;Why are those information useful?
Basically I have no clue how to test this, and I don't want to commit it without testing if this has a risk of breaking something for someone. Typically, there doesn't seem to be enough information in those metadata to know which git branch this is referring to.","03/Apr/12 15:37;stephenc;Shortcoming of the Maven SCM URI format is that Git branch info is lost.

The current SCM info points to SVN which is just plain wrong at this point!",03/Apr/12 15:39;stephenc;No risk of breaking anything for anyone. You are required to include the scm info in any artifacts that get published to Maven Central... the actual info will not be used by any programs... at best by a human,"03/Apr/12 16:41;slebresne;Ok +1 then, committed, thanks
","20/Feb/14 15:46;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/9
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
comments and documentation for index_interval are misleading,CASSANDRA-3074,12520009,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,mdennis,mdennis,mdennis,24/Aug/11 19:24,12/Mar/19 14:01,13/Mar/19 22:26,25/Aug/11 19:06,0.8.5,,,,,,1,,,,,"The comments and documentation for index_interval are misleading.  They state the larger the *sampling* the more effective the index as at the cost of space.  This is true, but in the context of the configuration variable it implies the larger the *setting* is the larger the index is while in fact it's the opposite of that.",,,,,,,,,,,,,,,,25/Aug/11 18:46;mdennis;3074-cassandra-0.8.patch;https://issues.apache.org/jira/secure/attachment/12491675/3074-cassandra-0.8.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-25 14:30:26.763,,,no_permission,,,,,,,,,,,,20957,,,Thu Aug 25 20:21:47 UTC 2011,,,,,,0|i0gfd3:,93919,jbellis,jbellis,,,,,,,,,25/Aug/11 14:30;jbellis;the proposed changes conflate the index entries themselves (always one per key) and the sampling rate (which is what index_interval affects).,25/Aug/11 18:45;mdennis;poor choice of words on my part.  new version attached.,"25/Aug/11 19:06;jbellis;committed, thanks!","25/Aug/11 20:21;hudson;Integrated in Cassandra-0.8 #295 (See [https://builds.apache.org/job/Cassandra-0.8/295/])
    clarify index_interval explanation
patch by mdennis for CASSANDRA-3074

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1161701
Files : 
* /cassandra/branches/cassandra-0.8/conf/cassandra.yaml
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LongCompactionSpeedTest running longer starting with builds on Aug31,CASSANDRA-3115,12520975,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,cdaw,cdaw,31/Aug/11 20:57,12/Mar/19 14:01,13/Mar/19 22:26,01/Sep/11 19:10,,,,,,,0,,,,,"The Long tests started consistently timing out as this build of cassandra: [https://jenkins.qa.datastax.com/job/CassandraLong/131/console]

The regression server shows pretty consistent run times for this test, and then consistent timeouts from this point forward.
{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 6, Failures: 0, Errors: 0, Time elapsed: 111.379 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 1637 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: 6144 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 2379 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=500000: 15690 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=500000 colsper=1: 20953 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=1000 colsper=5: 5672 ms
{code}

After increasing the timeout, the run time shows are now:
{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 6, Failures: 0, Errors: 0, Time elapsed: 409.486 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 2465 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: 29407 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 2456 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=500000: 14588 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=500000 colsper=1: 100794 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=1000 colsper=5: 19266 ms
{code}


*Single node local run:  Build 1056 / on Aug 30 / Macbook Pro w/ 8 GB ram (all apps shutdown)*
{panel}
+Run 1: Fresh install with no log or lib dir+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 850 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *3004 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 767 ms
    
+Run 2: Invoke test without restarting the server+
	[junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 826 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *3030 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 776 ms

+Run 3: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 830 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *2964 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 635 ms

+Run 4: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 931 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *2987 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 910 ms
{panel}

*Singled node local run: Build 1062 / on Aug 31 / Macbook pro w/ 8GB ram (all apps shutdown)*
{panel}
+Run 1: Fresh restart with no log or lib dir+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 802 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *17649 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 713 ms

+Run 2: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 832 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *16875 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 868 ms

+Run 3: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 809 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *16818 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 807 ms

+Run 4: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 834 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *16997 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 873 ms
{panel}


*Cassandra Build at time of test failure*
[https://jenkins.qa.datastax.com/job/Cassandra/168/changes]
{panel}
Revision: 1163394

* make Range and Bounds objects client-safe patch by Mck SembWever and jbellis for CASSANDRA-3108
* Catch invalid key_validation_class before instantiating UpdateColumnFamily patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3102
* Add validation that Keyspace names are case-insensitively unique patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3066
* merge from 0.7
* merge from 0.7
* merge from 0.7
* update CHANGES for #3023 and #3044

Revision 1163394 by jbellis: 
make Range and Bounds objects client-safe
patch by Mck SembWever and jbellis for CASSANDRA-3108
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/dht/Range.java
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/dht/Bounds.java

Revision 1163291 by xedin: 
Catch invalid key_validation_class before instantiating UpdateColumnFamily
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3102
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java

Revision 1163289 by xedin: 
Add validation that Keyspace names are case-insensitively unique
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3066
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/ClientState.java
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java

Revision 1163281 by jbellis: 
merge from 0.7
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
	/cassandra/branches/cassandra-0.8/conf/cassandra.yaml
	/cassandra/branches/cassandra-0.8/contrib
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
	/cassandra/branches/cassandra-0.8
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java

Revision 1163268 by jbellis: 
merge from 0.7
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
	/cassandra/branches/cassandra-0.8/conf/cassandra.yaml
	/cassandra/branches/cassandra-0.8/contrib
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
	/cassandra/branches/cassandra-0.8
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java

Revision 1163235 by jake: 
merge from 0.7
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
	/cassandra/branches/cassandra-0.8/contrib
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
	/cassandra/branches/cassandra-0.8
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/GCInspector.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java

Revision 1163205 by jbellis: 
update CHANGES for #3023 and #3044
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/utils/BloomFilter.java
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/utils/BloomCalculations.java
{panel}

","Cassandra-0.8 branch, nightly builds.
MacOS and Debian",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-09-01 19:04:35.395,,,no_permission,,,,,,,,,,,,4083,,,Fri Sep 02 07:22:58 UTC 2011,,,,,,0|i0gfyv:,94017,,,,,,,,,,,"01/Sep/11 19:04;brandon.williams;git bisect blames http://svn.apache.org/viewvc?view=rev&revision=1163205

The only obvious candidate there is the added debug statement, and indeed removing it allows the test to pass again.  Since the test passes in under 2 minutes but the timeout is 5, this is likely the per-row BF logging causing it.  Jonathan suggested that we move to the async appender: http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/AsyncAppender.html",01/Sep/11 19:10;brandon.williams;For now we decided the simplest thing to do is push the log statement to TRACE.  Done in r1164213.,"02/Sep/11 07:22;hudson;Integrated in Cassandra-0.8 #310 (See [https://builds.apache.org/job/Cassandra-0.8/310/])
    Push BF debug logging to trace to fix LongCompactionSpeedTest timeouts.
Patch by brandonwilliams for CASSANDRA-3115

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1164213
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/utils/BloomFilter.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
convert mmap assertion to if/throw,CASSANDRA-2417,12503397,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,05/Apr/11 14:06,12/Mar/19 14:01,13/Mar/19 22:26,05/Apr/11 17:29,0.7.5,,,,,,0,,,,,"This will allow scrub to catch this:

{noformat}
java.lang.AssertionError: mmap segment underflow; remaining is 73936639 but 1970430821 requested

                at org.apache.cassandra.io.util.MappedFileDataInput.readBytes(MappedFileDataInput.java:119)

                at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:315)

                at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:272)

                at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:76)
{noformat}",,,,,,,,,,,,,,,,05/Apr/11 14:11;jbellis;2417.txt;https://issues.apache.org/jira/secure/attachment/12475480/2417.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-05 17:28:47.077,,,no_permission,,,,,,,,,,,,20611,,,Tue Apr 05 18:32:16 UTC 2011,,,,,,0|i0gb93:,93253,slebresne,slebresne,,,,,,,,,05/Apr/11 17:28;slebresne;+1 (committed to 0.7 and trunk),"05/Apr/11 17:44;hudson;Integrated in Cassandra-0.7 #421 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/421/])
    Convert mmap assertion to if/throw
patch by jbellis; reviewed by slebresne for CASSANDRA-2417
","05/Apr/11 18:32;hudson;Integrated in Cassandra #829 (See [https://hudson.apache.org/hudson/job/Cassandra/829/])
    Merge CASSANDRA-2417 from 0.7
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""count"" doesn't accept UUIDs in CLI even though ""get"" does",CASSANDRA-2902,12514246,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,mdennis,mdennis,15/Jul/11 16:28,12/Mar/19 14:01,13/Mar/19 22:26,15/Jul/11 22:18,0.8.2,,,,,,0,,,,,"[default@V360HC_SCHEMA1] get RawValues[7dc75c1c-8af0-462a-a920-bc1dafc44f31] limit 1;
=> (column=1310593550317, value=aced00057709053fe9cc17a95b9093, timestamp=1310593550583438)
Returned 1 results.

[default@V360HC_SCHEMA1] count RawValues[7dc75c1c-8af0-462a-a920-bc1dafc44f31];
UUIDs must be exactly 16 bytes
",,,,,,,,,,,,,,,,15/Jul/11 21:05;xedin;CASSANDRA-2902.patch;https://issues.apache.org/jira/secure/attachment/12486682/CASSANDRA-2902.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-15 21:05:56.649,,,no_permission,,,,,,,,,,,,20885,,,Fri Jul 15 23:22:01 UTC 2011,,,,,,0|i0ge5j:,93723,brandon.williams,brandon.williams,,,,,,,,,"15/Jul/11 21:05;xedin;Proper support of key validation class and function calls for COUNT statement.

Rebased with latest cassandra-0.8 branch (last commit c1abc30b8c5deb67a2f13f0abe15868da24cb350)",15/Jul/11 22:18;brandon.williams;Committed.,"15/Jul/11 23:22;hudson;Integrated in Cassandra-0.8 #217 (See [https://builds.apache.org/job/Cassandra-0.8/217/])
    Proper support of key validation class and function calls for COUNT in
the cli.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2902

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147336
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rows that don't exist get cached,CASSANDRA-2723,12508780,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,31/May/11 01:54,12/Mar/19 14:01,13/Mar/19 22:26,31/May/11 02:10,0.8.1,,,,,,0,,,,,"We noticed that rows that don't exist were getting cached anyway. We end up storing an empty CF in cache.


",,,,,,,,,,,,,,,,31/May/11 02:02;lenn0x;0001-Do-not-cache-rows-that-do-not-exist-v2.patch;https://issues.apache.org/jira/secure/attachment/12480890/0001-Do-not-cache-rows-that-do-not-exist-v2.patch,31/May/11 01:58;lenn0x;0001-Do-not-cache-rows-that-do-not-exist.patch;https://issues.apache.org/jira/secure/attachment/12480889/0001-Do-not-cache-rows-that-do-not-exist.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-31 02:03:44.189,,,no_permission,,,,,,,,,,,,20790,,,Tue May 31 03:52:17 UTC 2011,,,,,,0|i0gd2f:,93547,stuhood,stuhood,,,,,,,,,"31/May/11 02:03;stuhood;+1, although I'd move the relevant portion of the comment above {{return returnCF;}} near the new return.","31/May/11 02:10;lenn0x;Thanks, commited based on comments.","31/May/11 03:52;hudson;Integrated in Cassandra-0.8 #147 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/147/])
    Fixed rows being cached if they do not exist.
patch by goffinet; reviewed by stuhood for CASSANDRA-2723

goffinet : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1129462
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossip state is not removed after a new IP takes over a token,CASSANDRA-3071,12519848,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,23/Aug/11 16:59,12/Mar/19 14:01,13/Mar/19 22:26,23/Aug/11 18:11,0.7.9,0.8.5,,,,,0,,,,,"When a new node takes over a token, the endpoint state in the gossiper is never removed for the old node.  ",,,,,,,,,,,,,,,,23/Aug/11 17:38;brandon.williams;3071-v2.txt;https://issues.apache.org/jira/secure/attachment/12491369/3071-v2.txt,23/Aug/11 17:00;brandon.williams;3071.txt;https://issues.apache.org/jira/secure/attachment/12491359/3071.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-23 17:10:03.752,,,no_permission,,,,,,,,,,,,20956,,,Tue Aug 23 18:29:18 UTC 2011,,,,,,0|i0gfbz:,93914,,,,,,,,,,,"23/Aug/11 17:00;brandon.williams;This was originally part of a patch in CASSANDRA-957, but looks worthy enough to break out and get committed in older versions.","23/Aug/11 17:10;jbellis;Is it possible for this to remove an endpoint that it shouldn't?

E.g., X has token T

X moves to token U but node N was down

N comes back up, thinks T and U are both owned by X

node Y takes token T

we remove X from gossip","23/Aug/11 17:17;brandon.williams;bq. N comes back up, thinks T and U are both owned by X

I don't think this can happen.  When N starts up, it will load the persisted tokens, BUT they won't be associated with IPs.  It can only learn that U is owned by X via gossip, and T will be down until it learns about Y.",23/Aug/11 17:21;jbellis;what behavior does this fix?  just gossip trying to reach the old node?,23/Aug/11 17:29;brandon.williams;I think it solves this: http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Completely-removing-a-node-from-the-cluster-td6705079.html  Jeremy also reported a problem with a large amount of hints that I think this solves since SP.shouldHint is directly impacted by this.,23/Aug/11 17:38;brandon.williams;v2 adds more protection around shouldHint by checking that the endpoint is a member.  ,"23/Aug/11 17:45;jbellis;If the node doesn't have a token, it doesn't matter if it's a gossip member, it won't be part of getWriteEndpoints and won't be hinted anyway.","23/Aug/11 17:51;jbellis;+1 on v1, -1 on conflating gossip membership w/ token ownership as in v2","23/Aug/11 18:11;brandon.williams;bq. If the node doesn't have a token, it doesn't matter if it's a gossip member, it won't be part of getWriteEndpoints and won't be hinted anyway.

I can't see a way for that either, but I'm still suspicious of the link to shouldHint.

bq. +1 on v1, -1 on conflating gossip membership w/ token ownership as in v2

Fair enough, committed v1.","23/Aug/11 18:29;hudson;Integrated in Cassandra-0.7 #541 (See [https://builds.apache.org/job/Cassandra-0.7/541/])
    Remove gossip state when a new IP takes over a token.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-3071

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1160825
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/StorageService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace token leaves the old node state in tact causing problems in cli,CASSANDRA-3259,12524654,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,26/Sep/11 15:51,12/Mar/19 14:01,13/Mar/19 22:26,30/Sep/11 21:48,0.8.7,1.0.0,,,,,0,,,,,"in the replace token patch we dont evict the node from the Gossip which will leave the node lingering around and causes issues in cli (UNReachable nodes)

As a part of the replace token if the token is replaced with another token we should remove the old nodes Gossip states.",JVM on CentOS,,,,,,,,,,,,,,,30/Sep/11 21:23;vijay2win@yahoo.com;0001-evict-replaced-node-immediately-v2.patch;https://issues.apache.org/jira/secure/attachment/12497221/0001-evict-replaced-node-immediately-v2.patch,30/Sep/11 05:39;vijay2win@yahoo.com;0001-evict-replaced-node-immediately.patch;https://issues.apache.org/jira/secure/attachment/12497115/0001-evict-replaced-node-immediately.patch,01/Oct/11 00:54;vijay2win@yahoo.com;0002-evict-during-replace-token-for-1.0.patch;https://issues.apache.org/jira/secure/attachment/12497254/0002-evict-during-replace-token-for-1.0.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-09-30 19:35:01.207,,,no_permission,,,,,,,,,,,,1832,,,Tue Oct 04 21:02:35 UTC 2011,,,,,,0|i0ghqn:,94304,brandon.williams,brandon.williams,,,,,,,,,30/Sep/11 05:39;vijay2win@yahoo.com;This patch fixes this issue. We can immediately delete the Application states of the token which we just replaced so that we dont gossip about it.,"30/Sep/11 19:35;brandon.williams;If this is causing problems in the cli, the cli is doing something wrong, because removeEndpoint already takes it out of the Gossiper's unreachable nodes.  What problem is this solving?

We do need to expire remove this endpoint but I don't think exposing evictFromMembership is the best way.","30/Sep/11 19:40;vijay2win@yahoo.com;The problem is that, when we do the replace token we dont change the status of the old node (to removed ip or something)... hence the Node will get the State back as it is still gossiping about the replaced node. Even if removeToken() the states are not removed and hence if someone is talking about this node after (Async evict by doStatus()) this node will be added back.... hence the node will never go out of the Application State. This patch will instead will evict immediately when it sees a irrelevant host. This is simpler solution than adding another status like ""remove ip"". This works as expected in my tests.",30/Sep/11 21:23;vijay2win@yahoo.com;v2 adds abstraction to evict based on brandon's comments in IRC.,"30/Sep/11 21:48;brandon.williams;Committed, thanks","30/Sep/11 22:14;hudson;Integrated in Cassandra-0.8 #352 (See [https://builds.apache.org/job/Cassandra-0.8/352/])
    Evict gossip state immediately when a token is taken over.
Patch by vijay, reviewed by brandonwilliams for CASSANDRA-3259

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177847
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
","01/Oct/11 00:52;vijay2win@yahoo.com;Hi Brandon, Really sorry about extra work... i missed one change for 1.0 i was separating these 2 patches and some how i missed it.... ","03/Oct/11 08:59;slebresne;This hasn't been merged up to 1.0 yet so it should probably be (with the last changes from Vijay?), and it could probably use an entry in the changelog when doing so.",04/Oct/11 21:02;brandon.williams;Done and done.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliClient print memtable threshold in incorrect order,CASSANDRA-2983,12516133,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,cywjackson,cywjackson,01/Aug/11 23:15,12/Mar/19 14:01,13/Mar/19 22:26,02/Aug/11 00:01,0.8.3,,,,,,0,,,,,"as a continuation from #2839 looks like it was incorrectly merged into 0.8 as well, hence affecting > 0.8.2.

for trunk, this is also changed (time is taken out). So I guess format wise, we would stick with the fixed format in 0.7.7 per #2839 , which is:

{code}
sessionState.out.printf(""      Memtable thresholds: %s/%s/%s (millions of ops/minutes/MB)%n"",
    cf_def.memtable_operations_in_millions, cf_def.memtable_flush_after_mins, cf_def.memtable_throughput_in_mb);
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-08-02 00:01:46.373,,,no_permission,,,,,,,,,,,,20923,,,Tue Aug 02 00:01:46 UTC 2011,,,,,,0|i0genr:,93805,,,,,,,,,,,02/Aug/11 00:01;brandon.williams;Fixed in r1152967,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutboundTcpConnection throws RuntimeException,CASSANDRA-3235,12523869,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,21/Sep/11 14:22,12/Mar/19 14:01,13/Mar/19 22:26,21/Sep/11 15:10,1.0.0,,,,,,0,,,,,"Regression introduced in CASSANDRA-1788, as reported by liangfeng on the user list.",,,,,,,,,,,,,,,,21/Sep/11 14:24;jbellis;3235.txt;https://issues.apache.org/jira/secure/attachment/12495369/3235.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-21 14:36:26.51,,,no_permission,,,,,,,,,,,,3530,,,Wed Sep 21 15:10:45 UTC 2011,,,,,,0|i0ghg7:,94257,slebresne,slebresne,,,,,,,,,21/Sep/11 14:36;slebresne;+1,21/Sep/11 15:10;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getRangeToEndpointMap() method removed,CASSANDRA-3106,12520793,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,30/Aug/11 19:31,12/Mar/19 14:01,13/Mar/19 22:26,31/Aug/11 19:57,1.0.0,,,Legacy/Tools,,,0,,,,,"When getRangeToRPCAddress was added, getRangeToEndpointMap was removed, however, both are useful. We should add it back.",,,,,,,,,,,,,,,,30/Aug/11 19:32;nickmbailey;0001-Add-getRangeToEndpointMap-method-back.patch;https://issues.apache.org/jira/secure/attachment/12492287/0001-Add-getRangeToEndpointMap-method-back.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-31 14:58:11.766,,,no_permission,,,,,,,,,,,,4090,,,Wed Aug 31 20:54:32 UTC 2011,,,,,,0|i0gfs7:,93987,brandon.williams,brandon.williams,,,,,,,,,"30/Aug/11 19:32;nickmbailey;Adds back getRangeToEndpointMap method.

Incidentally fixed trailing whitespace in both files since my vim setup does that automatically.",31/Aug/11 14:58;jbellis;Why isn't rpcaddress sufficient?,31/Aug/11 15:38;nickmbailey;The rpc address of a node doesn't really give any guarantee of uniqueness.,31/Aug/11 19:57;brandon.williams;Committed.,"31/Aug/11 20:54;hudson;Integrated in Cassandra #1063 (See [https://builds.apache.org/job/Cassandra/1063/])
    Restore getRangeToEndpointMap.
Patch by Nick Bailey, reviewed by brandonwilliams for CASSANDRA-3106

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163772
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageServiceMBean.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cli silently fails when classes are quoted,CASSANDRA-2899,12514105,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,14/Jul/11 18:38,12/Mar/19 14:01,13/Mar/19 22:26,15/Jul/11 16:08,0.7.8,0.8.2,,,,,0,,,,,"For example: CREATE COLUMN FAMILY autocomplete_meta WITH comparator = 'UTF8Type' AND default_validation_class = 'UTF8Type' AND key_validation_class = 'UTF8Type'

Neither validation class is actually set, but if you remove the quotes everything works.",,,,,,,,,,,,,,,,15/Jul/11 12:19;xedin;CASSANDRA-2899.patch;https://issues.apache.org/jira/secure/attachment/12486591/CASSANDRA-2899.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-15 12:19:59.307,,,no_permission,,,,,,,,,,,,20884,,,Fri Jul 15 20:32:04 UTC 2011,,,,,,0|i0ge4n:,93719,brandon.williams,brandon.williams,,,,,,,,,15/Jul/11 12:19;xedin;rebase with cassandra-0.8 (latest commit c3d275800284acca6300595db268ca0c92b9a581),15/Jul/11 12:21;xedin;can be applied on both 0.7 and 0.8,"15/Jul/11 16:08;brandon.williams;Committed, thanks!","15/Jul/11 20:32;hudson;Integrated in Cassandra-0.7 #529 (See [https://builds.apache.org/job/Cassandra-0.7/529/])
    Allow quoted class names in the cli.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2899

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147210
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/cli/CliClient.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Small typos in the cli,CASSANDRA-2839,12512152,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,j.casares,j.casares,j.casares,29/Jun/11 00:08,12/Mar/19 14:01,13/Mar/19 22:26,07/Jul/11 13:52,0.7.7,,,,,,0,,,,,"Memtable thresholds: %s/%s/%s (millions of ops/minutes/MB) was displaying ops/MB/minutes.

placement_strategy: the fully qualified class used to place replicas in
                        this keyspace. Valid values are
                        org.apache.cassandra.locator.SimpleStrategy,
                        org.apache.cassandra.locator.NetworkTopologyStrategy,
                        and org.apache.cassandra.locator.OldNetworkTopologyStrategy
was being displayed but would only accept SimpleStrategy.

",,,,,,,,,,,,,,,,29/Jun/11 00:10;j.casares;2839.diff;https://issues.apache.org/jira/secure/attachment/12484582/2839.diff,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-06 14:51:32.652,,,no_permission,,,,,,,,,,,,20858,,,Thu Jul 07 13:52:36 UTC 2011,,,,,,0|i0gdrr:,93661,,,,,,,,,,,"06/Jul/11 14:51;slebresne;I've committed the typo on the memtable thresholds. On the placement_strategy help, are you sure ? Because CliTest does use a fully qualified class and don't seem to fail.","06/Jul/11 15:02;hudson;Integrated in Cassandra-0.7 #523 (See [https://builds.apache.org/job/Cassandra-0.7/523/])
    Fix typo in CLI help
patch by j.casares; reviewed by slebresne for CASSANDRA-2839

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1143444
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/cli/CliClient.java
","06/Jul/11 17:58;j.casares;This is my output of my test on cassandra-0.7.6 just right now:

[default@unknown] create keyspace abc with placement_strategy=org.apache.cassandra.locator.SimpleStrategy;
Syntax error at position 47: missing EOF at '.'
[default@unknown] create keyspace abc with placement_strategy=org.apache.cassandra.locator.NetworkTopologyStrategy;
Syntax error at position 47: missing EOF at '.'
[default@unknown] create keyspace abc with placement_strategy=org.apache.cassandra.locator.OldNetworkTopologyStrategy;
Syntax error at position 47: missing EOF at '.'
[default@unknown] create keyspace abc with placement_strategy=SimpleStrategy;                                         
844d2b67-a7f9-11e0-b6a3-e700f669bcfc
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] update keyspace abc with placement_strategy=NetworkTopologyStrategy;
9182d288-a7f9-11e0-b6a3-e700f669bcfc
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] update keyspace abc with placement_strategy=OldNetworkTopologyStrategy;
96344ca9-a7f9-11e0-b6a3-e700f669bcfc
Waiting for schema agreement...
... schemas agree across the cluster
",06/Jul/11 18:15;slebresne;You'd have to use placement_strategy='org.apache.cassandra.locator.SimpleStrategy',06/Jul/11 18:34;j.casares;Works! Could we get the help lines to reflect this?,"07/Jul/11 13:52;slebresne;Help is updated, though the example section was pretty clear already.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RPM classpath evaluation include current directory (-cp:),CASSANDRA-2881,12513657,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,wmeler,wmeler,wmeler,11/Jul/11 12:12,12/Mar/19 14:01,13/Mar/19 22:26,22/Aug/11 21:54,0.8.5,,,Packaging,,,0,classpath,rpm,,,"/usr/share/cassandra/cassandra.in.sh builds CLASSPATH in a way that cause current directory inclusion (-cp:).
This should be avioded as can effect in config file change if one is present in current directory.",,,,,,,,,,,,,,,,22/Aug/11 21:10;thepaul;classpath-both.patch;https://issues.apache.org/jira/secure/attachment/12491263/classpath-both.patch,11/Jul/11 12:29;wmeler;classpath.patch;https://issues.apache.org/jira/secure/attachment/12486051/classpath.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-22 21:10:18.724,,,no_permission,,,,,,,,,,,,20879,,,Mon Aug 22 22:22:28 UTC 2011,,,,,,0|i0ge0f:,93700,thepaul,thepaul,,,,,,,,,"22/Aug/11 21:10;thepaul;Adds the same protection to the cassandra.in.sh snippet for Debian.

bin/cassandra.in.sh doesn't need it.

+1",22/Aug/11 21:54;jbellis;committed,"22/Aug/11 22:22;hudson;Integrated in Cassandra-0.8 #289 (See [https://builds.apache.org/job/Cassandra-0.8/289/])
    avoid including cwd in classpath for deb and rpm packages
patch by Wojciech Meler and Paul Cannon for CASSANDRA-2881

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1160459
Files : 
* /cassandra/branches/cassandra-0.8/debian/cassandra.in.sh
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/redhat/cassandra.in.sh
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
generate-eclipse-files still referencing drivers/ source,CASSANDRA-2764,12510076,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,kirktrue,kirktrue,kirktrue,13/Jun/11 05:00,12/Mar/19 14:01,13/Mar/19 22:26,13/Jun/11 11:59,1.0.0,,,,,,0,,,,,"In trunk, running ant generate-eclipse-files will reference the old drivers top-level directory. The result is that the generated project, once loaded into Eclipse causes errors about the non-existent source directories.",,,,,,,,,,,,,,,,13/Jun/11 05:01;kirktrue;trunk-2764.txt;https://issues.apache.org/jira/secure/attachment/12482322/trunk-2764.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-13 11:59:22.44,,,no_permission,,,,,,,,,,,,20814,,,Tue Jun 14 17:02:37 UTC 2011,,,,,,0|i0gdbb:,93587,,,,,,,,,,,13/Jun/11 05:01;kirktrue;Patch which removes the two directories from the Eclipse project generation.,"13/Jun/11 11:59;jbellis;committed, thanks!","13/Jun/11 12:20;hudson;Integrated in Cassandra #923 (See [https://builds.apache.org/job/Cassandra/923/])
    r/m drivers/ from generate-eclipse-files target
patch by Kirk True for CASSANDRA-2764

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1135104
Files : 
* /cassandra/trunk/build.xml
","14/Jun/11 17:02;hudson;Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exception generate when using same index names,CASSANDRA-2730,12509001,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,wubn2000,wubn2000,01/Jun/11 17:13,12/Mar/19 14:01,13/Mar/19 22:26,05/Jun/11 14:08,0.8.1,,,,,,0,cql,,,,"when using cqlsh tool to generate indexes, for example, suppose we have a column family Tuser, which has two columns: name and state.
cqlsh> create index name_key on Tuser(name);
cqlsh> create index name_key on Tuser(state);
note that name_key is used twice by mistake, then a javax.management.InstanceAlreadyExistsException will be thrown and this exception will prevent cassandra service from starting any more.
",,,,,,,,,,,,,,,,01/Jun/11 19:43;jbellis;2730.txt;https://issues.apache.org/jira/secure/attachment/12481136/2730.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-01 18:30:08.424,,,no_permission,,,,,,,,,,,,20794,,,Sun Jun 05 14:08:35 UTC 2011,,,,,,0|i0gd3z:,93554,,,,,,,,,,,"01/Jun/11 18:30;cdaw;*For clarity, this occurs only when you create the same named index referencing different columns*
* Workaround #1:  In CQLSH:  Prior to restarting server, drop this column family after you see this error, then future server restarts will be fine.
* Workaround #2: In cassandra-cli:  Prior to restarting server, run the ""drop index cf column"" command for both attempts (birth_year and session_token in my example), then future server restarts will be fine.

{code}
cqlsh> CREATE INDEX birth_year_key ON users (session_token);
Bad Request: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=IndexColumnFamilies,keyspace=cqldb,columnfamily=users.birth_year_key
{code}

*PASS: CQLSH allows multiple indexes without an issue*
{code}
cqlsh> CREATE INDEX gender_key ON users (gender);
cqlsh> CREATE INDEX state_key ON users (state);
cqlsh> CREATE INDEX birth_year_key ON users (birth_year);
{code}

*PASS: CQLSH correctly errors out creating the same index twice*
{code}
cqlsh> CREATE INDEX birth_year_key ON users (birth_year);
Bad Request: Index exists
{code}


","01/Jun/11 19:36;wubn2000;so this ""drop index"" is in 0.8.1? I can't find it in my 0.8.0-rc1.",01/Jun/11 19:42;jbellis;correct,"01/Jun/11 19:43;jbellis;Attached patch fixes the problem, but requires CASSANDRA-2617 to be finished first.",05/Jun/11 14:08;jbellis;fix rolled into CASSANDRA-2617 patch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Allow LOCAL_QUORUM, EACH_QUORUM CLs to work w/ any Strategy class",CASSANDRA-2516,12504742,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,20/Apr/11 02:00,12/Mar/19 14:01,13/Mar/19 22:26,21/Apr/11 15:19,0.7.5,,,,,,0,,,,,,,,,,,,,,,,,,,,,20/Apr/11 02:01;jbellis;2516.txt;https://issues.apache.org/jira/secure/attachment/12476817/2516.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-21 12:47:54.258,,,no_permission,,,,,,,,,,,,20669,,,Thu Apr 21 18:52:40 UTC 2011,,,,,,0|i0gbtr:,93346,slebresne,slebresne,,,,,,,,,"21/Apr/11 12:47;slebresne;+1
(but note that it's a 0.8 patch, so it should probably be rebased to 0.7 and applied there first)","21/Apr/11 15:19;jbellis;committed to 0.7, 0.8","21/Apr/11 18:52;hudson;Integrated in Cassandra-0.7 #452 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/452/])
    support LOCAL_QUORUM, EACH_QUORUM CLs outside of NTS
patch by jbellis; reviewed by slebresne for CASSANDRA-2516
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatabaseDescriptor.defsVersion should be volatile,CASSANDRA-2490,12504433,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,paladin8,paladin8,paladin8,16/Apr/11 04:57,12/Mar/19 14:01,13/Mar/19 22:26,16/Apr/11 16:15,0.7.5,,,,,,0,,,,,"(Probably affects other versions, but I am on 0.7.3).

DatabaseDescriptor.defsVersion should be protected by volatile since it is written to and read by multiple threads from unsynchronized methods. This can manifest itself in schema agreement never occurring due to a node broadcasting the wrong schema version.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-04-16 16:14:52.854,,,no_permission,,,,,,,,,,,,20651,,,Sat Apr 16 16:36:47 UTC 2011,,,,,,0|i0gbo7:,93321,jbellis,jbellis,,,,,,,,,16/Apr/11 16:14;jbellis;committed volatile change in r1094011. thanks!,"16/Apr/11 16:36;hudson;Integrated in Cassandra-0.7 #435 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/435/])
    make DD.defsVersion volatile
patch by Jeffrey Wang; reviewed by jbellis for CASSANDRA-2490
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG_OPTS bash variable interpolation doesn't work correctly when PIG_OPTS is set in the environment.,CASSANDRA-3160,12522273,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,eldondev,eldondev,08/Sep/11 16:01,12/Mar/19 14:01,13/Mar/19 22:26,08/Sep/11 17:12,0.8.6,1.0.0,,,,,0,,,,,PIG_OPTS bash variable interpolation doesn't work correctly when PIG_OPTS is set in the environment due to variable preceding quotes.,bash,,,,,,,,,,,,,,,08/Sep/11 16:08;eldondev;CASSANDRA-3160.patch;https://issues.apache.org/jira/secure/attachment/12493608/CASSANDRA-3160.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-08 17:12:55.93,,,no_permission,,,,,,,,,,,,4054,,,Thu Sep 08 17:33:00 UTC 2011,,,,,,0|i0ggj3:,94108,brandon.williams,brandon.williams,,,,,,,,,"08/Sep/11 17:12;brandon.williams;Committed, thanks.","08/Sep/11 17:33;hudson;Integrated in Cassandra-0.8 #321 (See [https://builds.apache.org/job/Cassandra-0.8/321/])
    Fix interpolation of PIG_OPTS.
Patch by Eldon Stegall, reviewed by brandonwilliams for CASSANDRA-3160

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166809
Files : 
* /cassandra/branches/cassandra-0.8/contrib/pig/bin/pig_cassandra
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple classpath entries in the cassandra-all.jar,CASSANDRA-3159,12522269,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,tjake,tjake,08/Sep/11 14:59,12/Mar/19 14:01,13/Mar/19 22:26,08/Sep/11 17:22,1.0.0,,,,,,0,,,,,"from CASSANDRA-2936

{code}
Sep 8, 2011 8:47:45 AM java.util.jar.Attributes read
WARNING: Duplicate name in Manifest: Class-Path.
Ensure that the manifest does not have duplicate entries, and
that blank lines separate individual sections in both your
manifest and in the META-INF/MANIFEST.MF entry in the jar file.
{code}",,,,,,,,,,,,,,,,08/Sep/11 15:24;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3159-remove-extra-cp-entry.txt;https://issues.apache.org/jira/secure/attachment/12493603/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3159-remove-extra-cp-entry.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-08 15:26:08.356,,,no_permission,,,,,,,,,,,,4055,,,Thu Sep 08 18:23:09 UTC 2011,,,,,,0|i0ggiv:,94107,,,,,,,,,,,"08/Sep/11 15:26;urandom;the attached patch merges the two classpath entries, and also renames the cookie jar to clientutil (a previous change somehow lost).",08/Sep/11 15:41;tjake;+1,08/Sep/11 17:22;urandom;committed.,"08/Sep/11 18:23;hudson;Integrated in Cassandra #1091 (See [https://builds.apache.org/job/Cassandra/1091/])
    remove extra cp entry

Patch by eevans; reviewed by tjake for CASSANDRA-3159

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166812
Files : 
* /cassandra/trunk/build.xml
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more data type checks when storing data from Pig to Cassandra,CASSANDRA-3356,12526977,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,jeromatron,jeromatron,13/Oct/11 05:21,12/Mar/19 14:00,13/Mar/19 22:26,22/Feb/12 01:21,,,,,,,0,pig,,,,"Pre-CASSANDRA-2810, the decompose method was tried before assuming the data was of type DataByteArray.  It needs to have both - first a check to see if it can be decomposed.  If that doesn't work (exception) it can try the cast.  If that doesn't work, then we can't store the object - or as Brandon says, we need to put another special case in ObjToBB.  This is all based on a conversation in IRC with Brandon Williams, Jacob Perkins, and Jeremy Hanna about a problem arising when trying to store an object of type long to Cassandra.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-02-22 01:21:40.866,,,no_permission,,,,,,,,,,,,85290,,,Wed Feb 22 01:21:40 UTC 2012,,,,,,0|i0givz:,94490,,,,,,,,,,,22/Feb/12 01:21;brandon.williams;Think I got this one in CASSANDRA-3886.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL CF creation skips most of the validation code,CASSANDRA-3565,12533699,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Dec/11 23:08,12/Mar/19 14:00,13/Mar/19 22:26,02/Dec/11 23:42,1.0.6,,,Legacy/CQL,,,0,cql,,,,"Most validation is done by ThriftValidation.validateCfDef, which we call from QP when creating an index but not on CF creation.",,,,,,,,,,,,,,,,02/Dec/11 23:09;jbellis;3565.txt;https://issues.apache.org/jira/secure/attachment/12505941/3565.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-02 23:42:45.539,,,no_permission,,,,,,,,,,,,219426,,,Fri Dec 02 23:42:45 UTC 2011,,,,,,0|i0glh3:,94909,xedin,xedin,,,,,,,,,02/Dec/11 23:09;jbellis;patch adds validateCfDef to QP CF creation.,02/Dec/11 23:42;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InvocationTargetException ConcurrentModificationException at startup,CASSANDRA-3417,12529294,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,scode,j.casares,j.casares,28/Oct/11 18:33,12/Mar/19 14:00,13/Mar/19 22:26,13/Feb/12 21:45,1.0.8,,,,,,0,,,,,"I was starting up the new DataStax AMI where the seed starts first and 34 nodes would latch on together. So far things have been working decently for launching, but right now I just got this during startup.


{CODE}
ubuntu@ip-10-40-190-143:~$ sudo cat /var/log/cassandra/output.log 
 INFO 09:24:38,453 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26
 INFO 09:24:38,456 Heap size: 1936719872/1937768448
 INFO 09:24:38,457 Classpath: /usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.4.0.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/joda-time-1.6.2.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.6.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.3.jar:/usr/share/cassandra/apache-cassandra-1.0.0.jar:/usr/share/cassandra/apache-cassandra-thrift-1.0.0.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar
 INFO 09:24:39,891 JNA mlockall successful
 INFO 09:24:39,901 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 09:24:40,057 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:24:40,069 Global memtable threshold is enabled at 616MB
 INFO 09:24:40,159 EC2Snitch using region: us-east, zone: 1d.
 INFO 09:24:40,475 Creating new commitlog segment /raid0/cassandra/commitlog/CommitLog-1319793880475.log
 INFO 09:24:40,486 Couldn't detect any schema definitions in local storage.
 INFO 09:24:40,486 Found table data in data directories. Consider using the CLI to define your schema.
 INFO 09:24:40,497 No commitlog files found; skipping replay
 INFO 09:24:40,501 Cassandra version: 1.0.0
 INFO 09:24:40,502 Thrift API version: 19.18.0
 INFO 09:24:40,502 Loading persisted ring state
 INFO 09:24:40,506 Starting up server gossip
 INFO 09:24:40,529 Enqueuing flush of Memtable-LocationInfo@1388314661(190/237 serialized/live bytes, 4 ops)
 INFO 09:24:40,530 Writing Memtable-LocationInfo@1388314661(190/237 serialized/live bytes, 4 ops)
 INFO 09:24:40,600 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-1-Data.db (298 bytes)
 INFO 09:24:40,613 Ec2Snitch adding ApplicationState ec2region=us-east ec2zone=1d
 INFO 09:24:40,621 Starting Messaging Service on /10.40.190.143:7000
 INFO 09:24:40,628 Joining: waiting for ring and schema information
 INFO 09:24:43,389 InetAddress /10.194.29.156 is now dead.
 INFO 09:24:43,391 InetAddress /10.85.11.38 is now dead.
 INFO 09:24:43,392 InetAddress /10.34.42.28 is now dead.
 INFO 09:24:43,393 InetAddress /10.77.63.49 is now dead.
 INFO 09:24:43,394 InetAddress /10.194.22.191 is now dead.
 INFO 09:24:43,395 InetAddress /10.34.74.58 is now dead.
 INFO 09:24:43,395 Node /10.34.33.16 is now part of the cluster
 INFO 09:24:43,396 InetAddress /10.34.33.16 is now UP
 INFO 09:24:43,397 Enqueuing flush of Memtable-LocationInfo@1629818866(20/25 serialized/live bytes, 1 ops)
 INFO 09:24:43,398 Writing Memtable-LocationInfo@1629818866(20/25 serialized/live bytes, 1 ops)
 INFO 09:24:43,417 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-2-Data.db (74 bytes)
 INFO 09:24:43,418 InetAddress /10.202.67.43 is now dead.
 INFO 09:24:43,419 InetAddress /10.116.215.81 is now dead.
 INFO 09:24:43,420 InetAddress /10.99.39.242 is now dead.
 INFO 09:24:43,421 InetAddress /10.80.110.28 is now dead.
 INFO 09:24:43,422 InetAddress /10.118.233.198 is now dead.
 INFO 09:24:43,423 InetAddress /10.40.177.173 is now dead.
 INFO 09:24:43,424 InetAddress /10.205.23.34 is now dead.
 INFO 09:24:43,425 InetAddress /10.101.41.8 is now dead.
 INFO 09:24:43,669 InetAddress /10.118.230.219 is now dead.
 INFO 09:24:43,670 InetAddress /10.80.41.192 is now dead.
 INFO 09:24:43,671 InetAddress /10.40.22.224 is now dead.
 INFO 09:24:43,672 InetAddress /10.39.107.114 is now dead.
 INFO 09:24:46,164 InetAddress /10.118.185.68 is now dead.
 INFO 09:24:46,166 InetAddress /10.84.205.93 is now dead.
 INFO 09:24:46,167 InetAddress /10.116.134.183 is now dead.
 INFO 09:24:46,670 InetAddress /10.118.179.67 is now dead.
 INFO 09:24:46,671 InetAddress /10.116.241.250 is now dead.
 INFO 09:24:48,441 InetAddress /10.118.94.62 is now dead.
 INFO 09:24:48,442 InetAddress /10.99.86.251 is now dead.
 INFO 09:24:50,176 InetAddress /10.113.42.21 is now dead.
 INFO 09:24:50,177 InetAddress /10.34.159.72 is now dead.
 INFO 09:24:50,178 InetAddress /10.32.79.134 is now dead.
 INFO 09:24:50,179 InetAddress /10.80.210.38 is now dead.
 INFO 09:24:50,180 InetAddress /10.34.70.73 is now dead.
 INFO 09:24:50,181 InetAddress /10.196.79.240 is now dead.
 INFO 09:25:01,713 InetAddress /10.82.210.172 is now dead.
 INFO 09:25:06,202 InetAddress /10.80.110.28 is now UP
 INFO 09:25:06,908 InetAddress /10.99.39.242 is now UP
 INFO 09:25:07,696 InetAddress /10.118.233.198 is now UP
 INFO 09:25:07,697 InetAddress /10.205.23.34 is now UP
 INFO 09:25:08,704 InetAddress /10.194.22.191 is now UP
 INFO 09:25:08,705 InetAddress /10.40.177.173 is now UP
 INFO 09:25:08,706 InetAddress /10.101.41.8 is now UP
 INFO 09:25:09,489 InetAddress /10.202.67.43 is now UP
 INFO 09:25:09,698 InetAddress /10.77.63.49 is now UP
 INFO 09:25:10,628 Joining: getting bootstrap token
 INFO 09:25:10,631 Enqueuing flush of Memtable-LocationInfo@1733057335(36/45 serialized/live bytes, 1 ops)
 INFO 09:25:10,631 Writing Memtable-LocationInfo@1733057335(36/45 serialized/live bytes, 1 ops)
 INFO 09:25:10,647 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-3-Data.db (87 bytes)
 INFO 09:25:10,649 Joining: sleeping 30000 ms for pending range setup
 INFO 09:25:10,689 InetAddress /10.85.11.38 is now UP
 INFO 09:25:10,708 InetAddress /10.34.74.58 is now UP
 INFO 09:25:10,912 InetAddress /10.194.29.156 is now UP
 INFO 09:25:11,261 Applying migration bb843dd0-0146-11e1-0000-b877c09da5ff Add keyspace: OpsCenter, rep strategy:SimpleStrategy{org.apache.cassandra.config.CFMetaData@55e29b99[cfId=1000,ksName=OpsCenter,cfName=pdps,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=300.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@105585dc,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@5ec736e4[cfId=1004,ksName=OpsCenter,cfName=rollups86400,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=2,maxCompactionThreshold=8,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@68e4e358,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@b09dc35[cfId=1003,ksName=OpsCenter,cfName=rollups7200,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=2,maxCompactionThreshold=8,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@3458213c,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@5ee04fd[cfId=1002,ksName=OpsCenter,cfName=rollups300,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=16,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@4d898115,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@7e79b177[cfId=1005,ksName=OpsCenter,cfName=events,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=OpsCenter raw event storage,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=8,maxCompactionThreshold=12,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@67723c7f,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@540523be[cfId=1006,ksName=OpsCenter,cfName=events_timeline,cfType=Standard,comparator=org.apache.cassandra.db.marshal.LongType,subcolumncomparator=<null>,comment=OpsCenter event timelines,rowCacheSize=0.0,keyCacheSize=5.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=8,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@1d6dba0a,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@ed0f59e[cfId=1007,ksName=OpsCenter,cfName=settings,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=OpsCenter settings,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=8,maxCompactionThreshold=12,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@38ad5fab,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@7e63f09e[cfId=1001,ksName=OpsCenter,cfName=rollups60,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@534a55e5,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]}, durable_writes: true
 INFO 09:25:11,273 Enqueuing flush of Memtable-Migrations@1767199109(12925/16156 serialized/live bytes, 1 ops)
 INFO 09:25:11,273 Writing Memtable-Migrations@1767199109(12925/16156 serialized/live bytes, 1 ops)
 INFO 09:25:11,274 Enqueuing flush of Memtable-Schema@1616586953(5820/7275 serialized/live bytes, 3 ops)
 INFO 09:25:11,358 Completed flushing /raid0/cassandra/data/system/Migrations-h-1-Data.db (12989 bytes)
 INFO 09:25:11,358 Writing Memtable-Schema@1616586953(5820/7275 serialized/live bytes, 3 ops)
 INFO 09:25:11,390 Completed flushing /raid0/cassandra/data/system/Schema-h-1-Data.db (5970 bytes)
 INFO 09:25:11,727 InetAddress /10.116.215.81 is now UP
 INFO 09:25:11,744 InetAddress /10.34.42.28 is now UP
 INFO 09:25:11,750 InetAddress /10.40.22.224 is now UP
 INFO 09:25:12,023 InetAddress /10.80.41.192 is now UP
 INFO 09:25:12,712 InetAddress /10.39.107.114 is now UP
 INFO 09:25:12,717 InetAddress /10.118.185.68 is now UP
 INFO 09:25:12,721 InetAddress /10.116.134.183 is now UP
 INFO 09:25:13,322 InetAddress /10.118.230.219 is now UP
 INFO 09:25:13,632 InetAddress /10.84.205.93 is now UP
 INFO 09:25:14,713 InetAddress /10.118.179.67 is now UP
 INFO 09:25:14,717 InetAddress /10.116.241.250 is now UP
 INFO 09:25:17,468 InetAddress /10.34.159.72 is now UP
 INFO 09:25:17,476 InetAddress /10.118.94.62 is now UP
 INFO 09:25:17,480 InetAddress /10.80.210.38 is now UP
 INFO 09:25:17,716 InetAddress /10.32.79.134 is now UP
 INFO 09:25:17,721 InetAddress /10.99.86.251 is now UP
 INFO 09:25:18,717 InetAddress /10.196.79.240 is now UP
 INFO 09:25:18,727 InetAddress /10.34.70.73 is now UP
 INFO 09:25:19,596 InetAddress /10.113.42.21 is now UP
 INFO 09:25:25,750 InetAddress /10.82.210.172 is now UP
 INFO 09:25:37,743 Enqueuing flush of Memtable-LocationInfo@288976631(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,744 Writing Memtable-LocationInfo@288976631(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,764 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-4-Data.db (89 bytes)
 INFO 09:25:37,773 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-1-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-3-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-4-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-2-Data.db')]
 INFO 09:25:37,776 Enqueuing flush of Memtable-LocationInfo@1950702248(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,777 Writing Memtable-LocationInfo@1950702248(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,821 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-5-Data.db (89 bytes)
 INFO 09:25:37,869 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-6-Data.db,].  548 to 443 (~80% of original) bytes for 3 keys at 0.006500MB/s.  Time: 65ms.
 INFO 09:25:38,740 Enqueuing flush of Memtable-LocationInfo@92917455(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,740 Writing Memtable-LocationInfo@92917455(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,757 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-8-Data.db (89 bytes)
 INFO 09:25:38,766 Enqueuing flush of Memtable-LocationInfo@1096488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,767 Writing Memtable-LocationInfo@1096488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,814 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-9-Data.db (89 bytes)
 INFO 09:25:38,816 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-6-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-9-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-8-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-5-Data.db')]
 INFO 09:25:38,823 Enqueuing flush of Memtable-LocationInfo@1734564525(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,823 Writing Memtable-LocationInfo@1734564525(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,893 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-10-Data.db (89 bytes)
 INFO 09:25:38,916 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-11-Data.db,].  710 to 548 (~77% of original) bytes for 3 keys at 0.005226MB/s.  Time: 100ms.
 INFO 09:25:39,538 Enqueuing flush of Memtable-LocationInfo@811507066(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,539 Writing Memtable-LocationInfo@811507066(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,555 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-13-Data.db (89 bytes)
 INFO 09:25:39,578 Enqueuing flush of Memtable-LocationInfo@1125690366(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,578 Writing Memtable-LocationInfo@1125690366(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,594 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-14-Data.db (89 bytes)
 INFO 09:25:39,596 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-11-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-10-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-14-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-13-Data.db')]
 INFO 09:25:39,613 Enqueuing flush of Memtable-LocationInfo@1870148830(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,614 Writing Memtable-LocationInfo@1870148830(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,652 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-16-Data.db (89 bytes)
 INFO 09:25:39,692 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-15-Data.db,].  815 to 653 (~80% of original) bytes for 3 keys at 0.006487MB/s.  Time: 96ms.
 INFO 09:25:39,731 Enqueuing flush of Memtable-LocationInfo@1279866611(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,731 Writing Memtable-LocationInfo@1279866611(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,747 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-18-Data.db (89 bytes)
 INFO 09:25:40,649 Starting to bootstrap...
 INFO 09:25:40,701 Finished streaming session 304272969286 from /10.205.23.34
 INFO 09:25:40,703 Enqueuing flush of Memtable-LocationInfo@1868577756(53/66 serialized/live bytes, 2 ops)
 INFO 09:25:40,703 Writing Memtable-LocationInfo@1868577756(53/66 serialized/live bytes, 2 ops)
 INFO 09:25:40,721 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-19-Data.db (163 bytes)
 INFO 09:25:40,722 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-19-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-15-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-18-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-16-Data.db')]
 INFO 09:25:40,726 Node /10.40.190.143 state jump to normal
 INFO 09:25:40,734 Enqueuing flush of Memtable-LocationInfo@641287650(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:40,735 Writing Memtable-LocationInfo@641287650(35/43 serialized/live bytes, 1 ops)
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
    at java.util.HashMap$EntryIterator.next(HashMap.java:834)
    at java.util.HashMap$EntryIterator.next(HashMap.java:832)
    at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:301)
    at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:293)
    at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1127)
    at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1084)
    at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:920)
    at org.apache.cassandra.service.StorageService.onChange(StorageService.java:805)
    at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:880)
    at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1027)
    at org.apache.cassandra.service.StorageService.setToken(StorageService.java:226)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:573)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:460)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:381)
    at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:215)
    at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:238)
    ... 5 more
Cannot load daemon
Service exit with a return value of 3
 INFO 09:35:35,156 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26
 INFO 09:35:35,159 Heap size: 1936719872/1937768448
 INFO 09:35:35,160 Classpath: /usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.4.0.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/joda-time-1.6.2.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.6.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.3.jar:/usr/share/cassandra/apache-cassandra-1.0.0.jar:/usr/share/cassandra/apache-cassandra-thrift-1.0.0.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar
 INFO 09:35:36,626 JNA mlockall successful
 INFO 09:35:36,636 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 09:35:36,757 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:35:36,769 Global memtable threshold is enabled at 616MB
 INFO 09:35:36,811 EC2Snitch using region: us-east, zone: 1d.
 INFO 09:35:37,030 Opening /raid0/cassandra/data/system/Schema-h-1 (5970 bytes)
 INFO 09:35:37,067 Opening /raid0/cassandra/data/system/Migrations-h-1 (12989 bytes)
 INFO 09:35:37,075 Opening /raid0/cassandra/data/system/LocationInfo-h-19 (163 bytes)
 INFO 09:35:37,075 Opening /raid0/cassandra/data/system/LocationInfo-h-18 (89 bytes)
 INFO 09:35:37,083 Opening /raid0/cassandra/data/system/LocationInfo-h-15 (653 bytes)
 INFO 09:35:37,085 Opening /raid0/cassandra/data/system/LocationInfo-h-16 (89 bytes)
 INFO 09:35:37,131 Loading schema version bb843dd0-0146-11e1-0000-b877c09da5ff
 INFO 09:35:37,372 Creating new commitlog segment /raid0/cassandra/commitlog/CommitLog-1319794537372.log
 INFO 09:35:37,384 Replaying /raid0/cassandra/commitlog/CommitLog-1319793880475.log
 INFO 09:35:37,416 Finished reading /raid0/cassandra/commitlog/CommitLog-1319793880475.log
 INFO 09:35:37,422 Enqueuing flush of Memtable-events@1830423861(164/205 serialized/live bytes, 5 ops)
 INFO 09:35:37,423 Writing Memtable-events@1830423861(164/205 serialized/live bytes, 5 ops)
 INFO 09:35:37,424 Enqueuing flush of Memtable-Versions@817138449(83/103 serialized/live bytes, 3 ops)
 INFO 09:35:37,472 Completed flushing /raid0/cassandra/data/OpsCenter/events-h-1-Data.db (230 bytes)
 INFO 09:35:37,479 Writing Memtable-Versions@817138449(83/103 serialized/live bytes, 3 ops)
 INFO 09:35:37,497 Completed flushing /raid0/cassandra/data/system/Versions-h-1-Data.db (247 bytes)
 INFO 09:35:37,497 Log replay complete, 4 replayed mutations
 INFO 09:35:37,509 Cassandra version: 1.0.0
 INFO 09:35:37,510 Thrift API version: 19.18.0
 INFO 09:35:37,510 Loading persisted ring state
 INFO 09:35:37,528 Starting up server gossip
 INFO 09:35:37,530 Enqueuing flush of Memtable-LocationInfo@1655441108(29/36 serialized/live bytes, 1 ops)
 INFO 09:35:37,530 Writing Memtable-LocationInfo@1655441108(29/36 serialized/live bytes, 1 ops)
 INFO 09:35:37,554 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-20-Data.db (80 bytes)
 INFO 09:35:37,555 Ec2Snitch adding ApplicationState ec2region=us-east ec2zone=1d
 INFO 09:35:37,562 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-16-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-18-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-19-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-20-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-15-Data.db')]
 INFO 09:35:37,566 Starting Messaging Service on /10.40.190.143:7000
 INFO 09:35:37,592 Using saved token 19444706681196483626478548996101040654
 INFO 09:35:37,593 Enqueuing flush of Memtable-LocationInfo@995684858(53/66 serialized/live bytes, 2 ops)
 INFO 09:35:37,593 Writing Memtable-LocationInfo@995684858(53/66 serialized/live bytes, 2 ops)
 INFO 09:35:37,616 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-22-Data.db (163 bytes)
 INFO 09:35:37,620 Node /10.40.190.143 state jump to normal
 INFO 09:35:37,639 Bootstrap/Replace/Move completed! Now serving reads.
 INFO 09:35:37,640 Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO 09:35:37,684 Binding thrift service to /0.0.0.0:9160
 INFO 09:35:37,687 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-21-Data.db,].  1,074 to 799 (~74% of original) bytes for 4 keys at 0.007620MB/s.  Time: 100ms.
 INFO 09:35:37,688 Using TFastFramedTransport with a max frame size of 15728640 bytes.
 INFO 09:35:37,692 Using synchronous/threadpool thrift server on /0.0.0.0 : 9160
 INFO 09:35:37,695 Listening for thrift clients...
 INFO 09:35:37,706 Node /10.118.230.219 is now part of the cluster
 INFO 09:35:37,707 InetAddress /10.118.230.219 is now UP
 INFO 09:35:37,708 Enqueuing flush of Memtable-LocationInfo@2035487037(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,709 Writing Memtable-LocationInfo@2035487037(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,725 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-24-Data.db (89 bytes)
 INFO 09:35:37,726 Node /10.34.42.28 is now part of the cluster
 INFO 09:35:37,727 InetAddress /10.34.42.28 is now UP
 INFO 09:35:37,729 Enqueuing flush of Memtable-LocationInfo@321887181(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,729 Writing Memtable-LocationInfo@321887181(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,747 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-25-Data.db (89 bytes)
 INFO 09:35:37,748 Node /10.77.63.49 has restarted, now UP
 INFO 09:35:37,748 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-24-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-22-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-25-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-21-Data.db')]
 INFO 09:35:37,748 InetAddress /10.77.63.49 is now UP
 INFO 09:35:37,749 Node /10.77.63.49 state jump to normal
 INFO 09:35:37,750 Node /10.34.70.73 is now part of the cluster
 INFO 09:35:37,750 InetAddress /10.34.70.73 is now UP
 INFO 09:35:37,752 Enqueuing flush of Memtable-LocationInfo@1354749546(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,752 Writing Memtable-LocationInfo@1354749546(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,789 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-26-Data.db,].  1,140 to 877 (~76% of original) bytes for 4 keys at 0.020399MB/s.  Time: 41ms.
 INFO 09:35:37,801 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-27-Data.db (89 bytes)
 INFO 09:35:37,801 Node /10.99.86.251 is now part of the cluster
 INFO 09:35:37,802 InetAddress /10.99.86.251 is now UP
 INFO 09:35:37,803 Enqueuing flush of Memtable-LocationInfo@793374785(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,804 Writing Memtable-LocationInfo@793374785(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,825 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-29-Data.db (89 bytes)
 INFO 09:35:37,826 Node /10.202.67.43 has restarted, now UP
 INFO 09:35:37,827 InetAddress /10.202.67.43 is now UP
 INFO 09:35:37,827 Node /10.202.67.43 state jump to normal
 INFO 09:35:37,828 Node /10.116.134.183 is now part of the cluster
 INFO 09:35:37,828 InetAddress /10.116.134.183 is now UP
 INFO 09:35:37,829 Enqueuing flush of Memtable-LocationInfo@1728699027(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,830 Writing Memtable-LocationInfo@1728699027(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,850 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-30-Data.db (89 bytes)
 INFO 09:35:37,852 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-30-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-27-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-26-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-29-Data.db')]
 INFO 09:35:37,853 Node /10.118.94.62 is now part of the cluster
 INFO 09:35:37,853 InetAddress /10.118.94.62 is now UP
 INFO 09:35:37,855 Enqueuing flush of Memtable-LocationInfo@2001229122(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,855 Writing Memtable-LocationInfo@2001229122(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,885 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-31-Data.db (89 bytes)
 INFO 09:35:37,886 Node /10.116.215.81 is now part of the cluster
 INFO 09:35:37,887 InetAddress /10.116.215.81 is now UP
 INFO 09:35:37,888 Enqueuing flush of Memtable-LocationInfo@1748800276(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,888 Writing Memtable-LocationInfo@1748800276(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,909 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-33-Data.db (89 bytes)
 INFO 09:35:37,910 Node /10.80.110.28 has restarted, now UP
 INFO 09:35:37,911 InetAddress /10.80.110.28 is now UP
 INFO 09:35:37,911 Node /10.80.110.28 state jump to normal
 INFO 09:35:37,912 Node /10.80.210.38 is now part of the cluster
 INFO 09:35:37,912 InetAddress /10.80.210.38 is now UP
 INFO 09:35:37,914 Enqueuing flush of Memtable-LocationInfo@1761382005(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,914 Writing Memtable-LocationInfo@1761382005(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,925 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-32-Data.db,].  1,144 to 982 (~85% of original) bytes for 4 keys at 0.014190MB/s.  Time: 66ms.
 INFO 09:35:37,927 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-35-Data.db (89 bytes)
 INFO 09:35:37,928 Node /10.40.177.173 has restarted, now UP
 INFO 09:35:37,929 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-31-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-32-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-33-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-35-Data.db')]
 INFO 09:35:37,929 InetAddress /10.40.177.173 is now UP
 INFO 09:35:37,929 Node /10.40.177.173 state jump to normal
 INFO 09:35:37,930 Node /10.101.41.8 has restarted, now UP
 INFO 09:35:37,931 InetAddress /10.101.41.8 is now UP
 INFO 09:35:37,931 Node /10.101.41.8 state jump to normal
 INFO 09:35:37,931 Node /10.205.23.34 has restarted, now UP
 INFO 09:35:37,932 InetAddress /10.205.23.34 is now UP
 INFO 09:35:37,932 Node /10.205.23.34 state jump to normal
 INFO 09:35:37,933 Node /10.118.185.68 is now part of the cluster
 INFO 09:35:37,933 InetAddress /10.118.185.68 is now UP
 INFO 09:35:37,934 Enqueuing flush of Memtable-LocationInfo@260440278(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,935 Writing Memtable-LocationInfo@260440278(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,970 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-36-Data.db (89 bytes)
 INFO 09:35:37,971 Node /10.116.241.250 is now part of the cluster
 INFO 09:35:37,972 InetAddress /10.116.241.250 is now UP
 INFO 09:35:37,973 Enqueuing flush of Memtable-LocationInfo@768673839(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,974 Writing Memtable-LocationInfo@768673839(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,003 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-38-Data.db (89 bytes)
 INFO 09:35:38,004 Node /10.113.42.21 is now part of the cluster
 INFO 09:35:38,005 InetAddress /10.113.42.21 is now UP
 INFO 09:35:38,007 Enqueuing flush of Memtable-LocationInfo@1610335061(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,008 Writing Memtable-LocationInfo@1610335061(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,014 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-37-Data.db,].  1,249 to 1,087 (~87% of original) bytes for 4 keys at 0.012196MB/s.  Time: 85ms.
 INFO 09:35:38,024 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-40-Data.db (89 bytes)
 INFO 09:35:38,024 Node /10.194.29.156 is now part of the cluster
 INFO 09:35:38,025 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-37-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-40-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-36-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-38-Data.db')]
 INFO 09:35:38,025 InetAddress /10.194.29.156 is now UP
 INFO 09:35:38,026 Enqueuing flush of Memtable-LocationInfo@1625488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,027 Writing Memtable-LocationInfo@1625488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,042 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-41-Data.db (89 bytes)
 INFO 09:35:38,043 Node /10.85.11.38 has restarted, now UP
 INFO 09:35:38,044 InetAddress /10.85.11.38 is now UP
 INFO 09:35:38,044 Node /10.85.11.38 state jump to normal
 INFO 09:35:38,045 Node /10.34.159.72 is now part of the cluster
 INFO 09:35:38,046 InetAddress /10.34.159.72 is now UP
 INFO 09:35:38,047 Enqueuing flush of Memtable-LocationInfo@747881713(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,048 Writing Memtable-LocationInfo@747881713(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,065 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-42-Data.db (89 bytes)
 INFO 09:35:38,067 Node /10.194.22.191 is now part of the cluster
 INFO 09:35:38,067 InetAddress /10.194.22.191 is now UP
 INFO 09:35:38,069 Enqueuing flush of Memtable-LocationInfo@709926392(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,069 Writing Memtable-LocationInfo@709926392(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,092 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-44-Data.db (89 bytes)
 INFO 09:35:38,093 Node /10.34.74.58 is now part of the cluster
 INFO 09:35:38,097 InetAddress /10.34.74.58 is now UP
 INFO 09:35:38,098 Enqueuing flush of Memtable-LocationInfo@1356841826(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,099 Writing Memtable-LocationInfo@1356841826(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,105 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-43-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-41-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-44-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-42-Data.db')]
 INFO 09:35:38,106 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-43-Data.db,].  1,354 to 1,192 (~88% of original) bytes for 4 keys at 0.014034MB/s.  Time: 81ms.
 INFO 09:35:38,144 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-46-Data.db (89 bytes)
 INFO 09:35:38,145 Node /10.40.22.224 is now part of the cluster
 INFO 09:35:38,146 InetAddress /10.40.22.224 is now UP
 INFO 09:35:38,147 Enqueuing flush of Memtable-LocationInfo@422797318(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,148 Writing Memtable-LocationInfo@422797318(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,155 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-47-Data.db,].  1,459 to 1,297 (~88% of original) bytes for 4 keys at 0.024738MB/s.  Time: 50ms.
 INFO 09:35:38,164 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-49-Data.db (89 bytes)
 INFO 09:35:38,165 Node /10.32.79.134 is now part of the cluster
 INFO 09:35:38,166 InetAddress /10.32.79.134 is now UP
 INFO 09:35:38,167 Enqueuing flush of Memtable-LocationInfo@1455093129(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,168 Writing Memtable-LocationInfo@1455093129(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,199 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-50-Data.db (89 bytes)
 INFO 09:35:38,200 Node /10.118.179.67 is now part of the cluster
 INFO 09:35:38,200 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-50-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-47-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-49-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-46-Data.db')]
 INFO 09:35:38,200 InetAddress /10.118.179.67 is now UP
 INFO 09:35:38,202 Enqueuing flush of Memtable-LocationInfo@1105436908(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,202 Writing Memtable-LocationInfo@1105436908(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,248 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-51-Data.db (89 bytes)
 INFO 09:35:38,249 Node /10.84.205.93 is now part of the cluster
 INFO 09:35:38,249 InetAddress /10.84.205.93 is now UP
 INFO 09:35:38,251 Enqueuing flush of Memtable-LocationInfo@1306980591(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,251 Writing Memtable-LocationInfo@1306980591(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,262 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-52-Data.db,].  1,564 to 1,402 (~89% of original) bytes for 4 keys at 0.021919MB/s.  Time: 61ms.
 INFO 09:35:38,294 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-54-Data.db (89 bytes)
 INFO 09:35:38,294 Node /10.34.33.16 has restarted, now UP
 INFO 09:35:38,295 InetAddress /10.34.33.16 is now UP
 INFO 09:35:38,296 Node /10.34.33.16 state jump to normal
 INFO 09:35:38,296 Node /10.39.107.114 is now part of the cluster
 INFO 09:35:38,297 InetAddress /10.39.107.114 is now UP
 INFO 09:35:38,298 Enqueuing flush of Memtable-LocationInfo@1038389338(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,299 Writing Memtable-LocationInfo@1038389338(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,311 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-55-Data.db (89 bytes)
 INFO 09:35:38,312 Node /10.196.79.240 is now part of the cluster
 INFO 09:35:38,312 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-52-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-55-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-54-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-51-Data.db')]
 INFO 09:35:38,313 InetAddress /10.196.79.240 is now UP
 INFO 09:35:38,314 Enqueuing flush of Memtable-LocationInfo@1850278722(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,315 Writing Memtable-LocationInfo@1850278722(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,354 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-56-Data.db (89 bytes)
 INFO 09:35:38,355 Node /10.99.39.242 has restarted, now UP
 INFO 09:35:38,356 InetAddress /10.99.39.242 is now UP
 INFO 09:35:38,356 Node /10.99.39.242 state jump to normal
 INFO 09:35:38,357 Node /10.118.233.198 has restarted, now UP
 INFO 09:35:38,358 InetAddress /10.118.233.198 is now UP
 INFO 09:35:38,358 Node /10.118.233.198 state jump to normal
 INFO 09:35:38,359 Node /10.82.210.172 is now part of the cluster
 INFO 09:35:38,359 InetAddress /10.82.210.172 is now UP
 INFO 09:35:38,364 Enqueuing flush of Memtable-LocationInfo@786665924(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,364 Writing Memtable-LocationInfo@786665924(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,439 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-58-Data.db (89 bytes)
 INFO 09:35:38,440 Node /10.80.41.192 is now part of the cluster
 INFO 09:35:38,440 InetAddress /10.80.41.192 is now UP
 INFO 09:35:38,442 Enqueuing flush of Memtable-LocationInfo@1647844754(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,442 Writing Memtable-LocationInfo@1647844754(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,451 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-57-Data.db,].  1,669 to 1,515 (~90% of original) bytes for 4 keys at 0.010470MB/s.  Time: 138ms.
 INFO 09:35:38,459 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-60-Data.db (89 bytes)
 INFO 09:35:38,459 Node /10.76.243.129 is now part of the cluster
 INFO 09:35:38,460 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-56-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-58-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-57-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-60-Data.db')]
 INFO 09:35:38,460 InetAddress /10.76.243.129 is now UP
 INFO 09:35:38,462 Enqueuing flush of Memtable-LocationInfo@585652261(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,462 Writing Memtable-LocationInfo@585652261(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,478 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-61-Data.db (89 bytes)
 INFO 09:35:38,486 Node /10.34.42.28 state jump to normal
 INFO 09:35:38,487 Node /10.34.70.73 state jump to normal
 INFO 09:35:38,488 Node /10.99.86.251 state jump to normal
 INFO 09:35:38,489 Node /10.118.94.62 state jump to normal
 INFO 09:35:38,489 Node /10.80.110.28 state jump to normal
 INFO 09:35:38,490 Node /10.80.210.38 state jump to normal
 INFO 09:35:38,491 Node /10.40.177.173 state jump to normal
 INFO 09:35:38,493 Node /10.101.41.8 state jump to normal
 INFO 09:35:38,493 Node /10.113.42.21 state jump to normal
 INFO 09:35:38,494 Node /10.85.11.38 state jump to normal
 INFO 09:35:38,495 Node /10.34.159.72 state jump to normal
 INFO 09:35:38,496 Node /10.34.74.58 state jump to normal
 INFO 09:35:38,497 Node /10.84.205.93 state jump to normal
 INFO 09:35:38,497 Node /10.118.179.67 state jump to normal
 INFO 09:35:38,498 Node /10.34.33.16 state jump to normal
 INFO 09:35:38,499 Node /10.196.79.240 state jump to normal
 INFO 09:35:38,500 Node /10.118.233.198 state jump to normal
 INFO 09:35:38,501 Node /10.80.41.192 state jump to normal
 INFO 09:35:38,502 Node /10.76.243.129 state jump to normal
 INFO 09:35:38,508 Node /10.118.185.68 state jump to normal
 INFO 09:35:38,524 Node /10.118.230.219 state jump to normal
 INFO 09:35:38,536 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-62-Data.db,].  1,782 to 1,620 (~90% of original) bytes for 4 keys at 0.020328MB/s.  Time: 76ms.
 INFO 09:35:38,537 Node /10.80.110.28 state jump to normal
 INFO 09:35:38,537 Node /10.40.177.173 state jump to normal
 INFO 09:35:38,538 Node /10.101.41.8 state jump to normal
 INFO 09:35:38,539 Node /10.116.241.250 state jump to normal
 INFO 09:35:38,540 Node /10.194.29.156 state jump to normal
 INFO 09:35:38,540 Node /10.34.74.58 state jump to normal
 INFO 09:35:38,541 Node /10.40.22.224 state jump to normal
 INFO 09:35:38,542 Node /10.32.79.134 state jump to normal
 INFO 09:35:38,543 Node /10.39.107.114 state jump to normal
 INFO 09:35:38,543 Node /10.99.39.242 state jump to normal
 INFO 09:35:38,550 Node /10.77.63.49 state jump to normal
 INFO 09:35:38,550 Node /10.34.42.28 state jump to normal
 INFO 09:35:38,551 Node /10.116.134.183 state jump to normal
 INFO 09:35:38,553 Node /10.76.243.129 state jump to normal
 INFO 09:35:38,557 Node /10.202.67.43 state jump to normal
 INFO 09:35:38,558 Node /10.118.94.62 state jump to normal
 INFO 09:35:38,562 Node /10.116.215.81 state jump to normal
 INFO 09:35:38,563 Node /10.80.210.38 state jump to normal
 INFO 09:35:38,564 Node /10.205.23.34 state jump to normal
 INFO 09:35:38,565 Node /10.39.107.114 state jump to normal
{CODE}",,,,,,,,,,,,,,,,27/Jan/12 20:44;jbellis;3417-2.txt;https://issues.apache.org/jira/secure/attachment/12512227/3417-2.txt,28/Jan/12 02:11;scode;3417-3.txt;https://issues.apache.org/jira/secure/attachment/12512273/3417-3.txt,29/Oct/11 02:28;jbellis;3417.txt;https://issues.apache.org/jira/secure/attachment/12501422/3417.txt,13/Feb/12 18:27;scode;CASSANDRA-3417-tokenmap-1.0-v1.txt;https://issues.apache.org/jira/secure/attachment/12514381/CASSANDRA-3417-tokenmap-1.0-v1.txt,12/Feb/12 00:31;scode;CASSANDRA-3417-tokenmap-v2.txt;https://issues.apache.org/jira/secure/attachment/12514252/CASSANDRA-3417-tokenmap-v2.txt,13/Feb/12 07:20;scode;CASSANDRA-3417-tokenmap-v3.txt;https://issues.apache.org/jira/secure/attachment/12514326/CASSANDRA-3417-tokenmap-v3.txt,02/Feb/12 01:09;scode;CASSANDRA-3417-tokenmap.txt;https://issues.apache.org/jira/secure/attachment/12512891/CASSANDRA-3417-tokenmap.txt,,,,,7.0,,,,,,,,,,,,,,,,,,,2011-10-28 19:38:05.788,,,no_permission,,,,,,,,,,,,215154,,,Mon Feb 13 21:45:16 UTC 2012,,,,,,0|i0gjn3:,94612,jbellis,jbellis,,,,,,,,,"28/Oct/11 19:38;jbellis;1.0 branch has moved so FTR here is line 1127 in 1.0.0 StorageService.java:

{code}
.       // At this stage pendingRanges has been updated according to leave operations. We can
        // now continue the calculation by checking bootstrapping nodes.

        // For each of the bootstrapping nodes, simply add and remove them one by one to
        // allLeftMetadata and check in between what their ranges would be.
        for (Map.Entry<Token, InetAddress> entry : bootstrapTokens.entrySet())
{code}","28/Oct/11 19:45;jbellis;Doesn't look like there is an easy way to make Guava BiMap threadsafe. :(

(This has been broken for forever but you're seeing it now because autobootstrap is the default in 1.0: CASSANDRA-2447)","28/Oct/11 19:54;j.casares;Okay, in that case I can drop autobootstrap to false the way it used to be.

Fair workaround?",28/Oct/11 20:46;jbellis;only if you don't care about adding nodes to the cluster,"28/Oct/11 21:35;brandon.williams;bq. Doesn't look like there is an easy way to make Guava BiMap threadsafe. 

synchronizedBiMap should be threadsafe and performant enough here I think.",28/Oct/11 21:45;jbellis;Where's that located?  I can't find it.,29/Oct/11 01:01;brandon.williams;In collect.Synchronized... which it turns out isn't public :(,29/Oct/11 02:28;jbellis;o hai Maps.synchronizedBiMap,"29/Oct/11 21:20;brandon.williams;bq. o hai Maps.synchronizedBiMap

I'm a useful idiot.  +1",31/Oct/11 16:11;jbellis;committed,"27/Jan/12 19:26;scode;How is this a correct fix?

http://google-collections.googlecode.com/svn/trunk/javadoc/com/google/common/collect/Maps.html#synchronizedBiMap(com.google.common.collect.BiMap

In order for concurrent iteration to be safe it's not enough to make individual atomic operations thread-safe. I didn't dig deep but the docs claims HashBiMap is just backed by two hash maps, and also specifically there is mention of needing to synchronize iterations. Which makes sense; imagine trying to implement locking across an iteration and the issues that involves.

Did anyone see the problem actually go away with this patch under circumstances where it was previously reliably re-producible?
","27/Jan/12 19:56;jbellis;You're right, it looks like you need to manually synchronize for iteration.",27/Jan/12 20:44;jbellis;patch to add synchronization blocks around bootstraptokens iteration,"28/Jan/12 02:11;scode;Attaching v3 which also adds synchronization in TokenMap.getTokenToEndpointMap() - both for bootstrapTokens and tokenToEndpointMap.

(x.putAll(y) is not an atomic observation from the perspective of y, even if it is from the perspective of x)","28/Jan/12 04:28;jbellis;commmitted v3, thanks!","01/Feb/12 21:33;scode;Still happening with this fixed. Can't give details now because I'm noticing this while testing something else (JIRA:s coming for those). Thus is bootstrapping a ~ 180 cluster w/ trunk, with all nodes joining at the same time.

{code}
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$EntryIterator.next(HashMap.java:834)
        at java.util.HashMap$EntryIterator.next(HashMap.java:832)
        at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:301)
        at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:293)
        at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:91)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:154)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:181)
        at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1291)
        at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1272)
        at org.apache.cassandra.service.StorageService.onRemove(StorageService.java:1537)
        at org.apache.cassandra.gms.Gossiper.removeEndpoint(Gossiper.java:312)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:629)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}
","02/Feb/12 00:19;scode;This is no longer the bootstrap tokens, but the token meta data being iterated over. I may be blind atm, but looks like it's the AbstractReplicationStrategy's tokenMetadata member.","02/Feb/12 00:26;scode;Right, which is the same as StorageService.instance.tokenMetadata_ ({{AbstractReplicationStrategy.createReplicationStrategy()}} which gets modified in the gossip stage - and the remove endpoint happening in the periodic gossip task is thus un-safe.

This is the FatClient removal path. We've seen this before, but it's not immediately clear to me why a fresh cluster believes there's a fat client to kick out.","02/Feb/12 00:33;scode;TokenMetadata definitely has read locks in other places around the token meta data, but entrySet() is just returning it flatly.",02/Feb/12 00:33;scode;{{NTS.calculateNaturalEndpoint()}} is the only caller.,"02/Feb/12 00:36;scode;Looks (without careful verification) that it was introduced in 406f4b3c6eccddfb65aadfb2cfe423cfe8cef062 for CASSANDRA-1103.
",02/Feb/12 01:09;scode;{{CASSANDRA\-3417\-tokenmap.txt}} attached.,"11/Feb/12 00:11;jbellis;Hmm, is switching from raw endpoints to endpoints-with-bootstrap-candidates fixing a bug, or introducing one?","11/Feb/12 00:18;jbellis;I think it's introducing one, because reads assume that getNaturalEndpoints returns nodes that have the data, and SS.getWriteEndpoints adds in the pending ranges.","11/Feb/12 08:37;scode;Hmm, what are you referring to? I certainly agree that if there is code that changes the endpoints used for reads to include bootstrap candidates, that's wrong. I'm not sure what part of this ticket or which other ticket you are referring to however.","11/Feb/12 15:45;jbellis;Here's the core of getTokenToEndpointMap:

{code}
.           Map<Token, InetAddress> map = new HashMap<Token, InetAddress>(tokenToEndpointMap.size() + bootstrapTokens.size());
            map.putAll(tokenToEndpointMap);
            synchronized (bootstrapTokens)
            {
                map.putAll(bootstrapTokens);
            }
            return map;
{code}",11/Feb/12 20:21;scode;Ok. I can't believe getTokenToEndpointMap() returns something different than tokenToEndPointMap. I completely missed that. I will submit an updated patch which also fixes naming to make this much more obvious.,"12/Feb/12 00:31;scode;Attaching {{v2}}. I decided I would submit a minimal patch to just fix this one problem. I will shortly open a separate ticket for further work on TokenMap.

I realized I have to carefully audit every use-case of TokenMap to detect any issues of a similar type (the first thing I looked at showed a problem - the duplicate token check during bootstrap is not considering all tokens, but only normal+bootstrapping).

I apologize for not being more thorough in my original patch submission and confirming what the method was actually doing. I remember looking closely at the method name and vaguely noticing the bootstrap tokens but it never sank in.","12/Feb/12 00:34;scode;Filed CASSANDRA-3892 for follow-up.
","13/Feb/12 07:20;scode;{{v3}} slightly adjusted to not use joiningEndpoints.size() when constructing the copy w/o joining endpoints.
","13/Feb/12 15:34;jbellis;I'm getting 3 of 5 hunks failing to apply to 1.0, did you switch branches?","13/Feb/12 18:27;scode;Attaching {{CASSANDRA\-3417\-tokenmap\-1.0\-v1.txt}} which is for 1.0.

Apologies for the confusion; I only ever triggered and tested this on 1.1/trunk since that's what I was testing, despite this bug originally being against 1.0.

I haven't done real testing with this patch for 1.0. Right now I can't use the cluster I was testing with to easily go to 1.0 to test either. But, the fix seems correct to me regardless of branch given that the iteration is clearly over a map that is getting modified. The biggest risk is a typo or similar mistake which is more easily spotted by review anyway.
",13/Feb/12 21:45;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix for Build Error in contrib/pig to accomodate refactoring of hexToBytes,CASSANDRA-3341,12526465,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,boneill,boneill,10/Oct/11 16:25,12/Mar/19 14:00,13/Mar/19 22:26,12/Oct/11 16:21,,,,,,,0,,,,,"hexToBytes moved to Hex from FBUtilities.
Need to update contrib/pig to accomodate that move.",linux using contrib/pig on trunk,,,,,,,,,,,,,,,10/Oct/11 16:27;boneill;trunk-3341.txt;https://issues.apache.org/jira/secure/attachment/12498435/trunk-3341.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-12 16:21:34.398,,,no_permission,,,,,,,,,,,,56852,,,Wed Oct 12 16:21:34 UTC 2011,,,,,,0|i0gipz:,94463,brandon.williams,brandon.williams,,,,,,,,,10/Oct/11 16:27;boneill;patch for contrib/pig,12/Oct/11 16:21;brandon.williams;Committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update lib/jamm-0.2.5 to the version deployed to Maven central,CASSANDRA-3310,12525733,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,stephenc,stephenc,04/Oct/11 21:08,12/Mar/19 14:00,13/Mar/19 22:26,05/Oct/11 01:32,1.0.0,,,,,,0,,,,,"https://oss.sonatype.org/content/groups/public/com/github/stephenc/jamm/0.2.5/jamm-0.2.5.jar is the version that is being synced to Maven central

MD5: 79f6bf54227abd15d4277ff0fe7038cb",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-10-05 01:32:52.731,,,no_permission,,,,,,,,,,,,45383,,,Wed Oct 05 01:32:52 UTC 2011,,,,,,0|i0gicn:,94403,,,,,,,,,,,"05/Oct/11 01:32;tjake;Thx, verified its working.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool.bat double quotes classpath,CASSANDRA-3744,12538403,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,14/Jan/12 16:30,12/Mar/19 14:00,13/Mar/19 22:26,27/Jan/12 19:56,1.0.8,,,Tool/nodetool,,,0,,,,,Windows sucks and double quoting things breaks stuff.,Windows,,,,,,,,,,,,,,,14/Jan/12 16:31;nickmbailey;0001-Don-t-double-quote-classpath.patch;https://issues.apache.org/jira/secure/attachment/12510587/0001-Don-t-double-quote-classpath.patch,18/Jan/12 22:28;jbellis;3744-v2.txt;https://issues.apache.org/jira/secure/attachment/12511050/3744-v2.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-01-16 23:05:37.43,,,no_permission,,,,,,,,,,,,223909,,,Fri Jan 27 19:56:44 UTC 2012,,,,,,0|i0gnpj:,95271,jbellis,jbellis,,,,,,,,,"16/Jan/12 23:05;jbellis;Can you give an example of a classpath where quoting breaks it?  It definitely works as-is for me right now.  I believe the purpose of the quoting is to allow paths with spaces, like the common C:\Program Files.","17/Jan/12 00:04;nickmbailey;Right we want to quote the classpath but it is currently getting double quoted which breaks things on windows. When we loop over the jars in the lib dir and call the ':append' block we pass each jar quoted. Then later we quote the entire classpath again when we call java. So we end up with:

{noformat}
""""jar1"";""jar2"";""jar3""""
{noformat}

That will actually break if any of those jars have spaces in the path.",18/Jan/12 22:28;jbellis;I see what you mean.  v2 attached to generalize fix to other .bats.,"26/Jan/12 04:30;nickmbailey;lgtm.

Tested all tools with a space in the classpath.",27/Jan/12 19:56;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
READ Operation with CL=EACH_QUORUM succeed when a DC is down (RF=3),CASSANDRA-3272,12525118,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,cywjackson,cywjackson,28/Sep/11 23:25,12/Mar/19 14:00,13/Mar/19 22:26,18/Oct/11 14:06,1.1.0,,,,,,0,,,,,"""READ EACH_QUORUM: 	Returns the record with the most recent timestamp once a quorum of replicas in each data center of the cluster has responded.""

In other words, if a DC is down and the QUORUM could not be reached on that DC, read should fail.

test case:
- Cassandra version 0.8.6:
INFO [main] 2011-09-28 22:26:24,297 StorageService.java (line 371) Cassandra version: 0.8.6

- 6-node cluster with 2 DC and 3 node each. RF=3 in each DC:
[default@Keyspace3] describe keyspace;
Keyspace: Keyspace3:
Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
Durable Writes: true
Options: [DC2:3, DC1:3]
Column Families:
ColumnFamily: test
Key Validation Class: org.apache.cassandra.db.marshal.BytesType
Default column value validator: org.apache.cassandra.db.marshal.BytesType
Columns sorted by: org.apache.cassandra.db.marshal.BytesType
Row cache size / save period in seconds: 0.0/0
Key cache size / save period in seconds: 200000.0/14400
Memtable thresholds: 1.0875/1440/232 (millions of ops/minutes/MB)
GC grace seconds: 864000
Compaction min/max thresholds: 4/32
Read repair chance: 1.0
Replicate on write: true
Built indexes: []

all nodes are up, insert a row:

$ nodetool -h localhost ring
Address DC Rack Status State Load Owns Token
141784319550391026443072753096570088106
10.34.79.179 DC1 RAC1 Up Normal 11.13 KB 16.67% 0
10.34.70.163 DC2 RAC1 Up Normal 11.14 KB 16.67% 28356863910078205288614550619314017621
10.35.81.147 DC1 RAC1 Up Normal 11.14 KB 16.67% 56713727820156410577229101238628035242
10.84.233.170 DC2 RAC1 Up Normal 11.14 KB 16.67% 85070591730234615865843651857942052864
10.195.201.236 DC1 RAC1 Up Normal 11.14 KB 16.67% 113427455640312821154458202477256070485
10.118.147.73 DC2 RAC1 Up Normal 11.14 KB 16.67% 141784319550391026443072753096570088106

- insert a value 

[default@Keyspace3] set test[utf8('test-key-1')][utf8('test-col')]=utf8('test-value');
Value inserted.

sanity check (cli connects to a node in DC1) :
[default@Keyspace3] consistencylevel as EACH_QUORUM;                                  
Consistency level is set to 'EACH_QUORUM'.
[default@Keyspace3] get test[utf8('test-key-1')];   
=> (column=746573742d636f6c, value=test-value, timestamp=1317249361722000)
Returned 1 results

shut down DC2:
$ nodetool -h localhost ring
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               141784319550391026443072753096570088106     
10.34.79.179    DC1         RAC1        Up     Normal  51.86 KB        16.67%  0                                           
10.34.70.163    DC2         RAC1        Down   Normal  51.88 KB        16.67%  28356863910078205288614550619314017621      
10.35.81.147    DC1         RAC1        Up     Normal  47.5 KB         16.67%  56713727820156410577229101238628035242      
10.84.233.170   DC2         RAC1        Down   Normal  51.88 KB        16.67%  85070591730234615865843651857942052864      
10.195.201.236  DC1         RAC1        Up     Normal  47.5 KB         16.67%  113427455640312821154458202477256070485     
10.118.147.73   DC2         RAC1        Down   Normal  51.88 KB        16.67%  141784319550391026443072753096570088106  

[default@Keyspace3] get test[utf8('test-key-1')];   
=> (column=746573742d636f6c, value=746573742d76616c7565, timestamp=1317249361722000)
Returned 1 results.

tried with pycassaShell:
>>> col_fam.get('test-key-1',read_consistency_level=pycassa.ConsistencyLevel.EACH_QUORUM)
OrderedDict([('test-col', 'test-value')])
",,,,,,,,,,,,,,,,14/Oct/11 22:02;jbellis;3272-v2.txt;https://issues.apache.org/jira/secure/attachment/12499103/3272-v2.txt,13/Oct/11 19:12;jbellis;3272.txt;https://issues.apache.org/jira/secure/attachment/12498911/3272.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-09-29 00:39:50.743,,,no_permission,,,,,,,,,,,,37574,,,Tue Oct 18 15:26:21 UTC 2011,,,,,,0|i0ghwf:,94330,slebresne,slebresne,,,,,,,,,29/Sep/11 00:39;jbellis;Why are you using EACH_QUORUM?,"29/Sep/11 00:40;jbellis;(I ask because I've never seen it be the ""right"" CL yet.)","29/Sep/11 00:52;brandon.williams;bq. In other words, if a DC is down and the QUORUM could not be reached on that DC, read should fail.

I can't think of any reason someone would actually want to do EACH_QUORUM on a read, when LOCAL_QUORUM would be sufficient if the write was done at EACH_QUORUM.  ","29/Sep/11 01:41;cywjackson;""Why are you using EACH_QUORUM?""
 -- no particular reason other than testing.

""I can't think of any reason someone would actually want to do EACH_QUORUM on a read, when LOCAL_QUORUM would be sufficient if the write was done at EACH_QUORUM. ""
 -- would it make sense to make such consistency level as an ""invalid to use"" for READ then?

What if write (update, say add a new column/value to the key) is done via LOCAL_QUORUM to DC1 but DC2 were down. 
anti-entropy repair wasn't done
stop DC1 (for some reason this step is important)
now bring DC2 back up (assume HH is off)
read LQ from DC2.. 
you get an outdated result. 

at least i think with EACH_QUORUM on a read, you should get an consistent result if 2 nodes replicas agreed on each DC (in the example where RF=3 in each DC)

(and you should get an UAE in the above example)",13/Oct/11 19:12;jbellis;currently EACH_QUORUM silently does LOCAL_QUORUM instead. Patch attached to raise an error saying EACH_QUORUM is only supported for writes.,"14/Oct/11 08:28;slebresne;Shouldn't we move the validation up. We already have a validateConsistencyLevel in ThriftValidation for instance, we could just add a flag to if it's a write or read operation and throw the right exception there. If only for consistency, but I think it's nice to keep validating the queries upfront so we don't have to later anyway.

Which btw make me see that this validateConsistencyLevel is not called by CQL (which thus don't do the right check this function already does I suppose, or duplicate it). It would be worth checking if that's the only place where CQL is more loose than thrift in validating the query (and maybe we could refactor a bit the validation code so it's easier to apply it to both thrift and CQL).",14/Oct/11 22:02;jbellis;v2 attached,18/Oct/11 10:34;slebresne;+1 with the only nitpick that validateStrategyForCL could probably be more accurately (though less concisely) named validateStrategyForMultiDataCenterCL since it don't really use the consistency level excepted for the exception message.,18/Oct/11 14:06;jbellis;renamed to requireNetworkTopologyStrategy and committed,"18/Oct/11 15:26;hudson;Integrated in Cassandra #1160 (See [https://builds.apache.org/job/Cassandra/1160/])
    EACH_QUORUM is only supported for writes
patch by jbellis; reviewed by slebresne for CASSANDRA-3272

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1185669
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/NEWS.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/RequestType.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/ThriftValidation.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Truncate leaves behind non-CFS backed secondary indexes,CASSANDRA-3844,12541078,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,tjake,tjake,03/Feb/12 14:28,12/Mar/19 14:00,13/Mar/19 22:26,07/Feb/12 15:25,1.0.8,,,Feature/2i Index,,,0,,,,,"If you setup a CF with a non-cfs backed secondary index then trucate it, nothing happens to the secondary index. we need a hook for CFStore to clean these up.",,,,,,,,,,,,,,,,07/Feb/12 14:48;xedin;CASSANDRA-3844.patch;https://issues.apache.org/jira/secure/attachment/12513618/CASSANDRA-3844.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-07 14:08:13.994,,,no_permission,,,,,,,,,,,,226430,,,Tue Feb 07 15:25:25 UTC 2012,,,,,,0|i0gowf:,95464,tjake,tjake,,,,,,,,,"07/Feb/12 14:08;xedin;This patch encapsulates truncate logic into SecondaryIndex.truncate(long) abstract method so every type of indices would be able to implement desired logic (currently implemented for KeysIndex), which also removes a good bit of code from CompactionManager.submitTruncate(...).",07/Feb/12 14:13;xedin;renamed CFS.truncateSSTables(long) to CFS.discardSSTables(long).,07/Feb/12 14:41;tjake;SIM.getIndexes() should use a IdenenityHashMap since PerRowSecondaryIndexes share the same instance across rows.,07/Feb/12 14:48;xedin;uses IdentityHashMap in SecondaryIndexManager.getIndexes(),07/Feb/12 14:52;tjake;+1,07/Feb/12 15:25;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming retry is no longer performed,CASSANDRA-3686,12536702,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,30/Dec/11 22:34,12/Mar/19 14:00,13/Mar/19 22:26,31/Dec/11 05:05,1.0.7,,,,,,0,stream,,,,"CASSANDRA-3532 changed exception handling when processing incoming stream, but since it wraps all exception into RuntimeException, streaming retry which had been occurred when IOException is thrown no longer works.",,,,,,,,,,,,,,,,30/Dec/11 22:36;yukim;0001-Fix-error-handling-for-streaming-retry.patch;https://issues.apache.org/jira/secure/attachment/12508958/0001-Fix-error-handling-for-streaming-retry.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-31 05:05:34.707,,,no_permission,,,,,,,,,,,,222385,,,Sat Dec 31 05:05:34 UTC 2011,,,,,,0|i0gmzz:,95156,jbellis,jbellis,,,,,,,,,"30/Dec/11 22:36;yukim;Patch attached to just throw IOException when it happens, otherwise wrap in RuntimeException.",31/Dec/11 05:05;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Whitespace in SimpleSeedProvider string makes seed ignored,CASSANDRA-3263,12524864,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,krummas,krummas,27/Sep/11 13:49,12/Mar/19 14:00,13/Mar/19 22:26,03/Oct/11 19:35,1.0.0,,,,,,0,,,,,"If a seeds given to SimpleSeedProvider contains whitespace, the seed will be ignored

for example ""1.2.3.4, 5.6.7.8"" will only make 5.6.7.8 a seed.

patch simply trim()s the host.",,,,,,,,,,,,,,,,27/Sep/11 13:50;krummas;3263.txt;https://issues.apache.org/jira/secure/attachment/12496684/3263.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-03 19:35:59.847,,,no_permission,,,,,,,,,,,,19204,,,Mon Oct 03 19:35:59 UTC 2011,,,,,,0|i0ghsf:,94312,jbellis,jbellis,,,,,,,,,27/Sep/11 13:50;krummas;trim() host,27/Sep/11 16:25;krummas;of course i meant that only 1.2.3.4 will became a seed in the example in the description..,03/Oct/11 19:35;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool info reports inaccurate datacenter/rack for localhost,CASSANDRA-3556,12533581,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,rbranson,rbranson,rbranson,02/Dec/11 08:10,12/Mar/19 14:00,13/Mar/19 22:26,02/Dec/11 18:10,0.8.9,1.0.6,,Tool/nodetool,,,0,,,,,The datacenter & rack information provided by 'nodetool info' is incorrect when using 'nodetool -h localhost info'. This is because the IP address passed to the EndpointSnitch to determine the datacenter & rack is sourced from the host parameter provided to nodetool and not the actual endpoint address used in the ring.,,,,,,,,,,,,,,,,02/Dec/11 16:03;rbranson;3556-v2.txt;https://issues.apache.org/jira/secure/attachment/12505902/3556-v2.txt,02/Dec/11 08:12;rbranson;3556.txt;https://issues.apache.org/jira/secure/attachment/12505857/3556.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-02 14:09:25.14,,,no_permission,,,,,,,,,,,,219309,,,Fri Dec 02 18:22:46 UTC 2011,,,,,,0|i0gld3:,94891,jbellis,jbellis,,,,,,,,,"02/Dec/11 08:13;rbranson;Attached patch resolves the issue by looking up the local endpoint's ""real"" hostname by token.","02/Dec/11 14:09;jbellis;""fall back if we can't find it for some reason"" is shouldn't-ever-happen territory, right?  Would prefer to throw assertionerror, if so.",02/Dec/11 16:03;rbranson;betta? :),02/Dec/11 18:10;jbellis;committed,"02/Dec/11 18:22;hudson;Integrated in Cassandra-0.8 #408 (See [https://builds.apache.org/job/Cassandra-0.8/408/])
    use cannonical host for local node in nodetool info
patch by Rick Branson; reviewed by jbellis for CASSANDRA-3556

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1209608
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/NodeProbe.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The word count example demonstrating hadoop integration fails in trunk,CASSANDRA-3215,12523306,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,mccloud35,mccloud35,16/Sep/11 09:47,12/Mar/19 14:00,13/Mar/19 22:26,19/Sep/11 08:47,1.0.0,,,,,,0,hadoop,,,,"The following stack traces after running, bin/hadoop in the trunk (0.8.2-dev-SNAPSHOT):

./bin/word_count
11/09/15 12:28:28 INFO WordCount: output reducer type: cassandra
11/09/15 12:28:29 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
11/09/15 12:28:30 INFO mapred.JobClient: Running job: job_local_0001
11/09/15 12:28:30 INFO mapred.MapTask: io.sort.mb = 100
11/09/15 12:28:30 INFO mapred.MapTask: data buffer = 79691776/99614720
11/09/15 12:28:30 INFO mapred.MapTask: record buffer = 262144/327680
11/09/15 12:28:30 WARN mapred.LocalJobRunner: job_local_0001
java.lang.RuntimeException: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:132)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getLocation(ColumnFamilyRecordReader.java:176)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:113)
	... 4 more
11/09/15 12:28:31 INFO mapred.JobClient:  map 0% reduce 0%
11/09/15 12:28:31 INFO mapred.JobClient: Job complete: job_local_0001
11/09/15 12:28:31 INFO mapred.JobClient: Counters: 0
11/09/15 12:28:31 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
11/09/15 12:28:32 INFO mapred.JobClient: Running job: job_local_0002
11/09/15 12:28:32 INFO mapred.MapTask: io.sort.mb = 100
11/09/15 12:28:32 INFO mapred.MapTask: data buffer = 79691776/99614720
11/09/15 12:28:32 INFO mapred.MapTask: record buffer = 262144/327680
11/09/15 12:28:32 WARN mapred.LocalJobRunner: job_local_0002
java.lang.RuntimeException: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:132)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getLocation(ColumnFamilyRecordReader.java:176)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:113)
	... 4 more
11/09/15 12:28:33 INFO mapred.JobClient:  map 0% reduce 0%
11/09/15 12:28:33 INFO mapred.JobClient: Job complete: job_local_0002
11/09/15 12:28:33 INFO mapred.JobClient: Counters: 0
11/09/15 12:28:33 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
11/09/15 12:28:34 INFO mapred.JobClient: Running job: job_local_0003
11/09/15 12:28:34 INFO mapred.MapTask: io.sort.mb = 100
11/09/15 12:28:34 INFO mapred.MapTask: data buffer = 79691776/99614720
11/09/15 12:28:34 INFO mapred.MapTask: record buffer = 262144/327680
11/09/15 12:28:34 WARN mapred.LocalJobRunner: job_local_0003
java.lang.RuntimeException: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:132)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getLocation(ColumnFamilyRecordReader.java:176)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:113)
	... 4 more
11/09/15 12:28:35 INFO mapred.JobClient:  map 0% reduce 0%
11/09/15 12:28:35 INFO mapred.JobClient: Job complete: job_local_0003
11/09/15 12:28:35 INFO mapred.JobClient: Counters: 0
11/09/15 12:28:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
11/09/15 12:28:36 INFO mapred.JobClient: Running job: job_local_0004
11/09/15 12:28:36 INFO mapred.MapTask: io.sort.mb = 100
11/09/15 12:28:37 INFO mapred.MapTask: data buffer = 79691776/99614720
11/09/15 12:28:37 INFO mapred.MapTask: record buffer = 262144/327680
11/09/15 12:28:37 WARN mapred.LocalJobRunner: job_local_0004
java.lang.RuntimeException: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:132)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getLocation(ColumnFamilyRecordReader.java:176)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:113)
	... 4 more
11/09/15 12:28:37 INFO mapred.JobClient:  map 0% reduce 0%
11/09/15 12:28:37 INFO mapred.JobClient: Job complete: job_local_0004
11/09/15 12:28:37 INFO mapred.JobClient: Counters: 0",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-09-16 21:15:31.644,,,no_permission,,,,,,,,,,,,4016,,,Mon Sep 19 08:47:54 UTC 2011,,,,,,0|i0gh7b:,94217,,,,,,,,,,,"16/Sep/11 21:15;brandon.williams;This shouldn't happen with the reverted CASSANDRA-2388, can you update to the latest trunk and try again?",19/Sep/11 08:47;mccloud35;This is fixed after updating trunk and building. As per Brandon's comment reverting CASSANDRA-2388 was the fix.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair Streaming hangs between multiple regions,CASSANDRA-3838,12540877,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jasobrown,vijay2win@yahoo.com,vijay2win@yahoo.com,02/Feb/12 06:10,12/Mar/19 13:59,13/Mar/19 22:26,05/Feb/12 21:40,1.0.8,,,,,,0,,,,,"Streaming hangs between datacenters, though there might be multiple reasons for this, a simple fix will be to add the Socket timeout so the session can retry.

The following is the netstat of the affected node (the below output remains this way for a very long period).
[test_abrepairtest@test_abrepair--euwest1c-i-1adfb753 ~]$ nt netstats
Mode: NORMAL
Streaming to: /50.17.92.159
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2221-Data.db sections=7002 progress=1523325354/2475291786 - 61%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2233-Data.db sections=4581 progress=0/595026085 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-g-2235-Data.db sections=6631 progress=0/2270344837 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2239-Data.db sections=6266 progress=0/2190197091 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2230-Data.db sections=7662 progress=0/3082087770 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2240-Data.db sections=7874 progress=0/587439833 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-g-2226-Data.db sections=7682 progress=0/2933920085 - 0%



""Streaming:1"" daemon prio=10 tid=0x00002aaac2060800 nid=0x1676 runnable [0x000000006be85000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at com.sun.net.ssl.internal.ssl.OutputRecord.writeBuffer(OutputRecord.java:297)
        at com.sun.net.ssl.internal.ssl.OutputRecord.write(OutputRecord.java:286)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:743)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:731)
        at com.sun.net.ssl.internal.ssl.AppOutputStream.write(AppOutputStream.java:59)
        - locked <0x00000006afea1bd8> (a com.sun.net.ssl.internal.ssl.AppOutputStream)
        at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:133)
        at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
        at com.ning.compress.lzf.LZFOutputStream.flush(LZFOutputStream.java:117)
        at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:152)
        at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Streaming from: /46.51.141.51
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2241-Data.db sections=7231 progress=0/1548922508 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2231-Data.db sections=4730 progress=0/296474156 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2244-Data.db sections=7650 progress=0/1580417610 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2217-Data.db sections=7682 progress=0/196689250 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2220-Data.db sections=7149 progress=0/478695185 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2171-Data.db sections=443 progress=0/78417320 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-g-2235-Data.db sections=6631 progress=0/2270344837 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2222-Data.db sections=4590 progress=0/1310718798 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2233-Data.db sections=4581 progress=0/595026085 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-g-2226-Data.db sections=7682 progress=0/2933920085 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2213-Data.db sections=7876 progress=0/3308781588 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2216-Data.db sections=7386 progress=0/2868167170 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2240-Data.db sections=7874 progress=0/587439833 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2254-Data.db sections=4618 progress=0/215989758 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2221-Data.db sections=7002 progress=1542191546/2475291786 - 62%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2239-Data.db sections=6266 progress=0/2190197091 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2210-Data.db sections=6698 progress=0/2304563183 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2230-Data.db sections=7662 progress=0/3082087770 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2229-Data.db sections=7386 progress=0/1324787539 - 0%


""Thread-198896"" prio=10 tid=0x00002aaac0e00800 nid=0x4710 runnable [0x000000004251b000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputRecord.java:293)
        at com.sun.net.ssl.internal.ssl.InputRecord.readV3Record(InputRecord.java:405)
        at com.sun.net.ssl.internal.ssl.InputRecord.read(InputRecord.java:360)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:798)
        - locked <0x00000005e220a170> (a java.lang.Object)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:755)
        at com.sun.net.ssl.internal.ssl.AppInputStream.read(AppInputStream.java:75)
        - locked <0x00000005e220a1b8> (a com.sun.net.ssl.internal.ssl.AppInputStream)
        at com.ning.compress.lzf.LZFDecoder.readFully(LZFDecoder.java:392)
        at com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:190)
        at com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254)
        at com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:129)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at java.io.DataInputStream.readLong(DataInputStream.java:399)
        at org.apache.cassandra.utils.BytesReadTracker.readLong(BytesReadTracker.java:115)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:119)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:37)
        at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:244)
        at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:148)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:90)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:185)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)
",,,,,,,,,,,,CASSANDRA-3569,,,,02/Feb/12 06:13;vijay2win@yahoo.com;0001-Add-streaming-socket-timeouts.patch;https://issues.apache.org/jira/secure/attachment/12512915/0001-Add-streaming-socket-timeouts.patch,05/Feb/12 18:30;vijay2win@yahoo.com;0001-CASSANDRA-3838-v2.patch;https://issues.apache.org/jira/secure/attachment/12513313/0001-CASSANDRA-3838-v2.patch,05/Feb/12 06:52;vijay2win@yahoo.com;0001-CASSANDRA-3838.patch;https://issues.apache.org/jira/secure/attachment/12513289/0001-CASSANDRA-3838.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-02-04 17:23:14.233,,,no_permission,,,,,,,,,,,,226231,,,Sun Feb 05 21:40:51 UTC 2012,,,,,,0|i0gotr:,95452,slebresne,slebresne,,,,,,,,,"04/Feb/12 17:23;slebresne;Is there any usefulness to set the SO_TIMEOUT on the socket that is writing?

I also wonder if we really should reuse the rpc timeout for this (and my initial intuition is that we probably shouldn't). As far as I'm concerned, I'm fine adding a new streaming_socket_timeout option for this (we don't even have to document it in the yaml if we consider it's an advanced thing).","04/Feb/12 21:51;vijay2win@yahoo.com;Hi Sylvain,
My observation on this is that... when there is network congestion the Routers will start to drop the packets and which will cause the write on the socket to hang.... Until we write again to the socket we will not know if the socket is closed or not... hence it will be better to have it in both the sides... 

I will add streaming_socket_timeout and add documentation in the next patch... if you are ok with the above Thanks!","05/Feb/12 00:57;scode;Note that simply adding a socket timeout is not a good idea unless both sides are truly expected to never starve (this is why I didn't suggest it for CASSANDRA-3569, and why TCP keep-alive is the ""correct"" solution because it does not generate spurious timeouts by lack of in-band data on the channel - but as noted in that ticket, the practical reality is that we don't control keep alive parameters on a per-socket basis).

For example if one of the ends is waiting for a few seconds for a particularly expensive fsync(), or waiting for some kind of lock, you'd have spurious failures (whereas this is not the case for keep-alive, because the transport is alive and kicking at the kernel level). Depending on surrounding logic, it could be dangerous if it causes the receiver to believe it received the file while the sender believes it doesn't (e.g. multiple streaming -> disk space explosion).

I would suggest TCP keep-alive for the reasons mentioned here and discussed in CASSANDRA-3569, and suggest that the TCP keep-alive settings be tweaked to fail quicker if that is desired.

If adding a socket timeout, thought needs to go into what kind of false failure cases will be created. If both ends are truly expected not to block on anything like compaction locks or whatever else there might be, it might be okay.

In either case, definitely *don't* use rpc timeout IMO; the concerns are completely different. A low-timeout cluster with an rpc timeout of 0.5 seconds for example would be extremely sensitive to even the slightest hiccup (such as waitnig 1 second for an fsync(), or a GC pause, etc) and it would truly be useless and extremely damaging to kill streams for that.

In general, as with CASSANDRA-3569, I strongly argue that streaming should not be caused to spuriously fail because the impact of that can be huge, particularly on clusters with large nodes.

As for reads vs. writes: You definitely want timeouts on both sides in order to guarantee that you never hang under any circumstance regardless of the nature of the TCP connection loss, unless you have some other method to accomplish the same thing.

If this (socket timeouts) does go in, I argue even more strongly than before that the tear-down of streams due to failure detector as in CASSANDRA-3569 is truly just negative rather than positive (but as noted in that ticket, not hanging forever on repairs and such remains a concern).
","05/Feb/12 00:58;scode;Vijay, I do believe though that if you don't care about having to wait for a few hours for streams to abort, simply setting keep alive is the easiest and least-likely-to-have-negative-side-effects fix to your problem of inter-dc streams.
","05/Feb/12 01:02;scode;Turns out we can do per-socket keep-alive on Linux if we're willing to be platform specific, see https://issues.apache.org/jira/browse/CASSANDRA-3569?focusedCommentId=13200613&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13200613","05/Feb/12 01:24;scode;Let me be more clear about why keep-alive is better.

TCP keep-alive is at the transport level, and thus independent of in-band data (or lack thereof). Imagine that you're implementing a remote procedure call protocol where the client sends:

{code}
INVOKE name-of-process arg1 arg2
{code}

The server invokes the method, and responds:

{code}
RET success|failure exit-value|exception
{code}

The first thing you need if you are using this in some kind of production scenario, is to ensure that requests can time out. But there is a problem. Suppose you're making the assumption that this software is running on well-connected networks and a high number of requests per second; there is no reason to not quickly time out requests if the remote host is unreachable. So you set a socket timeout to 1 second. The only problem is that it will also time out on all requests that take longer than 1 second because the method call legitimately took longer.

The conflict happens because the selection of timeout was made based on the transport level circumstances (fast local network, high throughput, no need to wait if a host is down) while the effect of the timeout is at the in-band data level and is thus triggered by a slow request.

One way to fix this is to extend the protocol between client and server such that they can constantly be exchanging PING/PONG type messages (witness IRC for an example of this). This allows you to utilize socket (or read/write op) timeouts to detect a broken transport, under the assumption/premise that both sides have dedicated code for the ping/pong stuff which is independent of any delay in processing the otherwise in-band data.

Disadvantages of this approach can include the need to actually change the protocol, and (depending on implementation) additional implementation complexity as you suddenly need to actively model the transport as such.

TCP keep-alive is a way to let the kernel/tcp, which is already supposed to support this, deal with this without adding complexity to the application. It allows what effectively boils down to a ""timeout"" at the transport level which can be selected based on use-case and expected networking characteristics, and is independent of the nature of the in-band data sent over that transport.

In the Cassandra case, the equivalent of the slow RPC call might be that a write() during streaming blocks for 5 seconds because socket buffers on both ends are full, and the other end is going a GC or waiting on an fsync().

By using keep-alives we get more ""correct"" behavior in that such blocks won't cause connection tear-downs, while at the same time not having to change the protocol and/or add complexity to the code base to implement a protocol-within-tcp in which to mux the actual payload for streaming.
","05/Feb/12 01:27;vijay2win@yahoo.com;>>>> In either case, definitely don't use rpc timeout IMO; the concerns are completely different. A low-timeout cluster with an rpc timeout of 0.5 seconds 
We will add a configuration  streaming_socket_timeout  which will be different than rpc_timeout...  

>>> If this (socket timeouts) does go in, I argue even more strongly than before that the tear-down of streams due to failure detector as in CASSANDRA-3569
I dont have any option on that ticket, but it looks reasonable. I would say so_timeout will be a better solution for streaming as it is not a long lived connections... but i also think Keep alive should be set for the Messaging connection as you mentioned in the other ticket.

>>> I do believe though that if you don't care about having to wait for a few hours for streams to abort
We definitely dont want to wait for hours.... And i dont think we have to wait for hours when we have a better option, even if we set streaming_socket_timeout to 30 Seconds or even a minute.

>>> As for reads vs. writes: You definitely want timeouts on both sides in order to guarantee that you never hang under any circumstance 
Agree, i will get the patch done in few min.","05/Feb/12 16:32;slebresne;The default for this should be to not timeout at all, we should be conservative.

I'm good with keep alive on principle, but I think the actual hitch Vijay is trying to hitch is to not wait hours to retry the repair. So given that for keep alive that would involve potential non-portable code and such, let's keep that for later unless someone is willing to actually write that keep-alive patch in a timely fashion. ",05/Feb/12 18:30;vijay2win@yahoo.com;Hi Sylvain the default is set to no timeout in the new patch. Thanks!,"05/Feb/12 20:03;scode;{quote}
So given that for keep alive that would involve potential non-portable code and such, let's keep that for later unless someone is willing to actually write that keep-alive patch in a timely fashion.
{quote}

I have nothing against that at all, to be clear. I didn't mean to sound like I was arguing against this going in. I am very much for it, +1. In fact I'd rather have this just be turned on by default (with a reasonably high default timeout) than having FD convictions kill streams. So no argument here.

Timeouts like these can be exchanged for keep-alive:s in the future without really affecting the surrounding logic, if that feature becomes available.","05/Feb/12 21:40;slebresne;Commmitted, thanks.

I've slightly updated the option name to streaming_socket_timeout_in_ms (I added the _in_ms) to mirror the rpc_timeout_in_ms option and to avoid having people wonder which unit that was.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in AntiEntropyService$RepairSession.completed(),CASSANDRA-3548,12533435,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,amorton,amorton,01/Dec/11 11:35,12/Mar/19 13:59,13/Mar/19 22:26,02/Dec/11 10:54,1.0.6,,,,,,0,,,,,"This may be related to CASSANDRA-3519 (cluster it was observed on is still 1.0.1), however i think there is still a race condition.

Observed on a 2 DC cluster, during a repair that spanned the DC's.  

{noformat}
INFO [AntiEntropyStage:1] 2011-11-28 06:22:56,225 StreamingRepairTask.java (line 136) [streaming task #69187510-1989-11e1-0000-5ff37d368cb6] Forwarding streaming repair of 8602 
ranges to /10.6.130.70 (to be streamed with /10.37.114.10)
...
 INFO [AntiEntropyStage:66] 2011-11-29 11:20:57,109 StreamingRepairTask.java (line 253) [streaming task #69187510-1989-11e1-0000-5ff37d368cb6] task succeeded
ERROR [AntiEntropyStage:66] 2011-11-29 11:20:57,109 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[AntiEntropyStage:66,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.completed(AntiEntropyService.java:712)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer$1.run(AntiEntropyService.java:912)
        at org.apache.cassandra.streaming.StreamingRepairTask$2.run(StreamingRepairTask.java:186)
        at org.apache.cassandra.streaming.StreamingRepairTask$StreamingRepairResponse.doVerb(StreamingRepairTask.java:255)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
{noformat}

One of the nodes involved in the repair session failed, e.g. (Not sure if this is from the same repair session as the streaming task above, but it illustrates the issue)

{noformat}
ERROR [AntiEntropySessions:1] 2011-11-28 19:39:52,507 AntiEntropyService.java (line 688) [repair #2bf19860-197f-11e1-0000-5ff37d368cb6] session completed with the following error
java.io.IOException: Endpoint /10.29.60.10 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
ERROR [GossipTasks:1] 2011-11-28 19:39:52,507 StreamOutSession.java (line 232) StreamOutSession /10.29.60.10 failed because {} died or was restarted/removed
ERROR [GossipTasks:1] 2011-11-28 19:39:52,571 Gossiper.java (line 172) Gossip error
java.util.ConcurrentModificationException
        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:782)
        at java.util.ArrayList$Itr.next(ArrayList.java:754)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:190)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)

{noformat}

When a node is marked as failed AntiEntropyService.RepairSession.forceShutdown() clears the activejobs map. But the jobs to other nodes will continue, and will eventually call completed(). 

RepairSession.terminated should stop completed() from checking the map, but there is a race between the map been cleared and if there is an error in finally block it wont be set. 
","Free BSD 8.2, JVM vendor/version: OpenJDK 64-Bit Server VM/1.6.0",,,,,,,,,,,,,,,01/Dec/11 11:43;amorton;0001-3548.patch;https://issues.apache.org/jira/secure/attachment/12505756/0001-3548.patch,01/Dec/11 12:03;slebresne;3548-v2.patch;https://issues.apache.org/jira/secure/attachment/12505760/3548-v2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-12-01 12:03:49.632,,,no_permission,,,,,,,,,,,,219163,,,Fri Dec 02 10:54:38 UTC 2011,,,,,,0|i0gl9j:,94875,amorton,amorton,,,,,,,,,01/Dec/11 11:43;amorton;check for null,01/Dec/11 12:03;slebresne;We also have the same race in rendezvous with the jobs. Attaching v2 that fix both.,"01/Dec/11 20:16;amorton;Much nicer. I'm not able to test in place on the cluster but +1.

thanks.","02/Dec/11 10:54;slebresne;bq. I'm not able to test in place on the cluster

You would probably have a hard time reproducing this anyway.

Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memtable_total_space_in_mb does not accept the value 0 in Cassandra 1.0,CASSANDRA-3246,12524380,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,thobbs,thobbs,23/Sep/11 05:48,12/Mar/19 13:59,13/Mar/19 22:26,23/Sep/11 07:28,1.0.0,,,,,,0,,,,,"This affects 1.0 beta1.

From the key explanation in cassandra.yaml it looks like it should accept the value ""0""

# Total memory to use for memtables. Cassandra will flush the largest
# memtable when this much memory is used.
# If omitted, Cassandra will set it to 1/3 of the heap.
# If set to 0, only the old flush thresholds are used.
memtable_total_space_in_mb: 0

However in the code I could see the following:

if (conf.memtable_total_space_in_mb <= 0)
throw new ConfigurationException(""memtable_total_space_in_mb must be positive"");
logger.info(""Global memtable threshold is enabled at {}MB"", conf.memtable_total_space_in_mb);",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-09-23 07:28:05.231,,,no_permission,,,,,,,,,,,,3527,,,Fri Sep 23 07:28:05 UTC 2011,,,,,,0|i0ghl3:,94279,,,,,,,,,,,"23/Sep/11 07:28;jbellis;The old thresholds were made no-ops in CASSANDRA-2449.  I removed the line ""If set to 0, only the old flush thresholds are used"" just now in r1174563.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inferred Rack and DC Values Should be Unsigned,CASSANDRA-2651,12507209,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,jpisk,jpisk,14/May/11 08:27,12/Mar/19 13:59,13/Mar/19 22:26,16/May/11 21:44,0.8.1,,,,,,0,,,,,RackInferringSnitch formats IP address octets as signed byte values when inferring rack and data center values.,,,,,,,,,,,,,,,,14/May/11 08:29;jpisk;trunk-2651.txt;https://issues.apache.org/jira/secure/attachment/12479219/trunk-2651.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-16 21:44:22.071,,,no_permission,,,,,,,,,,,,20753,,,Mon May 16 21:44:22 UTC 2011,,,,,,0|i0gcn3:,93478,,,,,,,,,,,14/May/11 08:29;jpisk;RackInferringSnitch patch,16/May/11 21:44;brandon.williams;Committed to 0.8.1 to go along with CASSANDRA-2531,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inconsistent implementation of 'cumulative distribution function' for Exponential Distribution,CASSANDRA-2597,12506089,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thepaul,jbellis,jbellis,03/May/11 19:28,12/Mar/19 13:59,13/Mar/19 22:26,23/May/11 19:44,0.8.1,,,,,,0,,,,,"As reported on the mailing list (http://mail-archives.apache.org/mod_mbox/cassandra-dev/201104.mbox/%3CAANLkTimdMSLE8-z0x+0kvzqp7za3AEMLaOFXvd4Z=tvc@mail.gmail.com%3E),

{quote}
I just found there are two implementations of 'cumulative distribution
function' for Exponential Distribution and there are inconsistent :

*FailureDetector*
{code:java}
org.apache.cassandra.gms.ArrivalWindow.p(double)
   double p(double t)
   {
       double mean = mean();
       double exponent = (-1)*(t)/mean;
       return *Math.pow(Math.E, exponent)*;
   }
{code}

*DynamicEndpointSnitch*
{code:java}
org.apache.cassandra.locator.AdaptiveLatencyTracker.p(double)
   double p(double t)
   {
       double mean = mean();
       double exponent = (-1) * (t) / mean;
       return *1 - Math.pow( Math.E, exponent);*
   }
{code}

According to the  Exponential Distribution cumulative distribution function
definition<http://en.wikipedia.org/wiki/Exponential_distribution#Cumulative_distribution_function>,
the later one is correct
{quote}

... however FailureDetector has been working as advertised for some time now.  Does this mean the Snitch version is actually wrong?",,,,,,,,,,,,,,,,19/May/11 23:57;thepaul;0001-simplify-failure-detection-calculations.txt;https://issues.apache.org/jira/secure/attachment/12479860/0001-simplify-failure-detection-calculations.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-19 21:15:03.393,,,no_permission,,,,,,,,,,,,20721,,,Mon May 23 20:32:28 UTC 2011,,,,,,0|i0gcbj:,93426,brandon.williams,brandon.williams,,,,,,,,,"19/May/11 21:15;thepaul;Neither are wrong, as far as getting valid answers. Both are wrong in that they do much more work than necessary.

h3. FailureDetector/ArrivalWindow

In creating their failure predictor between gossip nodes in a distributed system, the original Cassandra authors made a modification to the sample φ Accrual Failure Detector implementation from the [original paper|http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.7427&rep=rep1&type=pdf]. They mention in their own [Cassandra paper|http://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf] that ""Although the original paper suggests that the distribution is approximated by the Gaussian distribution we found the Exponential Distribution to be a better approximation, because of the nature of the gossip channel and its impact on latency."" Nothing more is said on the topic, but it was likely because the original Phi Accrual paper implementation was expecting regular heartbeat messages, while ArrivalWindow measures only the intervals between reception of Gossip 'Syn', 'Ack', and 'Ack2' messages from a given endpoint. Regular message transmissions experiencing typical random jitter [will follow a normal distribution|http://www.maxim-ic.com/app-notes/index.mvp/id/1916/CMP/WP-34], but since gossip messages from endpoint A to endpoint B are sent at random intervals, they likely make up a [Poisson process|http://en.wikipedia.org/wiki/Poisson_process], making the exponential distribution appropriate.

I'll show the math here, since someone at some point will probably wonder wtf is going on in this code again, and hopefully I can save them the depth of exploration I went to.

Take the definition of {{P_later}}, which determines how likely it is that endpoint B has failed. The {{t}} parameter is the amount of time elapsed since the last Syn/Ack/Ack2 gossip message seen from endpoint B:

{code}
    P_later(t) = 1 - F(t)
{code}

where {{F(t)}} is the [CDF|http://en.wikipedia.org/wiki/Cumulative_distribution_function] of the event distribution. For the exponential distribution, the CDF is {{1 - e^(-Lt)}}, where {{L}} is the rate parameter.

{code}
    P_later(t) = 1 - (1 - e^(-Lt))
{code}

The maximum likelihood estimation for the rate parameter L is given by {{1/mean}}, where mean is the arithmetic mean of observed times from the actual data (here, the most recent gossip message inter-arrival times from endpoint B). It is this rate parameter we expect to vary over time, making necessary the storage of the sliding window of arrival intervals.

{code}
    P_later(t) = 1 - (1 - e^(-t/mean))
{code}

The original Cassandra authors stopped here. The Apache Cassandra developers made the obvious simplification:

{code}
    P_later(t) = e^(-t/mean)
{code}

But I will go further and look at the way {{P_later}} is used in the phi calculation:

{code}
    phi(t) = -log10(P_later(t))
{code}

Expanding to:

{code}
    phi(t) = -log10(e^(-t/mean))
{code}

But wait, the log of an exponentiation? Doesn't that mean...

{code}
    phi(t) = -log(e^(-t/mean)) / log(10)
           = (t/mean) / log(10)
{code}

so approximately

{code}
    phi(t) = 0.4342945 * t/mean
{code}

Yep, that's a hell of a lot simpler for computers to calculate than

{code}
    (-1) * Math.log10(Math.pow(Math.e, ((-1) * (t)/mean)))
{code}

, the way we have been doing it.

h3. DynamicEndpointSnitch/AdaptiveLatencyTracker

This version originated as an optimization of BoundedStatsDeque plus ArrivalWindow. The aim was to sort known endpoints by their {{phi}} values, assuming the same constant {{t}} value for each.

{code}
    score(E) = phi_E(0.0001)
{code}

(E is the endpoint, and {{phi_E}} is the {{phi}} function which uses the mean of recent message inter-arrival times from E, which we'll call {{E_mean}}). Expanded:

{code}
score(E) = -log10(P_later_E(0.0001))

P_later_E(t) = e^(-t/E_mean)
{code}

However, when the developer found that the {{score()}} values were going _down_ for nodes with higher average latency instead of up, he most likely looked at the original version of {{P_later()}} and added back one of the ""one minuses"", and not the other, because that made the signs work as expected. It currently uses:

{code}
    P_later_E(t) = 1 - e^(-t/E_mean)
{code}

However, this was probably misguided: the scores were going down because the phi accrual failure detector assigns higher badness values to nodes with a low recent latency than to nodes with high recent latency, given the same {{t}} values for both (because it waits longer to convict a node with high recent latency). There's no good mathematical reason for the {{AdaptiveLatencyTracker.p()}} method to look exactly like it does. However, it's mathematically correct in that it will indeed sort endpoints by recent latency.

{code}
    score(E_mean) = -log10(1 - e^(-0.0001/E_mean))
{code}

The {{score()}} method is only used as a measure to sort endpoints. However, as defined, it is monotonically increasing \(*), meaning that the exact same sorting could be made using the identity function:

{code}
    score(E_mean) = E_mean
{code}

I will attach a patch which makes these simplifications and adjusts the related unit tests accordingly.

\(*) The math to show this seems unnecessary here. It's not too hard to work through though.

Jira wiki syntax sucks.",19/May/11 23:57;thepaul;No changes to the tests were necessary.,23/May/11 19:44;brandon.williams;Great analysis!  Committed.,"23/May/11 20:32;hudson;Integrated in Cassandra-0.8 #128 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/128/])
    Simplify FD/DES calculations.
Patch by Paul Cannon, reviewed by brandonwilliams for CASSANDRA-2597

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1126682
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/FailureDetector.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
include indexes in snapshots,CASSANDRA-2596,12506088,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,03/May/11 19:18,12/Mar/19 13:59,13/Mar/19 22:26,05/May/11 14:25,0.7.6,0.8.0,,,,,1,,,,,CFS.snapshot should include index sstables as well.  Since flushing the parent CF (which we do as part of snapshot) also flushes index CFs consistently w/ the parent data this should work as expected.,,,,,,,,,,,,,,,,03/May/11 19:19;jbellis;2596.txt;https://issues.apache.org/jira/secure/attachment/12478087/2596.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-05 14:25:34.526,,,no_permission,,,,,,,,,,,,20720,,,Tue May 10 22:30:29 UTC 2011,,,,,,0|i0gcbb:,93425,slebresne,slebresne,,,,,,,,,"05/May/11 14:25;slebresne;+1, committed.","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MarshalException is thrown when cassandra-cli creates the example Keyspace specified by conf/schema-sample.txt,CASSANDRA-2390,12502469,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,yaojingguo,yaojingguo,26/Mar/11 16:31,12/Mar/19 13:59,13/Mar/19 22:26,27/Mar/11 05:28,0.7.5,,,,,,0,,,,,"Use the following steps to recreate the bug:

1. Checkout the source code from trunk. For my case, revision is 1085753.
2. Run ""ant"" to build cassandra.
3. Run ""bin/cassandra -f"" to start cassandra.
4. Run ""bin/cassandra-cli -host localhost --file conf/schema-sample.txt"".

Then there is the following message:
{quote}
... schemas agree across the cluster
Line 9 => org.apache.cassandra.db.marshal.MarshalException: cannot parse 'birthdate' as hex bytes
{quote}
The root cause is BytesType's fromString method. FBUtilities's hexToBytes method is invoked with ""birthdate"". NumberFormatException is thrown since ""birthdate"" is not a hex string.

{code:title=BytesType.java|borderStyle=solid}

    public ByteBuffer fromString(String source)
    {
        try
        {
            return ByteBuffer.wrap(FBUtilities.hexToBytes(source));
        }
        catch (NumberFormatException e)
        {
            throw new MarshalException(String.format(""cannot parse '%s' as hex bytes"", source), e);
        }
    }
{code}",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-27 05:28:56.125,,,no_permission,,,,,,,,,,,,20598,,,Sun Mar 27 05:44:34 UTC 2011,,,,,,0|i0gb3j:,93228,,,,,,,,,,,26/Mar/11 16:42;yaojingguo;[CASSANDRA-2262|https://issues.apache.org/jira/browse/CASSANDRA-2262] makes BytesType's fromString method only accept hex strings.,27/Mar/11 05:28;jbellis;fixed in r1085877 by adding comparator=UTF8Type to the CF definition.  Thanks for catching that!,"27/Mar/11 05:44;hudson;Integrated in Cassandra-0.7 #409 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/409/])
    specify UTF8Type comparator to fix regression found by Jingguo Yao
patch by jbellis for CASSANDRA-2390
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error upgrading when replication_factor is stored in strategy_options,CASSANDRA-3011,12518462,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,10/Aug/11 14:54,12/Mar/19 13:59,13/Mar/19 22:26,10/Aug/11 19:21,0.8.4,,,,,,0,,,,,"[from the ML]

{noformat}
When I upgraded from 0.8.2 to 0.8.3 I encountered a exception during startup:
...
Caused by: org.apache.cassandra.config.ConfigurationException:
replication_factor is an option for SimpleStrategy, not
NetworkTopologyStrategy
       at org.apache.cassandra.locator.NetworkTopologyStrategy.<init>(NetworkTopologyStrategy.java:70)
...
{noformat}",,,,,,,,,,,,,,,,10/Aug/11 14:55;jbellis;3011.txt;https://issues.apache.org/jira/secure/attachment/12489985/3011.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-10 15:58:50.876,,,no_permission,,,,,,,,,,,,20936,,,Wed Aug 10 17:20:04 UTC 2011,,,,,,0|i0getz:,93833,brandon.williams,brandon.williams,,,,,,,,,10/Aug/11 15:58;brandon.williams;+1,"10/Aug/11 17:20;hudson;Integrated in Cassandra-0.8 #268 (See [https://builds.apache.org/job/Cassandra-0.8/268/])
    ignore saved replication_factor strategy_option for NTS
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3011

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156244
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/KSMetaData.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merkle tree splitting can exit early,CASSANDRA-2605,12506185,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,04/May/11 15:51,12/Mar/19 13:59,13/Mar/19 22:26,05/May/11 08:53,0.8.0,,,,,,0,,,,,"There was a small bug introduced by CASSANDRA-2324 that, depending on the key sample token, can make the merkle tree splitting process exit early, potentially resulting in a unnecessary imprecise tree.",,7200,7200,,0%,7200,7200,,,,,,,,,04/May/11 15:52;slebresne;0001-Avoid-stopping-merkle-tree-splitting-too-soon.patch;https://issues.apache.org/jira/secure/attachment/12478169/0001-Avoid-stopping-merkle-tree-splitting-too-soon.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-04 17:56:21.879,,,no_permission,,,,,,,,,,,,20727,,,Wed May 11 21:28:24 UTC 2011,,,,,,0|i0gcdb:,93434,stuhood,stuhood,,,,,,,,,04/May/11 17:56;stuhood;+1,"05/May/11 08:53;slebresne;Committed, thanks","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ","11/May/11 21:28;hudson;Integrated in Cassandra #892 (See [https://builds.apache.org/hudson/job/Cassandra/892/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fatal exception in thread Thread ,CASSANDRA-3522,12532356,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,brandon.williams,grosendorf,grosendorf,22/Nov/11 15:55,12/Mar/19 13:59,13/Mar/19 22:26,28/Nov/11 15:22,1.0.3,,,,,,0,,,,,"Seeing this recurring exception on all machines in a 5 node cluster.  Recently upgraded to 1.0.0 from 0.7.2. 

ERROR [MutationStage:20] 2011-11-22 15:25:55,758 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:20,5,main]
java.lang.AssertionError
	at org.apache.cassandra.locator.TokenMetadata.getToken(TokenMetadata.java:273)
	at org.apache.cassandra.service.StorageProxy$4.run(StorageProxy.java:350)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",Ubuntu 10.04 LTS - 8GB mem/ 360 GB disk - Hosted by RS,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-11-22 16:05:38.507,,,no_permission,,,,,,,,,,,,218089,,,Tue Nov 22 21:45:18 UTC 2011,,,,,,0|i0gkxz:,94823,,,,,,,,,,,22/Nov/11 16:05;jbellis;please upgrade to 1.0.3 to verify that this isn't an already-fixed bug like CASSANDRA-2961,"22/Nov/11 21:17;grosendorf;Upgraded to 1.0.3, still getting the same exception on all machines.  Disk usage is also going up quickly on all the servers.",22/Nov/11 21:26;jbellis;What does nodetool ring look like?,"22/Nov/11 21:30;grosendorf;Looks like this on one node:

Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               141784319550391032739561396922763706368     
10.183.65.16    datacenter1 rack1       Up     Normal  129.55 GB       16.67%  0                                           
10.183.47.195   datacenter1 rack1       Up     Normal  181.65 GB       33.33%  56713727820156407428984779325531226112      
10.182.96.44    datacenter1 rack1       Up     Normal  165.17 GB       16.67%  85070591730234615865843651857942052864      
10.179.64.134   datacenter1 rack1       Up     Normal  128.11 GB       16.67%  113427455640312814857969558651062452224     
10.183.65.18    datacenter1 rack1       Up     Normal  140.6 GB        16.67%  141784319550391032739561396922763706368 

But on another node, it all of a sudden thinks that a node that I decommissioned yesterday is still part of the ring, just down:

Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               141784319550391032739561396922763706368     
10.183.65.16    datacenter1 rack1       Up     Normal  129.55 GB       16.67%  0                                           
10.183.47.195   datacenter1 rack1       Up     Normal  181.65 GB       33.33%  56713727820156407428984779325531226112      
10.182.96.44    datacenter1 rack1       Up     Normal  165.17 GB       16.67%  85070591730234615865843651857942052864      
10.179.64.134   datacenter1 rack1       Up     Normal  128.11 GB       16.67%  113427455640312814857969558651062452224     
10.179.77.102   datacenter1 rack1       Down   Normal  ?               7.61%   126376470034382387444230631795475767049     
10.183.65.18    datacenter1 rack1       Up     Normal  140.6 GB        9.06%   141784319550391032739561396922763706368

","22/Nov/11 21:31;grosendorf;Also, since the upgrade from 1.0.0 to 1.0.3 I'm getting Hinted Handoff Exceptions:


 INFO [HintedHandoff:4] 2011-11-22 21:28:48,046 HintedHandOffManager.java (line 268) Started hinted handoff for token: 113427455640312814857969558651062452224 with IP: /10.179.64.134
ERROR [HintedHandoff:4] 2011-11-22 21:28:48,077 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:4,1,main]
java.lang.AssertionError
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:301)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
	at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",22/Nov/11 21:37;jbellis;The HH assertion means you had some hint corruption from 1.0.0 (CASSANDRA-3466).  You should remove your Hint column families from the system/ keyspace.,"22/Nov/11 21:45;grosendorf;Ok, removing the Hint CFs took care of the Hinted Handoff exception. I ran nodetool removetoken on that errant token from the machine that I decommissioned yesterday, so nodetool ring is the same on all machines.  I did a full cluster restart, and so far I'm not seeing any exceptions.  
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can create a Column Family with comparator CounterColumnType which is subsequently unusable,CASSANDRA-3422,12529368,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,kreynolds,kreynolds,29/Oct/11 16:46,12/Mar/19 13:59,13/Mar/19 22:26,07/Dec/11 16:10,0.8.9,1.0.6,,,,,0,,,,,"It's probably the case that this shouldn't be allowed at all but one is currently allowed to create a Column Family with comparator CounterColumnType which then appears unusable.

CREATE COLUMNFAMILY comparator_cf_counter (id text PRIMARY KEY) WITH comparator=CounterColumnType

# Fails
UPDATE comparator_cf_counter SET 1=1 + 1 WHERE id='test_key'

Error => invalid operation for non commutative columnfamily comparator_cf_counter",,,,,,,,,,,,,,,,06/Dec/11 15:50;slebresne;3422-v2.patch;https://issues.apache.org/jira/secure/attachment/12506266/3422-v2.patch,01/Nov/11 13:29;slebresne;3422.patch;https://issues.apache.org/jira/secure/attachment/12501768/3422.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-01 13:30:23.959,,,no_permission,,,,,,,,,,,,215228,,,Wed Dec 07 17:51:13 UTC 2011,,,,,,0|i0gjpb:,94622,jbellis,jbellis,,,,,,,,,"30/Oct/11 13:21;kreynolds;One can also create a table with CounterColumnType as the row_key type, probably should disallow that.",01/Nov/11 13:30;slebresne;Patch is against 1.0. I'll rebase against 0.8 for the commit.,01/Nov/11 14:19;jbellis;Shouldn't this go in ThriftValidation.validateCfDef (which is already called by QP as well as the thrift side)?,"01/Nov/11 14:25;slebresne;bq. which is already called by QP

I don't see that. I see it called for CREATE_INDEX but not for CREATE_COLUMNFAMILY, for which it seems we never create a thrift CfDef (which is reasonable). I mean, all this is imho a complete mess, between CFM, thrift CfDef, avro CfDef, etc... seems there is nothing consistent in there.
I'm happy to do this differently but I failed to see the correct place for that.",24/Nov/11 15:33;slebresne;Any opinion related to my previous comment?,"02/Dec/11 22:58;jbellis;You're right, it's all over the place.  CFMD is fine.  But it also needs to check that we don't use CCT as an individual column type.  (CASSANDRA-2614.)",06/Dec/11 15:50;slebresne;Attached v2 checks we don't mix counter and non-counters,06/Dec/11 16:12;jbellis;+1,"07/Dec/11 17:51;hudson;Integrated in Cassandra-0.8 #411 (See [https://builds.apache.org/job/Cassandra-0.8/411/])
    Detect misuses of CounterColumnType
patch by slebresne; reviewed by jbellis for CASSANDRA-3422

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211486
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/CreateColumnFamilyStatement.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove special-cased maximum on sstables-to-compact for leveled strategy,CASSANDRA-3182,12522760,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,12/Sep/11 16:01,12/Mar/19 13:59,13/Mar/19 22:26,13/Sep/11 04:46,1.0.0,,,,,,0,compaction,lcs,,,With CASSANDRA-3171 fixed we don't need to be scared of large numbers of compaction candidates anymore.,,,,,,,,,,,,,,,,12/Sep/11 16:05;jbellis;3182.txt;https://issues.apache.org/jira/secure/attachment/12494052/3182.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-09-13 04:21:14.666,,,no_permission,,,,,,,,,,,,4035,,,Tue Sep 13 04:46:48 UTC 2011,,,,,,0|i0ggsv:,94152,bcoverston,bcoverston,,,,,,,,,12/Sep/11 16:05;jbellis;as described.,13/Sep/11 04:21;bcoverston;+1,13/Sep/11 04:46;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigurationException when starting a node after deleting LocationInfo SStables,CASSANDRA-2632,12506792,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,11/May/11 01:19,12/Mar/19 13:59,13/Mar/19 22:26,11/May/11 09:10,0.7.6,,,,,,0,,,,,"from http://www.mail-archive.com/user@cassandra.apache.org/msg13170.html

SystemTable.checkHealth() assumes that if the LOCATION_KEY row is not in the STATUS system CF their should be no other files in the system data directory. If it's safe to delete the LocationInfo sstables this stops the server restarting.

I think the intention of the check is to assert that the reason the row was not found is that there is no data in the STATUS CF. ",,,,,,,,,,,,,,,,11/May/11 01:47;amorton;0001-only-check-for-existing-LocationInfo-SSTables-during.patch;https://issues.apache.org/jira/secure/attachment/12478755/0001-only-check-for-existing-LocationInfo-SSTables-during.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-11 09:10:51.908,,,no_permission,,,,,,,,,,,,20742,,,Wed May 11 09:38:52 UTC 2011,,,,,,0|i0gciv:,93459,slebresne,slebresne,,,,,,,,,11/May/11 01:47;amorton;Attached patch checks the SSTable count for the STATUS CF to determine if the reason the location info was not found was because the partitioner changed. ,"11/May/11 09:10;slebresne;+1, committed. Thanks.","11/May/11 09:38;hudson;Integrated in Cassandra-0.7 #477 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/477/])
    Allow removing LocationInfo sstables (to allow cluster rename)
patch by amorton; reviewed by slebresne for CASSANDRA-2632
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
index scan errors out when zero columns are requested,CASSANDRA-2653,12507224,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,jbellis,jbellis,14/May/11 15:02,12/Mar/19 13:59,13/Mar/19 22:26,29/Jun/11 19:41,0.7.7,0.8.2,,,,,0,,,,,"As reported by Tyler Hobbs as an addendum to CASSANDRA-2401,

{noformat}
ERROR 16:13:38,864 Fatal exception in thread Thread[ReadStage:16,5,main]
java.lang.AssertionError: No data found for SliceQueryFilter(start=java.nio.HeapByteBuffer[pos=10 lim=10 cap=30], finish=java.nio.HeapByteBuffer[pos=17 lim=17 cap=30], reversed=false, count=0] in DecoratedKey(81509516161424251288255223397843705139, 6b657931):QueryPath(columnFamilyName='cf', superColumnName='null', columnName='null') (original filter SliceQueryFilter(start=java.nio.HeapByteBuffer[pos=10 lim=10 cap=30], finish=java.nio.HeapByteBuffer[pos=17 lim=17 cap=30], reversed=false, count=0]) from expression 'cf.626972746864617465 EQ 1'
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1517)
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,29/Jun/11 18:07;slebresne;0001-Fix-scan-issue.patch;https://issues.apache.org/jira/secure/attachment/12484669/0001-Fix-scan-issue.patch,22/Jun/11 12:29;slebresne;0001-Handle-data-get-returning-null-in-secondary-indexes.patch;https://issues.apache.org/jira/secure/attachment/12483436/0001-Handle-data-get-returning-null-in-secondary-indexes.patch,22/Jun/11 13:22;slebresne;0001-Handle-null-returns-in-data-index-query-v0.7.patch;https://issues.apache.org/jira/secure/attachment/12483440/0001-Handle-null-returns-in-data-index-query-v0.7.patch,30/May/11 11:07;slebresne;0001-Reset-SSTII-in-EchoedRow-constructor.patch;https://issues.apache.org/jira/secure/attachment/12480836/0001-Reset-SSTII-in-EchoedRow-constructor.patch,27/Jun/11 17:34;slebresne;2653_v2.patch;https://issues.apache.org/jira/secure/attachment/12483978/2653_v2.patch,28/Jun/11 08:10;slebresne;2653_v3.patch;https://issues.apache.org/jira/secure/attachment/12484407/2653_v3.patch,27/May/11 20:34;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2653-reproduce-regression.txt;https://issues.apache.org/jira/secure/attachment/12480690/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2653-reproduce-regression.txt,,,,,7.0,,,,,,,,,,,,,,,,,,,2011-05-27 20:34:51.988,,,no_permission,,,,,,,,,,,,20755,,,Wed Jun 29 19:41:51 UTC 2011,,,,,,0|i0gcnj:,93480,,,,,,,,,,,"27/May/11 20:34;tjake;Attached testcase reproduces the error every time

Run like:

ant long-test -Dtest.name=IndexCorruptionTest","27/May/11 20:37;tjake;{noformat}
long-test:
     [echo] running long tests
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/jake/workspace/cassandra-git/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.db.IndexCorruptionTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 247.427 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 16:31:03,330 Fatal exception in thread Thread[ReadStage:5,5,main]
    [junit] java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(Token(bytes[004600460048004d00540049005900590048005400460059004a0048004b0055004e00550048004b00530055005400480055004b004f004b004a00460058004600000001000100010001000100010001000100e3000100010001000100e3000100010001000100e3000100010001000100e30001000100010001000100010001000100010001000100010000000100010001000100010001000100010003000100010001000100030001000100010001000300010001000100010003000100010001000100010001000100010001000100010001]), 30303237623366662d326230662d343235632d386332352d616362326335393534306530):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 'inode.73656e74696e656c EQ 78'
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1517)
    [junit] 	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
    [junit] 	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:680)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: runTest(org.apache.cassandra.db.IndexCorruptionTest):	Caused an ERROR
    [junit] TimedOutException()
    [junit] java.io.IOException: TimedOutException()
    [junit] 	at org.apache.cassandra.db.IndexCorruptionTest.listDeepSubPaths(IndexCorruptionTest.java:107)
    [junit] 	at org.apache.cassandra.db.IndexCorruptionTest.runTest(IndexCorruptionTest.java:64)
    [junit] Caused by: TimedOutException()
    [junit] 	at org.apache.cassandra.thrift.Cassandra$get_indexed_slices_result.read(Cassandra.java:13801)
    [junit] 	at org.apache.cassandra.thrift.Cassandra$Client.recv_get_indexed_slices(Cassandra.java:810)
    [junit] 	at org.apache.cassandra.thrift.Cassandra$Client.get_indexed_slices(Cassandra.java:782)
    [junit] 	at org.apache.cassandra.db.IndexCorruptionTest.listDeepSubPaths(IndexCorruptionTest.java:90)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.IndexCorruptionTest FAILED

BUILD FAILED
/Users/jake/workspace/cassandra-git/build.xml:1082: The following error occurred while executing this line:
/Users/jake/workspace/cassandra-git/build.xml:1037: Some long test(s) failed.

Total time: 4 minutes 14 seconds

{noformat}","27/May/11 21:12;tjake;Definitely related to compaction...

The memtable flushes minute and the test fails > 4

If you change the min compaction thresh to 2 it fails > 2","30/May/11 11:07;slebresne;This is indeed compaction related (but not related to secondary indexing at
all). The problem is that compaction may lose some rows.

Because of the way the ReducingIterator works, when we create a new
{Pre|Lazy|Echoed}CompactedRow, we have already decoded the next row key and
the file pointer if after that next row key. Both PreCompactedRow and
LazyCompactedRow handle this correctly by ""resetting"" their
SSTableIdentityIterator before reading (SSTII.getColumnFamilyWithColumns()
does it for PreCompactedRow and LazilyCompactedRow calls SSTII.reset()
directly). But EchoedRow doesn't handle this correctly. Hence when
EchoedRow.isEmpty() is called, it will call SSTII.hasNext(), that will compare
the current file pointer to the finishedAt value of the iterator. The pointer
being on the next row, this test will always fail and the row will be skipped.

Attaching a patch against 0.8 with a (smaller) unit test.

Note that luckily this doesn't affect 0.7, because it only uses EchoedRow for
cleanup compactions and clean compactions does not use ReducingIterator (and
thus, the underlying SSTII won't have changed when the EchoedRow is built).
I would still be in favor of committing the patch there too, just to make sure
we don't hit this later.",30/May/11 12:47;jbellis;+1 for 0.7 / 0.8,"30/May/11 13:27;hudson;Integrated in Cassandra-0.7 #500 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/500/])
    Reset SSTII in EchoedRow iterator (see CASSANDRA-2653)

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1129151
Files : 
* /cassandra/branches/cassandra-0.7/test/unit/org/apache/cassandra/db/CompactionsTest.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/CompactionManager.java
",20/Jun/11 04:14;jbellis;Is this actually fixed for the zero-columns-requested original problem?,20/Jun/11 04:44;slebresne;This really primarily fixes the error from Jake's test cases. I'll have to admit that's the only I looked. I did not realize the original problem was not necessarily related and so it is very possible (even likely) this does not fix the zero-columns-requested problem.,"20/Jun/11 16:53;jbellis;reopening to fix ""the tyler issue.""","22/Jun/11 12:29;slebresne;The ""Tyler"" problem is actually not limited to 0 column query. The problem is that when we query the rows for data, we use whatever filter the user provided (there's a number of optimiziation in the case we have more than 1 clause but that doesn't really matter for our problem). The thing is, there is no guarantee that whatever that filter is, it will include the column of the primary clause (having a column count of 0 is just one case where we're sure it won't include it). Thus the assertion that something will be returned is bogus.

Attaching a patch (against 0.8) to fix. Note that this mean we have no way to assert the sanity of the index during a read, unless we force the querying of the primary index clause, but this will have a performance impact (and a non negligible one in cases this would force us to do a new query just for that).
",22/Jun/11 13:22;slebresne;This also affects 0.7 actually so attaching a patch for 0.7.,22/Jun/11 13:49;jbellis;Is there a way we can keep a sanity check here?  CASSANDRA-2401 was not so long ago.,"22/Jun/11 13:59;slebresne;As I said earlier, I think the only way to keep one would be to force the querying of the primary index clause column name. In some cases, when we already do a NameQuery, either as part of the first data query or because we need a query for the extraFilter, this won't be a big deal. If it's a slice query and the primary index clause name is part of the return, we're good to. But otherwise, we'll have to do a specific query to validate the assert. Maybe the cases where we'll have to do an extra query are considered low enough than we think it's worth. But then there is the other problem.

The other problem is that this assertion is not thread safe, because the query to the index and the data is not atomic. ","22/Jun/11 18:46;jbellis;bq. I think the only way to keep one would be to force the querying of the primary index clause column name... but this will have a performance impact

I think we should take the impact.  (The common query that we want to be fast is name-based and this won't affect that.)","27/Jun/11 17:34;slebresne;I actually agree with taking the impact. Especially given that there is actually very little cases where it will make an actual difference anyway.

Attaching patch (2653_v2, based on 0.7) that implement the idea and add back the sanity check.","27/Jun/11 21:48;jbellis;doesn't this assert still have the ""the query to the index and the data is not atomic"" problem?","28/Jun/11 08:24;slebresne;bq. doesn't this assert still have the ""the query to the index and the data is not atomic"" problem?

No you're right, I focused on adding back the assert forgetting it wasn't safe in the first place. Attaching v3 based on v2, but instead of asserting that the row return contains the primary clause column, it skips the row if it doesn't contain it. That is, instead of asserting the non-corruption of the index, it ignores any possible corruption. But more importantly (one could hope we don't have a bug that corrupt indexes), it will avoid returning incoherent result to the user in the event of a race between reads and writes.

Trying to prevent the race from happening would require synchronization with write, which will be much harder and less efficient. And we probably need to have a fix for that out sooner than later (both the error when zero columns are requested and the possibly to throw assertion errors wrongly).

In the longer term, I think we should explore the possibility of stopping to care whether our secondary indexes are coherent at all time and repair them at read time as  this may allow us to get rid of the read-before-write. But it's a longer term goal at best and work for another ticket.

 ",28/Jun/11 15:16;jbellis;+1,"29/Jun/11 15:48;hudson;Integrated in Cassandra-0.7 #517 (See [https://builds.apache.org/job/Cassandra-0.7/517/])
    Fix scan wrongly throwing assertion errors
patch by slebresne; reviewed by jbellis for CASSANDRA-2653

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1141129
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
","29/Jun/11 18:07;slebresne;Actually, after having committed it, I realize there is a few issue with the previous patch. Two mostly:
# If the extraFilter query finds nothing (which it will only in case of the race between write and reads), getColumnFamily() will return null and the data.addAll() will NPE
# For 0.8 and for counters, we must make really sure that this extra query won't add column that were returned by the first query (which can happen in the current code), otherwise we'll overcount. I think this is actually a bug that predate the fix for this.

Anyway, attaching 0001-Fix-scan-issue that fixes both of those issue. It also add a slight optimization that avoids doing extra work if we know an extra query won't help.",29/Jun/11 19:27;jbellis;+1,"29/Jun/11 19:41;slebresne;Committed, thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fsync the directory after new sstable or commit log segment are created,CASSANDRA-3250,12524426,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,hanzhu,hanzhu,23/Sep/11 14:39,12/Mar/19 13:59,13/Mar/19 22:26,19/Dec/11 23:50,1.1.0,,,,,,0,,,,,"The mannual of fsync said:
bq.   Calling  fsync()  does  not  necessarily  ensure  that  the entry in the directory containing the file has also reached disk.  For that an explicit fsync() on a file descriptor for the directory is also needed.

At least on ext4, syncing the directory is a must to have step, as described by [1]. Otherwise, the new sstables or commit logs could be missed after crash even if itself is synced. 

Unfortunately, JVM does not provide an approach to sync the directory...

[1] http://www.linuxfoundation.org/news-media/blogs/browse/2009/03/don%E2%80%99t-fear-fsync
",,,,,,,,,,,,HDFS-5042,,,,17/Dec/11 14:36;xedin;CASSANDRA-3250-v2.patch;https://issues.apache.org/jira/secure/attachment/12507780/CASSANDRA-3250-v2.patch,18/Dec/11 16:27;xedin;CASSANDRA-3250-v3.patch;https://issues.apache.org/jira/secure/attachment/12507839/CASSANDRA-3250-v3.patch,12/Dec/11 11:38;xedin;CASSANDRA-3250.patch;https://issues.apache.org/jira/secure/attachment/12506992/CASSANDRA-3250.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-12-15 22:43:33.365,,,no_permission,,,,,,,,,,,,1835,,,Mon Dec 19 23:50:49 UTC 2011,,,,,,0|i0ghmn:,94286,jbellis,jbellis,,,,,,,,,"23/Sep/11 14:42;hanzhu;There is a relative discussion on PostgreSQL.

http://postgresql.1045698.n5.nabble.com/fsync-reliability-td4330289.html","26/Sep/11 05:32;hanzhu;On XFS, this is not a problem per the developer's response[1].

[1] http://www.spinics.net/lists/xfs/msg07229.html","15/Dec/11 22:43;jbellis;- shouldn't the directory cache be per-Writer, not static?
- why would the FD be null?  if it shouldn't be, let's use an assert there","15/Dec/11 22:54;xedin;bq. shouldn't the directory cache be per-Writer, not static?

It's useful to make it static instead of instantiation per-Writer because we need every writer to be able to see already open directories.

bq. why would the FD be null? if it shouldn't be, let's use an assert there

Directory FD could be null when it's the first time we see directory e.g. first time for commitlog, different data directories, also I made it that way because of CASSANDRA-2749.","17/Dec/11 05:19;jbellis;bq. we need every writer to be able to see already open directories

While it doesn't really matter with the current directory-per-keyspace, syncing every directory open globally when any sstable is written could be problematic with CASSANDRA-2749 (especially with leveled compaction, which does lots of small sstable writes).  What's the problem with just having writers sync directories for files they're writing?","17/Dec/11 13:25;xedin;No problem with that, here is the version where SequentialWriter syncs only directory for file it writes.",17/Dec/11 14:36;xedin;re-attached v2 which does only one directory sync per file (after first file sync).,"18/Dec/11 06:28;jbellis;v2 looks a lot simpler, I like it.

Is there a reason to limit this to Linux-only?  If open + fsync + close exist, it should work.  (And if they don't, we already have the link error to tell us that.)",18/Dec/11 12:56;xedin;The reason is O_DIRECTORY is a Linux-specific flag.,"18/Dec/11 16:00;jbellis;Is O_DIRECTORY necessary for the sync to work?  Or is that just a ""make sure this path is actually a directory"" flag?  If the latter, then we can leave it out, since we know the path is a directory by construction.","18/Dec/11 16:27;xedin;Ok, v3 O_DIRECTORY changed to O_RDONLY and checks for linux are removed from CLibrary.

O_DIRECTORY just causes open to fail if given path is not a directory, so it seems we can live without it :)",19/Dec/11 04:54;jbellis;+1,19/Dec/11 09:23;xedin;Committed.,"19/Dec/11 23:25;jbellis;... actually, can you back this out of 1.0 and put it in 1.1 instead?  It *should* be fine but let's not take chances with regressions.",19/Dec/11 23:50;xedin;Changed to 1.1 only.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vague primary key references in CQL,CASSANDRA-3036,12518838,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,kreynolds,kreynolds,14/Aug/11 19:24,12/Mar/19 13:59,13/Mar/19 22:26,17/Aug/11 22:18,0.8.5,,,Legacy/CQL,,,0,core,cql,,,"create columnfamily wonk (id 'utf8' primary key, id int)
update wonk set id=1 where id='test'
create index wonk_id on wonk (id)

This does what you would expect but then the results are unclear when using 'id' in a where clause.

""select * from wonk where id=1"" returns nothing and ""select * from wonk where id='test'"" works fine.

Perhaps secondary indexes should not be allowed on columns that have the same name as the key_alias? At least a warning or something should be thrown to indicate you've just made a useless index.
",,,,,,,,,,,,,,,,17/Aug/11 21:08;xedin;CASSANDRA-3036-v2.patch;https://issues.apache.org/jira/secure/attachment/12490699/CASSANDRA-3036-v2.patch,17/Aug/11 17:36;xedin;CASSANDRA-3036.patch;https://issues.apache.org/jira/secure/attachment/12490674/CASSANDRA-3036.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-16 16:19:59.725,,,no_permission,,,,,,,,,,,,20943,,,Wed Aug 17 23:20:53 UTC 2011,,,,,,0|i0gezb:,93857,jbellis,jbellis,,,,,,,,,"16/Aug/11 16:19;jbellis;We should validate that column_metadata does not get created with the key_alias, or column insertions where the column name is the key_alias.  (For both CQL and ""old"" paths.)",17/Aug/11 12:42;xedin;That is impossible to validate when comparator is not AsciiType because we force key_alias to be ascii,"17/Aug/11 13:34;jbellis;Well, there's two ways we can compare them anyway:

- compare the native bytes 
- compare the human-readable strings

I think I'd prefer #1, because 99.9% of the time these are going to give the same answer for names/aliases that do not conflict, and String creation is relatively expensive so I'd rather not do that.","17/Aug/11 14:05;xedin;Sorry this is my bad because it thought that AsciiType.decompose and UTF8Type.decompose won't produce the same bytes, tested now. So yes, we can just check for default/column comparator and if it is Ascii/UTF8/Bytes we can just compare native bytes.",17/Aug/11 14:26;jbellis;My point is we can *always* just compare native bytes.,17/Aug/11 14:28;xedin;ok,"17/Aug/11 20:43;jbellis;- let's name the ColumnDef variable in validateCfDef something other than ""column"" (which usually means an IColumn or similar)
- when we throw an invalid column name error, we should use the CF's comparator rather than AsciiType (which will error out on many binary column names)
- CCFS calls getByteBuffer which catches MarshallException and turns it into IRE.  Suggest doing the validation ones from CCFS.validate, that way we don't have to do it again in CCFS.getColumns.  (Can just call AbstractType.fromString directly)
- go ahead and commit the println removal separately :)","17/Aug/11 21:08;xedin;bq. let's name the ColumnDef variable in validateCfDef something other than ""column"" (which usually means an IColumn or similar)

Renamed to columnDef

bq. when we throw an invalid column name error, we should use the CF's comparator rather than AsciiType (which will error out on many binary column names)

I use cf.key_alias in there which we force to be AsciiType so it should be fine

bq. CCFS calls getByteBuffer which catches MarshallException and turns it into IRE. Suggest doing the validation ones from CCFS.validate, that way we don't have to do it again in CCFS.getColumns. (Can just call AbstractType.fromString directly)

changed CCFS.getColumns to use comparator.fromString and changed validate method as you mentioned.",17/Aug/11 21:50;jbellis;+1,17/Aug/11 22:18;xedin;Committed.,"17/Aug/11 23:20;hudson;Integrated in Cassandra-0.8 #284 (See [https://builds.apache.org/job/Cassandra-0.8/284/])
    Validate that column names in column_metadata does not equal to key_alias on create/update of the ColumnFamily and CQL 'ALTER' statement.
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3036

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1158939
Files : 
* /cassandra/branches/cassandra-0.8/test/system/test_cql.py
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/CreateColumnFamilyStatement.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/AlterTableStatement.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC ResultSet does not honor column value typing for the CF and uses default validator for all column value types.,CASSANDRA-2410,12503071,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,gdusbabek,ardot,ardot,31/Mar/11 20:55,12/Mar/19 13:59,13/Mar/19 22:26,16/Apr/11 23:10,0.8 beta 1,,,Legacy/CQL,,,0,cql,,,,"Assume a CF declared in CQL as :
{code}
CREATE COLUMNFAMILY TestCF(KEY utf8 PRIMARY KEY,description utf8, anumber int)
  WITH comparator = ascii AND default_validation = long;
{code}

If the {{ResultSet}} is fetched thusly:


{code}
Statement stmt = con.createStatement();
ResultSet rs = stmt.executeQuery(query);

String description;
Integer anumber;

    while (rs.next())
    {
      description = rs.getString(1);
      System.out.print(""description : ""+ description);
      anumber = rs.getInt(2);
      System.out.print(""anumber     : ""+ anumber);
    }
{code}

It will immediately fail with a message of: 

{code}
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 16
	at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:66)
	at org.apache.cassandra.cql.jdbc.TypedColumn.<init>(TypedColumn.java:45)
	at org.apache.cassandra.cql.jdbc.ColumnDecoder.makeCol(ColumnDecoder.java:158)
	at org.apache.cassandra.cql.jdbc.CassandraResultSet.next(CassandraResultSet.java:1073)
	at da.access.testing.TestJDBC.selectAll(TestJDBC.java:83)
         ...
{code}


It appears that the {{makeCol}} method of {{ColumnDecoder.java}} chooses NOT to use the {{CfDef}} to look up the possible occurrence of a column? That's not right. Right? 

{code}
    /** constructs a typed column */
    public TypedColumn makeCol(String keyspace, String columnFamily, byte[] name, byte[] value)
    {
        CfDef cfDef = cfDefs.get(String.format(""%s.%s"", keyspace, columnFamily));
        AbstractType comparator = getComparator(keyspace, columnFamily, Specifier.Comparator, cfDef);
        AbstractType validator = getComparator(keyspace, columnFamily, Specifier.Validator, null);
        return new TypedColumn(comparator, name, validator, value);
    }
{code}



",,,,,,,,,,,,,,,,16/Apr/11 21:08;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-honor-specific-column-validators-in-JDBC-driver.txt;https://issues.apache.org/jira/secure/attachment/12476536/ASF.LICENSE.NOT.GRANTED--v1-0001-honor-specific-column-validators-in-JDBC-driver.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-16 21:09:38.092,,,no_permission,,,,,,,,,,,,20607,,,Sun Apr 17 01:11:50 UTC 2011,,,,,,0|i0gb7j:,93246,,,,,,,,,,,16/Apr/11 21:09;gdusbabek;I noticed a failure of org.apache.cassandra.db.RecoveryManagerTest after applying the patch.  Chances are it was failing before--I can't see how a jdbc change would affect that.,16/Apr/11 21:57;jbellis;+1,16/Apr/11 23:10;gdusbabek;committed.,"17/Apr/11 01:11;hudson;Integrated in Cassandra-0.8 #10 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/10/])
    honor specific column validators in JDBC driver. patch by gdusbabek, reviewed by jbellis. CASSANDRA-2410
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL counter support is undocumented,CASSANDRA-3013,12518477,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,10/Aug/11 16:42,12/Mar/19 13:59,13/Mar/19 22:26,10/Aug/11 17:25,0.8.4,,,Legacy/CQL,Legacy/Documentation and Website,,0,cql,,,,"When counter support was added to CQL, the documentation wasn't updated.",,,,,,,,,,,,,,,,10/Aug/11 16:43;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3013-add-missing-CQL-counter-documentation.txt;https://issues.apache.org/jira/secure/attachment/12490004/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3013-add-missing-CQL-counter-documentation.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-10 16:56:12.63,,,no_permission,,,,,,,,,,,,20937,,,Wed Aug 10 18:13:58 UTC 2011,,,,,,0|i0geuf:,93835,,,,,,,,,,,10/Aug/11 16:56;jbellis;+1,10/Aug/11 17:25;urandom;committed,"10/Aug/11 18:13;hudson;Integrated in Cassandra-0.8 #269 (See [https://builds.apache.org/job/Cassandra-0.8/269/])
    add missing CQL counter documentation

Patch by eevans; reviewed by jbellis for CASSANDRA-3013

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156268
Files : 
* /cassandra/branches/cassandra-0.8/doc/cql/CQL.textile
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"recognize that ""SELECT first ... *"" isn't really ""SELECT *""",CASSANDRA-3445,12529869,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,02/Nov/11 14:48,12/Mar/19 13:59,13/Mar/19 22:26,03/Nov/11 19:00,1.1.0,,,Legacy/CQL,,,0,cql,,,,"QueryProcessor includes the row key in ""first *"" because it mistakenly thinks the full row is being requested.  ""first *"" should really be treated like a slice.",,,,,,,,,,,,,,,,03/Nov/11 03:21;jbellis;3445-skeleton.txt;https://issues.apache.org/jira/secure/attachment/12502094/3445-skeleton.txt,03/Nov/11 11:14;xedin;CASSANDRA-3445.patch;https://issues.apache.org/jira/secure/attachment/12502128/CASSANDRA-3445.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-03 11:14:17.053,,,no_permission,,,,,,,,,,,,215728,,,Thu Nov 03 19:19:00 UTC 2011,,,,,,0|i0gjzr:,94669,jbellis,jbellis,,,,,,,,,03/Nov/11 03:21;jbellis;Would like to take the approach outlined here but there isn't actually a way to tell if the FIRST count was specified in the statement or not w/ the current antlr.  Re-assigning to Pavel for antlr help.,"03/Nov/11 11:14;xedin;Used Jonathan's skeleton with one modification - SelectStatement.isFullWildcard () was changed to use !hasFirstSet because I think that intention here is not to include the key when ""FIRST"" keyword is set.",03/Nov/11 17:17;jbellis;+1,03/Nov/11 19:00;xedin;Committed.,"03/Nov/11 19:19;hudson;Integrated in Cassandra #1185 (See [https://builds.apache.org/job/Cassandra/1185/])
    recognize that ""SELECT first ... *"" isn't really ""SELECT *""
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3445

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1197270
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cql/Cql.g
* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java
* /cassandra/trunk/src/java/org/apache/cassandra/cql/SelectExpression.java
* /cassandra/trunk/src/java/org/apache/cassandra/cql/SelectStatement.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stress.java fails to run if the keyspace already exists,CASSANDRA-2483,12504322,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,nickmbailey,nickmbailey,14/Apr/11 23:00,12/Mar/19 13:59,13/Mar/19 22:26,04/May/11 21:16,0.8.0,,,Legacy/Tools,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,20646,,,Wed May 04 21:16:02 UTC 2011,,,,,,0|i0gbmv:,93315,,,,,,,,,,,04/May/11 21:16;nickmbailey;Someone already fixed this in the 0.8 branch and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
don't perform HH to client-mode nodes,CASSANDRA-2668,12507780,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,19/May/11 16:54,12/Mar/19 13:59,13/Mar/19 22:26,19/May/11 17:11,0.7.7,0.8.1,,,,,0,,,,,,,,,,,,,,,,,,,,,19/May/11 16:55;jbellis;2668.txt;https://issues.apache.org/jira/secure/attachment/12479800/2668.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-19 17:03:09.645,,,no_permission,,,,,,,,,,,,20763,,,Thu May 19 17:39:07 UTC 2011,,,,,,0|i0gcqn:,93494,brandon.williams,brandon.williams,,,,,,,,,19/May/11 16:55;jbellis;one-line fix,19/May/11 17:03;brandon.williams;+1,19/May/11 17:11;jbellis;committed,"19/May/11 17:39;hudson;Integrated in Cassandra-0.7 #490 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/490/])
    don't perform HH to client-mode
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2668

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1125002
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/gms/EndpointState.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/StorageService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null strategy_options on a KsDef leads to an NPE.,CASSANDRA-2713,12508463,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,26/May/11 19:14,12/Mar/19 13:59,13/Mar/19 22:26,26/May/11 19:26,0.8.0,,,,,,0,,,,,"For add/update keyspace, a KsDef with null strategy_options will cause an NPE.",,,,,,,,,,,,,,,,26/May/11 19:16;jhermes;2713-allow.txt;https://issues.apache.org/jira/secure/attachment/12480571/2713-allow.txt,26/May/11 19:16;jhermes;2713-disallow.txt;https://issues.apache.org/jira/secure/attachment/12480572/2713-disallow.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-26 19:26:34.409,,,no_permission,,,,,,,,,,,,20785,,,Thu Jun 02 01:42:19 UTC 2011,,,,,,0|i0gd07:,93537,jbellis,jbellis,,,,,,,,,"26/May/11 19:16;jhermes;Attaching two patches.

`2713-allow` will allow nulls and default them to an empty set on KSMD creation.
`2713-disallow` will explicitly block nulls during ThriftValidation.

Both avoid NPEs and either are technically correct. Looks like we need to allow nulls because of upgrade issues coming from 0.7.","26/May/11 19:26;jbellis;committed the ""allow"" approach","01/Jun/11 20:39;cdaw;I verified this fix by dropping in the c* 0.8.0 jar file into brisk.
Can we get this merged to the Cassandra 0.8 branch since brisk builds its jar file from there?",02/Jun/11 01:42;jbellis;merged to 0.8 branch in r1130370,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move allows you to move to tokens > 2**127,CASSANDRA-3501,12531820,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,nickmbailey,nickmbailey,17/Nov/11 17:55,12/Mar/19 13:59,13/Mar/19 22:26,18/Nov/11 22:56,1.0.4,,,Legacy/Tools,,,1,,,,,Currently you can move to tokens greater than what should be the max token in RP.,,,,,,,,,,,,,,,,18/Nov/11 22:31;jbellis;3501-v2.txt;https://issues.apache.org/jira/secure/attachment/12504280/3501-v2.txt,17/Nov/11 23:31;jbellis;3501.txt;https://issues.apache.org/jira/secure/attachment/12504138/3501.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-17 23:31:52.875,,,no_permission,,,,,,,,,,,,217556,,,Fri Nov 18 22:56:50 UTC 2011,,,,,,0|i0gkon:,94781,nickmbailey,nickmbailey,,,,,,,,,17/Nov/11 23:31;jbellis;Patch to bounds-check RandomPartitioner TokenFactor.,18/Nov/11 22:31;jbellis;v2 introduces RP.MAXIMUM constant,18/Nov/11 22:38;nickmbailey;+1,18/Nov/11 22:56;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cassandra.bat does not use SETLOCAL, can cause classpath issues",CASSANDRA-3506,12531982,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,18/Nov/11 19:06,12/Mar/19 13:59,13/Mar/19 22:26,18/Nov/11 21:08,1.0.4,,,,,,0,,,,,"In bin/cassandra.bat, we don't use SETLOCAL (although we do use ENDLOCAL for some reason), so modifications to the classpath within the batch script persist.  This means that if you run cassandra-1.0.0/bin/cassandra.bat, kill the process, and then run cassandra-1.0.3/bin/cassandra.bat, the 1.0.0 jars will still be in the classpath. ",Windows,,,,,,,,,,,,,,,18/Nov/11 19:12;thobbs;3506.txt;https://issues.apache.org/jira/secure/attachment/12504248/3506.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-18 19:25:28.667,,,no_permission,,,,,,,,,,,,217718,,,Fri Nov 18 21:08:38 UTC 2011,,,,,,0|i0gkqn:,94790,bcoverston,bcoverston,,,,,,,,,18/Nov/11 19:12;thobbs;Attached patch adds the same conditional setlocal to cassandra.bat that cassandra-cli.bat uses.,18/Nov/11 19:25;bcoverston;Reviewed patch +1.,18/Nov/11 21:08;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IncomingStreamReader uses socket.getRemoteSocketAddress() which might be diffrent than FB.getBroadcastAddress(),CASSANDRA-3503,12531835,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,17/Nov/11 19:57,12/Mar/19 13:59,13/Mar/19 22:26,17/Nov/11 22:17,1.1.0,,,,,,0,,,,,"We can add BCA to the streaming so the receiver can use this to StreamInSession.get(bca, sid)

Currently this causes the repairs to hang when the bca is diffrent than LocalAddress.",JVM,,,,,,,,,,,,,,,17/Nov/11 21:26;vijay2win@yahoo.com;0001-fixing-the-bca-lookup-for-streaming-v2.patch;https://issues.apache.org/jira/secure/attachment/12504114/0001-fixing-the-bca-lookup-for-streaming-v2.patch,17/Nov/11 21:00;vijay2win@yahoo.com;0001-fixing-the-bca-lookup-for-streaming.patch;https://issues.apache.org/jira/secure/attachment/12504113/0001-fixing-the-bca-lookup-for-streaming.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-17 22:17:56.753,,,no_permission,,,,,,,,,,,,217571,,,Thu Nov 17 23:27:05 UTC 2011,,,,,,0|i0gkpj:,94785,brandon.williams,brandon.williams,,,,,,,,,"17/Nov/11 21:00;vijay2win@yahoo.com;Attached patch add's InetAddress to the Header (Simple approach).

Alternative approach will be converting the StreamInSession.context into a UUID.",17/Nov/11 21:26;vijay2win@yahoo.com;Update the serializer type based on the feedback from IRS.,"17/Nov/11 22:17;brandon.williams;Committed, thanks!","17/Nov/11 23:27;hudson;Integrated in Cassandra #1212 (See [https://builds.apache.org/job/Cassandra/1212/])
    Streaming uses BroadcastAddress instead of the remote socket.
Patch by Vijay, reviewed by brandonwilliams for CASSANDRA-3503

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1203394
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamHeader.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL can't create column with compression or that use leveled compaction,CASSANDRA-3374,12527593,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,xedin,slebresne,slebresne,18/Oct/11 13:48,12/Mar/19 13:59,13/Mar/19 22:26,30/Jan/12 16:22,1.0.8,,,Legacy/CQL,,,0,cql,,,,"Looking at CreateColumnFamilyStatement.java, it doesn't seem CQL can create compressed column families, nor define a compaction strategy.",,,,,,,,,,,,,,,,27/Jan/12 21:06;xedin;CASSANDRA-3374-compaction_strategy_class.patch;https://issues.apache.org/jira/secure/attachment/12512230/CASSANDRA-3374-compaction_strategy_class.patch,22/Dec/11 21:51;xedin;CASSANDRA-3374.patch;https://issues.apache.org/jira/secure/attachment/12508452/CASSANDRA-3374.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-20 20:51:35.75,,,no_permission,,,,,,,,,,,,88837,,,Mon Jan 30 16:22:04 UTC 2012,,,,,,0|i0gj3z:,94526,thepaul,thepaul,,,,,,,,,20/Oct/11 20:51;tjake;There is also no way to specify a index_type,"03/Nov/11 13:27;slebresne;I think we want to support the strategy options and compaction options generically. This would make maintenance easier as we won't have to update this each time we add a new option, but more importantly if it's not generic, it means one cannot use a custom compressor or compaction strategy having options. I understand this complicate matters because the parser probably needs to be updated accordingly, but I believe we'd better bite the bullet now.

It would also be nice to add index_type support and to update the CQL doc.",03/Nov/11 13:31;xedin;If Jonathan says that we can support compound options like 'compression:<something>' in CREATE TABLE I'm not against it.,03/Nov/11 17:18;jbellis;I thought we'd use the syntax we use for replication strategy options.,"03/Nov/11 17:22;xedin;ok, I will modify patch to support that then.","28/Nov/11 16:24;xedin;CQLsh fails with ""Unmatched named substitution: 'sstable_compressor' not given for CREATE COLUMNFAMILY cf ( KEY text PRIMARY KEY ) WITH compression_parameters:sstable_compressor = 'SnappyCompressor' and compression_parameters:chunk_size_in_kb = 16 and comparator = ascii;""... Tested with raw thrift call - works as excepted. 

Jonathan do you have any idea how to fix that cqlsh problem?",02/Dec/11 05:38;jbellis;No idea. Assigning to Paul to have a look at that part.,"12/Dec/11 18:42;thepaul;This is due to a bug in the python driver: http://code.google.com/a/apache-extras.org/p/cassandra-dbapi2/issues/detail?id=11

Also, changes like this should probably include the relevant changes to the CQL doc at doc/cql/CQL.textile.

I'll look at making those changes if I have a few moments.","22/Dec/11 21:18;jbellis;The python driver bug is fixed, are we good to go for this patch?",22/Dec/11 21:39;xedin;rebased patch. everything works as expected with cql fixed!,22/Dec/11 21:51;xedin;patch adds update to CQL.doc,23/Dec/11 23:20;thepaul;+1,24/Dec/11 18:37;xedin;Committed.,"27/Jan/12 20:40;thepaul;Was this meant to expose 'compaction_strategy_class' to CQL, as well as compaction_strategy_options? It's added here as a valid option, but it doesn't actually do anything, and it appears there is no other possible way to set CFMetaData.compactionStrategyClass from CQL.","27/Jan/12 20:47;jbellis;bq. Was this meant to expose 'compaction_strategy_class' to CQL

Yes",27/Jan/12 21:06;xedin;it had that option but never actually bothered to set it :),30/Jan/12 16:06;thepaul;+1,30/Jan/12 16:22;xedin;Committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in new GCInspector log,CASSANDRA-3076,12520117,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,25/Aug/11 14:26,12/Mar/19 13:59,13/Mar/19 22:26,31/Aug/11 20:05,0.7.9,0.8.5,,,,,0,,,,,Small regression from CASSANDRA-2868,Lion OSX,,,,,,,,,,,,,,,25/Aug/11 14:29;tjake;3076.txt;https://issues.apache.org/jira/secure/attachment/12491637/3076.txt,31/Aug/11 19:51;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3076-avoid-div-by-zero.txt;https://issues.apache.org/jira/secure/attachment/12492514/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3076-avoid-div-by-zero.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-25 14:38:27.604,,,no_permission,,,,,,,,,,,,20959,,,Wed Aug 31 20:05:02 UTC 2011,,,,,,0|i0gfdr:,93922,jbellis,jbellis,,,,,,,,,"25/Aug/11 14:27;tjake;{code}
    [junit] ERROR 10:20:16,755 Fatal exception in thread Thread[ScheduledTasks:1,5,main]
    [junit] java.lang.AssertionError
    [junit] 	at org.apache.cassandra.service.GCInspector.logGCResults(GCInspector.java:110)
    [junit] 	at org.apache.cassandra.service.GCInspector.access$000(GCInspector.java:41)
    [junit] 	at org.apache.cassandra.service.GCInspector$1.run(GCInspector.java:85)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
    [junit] 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:680)
    [junit] ------------- ---------------- ---------------
{code}","25/Aug/11 14:38;jbellis;The assert is basically saying ""if total gc time has increased, count should have increased as well.""

If that's valid, then the ""if (previousTotal.equals(total)) continue"" check should handle this.  If it's not, we should probably remove the assert entirely.
","25/Aug/11 14:50;tjake;Right, I think it's likely a OSX lion thing.  Removing the assert works for me.","25/Aug/11 15:38;jbellis;ok, done in 0.7.  (Brandon already did that in 0.8 in r1161167.)","26/Aug/11 13:09;tjake;New error due to the count being zero. The original patch fixes this.


{code}
  [junit] ERROR 09:06:26,417 Fatal exception in thread Thread[ScheduledTasks:1,5,main]
    [junit] java.lang.ArithmeticException: / by zero
    [junit] 	at org.apache.cassandra.service.GCInspector.logGCResults(GCInspector.java:117)
    [junit] 	at org.apache.cassandra.service.GCInspector.access$000(GCInspector.java:41)
    [junit] 	at org.apache.cassandra.service.GCInspector$1.run(GCInspector.java:85)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
    [junit] 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:680)
{code}",26/Aug/11 16:35;jbellis;+1 if you fix the spacing then,30/Aug/11 14:35;tjake;committed,"30/Aug/11 14:57;hudson;Integrated in Cassandra-0.7 #546 (See [https://builds.apache.org/job/Cassandra-0.7/546/])
    Fix div by zero error
patch by tjake; reviewed by jbellis for CASSANDRA-3076

jake : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163234
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/GCInspector.java
","31/Aug/11 19:28;jbellis;Still seeing this post-patch in trunk:

{noformat}
    [junit] ERROR 14:23:17,606 Fatal exception in thread Thread[ScheduledTasks:1,5,main]
    [junit] java.lang.ArithmeticException: / by zero
    [junit] 	at org.apache.cassandra.service.GCInspector.logGCResults(GCInspector.java:119)
    [junit] 	at org.apache.cassandra.service.GCInspector.access$000(GCInspector.java:41)
    [junit] 	at org.apache.cassandra.service.GCInspector$1.run(GCInspector.java:85)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
    [junit] 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:680)
{noformat}",31/Aug/11 19:54;jbellis;+1 new patch,31/Aug/11 20:05;tjake;committed ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Word count example won't compile,CASSANDRA-2510,12504713,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,19/Apr/11 21:02,12/Mar/19 13:58,13/Mar/19 22:26,19/Apr/11 21:19,0.8 beta 1,,,,,,0,hadoop,,,,"On the 0.8 branch, the word count stuff isn't compiling.",,,,,,,,,,,,,,,,19/Apr/11 21:04;jeromatron;2510.txt;https://issues.apache.org/jira/secure/attachment/12476781/2510.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-19 21:19:30.286,,,no_permission,,,,,,,,,,,,20664,,,Wed Apr 20 02:20:33 UTC 2011,,,,,,0|i0gbsf:,93340,jbellis,jbellis,,,,,,,,,19/Apr/11 21:04;jeromatron;Updating the way it creates the keyspace and the data so it compiles and works properly.,19/Apr/11 21:19;jbellis;committed,"20/Apr/11 02:20;hudson;Integrated in Cassandra-0.8 #26 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/26/])
    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row cache provider reported wrong in cassandra-cli,CASSANDRA-3405,12528834,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,krummas,krummas,krummas,26/Oct/11 08:15,12/Mar/19 13:58,13/Mar/19 22:26,30/Oct/11 18:56,0.8.8,,,,,,0,,,,,"When doing ""show schema;"" in the CLI, the row_cache_provider is reported as ConcurrentLinkedHashCacheProvider while it really is SerializingCacheProvider

Same goes for ""describe keyspace"" (after CASSANDRA-3384) on the 0.8 branch",,,,,,,,,,,,,,,,26/Oct/11 08:18;krummas;3405.patch;https://issues.apache.org/jira/secure/attachment/12500838/3405.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-30 18:56:57.742,,,no_permission,,,,,,,,,,,,214694,,,Mon Oct 31 22:09:30 UTC 2011,,,,,,0|i0gjhj:,94587,xedin,xedin,,,,,,,,,26/Oct/11 08:18;krummas;Trivial patch that adds the class name to the cfm thrift message,"30/Oct/11 18:56;xedin;Committed, thanks!","30/Oct/11 21:06;hudson;Integrated in Cassandra-0.8 #390 (See [https://builds.apache.org/job/Cassandra-0.8/390/])
    CFMetaData.convertToThrift method to set RowCacheProvider
patch by Marcus Eriksson; reviewed by Pavel Yaskevich for CASSANDRA-3405

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1195218
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
",31/Oct/11 22:09;jbellis;(Also looks to not be a problem in the 1.0 branch.),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache saving broken on windows,CASSANDRA-3566,12533701,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,slebresne,jbellis,jbellis,02/Dec/11 23:26,12/Mar/19 13:58,13/Mar/19 22:26,06/Dec/11 20:52,1.1.0,,,,,,0,windows,,,,CASSANDRA-1740 broke cache saving on Windows.,,,,,,,,,,,,,,,,06/Dec/11 16:22;slebresne;3566.patch;https://issues.apache.org/jira/secure/attachment/12506274/3566.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-06 16:22:18.204,,,no_permission,,,,,,,,,,,,219428,,,Tue Dec 06 20:52:16 UTC 2011,,,,,,0|i0glhj:,94911,jbellis,jbellis,,,,,,,,,"02/Dec/11 23:26;jbellis;All the cache tests fail with an exception like this:

{noformat}
    [junit] Testcase: testKeyCacheLoad(org.apache.cassandra.db.KeyCacheTest):   Caused an ERROR
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to rename build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache7847563715233372577.tmp to build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache
    [junit] java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.IOException: Unable to rename build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache7847563715233372577.tmp to build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache
    [junit]     at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    [junit]     at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    [junit]     at org.apache.cassandra.db.KeyCacheTest.testKeyCacheLoad(KeyCacheTest.java:87)
    [junit] Caused by: java.lang.RuntimeException: java.io.IOException: Unable to rename build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache7847563715233372577.tmp to build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:37)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to rename build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache7847563715233372577.tmp to build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache
    [junit]     at org.apache.cassandra.cache.AutoSavingCache$Writer.saveCache(AutoSavingCache.java:265)
    [junit]     at org.apache.cassandra.db.compaction.CompactionManager$10.runMayThrow(CompactionManager.java:907)
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.db.KeyCacheTest FAILED
{noformat}
",02/Dec/11 23:27;jbellis;I'd be totally fine with just ripping the cancellation code out of cache saving.  Cache saving will be very fast compared to most actual compactions.,06/Dec/11 16:22;slebresne;I'm totally with that too. Patch attach to rip that part.,06/Dec/11 20:52;jbellis;+1 (went ahead and committed since this has been bugging me),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.cassandra.io.util.FileUtils.delete(List<String>) only deletes every second file in the list,CASSANDRA-4025,12545704,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,,mschuetz,mschuetz,08/Mar/12 19:48,12/Mar/19 13:58,13/Mar/19 22:26,08/Mar/12 20:16,,,,,,,0,,,,,The above mentioned function does not work as expected. I fixed it by iterating over the list in reverse.,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-03-08 20:16:40.273,,,no_permission,,,,,,,,,,,,230887,,,Thu Mar 08 20:16:40 UTC 2012,,,,,,0|i0gr33:,95818,,,,,,,,,,,"08/Mar/12 19:49;mschuetz;{code}
diff --git a/src/java/org/apache/cassandra/io/util/FileUtils.java b/src/java/org/apache/cassandra/io/util/FileUtils.java
index e264b8a..3d1b029 100644
--- a/src/java/org/apache/cassandra/io/util/FileUtils.java
+++ b/src/java/org/apache/cassandra/io/util/FileUtils.java
@@ -151,7 +151,7 @@ public static boolean delete(String file)
     public static boolean delete(List<String> files)
     {
         boolean bVal = true;
-        for ( int i = 0; i < files.size(); ++i )
+        for ( int i = files.size() - 1; i >= 0; --i )
         {
             String file = files.get(i);
             bVal = delete(file);
{code}",08/Mar/12 20:16;slebresne;Actually that function wasn't used anywhere so instead I just removed it. Thanks for the report.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh doesn't error out immediately for use of invalid keyspace,CASSANDRA-3764,12539273,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,21/Jan/12 05:54,12/Mar/19 13:58,13/Mar/19 22:26,26/Jan/12 01:30,1.0.8,,,Legacy/Tools,,,0,,,,,"{noformat}
cqlsh> use wordcoun;
cqlsh:wordcoun> select * from input_words;
Bad Request: Keyspace wordcoun does not exist
{noformat}",,,,,,,,,,,,,,,,21/Jan/12 06:02;jbellis;3764.txt;https://issues.apache.org/jira/secure/attachment/12511364/3764.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-23 20:12:28.774,,,no_permission,,,,,,,,,,,,224775,,,Thu Jan 26 01:30:33 UTC 2012,,,,,,0|i0gnxz:,95309,thepaul,thepaul,,,,,,,,,"21/Jan/12 05:57;jbellis;Looks like this is a server bug, not cqlsh's fault:

{code}
.   public void setKeyspace(String ks)
    {
        keyspace = ks;
    }
{code}
",21/Jan/12 06:02;jbellis;quick fix attached.,23/Jan/12 20:12;thepaul;I always thought this was expected behavior. Is there any chance this might break some existing code? I guess probably not.,24/Jan/12 18:51;thepaul;+1,26/Jan/12 01:30;jbellis;committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
correct dropped messages logging,CASSANDRA-3377,12527620,Bug,Resolved,CASSANDRA,Cassandra,software,zznate,The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale,http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,18/Oct/11 17:02,12/Mar/19 13:58,13/Mar/19 22:26,19/Oct/11 17:39,0.8.8,1.0.1,,,,,0,logging,,,,"CASSANDRA-3004 switched MessagingService back to logging only ""recent"" dropped messages instead of server lifetime totals, but the log message was not updated.",,,,,,,,,,,,,,,,18/Oct/11 17:05;jbellis;3377.txt;https://issues.apache.org/jira/secure/attachment/12499562/3377.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-18 18:31:19.251,,,no_permission,,,,,,,,,,,,88871,,,Wed Oct 19 14:02:02 UTC 2011,,,,,,0|i0gj5b:,94532,brandon.williams,brandon.williams,,,,,,,,,18/Oct/11 18:31;brandon.williams;+1,"19/Oct/11 14:02;hudson;Integrated in Cassandra-0.8 #381 (See [https://builds.apache.org/job/Cassandra-0.8/381/])
    correct dropped messages logging
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3377

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1186204
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
