Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Description,Environment,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Cloners),Outward issue link (Reference),Outward issue link (Regression),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (Hadoop Flags),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Patch Info),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Severity),Custom field (Severity),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Test and Documentation Plan),Custom field (Testcase included),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Pig 0.15 stopped working with Hadoop 1.x,PIG-4592,12836028,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,daijy,costin,costin,07/Jun/15 20:00,17/Jun/15 17:15,13/Mar/19 23:13,16/Jun/15 22:22,0.15.0,,,,,,,,,,,,,,,,0.15.0,,build,,,0,,,,,,,,"It looks like the Maven jar for Pig 0.15 is built against Hadoop 1.x instead of Hadoop 2.x.
Running Pig 0.15 against a Hadoop 1 environment (which works fine in 0.14) triggers a storm of class not found exceptions for classes in Hadoop 2 (like org/apache/hadoop/mapreduce/task/JobContextImpl).

The culprit seems to be the HadoopShim included in 0.15 which is different from 0.14 (and previous) - it's part of hadoop23 instead of hadoop20 package (whatever that means).

As it stands right now, using Pig 0.15 from Maven (common case) with Hadoop 1.x is impossible.",Hadoop 1.x and pig.jar retrieved from Maven,,,,,,,,,,,,,,,,16/Jun/15 21:16;daijy;PIG-4592-1.patch;https://issues.apache.org/jira/secure/attachment/12739974/PIG-4592-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-06-15 23:11:34.893,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Wed Jun 17 17:15:38 UTC 2015,,,,,,,0|i2fqjj:,9223372036854775807,,,,,,,,,,"15/Jun/15 23:11;rohini;[~daijy],
   Did we accidentally publish the hadoop 1.x version into maven for the one without -h2 classifier as well?",16/Jun/15 21:16;daijy;I see the problem. It is a by-produce of PIG-4499. Attach a patch.,"16/Jun/15 21:30;rohini;Shouldn't 
{code}tofile=""${output.jarfile.core}""{code} be {code}tofile=""${output.jarfile.core-h1}"" {code}?","16/Jun/15 21:35;daijy;Target ""signanddeploy"" takes output.jarfile.core instead of output.jarfile.core-h1. That's actually the pitfall Pig publish the h2 jar in place of h1.",16/Jun/15 22:03;rohini;+1,"16/Jun/15 22:22;daijy;Patch committed to both trunk and 0.15 branch.

Maven artifacts is republished. [~costin], can you try it?","17/Jun/15 17:15;costin;The latest artifact works for me. 
However, republishing a stable artifact is not a good practice since the previous binaries have been already distributed and Maven & co won't download the new ones. As far as Maven is concerned, once something is released, it is frozen (I wasn't even aware maven central allows republication).
In my case I had to nuke Pig from my local repository - I expect others to face the same issue.
Doing a 0.15.1 release, even if it's binary the same, clarifies the situation and avoid any confusion about when the jar was downloaded in my opinion,",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
please fix the change that created a dependency on org.apache.pig.impl.PigImplConstants,PIG-3330,12648232,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,billgraham,jadler,jadler,17/May/13 19:13,17/May/13 21:26,13/Mar/19 23:13,17/May/13 20:20,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"I can't build Pig from trunk because several source files (including org.apache.pig.Main.java) require org.apache.pig.impl.PigImplConstants, but that class isn't available.

I'm assuming someone left out a file on a recent commit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-05-17 20:20:42.984,,,no_permission,,,,,,,,,,,,,328588,,,,Fri May 17 21:26:40 UTC 2013,,,,,,,0|i1koyn:,328932,,,,,,,,,,"17/May/13 20:20;billgraham;My bad, I made the commit last night and forgot 'svn add'. Just made the fix by adding the missing file.",17/May/13 21:26;jadler;Thanks for checking in the file; looks like that was the only problem.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig does not work with ViewFileSystem,PIG-2791,12597844,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Blocker,Fixed,rohini,patwhitey2007,patwhitey2007,06/Jul/12 22:29,02/May/13 02:29,13/Mar/19 23:13,27/Aug/12 20:49,0.10.0,,,,,,,,,,,,,,,,,,grunt,,,0,,,,,,,,"The Yahoo Pig QE team ran into a blocking issue when trying to test Client-Side Mount Tables, on a Federated cluster with two NNs, this blocks Pig Testing on Federation. 

Federation relies strongly on the use of CSMT with viewFS, QE found that in this configuration it is not possible to enter grunt shell because Pig makes a call to getDefaultReplication() on the fs, which is ambiguous over viewFS and causes core to throw a org.apache.hadoop.fs.viewfs.NotInMountpointException: ""getDefaultReplication on empty path is invalid"".

This in turn cause Pig to exit with an internal error as follows:

2012-07-06 22:20:25,657 [main] INFO  org.apache.pig.Main - Apache Pig version 0.10.1.0.1206081058 (r1348169) compiled Jun 08 2012, 17:58:42
2012-07-06 22:20:26,074 [main] WARN  org.apache.hadoop.conf.Configuration - mapred.used.genericoptionsparser is deprecated. Instead, use mapreduce.client.genericoptionsparser.used
2012-07-06 22:20:26,076 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: viewfs:///
2012-07-06 22:20:26,080 [main] WARN  org.apache.hadoop.conf.Configuration - fs.default.name is deprecated. Instead, use fs.defaultFS
2012-07-06 22:20:26,522 [main] ERROR org.apache.pig.Main - ERROR 2999: Unexpected internal error. getDefaultReplication on empty path is invalid
2012-07-06 22:20:26,522 [main] WARN  org.apache.pig.Main - There is no log file to write to.
2012-07-06 22:20:26,522 [main] ERROR org.apache.pig.Main - org.apache.hadoop.fs.viewfs.NotInMountpointException: getDefaultReplication on empty path is invalid
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.getDefaultReplication(ViewFileSystem.java:482)
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.init(HDataStorage.java:77)
        at org.apache.pig.backend.hadoop.datastorage.HDataStorage.<init>(HDataStorage.java:58)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:205)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.init(HExecutionEngine.java:118)
        at org.apache.pig.impl.PigContext.connect(PigContext.java:208)
        at org.apache.pig.PigServer.<init>(PigServer.java:246)
        at org.apache.pig.PigServer.<init>(PigServer.java:231)
        at org.apache.pig.tools.grunt.Grunt.<init>(Grunt.java:47)
        at org.apache.pig.Main.run(Main.java:487)
        at org.apache.pig.Main.main(Main.java:111)


",Pig QE,,,,,,,,,,,,,PIG-2930,,HADOOP-8430,13/Aug/12 20:29;rohini;FixMiniCluster-branch10-1.patch;https://issues.apache.org/jira/secure/attachment/12540745/FixMiniCluster-branch10-1.patch,29/Jul/12 03:11;rohini;FixMiniCluster-branch10.patch;https://issues.apache.org/jira/secure/attachment/12538266/FixMiniCluster-branch10.patch,08/Jul/12 19:12;daijy;PIG-2791-0.patch;https://issues.apache.org/jira/secure/attachment/12535590/PIG-2791-0.patch,12/Jul/12 03:20;rohini;PIG-2791-1.patch;https://issues.apache.org/jira/secure/attachment/12536165/PIG-2791-1.patch,12/Jul/12 03:26;rohini;PIG-2791-2.patch;https://issues.apache.org/jira/secure/attachment/12536166/PIG-2791-2.patch,22/Jul/12 20:57;rohini;PIG-2791-3-branch10.patch;https://issues.apache.org/jira/secure/attachment/12537524/PIG-2791-3-branch10.patch,22/Jul/12 20:57;rohini;PIG-2791-3-trunk.patch;https://issues.apache.org/jira/secure/attachment/12537525/PIG-2791-3-trunk.patch,27/Jul/12 18:42;rohini;PIG-2791-4-branch10.patch;https://issues.apache.org/jira/secure/attachment/12538193/PIG-2791-4-branch10.patch,27/Jul/12 18:42;rohini;PIG-2791-4-trunk.patch;https://issues.apache.org/jira/secure/attachment/12538194/PIG-2791-4-trunk.patch,29/Jul/12 03:02;rohini;PIG-2791-5-trunk.patch;https://issues.apache.org/jira/secure/attachment/12538265/PIG-2791-5-trunk.patch,13/Jul/12 00:50;araceli;asf_test_notes.txt;https://issues.apache.org/jira/secure/attachment/12536316/asf_test_notes.txt,11.0,,,,,,,,,,,,,,,,,,,2012-07-08 02:38:38.226,,,no_permission,,,,,,,,,,,,,256418,,,,Mon Sep 24 18:07:30 UTC 2012,,,,,,,0|i0h5dr:,98134,,,,,,,,,,"08/Jul/12 02:38;daijy;Seems we can drop ""getDefaultReplication"". It does not get used anywhere.","11/Jul/12 00:25;araceli;Hi Daniel
I tried a couple of tests with the patch you provided. I'm getting a different error now.

Assuming the client side mount table has the following:

  <property><name>fs.viewfs.impl</name>
     <value>org.apache.hadoop.fs.viewfs.ViewFileSystem</value>
     <description>The File System for viewfs:uris</description>
  </property>

 <property><name>fs.default.name</name>
    <value>viewfs:///</value>
    <final>true</final>
  </property>

  <property>
    <name>fs.viewfs.mounttable.default.link./data1</name>
    <value>hdfs://mycluster.yahoo.com:8020/user/me/pig/tests/data</value>
  </property>

I confirm the file is visible ( I also tried fs -cat and it was successful)
-bash-3.1$ hadoop fs -ls /data1/singlefile/studenttab10k
-rw-r--r--   3 hadoopqa hdfs     219190 2012-07-10 23:02 /data1/singlefile/studenttab10k


Next I try to a simple load and dump or store as follows:

a = load '/data1/singlefile/studenttab10k' as (name, age, gpa);
dump a; 


This results in a stack trace:

RROR 1066: Unable to open iterator for alias a. Backend error : Trying to get information for an absent application application_1341957183614_0010

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias a. Backend error : Trying to get information for an absent application application_1341957183614_0010
        at org.apache.pig.PigServer.openIterator(PigServer.java:852)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:682)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:303)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:189)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:69)
        at org.apache.pig.Main.run(Main.java:490)
        at org.apache.pig.Main.main(Main.java:111)
Caused by: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Trying to get information for an absent application application_1341957183614_0010
        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:156)
        at $Proxy9.getApplicationReport(Unknown Source)
        at org.apache.hadoop.yarn.api.impl.pb.client.ClientRMProtocolPBClientImpl.getApplicationReport(ClientRMProtocolPBClientImpl.java:116)
        at org.apache.hadoop.mapred.ResourceMgrDelegate.getApplicationReport(ResourceMgrDelegate.java:338)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getProxy(ClientServiceDelegate.java:143)
        at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:298)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:383)
        at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:481)
        at org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:184)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:627)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:625)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
        at org.apache.hadoop.mapred.JobClient.getJobUsingCluster(JobClient.java:625)
        at org.apache.hadoop.mapred.JobClient.getTaskReports(JobClient.java:679)
        at org.apache.hadoop.mapred.JobClient.getMapTaskReports(JobClient.java:673)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getStats(Launcher.java:148)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:383)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1275)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1260)
        at org.apache.pig.PigServer.storeEx(PigServer.java:957)
        at org.apache.pig.PigServer.store(PigServer.java:924)
        at org.apache.pig.PigServer.openIterator(PigServer.java:837)
        ... 7 more
================================================================================
Pig Stack Trace
---------------
ERROR 2997: Encountered IOException. File or directory -l does not exist.

java.io.IOException: File or directory -l does not exist.
        at org.apache.pig.tools.grunt.GruntParser.processLS(GruntParser.java:766)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:366)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:189)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:69)
        at org.apache.pig.Main.run(Main.java:490)
        at org.apache.pig.Main.main(Main.java:111)


Additionally the following also fails:

cd /data1/singlefile
a = load 'studenttab10k' as (name, age, gpa);
dump a; 

",11/Jul/12 05:23;daijy;Can you see AM log on the webUI?,"11/Jul/12 18:54;rohini;Daniel,
  This is the actual cause. 

Caused by: org.apache.hadoop.fs.viewfs.NotInMountpointException: getDefaultBlockSize on empty path is invalid
	at org.apache.hadoop.fs.viewfs.ViewFileSystem.getDefaultBlockSize(ViewFileSystem.java:477)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:276)
","11/Jul/12 21:06;daryn;The problem is there is no single replication factor or block size with viewfs.  It's dependent on each mounted fs.  The {{getDefaultBlockSize()}} and {{getDefaultReplication()}} were deprecated in favor of methods that accept a {{Path}}.  This allows a fs like viewfs to resolve the mount point for a path and return the correct values.

The easy solution is don't call the methods at all and let the smaller signature create, etc methods implicitly get the replication and block size.  I understand that's not an option for most of pig's use cases.

Hbase encountered the same problem and fixed it with a little reflection magic on HBASE-6067.","12/Jul/12 03:20;rohini;Added getDefaultBlockSize(Path) to the HadoopShims.

Had to use 2.0.0-alpha (http://central.maven.org/maven2/org/apache/hadoop/hadoop-common/) as 0.23.1 from maven did not have fs.getDefaultBlockSize(Path) api. 

",12/Jul/12 07:18;daijy;2.0.0-alpha seems to be a big change. We need to carefully test it to make sure unit tests pass.,"12/Jul/12 13:23;daryn;You might consider replacing {{fs = new Path(""/"").getFileSystem(conf)}} with {{fs = FileSystem.get(conf)}} to get the default filesystem.  I'd suggest adding {{path = fs.getWorkingDirectory()}} after that line so the {{isFsPath}} boolean can be removed.

Do you need to add anything to support the new api on trunk?","12/Jul/12 15:55;rohini;@Daryn,
    The path variable could refer to other schemes like hbase://. Even though the variable is not used elsewhere I did not want to reassign a different value and change the meaning of it. Also there is a fs.setWorkingDirectory() in the code after that.

Source code: http://svn.apache.org/viewvc/pig/branches/branch-0.10/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigInputFormat.java?view=markup [Line 232]

@Daniel,
    It should not be a problem as long as the APIs are compatible. Only if there was a change of class from interface to abstract or vice versa between 0.23.1 and 2.0.0-alpha then there might be runtime issues even if compilation was successful. But just to be sure, I will kick off the e2e tests for 23 with the patch. I will also check with the core team folks who were doing 0.23 to 2.0.0 tests. ","12/Jul/12 16:19;cheolsoo;Hi, I did some work to get unit test passing against 2.0.0-snapthot a while ago and recorded what I found at PIG-2700.

Please feel free to use the patch if that's still applicable.

Thanks!
","13/Jul/12 00:50;araceli;Short answers: the patch looks good.
Comments:
Here is a list of the manual tests I attempted. I tried running without a client side mount table and with a client side table. With hdfs as the default and viewfs as the default. With two name nodes. Reading and writing accross namenodes. It looks pretty good. There were some errors, but I believe they are expected due to invalid syntax. 
","13/Jul/12 05:24;rohini;@Cheolsoo
   Thanks for the pointer. I had only run the test-commit unit tests. I will include your patch also and do a full unit test suite run to see if there are any other issues. Thanks again. ","22/Jul/12 21:03;rohini; In addition to PIG-2791-2.patch, this patch ontains fix for unit tests for hadoop 23. Even after including Cheolsoo's patch from PIG-2700, there were lot of unit test failures.

1) MiniYarnCluster node manager would not start with guice errors due to to jersey-guice-1.8 and guice-2.0 incompatibility. Took 2 days to figure this one out. Had to change dependency to guice-3.0 and add few more dependencies. 
2) FSShell copy command behaviour has changed. Had to do mkdir before copy commands as copy commands do not create the destination directory structure if it does not exist. Also had to add creation of fs working directory (user home directory in dfs) while creating MiniCluster.

  Ran full suite of unit and e2e tests for hadoop 23 with branch 10. 
",22/Jul/12 21:06;rohini;   Had also tried latest 0.23.3-SNAPSHOT instead of 2.0.0-alpha. Had same unit test failures. So decided to go with 2.0.0-alpha itself as dependency.,"23/Jul/12 14:12;daryn;Question: why does the shim use the {{Path}} based variant for 23 by not 2.0?  23, 2.0, and trunk all require use of the new api.  The new api is also present in later versions of 1.x but isn't strictly required since {{ViewFileSystem}} isn't in 1.x.","23/Jul/12 21:16;rohini;Daryn,
   The HadoopShims in hadoop20 is for 20.x and 1.x versions of hadoop and does not use Path based variant of the API. The HadoopShims in hadoop23 is for 23 and 2.0 versions of hadoop and uses the Path based variant of the API. I am assuming you have mistaken 20 as 2.0. ",24/Jul/12 17:16;daryn;+1 Rohini and I spoke and she cleared up my confusion.  Looks good!,"27/Jul/12 18:42;rohini;Just made a minor change to the patch so that if the ivy dependency was changed to 0.23.3 also all the tests pass. Changed Util.Hadoop2_0() to (Util.isHadoop23 || Util.isHadoop2_0()) for fs copy command.

 ",27/Jul/12 18:48;daijy;Commit PIG-2791-4-branch10.patch to 0.10 branch. Will check into trunk once trunk tests pass.,"29/Jul/12 03:02;rohini;Daniel,
   Realized what caused your test failure. Had removed conf_file.delete() in MiniDFSCluster.java as it was causing tests to randomly fail without hadoop-site.xml if two builds were running simultaneously in hudson (patch builds and actual builds). If you switch between versions (23 and then 20), then the hadoop-site.xml created in /<homedir>/pigtest/conf is not correct and causes failure during MiniDfsCluster setup. Saw that in 0.10 it is created in build/classes instead of home dir and that fixes the problem better. Updated the trunk patch putting the conf_file.delete() back. 

Should I create a separate jira for putting conf_file.delete back in pig 0.10 with build/classes or post the patch here itself as this jira is not closed yet. Don't want developers to waste time debugging this as a issue.",29/Jul/12 03:11;rohini;Uploaded the change for branch-0.10 to put back conf_file.delete and change homedir/pigtest/conf to build/classes,"13/Aug/12 20:29;rohini;Updated FixMiniCluster-branch10-1.patch as it did not apply cleanly (https://reviews.apache.org/r/6082/). 

Removed the change to build/classes as porting PIG-2326 to branch-0.10 will include that change and should handle the multiple builds failing in same build machine problem.",14/Aug/12 13:36;daryn;Updated summary since the issue is unrelated to federation.,27/Aug/12 20:49;daijy;Commit FixMiniCluster-branch10-1.patch to 0.10.,"31/Aug/12 18:48;rohini;Daniel,
   You have not committed the patch to trunk yet.",24/Sep/12 18:07;daijy;Committed to trunk.,,,,,,,,,,,,,,,
PigStorage with -tagFile/-tagPath produces incorrect results with column pruning,PIG-5341,13163979,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,knoguchi,knoguchi,knoguchi,04/Jun/18 20:42,05/Jun/18 14:35,13/Mar/19 23:13,05/Jun/18 14:35,,,,,,,,,,,,,,,,,0.17.1,,,,,0,,,,,,,,"I don't know why we didn't see this till now.

{code}
A = load 'test.txt' using PigStorage('\t', '-tagFile') as (filename:chararray, a0:int, a1:int, a2:int, a3:int);
B = FOREACH A GENERATE a0,a2;
dump B;
{code}
Input 
{noformat}
knoguchi@pig > cat  test.txt
0       1       2       3
0       1       2       3
0       1       2       3
{noformat}
Expected Results
{noformat}
(0,2)
(0,2)
(0,2)
{noformat}
Actual Results
{noformat}
(,1)
(,1)
(,1)
{noformat}
This is really bad...",,,,,,,,,,,,,,,,,04/Jun/18 20:47;knoguchi;pig-5341-v01.patch;https://issues.apache.org/jira/secure/attachment/12926447/pig-5341-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-06-05 08:50:20.457,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Tue Jun 05 14:35:12 UTC 2018,,,,,,,0|i3uhyf:,9223372036854775807,,,,,,,,,,"04/Jun/18 20:50;knoguchi;Attached {{pig-5341-v01.patch}} that fixes the off-by-one bug in determining which fields to keep. 
Also, tagfile/tagpath was always being added to tuple even when columnpruning was setting {{mRequiredColumns[0]}} to false.

Separate from this, found {{testColumnPrune}} wasn't really comparing the output.  Fixed that as well.","05/Jun/18 08:50;szita;+1, thanks for fixing this Koji!","05/Jun/18 14:35;knoguchi;Thanks for the review Adam!!! 
Committed to 0.17 branch and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in PigJobControl.run() when job status is null,PIG-4967,12995673,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,water,water,water,08/Aug/16 15:19,18/Nov/16 08:43,13/Mar/19 23:13,01/Sep/16 06:49,0.15.0,,,,,,,,,,,,,,,,0.16.1,,,,,0,,,,,,,,"{code}
[JobControl] ERROR org.apache.pig.backend.hadoop23.PigJobControl  - Error while trying to run jobs.
java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.Job.getJobName(Job.java:426)
	at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.toString(ControlledJob.java:93)
	at java.lang.String.valueOf(String.java:2982)
	at java.lang.StringBuilder.append(StringBuilder.java:131)
	at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:182)
	at java.lang.Thread.run(Thread.java:745)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:276)
{code}",,,,,,,,,,,,,MAPREDUCE-6762,,,,19/Aug/16 08:06;water;PIG-4967-0.patch;https://issues.apache.org/jira/secure/attachment/12824514/PIG-4967-0.patch,23/Aug/16 09:50;water;PIG-4967-1.patch;https://issues.apache.org/jira/secure/attachment/12825019/PIG-4967-1.patch,26/Aug/16 07:45;water;PIG-4967-2.patch;https://issues.apache.org/jira/secure/attachment/12825628/PIG-4967-2.patch,30/Aug/16 09:18;water;PIG-4967-3.patch;https://issues.apache.org/jira/secure/attachment/12826154/PIG-4967-3.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2016-08-08 19:56:00.168,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Thu Sep 08 14:38:21 UTC 2016,,,,,,,0|i321i7:,9223372036854775807,,,,,,,,,,"08/Aug/16 15:38;water;Line 181 and 182 of PigJobControl are
{code}
ControlledJob j = it.next();
log.debug(""Checking state of job ""+j);
{code}

Then it gets into Hadoop's code
Line 93 of ControlledJob 
{code}
sb.append(""job name:\t"").append(this.job.getJobName()).append(""\n"");
{code}

Then in Job
{code}
public String getJobName() {
  if (state == JobState.DEFINE) {
    return super.getJobName();
  }
  ensureState(JobState.RUNNING);
  return status.getJobName();   <-- status is null somehow
}
{code}",08/Aug/16 15:38;water;We are using Pig 0.15.0 and Hadoop 2.7.1,"08/Aug/16 15:42;water;Hadoop should fix this, but I think we can do something on Pig side.","08/Aug/16 19:56;daijy;Sure, you can decompose ControlledJob and do null check on individual variable. Would you like to give a try?","09/Aug/16 03:56;water;Hi Daniel, thanks for the comments!

1. Could you please elaborate more on ""decompose ControlledJob""?
Do you mean that we should do some checkings before calling line 182 in PigJobControl as follow:
{code}
log.debug(""Checking state of job ""+j);
{code}

2. When null is detected, do you think I can throw an Exception(May be IOException?) here, or I just log a waring, and give up logging in line 182 and make Pig continue. I prefer the latter one, as line 182 is only to write a log for debug, we should not stop Pig running if the logging has something wrong? Does it make sense to you?","09/Aug/16 05:22;daijy;Decompose means instead of invoking j.toString, you can construct the string yourself:
{code}
....
sb.append(""job name:\t"");
try {
  sb.append(j.getJobName());
} catch (NPE) {
}
log.debug(sb);
{code}
I am not sure what's the nature of status=null. If it is truly a bug in Hadoop, we shall just warn and continue.","17/Aug/16 16:05;water;Hi Daniel, thanks for the explanation!

Regarding
bq. I am not sure what's the nature of status=null
Something I found so far:
In the class of Job of Hadoop, JobStatus status is updated by the function called updateStatus(), starting from line 318
{code}
synchronized void updateStatus() throws IOException {
    try {
      this.status = ugi.doAs(new PrivilegedExceptionAction<JobStatus>() {
        @Override
        public JobStatus run() throws IOException, InterruptedException {
          return cluster.getClient().getJobStatus(status.getJobID());
        }
      });
    }
    catch (InterruptedException ie) {
      throw new IOException(ie);
    }
    if (this.status == null) {
      throw new IOException(""Job status not available "");
    }
    this.statustime = System.currentTimeMillis();
  }
{code}

I think it is not safe, because this.status will be set no matter what is returned by ugi.doAs(). Even if it returns null (maybe due to some network problems), this.status will be set to null directly. Another thread calling getJobName() has status=null.
The code followed will check if this.status is null and throw IOException. But it is weird that I did not this IOException in the hadoop log.

We also found the following message in app-master log
bq.INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.SocketTimeoutException: 70000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=xxxx remote=xxxx
but so far I could not tell if status=null has something to do with that timeout.

Anyway, I will upload the patch soon. Thanks!","19/Aug/16 08:16;water;[~daijy] 
I uploaded the very first patch
1. It is almost to clone ControlledJob.toString() and then add try and catch to handle NPE.
2. The patch applies to branch-0.15, branch-0.16 and trunk.
3. PigJobControl for hadoop20 (shims/src/hadoop20/org/apache/pig/backend/hadoop20/PigJobControl.java) does not have the same issue
4. PigJobControl originally has indent=2 spaces.

I am trying to see if I can add UT for this fix, but currently, I have no idea how to simulate the condition of status=null. Keep studying.

Would you please review the patch at your earliest convenience? Thanks for your patience and guidance.","23/Aug/16 00:31;daijy;Thank for your patch, but it is longer than I expected. Can we try to detect if j.getJob().getStatus()==null? That seems doable. I don't think it is easy for unit test for this, so I am fine without a test.","23/Aug/16 09:53;water;[~daijy] Thanks for the comments.
Regarding 
bq.Can we try to detect if j.getJob().getStatus()==null? That seems doable
It is addressed by patch 1 uploaded just now. Some comments are also refined.
Please review, thanks!","23/Aug/16 15:25;water;Hi Daniel, sorry, patch 1 breaks some core set UT (launched by ""ant -Djavac.args=""-Xlint -Xmaxwarns 1000"" clean test-commit -Dhadoopversion=23"")
The error is like
{code}
2016-08-23 21:31:37,226 [JobControl] ERROR org.apache.pig.backend.hadoop23.PigJobControl  - Error while trying to run jobs.
java.lang.IllegalStateException: Job in state DEFINE instead of RUNNING
        at org.apache.hadoop.mapreduce.Job.ensureState(Job.java:292)
        at org.apache.hadoop.mapreduce.Job.getStatus(Job.java:337)
        at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:190)
        at java.lang.Thread.run(Thread.java:745)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:276)
{code}

I will study it tomorrow. Sorry for the inconvenience.","23/Aug/16 17:11;daijy;I assume by doing "" j.getJob().getStatus()==null"", we don't need to pull Job.toString() into Pig, is that right?","25/Aug/16 08:48;water;MAPREDUCE-6762 fixes this issue on Hadoop side. When status=null, the logic is changed to get job name from the super class of Job, which is JobContextImpl, and eventually get job name from conf.
The patch is applied into branch-2 (and trunk) of Hadoop, which will be picked up by the next 2.x release (maybe 2.9?). Pig currently picks up Hadoop 2.6.0 in trunk. I think I need to add a TODO in the pig fix to remind revisiting it after Pig picks up a Hadoop version with MAPREDUCE-6762 applied.","25/Aug/16 09:43;water;HI Daniel,
do you mean ""ControlledJob.toString"" rather than ""Job.toString"" in your comment above?","25/Aug/16 12:04;water;Looking into the exception which breaks UT against patch 1. It seems that I can not call the following statement
{code}
j.getJob().getStatus() 
{code}
to check if status is null, in order to avoid NPE.
Here is the code of Job#getStatus():
{code}
  public JobStatus getStatus() throws IOException, InterruptedException {
    ensureState(JobState.RUNNING);
    updateStatus();
    return status;
  }
{code}
At the beginning, job state is DEFINE which causes the exception in ensureState(JobState.RUNNING).
When calling Job#getJobName() directly(the original code or in patch 0), it will check if state is DEFINE firstly and returns conf.getJobName() if state is DEFINE.

I might need to change it back to use try...catch. Does it make any sense to you?","25/Aug/16 12:21;water;[~daijy] I reviewed all your previous comments and need to check with you if I get you correctly. 
When you mentioned
bq. Decompose means instead of invoking j.toString, you can construct the string yourself
and 
bq. it is longer than I expected
do you mean that I could clone what ControlledJob#toString() does but do not need to include too much details so as to make it shorter?","25/Aug/16 16:58;daijy;Sorry for confusion. When I say clone ControlledJob#toString(), I don't realize it is very long. Now I prefer just do a null check on ""j.getJob().getStatus()"". If it is null, don't invoke j.toString(). I don't want to copy the toString() to Pig now.","26/Aug/16 07:23;water;Daniel, thanks for clarification! Got your idea.

I found it breaks UT (and also breaks Pig run on a cluster) if we would like to use j.getJob().getStatus() to check if job status is null.
Job#getStatus() calls ensureState() at the very beginning, to check if the state is RUNNING. If it is not, throw IllegalStateException, as
{code}
[JobControl] ERROR org.apache.pig.backend.hadoop23.PigJobControl  - Error while trying to run jobs.
java.lang.IllegalStateException: Job in state DEFINE instead of RUNNING
        at org.apache.hadoop.mapreduce.Job.ensureState(Job.java:292)
        at org.apache.hadoop.mapreduce.Job.getStatus(Job.java:337)
        at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:190)
        at java.lang.Thread.run(Thread.java:745)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:276)
{code}
The code of Job#getStatus() is 
{code}
  public JobStatus getStatus() throws IOException, InterruptedException {
    ensureState(JobState.RUNNING);
    updateStatus();
    return status;
  }
{code}
I think the easiest way to fix this JIRA is to surround
{code}
log.debug(""Checking state of job "" + j);
{code}
with try block and catch NPE.
Do you think so?","26/Aug/16 07:45;water;[~daijy], I uploaded patch 2 to use a try block to surround log.debug(). 
- If job status is not null, it calls ControlledJob#toString() as before
- if job status is null, the NPE will be caught and a warning is logged

UT (test-commit) has no failure and I also tested the patch on a cluster with Pig 0.15.0.

Could you please review at your earliest convenience?","30/Aug/16 03:06;water;Hi, [~daijy], any comments?","30/Aug/16 08:16;daijy;I am fine with the fix. The line ""Because MAPREDUCE-6762 fixes PIG-4967 from Hadoop side"" can be removed as it is a little wordy. Will commit shortly.","30/Aug/16 09:17;water;Daniel, thanks for the comments! I uploaded patch 3 with the following sentence removed from the comment.
-// Because MAPREDUCE-6762 fixes PIG-4967 from Hadoop side.-",01/Sep/16 06:49;daijy;Patch committed to trunk. Thanks Xiang for the first patch get committed!,"01/Sep/16 09:50;water;Hi Daniel, thanks very much for the review and guide! My pleasure to be a part of Pig community.","05/Sep/16 05:57;water;[~daijy] I set the fix version to 0.16.0, will you consider to commit this JIRA to origin/branch-0.16 as well?",08/Sep/16 07:06;daijy;That's fine for me. Patch committed to branch-0.16 as well.,"08/Sep/16 14:38;knoguchi;0.16.0 is already released.  
Changing it to 0.16.1.  ",,,,,,,,,,,,,,
Pig on TEZ creates wrong result with replicated join,PIG-4789,12934812,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,,mprim,mprim,28/Jan/16 13:59,01/Jul/16 14:15,13/Mar/19 23:13,01/Jul/16 14:15,0.15.0,,,,,,,,,,,,,,,,0.16.0,,tez,,,1,,,,,,,,"Please find below a minimal example of a Pig script that uses splits and replicated joins and where the output differs between MapReduce and TEZ as execution engine. The attachment also contains the sample input data.

The expected output, as created by MapReduce engine is:
{code}
(id1,123,A,)
(id2,234,,B)
(id3,456,,)
(id4,567,A,)
{code}
whereas TEZ produces
{code}
(id1,123,A,A)
(id2,234,B,B)
(id3,456,,)
(id4,567,A,A)
{code}

Removing the {{USING 'replicated'}} and using a regular join yields correct results. I am not sure if this is a Pig issue or a TEZ issue. However, as this issue silently can lead to data corruption I rated it critical. So far searching didn't indicate a similar bug or anybody being aware of it.

{code}
classdata = LOAD '/tez_bug_input1.csv' USING PigStorage(',') AS (classid:chararray, class:chararray);

data = LOAD '/tez_bug_input2.csv' USING PigStorage(',') AS (eventid:chararray, classid:chararray);

basedata = LOAD '/tez_bug_input3.csv' USING PigStorage(',') AS (eventid:chararray, foo:int);

dataJclassdata = JOIN classdata BY classid, data BY classid;

SPLIT dataJclassdata INTO classA IF class == 'A', classB IF class == 'B';

dataA = JOIN basedata BY eventid LEFT OUTER, classA BY data::eventid USING 'replicated';
dataA = foreach dataA generate basedata::eventid as eventid
	, basedata::foo as foo
	, classA::classdata::class as classA;

dataB = JOIN dataA BY eventid LEFT OUTER, classB BY eventid USING 'replicated';
dataB = foreach dataB generate dataA::eventid as eventid
	, dataA::foo as foo
	, dataA::classA as classA
    , classB::classdata::class as classB;

DUMP dataB;
{code}",,,,,,,,,,,,,,,,,28/Jan/16 13:59;mprim;tez_bug.pig;https://issues.apache.org/jira/secure/attachment/12784928/tez_bug.pig,28/Jan/16 13:59;mprim;tez_bug_input1.csv;https://issues.apache.org/jira/secure/attachment/12784925/tez_bug_input1.csv,28/Jan/16 13:59;mprim;tez_bug_input2.csv;https://issues.apache.org/jira/secure/attachment/12784926/tez_bug_input2.csv,28/Jan/16 13:59;mprim;tez_bug_input3.csv;https://issues.apache.org/jira/secure/attachment/12784927/tez_bug_input3.csv,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2016-02-04 15:46:07.409,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 04 05:55:54 UTC 2016,,,,,,,0|i2s3gn:,9223372036854775807,,,,,,,,,,"28/Jan/16 13:59;mprim;Attached the input sample files and the pig script, not possible during ticket creation.",04/Feb/16 15:46;rohini;Can you test if you still get wrong results with the current trunk? Couple of fixes have gone in. ,"04/Feb/16 15:52;rohini;Tested it and trunk returns the right results, but I am not sure which of the jiras fixed this issue as I can't remember fixing something for this particular case.","04/Feb/16 15:55;mprim;Is there some ETA for a new 0.16.0 or 0.15.1 release soon, which would include this fixes?

Also if you have an idea which patch fixes this, we could think of cherry-picking it, running trunk in production is unfortunately no option.",04/Feb/16 19:15;Krzysztof Indyk;It looks like it's same bug as in https://issues.apache.org/jira/browse/PIG-4695,"05/Feb/16 23:18;rohini;bq. Also if you have an idea which patch fixes this, we could think of cherry-picking it, running trunk in production is unfortunately no option.
   Definitely should be some fix that went into MultiQueryOptimizerTez. But many patches have gone in with fixes and with dependencies on other classes that might be hard to cherry-pick anymore or even just copy that whole class without copying other classes like TezCompiler, TezLauncher, UnionOptimizer, TezOperator, etc. 

bq. Is there some ETA for a new 0.16.0 or 0.15.1 release soon, which would include this fixes?
  We are planning to do 0.15.1 in next two weeks and 0.16 after three months. Thinking of merging all Tez related changes into 0.15.1. So it should be fixed for you in 0.15.1. ",04/Mar/16 05:55;daijy;It is PIG-4690 fixed the issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestAccumuloPigCluster always failed with timeout error,PIG-4083,12730728,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,fang fang chen,fang fang chen,fang fang chen,30/Jul/14 08:56,04/Aug/14 19:34,13/Mar/19 23:13,04/Aug/14 19:34,0.13.0,,,,,,,,,,,,,,,,0.13.1,,,,,0,,,,,,,,"TestAccumuloPigCluster always failed with timeout error.
Tried with sun jdk 6 and sun jdk 7.",,,,,,,,,,,,,ACCUMULO-3036,,,,01/Aug/14 02:28;elserj;PIG-4083-debug.patch;https://issues.apache.org/jira/secure/attachment/12659078/PIG-4083-debug.patch,04/Aug/14 04:38;fang fang chen;PIG-4083.patch;https://issues.apache.org/jira/secure/attachment/12659612/PIG-4083.patch,01/Aug/14 06:18;fang fang chen;pig_junit_tmp827919480.tar.gz;https://issues.apache.org/jira/secure/attachment/12659108/pig_junit_tmp827919480.tar.gz,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-07-31 15:53:51.34,,,no_permission,,,,,,,,,,,,,408801,Reviewed,,,Mon Aug 04 19:34:39 UTC 2014,,,,,,,0|i1yc7z:,408799,,,,,,,,,,"31/Jul/14 15:53;elserj;I'll try to look into this, [~fang fang chen]. Any logs or other information you have would be helpful.",31/Jul/14 16:35;elserj;This is passing for me using Oracle 1.7.0_55. It's possible that the MiniAccumuloCluster being started by the test is failing to start for a variety of reasons (lack of memory probably the most common). I can provide a quick patch which will add some extra logging information if you want to help me debug this.,"01/Aug/14 02:00;fang fang chen;Hi Josh, 
For your 1# comment:
Here is all the output from log file. I did not find any useful information for debug.
Testcase: test took 0.001 sec
	Caused an ERROR
Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.

For your 2# comment:
Yes, please provide the quick patch for debugging. Thanks","01/Aug/14 02:05;fang fang chen;BTW, I was uring sun jdk 1.7.0_60/1.6.0_45 and ibm jdk 1.6.0/1.7.0. All failed. If this is caused by environment, I want to know what caused this issue and how to resolve. This would be helpful if pig can provide this information. Thanks.","01/Aug/14 02:09;elserj;Sounds good, I'll get a patch with some extra debugging here for you. Out of curiosity, does it fail quickly?","01/Aug/14 02:28;elserj;Ok, [~fang fang chen]. You can apply this using {{patch -p1 PIG-4083-debug.patch}}.

Then, run just the testcase {{ant test -Dtestcase=TestAccumuloPigCluster}}.

After, please attach {{build/test/logs/TEST-org.apache.pig.backend.hadoop.accumulo.TestAccumuloPigCluster.txt}}.

Also, in that same log file, you will also see a line that matches {{INFO  org.apache.pig.backend.hadoop.accumulo.TestAccumuloPigCluster  - Starting MiniAccumuloCluster in ...}}, where {{...}} is some directory on your local filesystem. That directory is where the MiniAccumuloCluster was started from. Please attach the contents of the {{logs}} directory beneath the temporary directory path, as well.

Those two logs should help me better understand why this test was failing for you. Thanks.","01/Aug/14 06:22;fang fang chen;Hi Josh, the test is not finished yet. Do you know how long it will last until timeout?

I saw following log during ut:

test-core:
    [mkdir] Created dir: /root/ff/git/pig/build/test/logs
    [mkdir] Created dir: /tmp/pig_junit_tmp827919480

I assume ""/tmp/pig_junit_tmp827919480"" is the one what you want. Attach here.
","01/Aug/14 15:19;elserj;The timeout is set globally for the Pig Junit Ant task for 2 hours. This is defined in the build.xml and you can change it. This failure case should occur in a minute or so -- very quickly.

Looking at the logs you provided showed that when the Accumulo processes attempted to start, they had the incorrect Thrift jars on the classpath which caused:

{noformat}
Caused by: java.lang.NoSuchMethodError: org.apache.thrift.EncodingUtils.setBit(BIZ)B
{noformat}

The classpath was also printed out in the same log files that you attached in that tarball. It appears that there is only libthrift-0.9.0 included, which is correct. The problem now is trying to figure out what artifact included on the classpath is also bundling Thrift classes. I ran a diff against the classpaths that your accumulo test was running with and mine, and found the offender: {{/root/ff/git/pig/lib/hive-exec-0.8.0.jar}}. That old version of Hive is shading in Thrift classes (from 0.6 IIRC) which is where the errors are coming from.

Your classpath actually has a bunch more entries than mine, notably a dozen or so all from {{/root/ff/git/pig/lib/}}. I'll try to poke around -- I forget where/how that lib directory is used. I'll also see what I can do about the underlying AccumuloMiniCluster issue. We may have a better fix in a newer version of Accumulo we could upgrade to, otherwise, we might be able to determine when the processes immediately died and avoid the infinite loop of the test code trying to connect to an Accumulo that isn't alive.


","04/Aug/14 02:27;fang fang chen;Hi Josh,

Thanks for pointing out the root cause. Do you know how to fix this? Or is there any work around for this? 
If this is caused by thrift version(hive-0.8.0 bring), I think this issue should also happened in your environment. But it is not. Is there any special step I should do to avoid this issue?

Thanks","04/Aug/14 02:31;elserj;I have no idea what is pulling in that jar in the lib directory. It doesn't for me. I can only really give you recommendation to start from a clean repository and try to build again. If you can figure out what placed that jar there, we can see if there's something we can do to address the test failure.",04/Aug/14 04:38;fang fang chen;After upgrade hive from 0.8.0 to 0.13.1. The test case passed. Attach the patch which is for pig-0.13.,04/Aug/14 19:31;daijy;I see. This will not happen on trunk since we switch to use hive-exec-core.jar which should not wrap thrift. I can check the patch into Pig 0.13 branch if it solves your issue.,04/Aug/14 19:34;daijy;Patch committed to 0.13 branch. Thanks Fang Fang!,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Rohini to list of Apache Pig committers,PIG-3012,12613809,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,rohini,jcoveney,jcoveney,28/Oct/12 18:48,01/Nov/12 00:12,13/Mar/19 23:13,01/Nov/12 00:12,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"It's always fun for someone's first commit to be adding themselves to this page: http://pig.apache.org/whoweare.html (it's also a good chance to make sure your dev setup is properly configured to allow committing)

Welcome aboard!",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,252611,,,,2012-10-28 18:48:43.0,,,,,,,0|i0cvfr:,73037,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Cheolsoo to list of Apache Pig committers,PIG-3011,12613808,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,cheolsoo,jcoveney,jcoveney,28/Oct/12 18:48,29/Oct/12 19:50,13/Mar/19 23:13,29/Oct/12 19:50,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"It's always fun for someone's first commit to be adding themselves to this page: http://pig.apache.org/whoweare.html (it's also a good chance to make sure your dev setup is properly configured to allow committing)

Welcome aboard!",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,252610,,,,2012-10-28 18:48:24.0,,,,,,,0|i0cvfj:,73036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache Pig does not work on Amazon's Elastic MapReduce,PIG-2562,12544567,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Critical,Fixed,,russell.jurney,russell.jurney,29/Feb/12 05:53,16/May/12 19:47,13/Mar/19 23:13,16/May/12 19:47,0.10.0,0.11,0.9.1,0.9.2,,,,,,,,,,,,,0.10.0,,,,,0,am,does,emr,i,not,sad,work,"See https://forums.aws.amazon.com/thread.jspa?messageID=323063

According to this thread, only Amazon's proprietary hadoop-core.jar enables S3 to work on with Pig.  Apache Pig does not work.

Example:

Apache Pig branch-0.9 as of today:

hadoop@ip-10-195-159-114:~$ pig/bin/pig
grunt> cd s3://elasticmapreduce/samples/pig-apache/input/
2012-02-29 05:45:22,282 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. This file system object (hdfs://10.195.159.114:9000) does not support access to the request path 's3://elasticmapreduce/samples/pig-apache/input' You possibly called FileSystem.get(conf) when you should have called FileSystem.get(uri, conf) to obtain a file system supporting your path.
Details at logfile: /home/hadoop/pig_1330494091268.log
grunt> quit

EMR's Pig as of today:
hadoop@ip-10-195-159-114:~$ pig
2012-02-29 05:45:35,626 [main] INFO  org.apache.pig.Main - Logging error messages to: /home/hadoop/pig_1330494335621.log
2012-02-29 05:45:35,841 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://10.195.159.114:9000
2012-02-29 05:45:36,200 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: 10.195.159.114:9001
grunt> cd s3://elasticmapreduce/samples/pig-apache/input/

",Amazon Elastic MapReduce,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-03-14 08:39:31.654,,,no_permission,,,,,,,,,,,,,229753,,,,Wed May 16 18:42:49 UTC 2012,,,,,,,0|i0h38v:,97788,,,,,,,,,,14/Mar/12 08:39;daijy;how about load/store?,14/Mar/12 18:44;russell.jurney;Nothing with s3 works.,"29/Apr/12 21:17;dvryaboy;According to the hortonworks blog post, 10 should work with EMR. Can you verify that the 0.10 release still has this problem, and post instructions to reproduce?","01/May/12 00:38;daijy;This should be solved in 0.10.0. If you saw any more issue, please open a new Jira with more specific information.",12/May/12 01:59;russell.jurney;I think this should be verified before we close it.,"13/May/12 08:09;daijy;Mostly Pig works with s3. The case here is home directory cannot be on s3, which should be a minor feature. We shall open a new Jira with more specific title to track it.",13/May/12 13:14;russell.jurney;Pig 0.9 did not work with S3 at all.  This needs testing by somebody in 0.10 to be closed.,"13/May/12 17:39;daijy;Yes, we committed several s3 patches to 0.10. I tested s3 works for script file, jars, macros, parameter files, script udf files in 0.10.0.",16/May/12 18:42;danoyoung;Is this still a known issue?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SAMPLE/RANDOM(udf) before skewed join failing with NPE,PIG-5372,13204098,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,12/Dec/18 21:07,25/Jan/19 21:53,13/Mar/19 23:13,25/Jan/19 21:53,0.16.0,,,,,,,,,,,,,,,,0.17.1,,,,,0,,,,,,,,"Sample short code like below
{code}
A = LOAD 'input.txt' AS (a1:int, a2:chararray, a3:int);
B = LOAD 'input.txt' AS (b1:int, b2:chararray, b3:int);

A2 = FOREACH A generate *, RANDOM() as randnum;

D = join A2 by a1, B by b1 using 'skewed' parallel 2;

store D into '$output';
{code}

Fails with NPE. 
{noformat}
2018-12-12 16:06:04,860 [Dispatcher thread: Central] INFO  org.apache.tez.dag.history.HistoryEventHandler - [HISTORY][DAG:dag_1544648742542_0001_1][Event:TASK_FINISHED]: vertexName=scope-55, taskId=task_1544648742542_0001_1_02_000000, startTime=1544648745036, finishTime=1544648764857, timeTaken=19821, status=KILLED, successfulAttemptID=null, diagnostics=TaskAttempt 0 failed, info=[Error: Failure while running task:org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing (Name: Local Rearrange[tuple]{int}(false) - scope-29 ->       scope-58 Operator Key: scope-29): org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [POUserFunc (Name: POUserFunc(org.apache.pig.builtin.RANDOM)[double] - scope-40 Operator Key: scope-40) children: null at []]: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:315)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNextTuple(POLocalRearrange.java:287)
        at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POLocalRearrangeTez.getNextTuple(POLocalRearrangeTez.java:131)
        at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:420)
        at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:282)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:337)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing [POUserFunc (Name: POUserFunc(org.apache.pig.builtin.RANDOM)[double] - scope-40 Operator Key: scope-40) children: null at []]: java.lang.NullPointerException
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:367)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:408)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:325)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:305)
        ... 17 more
Caused by: java.lang.NullPointerException
        at org.apache.pig.builtin.RANDOM.exec(RANDOM.java:51)
        at org.apache.pig.builtin.RANDOM.exec(RANDOM.java:37)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:332)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNextDouble(POUserFunc.java:396)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:343)
        ... 20 more
]
{noformat}
",,,,,,,,,,,,,,,,,12/Dec/18 21:20;knoguchi;pig-5372-v1.patch;https://issues.apache.org/jira/secure/attachment/12951558/pig-5372-v1.patch,18/Jan/19 17:47;knoguchi;pig-5372-v2.patch;https://issues.apache.org/jira/secure/attachment/12955444/pig-5372-v2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2018-12-18 17:31:36.248,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Fri Jan 25 21:53:15 UTC 2019,,,,,,,0|s01gnc:,9223372036854775807,,,,,,,,,,"12/Dec/18 21:20;knoguchi;Same would happen for SAMPLE since it internally calls RANDOM.

NPE happens at 
{code:java}
            int jobidhash = PigMapReduce.sJobConfInternal.get().get(MRConfiguration.JOB_ID).hashCode();
{code}
when job_id returns null.

This was happening when SkewedPartitioner.setConf was replacing a configuration dropping the original one with necessary job info. 
Attaching a patch({{pig-5372-v1.patch}}) that would just add instead of replacing the conf.","18/Dec/18 17:31;rohini;I think it should be fine to remove the below lines instead. They seem to be not used

{code}
PigMapReduce.sJobConfInternal.set(conf);
        PigMapReduce.sJobConf = conf;
{code}","18/Dec/18 17:59;knoguchi;bq. I think it should be fine to remove the below lines instead. They seem to be not used

I think the setting came from PIG-1467.

","21/Dec/18 19:50;rohini;This failure is only with Tez or mapreduce as well? I am not able to tell exactly why Daniel is doing that in https://issues.apache.org/jira/browse/PIG-1467 as the description of jira does not have proper stacktrace.  In Tez, we read the quantile file from memory (broadcast edge) instead of local file (distributed cache) like in mapreduce. So we can override setConf() in SkewedPartitionerTez if this issue is specific to Tez.","24/Dec/18 16:51;knoguchi;bq. I am not able to tell exactly why Daniel is doing that in https://issues.apache.org/jira/browse/PIG-1467

Me neither.  [~daijy], can you help us? 

Without fully understanding what's happening there, I would rather keep the current patch to avoid introducing unexpected regression.",02/Jan/19 19:51;daijy;Wow that's back in 2010 :). I think SkewedPartitioner.setConf is passing conf to MapRedUtil.loadPartitionFileFromLocalCache via PigMapReduce.sJobConf. This is no longer necessary as MapRedUtil.loadPartitionFileFromLocalCache takes mapConf parameter (in a later patch). We can change MapRedUtil.loadPartitionFileFromLocalCache to retrieve fs.file.impl/fs.hdfs.impl from mapConf. Then we don't need overwrite PigMapReduce.sJobConf in SkewedPartitioner.setConf.,"18/Jan/19 17:50;knoguchi;Thanks [~daijy]! 

{quote}
We can change MapRedUtil.loadPartitionFileFromLocalCache to retrieve fs.file.impl/fs.hdfs.impl from mapConf. Then we don't need overwrite PigMapReduce.sJobConf in SkewedPartitioner.setConf.
{quote}
Attaching {{pig-5372-v2.patch}} which does this.
Ideally we should have a test case for ""fs.file.impl/fs.hdfs.impl "" but I fail to see when we ever want to disable cache for fs.file&fs.hdfs in the first place.",24/Jan/19 23:14;rohini;+1,"25/Jan/19 21:53;knoguchi;Thanks for the review Rohini, Daniel!!! 

Committed to trunk and branch-0.17.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use CircularFifoBuffer in InterRecordReader,PIG-5374,13208251,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,szita,szita,08/Jan/19 09:26,08/Jan/19 10:38,13/Mar/19 23:13,08/Jan/19 10:38,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"We're currently using CircularFifoQueue in InterRecordReader, and it comes from commons-collections4 dependency. Hadoop 2.8 installations do not have this dependency by default, so for now we should switch to the older CircularFifoBuffer instead (which comes from commons-collections and it's present).

We should open a separate ticket for investigating what libraries should we update. ",,,,,,,,,,,,,,,,,08/Jan/19 09:35;szita;PIG-5374.0.patch;https://issues.apache.org/jira/secure/attachment/12954137/PIG-5374.0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2019-01-08 09:38:21.409,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 08 10:38:27 UTC 2019,,,,,,,0|u00m88:,9223372036854775807,,,,,,,,,,08/Jan/19 09:38;nkollar;+1,"08/Jan/19 10:38;szita;Thanks Nandor, committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InterRecordReader might skip records if certain sync markers are used,PIG-5373,13205719,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,szita,szita,20/Dec/18 13:16,03/Jan/19 15:57,13/Mar/19 23:13,03/Jan/19 15:57,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"Due to bug in InterRecordReader#skipUntilMarkerOrSplitEndOrEOF(), it can happen that sync markers are not identified while reading the interim binary file used to hold data between jobs.

In such files sync markers are placed upon writing, which later help during reading the data. These are random generated and it seems like that in some rare combinations of markers and data preceding it, they cannot be not found. This can result in reading through all the bytes (looking for the marker) and reaching split end or EOF, and extracting no records at all.

This symptom is also observable from JobHistory stats, where if a job is affected by this issue, will have tasks that have HDFS_BYTES_READ or FILE_BYTES_READ about equal to the number bytes of the split, but at the same time having MAP_INPUT_RECORDS=0

One such (test) example is this:
{code:java}
marker: [-128, -128, 4] , data: [127, -1, 2, -128, -128, -128, 4, 1, 2, 3]{code}
Due to a bug, such markers whose prefix overlap with the last data chunk are not seen by the reader.",,,,,,,,,,,,,,,,,20/Dec/18 13:20;szita;PIG-5373.0.patch;https://issues.apache.org/jira/secure/attachment/12952532/PIG-5373.0.patch,03/Jan/19 13:32;szita;PIG-5373.1.patch;https://issues.apache.org/jira/secure/attachment/12953633/PIG-5373.1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2018-12-21 18:36:15.472,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 03 15:57:11 UTC 2019,,,,,,,0|u006uw:,9223372036854775807,,,,,,,,,,"20/Dec/18 13:23;szita;Attached [^PIG-5373.0.patch] which corrects the reading of sync markers using a fifo, and compares the fifo content with the expected marker.

Test case attached, which verifies in a brute force way, that such prefix scenarios are handled well.

[~nkollar], [~rohini] can you take a look please?","21/Dec/18 18:36;rohini;[~szita],
   This is broken by PIG-3655 right? If so affected version should be  0.18.0 ","23/Dec/18 17:53;szita;[~rohini], right, it is not even released yet, so I just leave it blank then","02/Jan/19 12:29;nkollar;I have one observation to the patch: to be future-proof, instead CircularFifoBuffer from commons-collection I think we should use CircularFifoQueue from commons-collections4. On one hand CircularFifoBuffer was removed from the latest commons collections code, on the other hand CircularFifoQueue is generic, so we can eliminated iterating through Object items and casting to integer. Be aware of one thing: the semantic of isFull has changed, CircularFifoQueue is never full. The isFull call should be replaced to {{queue.size() == queue.maxSize()}}.","03/Jan/19 10:16;szita;Thanks for taking a look [~nkollar], I've uploaded a new patch that uses CircularFifoQueue from commons-collections4.",03/Jan/19 14:45;nkollar;+1 on PIG-5373.1.patch,"03/Jan/19 15:57;szita;Committed to trunk, thanks a lot for reviewing Nandor!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinStorage and InterStorage approach to record markers is broken,PIG-3655,12687778,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,jeffplaisance,jeffplaisance,08/Jan/14 01:08,21/Dec/18 18:35,13/Mar/19 23:13,22/Jul/17 11:40,0.10.0,0.10.1,0.11,0.11.1,0.12.0,0.2.0,0.3.0,0.4.0,0.5.0,0.6.0,0.7.0,0.8.0,0.8.1,0.9.0,0.9.1,0.9.2,0.18.0,,,,,0,,,,,,,,"The way that the record readers for these storage formats seek to the first record in an input split is to find the byte sequence 1 2 3 110 for BinStorage or 1 2 3 19-21|28-30|36-45 for InterStorage. If this sequence occurs in the data for any reason (for example the integer 16909166 stored big endian encodes to the byte sequence for BinStorage) other than to mark the start of a tuple it can cause mysterious failures in pig jobs because the record reader will try to decode garbage and fail.

For this approach of using an unlikely sequence to mark record boundaries, it is important to reduce the probability of the sequence occuring naturally in the data by ensuring that your record marker is sufficiently long. Hadoop SequenceFile uses 128 bits for this and randomly generates the sequence for each file (selecting a fixed, predetermined value opens up the possibility of a mean person intentionally sending you that value). This makes it extremely unlikely that collisions will occur. In the long run I think that pig should also be doing this.

As a quick fix it might be good to save the current position in the file before entering readDatum, and if an exception is thrown seek back to the saved position and resume trying to find the next record marker.",,,,,,,,,,,,,,PIG-5373,,,11/Jul/17 14:14;szita;PIG-3655.0.patch;https://issues.apache.org/jira/secure/attachment/12876629/PIG-3655.0.patch,11/Jul/17 16:41;szita;PIG-3655.1.patch;https://issues.apache.org/jira/secure/attachment/12876653/PIG-3655.1.patch,12/Jul/17 08:01;szita;PIG-3655.2.patch;https://issues.apache.org/jira/secure/attachment/12876782/PIG-3655.2.patch,18/Jul/17 14:35;szita;PIG-3655.3.patch;https://issues.apache.org/jira/secure/attachment/12877809/PIG-3655.3.patch,20/Jul/17 18:04;szita;PIG-3655.4.patch;https://issues.apache.org/jira/secure/attachment/12878221/PIG-3655.4.patch,21/Jul/17 08:59;szita;PIG-3655.5.patch;https://issues.apache.org/jira/secure/attachment/12878323/PIG-3655.5.patch,25/Jul/17 14:58;szita;PIG-3655.sparkNulls.2.patch;https://issues.apache.org/jira/secure/attachment/12878809/PIG-3655.sparkNulls.2.patch,24/Jul/17 12:09;szita;PIG-3655.sparkNulls.patch;https://issues.apache.org/jira/secure/attachment/12878609/PIG-3655.sparkNulls.patch,,,,8.0,,,,,,,,,,,,,,,,,,,2014-05-14 15:55:07.461,,,no_permission,,,,,,,,,,,,,366778,,,,Mon Jul 31 11:31:13 UTC 2017,,,,,,,0|i1r80v:,367089,,,,,,,,,,"08/Jan/14 01:10;jeffplaisance;see also PIG-648, PIG-691, and PIG-814","14/May/14 15:55;vanaepi;I suspect I am suffering from this issue. We are generating serialized objects in an UDF returning DataByteArray. The script runs in two mapred jobs and fails during the map stage of the second job with the following error message :

{quote}
java.lang.RuntimeException: Unexpected data type 48 found in stream.
at org.apache.pig.data.BinInterSedes.readDatum(BinInterSedes.java:422)
at org.apache.pig.data.BinInterSedes.readDatum(BinInterSedes.java:313)
at org.apache.pig.data.utils.SedesHelper.readGenericTuple(SedesHelper.java:144)
at org.apache.pig.data.BinInterSedes.readDatum(BinInterSedes.java:344)
at org.apache.pig.impl.io.InterRecordReader.nextKeyValue(InterRecordReader.java:113)
at org.apache.pig.impl.io.InterStorage.getNext(InterStorage.java:77)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:211)
at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:483)
at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:76)
at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:85)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:139)
at org.apache.hadoop.mapred.MapTask.runNew
{quote}

I suspect that by accident the exact same sequence that is used as a record splitter also occurs somewhere in one of our serialized objects but the amount of records is so large that I'm unsure how to go about isolating the issue in order to reproduce it. A custom UDF returning the exact same byte sequence does not cause the script to fail, which seems odd imo.","15/Apr/15 20:58;mwagner;This happened to us as well. The ""Unexpected data type"" message is a signal that this has happened. It can also show up as ClassCastExceptions further down in the pipeline. I have a patch which I'll prepare for trunk.",11/Jul/17 14:13;szita;This ticket was not updated for a while so unless any objections I'm happy to take this over as our customers have also encountered this problem.,"11/Jul/17 14:30;szita;I agree that we should follow Hadoop's approach and generate a longer and random record marker instead of 0x010203.
I propose we use this ticket for fixing InterStorage, doing the same approach for BinStorage would cause an incompatibility for files already written in the past (and I also saw some other problems in BinStorage e.g. writing bigdecimals..)

So for InterStorage we're not bound by any contract and we can change its format freely.
I uploaded a fix [^PIG-3655.0.patch]: InterRecordWriter will now generate a random record marker the same way [Hadoop does it for a SequenceFile|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java#L870]

InterRecordReader will read this 16 bytes long magic sequence at initialization time from the beginning of the file (+3 because I also kept the original marker 0x010203). Later during reading records it will always compare the record marker seen with the one the reader was initialized with.
In theory this should result in much less collisions with data, the original sequence was 3+1 bytes (+1 for marker a Tuple type), but now we have 20 bytes in total (original 3 + 16 random + 1 tuple type)","11/Jul/17 16:46;szita;I realized some users might find the new overhead too much for their use case, so I'm attaching a new patch [^PIG-3655.1.patch] with the record marker sequence length being configurable.
[~rohini] what's your opinion?",12/Jul/17 08:05;szita;Only fixing some imports in [^PIG-3655.2.patch],"12/Jul/17 12:11;nkollar;Unit tests passed, looks good to me!","12/Jul/17 18:03;knoguchi;While we wait for Rohini (or others) for the review, can you check {{TestFRJoin2.testTooBigReplicatedFile}} ?  I think this started failing after your patch.  (You probably just need to adjust the max in the test.)","12/Jul/17 21:09;szita;Thanks for taking a look Koji!

Indeed this is just a test issue I will fix it in the next patch. For making it clearer I'll introduce the following in this test:
{code}
int expectedReplicateSize = 2*(3+PigConfiguration.PIG_INTERSTORAGE_RECORDMARKER_SIZE_DEFAULT+1 +1+1)
                          + 1*(3+PigConfiguration.PIG_INTERSTORAGE_RECORDMARKER_SIZE_DEFAULT+1 +1);
{code}

 I'm also running unit tests now on trunk, so I'll wait until those are finished before attaching a new patch (I've been running unit tests on 0.12.0 first because the customer is on that version + E2E tests)","12/Jul/17 22:40;rohini;SequenceFile uses a sync marker every couple of records (after SYNC_INTERVAL bytes) to be used to determine boundaries while reading across splits unlike InterStorage logic which writes out record marker for every record which is actually bad. 

https://github.com/apache/hadoop/blob/62857be2110aaded84a93fc9891742a1271b2b85/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java#L1397-L1402

Can we get rid of the original record markers and just write the sync marker similar to SequenceFile? We can keep sync interval and sync marker size as configurable parameters.

For a default 2000 sync interval and 16 byte sync marker size, for 16 records
    - overhead would be roughly same for record sizes of 399 bytes. It would take 48 bytes in both cases
    - Any records with lesser size will have lesser overhead. For eg: If record size is 125 bytes, 3 byte record marker will have 48 bytes overhead for 16 records while there will be only one 16 byte sync marker.
    - For records of size (especially case of bags) bigger than 399 bytes, it will have higher overhead than 3 byte markers.

If we use 10 byte as default sync marker size, for 10 records
    - Overhead would be roughly same for record sizes of 600 bytes. It would take 30 bytes in both cases.
    - Any records with lesser size will have lesser overhead. For eg: If record size is 200 bytes, 3 byte record marker will have 30 bytes overhead for 10 records while there will be only one 10 byte sync marker.
   - For records of size (especially case of bags) bigger than 600 bytes, it will have higher overhead than 3 byte markers.

So 10 byte sync marker size should be decent for our user cases. Anyone running into collision can increase it to 16 bytes.","12/Jul/17 22:47;rohini;[~szita],
   In future, you can consider using mock.Storage for tests like these. Simplifies test writing by not having to create input files or cleaning up output files and also runs faster.","13/Jul/17 08:59;szita;[~rohini] I see that a sync interval approach (rather than per record approach) is resulting less overhead - however, what happens if the record reader is starting to read a split where there are multiple records are to be found before the first sync marker? I think this would result in data loss, because we would be skipping bytes until the marker is found (see 2nd split (row) below):
{code}
M.size = 10, M.interval = 1000, mr.max.split=1500 (rows below denote splits)

[M] [record(500)] [record(500)] [M] [record(480)-
(20)] [record(500)] [M] [record(500)] [record(470)-
(30)]
{code}","13/Jul/17 15:06;rohini;In the first split, currently we read past the split end till we reach a record marker. That is why we skip and read after the first record marker in second split. Logic will remain the same except that it will be sync marker now. In the first split, records should be processed after the end till a syncmarker is reached. If the sync marker itself is divided across split boundaries, you will have to read till the next sync marker. Same as the case with record markers.","13/Jul/17 16:25;szita;I see.. Well actually right now the way we read past a split is to finish the current record, and then check if we have reached / are past the split.end, and we don't care about record markers there..

But then I guess this is how we want this to be if we have sync markers with sync intervals: we don't want to depend on split end, but rather on next sync marker (or the end of file of course).
Is this what you had in mind?",13/Jul/17 16:51;rohini;Yes. Same as SequenceFile.Reader.,"18/Jul/17 14:37;szita;[~rohini] I've attached [^PIG-3655.3.patch] with the sync interval approach. I'm currently running unit and e2e tests, will update once I have a result.",19/Jul/17 20:57;szita;I had clean unit test run and a clean e2e full test suite run. Please let me know what you think of the latest patch,"20/Jul/17 14:23;rohini;Few comments:
1) Instead of using out.size(), you should making it FSDataOutputStream.getPos() and change InterRecordWriter to take FSDataOutputStream in the constructor. DataOutputStream.size() is limited to INTEGER.MAX_VALUE and wraps around on overflow.

2) Can you add a comment explaining the logic
{code}
long expectedReplicateSize = PigConfiguration.PIG_INTERSTORAGE_SYNCMARKER_SIZE_DEFAULT + 2*(1 +1+1)
	                                       + PigConfiguration.PIG_INTERSTORAGE_SYNCMARKER_SIZE_DEFAULT + 1*(1 +1);
{code}

I have couple of more questions. Will catch you on chat for those.","20/Jul/17 15:06;rohini;Comments on the read logic:
1) In readSyncFullyOrEOF() method, {{if (b == -1) return false}} should be done separately for the first byte read and not inside the while block. If we encounter -1 half way reading sync marker, it should throw the Corrupt data file exception.
2) skipUntilMarkerOrEOF() should be skipUntilMarkerOrSplitEndOrEOF(). If you have not found the syncmarker after reading till split end, then you need to bail. Currently it reads till EOF which is wasteful. With that change you can also get rid of  {{if (in.getPosition()-syncMarker.length > end) return false}} condition in nextKeyValue()
3) Could you add a test for sync marker starting exactly at second split end. Listing out all we need including what you already have
  -  Sync marker only at the beginning of the file. Nothing between splits.
  -  Sync marker ending exactly at the first split end
  -  Sync marker starting exactly at the second split beginning
  -  Sync marker in between two split boundaries (sync marker starting at first split and ending at second split)
  -  Sync marker after first split end (few bytes after beginning of the second split)
Can you add these as comments to each test as well to make it more clear. Test names are good, but a comment will help understand better.","20/Jul/17 18:08;szita;[~rohini] Thanks for useful the comments! I've addressed them in [^PIG-3655.4.patch]
For making the test cases more clear with comments - I decided to illustrate the bytes themselves as they are organized into splits. I think that is worth more than any words.

I'm starting another run of testing and will update later as usual.","20/Jul/17 19:26;rohini;Some comments on the latest patch:
1) Would be good if we got rid of the i > 0 check within the loop. b == -1 check can also be removed when throwing exception
{code}
if ((byte) b != syncMarker[0]) {
         throw new IOException(""Corrupt data file, expected sync marker at position "" + in.getPosition());
      } 
      int i = 1;                                                           
      while (i < syncMarker.length) {                                      
          b = in.read();
          if ((byte) b != syncMarker[i]) {
              throw new IOException(""Corrupt data file, expected sync marker at position "" + in.getPosition());
          }                                                                                                                  
          ++i;
      }
{code}

2) The split illustration is very useful and makes the test easier to understand. Thanks for adding that. Still it would be good to have the one line explanation of the test in the beginning of the javadoc as well for readability. 

3) Please do shorten the test name of testSyncMarkerOverlappingMarkerWithSplitsWithoutMarkerBeforeAndAfter or move it into testSyncMarkerOverlappingMarker() test case itself.

4) testSyncMarkerOneMarkerAtBeginningOnly - Test has testInterStorageSyncMarker(1024, 10, 2000L) which means there will only be one split. Can you lower split size so that there is at least 3 splits?

5) Representation of [ 22 ] has spaces while other record sizes don't","21/Jul/17 09:03;szita;[~rohini] please see your comments addressed in [^PIG-3655.5.patch]

The representation of [ 22 ] was deliberately decorated with spaces so that it makes an impression of being a longer sequence of bytes - I think at first look it helps to see all those records through quicker. For the rest of your questions I've made the adjustments.",21/Jul/17 17:34;szita;The test results came back all green.,"21/Jul/17 17:42;rohini;+1.

","22/Jul/17 11:40;szita;[^PIG-3655.5.patch] committed to trunk, thanks for the review Rohini, Nandor and Koji!","24/Jul/17 13:10;szita;[~rohini] it seems like Spark is writing some NULLs after the last record, and I made InterStorage more strict than it was: it used to eat and skip over these bytes of garbage but now it's throwing an [exception|https://builds.apache.org/job/Pig-spark/lastCompletedBuild/testReport/org.apache.pig.test/TestEvalPipeline/testCogroupAfterDistinct/] when it finds this NULL. I suppose we could adjust the condition for EOF in {{readDataOrEOF}} as per [^PIG-3655.sparkNulls.patch]","24/Jul/17 20:18;rohini;bq. it seems like Spark is writing some NULLs after the last record
  Wouldn't it be better if we fix the code that writes unnecessary NULLs instead?","25/Jul/17 14:14;szita;It seems like this is an effect of code in JoinGroupSparkConverter.java and PairRDDFunctions.scala.

If the POPackage operator returns a [POStatus.STATUS_NULL|https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/spark/converter/JoinGroupSparkConverter.java#L211], instead of skipping and awaiting the next tuple [as MR mode does|https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java#L438] the method in Spark converter returns with null. This null will be picked up by the caller in Spark code and then [gets written into the (FS) outputstream|https://github.com/apache/spark/blob/v1.6.1/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L1113].

This is how we end up with NULLs in the files.

What I can think of as a fix is to return with a special kind of Tuple which the record writer knows it has to skip; as it can be seen in [^PIG-3655.sparkNulls.2.patch]. Although this seems like a hack, the other option I can think of is to skip NULLs while reading in InterRecordReader - but then it's just handling the sympthoms and not the root cause.
Any ideas [~rohini], [~kellyzly]?","25/Jul/17 22:28;rohini;bq. What I can think of as a fix is to return with a special kind of Tuple which the record writer knows it has to skip
  This is hacky as you mentioned and do not prefer doing that.

bq. If the POPackage operator returns a POStatus.STATUS_NULL, instead of skipping and awaiting the next tuple as MR mode does the method in Spark converter returns with null. This null will be picked up by the caller in Spark code and then gets written into the (FS) outputstream.
   Spark also should be doing the same. It should read next record (pkgOp.getNextTuple()) if STATUS_NULL is received instead of returning null. ","26/Jul/17 08:47;kellyzly;[~szita]:  Can you add a filter to filter the empty tuple in rdd in JoinGroupSparkConverter?
from 
{code}
   return rdd.toJavaRDD().map(new GroupPkgFunction(pkgOp)).rdd();
{code}
to 
{code}
  return rdd.toJavaRDD().map(new GroupPkgFunction(pkgOp)).filter(new Function<Tuple, Boolean>() {
                @Override
                public Boolean call(Tuple objects) throws Exception {
                    if( objects == null){
                        return false;
                    }  else{
                        return true;
                    }
                }
            }).rdd();

{code}

 have not fully compiled or tested above code. just for reference.",26/Jul/17 16:42;rohini;What if there is actually a null tuple in output?,"26/Jul/17 22:01;kellyzly;[~szita]: can you provide simple script which i can reproduce the error?  sorry i have not read all the comments, so may be my understanding is not right.

{quote}

it seems like Spark is writing some NULLs after the last record
{quote}

  this only happens in this case or all cases in spark mode?   If only in this case, can you provide the script? thanks!","31/Jul/17 11:31;szita;[~rohini], [~kellyzly] let's move the Spark related conversation to a new Jira ticket and continue there instead: PIG-5277",,,,,,,
Braces without escaping in regexes throws error in recent perl versions,PIG-5368,13196782,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,abstractdog,abstractdog,abstractdog,07/Nov/18 10:22,10/Dec/18 21:23,13/Mar/19 23:13,20/Nov/18 23:34,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,," 
|In perl v5.22, using a literal { in a regular expression was deprecated, and will emit a warning if it isn't escaped: \{. In v5.26, this won't just warn, it'll cause a syntax error.|

Example: [https://github.com/apache/pig/blob/e766b6bf29e610b6312f8447fc008bed6beb4090/test/e2e/pig/tests/cmdline.conf#L47]  

{code}
$ perl -e 'print ""It matches\n"" if ""Hello World"" =~ /World{abc}/'
Unescaped left brace in regex is illegal here in regex; marked by <-- HERE in m/World\{ <-- HERE abc}/ at -e line 1.

$ perl -e 'print ""It matches\n"" if ""Hello World"" =~ /World\{abc}/'

{code}


 ",,,,,,,,,,,,,,,,,10/Dec/18 19:43;abstractdog;PIG-5368-1.addendum.patch;https://issues.apache.org/jira/secure/attachment/12951253/PIG-5368-1.addendum.patch,09/Nov/18 13:23;abstractdog;PIG-5368-1.patch;https://issues.apache.org/jira/secure/attachment/12947576/PIG-5368-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2018-11-20 23:34:50.388,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Mon Dec 10 21:23:12 UTC 2018,,,,,,,0|s007u0:,9223372036854775807,,,,,,,,,,20/Nov/18 23:34;daijy;Patch committed to trunk. Thanks Laszlo!,10/Dec/18 21:23;daijy;Also committed PIG-5368-1.addendum.patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uids not set in inner schemas after UNION ONSCHEMA,PIG-5312,13115706,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,tmwoodruff,tmwoodruff,tmwoodruff,02/Nov/17 18:36,30/Nov/18 21:12,13/Mar/19 23:13,27/Dec/17 15:57,0.16.0,0.17.0,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"Ran into a failure with a script of the form:

{code}
u = union onschema x, y;  -- schema: (a, b: {(m:int, n: chararray)})
z = foreach u {
    i = foreach b generate m + 5;
    generate a, i;
}
{code}

The issue ended up being that {{LOUnion}} is not setting uids on inner schemas. This means that uids on inner schema fields are all -1, so when {{ProjectExpression.getFieldSchema()}} tries to look up the fields in the inner select from the inner schema, all the fields match, and the last field's schema ends up being returned. In the example above this causes {{TypeCheckingExpVisitor.addCastsToNumericBinExpression()}} to fail for the addition operator (since the returned field schema is a chararray).

This only seems to affect the schema, so I don't think this should cause bad data to be produced.",,,,,,,,,,,,,PIG-5370,,,,02/Nov/17 18:39;tmwoodruff;PIG-5312.patch;https://issues.apache.org/jira/secure/attachment/12895463/PIG-5312.patch,27/Dec/17 15:53;knoguchi;PIG-5312_fixedspace.patch;https://issues.apache.org/jira/secure/attachment/12903817/PIG-5312_fixedspace.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-11-15 00:12:48.375,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Wed Dec 27 15:57:37 UTC 2017,,,,,,,0|i3mbqn:,9223372036854775807,,,,,,,,,,"15/Nov/17 00:12;rohini;[~knoguchi],
   Possible for you to review this? You are more familiar with the uid stuff than me.",15/Nov/17 07:40;knoguchi;On parental leave but I'll see if I can review it next week.  (Sorry for the delay Travis!),"21/Dec/17 20:44;knoguchi;Sorry for such a delay in review.  Took me time to understand the original LOUnion.java (before this patch). :)

Patch looks good.  Only question I have to myself is,
{code}
168                     if (inputFieldSchema.schema != null) {
169                         fieldInputSchemas.add(inputFieldSchema.schema);
170                     } 
...
203             if (outputFieldSchema.schema != null) {
204                 setMergedSchemaUids(outputFieldSchema.schema, fieldInputSchemas);
205             }
{code}
For line 168 and 203 where we check for null schema, do we also need to check for {{DataType.isSchemaType(type)}} ?  
I've seen places where we do put those checks but felt they were redundant. 

I'll wait 1-2 days before committing in case Rohini or Daniel has feedback.","22/Dec/17 04:31;rohini;+1. Looks good to me.

DataType.isSchemaType(type) only tells if it is a bag, map or tuple. It is redundant.",27/Dec/17 15:53;knoguchi;Committing to trunk now.  Took out the empty spaces in the original patch.,"27/Dec/17 15:57;knoguchi;Committed to trunk (0.18). 
Thank you for your patch [~tmwoodruff]!!! 

Also, thanks for the additional review Rohini!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Union onschema + columnprune dropping used fields ,PIG-5370,13199919,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,21/Nov/18 19:49,30/Nov/18 21:12,13/Mar/19 23:13,30/Nov/18 21:12,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"After PIG-5312, below query started failing.
{code}
A = load 'input.txt' as (a1:int, a2:chararray, a3:int);
B = FOREACH (GROUP A by (a1,a2)) {
    A_FOREACH = FOREACH A GENERATE a2,a3;
    GENERATE A, FLATTEN(A_FOREACH) as (a2,a3);
}
C = load 'input2.txt' as (A:bag{tuple:(a1: int,a2: chararray,a3:int)},a2: chararray,a3:int);

D = UNION ONSCHEMA B, C;

dump D;
{code}

{code:title=input1.txt}
1       a       3
2       b       4
2       c       5
1       a       6
2       b       7
1       c       8
{code}

{code:title=input2.txt}
{(10,a0,30),(20,b0,40)} zzz     222
{code}
{noformat:title=Expected output}
({(10,a0,30),(20,b0,40)},zzz,222)
({(1,a,6),(1,a,3)},a,6)
({(1,a,6),(1,a,3)},a,3)
({(1,c,8)},c,8)
({(2,b,7),(2,b,4)},b,7)
({(2,b,7),(2,b,4)},b,4)
({(2,c,5)},c,5)
{noformat}
{noformat:title=Actual (incorrect) output}
({(10,a0,30),(20,b0,40)})    ****ONLY 1 Field ****
({(1,a,6),(1,a,3)},a,6)
({(1,a,6),(1,a,3)},a,3)
({(1,c,8)},c,8)
({(2,b,7),(2,b,4)},b,7)
({(2,b,7),(2,b,4)},b,4)
({(2,c,5)},c,5)
{noformat}",,,,,,,,,,,,,,,,,28/Nov/18 05:56;knoguchi;pig-5370-v1.patch;https://issues.apache.org/jira/secure/attachment/12949796/pig-5370-v1.patch,29/Nov/18 17:28;knoguchi;pig-5370-v2.patch;https://issues.apache.org/jira/secure/attachment/12950047/pig-5370-v2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2018-11-29 07:57:39.836,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Fri Nov 30 21:12:09 UTC 2018,,,,,,,0|s00r2w:,9223372036854775807,,,,,,,,,,"21/Nov/18 20:27;knoguchi;Pig does create the correct result if we skip ColumnMapKeyPrune optimization.

Calling ""explain D"" with '-t ColumnMapKeyPrune', it shows
{noformat}
|---D: (Name: LOUnion Schema: A#40:bag{tuple#41:tuple(a1#42:int,a2#43:chararray,a3#44:int)},a2#43:chararray,a3#44:int)
    |
    |---B: (Name: LOForEach Schema: A#36:bag{#37:tuple(a1#9:int,a2#*10*:chararray,a3#*11*:int)},a2#*10*:chararray,a3#*11*:int)
    |
    |---C: (Name: LOForEach Schema: A#17:bag{tuple#49:tuple(a1#50:int,a2#51:chararray,a3#52:int)},a2#22:chararray,a3#23:int)
{noformat}

This issue only happen when we have a relation like B where inner schema contains a field with same uid as the one at the root level.  In the above example, uid {{\*10\*}} and {{\*11\*}}.

Before PIG-5312, schema of the inner bag was set to null so we didn't have this issue.
With PIG-5312, and the way LOUnion determines the output UIDS based on input UIDs, two issues are happening.
# schema of LOUnion is using the same uid for inner bag and outside. (UID 43 & 44)
# ColumnMapKeyPrune is (incorrectly) determining that a2#22 & a3#23 are not being used and dropping them. 

Reading {{DuplicateForEachColumnRewriteVisitor.java}}, ""relation B using the same uid"" is a correct behavior since they are not at the same level.  So I'm guessing the required fix would be in the LOUnion.","28/Nov/18 06:20;knoguchi;I can think of two different approaches.

\(i) Even for overlapping uids on different nested level, do not allow them and
 force IdentityColumn. This way, all uids will be unique.

(ii) Change LOUnion uidMapping logic from (output_uid, input_uid) lists to
 (output_uid, nested_uids).

Attaching a patch that tries (ii). If possible, I'd like to avoid \(i) which is already
 creating more uids to keep track.

Taking one relation as example,
{noformat}
B: (Name: LOForEach Schema: A#36:bag{#37:tuple(a1#9:int,a2#*10*:chararray,a3#*11*:int)},a2#*10*:chararray,a3#*11*:int)
{noformat}
Before the patch, input_uid
 36,9,10,11,10,11
were used for uidMapping.

After the patch, it'll use nested_uids,
 _36, _36_9, _36_10, _36_11, _10, _11

This way, there won't be any incorrect list lookup.

 [~daijy], would this approach work? 
","29/Nov/18 07:57;daijy;+1, sounds good to me.","29/Nov/18 17:29;knoguchi;Sorry, somehow my last patch didn't include a unit-test.  Uploaded {{pig-5370-v2.patch}} with a test.","30/Nov/18 18:09;knoguchi;bq. Sorry, somehow my last patch didn't include a unit-test. Uploaded pig-5370-v2.patch with a test.

[~daijy], sorry to bug you again on this.  Can you take a look at my v2 patch that added a unit test? ",30/Nov/18 20:58;daijy;+1,"30/Nov/18 21:12;knoguchi;Thanks for the review Daniel!!! 

 Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable PigStreamingDepend to load from current directory in newer Perl versions,PIG-5366,13193036,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,abstractdog,abstractdog,abstractdog,20/Oct/18 12:30,26/Oct/18 07:31,13/Mar/19 23:13,26/Oct/18 07:31,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"A perl related issue found while testing streaming. In newer perl versions (>5.26), current directory (""."") is not included in @INC, so PerlStreamingDepend may fail during ""use PigStreamingModule;"". A possible solution is to let this module add current directory for itself to make it more independent from the environment (current perl version).

Test case was:

{code}
define CMD `perl PigStreamingDepend.pl - sio_5_1 sio_5_2` input(stdin) output('sio_5_1', 'sio_5_2') ship('./libexec/PigStreamingDepend.pl', './libexec/PigStreamingModule.pm');
A = load '/user/hrt_qa/tests/data/singlefile/studenttab10k';
B = stream A through CMD;
store B into '/user/hrt_qa/out/hrtqa-1539851229-streaming.conf-StreamingIO/StreamingIO_5.out';
{code}",,,,,,,,,,,,,,,,,20/Oct/18 12:43;abstractdog;PIG-5366_1.patch;https://issues.apache.org/jira/secure/attachment/12944858/PIG-5366_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-10-20 21:03:12.867,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Fri Oct 26 07:31:59 UTC 2018,,,,,,,0|i3zg1z:,9223372036854775807,,,,,,,,,,20/Oct/18 12:32;abstractdog;Could I have this issue assigned to me and get contributor rights?,"20/Oct/18 21:03;daijy;[~abstractdog], assigned to you.",26/Oct/18 07:31;daijy;+1. Patch committed to trunk. Thanks [~abstractdog]!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Negative progress report by HBaseTableRecordReader,PIG-5355,13181976,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,satishsaley,satishsaley,satishsaley,29/Aug/18 19:35,27/Sep/18 18:16,13/Mar/19 23:13,27/Sep/18 18:16,,,,,,,,,,,,,,,,,0.17.1,,,,,0,,,,,,,,"The logic for padding the current row does not consider the updated padded row during the comparison. It ends up with different length then expected. This results in negative value for {{processed}}.

{code}
            byte[] lastPadded = currRow_;
            if (currRow_.length < endRow_.length) {
                lastPadded = Bytes.padTail(currRow_, endRow_.length - currRow_.length);
            }
            if (currRow_.length < startRow_.length) {
                lastPadded = Bytes.padTail(currRow_, startRow_.length - currRow_.length);
            }

            byte [] prependHeader = {1, 0};
            BigInteger bigLastRow = new BigInteger(Bytes.add(prependHeader, lastPadded));
            if (bigLastRow.compareTo(bigEnd_) > 0) {
                return progressSoFar_;
            }
            BigDecimal processed = new BigDecimal(bigLastRow.subtract(bigStart_));
{code}
The fix is to use {{lastPadded}} in the second {{if}} comparison and {{Bytes.padTail}} call inside that {{if}}

PIG-4700 added progress reporting. This enabled ProgressHelper in Tez. It calls {{getProgress}} [here |https://github.com/apache/tez/blob/master/tez-api/src/main/java/org/apache/tez/common/ProgressHelper.java#L50] on {{PigRecrodReader}} https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigRecordReader.java#L159 . Since Pig is reporting negative progress, job is getting killed by AM.
 

 ",,,,,,,,,,,,,PIG-4700,,,,04/Sep/18 21:13;satishsaley;PIG-5355-1.patch;https://issues.apache.org/jira/secure/attachment/12938361/PIG-5355-1.patch,07/Sep/18 21:25;satishsaley;PIG-5355-2.patch;https://issues.apache.org/jira/secure/attachment/12938901/PIG-5355-2.patch,10/Sep/18 17:12;satishsaley;PIG-5355-3.patch;https://issues.apache.org/jira/secure/attachment/12939125/PIG-5355-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2018-09-04 21:38:26.274,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Thu Sep 27 18:16:19 UTC 2018,,,,,,,0|i3xkbz:,9223372036854775807,,,,,,,,,,04/Sep/18 21:38;rohini;+1,"10/Sep/18 16:58;rohini;It should be 
{code}
if (currRow_.length < maxRowLength) {		
              lastPadded = Bytes.padTail(currRow_, maxRowLength - currRow_.length);		
 }
{code}
 in your first patch. Sorry I missed it. Thanks Koji for catching it",10/Sep/18 17:12;satishsaley;updated patch,"10/Sep/18 20:06;knoguchi;Thanks for the fix.  Looks good to me.  +1 

Outside of this jira, I still don't like the logic of {{HBaseTableInputFormat.getProgress()}}
{code:java}
if (bigLastRow.compareTo(bigEnd_) > 0) {
  return progressSoFar_;
}
{code}
which means when records have longer key length than {{max(startRow_.length,endRow_.length)}}, progress stays the same.","19/Sep/18 14:28;knoguchi;{quote}
Outside of this jira, I still don't like the logic of {{HBaseTableInputFormat.getProgress()}}
{code:java}
if (bigLastRow.compareTo(bigEnd_) > 0) {
  return progressSoFar_;
}
{code}
which means when records have longer key length than {{max(startRow_.length,endRow_.length)}}, progress stays the same.
{quote}
[~satishsaley], [~rohini], how about we truncate (by calling Bytes.head) when  
{{maxRowLength < currRow_.length}} ?

Or, I'm fine committing as is.  Most important of the patch is avoiding the negative progress report.","21/Sep/18 21:49;rohini;bq. Or, I'm fine committing as is.
 I think better to leave it at this. Logic seems to be taken from here - https://github.com/apache/hbase/blob/master/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java#L1846-L1867","27/Sep/18 18:16;knoguchi;Committed to 0.17 branch and trunk. 

Thanks Satish!!! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable progress reporting for Tasks in Tez,PIG-4700,12904938,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,satishsaley,rohini,rohini,14/Oct/15 17:35,29/Aug/18 19:38,13/Mar/19 23:13,05/Jun/17 20:56,,,,,,,,,,,,,,,,,0.18.0,,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,24/May/17 22:47;satishsaley;PIG-4700-1.patch;https://issues.apache.org/jira/secure/attachment/12869742/PIG-4700-1.patch,02/Jun/17 16:56;satishsaley;PIG-4700-2.patch;https://issues.apache.org/jira/secure/attachment/12871009/PIG-4700-2.patch,02/Jun/17 22:09;satishsaley;PIG-4700-3.patch;https://issues.apache.org/jira/secure/attachment/12871070/PIG-4700-3.patch,02/Jun/17 22:15;satishsaley;PIG-4700-4.patch;https://issues.apache.org/jira/secure/attachment/12871072/PIG-4700-4.patch,02/Jun/17 22:18;satishsaley;PIG-4700-5.patch;https://issues.apache.org/jira/secure/attachment/12871073/PIG-4700-5.patch,05/Jun/17 20:30;satishsaley;PIG-4700-6.patch;https://issues.apache.org/jira/secure/attachment/12871294/PIG-4700-6.patch,05/Jun/17 20:42;satishsaley;PIG-4700-7.patch;https://issues.apache.org/jira/secure/attachment/12871296/PIG-4700-7.patch,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Mon Jun 05 20:56:50 UTC 2017,,,,,,,0|i2n0mv:,9223372036854775807,,,,,,,,,,12/Jul/16 21:34;rohini;MRToTezHelper also wrongly translates mapreduce.task.timeout to tez.am.progress.stuck.interval-ms instead of  tez.task.progress.stuck.interval-ms which needs to be fixed for the Progress reporting timeout to be honored.,"02/Jun/17 21:12;rohini;We should also check for presence of ProgressHelper class (introduced in TEZ-3437) before translating the settings. Before 0.8.5, due to TEZ-3549 bug, the jobs will fail if the setting was set.",02/Jun/17 22:16;rohini;The comment //TezConfiguration.TEZ_TASK_PROGRESS_STUCK_INTERVAL_MS TEZ-808 in Tez 0.8 also needs to be moved,"05/Jun/17 16:40;rohini;bq. progressHelper.scheduleProgressTaskService(0, 100);
  Could you make it 1000? i.e schedule it every second.","05/Jun/17 17:39;rohini;bq. Could you make it 1000? i.e schedule it every second
  Actually it can be optimized better. Progress information is sent as part of task heartbeat. In setups like ours heartbeat interval is set to 3 seconds so that the AM can handle 10s of thousands of tasks. So we should set it to a value slightly lower than heartbeat interval.

{code}
progressHelper.scheduleProgressTaskService(100, Math.max(1000, (conf.getInt(TezConfiguration.TEZ_TASK_AM_HEARTBEAT_INTERVAL_MS, TezConfiguration.TEZ_TASK_AM_HEARTBEAT_INTERVAL_MS_DEFAULT) - 50)));
{code}",05/Jun/17 20:33;rohini;Patch is not applying cleanly. Can you fix it?,05/Jun/17 20:56;rohini;+1. Committed to trunk. Thanks for the fix [~satishsaley],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ava.lang.OutOfMemoryError: Java heap space,PIG-766,12422836,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,vzaliva,vzaliva,14/Apr/09 23:33,09/Aug/18 09:33,13/Mar/19 23:13,01/May/10 00:35,0.2.0,0.7.0,,,,,,,,,,,,,,,,,impl,,,1,,,,,,,,"My pig script always fails with the following error:

Java.lang.OutOfMemoryError: Java heap space
       at java.util.Arrays.copyOf(Arrays.java:2786)
       at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:94)
       at java.io.DataOutputStream.write(DataOutputStream.java:90)
       at java.io.FilterOutputStream.write(FilterOutputStream.java:80)
       at org.apache.pig.data.DataReaderWriter.writeDatum(DataReaderWriter.java:213)
       at org.apache.pig.data.DefaultTuple.write(DefaultTuple.java:291)
       at org.apache.pig.data.DefaultAbstractBag.write(DefaultAbstractBag.java:233)
       at org.apache.pig.data.DataReaderWriter.writeDatum(DataReaderWriter.java:162)
       at org.apache.pig.data.DefaultTuple.write(DefaultTuple.java:291)
       at org.apache.pig.impl.io.PigNullableWritable.write(PigNullableWritable.java:83)
       at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)
       at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)
       at org.apache.hadoop.mapred.IFile$Writer.append(IFile.java:156)
       at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.spillSingleRecord(MapTask.java:857)
       at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:467)
       at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:101)
       at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:219)
       at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:208)
       at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.map(PigMapReduce.java:86)
       at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
       at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2198)

","Hadoop-0.18.3 (cloudera RPMs).
mapred.child.java.opts=-Xmx1024m
",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-14 23:48:25.915,,,no_permission,,,,,,,,,,,,,164329,,,,Thu Aug 09 09:31:30 UTC 2018,,,,,,,0|i0go3j:,95334,,,,,,,,,,"14/Apr/09 23:48;alangates;Can you attach your script, and give an idea of the data sizes you are trying to join?  Of particular interest is the cardinality of the keys.

Also, if your inputs of very different sizes, you can try reducing the join order.  Pig always materializes the first input's keys in memory and streams the second.  If one table has many more instances of a given key than the other this can resolve the out of memory issue.","15/Apr/09 00:02;vzaliva;Data size is just 60Gb. I am already doing JOIN optimizations you mention.

Although I am not sure it is failing on JOIN.

","15/Apr/09 00:11;alangates;It isn't overall data size that matters.  It is the size of a given key.  So if you have a 2G data set up it has only one key (that is, every row has that key), then you'll hit this problem (assuming you can't fit 2G in memory on your data nodes).  Pig does try to spill to avoid this, but has a hard time knowing when and how much to spill, and thus often runs out of memory.

But I think you're right that this isn't in the join.  From the stack it looks like it's trying to write data out of the map task.  Do you have very large rows in this data?","15/Apr/09 00:25;vzaliva;I have at most 17m rows in my dataset.
At some point I am doing GROUP BY and longest row about 500,000 tuples.

",15/Apr/09 01:28;olgan;I asked a member of hadoop team to take a look. A possible problem is that there is a single record that does not fit into combiner buffer. Hopefully we will get some help with this.,15/Apr/09 01:37;olgan;I got confirmation from Hadoop dev that this is a case of one huge record that is larger than combiner buffer which means that it is over 90 MB. Does this sound right for your data? Is it possible you have data corruption? Do you have another data set to try this query with?,"15/Apr/09 01:48;vzaliva;I know for sure that I have at some point a record of 500K tuples, but tuples being 40-50 bytes each, it is far from 90M.

Even if this is the case, I do not see how this could cause OutOfMemory exception in java.util.Arrays.copyOf(). Even if this record
is, say, 200Mb, given JVM total heap memory size 1Gb, it could happen only if all of this memory is already used and does not have
200Mb left.

How can I increase combiner buffer size?  What are the possible remedies/workarounds for this problem?

","15/Apr/09 02:08;sms;You can specify the I/O sort buffer size on the command line as:

java -Dio.sort.mb=200 -cp pig.jar:/<path_to_hadoop_site.xml>

Reference: http://hadoop.apache.org/core/docs/current/hadoop-default.html","15/Apr/09 17:01;vzaliva;increasing sort buffer to 500Mb did not work for me.

since implementation of many basic algorithms (like counting number of records in relationship) in PIG requires using GROUP BY which could produce very long records (up to number of tuples in relationship), this is a very serious problem. Potentially record could exceed available Java heap memory. What are the strategies for overcoming this limitation? Does pig plan to address this?
","15/Apr/09 19:50;olgan;The issue is not about combiner buffer the issue is that you have a really large record (and the records in memory are much bigger than on disk). Hadoop realizes that the record is too large for combiner and just tries to write it to disk. As part of writing it to disk, more memory needs to be allocated and as the result the process runs out of memory. 

Is there a possibility to increase memory size for your processing?","15/Apr/09 20:51;vzaliva;I have increased it to 500Mb and it is still not enough. I see this as a more general problem, as at some point the memory I need to allocate for processing  big dataset will exceed all possible VM limits.

In other words I am concerned that size of dataset I can process is limited by memory I can allocate to Java VM. So the system have a fundamental scalability limit.

",15/Apr/09 22:46;olgan;I did not mean increasing combiner buffer size - I meant overall memory that is given to the process.,"16/Apr/09 23:09;vzaliva;I have 1Gb now, could not go any higher.
","01/May/10 00:35;olgan;Many memory changes went in. Please, reopen if this is still a problem.","25/May/10 07:59;dirksan;>Many memory changes went in. Please, reopen if this is still a problem. 

I found the problem described by Vadim still existing for the following configuration:

- Apache-Hadoop 0.20.2
- Pig 0.7.0 and also for  0.8.0-dev (18/may)","25/May/10 17:15;ashutoshc;Dirk,

1. Are you getting the exact same stack trace as mentioned in the jira?
2. Which operations are you doing in your query - join, group-by, any other ?
3. What load/store func are you using to read and write data? PigStorage or your own ?
4. What is your data size and memory available to your tasks?
5. Do you have very large records in your dataset, like hundreds of MB for one record ?

It would be great if you can paste here the script from which you get this exception.","26/May/10 12:34;dirksan;{quote}1. Are you getting the exact same stack trace as mentioned in the jira?{quote}
Yes the same and some similar traces:
{noformat}
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:2786)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:94)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:80)
	at org.apache.pig.data.DataReaderWriter.writeDatum(DataReaderWriter.java:279)
	at org.apache.pig.data.DefaultTuple.write(DefaultTuple.java:264)
	at org.apache.pig.data.DefaultAbstractBag.write(DefaultAbstractBag.java:249)
	at org.apache.pig.data.DataReaderWriter.writeDatum(DataReaderWriter.java:214)
	at org.apache.pig.data.DefaultTuple.write(DefaultTuple.java:264)
	at org.apache.pig.data.DataReaderWriter.writeDatum(DataReaderWriter.java:209)
	at org.apache.pig.data.DefaultTuple.write(DefaultTuple.java:264)
	at org.apache.pig.impl.io.PigNullableWritable.write(PigNullableWritable.java:123)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)
	at org.apache.hadoop.mapred.IFile$Writer.append(IFile.java:179)
	at org.apache.hadoop.mapred.Task$CombineOutputCollector.collect(Task.java:880)
	at org.apache.hadoop.mapred.Task$NewCombinerRunner$OutputConverter.write(Task.java:1201)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.processOnePackageOutput(PigCombiner.java:199)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:161)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:51)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1222)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.doInMemMerge(ReduceTask.java:2563)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.run(ReduceTask.java:2501)



java.lang.OutOfMemoryError: Java heap space
	at org.apache.pig.data.DefaultTuple.(DefaultTuple.java:58)
	at org.apache.pig.data.DefaultTupleFactory.newTuple(DefaultTupleFactory.java:35)
	at org.apache.pig.data.DataReaderWriter.bytesToTuple(DataReaderWriter.java:61)
	at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:142)
	at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:136)
	at org.apache.pig.data.DefaultAbstractBag.readFields(DefaultAbstractBag.java:263)
	at org.apache.pig.data.DataReaderWriter.bytesToBag(DataReaderWriter.java:71)
	at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:145)
	at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:136)
	at org.apache.pig.data.DataReaderWriter.bytesToTuple(DataReaderWriter.java:63)
	at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:142)
	at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:136)
	at org.apache.pig.data.DefaultTuple.readFields(DefaultTuple.java:284)
	at org.apache.pig.impl.io.PigNullableWritable.readFields(PigNullableWritable.java:114)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)
	at org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:116)
	at org.apache.hadoop.mapreduce.ReduceContext$ValueIterator.next(ReduceContext.java:163)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage.getNext(POCombinerPackage.java:155)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage.getNext(POMultiQueryPackage.java:242)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.processOnePackageOutput(PigCombiner.java:170)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:161)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:51)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1222)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.doInMemMerge(ReduceTask.java:2563)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.run(ReduceTask.java:2501)



java.lang.OutOfMemoryError: Java heap space
	at java.util.ArrayList.(ArrayList.java:112)
	at org.apache.pig.data.DefaultTuple.(DefaultTuple.java:58)
	at org.apache.pig.data.DefaultTupleFactory.newTuple(DefaultTupleFactory.java:35)
	at org.apache.pig.data.DataReaderWriter.bytesToTuple(DataReaderWriter.java:61)
	at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:142)
	at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:136)
	at org.apache.pig.data.DefaultAbstractBag.readFields(DefaultAbstractBag.java:263)
	at org.apache.pig.data.DataReaderWriter.bytesToBag(DataReaderWriter.java:71)
	at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:145)
	at org.apache.pig.data.DataReaderWriter.readDatum(DataReaderWriter.java:136)
	at org.apache.pig.data.DefaultTuple.readFields(DefaultTuple.java:284)
	at org.apache.pig.data.InternalCachedBag$CachedBagIterator.hasNext(InternalCachedBag.java:221)
	at org.apache.pig.builtin.Distinct.getDistinctFromNestedBags(Distinct.java:138)
	at org.apache.pig.builtin.Distinct.access$200(Distinct.java:40)
	at org.apache.pig.builtin.Distinct$Intermediate.exec(Distinct.java:103)
	at org.apache.pig.builtin.Distinct$Intermediate.exec(Distinct.java:96)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:209)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:250)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:341)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:289)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:276)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:259)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux.runPipeline(PODemux.java:217)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux.getNext(PODemux.java:207)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.processOnePackageOutput(PigCombiner.java:183)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:161)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine.reduce(PigCombiner.java:51)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1222)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.doInMemMerge(ReduceTask.java:2563)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.run(ReduceTask.java:2501)
{noformat}

{quote}
2. Which operations are you doing in your query - join, group-by, any other ?
3. What load/store func are you using to read and write data? PigStorage or your own ?
4. What is your data size and memory available to your tasks?
5. Do you have very large records in your dataset, like hundreds of MB for one record ?
It would be great if you can paste here the script from which you get this exception.
{quote}

As we started to test the transformation (see below) the OutOfMemory-Error first occured at input-datasets of about 150MB.
Increasing the Memory for the child-vms by setting {{mapred.child.java.opts}} to {{600m}} fixed it for a while.
When using larger input-dataset the problem reappears.

*Input-Data:*
A CSV-File, ~14GB Dataset, ~100,000,000 Records per Dataset, ~145 Byte per Record

*Example:*
{noformat} 
  USER_ID                       REQUEST_DATE    SESSION                                 COMPANY SERVICENAME  SECTION_1  SECTION_2  SECTION_3  SECTION_4  SECTION_5  SECTION_6     SECTION SECTION_NEW
  ac14263e-22082-2263455080-9   2010-03-02      ac14263e-22082-2263455080-9-1273015305  ABC     (NULL)       main       (NULL)     (NULL)     (NULL)     (NULL)     abc/main/mail /main/mail
  ...
  ...
{noformat} 

*The Pig-Script*
{code} 
A = LOAD 'full_load' USING PigStorage('\t');

B = FOREACH A GENERATE $4 AS servicename, $3 AS company, $2 AS session, $0 as user_id
                       , $5 AS section_1, $6 AS section_2, $7 AS section_3, $8 as section_4
                       , $9 as section_5, $10 as section_6, $11 AS section;
                        
/* 1st aggregation */
S0 = GROUP B BY (servicename, company);
S0_A = FOREACH S0 {
                    unique_clients = DISTINCT B.user_id;
                    visits = DISTINCT B.session;
                    GENERATE FLATTEN(group), COUNT(B) AS pi_count, COUNT(unique_clients) AS unique_clients_count, COUNT(visits) AS visit_count;
                  }
S0_B = FOREACH S0_A GENERATE servicename, company, '' as section_1, '' as section_2, '' as section_3, '' as section_4
                           , '' as section_5, '' as section_6, '' as section, pi_count, unique_clients_count
                           , visit_count, 0 as level;

/* 2nd aggregation */
S1 = GROUP B BY (servicename, company, section_1);
S1_A = FOREACH S1 {
                    unique_clients = DISTINCT B.user_id;
                    visits = DISTINCT B.session;
                    GENERATE FLATTEN(group), COUNT(B) AS pi_count, COUNT(unique_clients) AS unique_clients_count, COUNT(visits) AS visit_count;
                  }
S1_B = FOREACH S1_A GENERATE servicename, company, section_1, '' as section_2, '' as section_3, '' as section_4
                             , '' as section_5, '' as section_6, '' as section, pi_count, unique_clients_count
                             , visit_count, 1 as level;

/* 3rd - 7th aggregation may follow here */

/* build result*/
X = UNION S0_B, S1_B;
STORE X INTO 'result' USING PigStorage ('\t');
{code} ","09/Aug/18 09:31;bswaroo;I'm still facing this issue.

Apache Pig version 0.16.0

Hadoop 2.8.3

 

Can someone help me to understand what should I include to avoid this issue?",,,,,,,,,,,,,,,,,,,,,,,
Spark UT failures after merge from trunk,PIG-4822,12946489,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,pallavi.rao,pallavi.rao,03/Mar/16 09:41,18/Jun/18 08:42,13/Mar/19 23:13,18/Jun/18 08:42,,,,,,,,,,,,,,,,,spark-branch,,spark,,,0,spork,,,,,,,"New failures:
    [junit] Tests run: 26, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 106.46 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED
--
    [junit] Tests run: 14, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 55.261 sec
    [junit] Test org.apache.pig.test.TestMultiQuery FAILED
--
    [junit] Tests run: 31, Failures: 1, Errors: 0, Skipped: 1, Time elapsed: 76.905 sec
    [junit] Test org.apache.pig.test.TestPigRunner FAILED
--
    [junit] Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 43.832 sec
    [junit] Test org.apache.pig.test.TestPigServerLocal FAILED
--
    [junit] Tests run: 71, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 73.983 sec
    [junit] Test org.apache.pig.test.TestPruneColumn FAILED
--
    [junit] Tests run: 31, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 7.692 sec
    [junit] Test org.apache.pig.test.TestSchema FAILED
--
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] Test org.apache.pig.test.TestScriptLanguage FAILED
--
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] Test org.apache.pig.test.TestScriptLanguageJavaScript FAILED
--
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] Test org.apache.pig.test.TestScriptUDF FAILED
--
    [junit] Tests run: 0, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 6.21 sec
    [junit] Test org.apache.pig.test.TestSkewedJoin FAILED
--
    [junit] Tests run: 0, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 6.399 sec
    [junit] Test org.apache.pig.test.TestSplitStore FAILED
--
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] Test org.apache.pig.test.TestStore FAILED
--
    [junit] Tests run: 0, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 5.11 sec
    [junit] Test org.apache.pig.test.TestStoreInstances FAILED
--
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] Test org.apache.pig.test.TestStoreOld FAILED
--
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] Test org.apache.pig.test.TestStreaming FAILED
--
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] Test org.apache.pig.test.TestToolsPigServer FAILED
--
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
    [junit] Test org.apache.pig.test.TestUDF FAILED",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 07 05:39:07 UTC 2016,,,,,,,0|i2u37b:,9223372036854775807,,,,,,,,,,03/Mar/16 09:43;pallavi.rao;We might need to create sub-tasks for each of the failures (or a group of them) and investigate separately.,"03/Mar/16 12:33;pallavi.rao;After applying the patch in PIG-4823, the number of failures have reduced to 7:
{noformat}
    [junit] Tests run: 26, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 107.794 sec
    [junit] Test org.apache.pig.test.TestEvalPipeline FAILED
--
    [junit] Tests run: 14, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 55.294 sec
    [junit] Test org.apache.pig.test.TestMultiQuery FAILED
--
    [junit] Tests run: 31, Failures: 1, Errors: 0, Skipped: 1, Time elapsed: 79.324 sec
    [junit] Test org.apache.pig.test.TestPigRunner FAILED
--
    [junit] Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 43.865 sec
    [junit] Test org.apache.pig.test.TestPigServerLocal FAILED
--
    [junit] Tests run: 71, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 73.725 sec
    [junit] Test org.apache.pig.test.TestPruneColumn FAILED
{noformat}",07/Mar/16 05:39;pallavi.rao;https://builds.apache.org/job/Pig-spark/312/testReport/ - Failures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix failing e2e tests with spark exec type,PIG-5162,13047313,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,nkollar,nkollar,01/Mar/17 13:36,18/Jun/18 08:32,13/Mar/19 23:13,18/Jun/18 08:32,,,,,,,,,,,,,,,,,0.17.0,spark-branch,spark,,,0,,,,,,,,"Tests were executed on spark branch in spark mode, old Pig was also Pig on spark branch (same), but executed in mapreduce mode",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,2017-03-01 13:36:44.0,,,,,,,0|i3asgv:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error message from range projection completely misleading,PIG-5335,13151400,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,10/Apr/18 14:29,15/May/18 04:21,13/Mar/19 23:13,15/May/18 04:21,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"{code}
A = load 'input.txt' as (a0,a1,a2,a3);
B = FOREACH A GENERATE a0, a1, a2, a3;
store B into '/tmp/deleteme';

C = FOREACH A GENERATE a0, b1, a2, a3;
D = FOREACH C GENERATE a0..a2;
(end of script, no store, nothing)
{code}

Error message
{panel}
2018-04-10 10:22:33,360 \[main] ERROR org.apache.pig.PigServer - exception during parsing: Error during parsing. Invalid field projection. Projected field \[a0] does not exist in schema: a0:bytearray,a0:bytearray,a2:bytearray,a3:bytearray.
{panel}

At least two issues.
# Error should be about FOREACH for C referencing non-existing field 'b1'.  But the error message is saying something about 'a0'.
# Script itself is not using relation C and D at all.  It's confusing to see errors coming out of unused relations.
",,,,,,,,,,,,,,,,,26/Apr/18 04:34;knoguchi;pig-5335-v01.patch;https://issues.apache.org/jira/secure/attachment/12920730/pig-5335-v01.patch,11/May/18 18:29;knoguchi;pig-5335-v02.patch;https://issues.apache.org/jira/secure/attachment/12923077/pig-5335-v02.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2018-05-11 16:48:15.653,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Tue May 15 04:21:58 UTC 2018,,,,,,,0|i3sdh3:,9223372036854775807,,,,,,,,,,"10/Apr/18 14:30;knoguchi;Full trace.
{noformat}
2018-04-10 10:22:33,360 [main] ERROR org.apache.pig.PigServer - exception during parsing: Error during parsing. Invalid field projection. Projected field [a0] does not exist in schema: a0:bytearray,a0:bytearray,a2:bytearray,a3:bytearray.
Failed to parse: Pig script failed to parse: <file test.pig, line 6, column 4> pig script failed to validate: org.apache.pig.impl.plan.PlanValidationException: ERROR 1025: Invalid field projection. Projected field [a0] does not exist in schema: a0:bytearray,a0:bytearray,a2:bytearray,a3:bytearray.
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:199)
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1824)
        at org.apache.pig.PigServer$Graph.access$000(PigServer.java:1532)
        at org.apache.pig.PigServer.parseAndBuild(PigServer.java:461)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:486)
        at org.apache.pig.PigServer.executeBatch(PigServer.java:472)
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:172)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:235)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:206)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
        at org.apache.pig.Main.run(Main.java:630)
        at org.apache.pig.Main.main(Main.java:175)
Caused by: <file test.pig, line 6, column 4> pig script failed to validate: org.apache.pig.impl.plan.PlanValidationException: ERROR 1025: Invalid field projection. Projected field [a0] does not exist in schema: a0:bytearray,a0:bytearray,a2:bytearray,a3:bytearray.
        at org.apache.pig.parser.LogicalPlanBuilder.buildForeachOp(LogicalPlanBuilder.java:1066)
        at org.apache.pig.parser.LogicalPlanGenerator.foreach_clause(LogicalPlanGenerator.java:15896)
        at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1933)
        at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1102)
        at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:560)
        at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:421)
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:191)
        ... 11 more
Caused by: org.apache.pig.impl.plan.PlanValidationException: ERROR 1025: Invalid field projection. Projected field [a0] does not exist in schema: a0:bytearray,a0:bytearray,a2:bytearray,a3:bytearray.
        at org.apache.pig.newplan.logical.expression.ProjectExpression.findColNum(ProjectExpression.java:191)
        at org.apache.pig.newplan.logical.expression.ProjectExpression.setColumnNumberFromAlias(ProjectExpression.java:154)
        at org.apache.pig.newplan.logical.visitor.ProjectStarExpander.visit(ProjectStarExpander.java:344)
        at org.apache.pig.parser.LogicalPlanBuilder.buildForeachOp(LogicalPlanBuilder.java:1062)
        ... 17 more
{noformat}","10/Apr/18 14:56;knoguchi;bq. 2. Script itself is not using relation C and D at all. It's confusing to see errors coming out of unused relations.

This is probably due to checks being done inside {{LogicalPlanBuilder}} when the ForEach and others are  built.
Maybe a silly question but asking [~daijy]. Can {{ProjectStarExpander}} and {{ProjStarInUdfExpander}} be moved from LogicalPlanBuilder to LogicalPlan.validate() ?","26/Apr/18 04:11;knoguchi;{quote}
Maybe a silly question but asking Daniel Dai. Can ProjectStarExpander and ProjStarInUdfExpander be moved from LogicalPlanBuilder to LogicalPlan.validate() ?
{quote}
First tried this but saw that many tests from LOCube and others started failing since these LogicalOperator depended on having these range expanded at creation time.   I could have tried to move most of  LogicalPlanBuilder  steps to LogicalPlan.validate() but that would be too much work for the minor jira with just error message fixing.","26/Apr/18 04:29;knoguchi;Looking at why ProjectStarExpander was seeing a completely random schema with redundant a0
{panel}
Projected field \[a0] does not exist in schema: {color:red}a0:bytearray,a0:bytearray{color},a2:bytearray,a3:bytearray
{panel}

It was coming from LogicalPlanBuilder.buildForeachOp --> ProjectExpression.getFieldSchema where ProjectExpression was always picking the first field (column 0) when provided alias was not found.
This random behavior worked because later in LogicalPlan.validate()->ColumnAliasConversionVisitor, it did correctly identify the incorrect alias reference (""b1"").

However, for this jira, before reaching to LogicalPlan.validate(), it is failing within LogicalPlanBuilder's phase where ProjectStarExpander looked up for range ""a0..a2"", and LogicalSchema.getField(""a0"") didn't return due to 

{code:title=LogicalSchema.java}
 610          StringBuilder sb = new StringBuilder(""Found more than one match: "" + result.alias + "", "" + fs.alias);
 611          throw new FrontendException(sb.toString(), 1025);
{code}
and that exception was swallowed and got replaced with the misleading error message shown in the description.

","26/Apr/18 04:39;knoguchi;bq. However, for this jira, before reaching to LogicalPlan.validate(), it is failing within LogicalPlanBuilder's phase where ProjectStarExpander looked up

I tried moving ProjectStarExpander to later phase in  LogicalPlan.validate() but as I mentioned, some LogicalOperator depended on these projects range to be expanded at LogicalPlanBuilder time.

So, in {{pig-5335-v01.patch}}, focused on 

bq. ProjectExpression was always picking the first field (column 0) when provided alias was not found.

and tried returning a new fieldschema with the invalid lookup fieldname. I believe this allowed the compilation to move forward and get the right error message inside LogicalPlan.validate()->ColumnAliasConversionVisitor.

Running tests.",11/May/18 16:48;rohini;The patch is setting LogicalExpression.getNextUid() instead of leaving it at default -1 unlike other cases. Can you provide the explanation for that here and also upload a patch with a comment giving link to your jira explanation?,"11/May/18 18:29;knoguchi;Uploading a new patch with comment updated.
 Basically with uid left with -1, {{TestColumnAliasConversion.testInvalidNestedProjection}} started failing at LogicalPlanBuilder time.

Just to summarize.
 For these type of script errors, there are essentially two locations catching the errors.
 (check1) LogicalPlanBuilder.buildForeachOp
 (check2) LogicalPlan.validate

Check1 is done at script reading time and errors out irrespective of if the corresponding relation is used or not. 
Check2 is done much later and is only performed for relations that are part of Dump/Store. 

 For this jira, I've tried a couple of patterns.
 (a) Move ProjectStarExpander and ProjStarInUdfExpander from check1 to check2. 
 This didn't work due to other part of the code depended upon these visitors within check1. 
 (b) Throw exception when invalid field is referenced in ProjectExpression.java. 
 This moved bunch of negative tests that used to fail in check2 to check1. (Meaning, user may start to see their pig scripts failing due to pig compiler catching errors in unused relations).
(c) Create invalid field with uid=-1. 
This mostly worked but TestColumnAliasConversion.testInvalidNestedProjection 
{code: title=TestColumnAliasConversion.java}
        String query = ""A = load 'x' as (field);"" +
                       ""B = foreach A {"" +
                       ""  C = LIMIT invalidName 1;"" +
                       ""  generate C.foo;"" +
                       ""};"";
{code}
started to fail at check1 instead of check2 due to 
{noformat}
<line 1, column 28> pig script failed to validate: org.apache.pig.impl.plan.PlanValidationException: ERROR 2271: Logical plan invalid state: invalid uid -1 in schema : invalidName#-1:bytearray
	at org.apache.pig.parser.LogicalPlanBuilder.buildForeachOp(LogicalPlanBuilder.java:1066)
	at org.apache.pig.parser.LogicalPlanGenerator.foreach_clause(LogicalPlanGenerator.java:15896)
	at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1933)
	at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1102)
	at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:560)
	at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:421)
	at org.apache.pig.parser.ParserTestingUtils.generateLogicalPlan(ParserTestingUtils.java:76)
	at org.apache.pig.parser.TestColumnAliasConversion.validate(TestColumnAliasConversion.java:179)
	at org.apache.pig.parser.TestColumnAliasConversion.testInvalidNestedProjection(TestColumnAliasConversion.java:170)
Caused by: org.apache.pig.impl.plan.PlanValidationException: ERROR 2271: Logical plan invalid state: invalid uid -1 in schema : invalidName#-1:bytearray
	at org.apache.pig.newplan.logical.optimizer.SchemaResetter.validate(SchemaResetter.java:243)
	at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:195)
	at org.apache.pig.newplan.logical.relational.LOLimit.accept(LOLimit.java:79)
	at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
	at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:114)
	at org.apache.pig.parser.LogicalPlanBuilder.buildForeachOp(LogicalPlanBuilder.java:1064)
{noformat}

(d) Then I gave a newuid for this fake field so that the above testcase would again fail at check2.

(d) is my patch (both v1 and v2).


 ",14/May/18 19:13;rohini;+1,"15/May/18 04:21;knoguchi;Thanks for the review Rohini!
Committed to trunk. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoadCaster sometimes not set for complex type,PIG-5333,13150412,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,05/Apr/18 19:08,06/Apr/18 17:09,13/Mar/19 23:13,06/Apr/18 17:09,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"TypeCasting a bytearray field in the nested foreach below

{code}
A = load 'input.txt' as (a0:bytearray, a1:bytearray);
B = group A by (a0,a1);
C = FOREACH B GENERATE SIZE(A) as sizeofgroup, A;
SPLIT C into D if sizeofgroup > 2, Z OTHERWISE;

F = FOREACH D {
    F1 = FOREACH A generate (chararray) a0;
    GENERATE F1;
}

store F into '/tmp/output/F';
store Z into '/tmp/output/Z';
{code}

Fails with 
{noformat}
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF or Union from two different Loaders. Cannot determine how to convert the bytearray to string for [a0[7,29]]
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNextString(POCast.java:1126)
{noformat}",,,,,,,,,,,,,,,,,05/Apr/18 19:10;knoguchi;pig-5333-v01.patch;https://issues.apache.org/jira/secure/attachment/12917757/pig-5333-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-04-05 20:39:28.884,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Fri Apr 06 17:09:03 UTC 2018,,,,,,,0|i3s7dr:,9223372036854775807,,,,,,,,,,"05/Apr/18 19:14;knoguchi;Looking at {{LineageFindRelVisitor.java}}, we update the uid to funcSpec mapping by calling {{addUidLoadFuncToMap}} method.  After that, we sometimes update all the fields inside any complex type (bag/map/tuple) by calling {{setLoadFuncForUids}} right after but sometimes not.

I don't see why we don't want to make this true for all the addUidLoadFuncToMap calls.  Uploading pig-5333-v01.patch which does that.",05/Apr/18 20:39;rohini;+1,"06/Apr/18 17:09;knoguchi;Thanks for the review Rohini! 
Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
expressionOperator Divide.equalsZero(DataType.BIGDECIMAL) is invalid,PIG-5328,13131669,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,michaelthoward,michaelthoward,michaelthoward,17/Jan/18 19:21,27/Jan/18 04:18,13/Mar/19 23:13,27/Jan/18 04:18,0.16.0,0.17.0,,,,,,,,,,,,,,,0.18.0,,impl,,,0,,,,,,,,"Divide.equalsZero(DataType.BIGDECIMAL) is flawed in that it uses an invalid test for == ZERO in the case of BigDecimal. 

 

./physicalLayer/expressionOperators/Divide.java tests the divisor for zero in order to avoid DivideByZero.

The test is performed using a method equalsZero(...)

Divide.equalsZero() is given 'protected' access, but I could not find other references ... should be 'private'

equalsZero() implementation dispatches on dataType to type-specific predicates ... the BigDecimal implementation is incorrect 

The method BigDecimal.equals(other) is intended to be used for object equality, not numerical equality. (Their justification is that equals() is used in hash-table lookups in java Collections.) BigDecimal numbers are not normalized and scale is an important attribute. Scale is included in BigDecimal.equals(). The values ""0"" and ""0.00"" have different scales and are not considered ""equals()""

Comparisons for numeric equality need to be done using compareTo()

In the special case of comparing to zero, BigDecimal.signum() is the best. 

The current code is

{{     case DataType.BIGDECIMAL:}}

{{            return BigDecimal.ZERO.equals((BigDecimal) a);}}

needs to be changed to

{{     case DataType.BIGDECIMAL:}}

{{            return ((BigDecimal) a).signum() == 0;}}

 ",pig source HEAD as of Jan 2018 ... probably goes all the way back to initial implementation of BigDecimal support,"I don't contribute frequently, so most time was spent figuring out how to make the patch;25/Jan/18 17:21;michaelthoward;7200",900,0,7200,800%,900,0,7200,,,,,,,,24/Jan/18 19:04;michaelthoward;patchPig5328-take01.patch;https://issues.apache.org/jira/secure/attachment/12907541/patchPig5328-take01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-01-17 22:04:25.43,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Sat Jan 27 04:18:48 UTC 2018,,,,,,,0|i3p107:,9223372036854775807,"the division operator checks for a zero denominator to prevent a divide-by-zero exception. Unlike other numeric types, BigDecimal has multiple representations of zero with different scales ... 0, 0.0, 0.00, etc. Previous versions of pig were were not properly detecting zero values with non-zero scales when performing denominator check prior to division. ",,,,,,,,,"17/Jan/18 21:13;michaelthoward;Context/background:

BigDecimal is difficult to work with.

I did not see references in the pig documentation to how BigDecimal issues of precision, scale, and RoundingMode are handled for the pig 'bigdecimal' data type. So I specifically went looking at the Divide code to see how the pig implementation addressed these issues. I observed that the divide operation is performed with a min of 6 digits of scale to the right of the decimal point and RoundingMode.HALF_UP. (Arguably, RoundingMode.HALF_EVEN would have been the preferred choice.)

I stumbled on the equalsZero() code and got somewhat nervous when I saw that it used BigDecimal.equals(). The behavior of BigDecimal.equals() is pretty fundamental to the understanding and proper usage of the BigDecimal API. The fact that the equalsZero() code was written this way is a strong indication that the implementor had very little exposure/experience with the BigDecimal package. 

I am not familiar with the pig code, but I will try to help. ","17/Jan/18 21:23;michaelthoward;same problem ... use of of BigDecimal.ZERO.equals(BigDecimal) also exists in

expressionOperators/POCast.java - lines 471 & 1434

data/DataType.java - line 665

 

{{$ find . -name \*.java | xargs grep --line-number ZERO\.equals\( | grep -i decimal}}

{{./backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java:471:                res.result = Boolean.valueOf(!BigDecimal.ZERO.equals((BigDecimal)res.result));}}

{{./backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java:1434:                result = Boolean.valueOf(!BigDecimal.ZERO.equals((BigDecimal)obj));}}

{{./backend/hadoop/executionengine/physicalLayer/expressionOperators/Divide.java:113:            return BigDecimal.ZERO.equals((BigDecimal) a);}}

{{./data/DataType.java:665:                return Boolean.valueOf(!BigDecimal.ZERO.equals(((BigDecimal) o)));}}

{{$ }}","17/Jan/18 21:49;michaelthoward;References to RoundingMode in:

./builtin/FloatRoundTo.java

./builtIn/ROUND_TO.java

/builtin/DoubleRoundTo.java

all look OK to me.

 

[Although ideally one would not convert back to a (binary float) double  after generating a (decimal float) BigDecimal with specific scale.] ","17/Jan/18 22:04;knoguchi;Thanks for your feedback  [~michaelthoward]! 

Please let me know if you want to contribute here as a patch OR I can try to come up with a patch following your suggestion and you can tell me if I'm doing it right. ","18/Jan/18 01:41;michaelthoward;I have gone through all pig source files in the following:

{{$ pwd}}

{{[snip]/pig/src/org/apache/pig}}{{$ find . -name \*.java | xargs grep -l BigDecimal >foo}}

I did not identify any other issues with BigDecimal other than the .equals() problem. ","18/Jan/18 02:42;knoguchi;Thanks again Michael.  Assigning the jira to you.
Let us know if you need help in preparing a patch.","18/Jan/18 19:27;michaelthoward;> Let us know if you need help in preparing a patch.

I may well need some help ... will be my first time submitting a patch in many years. 

Will advise if I get stuck. ","24/Jan/18 19:03;michaelthoward;TestDivide unit test updated

BigDecimal.signum() == 0 now used to test for values numerically equal to zero. 

 ","26/Jan/18 02:51;knoguchi;Thanks Michael!  +1 on the patch.
Confirmed your testcase for divide-0 fails without your changes and works after the change.

Also, I see that {{TestPOCast.java}} is missing {{testBigDecimalToOther}} test method which would have been a good place to add the testing for typecasting BigDecimal to boolean.  I can create a new jira for that.

I'll commit your patch tomorrow.","27/Jan/18 04:18;knoguchi;Committed to trunk (0.18).  
Thanks [~michaelthoward] for your contribution! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestCubeOperator#testRollupBasic is flaky on Spark 2.2,PIG-5320,13124617,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,nkollar,nkollar,13/Dec/17 10:00,15/Jan/18 15:12,13/Mar/19 23:13,15/Jan/18 15:12,,,,,,,,,,,,,,,,,0.18.0,,spark,,,0,,,,,,,,"TestCubeOperator#testRollupBasic occasionally fails with
{code}
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable to store alias c
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1779)
	at org.apache.pig.PigServer.registerQuery(PigServer.java:708)
	at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:1110)
	at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:512)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)
	at org.apache.pig.PigServer.registerScript(PigServer.java:781)
	at org.apache.pig.PigServer.registerScript(PigServer.java:858)
	at org.apache.pig.PigServer.registerScript(PigServer.java:821)
	at org.apache.pig.test.Util.registerMultiLineQuery(Util.java:972)
	at org.apache.pig.test.TestCubeOperator.testRollupBasic(TestCubeOperator.java:124)
Caused by: org.apache.pig.impl.plan.VisitorException: ERROR 0: fail to get the rdds of this spark operator: 
	at org.apache.pig.backend.hadoop.executionengine.spark.JobGraphBuilder.visitSparkOp(JobGraphBuilder.java:115)
	at org.apache.pig.backend.hadoop.executionengine.spark.plan.SparkOperator.visit(SparkOperator.java:140)
	at org.apache.pig.backend.hadoop.executionengine.spark.plan.SparkOperator.visit(SparkOperator.java:37)
	at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:87)
	at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:46)
	at org.apache.pig.backend.hadoop.executionengine.spark.SparkLauncher.launchPig(SparkLauncher.java:237)
	at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:293)
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1475)
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1460)
	at org.apache.pig.PigServer.execute(PigServer.java:1449)
	at org.apache.pig.PigServer.access$500(PigServer.java:119)
	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1774)
Caused by: java.lang.RuntimeException: Unexpected job execution status RUNNING
	at org.apache.pig.tools.pigstats.spark.SparkStatsUtil.isJobSuccess(SparkStatsUtil.java:138)
	at org.apache.pig.tools.pigstats.spark.SparkPigStats.addJobStats(SparkPigStats.java:75)
	at org.apache.pig.tools.pigstats.spark.SparkStatsUtil.waitForJobAddStats(SparkStatsUtil.java:59)
	at org.apache.pig.backend.hadoop.executionengine.spark.JobGraphBuilder.sparkOperToRDD(JobGraphBuilder.java:225)
	at org.apache.pig.backend.hadoop.executionengine.spark.JobGraphBuilder.visitSparkOp(JobGraphBuilder.java:112)
{code}

I think the problem is that in JobStatisticCollector#waitForJobToEnd {{sparkListener.wait()}} is not inside a loop, like suggested in wait's javadoc:
{code}
     * As in the one argument version, interrupts and spurious wakeups are
     * possible, and this method should always be used in a loop:
{code}

Thus due to a spurious wakeup, the wait might pass without a notify getting called.",,,,,,,,,,,,,,,,,13/Dec/17 11:23;nkollar;PIG-5320_1.patch;https://issues.apache.org/jira/secure/attachment/12901882/PIG-5320_1.patch,14/Dec/17 15:44;nkollar;PIG-5320_2.patch;https://issues.apache.org/jira/secure/attachment/12902094/PIG-5320_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-12-14 03:52:58.192,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 15 15:12:17 UTC 2018,,,,,,,0|i3nuiv:,9223372036854775807,,,,,,,,,,"13/Dec/17 11:30;nkollar;I think this is a problem with Spark 1.6.x too, checking for the condition in a loop should solve the problem. I also changed the map and set implementation to sorted one, since we use integer job ids, I hope it would slightly improve performance in case of many jobs. [~kellyzly], [~szita] could you please have a look at my patch? My only concern is: is SparkListener#onJobEnd() called when the job fails? If not, then Pig would stuck in an infinite loop.","14/Dec/17 03:52;kellyzly;patch is ok
But why need to change from 
{code}
  private final Set<Integer> finishedJobIds = Sets.newHashSet();
 {code}
to
{code}
   private final Set<Integer> finishedJobIds = Sets.newTreeSet();
{code}

{quote} 
My only concern is: is SparkListener#onJobEnd() called when the job fails? If not, then Pig would stuck in an infinite loop.
{quote}
I think it will call SparkListener#onJobEnd() when job fails.  in the [link|https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-webui-JobProgressListener.html#onJobEnd], it is said that even job fails, onJobEnd will be called. Not read source code, not running cases to verify.","14/Dec/17 13:38;szita;[~nkollar] patch looks good, but can you elaborate on your reason for changing hash-based implementations to tree-based ones for the sets and maps used in this class? I would think that the number of jobs here would very rarely be high (if I think that most Pig jobs are started in batch mode with a script specified, so the only jobs here are what that one script generates)","14/Dec/17 15:48;nkollar;[~szita], [~kellyzly] the intention was to achieve some performance improvement, but I think I confused the intent of different set implementation, keeping the item (and keys) sorted wouldn't result in better performance here I guess. Reverted that change.","15/Jan/18 15:12;szita;+1 on [^PIG-5320_2.patch], and committed to trunk. Thanks Nandor!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check for DAG status before trying to kill,PIG-5327,13128962,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,06/Jan/18 00:56,09/Jan/18 13:26,13/Mar/19 23:13,09/Jan/18 13:26,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"The killJob is invokedby
      - grunt kill command
      - thread interrupt in Main (mostly in case of Oozie)
      - HangingJobKiller during shutdown

 killJob tries to kill the DAG again even if it was completed. It closes the session but does not remove it from the TezSessionManager session pool. Though both of these are minor issues and have no adverse impact (trying to kill a done DAG would be ignored by Tez AM and TezSessionManager would later check status of the session and remove it from the pool) the fix would avoid the two unnecessary calls to AM.",,,,,,,,,,,,,,,,,06/Jan/18 00:58;rohini;PIG-5327-1.patch;https://issues.apache.org/jira/secure/attachment/12904890/PIG-5327-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-01-08 21:38:30.715,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Tue Jan 09 13:26:09 UTC 2018,,,,,,,0|i3ol7j:,9223372036854775807,,,,,,,,,,"08/Jan/18 21:38;knoguchi;Looks good to me.  +1

But can you share some background on why you wanted this change? ","09/Jan/18 13:26;rohini;bq. But can you share some background on why you wanted this change?
  Updated the description.

Committed to trunk. Thanks for the review Koji.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema disambiguation can't be turned off for nested schemas,PIG-5325,13128249,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,szita,szita,03/Jan/18 14:39,09/Jan/18 10:07,13/Mar/19 23:13,09/Jan/18 10:07,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"PIG-5110 introduced the feature to turn off automatic schema field alias disambiguation, removing parent alias and the ':' char. It seems like this doesn't work for nested schemas.",,,,,,,,,,,,,,,,,03/Jan/18 16:26;szita;PIG-5325.0.patch;https://issues.apache.org/jira/secure/attachment/12904426/PIG-5325.0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-01-08 21:18:13.006,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 09 10:07:41 UTC 2018,,,,,,,0|i3ogtr:,9223372036854775807,,,,,,,,,,"03/Jan/18 16:27;szita;Attached [^PIG-5325.0.patch] for fixing this. [~mikebush], [~rohini] can you take a look please?",08/Jan/18 21:18;rohini;+1,"09/Jan/18 10:07;szita;Patch committed to trunk, thanks for the review Rohini!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POReservoirSample fails for more than Integer.MAX_VALUE records,PIG-5311,13112680,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,27/Oct/17 21:00,31/Dec/17 01:31,13/Mar/19 23:13,31/Dec/17 01:31,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"https://github.com/apache/pig/blob/branch-0.17/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POReservoirSample.java#L128

The rowProcessed is a int. When it exceeds the int range it wraps around and becomes a negative number throwing below exception. It needs to be changed to long.

{code}
Caused by: java.lang.IllegalArgumentException: bound must be positive
  at java.util.Random.nextInt(Random.java:388)
  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POReservoirSample.getNextTuple(POReservoirSample.java:128)
  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:305)
  at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:284)
{code}",,,,,,,,,,,,,,,,,21/Dec/17 23:12;rohini;PIG-5311-1.patch;https://issues.apache.org/jira/secure/attachment/12903328/PIG-5311-1.patch,27/Dec/17 21:54;rohini;PIG-5311-2.patch;https://issues.apache.org/jira/secure/attachment/12903837/PIG-5311-2.patch,28/Dec/17 20:18;rohini;PIG-5311-3.patch;https://issues.apache.org/jira/secure/attachment/12903937/PIG-5311-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-12-27 21:09:15.912,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Sun Dec 31 01:31:23 UTC 2017,,,,,,,0|i3lt3b:,9223372036854775807,,,,,,,,,,"21/Dec/17 23:14;rohini; Keeping it as long, required typecasting at multiple places (array index and argument to random.nextInt method). So used both int and a long variable for the fix to avoid that.","27/Dec/17 21:09;knoguchi;First time looking at sampling.  
I need to test it myself but two questions.

\(i) Unrelated to this patch, but 
{code:java}
  118         if (res == null || res.returnStatus != POStatus.STATUS_EOP) {
  119             Random randGen = new Random();
{code}
Do we need to set the seed like we did in PIG-4819 ?

(ii) 
{code:java}
  182     private Result retrieveSample() throws ExecException {
  183         if(nextSampleIdx < Math.min(rowProcessed, samples.length)){
...
  193         }
  194         else{
  195             samples = null; // Free memory
  196             return RESULT_EOP;
  197         }
{code}
When rowProcessed is wrapped around to a negative number, wouldn't this always return null ?
",27/Dec/17 21:54;rohini;Good catch. Fixed both.,"28/Dec/17 16:07;knoguchi;Thanks Rohini.   
For readability, I'm not sure if having 
-- both rowProcessed and rowProcessedLong 
is better than 
-- having a single count and typecasting at multiple places.  
I think latter is better but no strong preference. 

Outside of that, found one critical issue.
Learning the ""Reservoir sampling"" from 
https://en.wikipedia.org/wiki/Reservoir_sampling
and looking at the following change,
{code:java}
-                int rand = randGen.nextInt(rowProcessed + 1);
+                int rand = randGen.nextInt(getPositiveVal(rowProcessed));
                 if (rand < numSamples) {
                     samples[rand] = res;
                 }
{code}
this breaks the reservoir sampling algorithm (when count is larger than Integer.MAX_VALUE).  
Random number generated here has to be from 0...rowProcessedLong in order to preserve the feature of this algorithm: each item is kept with probability of {{numSamples / total}}. ","28/Dec/17 20:26;rohini;bq. both rowProcessed and rowProcessedLong
  Was just the recent influence from writing bytecode :)

bq. this breaks the reservoir sampling algorithm
  Switched form Random to RandomDataGenerator from commons-math3 (http://commons.apache.org/proper/commons-math/javadocs/api-3.6.1/index.html). Thanks for catching this and not letting me take a shortcut saying this is a very rare case.

Other things that [~knoguchi] suggested in our offline discussion were
1) https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadLocalRandom.html
Discarded this as it had nextLong but no option to provide seed.
2) https://stackoverflow.com/questions/2546078/java-random-long-number-in-0-x-n-range
RandomDataGenerator and extending Random with nextLong() were two of the solutions I tried from this. The nextLong() produced some negative numbers. Did not spend time debugging that. Tried RandomDataGenerator with some ranges comparing it to Random. More or less similar. So went with that.

{code}
long taskIdHashCode = ""task_1509095573435_5386881_1_04_000462"".hashCode();
        long randomSeed = ((long)taskIdHashCode << 32) | (taskIdHashCode & 0xffffffffL);
        Random rand = new Random(randomSeed);
        BufferedWriter bo = new BufferedWriter(new FileWriter(""/tmp/int""));
        for( int i = 1 ; i < Integer.MAX_VALUE; i++) {
            long val = rand.nextInt(i);
            if (val < 100) {
                bo.write(i + "" = "" + val + ""\n"");
                System.out.println(i + "" = "" + val);
            }
            if (i % 10000 == 0) {
                bo.flush();
            }
        }
        bo.close();

        RandomDataGenerator randGen = new RandomDataGenerator();
        randGen.reSeed(randomSeed);
        BufferedWriter bo1 = new BufferedWriter(new FileWriter(""/tmp/long""));
        for( long i = 1 ; i < 100000000000L; i++) { //100 Billion
            long val = randGen.nextLong(0, i);
            if (val < 100) {
                bo1.write(i + "" = "" + val + ""\n"");
                System.out.println(i + "" = "" + val);
            }
            if (i % 10000 == 0) {
                bo.flush();
            }
        }
        bo1.close();
        System.out.println(""Done"");
{code} ","29/Dec/17 02:24;knoguchi;{{PIG-5311-3.patch}} looks good. +1
Thanks for being patient with me. ",31/Dec/17 01:31;rohini;Committed to trunk. Thanks for the very thorough review Koji.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConstantCalculator optimizer is not applied for split,PIG-5322,13126652,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rohini,rohini,rohini,21/Dec/17 21:34,28/Dec/17 03:48,13/Mar/19 23:13,27/Dec/17 22:02,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"SPLIT A INTO B IF x == -2L, C OTHERWISE;

generates 
{code}
|       |       |---PONegative[long] - scope-15
|       |           |
|       |           |---Constant(2) - scope-14
{code}

instead of just
{code}
 |---Constant(-2) - scope-24
{code}

",,,,,,,,,,,,,,,,,21/Dec/17 23:15;rohini;PIG-5322-1.patch;https://issues.apache.org/jira/secure/attachment/12903329/PIG-5322-1.patch,26/Dec/17 22:51;rohini;PIG-5322-2.patch;https://issues.apache.org/jira/secure/attachment/12903740/PIG-5322-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-12-27 21:25:17.045,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Thu Dec 28 03:48:15 UTC 2017,,,,,,,0|i3o707:,9223372036854775807,,,,,,,,,,27/Dec/17 21:25;knoguchi;+1,27/Dec/17 22:02;rohini;Committed to trunk. Thanks Koji for the review.,28/Dec/17 03:48;knoguchi;{{SplitConstantCalculator.java}} was missed in the last commit.  Just added.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ToDate(userstring, format, timezone) computes DateTime with strange handling of Daylight Saving Time with location based timezones",PIG-3864,12706415,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,fredericschmaljohann,fredericschmaljohann,03/Apr/14 13:41,27/Dec/17 19:11,13/Mar/19 23:13,27/Dec/17 19:11,0.11.1,0.12.0,,,,,,,,,,,,,,,0.18.0,,,,,1,,,,,,,,"When using ToDate with a location based timezone (e.g. ""Europe/Berlin"") the handling of the timezone offset is based on whether the timezone is currently in daylight saving and not based on whether the timestamp is in daylight saving time or not.

Example:
{noformat}
B = FOREACH A GENERATE ToDate('2014-02-02 18:00:00.000Z', 'yyyy-MM-dd HH:mm:ss.SSSZ', 'Europe/Berlin') AS Timestamp;
{noformat}

This yields 
{noformat}2014-02-02 20:00:00.000+02{noformat}
when called during daylight saving in Europe/Berlin although I would expect 
{noformat}2014-02-02 19:00:00.000+01{noformat}
During standard time In Europe/Berlin, the above call yields 
{noformat}2014-02-02 19:00:00.000+01{noformat}

In Europe/Berlin DST started on March 30th, 2014.

This seems pretty strange to me. If it is on purpose it should at least be noted in the documentation.",,,,,,,,,,,,,,,,,26/May/15 23:41;daijy;PIG-3864-1.patch;https://issues.apache.org/jira/secure/attachment/12735471/PIG-3864-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-05-26 09:56:33.479,,,no_permission,,,,,,,,,,,,,384738,Reviewed,,,Wed Dec 27 19:11:31 UTC 2017,,,,,,,0|i1ua4n:,385005,,,,,,,,,,26/May/15 09:56;eirikora;I agree. The timezone adjustment should be set according to the date read and not according to current date (time of reading in the file).,26/May/15 23:41;daijy;This should be a bug. Attach patch.,"26/Sep/17 19:32;rohini;[~szita],
   Can you review this one? You would be more familiar with the different forms of specifying the timezone after PIG-4748.","26/Sep/17 22:27;eirikora;I'm not in a situation where I can test this now (not on that project with Pig
installed), but I read the patch and it seems good as also now the unit test
handles the case I had described. Thanks.  





On Tue, Sep 26, 2017 9:33 PM, Rohini Palaniswamy (JIRA) jira@apache.org  wrote:



  [
https://issues.apache.org/jira/browse/PIG-3864?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16181402#comment-16181402
] 




Rohini Palaniswamy commented on PIG-3864:

-----------------------------------------




[~szita],

  Can you review this one? You would be more familiar with the different forms
of specifying the timezone after PIG-4748.




of Daylight Saving Time with location based timezones

----------------------------------------------------------------------------------------------------------------------------------














handling of the timezone offset is based on whether the timezone is currently in
daylight saving and not based on whether the timestamp is in daylight saving
time or not.



HH:mm:ss.SSSZ', 'Europe/Berlin') AS Timestamp;









noted in the documentation.










--

This message was sent by Atlassian JIRA

(v6.4.14#64029)
","03/Oct/17 11:39;szita;+1 on [^PIG-3864-1.patch], it is a good fix [~daijy]",27/Dec/17 19:11;rohini;+1. Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hashCode for Bag needs to be order independent ,PIG-5300,13101750,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,12/Sep/17 19:47,21/Dec/17 21:40,13/Mar/19 23:13,21/Dec/17 21:40,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"This is a follow up from PIG-5041 where [~daijy] and others discussed about 

{quote}
Anyway, the bag as the key is a different issue. Need to address separately. Maybe we can write a hashcode which is order independent.
{quote}",,,,,,,,,,,,,,,,,13/Sep/17 17:17;knoguchi;pig-5300-v01.patch;https://issues.apache.org/jira/secure/attachment/12886914/pig-5300-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-09-15 21:00:07.355,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Thu Dec 21 21:40:29 UTC 2017,,,,,,,0|i3jzfr:,9223372036854775807,,,,,,,,,,"13/Sep/17 17:31;knoguchi;Attaching my first try, {{pig-5300-v01.patch}}.

This change will update 
{noformat}
 DefaultDataBag
 DistinctDataBag
 InternalCachedBag
 InternalDistinctBag
 InternalSortedBag
 SortedDataBag
{noformat}
which extends from {{DefaultAbstractBag}} and also 
{{SingleTupleBag}} and {{NonSpillableDataBag}} which have their own implementation of DataBag.

NOT touching {{AccumulativeBag}}, {{LimitedSortedDataBag}} and {{ReadOnceBag}} in the belief that they will not be used in hashcode/equals/compareTo.  At the same time, I didn't have the courage to throw Exceptions from them.

Also added hashcode method to DataBag interface just for documentation purposes. (Just like Map interface in Java.)  Also added null check for ""equals"" calls since java api documents that they should return false (and not throw NPE).
","15/Sep/17 17:51;knoguchi;Tests passed.  Making it patch available. 

[~daijy], appreciate if you could take a look when you have time.","15/Sep/17 21:00;rohini;+1.  [~daijy], can you please take a look as well?","24/Oct/17 19:08;knoguchi;bq. +1. Daniel Dai, can you please take a look as well?

Pinging [~daijy].","21/Dec/17 21:40;knoguchi;Committed to trunk.  Thanks for the review Rohini!!! 

Daniel, still appreciate your feedback when you get some time next year.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test failures on Pig on Spark with Spark 2.2,PIG-5318,13121331,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,nkollar,nkollar,28/Nov/17 14:42,13/Dec/17 10:34,13/Mar/19 23:13,13/Dec/17 10:34,,,,,,,,,,,,,,,,,0.18.0,,spark,,,0,,,,,,,,"There are several failing cases when executing the unit tests with Spark 2.2:
{code}
 org.apache.pig.test.TestAssert#testNegativeWithoutFetch
 org.apache.pig.test.TestAssert#testNegative
 org.apache.pig.test.TestEvalPipeline2#testNonStandardDataWithoutFetch
 org.apache.pig.test.TestScalarAliases#testScalarErrMultipleRowsInInput
 org.apache.pig.test.TestStore#testCleanupOnFailureMultiStore
 org.apache.pig.test.TestStoreInstances#testBackendStoreCommunication
 org.apache.pig.test.TestStoreLocal#testCleanupOnFailureMultiStore
{code}

All of these are related to fixes/changes in Spark.

TestAssert, TestScalarAliases and TestEvalPipeline2 failures could be fixed by asserting on the message of the exception's root cause, looks like on Spark 2.2 the exception is wrapped into an additional layer.
TestStore and TestStoreLocal failure are also a test related problems: looks like SPARK-7953 is fixed in Spark 2.2
The root cause of TestStoreInstances is yet to be found out.",,,,,,,,,,,,,,,,,29/Nov/17 15:51;nkollar;PIG-5318_1.patch;https://issues.apache.org/jira/secure/attachment/12899839/PIG-5318_1.patch,01/Dec/17 12:38;nkollar;PIG-5318_2.patch;https://issues.apache.org/jira/secure/attachment/12900212/PIG-5318_2.patch,04/Dec/17 10:55;nkollar;PIG-5318_3.patch;https://issues.apache.org/jira/secure/attachment/12900467/PIG-5318_3.patch,04/Dec/17 13:05;nkollar;PIG-5318_4.patch;https://issues.apache.org/jira/secure/attachment/12900486/PIG-5318_4.patch,07/Dec/17 12:52;nkollar;PIG-5318_5.patch;https://issues.apache.org/jira/secure/attachment/12901062/PIG-5318_5.patch,08/Dec/17 10:30;nkollar;PIG-5318_6.patch;https://issues.apache.org/jira/secure/attachment/12901229/PIG-5318_6.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2017-11-29 18:05:00.049,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 13 10:33:52 UTC 2017,,,,,,,0|i3nad3:,9223372036854775807,,,,,,,,,,"29/Nov/17 15:55;nkollar;[~szita], [~kellyzly] could you please have review?","29/Nov/17 18:05;rohini;Few Comments:
  1) TestStoreBase 
    - Please add a isSpark2_x() method to Util.java after isHadoop1_x() and use that
   - mode.toString().startsWith(""SPARK"") -> Util.isSparkExecType(mode)
  2) Changing test case of TestStoreInstances beats the purpose of the test.
","30/Nov/17 01:45;kellyzly;[~nkollar]:
thanks for working on it.
thanks [~rohini] 's comments.
 just quick scan. several questions
1. the modification for {{TestAssert}},{{TestEvalPipeline}},{{TestScalarAliases}} suit for Pig on MR or Pig on Tez? I guess it will not hurt other engine, just want to confirm with it.
2. not very understand  the purpose about the modification of TestStoreInstances, are there some problems with previous code?
 before
 {code}
 private ArrayList<Tuple> outRows;
 {code}
 
 After
 {code}
  private static Map<String, ArrayList<Tuple>> outRows = new HashMap<>();
 private String location;
 {code}","30/Nov/17 10:38;nkollar;Thanks [~rohini] and [~kellyzly] for your review!
Hm, I think I understood the point of TestStoreInstances, and indeed, my change on that test looks pointless. I'm afraid this might be a bug and not a test issue. I'll continue the investigation why it is failing, and what how to fix it, so far it looks like commitTask is not called on the correct OutputCommitterTestInstances instance, the array is empty.","01/Dec/17 13:24;nkollar;Attached PIG-5318_2.patch, I addressed Rohini's comments there.

As of {{TestStoreInstances}} failure, it looks like Spark (unlike Tez and MapReduce) creates multiple instances from {{PigOutputFormat}} while setting up the output committers: [setupCommitter|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala#L74] is called from both [setupJob|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala#L138] and from [setupTask|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala#L165], and {{setupCommitter}} creates a new {{PigOutputFormat}} each time, saving in a private variable. In addition, when Spark writes to files, a new {{PigOutputFormat}} is [getting created|https://github.com/apache/spark/blob/branch-2.2/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopMapReduceWriter.scala#L75] too, and since POStores are saved and deserialized in configuration, but StoreFuncInterface inside stores are [transient|https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStore.java#L53], a new instance of {{STFuncCheckInstances}} is getting created, each time, thus {{putNext}} and {{commitTask}} will use different array instances. Not sure if it is a bug in Pig, or in Spark, should Spark consistently use the same OutputFormat instance in this case?

Making {{reduceStores}}, {{mapStores}}, {{currentConf}} static inside {{TestStoreInstances}} would solve the problem, [~rohini], [~kellyzly] what do you think about this solution?","01/Dec/17 18:49;rohini;You should just do isSpark2_x (sparkVersion.startsWith(""2."")) instead of isSpark2_2_x . If Spark 2.3 gets released, then code will have to change. 

bq. Not sure if it is a bug in Pig, or in Spark, should Spark consistently use the same OutputFormat instance in this case?
  Spark should consistently use the same OutputFormat instance in this case. We should not be modifying the test case. There will be users who will be using local variables in StoreFunc for some computation at least.","04/Dec/17 09:58;nkollar;bq. You should just do isSpark2_x (sparkVersion.startsWith(""2."")) instead of isSpark2_2_x . If Spark 2.3 gets released, then code will have to change.

You're right, but matching for 2.x is not good enough. On Spark 2.1, abortTask and abortJob is not called (see SPARK-7953), but looks like in Spark 2.2 this is fixed (at least it looks like it is fixed). I'll update the patch soon, we should match Spark 2.2+.

bq. Spark should consistently use the same OutputFormat instance in this case

Ok, so I guess this should be a new Jira for Spark, however Spark 2.2 is already released, and creates more OutputFormat instances like said before. Indeed, we shouldn't modify the test case, but how about modifying PigOutputformat, like I did in the patch (making the relevant variables static)?","04/Dec/17 13:07;nkollar;Attached PIG-5318_4.patch, it looks like the way I wanted to tell Spark version doesn't work on Spark 1.x, using SparkContext#version instead.","04/Dec/17 17:25;rohini;bq. but how about modifying PigOutputformat, like I did in the patch (making the relevant variables static)?
 This cannot be done. It is hacky and will break Pig local mode and Tez. In local mode, save jvm is used to execute the whole script which can have parallel STORE statements. Tez also allows storing to multiple outputs from same vertex in a DAG - i.e multiple PigOutputFormat in the save jvm.

bq. isSpark2_1_minus
  Can you make it  isSpark2_2_plus which is slightly more intuitive than 2_1_minus. Also instantiating SparkContext just to get version seems overkill. Prefer the previous logic you had. Is there any reason that could not be used?
","04/Dec/17 17:26;rohini;bq. it looks like the way I wanted to tell Spark version doesn't work on Spark 1.x
  Missed this earlier. If the spark-version-info.properties file is not there, you could just return false for isSpark2_2_plus which will be easier.","06/Dec/17 16:26;nkollar;[~rohini] I agree, this looks like a hacky solution, but I couldn't think of a better one, I'll try to figure out something better.
bq. It is hacky and will break Pig local mode and Tez.
Are you saying, that if I add a test case which stores in two different stores, similar to the existing one in TestStoreInstances but with two STORE statements, then that should pass too? I tried this:
{code}
    @Test
    public void testBackendMultiStoreCommunication() throws Exception {
        ExecType[] execTypes = { Util.getLocalTestMode() };
        PigServer pig = null;
        for(ExecType execType : execTypes){
            Util.resetStateForExecModeSwitch();
            System.err.println(""Starting test mode "" + execType);
            if (execType == cluster.getExecType()) {
                pig = new PigServer(cluster.getExecType(),
                        cluster.getProperties());
            } else {
                pig = new PigServer(execType);
            }
            final String outFile = ""TestStoreInst1"";
            final String outFile2 = ""TestStoreInst2"";
            Util.deleteFile(pig.getPigContext(), outFile);
            Util.deleteFile(pig.getPigContext(), outFile2);
            pig.setBatchOn();
            String query =
                    ""  l1 = load '"" + INP_FILE_2NUMS + ""' as (i : int, j : int);"" +
                            "" store l1 into '"" + outFile + ""' using "" + CHECK_INSTANCE_STORE_FUNC +
                            "";"" +
                            "" store l1 into '"" + outFile2 + ""' using "" + CHECK_INSTANCE_STORE_FUNC +
                            "";"";
            Util.registerMultiLineQuery(pig, query);
            List<ExecJob> execJobs = pig.executeBatch();
            assertEquals(""num jobs"", 2, execJobs.size());
            assertEquals(""status "", JOB_STATUS.COMPLETED, execJobs.get(0).getStatus());
        }
    }
{code}

But it failed in local mode.

bq. Can you make it isSpark2_2_plus which is slightly more intuitive than 2_1_minus. Also instantiating SparkContext just to get version seems overkill.
Ok, I'll revert to the original option, and not instantiate SparkContext (agree, it is an overkill, that's why I tried an other solution first), though I think that's the most reliable way to tell the Spark version, leaving the details for Spark code. Checking for {{spark-version-info.properties}} looks very version dependent solution.

Should we open a separate Jira for fixing TestStoreInstances in spark mode? It seems that the other items are straightforward, and more test related issues, but this one seems to be something to fix in code instead of in test.","06/Dec/17 18:36;rohini;bq. Should we open a separate Jira for fixing TestStoreInstances in spark mode?
 Sure. It will require more time for you to come up with a solution. We can get the other ones fixed in this jira.",07/Dec/17 11:01;nkollar;Created PIG-5319 to address TestStoreInstances failure. ,"07/Dec/17 13:04;nkollar;Attached PIG-5318_5.patch which includes fix for TestAssert, TestScalarAliases, TestEvalPipeline2, TestStore and TestStoreLocal test cases, but doesn't fix TestStoreInstances failure. The Spark version is determined like Rohini suggested. I also noticed, that testKeepGoigFailed (fixed the typo in method name, now testKeepGoingFailed) was excluded from spark exec type, I enabled this test case, since it passed in my environment with 1.6, 2.1 and 2.2 Spark versions. [~kellyzly] do you remember why this was excluded? Looks like the Jira it is referring to is not yet fixed, despite this the test passes with 1.6.x Spark.","08/Dec/17 03:56;kellyzly;[~nkollar]:  testKeepGoingFailed is excluded because of SPARK-7953. At that time we used spark 1.3.  And after upgrading to 1.6, not enable this test again.","08/Dec/17 09:27;nkollar;[~kellyzly] thanks for the explanation, in this case I think enabling this test is fine, and there's no need to check for Spark version, we don't support older Spark versions.","08/Dec/17 10:33;nkollar;Attached PIG-5318_6.patch, found an universal way to tell the current Spark version, that works with both Spark 1.6.x and Spark 2.x too, and there's no need to start SparkContext. (thanks [~gezapeti] :) )","09/Dec/17 00:26;rohini;+1 on the patch from my side.

bq. found an universal way to tell the current Spark version
   Did not suggest that as [~gezapeti] has mentioned earlier that it is internal to Spark - https://issues.apache.org/jira/browse/OOZIE-2606?focusedCommentId=15528793&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15528793. It is ok here though as it is only for tests.

bq. testKeepGoingFailed is excluded because of SPARK-7953
  SPARK-7953 still does not seem to be fixed as you mentioned. Can you try to find which jira actually fixed it and probably close SPARK-7953 if it is not required anymore. Identifying what behavior change caused this might also help find other places in Pig on Spark that have to be fixed or changed for the new behavior.

","11/Dec/17 09:00;nkollar;[~rohini] indeed, it is internal, but I think checking for existence of {{spark-version-info.properties}} and the property value inside it is also Spark's internal. The public API is {{SparkContext#version()}}, but that means we should create a SparkContext just for telling the version, and we agreed that it is overkill.

As for SPARK-7953, I think SPARK-18219 fixed this problem too. It looks like in Spark 2.2 they completely refactored the task and job commit protocol: they [moved|https://github.com/apache/spark/commit/937af592e65f4dd878aafcabf8fe2cfe7fa3d9b3#diff-d97cfb5711116287a7655f32cd5675cb] the Spark SQL implementation to the Spark code module. I asked on the mentioned Jira if it is still an outstanding problem or not (maybe I missed some detail), I'd like to give some time for answers before I close it.","13/Dec/17 10:33;szita;[~nkollar], +1 for [^PIG-5318_6.patch], committed to trunk.
I think we should also upgrade the spark 2 minor version in Pig On Spark to 2.2. We don't want to maintain a 1.6.1, 2.1.1, and 2.2.0 support at the same time, rather have one minor per major.
Created PIG-5321 to track the upgrade.",,,,,,,,,,,,,,,,,,,,,
Null handling on FLATTEN,PIG-5201,13059808,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,28/Mar/17 18:12,09/Dec/17 06:45,13/Mar/19 23:13,09/Dec/17 06:45,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"Sometimes, FLATTEN(null) or FLATTEN(bag-with-null) seem to produce incorrect results.

Test code/script to follow.",,,,,,,,,,,,,PIG-3368,,,,28/Mar/17 19:22;knoguchi;pig-5201-v00-testonly.patch;https://issues.apache.org/jira/secure/attachment/12860913/pig-5201-v00-testonly.patch,07/Apr/17 20:05;knoguchi;pig-5201-v01.patch;https://issues.apache.org/jira/secure/attachment/12862524/pig-5201-v01.patch,11/Apr/17 16:26;knoguchi;pig-5201-v02.patch;https://issues.apache.org/jira/secure/attachment/12862889/pig-5201-v02.patch,31/May/17 20:37;knoguchi;pig-5201-v03.patch;https://issues.apache.org/jira/secure/attachment/12870642/pig-5201-v03.patch,20/Sep/17 16:38;knoguchi;pig-5201-v04.patch;https://issues.apache.org/jira/secure/attachment/12888084/pig-5201-v04.patch,28/Oct/17 03:59;knoguchi;pig-5201-v05.patch;https://issues.apache.org/jira/secure/attachment/12894514/pig-5201-v05.patch,01/Nov/17 21:28;knoguchi;pig-5201-v06.patch;https://issues.apache.org/jira/secure/attachment/12895284/pig-5201-v06.patch,05/Nov/17 06:04;knoguchi;pig-5201-v07.patch;https://issues.apache.org/jira/secure/attachment/12896114/pig-5201-v07.patch,22/Nov/17 21:22;knoguchi;pig-5201-v08.patch;https://issues.apache.org/jira/secure/attachment/12898943/pig-5201-v08.patch,22/Nov/17 21:36;knoguchi;pig-5201-v09-with-testUtilchange.patch;https://issues.apache.org/jira/secure/attachment/12898944/pig-5201-v09-with-testUtilchange.patch,,10.0,,,,,,,,,,,,,,,,,,,2017-03-28 19:53:27.152,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Sat Dec 09 06:45:19 UTC 2017,,,,,,,0|i3cwnj:,9223372036854775807,,,,,,,,,,"28/Mar/17 18:56;knoguchi;{code:title=test.pig}
A = load 'input.txt' as (a1:bag {(a1_1:chararray, a1_2:chararray)}, a2:bag{(a2_1:chararray, a2_2:chararray)}, a3:chararray);
B = FOREACH A generate flatten(a1), flatten(a2), a3;
DUMP B;
{code}

Input
||a1 || a2 || a3 ||
| {(1,2)} | | 1 |
| {(2,3),(3,4)} | {(a,b),(c,d)} | 2 |
| | | 3 |
| | {(e,f)} | 4 |
| {,(7,8)} | {(g,h),(i,j)} | 5 |
Output
{noformat}
(1,2,,1)
(2,3,a,b,2)
(2,3,c,d,2)
(3,4,a,b,2)
(3,4,c,d,2)
(,,3)
(,e,f,4)
(7,8,g,h,5)
(7,8,i,j,5)
{noformat}

Formatting the output in a table
||a2_1|| a2_2 || a3_1 || a3_2 || a4 ||
| 1 | 2 |  | 1 |
| 2 | 3 | a | b | 2 |
| 2 | 3 | c | d | 2 |
| 3 | 4 | a | b | 2 |
| 3 | 4 | c | d | 2 |
|  |  | 3 |
|  | e | f | 4 |
| 7 | 8 | g | h | 5 |
| 7 | 8 | i | j | 5 |

Using different loader({{mock.Storage}}),  I can also see null and \{(g,h),(i,j)\} pair creating 
||a2_1|| a2_2 || a3_1 || a3_2 || a4 ||
| | g | h | 5 | 
| | i | j | 5 |

A couple of quetions.

(1) What should be the behavior for FLATTEN(null)?   For FLATTEN(empty_bag), we drop them.  Shall we do the same?  (Offline, Rohini said no.)

(2) What should be the behavior for null record within the bag in FLATTEN(bag-with-null) ?  

and probably a separate jira, but 
(3) Why is PigStorage not showing the empty item for \{,(7,8)\} ?","28/Mar/17 19:22;knoguchi;Attaching a unit test I'm using for testing FLATTEN(null) and FLATTEN(bag-with-null). 

Please ignore the expected results part.  For that, I'm waiting for feedback from others like [~rohini], [~daijy], etc.
","28/Mar/17 19:53;rohini;In my opinion, FLATTEN(null) or FLATTEN(bagwithnull) should produce nulls based on the number of elements in the schema of the field instead of a single null.

For eg:
  - FLATTEN(null) where the field is a tuple with 3 inner fields, it should produce three nulls
  - FLATTEN(bagwithnull) where the bag contains 3 inner fields, it should produce three nulls. Same as above.
 - FLATTEN(null) where the column is a map, it should produce two nulls (one each for key and value)

It is tricky when the tuple or bag does not have schema. In that case we can probably retain the current behavior of producing single null and document it.
","29/Mar/17 17:16;knoguchi;Reading my bible (==Programming Pig), 
{quote}
The record with the empty bag would be swallowed by foreach. There are a couple of reasons for this behavior. One, since Pig may or may not have the schema of the data in the bag, it might have no idea how to fill in nulls for the missing fields. Two, from a mathematical perspective, this is what you would expect. Crossing a set S with the empty set results in the empty set.
{quote}
using the same reasonings, one option would be to drop for the cases discussed on this jira?  
","30/Mar/17 14:33;knoguchi;[~daijy], appreciate your feedback on this when you have time.","01/Apr/17 06:15;daijy;This is PIG-2537, this is a bug we shall fix.","02/Apr/17 01:20;knoguchi;bq. This is PIG-2537, this is a bug we shall fix.

Thanks [~daijy].  Question.
PIG-2537 is about null Tuple.  
I'm not convinced that same should apply to null Bag.  (This is my question (1) above)  
(There is a separate question of whether runtime system can distinguish null bag to null tuple but let's put that aside.)

To me, if Flatten(empty-bag) is defined to drop, FLATTEN(null-bag) should also do the same.

As for my (2) FLATTEN(Bag-with-null), we probably want to follow whatever we're going to do in PIG-2537.","03/Apr/17 21:09;daijy;Thanks for pointing out, yes, PIG-2537 is for tuple, for bag, I agree null bag should drop (same as empty bag), bag with null item should generating according to schema (same as PIG-2537), in case no schema, we can generate single null column.","07/Apr/17 20:05;knoguchi;I have little understanding on the back-end side so this may be off.
Tried grabbing the number of fields at compile time and using it at runtime when null is passed for flatten.",11/Apr/17 16:26;knoguchi;Adding one more testcase with column pruning.  I thought I would need to readjust the array but it seems to work as-is probably due to (re-)creating the numFields array in LOGenerate.getSchema call.,"13/Apr/17 02:34;rohini;bq. To me, if Flatten(empty-bag) is defined to drop, FLATTEN(null-bag) should also do the same.
  I don't think we should do that as existing scripts will end up losing more records.","13/Apr/17 13:52;knoguchi;{quote}
bq. To me, if Flatten(empty-bag) is defined to drop, FLATTEN(null-bag) should also do the same.
I don't think we should do that as existing scripts will end up losing more records.
{quote}
Dropping on FLATTEN(empty-bag) is a documented behavior.
I don't see any reasons FLATTEN(null-bag) to behave differently. 

Also, not sure how many users depend on the current {{FLATTEN(null) = null}} given it corrupts the output in most cases.  (It incorrectly shifts the fields like described in PIG-2537)","20/Apr/17 17:58;rohini;bq. I don't see any reasons FLATTEN(null-bag) to behave differently.
   It is hard for me to see empty and null as same. This means dropping a whole record when one of the fields is null. I doubt users would want that. You might want to check with a couple of them to be sure. 

bq. Also, not sure how many users depend on the current FLATTEN(null) = null given it corrupts the output in most cases.
  Most of the users have bags with just one element and this shifting issue is not a problem in those cases. Bag of strings is way more common than a bag of tuples and that is why we have not come across this for a long time.

","20/Apr/17 18:07;knoguchi;bq. It is hard for me to see empty and null as same.

We defined FLATTEN(empty-bag) to drop since ""Crossing a set S with the empty set results in the empty set."". 
To be consistent, FLATTEN(null-bag) should behave the same.

Anyways, we can continue the discussion once you're back next month.  Let's keep the jira open till then.",25/May/17 14:31;knoguchi;It'll be nice if we can fix this for 0.17 since this leads to incorrect outputs (and confusions).,"26/May/17 17:42;rohini;I guess it should be ok to push this to 0.18 as this has been there from the beginning. [~daijy], thoughts?","26/May/17 18:22;daijy;It is not a regression, so it is certainly fine to push out. Koji can still try but it should not be a release blocker.","26/May/17 18:33;knoguchi;I'm ok to push it to 0.18.  But one correction.

bq. Koji can still try but it should not be a release blocker.

Current issue here is more on [~daijy] and [~rohini] to agree on what the behavior of flatten(null-bag) should be. :)
","26/May/17 18:39;rohini;bq. Current issue here is more on Daniel Dai and Rohini Palaniswamy to agree on what the behavior of flatten(null-bag) should be
  :). Yes. The time is to sort that out. I am still not sure on this and worried that users might be losing records silently after this change. Have asked [~knoguchi] to check with couple of internal users who are both Pig and data pipeline experts and will be affected by this.","26/May/17 18:42;knoguchi;bq. and will be affected by this.

Who _may_ get affected by this.  
",31/May/17 20:37;knoguchi;Noticed I forgot to take out some debug print/dump statements.  Taking them out and now calling {{Util.checkQueryOutputsAfterSort}} for map output comparisons. ,"20/Jun/17 21:10;knoguchi;bq.  Have asked Koji Noguchi to check with couple of internal users who are both Pig and data pipeline experts and will be affected by this.

From the users, learned that there's a common pattern users use which can easily break when FLATTEN(null-bag) start dropping records as I proposed... 

Basically their code looks like
{code}
...
C = FOREACH B GENERATE record_type, FLATTEN(type_a_bag), FLATTEN(type_b_bag); 
...
{code}
When record_type is 'a', type_b_bag is null, and vice-versa. 
Instead of checking the record_type up-front, user simply flatten both and later examine the record_type.

I hate inconsistency and I hate being wrong (and Rohini being right), but it looks like I would have to keep the current behavior of FLATTEN(null-bag) _not_ dropping.  ","09/Aug/17 18:58;knoguchi;[~daijy], as I wrote above, are you ok with FLATTEN(null-bag) not dropping ?  
(e.g. if this bag has schema, then FLATTEN(null) be expanded to multiple nulls to match the number of fields)",09/Aug/17 19:51;daijy;It is equivalent to flatten a scalar. That's sounds fine. How about columns? Shall we produce the same number of nulls columns according to schema? It might be the same as PIG-2537.,"09/Aug/17 19:59;knoguchi;bq. Shall we produce the same number of nulls columns according to schema?
Yes, we should.

bq. It might be the same as PIG-2537.
Could be, but I don't think we can fix at Utf8StorageConverter loadcaster level.
",10/Aug/17 05:26;daijy;What's your idea for the column padding?,"10/Aug/17 18:43;knoguchi;bq. What's your idea for the column padding?
I don't remember much on what I tried on this jira but my patch seems like it's trying to pad null inside POForeach when flatten is called.  Utf8StorageConverter is just one type of loadcaster, so we cannot assume all the data use this loadcaster.  At least not for fixing a data correctness issue like this one.","20/Sep/17 16:40;knoguchi;Uploading a patch that now expands FLATTEN(null-bag) to null * num_fields and FLATTEN(null-map) to 2 fields.  

I still need to look at PIG-2537 and its patch to understand Daniel's approach.","26/Oct/17 17:08;rohini;Comments:
1) Can we rename flattenNumFieldsForNull to flattenNumFields? Naming it based on the purpose will make it odd when it is reused for something else.
2) POForeach clone should copy flattenNumFieldsForNull
3) isToBeFlattenedArray[ i ] is checked in 3 if statements. Can we create a outer block with that condition to simplify?
4) TestFlatten.java - license is modified. Unintended change?
6)  :)
{code}
System.err.println(""KOJIKOJI"");
152	        for( Tuple t : actualResults ) {
153	            System.err.println(t);
154	        }

        System.err.println(""KOJIKOJI2"");
167	        for( Tuple t : expectedResults ) {
168	            System.err.println(t);
169	        }
170	
{code}

7) testFlattenOnNullBagWithColumnPrune is redundant. ColumPrune does not affect Flatten. Currently we don’t column prune nested data structures - tuple, bag (PIG-1324). We can remove this test or merge it into one of the other tests considering our unit test run time is already too high. ","28/Oct/17 04:07;knoguchi;Thanks for the review Rohini! 

bq. 1) Can we rename flattenNumFieldsForNull to flattenNumFields? 
Done.

bq. 2) POForeach clone should copy flattenNumFieldsForNull
Completely missed that. Done.

bq. 3) isToBeFlattenedArray[ i ] is checked in 3 if statements. Can we create a outer block with that condition to simplify?
Took a look but I'm already afraid of introducing a regression with this current change.  Prefer not to touch other places if not necessary. 

bq. 4) TestFlatten.java - license is modified. Unintended change?
Ouch. Fixed.

bq. System.err.println(""KOJIKOJI"");
Another ouch. Wiped. 

{quote}
7) testFlattenOnNullBagWithColumnPrune is redundant. ColumPrune does not affect Flatten. Currently we don’t column prune nested data structures - tuple, bag (PIG-1324). 
{quote}
Actually this was testing what would happen when a simple field is pruned. In the test with schema {noformat}
(a0:int, bag1:bag {(a1_1:int, a1_2:chararray)}, bag2:bag{(a2_1:chararray, a2_2:chararray)})
{noformat}
this test is pruning a0:int and checking if {{flattenNumFields}} needs to be shifted.

Lastly, I added a document for this new behavior.","30/Oct/17 17:48;rohini;1) 
bq. Took a look but I'm already afraid of introducing a regression with this current change. Prefer not to touch other places if not necessary.
  It should be a simple change. Don't see any cause for concern of regression. I am asking this because, currently isToBeFlattenedArray is being checked three times before going to the case of not FLATTEN while it should be done only once. This is very inefficient as the number of flatten compared to fields without flatten in a foreach statement is very less. Also there are many users with 100+ columns in a foreach statement and it will be bad for those cases.

2) Doc minor issues
 some null Tuples -> some null tuples
Flatten operataor -> FLATTEN operator  (capitalize + typo in operator)


","01/Nov/17 21:31;knoguchi;{quote}
bq. Took a look but I'm already afraid of introducing a regression with this current change. Prefer not to touch other places if not necessary.
It should be a simple change.
{quote}
Ok,  Tried to add this change in POForEach.createTuple method.

bq. 2) Doc minor issues

Forgot to run a spell checker. Thanks.",01/Nov/17 21:39;rohini;+1,"05/Nov/17 06:12;knoguchi;With {{pig-5201-v06.patch}}, saw a couple of unit test failures.
Turns out the change I made in test/Util.java was causing it.

My change was meant to help show the exact output difference when comparisons fail, but {{checkQueryOutputs}} was calling toString and was incorrectly failing when comparing two bags with different tuple orders.   (Bag is order-independent, so comparing two bags with different order should still match.)
{code}
diff --git test/org/apache/pig/test/Util.java test/org/apache/pig/test/Util.java
index 8d4282dfb..f2cec4529 100644
--- test/org/apache/pig/test/Util.java
+++ test/org/apache/pig/test/Util.java
@@ -613,8 +613,7 @@ public class Util {
          Collections.sort(actualResList);
          Collections.sort(expectedResList);

-         Assert.assertEquals(""Comparing actual and expected results. "",
-                 expectedResList, actualResList);
+         checkQueryOutputs(actualResList.iterator(), expectedResList);

     }
{code}
Attaching a new patch {{pig-5201-v07.patch}} with change to {{test/Util.java}} reverted. ",06/Nov/17 13:11;rohini;+1,"22/Nov/17 21:34;knoguchi;Sorry for taking this long to commit.
Two more issues.

(a) My last patch has one test failure from the test I added at 
{{org.apache.pig.test.TestFlatten.testFlattenOnNullMap}} 

{panel}
Comparing actual and expected results.  
expected: java.util.ArrayList<\[(,,11,12), (,,13,14), (a,b,1,2), (a,b,3,4), (c,d,1,2), (c,d,3,4), (k,l,,), (m,n,,)]> 
but was: java.util.ArrayList<\[(,,11,12), (,,13,14), (a,b,1,2), (a,b,3,4), (c,d,1,2), (c,d,3,4), (k,l,,), (m,n,,)]>
{panel}

Above error message is showing identical strings but it's actually failing since {{Util.checkQueryOutputs}} only compares the string representation but {{Util.checkQueryOutputsAfterSort}} actually compares Object to Object and is more strict.  Above test is failing since one has null and another has empty string.

This patch fixed the test case so that it correctly passes ""null"" instead of empty """" as the expected outputs.

We can go one step further and make checkQueryOutputs and checkQueryOutputsAfterSort consistent but this requires more changes elsewhere. 

(b) {{org.apache.pig.test.TestProjectRange.testRangeOrderByMixNOSchema}} and  {{org.apache.pig.test.TestProjectRange.testRangeOrderByStartNOSchema}} fails with my previous patch when comparing with the golden file.    Looking at them, it seems like golden files for these two tests include the serialized class as the output (and my patch did add extra field in POForEach.   For now, replaced the golden files but we may want to see if we can get rid of that serialized output line.

","22/Nov/17 21:39;knoguchi;bq. We can go one step further and make checkQueryOutputs and checkQueryOutputsAfterSort consistent but this requires more changes elsewhere.

Attaching {{pig-5201-v09-with-testUtilchange.patch}} which does this but not sure if this just makes the testing more confusing...
You would now need to consider 
* if number if integer, long or String, 
* if object is actually String or DataByteArray, 
* etc, etc
","27/Nov/17 17:25;rohini;bq. We can go one step further and make checkQueryOutputs and checkQueryOutputsAfterSort 
This is absolutely better as there will be better data validation and type checking in tests.

I just have one question. Why do you have to change checkQueryOutputsAfterSort to call checkQueryOutputs? Isn't that already doing object comparison?

","28/Nov/17 02:52;knoguchi;bq.  Why do you have to change checkQueryOutputsAfterSort to call checkQueryOutputs?

I just liked the way checkQueryOutputs showed only the different part of the outputs instead of showing the entire outputs.  I tried to keep this behavior by adding extra comparisons by toString() before making sure it fails with direct comparisons. 

{code}
+              // If this tuple contains any bags, bags will be sorted before comparisons
+              if( !expected.equals(actual) ) {
+                  // Using string comparisons since error message is more readable
+                  // (only showing the part which differs)
+                  Assert.assertEquals(expected.toString(), actual.toString());
+                  // if above goes through, simply failing with object comparisons
+                  Assert.assertEquals(expected, actual);
+              }
{code}

And I didn't want to write this code twice in both checkQueryOutputs and checkQueryOutputsAfterSort.",28/Nov/17 15:49;rohini;+1,"09/Dec/17 06:45;knoguchi;Thanks for the review Rohini! 
Also, Daniel, thank you for all your feedback. 

Committed the patch to trunk (0.18).
"
MergeJoin throwing NullPointer Exception,PIG-5310,13112144,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,satishsaley,satishsaley,satishsaley,25/Oct/17 23:15,29/Nov/17 19:39,13/Mar/19 23:13,29/Nov/17 19:39,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"Merge join throws NullPointerException if left input's first key doesn't exist in right input and if it is smaller than first key of right input.
For ex

|left|right|
|1|3|
|1|5|
|1| |

Error we get - 
{code}
ERROR 2998: Unhandled internal error. Vertex failed, vertexName=scope-16, vertexId=vertex_1509400259446_0001_1_02, diagnostics=[Task failed, taskId=task_1509400259446_0001_1_02_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1509400259446_0001_1_02_000000_0:java.lang.NullPointerException
	at java.lang.Integer.compareTo(Integer.java:1216)
	at java.lang.Integer.compareTo(Integer.java:52)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNextTuple(POMergeJoin.java:525)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:305)
	at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POStoreTez.getNextTuple(POStoreTez.java:123)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:416)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:281)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1945)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

Here, the key used in join is an integer. Integer.compareTo(other) method throws null pointer exception if comparison is made against null. ",,,,,,,,,,,,,,,,,27/Oct/17 20:30;satishsaley;PIG-5310-1.patch;https://issues.apache.org/jira/secure/attachment/12894424/PIG-5310-1.patch,28/Nov/17 00:58;satishsaley;PIG-5310-2.patch;https://issues.apache.org/jira/secure/attachment/12899518/PIG-5310-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-10-30 12:37:37.028,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Wed Nov 29 19:39:48 UTC 2017,,,,,,,0|i3lpsn:,9223372036854775807,,,,,,,,,,"30/Oct/17 12:37;nkollar;[~satishsaley] could you please help me to reproduce this issue? The test case in your patch passes even when I *don't* apply the changes you made on POMergeJoin.java (expecting a failing test case). I tried with {{-Dtest.exec.type=spark}}, {{-Dtest.exec.type=tez}}, and {{-Dtest.exec.type=mr}}, each finished successfully.","28/Nov/17 00:58;satishsaley;Fixed the test case. 
The test case earlier was not throwing exception without the fix because I was not loading key as integer. By default it was DataByteArray. ","28/Nov/17 09:42;nkollar;Thanks, second patch looks good to me!",28/Nov/17 19:51;rohini;What happens if the left side had nulls and it is a Left outer join? Will those records be skipped? Can you add a test case for that case.,"29/Nov/17 01:31;satishsaley;I checked the execution plans for Left/Right outer joins using merge, it takes different code path.",29/Nov/17 10:45;szita;+1 on [^PIG-5310-2.patch],"29/Nov/17 19:39;rohini;bq. it takes different code path
  Did not realize that outer join is through POMergeCogroup

+1. Committed to trunk. Thanks for fixing this Satish. Thanks Nandor and Adam for the reviews.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Abort method is not implemented in PigProcessor,PIG-5314,13119333,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,satishsaley,rohini,rohini,17/Nov/17 21:26,28/Nov/17 15:53,13/Mar/19 23:13,28/Nov/17 15:53,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"Found a hung job caused by a task stuck in a infinite loop in the 
https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/tez/runtime/PigProcessor.java#L308-L310

{code}
2017-11-08 23:23:47,904 [INFO] [TezChild] |task.TezTaskRunner2|: returning canCommit=false since task is not in a running state
{code}

The task runner keeps returning false for canCommit because task abort has been already called which Pig ignored.",,,,,,,,,,,,,,,,,21/Nov/17 02:27;satishsaley;PIG-5314-1.patch;https://issues.apache.org/jira/secure/attachment/12898596/PIG-5314-1.patch,22/Nov/17 00:34;satishsaley;PIG-5314-2.patch;https://issues.apache.org/jira/secure/attachment/12898760/PIG-5314-2.patch,27/Nov/17 22:25;satishsaley;PIG-5314-3.patch;https://issues.apache.org/jira/secure/attachment/12899501/PIG-5314-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-11-27 22:26:02.129,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Tue Nov 28 15:53:50 UTC 2017,,,,,,,0|i3my27:,9223372036854775807,,,,,,,,,,"21/Nov/17 13:16;rohini;This should be in abort() method
{code} 
LOG.warn(""Aborting execution"");
abortOutput();
{code}","22/Nov/17 17:33;rohini;Can you remove the javadoc ""Sets isAborted to true"" ? Can you also add a comment  ""// TODO add @Override when we upgrade to Tez 0.9 dependency",27/Nov/17 22:26;satishsaley;updated,28/Nov/17 15:53;rohini;+1. Committed to trunk. Thanks Satish for the patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflowError when compiling in Tez mode (with union and replicated join),PIG-5271,13087029,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,13/Jul/17 22:14,22/Sep/17 13:15,13/Mar/19 23:13,20/Sep/17 16:45,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"Sample script
{code}
a4 = LOAD 'studentnulltab10k' as (name, age:int, gpa:float);
a4_1 = filter a4 by gpa is null or gpa >= 3.9;
a4_2 = filter a4 by gpa < 1;
b4 = union a4_1, a4_2;
b4_1 = filter b4 by age < 30;
b4_2 = foreach b4 generate name, age, FLOOR(gpa) as gpa;

c4 = load 'voternulltab10k' as (name, age, registration, contributions);
d4 = join b4_2 by name, c4 by name using 'replicated';
e4 = foreach d4 generate b4_2::name as name, b4_2::age as age, gpa, registration, contributions;
f4 = order e4 by name, age DESC;
store f4 into 'tmp_table_4' ;

a5_1 = filter a4 by gpa is null or gpa <= 3.9;
a5_2 = filter a4 by gpa < 2;
b5 = union a5_1, a5_2;
d5 = join c4 by name, b5 by name using 'replicated';
store d5 into 'tmp_table_5' ;
{code}
This script fails to compile with StackOverflowError.
{noformat}
    at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)
Pig Stack Trace
---------------
ERROR 2998: Unhandled internal error. null

java.lang.StackOverflowError
    at java.lang.reflect.Constructor.newInstance(Constructor.java:415)
    at java.lang.Class.newInstance(Class.java:442)
    at org.apache.pig.impl.util.Utils.mergeCollection(Utils.java:490)
    at org.apache.pig.impl.plan.DependencyOrderWalker.doAllPredecessors(DependencyOrderWalker.java:101)
    at org.apache.pig.impl.plan.DependencyOrderWalker.doAllPredecessors(DependencyOrderWalker.java:105)
    at org.apache.pig.impl.plan.DependencyOrderWalker.doAllPredecessors(DependencyOrderWalker.java:105)
    at org.apache.pig.impl.plan.DependencyOrderWalker.doAllPredecessors(DependencyOrderWalker.java:105)
    at org.apache.pig.impl.plan.DependencyOrderWalker.doAllPredecessors(DependencyOrderWalker.java:105)
    at org.apache.pig.impl.plan.DependencyOrderWalker.doAllPredecessors(DependencyOrderWalker.java:105)
...
{noformat}",,,,,,,,,,,,,,,,,13/Jul/17 22:23;knoguchi;pig-5271-v01.patch;https://issues.apache.org/jira/secure/attachment/12877176/pig-5271-v01.patch,14/Jul/17 19:07;knoguchi;pig-5271-v02.patch;https://issues.apache.org/jira/secure/attachment/12877368/pig-5271-v02.patch,20/Sep/17 05:44;knoguchi;pig-5271-v03.patch;https://issues.apache.org/jira/secure/attachment/12888015/pig-5271-v03.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-08-22 23:21:38.457,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Fri Sep 22 13:15:18 UTC 2017,,,,,,,0|i3hi5b:,9223372036854775807,,,,,,,,,,"13/Jul/17 22:24;knoguchi;First time touching a Tez compiler.  
Attaching a patch that would give up on UnionOptimizer when it can create a loop.
",14/Jul/17 19:07;knoguchi;Only updating a comment in the test from the previous patch.,"22/Aug/17 23:21;daijy;Looks good to me. [~rohini], do you want a second look?","23/Aug/17 22:24;rohini;Yes. Promised [~knoguchi] will take a look weeks back, but was not able to get to it. Will do it this weekend.","19/Sep/17 20:22;rohini;Few minor comments:
   1) ypig-223/input1.txt -> file:///tmp/input to fix file:///Users/knoguchi/git/pig/ypig-223/input1.txt in the golden file.
   2) This is redundant and can be removed.
{code}
setProperty(PigConfiguration.PIG_OPT_MULTIQUERY, """" + true);
setProperty(PigConfiguration.PIG_TEZ_OPT_UNION, """" + true);
{code}","20/Sep/17 05:45;knoguchi;Thanks for the review Daniel, Rohini! 

Attaching {{pig-5271-v03.patch}} fixing the two changes suggested by Rohini.",20/Sep/17 16:32;rohini;+1,"20/Sep/17 16:45;knoguchi;Thanks again for the review Rohini, Daniel! 

Committed to trunk.",22/Sep/17 07:44;nkollar;[~knoguchi] I think you forgot to commit TEZC-Union-22.gld from this patch! Could you please commit this file too?,"22/Sep/17 13:15;knoguchi;bq. Koji Noguchi I think you forgot to commit TEZC-Union-22.gld from this patch! Could you please commit this file too?

Ouch.  It was from my own patch and I still missed adding this file.  Thanks for pointing it out.  Added now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartitionFilterOptimizer failing at compile time,PIG-5299,13100579,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,07/Sep/17 21:00,19/Sep/17 18:30,13/Mar/19 23:13,19/Sep/17 18:30,,,,,,,,,,,,,,,,,0.17.1,,,,,0,,,,,,,,"Following (rather simple) code 
{code:title=test.pig}
A = LOAD '/tmp/testinput' using org.apache.pig.test.TestLoader ('srcid:int, mrkt:chararray, dstid:int, name:chararray', 'srcid'); --srcid is the partition-key
B= filter A by dstid != 10 OR ((dstid < 3000 and srcid == 1000) OR (dstid >= 3000 and srcid == 2000));
dump B;
{code}
is failing with 
{panel}
2017-09-07 16:37:03,210 \[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2243: Attempt to remove operator GreaterThanEqual that is still connected in the plan
{panel}
",,,,,,,,,,,,,,,,,08/Sep/17 14:38;knoguchi;pig-5299-delayremoval-v1.patch;https://issues.apache.org/jira/secure/attachment/12886091/pig-5299-delayremoval-v1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-09-19 18:10:11.629,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Tue Sep 19 18:30:09 UTC 2017,,,,,,,0|i3js7j:,9223372036854775807,,,,,,,,,,"07/Sep/17 21:02;knoguchi;Log file showing the trace.
{noformat}
Pig Stack Trace
---------------
ERROR 2243: Attempt to remove operator GreaterThanEqual that is still connected in the plan

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias B
        at org.apache.pig.PigServer.openIterator(PigServer.java:1020)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:782)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:383)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
        at org.apache.pig.Main.run(Main.java:630)
        at org.apache.pig.Main.main(Main.java:175)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias B
        at org.apache.pig.PigServer.storeEx(PigServer.java:1123)
        at org.apache.pig.PigServer.store(PigServer.java:1082)
        at org.apache.pig.PigServer.openIterator(PigServer.java:995)
        ... 7 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2000: Error processing rule PartitionFilterOptimizer. Try -t PartitionFilterOptimizer
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:125)
        at org.apache.pig.newplan.logical.relational.LogicalPlan.optimize(LogicalPlan.java:281)
        at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1458)
        at org.apache.pig.PigServer.storeEx(PigServer.java:1119)
        ... 9 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2243: Attempt to remove operator GreaterThanEqual that is still connected in the plan
        at org.apache.pig.newplan.BaseOperatorPlan.remove(BaseOperatorPlan.java:172)
        at org.apache.pig.newplan.FilterExtractor.removeFromFilteredPlan(FilterExtractor.java:340)
        at org.apache.pig.newplan.FilterExtractor.removeFromFilteredPlan(FilterExtractor.java:337)
        at org.apache.pig.newplan.FilterExtractor.removeFromFilteredPlan(FilterExtractor.java:337)
        at org.apache.pig.newplan.FilterExtractor.checkPushDown(FilterExtractor.java:255)
        at org.apache.pig.newplan.FilterExtractor.visit(FilterExtractor.java:106)
        at org.apache.pig.newplan.logical.rules.PartitionFilterOptimizer$PartitionFilterPushDownTransformer.transform(PartitionFilterOptimizer.java:155)
        at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:110)
        ... 12 more
{noformat}
Basically when PartitionFilterOptimizer gives up on pushing down certain filter condition, updating the filteredPlan is failing.","07/Sep/17 21:30;knoguchi;Pasting filter plan with hashcode (to check the uniqueness)
{noformat}
|---(Name: Constant Type: null Uid: null):1947185929
(Name: And Type: null Uid: null):1774720883
|
|---(Name: Or Type: null Uid: null):1605851606
|   |
|   |---(Name: Equal Type: null Uid: null):1534754611
|   |   |
|   |   |---(Name: Project Type: null Uid: null Input: 0 Column: 0):944140566
|   |   |
|   |   |---(Name: Constant Type: null Uid: null):1551446957
|   |
|   |---(Name: GreaterThanEqual Type: null Uid: null):1020154737
|       |
|       |---(Name: Project Type: null Uid: null Input: 0 Column: 2):987249254
|       |
|       |---(Name: Constant Type: null Uid: null):1850954068
|
|---(Name: And Type: null Uid: null):1440621772
    |
    |---(Name: Or Type: null Uid: null):352083716
    |   |
    |   |---(Name: LessThan Type: null Uid: null):713312506
    |   |   |
    |   |   |---(Name: Project Type: null Uid: null Input: 0 Column: 2):597307515
    |   |   |
    |   |   |---(Name: Constant Type: null Uid: null):770010802
    |   |
    |   |---(Name: Equal Type: null Uid: null):223693919
    |       |
    |       |---(Name: Project Type: null Uid: null Input: 0 Column: 0):1758056825
    |       |
    |       |---(Name: Constant Type: null Uid: null):361268035
    |
    |---(Name: Or Type: null Uid: null):1787189503
        |
        |---(Name: LessThan Type: null Uid: null):713312506
        |   |
        |   |---(Name: Project Type: null Uid: null Input: 0 Column: 2):597307515
        |   |
        |   |---(Name: Constant Type: null Uid: null):770010802
        |
        |---(Name: GreaterThanEqual Type: null Uid: null):1020154737
            |
            |---(Name: Project Type: null Uid: null Input: 0 Column: 2):987249254
            |
            |---(Name: Constant Type: null Uid: null):1850954068
{noformat}
This {{|---(Name: GreaterThanEqual Type: null Uid: null):1020154737}} is used twice in the plan.
One from
{{|---(Name: Or Type: null Uid: null):1605851606}} 
and another from
{{|---(Name: Or Type: null Uid: null):1787189503}}

Error message 
bq. Attempt to remove operator GreaterThanEqual that is still connected in the plan

is coming when FilterExtractor tries to remove GreaterThanEqual from the first OR:1605851606 but failing when GreaterThanEqual is still connected to OR:1787189503.","07/Sep/17 22:07;knoguchi;Two ways of handling this.
(1) Deepcopy the expressions everywhere so that same expression is not used in multiple locations.
Or
(2) Delay the removal for expression that is used elsewhere. 

I'll try getting a patch with (2) and see how it looks.","08/Sep/17 14:39;knoguchi;bq. (2) Delay the removal for expression that is used elsewhere.

Attaching a patch and running a full test now.","15/Sep/17 15:08;knoguchi;bq. Attaching a patch and running a full test now.

Tests look good.  Making it patch-available. ",19/Sep/17 18:10;rohini;+1,"19/Sep/17 18:30;knoguchi;Thanks for the review Rohini! 

Committed to branch 0.17 and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User Cache upload contention can cause job failures,PIG-5290,13094128,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xkrogen,xkrogen,xkrogen,11/Aug/17 15:55,12/Sep/17 22:23,13/Mar/19 23:13,08/Sep/17 23:06,0.13.0,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"We recently enabled the User Cache (PIG-2672) feature and found that occasionally jobs would fail because of contention when uploading JARs into the cache. Although the cache is designed to be fail-safe, i.e. to fall back to normal behavior if anything goes wrong by catching all {{IOException}}, the portion of code which closes the output stream _is not_ wrapped within a {{try}} statement and thus an exception during the closing of that stream causes the entire job to fail. If multiple jobs are attempting to upload the same JAR failure simultaneously, the contention can cause this close statement to fail.

The current strategy also has two other flaws. First, consider the scenario where job A begins uploading jar X. Job B also needs jar X, sees that the file exists, and launches its tasks. Yet, job A has not yet finished uploading jar X (perhaps it is large). So, the tasks are localizing a half-completed version of jar X. Second, the original design allowed for the same JAR (identical contents) to be shared between jobs even if a different name was used. In PIG-3815, however, this ability was removed, and now JARs are only shared if they have the same name.

I propose we solve all of these issues simultaneously by returning to the listStatus based behavior (used prior to PIG-3815), but filter out entries ending in {{.tmp}}. When uploading, upload to {{randomNumber.tmp}}, then once the file is completed, do a rename to the original name of the JAR file. This ensures that incomplete files are never in a location that would be accessed by other jobs, and the only write operation accessing a shared path is a single rename operation.

An alternative design is to use a single canonicalized name for all JAR files (they will still be unique since they are inside of directories based on their SHA1). Upload to a tmp file as previously described, then rename to the canonical name. This removes the need to do a listStatus call; however it will result in classpaths that are human unreadable since the name of the JAR file has been lost. I think it's worth it from a debugging standpoint to go with the first design.",,,,,,,,,,,,,,,,,07/Sep/17 20:29;xkrogen;PIG-5290-1.patch;https://issues.apache.org/jira/secure/attachment/12885902/PIG-5290-1.patch,16/Aug/17 23:45;xkrogen;PIG-5290.patch;https://issues.apache.org/jira/secure/attachment/12882234/PIG-5290.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-08-14 16:20:46.238,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Mon Sep 11 14:44:53 UTC 2017,,,,,,,0|i3ip6v:,9223372036854775807,,,,,,,,,,"11/Aug/17 16:00;xkrogen;Ping [~rohini], [~knoguchi] from previous involvement with the two JIRAs mentioned in the description.","14/Aug/17 16:20;rohini;[~xkrogen],
   Using randomNumber.tmp is simple and effective. +1 on the idea. Have added you to the contributors list. You can now assign the jira to yourself if you are going to work on the patch.","14/Aug/17 16:22;xkrogen;Will do, thanks [~rohini]!","16/Aug/17 23:47;xkrogen;Attaching initial patch implementing the ideas I discussed in the JIRA. Uploads at a random file name, then moves it to the original filename. Uses listStatus to be able to share identical JAR files with nonidentical names. Makes an attempt at cleaning up hanging temp files if an issue occurs while uploading / renaming.

I did not add any unit tests as it was not very clear to me how to do so / what the policy on that would be. Let me know if I should add some and if so, where they would go. I did manually test the patch on our Hadoop 2.6 cluster and it worked as expected, populating the cache when empty and reusing files when present (even at different names).

[~rohini], please let me know if you have a chance to review. Thanks!","06/Sep/17 22:56;rohini;Few comments:
  1) Can you compile the Pattern and reuse it instead of calling matches every time? That will be more cost effective.
  2) If there is still a collision during writing temp file due to random number collision (we have run into the rare scenario where different jobs had random numbers collide during temp dir creation) or renaming to final file name (more likely than first scenario) you will still end up with an error. First case is really rare and so we can skip for now. But we should handle the rename problem by checking for FileAlreadyExistsException in a separate try/catch block and verify there if the existing file size is same as the temp file and if not thrown an error.
  3) Regarding reverting to the listStatus behavior, I checked PIG-3815-3.patch and before the change looks like it still did check to see if filename matched.
https://github.com/apache/pig/blob/c2aedcc66486ddc721a32dc4984547f049aa5541/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java#L1638
     I don't think we should be doing the listStatus check you are doing and pick any non-tmp file in that directory. Might end up with wrong file in case of checksum collisions. Comment 1) would be not necessary if listStatus behavior is reverted.
  4) We generally expect unit tests to be added with every patch. But in this case simulating a race condition is hard, so I would say we can skip and rely on the older tests. But on checking realized that there is no actually no test for this feature. Can you add -Dpig.user.cache.enabled=true to java_params of UdfDistributedCache and MonitoredUDF tests in nightly.conf?","07/Sep/17 20:30;xkrogen;Thank you for the review [~rohini]!

I see you are right about (3); I missed the line you linked. I can keep it at the old behavior.

For (2), I agree that there is potential for collision during writing temp file, but figured that since it will fall back to default behavior if it had an error while uploading, this is not too much of an issue (just an extra JAR upload rather than using cache). I don't understand the ""renaming to final file name"" issue. Renames on HDFS overwrite, so if two users upload the same simultaneously, one of them will simply win (whoever comes second). Let me know if I am misunderstanding.

Sounds good on (4), thanks for the pointer.

Attaching v1 patch addressing your comments in (3), (4), comment in (1) is no longer relevant. Will wait on your response about (2).","07/Sep/17 21:19;rohini;+1. Looks good. Will run the e2e tests and commit later in the day.

bq. Renames on HDFS overwrite, so if two users upload the same simultaneously, one of them will simply win (whoever comes second). Let me know if I am misunderstanding.
  It was a misunderstanding on my part. I could not exactly remember whether rename wins on overwrite or fails with FileAlreadyExistsException. To confirm, I ended up looking at the javadoc of new rename API which said rename fails if overwrite option is not passed and got mislead by that.
https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L1417-L1451
  Verified that the old API (without the options) behavior is that last one wins and there are no failures on rename if file exists.
","08/Sep/17 23:06;rohini;+1. e2e on both MR and Tez good and verified that files are being used from cache. 

Committed to trunk. Thanks for fixing this [~xkrogen].","11/Sep/17 14:44;xkrogen;Fantastic, thank you Rohini!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Suspicious code as missing `this' for a member,PIG-5293,13094586,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,lifove,lifove,lifove,14/Aug/17 20:09,23/Aug/17 04:55,13/Mar/19 23:13,23/Aug/17 04:55,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"Hi
In a recent github mirror, I've found suspicious code.
Branch: trunk
Path: src/org/apache/pig/pen/util/ExampleTuple.java

{code:java}
...
 39     Tuple t = null;
...
110     @Override
111     public void reference(Tuple t) {
112         t.reference(t);
113     }
{code}

In Line 112, `t.reference' should be `this.t.reference'? This might be just a trivial thing as the class name as ExampleTuple. But I wanted to report just in case.

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-23 04:29:59.611,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Wed Aug 23 04:55:03 UTC 2017,,,,,,,0|i3irzz:,9223372036854775807,,,,,,,,,,23/Aug/17 04:29;daijy;That sounds valid. Can you upload a patch?,"23/Aug/17 04:50;githubbot;GitHub user lifove opened a pull request:

    https://github.com/apache/pig/pull/31

    PIG-5293: Add a missing  to access a field in a method, reference

    Patch for PIG-5293
    
    https://issues.apache.org/jira/browse/PIG-5293

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/lifove/pig trunk

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/pig/pull/31.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #31
    
----
commit 553cf7a5527af3f4d1b669b0c7db3f91ee529ebe
Author: JC <jc@lifove.net>
Date:   2017-08-23T04:48:45Z

    PIG-5293: Add a missing  to access a field in a method, reference

----
",23/Aug/17 04:55;daijy;Patch committed to trunk. Thanks [~lifove]!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark unit tests are always run in spark1 mode,PIG-5294,13095362,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,szita,szita,17/Aug/17 14:16,18/Aug/17 13:35,13/Mar/19 23:13,18/Aug/17 13:35,,,,,,,,,,,,,,,,,0.18.0,,build,spark,,0,,,,,,,,"The ant target {{test}} depends on {{jar}} which builds Pig with sparkversion=2 then sparkversion=1, hence we will always run the tests with Spark 1 (1.6.1 currently to be exact)",,,,,,,,,,,,,,,,,17/Aug/17 16:57;szita;PIG-5294.0.patch;https://issues.apache.org/jira/secure/attachment/12882394/PIG-5294.0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-08-18 13:04:25.842,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 13:35:33 UTC 2017,,,,,,,0|i3iwr3:,9223372036854775807,,,,,,,,,,"17/Aug/17 17:01;szita;Some unit test related targets should not depend on {{jar}}, but rather on {{jar-simple}} instead because:
* There is no point of compiling against two versions of Spark when at the end we only use one to run the tests with
* There is no way to override what Spark version we want, it will always be spark1

Please find the fix in [^PIG-5294.0.patch]","18/Aug/17 13:04;nkollar;Thanks Adam, indeed, looks like we can't execute unit test in spark2 mode right now without your patch. Looks good to me!",18/Aug/17 13:35;szita;Thanks for taking a look Nandor! This is now committed to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration is not passed to SparkPigSplits on the backend,PIG-5283,13091909,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,szita,szita,02/Aug/17 14:57,09/Aug/17 08:43,13/Mar/19 23:13,09/Aug/17 08:43,,,,,,,,,,,,,,,,,0.18.0,,spark,,,0,,,,,,,,"When a Hadoop ObjectWritable is created during a Spark job, the instantiated PigSplit (wrapped into a SparkPigSplit) is given an empty Configuration instance.
This happens [here|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SerializableWritable.scala#L44]",,,,,,,,,,,,,,,,,02/Aug/17 15:05;szita;PIG-5283.0.patch;https://issues.apache.org/jira/secure/attachment/12880058/PIG-5283.0.patch,08/Aug/17 12:51;szita;PIG-5283.1.patch;https://issues.apache.org/jira/secure/attachment/12880823/PIG-5283.1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-08-03 08:59:12.727,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 09 08:42:50 UTC 2017,,,,,,,0|i3ibsf:,9223372036854775807,,,,,,,,,,"02/Aug/17 15:06;szita;I propose we make use of SparkPigSplit and write the conf along with the split as seen in: [^PIG-5283.0.patch]
[~kellyzly], [~nkollar] let me know what you think","03/Aug/17 08:59;kellyzly;[~szita]: In  [PigInputFormatSpark#createRecordReader|https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/spark/running/PigInputFormatSpark.java#L59],  it  initialize pigSplit configuration.  In my understanding,the configuration of pigSplit is initialized correctly, so currently you met a problem of invalid initialization of pigSplit's configuration?  If yes, can you provide simple script to show your problem.
","03/Aug/17 09:41;nkollar;This is a problem with Hadoop 3 (HADOOP-14459) and Spark with Pig. Any Pig script will fail on Hadoop 3 with spark exec type, and like Adam mentioned in the description, the problem might be with {{ow.setConf(new Configuration(false))}} in Spark SerializableWritable class. It creates a new configuration, so whatever was set before is gone. HADOOP-8588 was committed in Hadoop 3, supposed to fix an NPE, but actually causes an other one, if the required key is not set in Configuration (that's why it is fine on Hadoop 2).","03/Aug/17 09:43;nkollar;I think Adam's patch is a good workaround for this problem, looks good to me.","04/Aug/17 02:19;kellyzly;[~nkollar] and [~szita]: Can we set the correct value of CommonConfigurationKeys.IO_SERIALIZATIONS_KEY in  pig on spark to avoid the problem?
If we can not  ,+1 for the patch.","06/Aug/17 01:48;rohini;bq.  HADOOP-8588 was committed in Hadoop 3, supposed to fix an NPE, but actually causes an other one, if the required key is not set in Configuration 
  You should comment on the hadoop jira and have them fix it properly instead of working around in Pig.","07/Aug/17 10:02;szita;[~kellyzly], it doesn't matter what we set in the configuration, unfortunately *Spark will always set an empty configuration to the deserialized PigSplit*.

[~rohini], this is a problem by itself, but we didn't see it before, with Hadoop 2. Hadoop 3 on the other hand has another bug - as [~nkollar] explains - which depends on {{io.serializations}} property being null or not. (In H2 this is handled by Hadoop code properly, in H3 it became buggy) This however doesn't change the fact that the configuration is not passed down properly to the deseralized PigSplits.

I don't see a way to go around the Spark code that creates the empty config during deseralization so I intend to package the conf along with the SparkPigSplit instances.
My only question is that if we should only write those properties that are required for a PigSplit instead of writing the full jobConf (6-700 entries) for optimization.

At first glance I see that only the following properties are needed:
{code}
CommonConfigurationKeys.IO_SERIALIZATIONS_KEY (""io.serializations"")
PigConfiguration.PIG_COMPRESS_INPUT_SPLITS
{code}

","07/Aug/17 11:23;rohini;bq. My only question is that if we should only write those properties that are required for a PigSplit instead of writing the full jobConf (6-700 entries) for optimization.
  I would suggest trimming down and also see if it is possible to serialize only once. You are serializing the config with each split which is not good. That is a lot of overhead and will impact performance. Had run into performance issues and OOMs with Tez on huge configs and serializing configs multiple times and had to trim down.

","08/Aug/17 08:59;kellyzly;[~szita]:  
{quote}
My only question is that if we should only write those properties that are required for a PigSplit instead of writing the full jobConf (6-700 entries) for optimization.

{quote}

not initialize all the items. it is ok to just initialize few items to make it work. Will PigInputFormatSpark#createRecordReader initialize all items after bypassing current issue?","08/Aug/17 13:02;szita;Attached [^PIG-5283.1.patch] with the feature of only writing out the necessary keys of the configuration.
Unfortunately I don't see any way to write the config only once (instead of per split), as I need to have it ready at the very first stages of Spark task execution: the deseralization of the task.

[~kellyzly] yes, the PigInputFormatSpark#createRecordReader part comes much later in the execution, and will work just like before. In a way it is irrelevant of the current issue, because it will set the full configuration on each split, but it's too late for this issue since we need the configuration during task deseralization time already.
",09/Aug/17 01:34;kellyzly;[~szita]:  I can understand the reason why need set {{CommonConfigurationKeys.IO_SERIALIZATIONS_KEY}}. why need set {{PigConfiguration.PIG_COMPRESS_INPUT_SPLITS}} in the configuration?,09/Aug/17 06:25;szita;[~kellyzly] because insisde {{readFields}} [compression configs are used|https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java#L307],09/Aug/17 06:46;kellyzly;[~szita]: thanks for your explaination. +1,"09/Aug/17 08:42;szita;[^PIG-5283.1.patch] committed to trunk. Thanks for the review Rohini, Liyun and Nandor",,,,,,,,,,,,,,,,,,,,,,,,,,,
Hit Ctrl-D to quit grunt shell fail,PIG-5254,13078141,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,wjqian,daijy,daijy,07/Jun/17 22:01,08/Aug/17 00:11,13/Mar/19 23:13,08/Aug/17 00:11,0.17.1,0.18.0,,,,,,,,,,,,,,,0.17.1,0.18.0,impl,,,0,,,,,,,,"Exception:
{code}
java.lang.NullPointerException
        at org.apache.pig.tools.grunt.ConsoleReaderInputStream$ConsoleLineInputStream.read(ConsoleReaderInputStream.java:107)
        at java.io.InputStream.read(InputStream.java:170)
        at java.io.SequenceInputStream.read(SequenceInputStream.java:207)
        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
        at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
        at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
        at java.io.InputStreamReader.read(InputStreamReader.java:184)
        at java.io.BufferedReader.fill(BufferedReader.java:161)
        at java.io.BufferedReader.read1(BufferedReader.java:212)
        at java.io.BufferedReader.read(BufferedReader.java:286)
        at org.apache.pig.tools.pigscript.parser.JavaCharStream.FillBuff(JavaCharStream.java:143)
        at org.apache.pig.tools.pigscript.parser.JavaCharStream.ReadByte(JavaCharStream.java:171)
        at org.apache.pig.tools.pigscript.parser.JavaCharStream.readChar(JavaCharStream.java:274)
        at org.apache.pig.tools.pigscript.parser.JavaCharStream.BeginToken(JavaCharStream.java:193)
        at org.apache.pig.tools.pigscript.parser.PigScriptParserTokenManager.getNextToken(PigScriptParserTokenManager.java:3215)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.jj_ntk(PigScriptParser.java:1511)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:117)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:66)
        at org.apache.pig.Main.run(Main.java:564)
        at org.apache.pig.Main.main(Main.java:175)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{code}",,,,,,,,,,,,,,,,,03/Aug/17 01:26;wjqian;PIG-5254.patch;https://issues.apache.org/jira/secure/attachment/12880145/PIG-5254.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-08-03 01:24:09.362,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Tue Aug 08 00:11:48 UTC 2017,,,,,,,0|i3g0b3:,9223372036854775807,,,,,,,PIG-5254,,,"03/Aug/17 01:24;wjqian;Cause:
When hit Ctrl + D in grunt shell, the method ConsoleLineInputStream.read() in ConsoleReaderInputStream.java will call reader.readline(), which will return null. In this case, if we use the null to call getBytes(), it will throw a NullPointerException.

Solution:
Define a temporary String buff to store the return value from reader.readline(), check if it is null(the reason why it returns null could be found in jline.console.ConsoleReader.readLine()), if yes, assign ""quit"" to the temp buff, and then it will function as if we input a quit in the console.

Possible Improvement: There do exist a EOF token, a quit() hook in PigScriptParser.jj, and a corresponding case of EOF could also be found in PigScriptParser.java, however the process of converting a bytestream we get from ConsoleLineInputStream to a token recognizable by PigScriptParser is not clear(relates to a bunch of java.io files). If we can figure out how it works, we may resolve this problem in a more elegant way.
https://issues.apache.org/jira/browse/PIG-5254",03/Aug/17 07:33;nkollar;Looks good to me.,08/Aug/17 00:11;daijy;Patch committed to both trunk and 0.17 branch. Thanks Weijun for contributing!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test failures because of PIG-5264,PIG-5278,13091254,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,satishsaley,satishsaley,31/Jul/17 17:22,04/Aug/17 13:46,13/Mar/19 23:13,02/Aug/17 13:54,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"Following unit tests are failing after commit cdd48d8c448221b2bde7f423dd26bbfc51102399
PIG-5264 https://github.com/apache/pig/commit/cdd48d8c448221b2bde7f423dd26bbfc51102399

# TestAutoLocalMode
# TestMultiQueryCompiler",,,,,,,,,,,,,,,,,02/Aug/17 12:14;nkollar;PIG-5264_2.patch;https://issues.apache.org/jira/secure/attachment/12880020/PIG-5264_2.patch,31/Jul/17 20:03;nkollar;PIG-5278_1.patch;https://issues.apache.org/jira/secure/attachment/12879702/PIG-5278_1.patch,31/Jul/17 17:23;satishsaley;TEST-org.apache.pig.test.TestAutoLocalMode.txt;https://issues.apache.org/jira/secure/attachment/12879666/TEST-org.apache.pig.test.TestAutoLocalMode.txt,31/Jul/17 17:25;satishsaley;TEST-org.apache.pig.test.TestMultiQueryCompiler.txt;https://issues.apache.org/jira/secure/attachment/12879667/TEST-org.apache.pig.test.TestMultiQueryCompiler.txt,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2017-07-31 20:16:24.447,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Fri Aug 04 13:46:16 UTC 2017,,,,,,,0|i3i7rj:,9223372036854775807,,,,,,,,,,"31/Jul/17 20:16;nkollar;[~rohini], [~knoguchi] could either of you please help with this? Looks like these two test cases fail in Tez mode, because these were executed only in MR mode before. The auto local mode test seems to check the logs, in case of Tez, I don't know what to check for, how can I tell that the script was executed in local mode? As for the multiquery test case, I think this test is written for MR only, if we'd like to execute it in Tez mode too, then we have to rewrite it.
Attached a patch, which now executes these only in MR mode (as before), should we open a new Jira to enable Tez exec type for these tests (or these cases are already covered by e2e tests)?","31/Jul/17 20:40;szita;..also what do we need to get the usual build working again? It's been almost 2 months now without a full test run in Jenkins and it always failed with env/conf errors during build-time:
https://builds.apache.org/job/Pig-trunk-commit/","02/Aug/17 12:02;rohini;Could you add these tests test/excluded-tests-mr  and test/excluded-tests-spark as well.  

bq. As for the multiquery test case, I think this test is written for MR only.  if we'd like to execute it in Tez mode too, then we have to rewrite it.
  Yes. It is MR specific as it tests MR plan. Not required to rewrite for Tez.

Auto local mode is not implemented in Tez or Spark. We should open separate jiras for that.
","02/Aug/17 12:16;nkollar;[~rohini] thanks, added the tests to the excludes files, and created two separate Jira to implement auto local mode for Spark and Tez","02/Aug/17 12:23;rohini;bq.also what do we need to get the usual build working again?
 Fixed it. Changed JDK (dropdown in Jenkins Configure) to JDK 1.8 (latest) from OpenJDK 7 (on Ubuntu only) and removed export JAVA_HOME

{code}
echo ""JAVA_HOME: ${JAVA_HOME}""
#export JAVA_HOME=/home/jenkins/jenkins-slave/workspace/Pig-trunk-commit/java-8-openjdk-amd64
{code} 

{code}
JAVA_HOME: /home/jenkins/tools/java/latest1.8
{code}

Also removed -Djava5.home=$JAVA5_HOME which is not defined in code anywhere.",02/Aug/17 13:54;rohini;+1. Committed to trunk.,"04/Aug/17 13:46;szita;Thanks for taking care of https://builds.apache.org/job/Pig-trunk-commit/, Rohini!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flakyness introduced by PIG-3655,PIG-5284,13092202,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,szita,szita,03/Aug/17 13:59,03/Aug/17 18:52,13/Mar/19 23:13,03/Aug/17 18:52,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"It seems like some tests are flaky after PIG-3655.

A recent error is:
{code}
Failed

org.apache.pig.test.TestBinInterSedes.testSyncMarkerOverlappingMarker

Failing for the past 1 build (Since Unstable#2523 )
Took 13 sec.
Error Message

Comparing actual and expected results.  expected:<[(apple,1,1), (kiwi,16909095,72624011372134400), (orange,2,2), (orange,4,4)]> but was:<[(apple,1,1), (kiwi,16909095,72624011372134400), (orange,2,2), (orange,4,4), (orange,4,4)]>
Stacktrace

junit.framework.AssertionFailedError: Comparing actual and expected results.  expected:<[(apple,1,1), (kiwi,16909095,72624011372134400), (orange,2,2), (orange,4,4)]> but was:<[(apple,1,1), (kiwi,16909095,72624011372134400), (orange,2,2), (orange,4,4), (orange,4,4)]>
	at org.apache.pig.test.Util.checkQueryOutputsAfterSortRecursive(Util.java:1290)
	at org.apache.pig.test.TestBinInterSedes.testInterStorageSyncMarker(TestBinInterSedes.java:428)
	at org.apache.pig.test.TestBinInterSedes.testSyncMarkerOverlappingMarker(TestBinInterSedes.java:350)
{code}

I've made the tests in TestBinInterSedes run for couple of hundred times and have spotted some failures that may come up depending on the generated random sync marker and data.",,,,,,,,,,,,,,,,,03/Aug/17 15:07;szita;PIG-5284.0.patch;https://issues.apache.org/jira/secure/attachment/12880247/PIG-5284.0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-08-03 18:42:49.816,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 03 18:52:45 UTC 2017,,,,,,,0|i3idl3:,9223372036854775807,,,,,,,,,,"03/Aug/17 15:15;szita;There were two issues I fixed in [^PIG-5284.0.patch] both of them inside {{InterRecordReader#skipUntilMarkerOrSplitEndOrEOF}}

One is that we were using 0 as default value of {{int b}} which is a valid first byte for a sync marker. I've set this now to Integer.MIN_VALUE. Without this we may end up having the same records appearing multiple times in the output.

The other problem was the handling of sync markers on split beginnings: when the next sync marker is exactly at the next split's beginning and the last byte before (in the previous data/record) is the same as the first byte of the marker we will read past the split end instead of stopping. This also results in same records appearing multiple times in the output.

[~rohini], [~nkollar] let me know what you think please",03/Aug/17 18:42;rohini;+1,03/Aug/17 18:52;szita;Thanks for reviewing so quickly [~rohini]! Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Modify bin/pig about SPARK_HOME, SPARK_ASSEMBLY_JAR after upgrading spark to 2",PIG-5246,13076127,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,31/May/17 08:25,25/Jul/17 01:55,13/Mar/19 23:13,25/Jul/17 01:55,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"in bin/pig.
we copy assembly jar to pig's classpath in spark1.6.
{code}
# For spark mode:
# Please specify SPARK_HOME first so that we can locate $SPARK_HOME/lib/spark-assembly*.jar,
# we will add spark-assembly*.jar to the classpath.
if [ ""$isSparkMode""  == ""true"" ]; then
    if [ -z ""$SPARK_HOME"" ]; then
       echo ""Error: SPARK_HOME is not set!""
       exit 1
    fi

    # Please specify SPARK_JAR which is the hdfs path of spark-assembly*.jar to allow YARN to cache spark-assembly*.jar on nodes so that it doesn't need to be distributed each time an application runs.
    if [ -z ""$SPARK_JAR"" ]; then
       echo ""Error: SPARK_JAR is not set, SPARK_JAR stands for the hdfs location of spark-assembly*.jar. This allows YARN to cache spark-assembly*.jar on nodes so that it doesn't need to be distributed each time an application runs.""
       exit 1
    fi

    if [ -n ""$SPARK_HOME"" ]; then
        echo ""Using Spark Home: "" ${SPARK_HOME}
        SPARK_ASSEMBLY_JAR=`ls ${SPARK_HOME}/lib/spark-assembly*`
        CLASSPATH=${CLASSPATH}:$SPARK_ASSEMBLY_JAR
    fi
fi

{code}
after upgrade to spark2.0, we may modify it",,,,,,,,,,,,,,,,,02/Jun/17 17:01;rohini;HBase9498.patch;https://issues.apache.org/jira/secure/attachment/12871010/HBase9498.patch,01/Jun/17 07:56;kellyzly;PIG-5246.1.patch;https://issues.apache.org/jira/secure/attachment/12870752/PIG-5246.1.patch,21/Jun/17 08:14;kellyzly;PIG-5246.3.patch;https://issues.apache.org/jira/secure/attachment/12873829/PIG-5246.3.patch,01/Jun/17 06:19;kellyzly;PIG-5246.patch;https://issues.apache.org/jira/secure/attachment/12870746/PIG-5246.patch,13/Jun/17 08:54;kellyzly;PIG-5246_2.patch;https://issues.apache.org/jira/secure/attachment/12872822/PIG-5246_2.patch,14/Jul/17 08:54;kellyzly;PIG-5246_4.patch;https://issues.apache.org/jira/secure/attachment/12877259/PIG-5246_4.patch,,,,,,6.0,,,,,,,,,,,,,,,,,,,2017-06-01 07:41:57.056,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 01:55:41 UTC 2017,,,,,,,0|i3fonz:,9223372036854775807,,,,,,,,,,"01/Jun/17 06:19;kellyzly;[~nkollar], [~szita]: help review 
in spark2, spark-assembly*.jar does not exist, so we need append all jars under $SPARK_HOME/jars/ to the pig classpath.
{code}
+    if [ ""$sparkversion"" == ""21"" ]; then
+          if [ -n ""$SPARK_HOME"" ]; then
+             echo ""Using Spark Home: "" ${SPARK_HOME}
+              for f in $SPARK_HOME/jars/*.jar; do
+                   CLASSPATH=${CLASSPATH}:$f
+              done
+          fi
+     fi
{code}

the way to use 

1. build pig with spark21
{noformat}
   ant clean -v  -Dsparkversion=21   -Dhadoopversion=2 jar
{noformat}
2. run pig with spark21
{noformat}
  /pig -x $mode -sparkversion 21 -log4jconf $PIG_HOME/conf/log4j.properties -logfile $PIG_HOME/logs/pig.log  $PIG_HOME/bin/testJoin.pig
{noformat}
  ",01/Jun/17 07:41;nkollar;[~kellyzly] one comment: Rohini suggested that instead of {{sparkversion=21}} and {{sparkversion=16}} we should use {{sparkversion=2}} and {{sparkversion=1}}. On [RB|https://reviews.apache.org/r/59530/] you can see that the latest patch is modified accordingly.,"01/Jun/17 07:56;kellyzly;[~nkollar]: in PIG-5246.1.patch, modify {{sparkversion=16}} to {{sparkversion=1}} and {{sparkversion=21}} to {{sparkversion=2}}","02/Jun/17 07:37;kellyzly;[~szita], [~nkollar], [~rohini] and [~jeffzhang]:
It is not very convenient to let users to type {{-sparkversion 2}} when they use pig like( the default sparkversion is 1, need not type)
{code}
./pig -x $mode -sparkversion 2 -log4jconf $PIG_HOME/conf/log4j.properties -logfile $PIG_HOME/logs/pig.log  $PIG_HOME/bin/testJoin.pig
{code} 
some options to improve this
1. save {{sparkversion}} in file and parse {{sparkversion}} from the file in bin/pig
2. judge the spark version from spark-assembly*jar. in spark1, there is spark-assembly*jar in $SPARK_HOME/lib while in spark2, there is no  $SPARK_HOME/lib/spark-assembly*jar

Please give me your opinion or you think it is acceptable to let users to specified ${{sparkversion}} in command.","02/Jun/17 17:00;rohini;Users should not have to specify -sparkversion 1 or 2 to determine which version. You should detect that in the script. For Hadoop 1.x and 2.x it was done by checking for hadoop-core.jar. You can do same thing here. Currently we still have problem of having to compile the shims classes against different versions.

There is a hack I did internally for hbase 0.94 to hbase 0.98 migration for HBaseStorage to support both HBase 0.94 and 0.98 with same pig jar during the migration. Have attached the patch for it. It is more code and slightly convoluted as each class now redirects to the shims class based on version detection. For eg: In Spark JobMetricsListener will redirect to JobMetricsListenerSpark1 or JobMetricsListenerSpark2. But for users it makes it very simple as they can use same pig installation to run against any version. [~nkollar], do you want to try this approach as part of PIG-5157 (Spark 2 support) and PIG-5191 (HBase 2 support) ?

 Similarly we can add a target to compile against all versions of both spark and hbase (and hadoop 3.0 in future if required) and create a pig.jar which will run with anything. 

","03/Jun/17 03:03;zjffdu;Agree with [~rohini], we should not ask user to specify spark version, it should be transparent to users. 
Actually SPARK_ASSEMBLY_JAR is not a must-have thing for spark. It is just for performance optimization. IMO, pig don't need to specify that, it is supposed to be set in spark-defaults.conf which would apply to all spark apps.



","05/Jun/17 01:55;kellyzly;[~rohini]: thanks for suggestion, for spark1 and spark2, it will be done by checking for spark-assembly.jar or other things in the script and user need not specify the version of spark.
bq. For eg: In Spark JobMetricsListener will redirect to JobMetricsListenerSpark1 or JobMetricsListenerSpark2. But for users it makes it very simple as they can use same pig installation to run against any version.
It will be convenient for users in that way but not sure whether there is conflicts if both jars of spark1 and spark2 in the pig classpath.
 [~zjffdu]:  
bq. Actually SPARK_ASSEMBLY_JAR is not a must-have thing for spark. 
  If SPARK_ASSEMBLY_JAR is not a must-have thing for spark1, how to judge spark1 or spark2?
bq.IMO, pig don't need to specify that, it is supposed to be set in spark-defaults.conf which would apply to all spark apps.
  Pig on Spark use spark installation and will copy $SPARK_HOME/lib/spark-assembly*jar(spark1) and $SPARK_HOME/jars/*jar to the classpath of pig. But we don't read spark-defaults.conf.  We parse pig.properties and save the configuration about spark to [SparkContext|https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/spark/SparkLauncher.java#L584].

 ","05/Jun/17 02:04;zjffdu;bq. If SPARK_ASSEMBLY_JAR is not a must-have thing for spark1, how to judge spark1 or spark2?
There's lot of ways to judge spark1 and spark2. e.g. we can run command 'spark-submit --version' under SPARK_HOME/bin to get the version number. 

bq. Pig on Spark use spark installation and will copy $SPARK_HOME/lib/spark-assembly*jar(spark1) and $SPARK_HOME/jars/*jar to the classpath of pig. But we don't read spark-defaults.conf. We parse pig.properties and save the configuration about spark to SparkContext.

Why copying the assembly jar instead of including it in the classpath of pig ? And it is also weird to me not loading spark-defaults.conf as this would cause extra administration overhead. If I am a cluster administrator, I only want to maintenance one copy of spark configuration in spark-defaults.conf, rather than copying the same configuration from spark-defaults.conf to pig.properties.

","05/Jun/17 02:21;kellyzly;[~jeffzhang]: thanks for suggestion
bq. Why copying the assembly jar instead of including it in the classpath of pig ?
sorry for mistake in my last comment. We just include the spark-assembly*.jar in the [classpath of pig|https://github.com/apache/pig/blob/trunk/bin/pig#L415]

bq. And it is also weird to me not loading spark-defaults.conf as this would cause extra administration overhead. 
yes, i agree this can be improved by directly parsing $SPARK_HOME/spark-defaults.conf. see PIG-5252



","06/Jun/17 15:31;nkollar;[~rohini] sure, I can try the approach you did for HBase.","13/Jun/17 08:56;kellyzly;[~zjffdu]:
in PIG-5246_2.patch
use following way to judge spark version
{code}
+    $SPARK_HOME/bin/spark-submit --version >/tmp/spark.version 2>&1
+    isSpark1=`grep ""version 1"" /tmp/spark.version|wc -l`
+    if [ ""$isSpark1"" -eq 0 ];then 
+          sparkversion=""2""
     fi
{code}

redirect the output of ""spark-submit --version"" to /tmp/spark.version(later will remove this file), Is there any better way to judge the version? ","15/Jun/17 08:39;kellyzly;[~nkollar]: can you help review PIG-5246_2.patch?

in PIG-5246_2.patch
use following way to judge spark version
{code}
+    $SPARK_HOME/bin/spark-submit --version >/tmp/spark.version 2>&1
+    isSpark1=`grep ""version 1"" /tmp/spark.version|wc -l`
+    if [ ""$isSpark1"" -eq 0 ];then 
+          sparkversion=""2""
     fi
{code}
redirect the output of ""spark-submit --version"" to /tmp/spark.version(later will remove this file), Is there any better way to judge the version?","15/Jun/17 09:53;nkollar;I've two comments:

For Spark 2.x do we have to add all jar under $SPARK_HOME/jars?

Could we avoid creating temp files? Instead of creating spark.version, would something like this work?
{code}
isSpark1=`$SPARK_HOME/bin/spark-submit --version 2>&1 | grep ""version 1"" | wc -l`
{code}","16/Jun/17 06:48;kellyzly;[~nkollar]:
  bq. For Spark 2.x do we have to add all jar under $SPARK_HOME/jars?
some guy suggested to to add all jar under $SPARK_HOME/jars in Hive on Spark([HIVE-15302|https://issues.apache.org/jira/browse/HIVE-15302]), It seems this is not accepted by [~vanzin]. But in [Hive wiki|https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started], it is said that we need not append all jars under $SPARK_HOME/jars.
{noformat}
Configuring Hive
To add the Spark dependency to Hive:
Prior to Hive 2.2.0, link the spark-assembly jar to HIVE_HOME/lib.
Since Hive 2.2.0, Hive on Spark runs with Spark 2.0.0 and above, which doesn't have an assembly jar.
To run with YARN mode (either yarn-client or yarn-cluster), link the following jars to HIVE_HOME/lib.
scala-library
spark-core
spark-network-common
To run with LOCAL mode (for debugging only), link the following jars in addition to those above to HIVE_HOME/lib.
chill-java  chill  jackson-module-paranamer  jackson-module-scala  jersey-container-servlet-core
jersey-server  json4s-ast  kryo-shaded  minlog  scala-xml  spark-launcher
spark-network-shuffle  spark-unsafe  xbean-asm5-shaded
{noformat}

 I don't know whether there is performance influence if we append all jar under $SPARK_HOME/jars to the pig classpath.
bq.Could we avoid creating temp files? Instead of creating spark.version, would something like this work?
yes, this works, thanks for suggestion.","16/Jun/17 22:01;rohini;bq. $SPARK_HOME/bin/spark-submit --version >/tmp/spark.version 2>&1
  This is a bad idea as it will launch a jvm which is costly. I would suggest checking for presence of spark-tags*.jar which is only present in Spark 2. If it is not present, then assume spark 1.

Also instead of doing
{code}
 SPARK_ASSEMBLY_JAR=`ls ${SPARK_HOME}/lib/spark-assembly*`
 CLASSPATH=${CLASSPATH}:$SPARK_ASSEMBLY_JAR
{code}

you should be able to just refer directly with wildcard
{code}
 CLASSPATH=${CLASSPATH}:${SPARK_HOME}/lib/spark-assembly*
{code}","17/Jun/17 01:13;zjffdu;Pig don't need to load all the jars under SPARK_HOME/jars. These jars are only needed when spark-submit script is launched, not necessary to be included in pig's classpath. Pig has already specify spark dependencies in ivy.

","19/Jun/17 04:50;kellyzly;[~rohini]:
bq. I would suggest checking for presence of spark-tags*.jar which is only present in Spark 2. If it is not present, then assume spark 1.
thanks for suggestion.
[~jeffzhang]:
bq. Pig don't need to load all the jars under SPARK_HOME/jars. Pig has already specify spark dependencies in ivy.
yes, the spark dependencies in ivy is for compile, i can select jars which pig on spark really needs from $SPARK_HOME/jars. But If users use a different spark which is different from compile. Will the dependencies be different?
My question is: is there big performance influence if we append all jar under $SPARK_HOME/jars to the pig classpath?","19/Jun/17 11:48;zjffdu;bq. My question is: is there big performance influence if we append all jar under $SPARK_HOME/jars to the pig classpath?
I am a little confused. My concern is why pig would need $SPARK_HOME/jars to the pig classpath. It doesn't bring any benefit including performance. ","20/Jun/17 15:36;rohini;bq. My concern is why pig would need $SPARK_HOME/jars to the pig classpath
  You need to have spark jars in classpath to run pig. This is similar to picking hadoop jars from HADOOP_HOME during runtime. We do not use the bundled hadoop/spark or tez jars to run and always use the user provided installation which may be Spark 1.x or 2.x. The dependencies specified in ivy.xml are only used during compile time as Liyun mentioned. 

bq. It doesn't bring any benefit including performance.
You can cherry pick jars to include in the classpath, but it is not going to make much difference. If a new jar is added in a new version, then it will actually be a problem and pig will have to be updated to include that jar.","21/Jun/17 01:31;kellyzly;[~rohini]:
bq.You can cherry pick jars to include in the classpath, but it is not going to make much difference. If a new jar is added in a new version, then it will actually be a problem and pig will have to be updated to include that jar.
thanks for explanation, that's the reason why I mentioned in previous comment ""But If users use a different spark which is different from compile. Will the dependencies be different?""","21/Jun/17 08:18;kellyzly;[~nkollar],[~szita],[~rohini],[~jeffzhang]: update PIG-5246.3.patch
changes
1. use spark-tags*.jar to verify whether current spark is spark1 or spark2
2. small fix like
{code}
SPARK_ASSEMBLY_JAR=`ls ${SPARK_HOME}/lib/spark-assembly*`
 CLASSPATH=${CLASSPATH}:$SPARK_ASSEMBLY_JAR

{code}

to
{code}
CLASSPATH=${CLASSPATH}:${SPARK_HOME}/lib/spark-assembly*
{code}","23/Jun/17 12:43;zjffdu;hmm, I just found that pig launch spark internally rather than using spark-submit script. This might cause some issues, Because spark-submit script will do some enviroment setup. e.g. I suspect pig on spark could not work in secured cluster. User can not specify keytab and principal. One approach to solve the issue is to launch pig grunt via spark-submit. In this way, pig don't need to specify any spark jars, spark-submit will pick up the jar automatically.  

","26/Jun/17 08:35;kellyzly;[~jeffzhang]: I don't know  pig on spark does not support secured cluster or not. But if the keytab and  principal is only transferred by the parameter of command spark-submit not by sparkContext, i guess it does not suport.","14/Jul/17 09:00;kellyzly;[~nkollar]:
changes in PIG-5246_4.patch:
{code}
CLASSPATH=${CLASSPATH}:${SPARK_HOME}/lib/spark-assembly*
{code}

to 
{code}
       SPARK_ASSEMBLY_JAR=`ls ${SPARK_HOME}/lib/spark-assembly*`
       CLASSPATH=${CLASSPATH}:$SPARK_ASSEMBLY_JAR

{code}

can not use wildcard to locate the spark_assembly_jar.
After all unit tests pass on my local jenkins.  will close PIG-5157","14/Jul/17 11:12;nkollar;[~kellyzly] does this mean, that the problem you mentioned in PIG-5157 with the basic script skew join is now fixed?",17/Jul/17 01:46;kellyzly;[~nkollar]: the problem about basic script passed on yarn-client mode. now running all unit tests in local mode. ,"20/Jul/17 01:37;kellyzly;[~nkollar]: as PIG-5157 is resolved, please help review PIG-5246_3.patch to let users use pig on spark in spark 2. thanks!","24/Jul/17 09:44;nkollar;[~kellyzly] sorry for the delay in the review. I tested the PIG-5246_4.patch on my cluster both in Spark 1.x and Spark 2.x mode. Both worked fine, looks good to me!","25/Jul/17 01:55;kellyzly;[~nkollar]: commit to trunk, thanks for [~nkollar], [~rohini], [~jeffzhang] reviewing.",,,,,,,,,,,,
TestEvalPipelineLocal#testSetLocationCalledInFE is failing in spark mode after PIG-5157,PIG-5274,13088722,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,nkollar,nkollar,20/Jul/17 11:42,22/Jul/17 11:29,13/Mar/19 23:13,22/Jul/17 11:29,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,20/Jul/17 15:43;nkollar;PIG-5274_1.patch;https://issues.apache.org/jira/secure/attachment/12878200/PIG-5274_1.patch,21/Jul/17 14:06;nkollar;PIG-5274_2.patch;https://issues.apache.org/jira/secure/attachment/12878352/PIG-5274_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-07-20 22:23:40.055,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 22 11:29:34 UTC 2017,,,,,,,0|i3hskv:,9223372036854775807,,,,,,,,,,20/Jul/17 22:23;rohini;Having to pass SparkShims to backend does not look nice. Can we instead fix SparkShims.getInstance() to work in backend as well.,"21/Jul/17 14:37;nkollar;[~rohini], [~szita] how about PIG-5274_2.patch? To tell the Spark version on frontend we use SparkContext and on backend we use JobContext, so we can avoid passing SparkSims to backend.","21/Jul/17 17:43;szita;[~nkollar] [^PIG-5274_2.patch] looks good to me, +1","22/Jul/17 11:29;szita;[^PIG-5274_2.patch] committed to trunk, thanks [~nkollar] for the fix!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition filter not pushed down when filter clause references variable from another load path,PIG-4767,12926850,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,erwaman,erwaman,03/Jan/16 23:47,19/Jul/17 17:40,13/Mar/19 23:13,13/Jul/17 18:29,0.15.0,,,,,,,,,,,,,,,,0.18.0,,,,,1,,,,,,,,"To reproduce:
{noformat:title=test.pig}
a = load 'a.txt';
a_group = group a all;
a_count = foreach a_group generate COUNT(a) as count;

b = load 'mytable' using org.apache.hcatalog.pig.HCatLoader();
b = filter b by datepartition == '2015-09-01-00' and foo == a_count.count;

dump b;
{noformat}
The above query ends up reading all the table partitions. If you remove the {{foo == a_count.count}} clause or replace {{a_count.count}} with a constant, then partition filtering happens properly.",,,,,,,,,,,,,,,,,28/Jun/17 20:17;knoguchi;pig-4767-v01.patch;https://issues.apache.org/jira/secure/attachment/12874951/pig-4767-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-12-12 20:54:33.611,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Wed Jul 19 17:40:27 UTC 2017,,,,,,,0|i2qqe7:,9223372036854775807,,,,,,,,,,12/Dec/16 20:54;knoguchi;Taking a look.,"28/Jun/17 20:26;knoguchi;Skipping of optimization with filter expression containing scalar was done at PIG-1669.  However, I don't think we need to apply this to {{PartitionFilterOptimizer}} and {{PredicatePushdownOptimizer}} since {{FilterExtractor}} that both optimizer depend on would keep the filter expression that contains {{ScalarExpression}}. Original LOFilter will be preserved with updated filter expression containing this scalar.

Reverted the changes in both optimizers and added a test for PartitionFilter with scalar.
(I looked at Predicate/Orc testing but didn't find an easy way to add this test.)

[~daijy], appreciate if you could take a look.
","11/Jul/17 16:51;erwaman;Thanks for the patch, [~knoguchi]! I tested it out and it solves my problem.","12/Jul/17 21:41;daijy;That's right, PartitionFilterOptimizer and PredicatePushdownOptimizer does not push filter up. The problem PIG-1669 try to solve does not exist. +1.","13/Jul/17 18:29;knoguchi;Thanks for the review Daniel! 
Committed it to trunk.","13/Jul/17 18:30;knoguchi;Forgot to mention, Anthony, sorry for taking this long to fix this bug.  ","14/Jul/17 19:59;erwaman;No problem, [~knoguchi]. Thanks for the fix!",19/Jul/17 17:40;knoguchi;Can we push this to 0.17 branch?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MapReduceLauncher and MRJobStats imports org.python.google.common.collect.Lists instead of org.google.common.collect.Lists,PIG-5269,13084527,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,nkollar,nkollar,04/Jul/17 12:03,07/Jul/17 08:39,13/Mar/19 23:13,07/Jul/17 08:39,0.17.0,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,04/Jul/17 12:05;nkollar;PIG-5269.patch;https://issues.apache.org/jira/secure/attachment/12875625/PIG-5269.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-07-07 08:39:41.106,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 07 08:39:41 UTC 2017,,,,,,,0|i3h2sn:,9223372036854775807,,,,,,,,,,04/Jul/17 12:06;nkollar;[~szita] could you please review this patch?,07/Jul/17 08:39;szita;[^PIG-5269.patch] committed to trunk. Thanks [~nkollar]!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Records Lost With Specific Combination of Commands and Streaming Function,PIG-4548,12829298,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,brane2,brane2,12/May/15 19:14,26/Jun/17 16:48,13/Mar/19 23:13,26/Jun/17 16:48,0.12.0,0.14.0,,,,,,,,,,,,,,,0.17.1,0.18.0,,,,1,,,,,,,,"The below is the bare minimum I was able to extract from my original
problem to in order to demonstrate the bug.  So, don't expect the following
code to serve any practical purpose.  :)

My input file (test_in) is two columns with a tab delimiter:

1   F
2   F

My streaming function (sf.py) ignores the actual input and simply generates
2 records:

#!/usr/bin/python
if __name__ == '__main__':
    print 'x'
    print 'y'

(But I should mention that in my original problem the input to output was
one-to-one.  I just ignored the input here to get to the bare minimum
effect.)

My pig script:

MY_INPUT = load 'test_in' as ( f1, f2);
split MY_INPUT into T if (f2 == 'T'), F otherwise;
T2 = group T by f1;
store T2 into 'test_out/T2';
F2 = group F by f1;
store F2 into 'test_out/F2';  -- (this line is actually optional to demo
the bug)
F3 = stream F2 through `sf.py`;
store F3 into 'test_out/F3';

My expected output for test/out/F3 is two records that come directly from
sf.py:

x
y

However, I only get:

x

I've tried all of the following to get the expected behavior:

   - upgraded Pig from 0.12.0 to 0.14.0
   - local vs. distributed mode
   - flush sys.stdout in the streaming function
   - replace sf.py with sf.sh which is a bash script that used ""echo x;
   echo y"" to do the same thing.  In this case, the final contents of
   test_out/F# would vary - sometimes I would get both x and y, and sometimes
   I would just get x.

Aside from removing the one Pig line that I've marked optional, any other
attempts to simplify the Pig script or input file causes the bug to not
manifest.

Log files can be found at http://www.mail-archive.com/user@pig.apache.org/msg10195.html",Amazon EMR (Elastic Map-Reduce) AMI 3.3.1,,,,,,,,,,,,,,,,05/Jun/17 20:33;knoguchi;pig-4548-v1.patch;https://issues.apache.org/jira/secure/attachment/12871295/pig-4548-v1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-06-05 20:33:28.499,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Mon Jun 26 16:48:22 UTC 2017,,,,,,,0|i2emz3:,9223372036854775807,,,,,,,,,,"05/Jun/17 20:33;knoguchi;Noticed this jira during reflagging to 0.18.
Any incorrect results should be treated as priority.  I've missed this one.

Using the test-case attached on the description, I was able to reproduce the behavior on trunk but only for map-reduce mode and not tez.

Tracing the issue, it seems like a bug in {{PODemux}} when all input has been sent, PODemux assumes there's at most one output to be consumed in {{getStreamCloseResult()}}.  
Attaching a patch that would continue to read till EOP.
PhysicalPlan is not my strength and PODemux.java hasn't been touched for years.  Appreciate if [~daijy] or [~rohini] can take a look.","06/Jun/17 15:16;brane2;Thanks so much, Koji!  This bug has been a thorn in my side for al long time.  I appreciate any extra attention that it gets.  I have many Pig scripts in production where I have to use inefficient work-arounds like storing and reloading to avoid loosing data.  For the longest time I thought I was the only one who could see this problem.  Maybe no one else uses streaming functions as heavily as I do.  :)","20/Jun/17 20:47;knoguchi;[~daijy], when you have time, can you take a look at my patch? ","23/Jun/17 20:45;rohini;+1. Patch looks good.  

I noticed an inefficiency that we are attaching input to just one child plan based on record index but instead of just processing that plan, we are iterating through all child plans and processing them. Not so familiar with PODemux. So not suggest to optimize it now. Might break something like case of nested PODemux. 

bq. I was able to reproduce the behavior on trunk but only for map-reduce mode and not tez
 PODemux is only used for multi-query. For eg: Combining multiple groupbys on same input into one hadoop job which is the case mentioned in the jira.  We do not use PODemux operator at all in Tez. Tez supports multiple outputs and inputs. So multi-query processing is very different with it. In this case, it will create three vertices. 
{code}
           V1 (Load) 
           /\
          /  \
         /    \
       V2     V3  
{code}
where V2 is the reducer for T2 groupby and V3 for F2 group by.

bq. I have many Pig scripts in production where I have to use inefficient work-arounds like storing and reloading to avoid loosing data.
  That is bad. As [~knoguchi] said, we always treat data loss as critical. Sorry we missed fixing this earlier. There will sure be others who are experiencing this, but must not have not noticed that records are lost. ","26/Jun/17 16:48;knoguchi;Thanks for the review Rohini. 
Committed to Trunk(0.18) and 0.17-branch. 

Thanks [~brane2] for reporting this critical issue!!!
(and sorry again for missing this for such a long time.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix jdiff related issues: fail build upon error, correct xml character escaping",PIG-5262,13081099,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,szita,szita,szita,20/Jun/17 11:40,21/Jun/17 10:20,13/Mar/19 23:13,21/Jun/17 10:20,,,,,,,,,,,,,,,,,0.17.1,,,,,0,,,,,,,,"For some Scala classes jdiff produces returntype=""<any>"". Then it fails parsing the XML because of the < and > characters. Need to fix this, and also fail the build if such thing happens, rather than having it continue and end as ""BUILD SUCCESSFUL"":

{code} 
[javadoc] JDiff: reading the new API in from file '/Users/szita/shadow/apache/pig/17RC/src/docs/jdiff/pig_0.17.0-SNAPSHOT.xml'...Fatal Error (30267): parsing XML API file:org.xml.sax.SAXParseException; systemId: file:///Users/szita/shadow/apache/pig/17RC/src/docs/jdiff/pig_0.17.0-SNAPSHOT.xml; lineNumber: 30267; columnNumber: 40; The value of attribute ""return"" associated with an element type ""method"" must not contain the '<' character.

BUILD SUCCESSFUL
Total time: 43 seconds
{code}",,,,,,,,,,,,,,,,,20/Jun/17 11:43;szita;PIG-5262.0.patch;https://issues.apache.org/jira/secure/attachment/12873659/PIG-5262.0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-06-20 11:48:25.007,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 21 10:20:05 UTC 2017,,,,,,,0|i3ghov:,9223372036854775807,,,,,,,,,,20/Jun/17 11:48;nkollar;Looks good to me,20/Jun/17 20:16;rohini;+1,21/Jun/17 10:20;szita;Committed to branch-0.17 and trunk. Thanks for review Nandor and Rohini,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several unit tests are not annotated with @Test,PIG-5225,13064797,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,nkollar,nkollar,18/Apr/17 17:08,01/Jun/17 19:54,13/Mar/19 23:13,01/Jun/17 19:54,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"Several test cases are not annotated with @Test. Since we use JUnit 4, these test cases seems to be excluded.",,,,,,,,,,,,,,,,,18/Apr/17 17:12;nkollar;PIG-5225.patch;https://issues.apache.org/jira/secure/attachment/12863850/PIG-5225.patch,31/May/17 22:07;nkollar;PIG-5225_2.patch;https://issues.apache.org/jira/secure/attachment/12870675/PIG-5225_2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-05-14 05:55:05.615,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Thu Jun 01 19:54:31 UTC 2017,,,,,,,0|i3dren:,9223372036854775807,,,,,,,,,,"18/Apr/17 17:17;nkollar;[~rohini], [~daijy] I noticed that several test cases are not annotated with @Test, do you recall why were these tests excluded? Some might be covered by E2E tests, but for example the omitted test case in TestHBaseStorage is not.","14/May/17 05:55;rohini;bq. do you recall why were these tests excluded?
  Most likely done by mistake.

+1. Will run the tests and commit.","14/May/17 20:24;rohini;[~nkollar], 
   One of the newly enabled tests is failing. Can you take a look at it?
{code}
Testcase: testMapUDFFail took 3.435 sec
        FAILED
Error expected.
junit.framework.AssertionFailedError: Error expected.
        at org.apache.pig.test.TestEvalPipelineLocal.testMapUDFFail(TestEvalPipelineLocal.java:845)
{code}","17/May/17 15:32;nkollar;[~rohini]
Could you please help to solve this failure? In the test, we expected failure, but the script doesn't fail: instead the output is an empty tuple. Is it the expected outcome in this case? If we have a map with a byte array value for a key, and try to multiply this value with an integer then it should fail, or result in an empty tuple?","29/May/17 01:58;rohini;bq.  If we have a map with a byte array value for a key, and try to multiply this value with an integer then it should fail, or result in an empty tuple?
  Considering that the bytearray which is a string cannot be converted to a integer, you would expect it to fail. But Utf8StorageConverter, just increments PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED  for type conversion failures and returns null. I believe it was done to avoid script failing for few invalid values as that is a common scenario. So empty tuple is expected. The test seems to have been added before the code was added to Apache svn. So couldn't find the history of why it was added.

[~daijy], 
   Can you confirm the expected behavior and is it ok to remove this test?

","31/May/17 21:38;daijy;The test is added even before me. The test won't throw exception, it will get a null result and a warning counter like Rohini points out. However, the test name suggest it is testing a failed UDF. I don't this is valid anymore and fine to remove it.","31/May/17 22:08;nkollar;Ok, uploaded PIG-5225_2.patch with the test case removed.",01/Jun/17 19:54;rohini;Committed to trunk. Thanks [~nkollar] for identifying this and fixing it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge spark branch to trunk,PIG-4854,12954196,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,pallavi.rao,pallavi.rao,29/Mar/16 05:19,04/May/17 01:34,13/Mar/19 23:13,04/May/17 01:34,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Believe the spark branch will be shortly ready to be merged with the main branch (couple of minor patches pending commit), given that we have addressed most functionality gaps and have ensured the UTs are clean. There are a few optimizations which we will take up once the branch is merged to trunk.

[~xuefuz], [~rohini], [~daijy],
Hopefully, you agree that the spark branch is ready for merge. If yes, how would like us to go about it? Do you want me to upload a huge patch that will be merged like any other patch or do you prefer a branch merge?",,,,,,,,,,,,,,,,,04/Apr/16 05:24;pallavi.rao;PIG-On-Spark.patch;https://issues.apache.org/jira/secure/attachment/12796773/PIG-On-Spark.patch,26/Oct/16 08:48;kellyzly;PigOnSpark_3.patch;https://issues.apache.org/jira/secure/attachment/12835289/PigOnSpark_3.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-03-29 16:37:14.081,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Thu May 04 01:34:30 UTC 2017,,,,,,,0|i2vbsf:,9223372036854775807,,,,,,,,,,29/Mar/16 16:37;rohini;Huge patch in review board. It needs to go through review before it can be merged to trunk.,"06/Apr/16 18:00;rohini;Will do 50% of it this weekend and rest next week. Going to take at least two weeks as me and Daniel are not very familiar with Spark and the patch is really huge. We will try to speed it up by focusing first on the core changes and then Spark changes.  

 [~dvryaboy] and [~julienledem],
    If you folks get time, please do a very high level review to see how the design has come out. Though it started with spork code base developed by you folks, it has undergone a lot of change.","12/Apr/16 07:14;pallavi.rao;[~rohini], [~daijy], [~xuefuz], while this patch is getting reviewed, we don't want to hold off on committing patches to the spark branch.  So, you will probably see a smaller cumulative patch right after this one goes in. Or, do you have any other suggestion?","12/Apr/16 13:10;xuefuz;Yes, we can still commit to Spark branch and later merge back to trunk.","12/Apr/16 14:44;rohini;bq. So, you will probably see a smaller cumulative patch right after this one goes in. 
  That should be fine. It is easy to review just the changes in the new diff with review board. You can update the current patch or put it as a new patch for merging later too. ","03/May/16 01:09;kellyzly;[~xuefuz],[~rohini] and [~daijy]: Any feedback about merging spark branch to trunk? from the mail [~rohini] sent to the community, It is said that community would first create 0.16 release, then merge spark branch to trunk.",05/May/16 15:17;rohini;Sorry. I did start with the 0.16 release work and Spark review but was out sick with Flu most of last week and have had time to only catch up with internal Tez issues this week. Will complete the review this weekend. Spoke with [~daijy]. I will do the first round of review and [~daijy] will take a look at the second revision after my comments are addressed.,"17/May/16 01:30;kellyzly;[~rohini],[~xuefuz] and [~daijy]:  Any comments about the patch? I know that it needs long time to review this big patch. If you have any comment, please comment it on the review board then i can do some work in my local environment concurrently.","22/May/16 22:01;rohini;Apologies again for the delay. I commented on all the non-spark classes and hope that gives you something to work on. Will complete the spark ones by Wed. Once full pass of the first revision is done, review of further revisions should be really fast.","11/Jul/16 06:26;kellyzly;[~rohini] :  have updated the patch for the first round of review. Please review, thanks!","26/Oct/16 08:48;kellyzly;[~rohini], [~daijy],[~xuefuz]: update PigOnSpark_3.patch.
modification between PigOnSpark_3.patch and previous patch:
1. revert modification to core code(PigContext.java TestHBaseStorage.java) after PIG-4920 because we don;t store UDFContext properties in PigContext any more in spark mode.",04/May/17 01:34;kellyzly;duplicate with PIG-5215,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the RuntimeException throws in SecondaryKeySortUtil,PIG-5230,13067258,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,27/Apr/17 06:31,28/Apr/17 08:55,13/Mar/19 23:13,28/Apr/17 08:55,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"there is possibility that [curKey is null| https://github.com/apache/pig/blob/63968e3132ad1fee06dffcacb8ea5d399e0edef5/src/org/apache/pig/backend/hadoop/executionengine/spark/converter/SecondaryKeySortUtil.java#L116] after PIG-5164.  we should remove the code to avoid RuntimeException.


following script can trigger the exception.
{code}
a = load './studenttab10k.mk1' as (name, age:int, gpa:float);
a1 = filter a by gpa is null or gpa >= 3.9;
a2 = filter a by gpa < 2;
b = union a1, a2;
c = load './voternulltab10k' as (name, age, registration, contributions);
d = join b by name left outer, c by name using 'replicated';
e = stream d through `cat` as (name, age, gpa, name1, age1, registration, contributions);
f = foreach e generate name, age, gpa, registration, contributions;
g = group f by name;
g1 = group f by name; -- Two separate groupbys to ensure secondary key partitioner
h = foreach g { 
    inner1 = order f by age, gpa, registration, contributions;
    inner2 = limit inner1 1;
    generate inner2, SUM(f.age); };
i = foreach g1 {
    inner1 = order f by age asc, gpa desc, registration asc, contributions desc;
    inner2 = limit inner1 1;
    generate inner2, SUM(f.age); };
store h into './MultiQuery_Union_3.1.out';
store i into './MultiQuery_Union_3.2.out';
{code}


cat studenttab10k.mk1
{code}
ulysses thompson	64	1.90
katie carson	25	3.65
	65	0.73
holly davidson	57	2.43
fred miller	55	3.77

{code}",,,,,,,,,,,,,,,,,27/Apr/17 12:57;szita;PIG-5230.2.patch;https://issues.apache.org/jira/secure/attachment/12865358/PIG-5230.2.patch,27/Apr/17 06:32;kellyzly;PIG-5230.patch;https://issues.apache.org/jira/secure/attachment/12865288/PIG-5230.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-04-27 13:00:35.641,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 28 08:52:57 UTC 2017,,,,,,,0|i3e5zr:,9223372036854775807,,,,,,,,,,27/Apr/17 06:32;kellyzly;[~szita]: help review.,"27/Apr/17 13:00;szita;[~kellyzly] right, I left this untouched. I think the intent here was to signal if no tuples were processed by AccumulateByKey function. Since I started using the {{initialzed}} flag I think we should keep depending on that. See [^PIG-5230.2.patch]","28/Apr/17 08:52;kellyzly;[~szita]: +1, ready to commit to spark branch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Streaming does not untar cached files,PIG-5193,13058123,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,vparakh,vparakh,22/Mar/17 06:54,22/Mar/17 19:50,13/Mar/19 23:13,22/Mar/17 19:50,0.12.0,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Pig Streaming does not seem to untar files passed through cache(). Is this an expected behavior. I 

This is my code : 
DEFINE streamCLF1 `/usr/local/bin/python2.7 mapper2.py $OUTPUT '$DEVICE1' $LATEST_WEEKLY_PROFILE_DT` input(stdin using PigStreaming('\u0001')) ship('mapper2.py') cache('$OUTPUT/models/models.tar.gz#models');

In my python udf, when I print the contents of the current directory, I can see the jar file. But I get the following error when trying to access the symlink(models). 

Traceback (most recent call last):
  File ""mapper2.py"", line 7, in <module>
    sys.stderr.write('****' + repr(os.listdir('./models'))+'\n')
OSError: [Errno 20] Not a directory: './models'
",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,2017-03-22 06:54:16.0,,,,,,,0|i3cm9b:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Backport PIG-4916, PIG-4921 and PIG-4957 to 0.16 branch",PIG-5121,13039356,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,01/Feb/17 01:33,08/Feb/17 07:15,13/Mar/19 23:13,08/Feb/17 07:15,,,,,,,,,,,,,,,,,0.16.1,,,,,0,,,,,,,,"We see Pig hanging on shutdown hook intermittently. Need to backport PIG-4916, PIG-4921 and PIG-4957 to branch 0.16. The original patch does not work with Hadoop 1 and need some rework. Create the ticket to backport them together as 3 patches are overlapping each other.",,,,,,,,,,,,,,,,,01/Feb/17 01:34;daijy;PIG-5121-1.patch;https://issues.apache.org/jira/secure/attachment/12850354/PIG-5121-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-02-08 00:02:53.645,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Wed Feb 08 07:15:42 UTC 2017,,,,,,,0|i39g3b:,9223372036854775807,,,,,,,,,,08/Feb/17 00:02;rohini;+1,08/Feb/17 07:15;daijy;Patch committed to 0.16 branch. Thanks Rohini for review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileBasedOutputSizeReader does not calculate size of files in sub-directories,PIG-3891,12708343,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nkollar,rohini,rohini,14/Apr/14 19:01,02/Dec/16 17:38,13/Mar/19 23:13,02/Dec/16 17:28,0.12.0,,,,,,,,,,,,,,,,,,,,,1,,,,,,,,"FileBasedOutputSizeReader only includes files in the top level output directory. So if files are stored under subdirectories (For eg: MultiStorage), it does not have the bytes written correctly. 

0.11 shows the correct number of total bytes written and this is a regression. A quick look at the code shows that the JobStats.addOneOutputStats() in 0.11 also does not recursively iterate and code is same as  FileBasedOutputSizeReader. Need to investigate where the correct value comes from in 0.11 and fix it in 0.12.1/0.13.",,,,,,,,,,,,,,,,,25/Apr/14 00:29;chitnis;PIG-3891-1.patch;https://issues.apache.org/jira/secure/attachment/12641840/PIG-3891-1.patch,30/May/14 00:53;chitnis;PIG-3891-2.patch;https://issues.apache.org/jira/secure/attachment/12647500/PIG-3891-2.patch,20/Jul/16 13:17;nkollar;PIG-3891-3.patch;https://issues.apache.org/jira/secure/attachment/12819080/PIG-3891-3.patch,12/Sep/16 10:33;nkollar;PIG-3891-4.patch;https://issues.apache.org/jira/secure/attachment/12828019/PIG-3891-4.patch,02/Dec/16 13:50;nkollar;PIG-3891-5.patch;https://issues.apache.org/jira/secure/attachment/12841486/PIG-3891-5.patch,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-04-23 01:10:35.5,,,no_permission,,,,,,,,,,,,,386666,Reviewed,,,Fri Dec 02 17:38:57 UTC 2016,,,,,,,0|i1ulzj:,386930,,,,,,,,,,"23/Apr/14 01:10;chitnis;Linking the original JIRA introducing this change. The issue is probably in reporting the counters as a whole as I'm getting the following output for a sample pig test (map-reduce mode of course), even though its successful and produced output successfully.

{quote}
Input(s):
Successfully read 0 records from: ""/user/pig/tests/data/pigmix/page_views""

Output(s):
Successfully stored 0 records in: ""/user/chitnis//L1out""

Counters:
Total records written : 0
Total bytes written : 0
Spillable Memory Manager spill count : 0
Total bags proactively spilled: 0
Total records proactively spilled: 0
{quote}",23/Apr/14 20:56;chitnis;above observation was due to a misconfigured single-node cluster. Now able to reproduce that problem lies only in output records and bytes written,"24/Apr/14 19:04;rohini;bq. 0.11 shows the correct number of total bytes written and this is a regression. A quick look at the code shows that the JobStats.addOneOutputStats() in 0.11 also does not recursively iterate and code is same as FileBasedOutputSizeReader. Need to investigate where the correct value comes from in 0.11 and fix it in 0.12.1/0.13

  It is not a regression. I was checking case of single store (in 0.11) vs multi store (in trunk). Single store always uses the mapreduce counter to get the hdfs bytes written and does not use FileBasedOutputSizeReader. ","25/Apr/14 00:29;chitnis;Attaching patch. End-to-end test done - using MultiStorage, split and both.
Example output:
{code}
Output(s):
Successfully stored 4 records (94 bytes) in: ""hdfs://localhost:8020/user/chitnis/split1""
Successfully stored 2 records (47 bytes) in: ""hdfs://localhost:8020/user/chitnis/split2""
Successfully stored 3 records (60 bytes) in: ""hdfs://localhost:8020/user/chitnis/split3""
{code}

Unit tests included in patch, but can go through minor tweaks",25/Apr/14 00:30;chitnis;Attaching fix for class JobStats.java (pig 0.11),"29/May/14 22:24;rohini;Few comments:
 - You can't refer to piggybank classes from pig.
 - getLeavesSize -> getPathSize
 - If the path is a file and not directory it will not get the filesize","30/May/14 00:53;chitnis;wouldnt this block take care of path being file  and adding its length directly?
{code}
 if (status.isFile()) {
       if (status.getLen() > 0)
            bytes += status.getLen();
 }
 else { // recursively count nested leaves' (files) sizes
      bytes += getPathSize(status.getPath(), fs);
 }
{code}

I moved the unit tests to piggybank's TestMultiStorage class then and cleaned up the i/o. Patch rebased to trunk","04/Nov/15 09:14;egmont@c;Hi,

Friendly ping – any updates on this?","20/Jul/16 13:31;nkollar;The patch for FileBasedOutputSizeReader looks good for me, but I think a new test case is required in TestMRJobStats to test this case not just in piggybank. I attached a 3rd version of this patch with a new test case with and changed the MultiStore test case too: it seems that FileBasedOutputSizeReader is used when the script is executed in batch mode, and it stores the result in multiple stores (using MultiStores in subdirectories), and not just in one (for just one store command, the mapreduce counters are taken into account).

[~rohini] wouldn't testGetOutputSizeUsingFileBasedStorage in TestMRJobStats test the filesize if it is a file and not a path? With the patch applied, this test is green.","21/Jul/16 05:22;rohini;Few comments:
  - Could you fix this to run in Tez mode as well? Did not realize this one was not fixed.
{code}
pigServer = new PigServer(cluster.getExecType(), cluster.getProperties());
     pigServerLocal = new PigServer(ExecType.LOCAL);
{code}

to 

{code}
pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
     pigServerLocal = new PigServer(Util.getLocalTestMode());
{code}

 It would also involve changes to the test like MRJobStats->JobStats, etc. You can test by running with ant test -Dhadoopversion=23 -Dexectype=tez -Dtestcase=TestMultiStorage

- Can you add asserts for getMultiStoreCounters() as well for the individual output bytes written
- Test name is too verbose. Could you rename the test as just testOutputStats and add a comment in the beginning of the test saying 
//Test if bytes written is correct with sub-directories and multiple MultiStorage statements.

Rest looks good.

","12/Sep/16 10:51;nkollar;Attached patch:
- executed TestMultiStorage in Tez mode, after minor adjustments it passed in Tez mode too. When I executed the tests, it seemed that in Tez mode the statistics written to the console are not collected via FileBasedOutputSizeReader, I could see the correct values there even without the fix, but in MR mode the console output was incorrect without the recursive traversal fix in FileBasedOutputSizeReader. I don't know what kind of changes I should do in MRJobStats, JobStats, those tests passed even in Tez and in MR mode.
- in TestMultiStorage I added asserts for getMultiStoreCounters
- renamed the method, added a comment
- in addition, I fixed typos in methods in TestMRJobStats.java

[~rohini] could you please take a look at the latest patch?","04/Nov/16 14:50;nkollar;[~rohini], [~daijy] could you please review my patch, and let me know if it needs some more adjustment?","30/Nov/16 12:25;szita;LGTM, [~rohini] can you please help [~nkollar] by committing this?","01/Dec/16 17:47;rohini;Comments:
     - CHANGES.txt will be modified when committing. Need not make any changes to that as part of patch
     - Please revert changes to ExecType and TezMiniCluster. We can't have public static changed to package protected as it is already being used by users. Once PIG-4923 goes in, we can add TEZ and SPARK there. 
   - In TestMRJobStats, can you change ""The returned output size is expected to be the same as the file size"" to ""The returned output size is expected to be sum of file sizes in the sub-directories""
   - We try to avoid if (Tez) else (MR) conditions as much as possible in tests. For testOutputStats test in TestMultiStorage, can we just do following asserts and put hardcoded values instead of getting values from MR and Tez counters. That way test is more solid.  Also please do add a FILTER statement for out2 to filter couple of records so that bytes and records are not same as out1.  
{code}
Map<String, Long> multiStoreCounters = dagStats.getMultiStoreCounters();
+        PigStats stats = job.getStatistics();
+        assertEquals(HardCodedValueHere, stats.getBytesWritten());
+        List<OutputStats> outputStats = SimplePigStats.get().getOutputStats();
+        assertEquals(2, outputStats.size()); // 2 split conditions
+        assertEquals(HardCodedValueHere, outputStats.get(0).getBytes());
+        assertEquals(HardCodedValueHere, outputStats.get(1).getBytes());
+        assertEquals(HardCodedValueHere, outputStats.get(0).getRecords());
+        assertEquals(HardCodedValueHere, outputStats.get(1).getRecords());
+        assertEquals(9L, multiStoreCounters.get(""Output records in _1_out2"").longValue());
+        assertEquals(9L, multiStoreCounters.get(""Output records in _0_out1"").longValue());
{code}
","02/Dec/16 13:43;nkollar;Thanks [~rohini] for your comments, I made the required adjustments:
- Reverted CHANGES.txt
- Reverted the changes on ExecType and TezMiniCluster, but I think my patch didn't change the visibility of the exec types, since those were declared inside an interface.
- Changed the assert message.
- Rewrote the assert as you suggested, indeed, it looks better without the branch. Test passes both on Tez and MR mode.

Attached new version: PIG-3891-5.patch. Could you please have a look at it?","02/Dec/16 17:28;rohini;Thanks for the changes. Patch is perfect now. I missed that ExecType was an interface. So that change should not have been a problem.

+1.  Committed to trunk. 

",02/Dec/16 17:38;nkollar;Thanks Rohini for committing and reviewing my patch!,,,,,,,,,,,,,,,,,,,,,,,,
A simple map reference fail to cast,PIG-4974,13001832,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,31/Aug/16 21:43,08/Sep/16 19:42,13/Mar/19 23:13,08/Sep/16 19:42,,,,,,,,,,,,,,,,,0.16.1,,,,,0,,,,,,,,"{code}
A = load 'input.txt' as (m:[bytearray]);
B = FOREACH A GENERATE m#'a' as a, m#'e' as e;
C = FILTER B by a != 'abc' and e != 'abc';
store C into 'output';
{code}

This fails with 
{panel}
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF or Union from two different Loaders. Cannot determine how to convert the bytearray to string
{panel}",,,,,,,,,,,,,PIG-3938,,,,31/Aug/16 22:01;knoguchi;pig-4974-type1-v01.patch;https://issues.apache.org/jira/secure/attachment/12826504/pig-4974-type1-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-09-01 07:10:44.591,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Thu Sep 08 19:42:28 UTC 2016,,,,,,,0|i333hj:,9223372036854775807,,,,,,,,,,"31/Aug/16 21:47;knoguchi;Looking at the logicalplan 

{code}
    |---B: (Name: LOForEach Schema: a#2:bytearray,e#16:bytearray)
        |   |
        |   (Name: LOGenerate[false,false] Schema: a#2:bytearray,e#16:bytearray)ColumnPrune:OutputUids=[16, 2]ColumnPrune:InputUids=[1]
        |   |   |
        |   |   (Name: Map Type: null Uid: null Key: a)
        |   |   |
        |   |   |---m:(Name: Project Type: map Uid: 1 Input: 0 Column: (*))
        |   |   |
        |   |   (Name: UserFunc(org.apache.pig.impl.builtin.IdentityColumn) Type: bytearray Uid: 16)
        |   |   |
        |   |   |---(Name: Map Type: null Uid: null Key: e)
        |   |       |
        |   |       |---m:(Name: Project Type: map Uid: 1 Input: 1 Column: (*))
        |   |
        |   |---(Name: LOInnerLoad[0] Schema: m#1:map(#25:bytearray))
        |   |
        |   |---(Name: LOInnerLoad[0] Schema: m#1:map(#25:bytearray))
        |
{code}

this seems to be coming from the implicit {{org.apache.pig.impl.builtin.IdentityColumn}} that is inserted for avoiding uid overlap.","31/Aug/16 22:01;knoguchi;From simple to complex, I can think of a couple of ways of fixing this.

(1) Take IdentityColumn UDF as a special case and forward loadcaster  from the argument.   Attaching a patch for this (pig-4974-type1-v01.patch).

(2) Extend (1)  and include any UDF that returns true for {{forwardLoadCaster}} boolean method.  This may help for udf like MapToBag discussed in PIG-3938.

(3) Extend (2) to allow udf to specify its own loadcaster. 

I don't want to go to (3) for now.  [~daijy], let me know your thoughts on this.",01/Sep/16 07:10;daijy;+1 for (1) for this issue. We can do it later but we need (3) as long term solution.,08/Sep/16 19:13;rohini;+1.  [~knoguchi] can we commit this? (3)  can be pursued later as part of PIG-3938,08/Sep/16 19:25;knoguchi;Sure.  I'll commit to 0.16 branch and trunk in a minute.,"08/Sep/16 19:42;knoguchi;Committed to 0.16 branch and trunk.

Thanks for the review Daniel, Rohini! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix mvn-install ant target,PIG-4942,12986493,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,petersla,petersla,petersla,03/Jul/16 22:51,17/Jul/16 18:13,13/Mar/19 23:13,17/Jul/16 18:13,0.16.0,,,,,,,,,,,,,,,,0.16.1,,build,,,0,,,,,,,,"The mvn-install ant target which allows pig artifacts to be placed in the local repository became broken by PIG-4499 in Pig 0.15. That patch removed the mvn-jar target which the mvn-install target had dependent on. Instead, the jar-h12 target was used as a replacement. However, this target is insufficient as it does not move built jars to their expected output locations and does not handle well the case where we are building Pig only for Hadoop 2 via the {{-Dhadoopversion=23}} option. 

There is an important detail to make note. The {{jar-h12}} target uses propertyreset scripts to change the hadoopversion property before it calls ant targets to change the behavior of the ant targets. These propertyreset scripts do nothing when the {{-Dhadoopversion}} option is provided through the command line. This is because ant does not allow overwriting user properties.",,,,,,,,,,,,,,,,,03/Jul/16 22:55;petersla;PIG-4942.1.patch;https://issues.apache.org/jira/secure/attachment/12815966/PIG-4942.1.patch,11/Jul/16 04:19;petersla;PIG-4942.2.patch;https://issues.apache.org/jira/secure/attachment/12817078/PIG-4942.2.patch,17/Jul/16 07:47;daijy;PIG-4942.3.patch;https://issues.apache.org/jira/secure/attachment/12818443/PIG-4942.3.patch,17/Jul/16 09:07;petersla;PIG-4942.4.patch;https://issues.apache.org/jira/secure/attachment/12818447/PIG-4942.4.patch,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2016-07-17 07:47:55.995,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Sun Jul 17 18:13:14 UTC 2016,,,,,,,0|i30he7:,9223372036854775807,,,,,,,,,,"03/Jul/16 22:55;petersla;The attached patch fixes the mvn-install target so that it works in both desired use cases:
  1. Pig is built for both Hadoop 1 and 2.
  2. Pig is built for only Hadoop 2.
 
Note the second use case is useful for the Apache Bigtop project as they only build Pig for Hadoop 2.",11/Jul/16 04:19;petersla;Attached PIG-4942.2.patch. There is no difference between the first one other than it is in the proper format.,"17/Jul/16 07:47;daijy;Yes mvn-install is broken, we shall fix it. Since mvn-deploy does more than mvn-install, and mvn-deploy is working, the fix can be simpler. We can just follow mvn-deploy to fix mvn-install. Attach PIG-4942.3.patch, can you check if it works for you?

This fix shall only go to 0.16 branch, since we are dropping hadoop-1 support in 0.17, build and release script will be much simplified.","17/Jul/16 08:34;petersla;Hi Daniel,

Thank you for the reply and simplified patch. 

Should mvn-install still depend on the set-version target like it did before? set-version looks to be generating the pom files which mvn-install refers to.

When building Pig only for Hadoop 2, mvn-build will fail as the pig-*h1.jar will not be built. This is the case when -Dhadoopversion=23 is passed through the command line. I understand this is not a problem for your use case as Pig is built for both Hadoop 1 and 2. The patch I provided did handle this case.

Since Hadoop-1 support is being dropped in 0.17, I am not too concern with going with your simplified approach on the 0.16 branch anyhow.

ant clean mvn-install -Dhadoopversion=23
...
BUILD FAILED
.../build.xml:1278: Warning: Could not find file /.../pig-0.16.0-SNAPSHOT-core-h1.jar to copy.
","17/Jul/16 08:43;petersla;Actually, I am myself currently looking into whether set-version is needed.","17/Jul/16 09:07;petersla;It looks like set-version is required. Otherwise, stale poms files are used from the ivy directory. (Or, possibly fail because the pom files are not present for a newly cloned or cleaned repository)

In my test, I changed the pig version to be 0.18.0-SNAPSHOT, however, since set-version was not performed, these 0.18.0-SNAPSHOT jars were installed as 0.16.0-SNAPSHOT jars:

Installing /Users/petersla/Projects/Aws157Pig/build/pig-0.18.0-SNAPSHOT-smoketests.jar to /Users/petersla/.m2/repository/org/apache/pig/pigsmoke/0.16.0-SNAPSHOT/pigsmoke-0.16.0-SNAPSHOT.jar

I have attached an updated patch fixing this. mvn-deploy does depend on set-version indirectly via mvn-publish.
","17/Jul/16 18:13;daijy;+1 for PIG-4942.4.patch.

Patch committed to 0.16 branch. Thanks Peter!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetch yarn job diagnostics message and display for failed jobs,PIG-4356,12761126,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,rohini,rohini,11/Dec/14 18:25,24/May/16 22:22,13/Mar/19 23:13,24/May/16 22:21,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"It is useful to get the Yarn Application diagnostics message and display.
For eg: Application appattempt_1396562772944_0011_000001 submitted by user rohinip to unknown queue: wrong",,,,,,,,,,,,,,,TEZ-1017,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Tue May 24 22:21:42 UTC 2016,,,,,,,0|i23btz:,9223372036854775807,,,,,,,,,,24/May/16 22:21;rohini;This is already fixed as part of PIG-3994,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MapKeyType of splitter was set wrongly in specific multiquery case,PIG-4883,12963590,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,licstar,licstar,28/Apr/16 18:41,13/May/16 17:42,13/Mar/19 23:13,13/May/16 17:42,0.15.0,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"The following code and data will raise an exception.
However, if I remove any of the four ""store""s, the code will be fine.

{code:title=test.pig|borderStyle=solid}
r = load 'test.txt' as (id: chararray, val: long);

t1 = filter r by val >= 12 and val < 20;
grpd = group t1 by val;
t1_cnt = foreach grpd generate group as name, COUNT(t1) as value;
t1_cnt = foreach t1_cnt generate (chararray)name, value;
grpd = group t1 all;
t1_cnt_total = foreach grpd generate 't1' as name, COUNT(t1) as value; 

t2 = filter r by val >= 20 and val < 30;
grpd = group t2 by val;
t2_cnt = foreach grpd generate group as name, COUNT(t2) as value;
--t2_cnt = foreach t2_cnt generate (chararray)name, value;
grpd = group t2 all;
t2_cnt_total = foreach grpd generate 't2' as name, COUNT(t2) as value;

store t1_cnt  into 'outx/3';
store t2_cnt  into 'outx/4';
store t1_cnt_total into 'outx/5';
store t2_cnt_total into 'outx/6';
{code}
and
{code:title=test.txt|borderStyle=solid}
c	12
{code}


will cause error:
Caused by: java.lang.ClassCastException: org.apache.pig.data.BinSedesTuple cannot be cast to java.lang.Long
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNextString(POCast.java:1167)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:343)
        ... 14 more


I don't know why the code need to cast a BinSedesTuple to Long.
",,,,,,,,,,,,,,,,,10/May/16 08:42;kellyzly;PIG-4883.patch;https://issues.apache.org/jira/secure/attachment/12803193/PIG-4883.patch,13/May/16 05:35;kellyzly;PIG-4883_1.patch;https://issues.apache.org/jira/secure/attachment/12803802/PIG-4883_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-05-10 08:37:02.268,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Fri May 13 17:42:15 UTC 2016,,,,,,,0|i2wx4v:,9223372036854775807,,,,,,,,,,"10/May/16 08:37;kellyzly;   the problem in the above case is because if the map key type of all splittees are not same, we just use Tuple to set the map key type of splitter.
   But it will meet problem in following case:
{noformat}
                      A        B                 C        D
                       \      /                   \     /
                     Splitter(E)                 Splitter(F)
                            \                       /
                                 Slitter(G)
                 If the mapKeyType of A and B are not same, mapKeyType of E will be Tuple
                 If the mapKeyType of C and D are not same, mapKeyType of F will be Tuple
                 we will consider G hasSameMapKeyType but actually not.
{noformat}

In above case, the value of splitter's hasSameMapKeyType is true. It causes the key is not unwrapped in MultiQueryPackager#getNext
{code}
  @Override
    public Result getNext() throws ExecException {
        ....
        // check to see if we need to unwrap the key. The keys may be
        // wrapped inside a tuple by LocalRearrange operator when jobs
        // with different map key types are merged
        PigNullableWritable curKey = keyWritable;
        if (!sameMapKeyType && !inCombiner && isKeyWrapped.get(index)) {
            Tuple tup = (Tuple) keyWritable.getValueAsPigType();
            curKey = HDataType.getWritableComparableTypes(tup.get(0),
                    pkgr.getKeyType());
            curKey.setIndex(origIndex);
        }
.....
{code}
   
[~rohini] or [~daijy]: can you help review this patch?            ","10/May/16 08:42;kellyzly;in PIG-4883.patch:
1. add an attribute ""mapKeyTypeForSplitter"" for MapReduceOperator to record the map key type of all splittees if have.
2.modify MultiQueryOptimizer#hasSameMapKeyType. If the splittees of the splitter are splitter before(like the case i mention in the patch), we just compare the mapKeyTypeForSplitter of all the splittees. If all the elements in the mapKeyTypeForSplitter  are same, then return true for function ""hasSameMapKeyType"".","11/May/16 23:28;rohini;Can't we just do something like below to check if key type of splitter and splittee are same?
{code}
private boolean hasSameMapKeyType(MapReduceOper splitter, List<MapReduceOper> splittees) {
        Set<Byte> keyTypes = new HashSet<Byte>();
        for (MapReduceOper splittee : splittees) {
            keyTypes.add(splittee.mapKeyType);
        }
         boolean sameKeyType = keyTypes.size() == 1;
        if (sameKeyType && splitter.mapKeyType != DataType.UNKNOWN) {
            sameKeyType = splitter.mapKeyType == keyTypes.iterator().next().byteValue();
        }
        return sameKeyType;
    }
{code}","12/May/16 01:43;kellyzly;[~rohini]: The problem is because  for multiquery case,  we will set the mapKeyType of splitter as DataType.Tuple([detailed code|https://github.com/apache/pig/blob/27b153dbd688d8328e00d2d4bead84f3c879b2ae/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java#L1035]) if the mapKeyType of splittees are not same. 

so I think your code can not resolve current problem.   Make an example to explain it:
{noformat}
                      A(int)        B(long)              C(int)              D(long)
                            \      /                                  \            /
                           Splitter(E)                           Splitter(F)
                                      \                                   /
                                                    Splitter(G)
                 the mapKeyType of A and B are not same, mapKeyType of E will be Tuple
                 the mapKeyType of C and D are not same, mapKeyType of F will be Tuple
                 the mapKeyType of E and F are both DataType.Tuple and are same.  Function ""hasSameMapKeyType"" returns true. 
{noformat}

In above case, we can not judge whether the mapKeyType of E and F are same if we don't save the mapType of splittees for splitter.  In PIG-4883.patch: we create an attribute ""mapKeyTypeForSplitter"" in MapReduceOper to save the mapKeyType of all splittees:
{noformat}
                      A(int)        B(long)                     C(int)           D(long)
                            \      /                                           \     /
                           Splitter(E)                                  Splitter(F)
                                      \                                        /
                                                    Splitter(G)
                 the mapKeyType of A and B are not same,mapKeyTypeForSplitter of E will be [int,long]
                 the mapKeyType of C and D are not same, mapKeyTypeForSplitter of F will be [int,long]
                 if all the elements in E.mapKeyTypeForSplitter and F.mapKeyTypeForSplitter are same, function ""hasSameMapKeyType"" return true.
{noformat}

","12/May/16 19:22;rohini;Got it. Thanks for the explanation. Few comments on the patch
  - Please add a test case for this to TestMultiQuery
  -  Rename mapKeyTypeForSplitter to mapKeyTypeOfSplittees
  -  Main logic can be simplified to few lines.

{code}
private boolean hasSameMapKeyType(List<MapReduceOper> splittees) {
        Set<Byte> keyTypes = new HashSet<Byte>();
        for (MapReduceOper splittee : splittees) {
            keyTypes.add(splittee.mapKeyType);
            if (splittee.mapKeyTypeOfSplittees != null) {
                for (int i = 0; i < mapKeyTypeOfSplittees.length; i++) {
                keyTypes.add(mapKeyTypeOfSplittees[i]);
            }
            }
        }
        return keyTypes.size() == 1;
    }
{code}",13/May/16 05:35;kellyzly;[~rohini]:Thanks for your review. Have submitted PIG-4883_1.patch for your review.,13/May/16 17:42;rohini;+1. Committed to trunk. Thank Liyun for the patch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backend code should not call AvroStorageUtils.getPaths,PIG-4686,12896418,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mitdesai,mitdesai,mitdesai,25/Sep/15 15:56,07/Feb/16 23:53,13/Mar/19 23:13,07/Feb/16 23:53,0.14.0,,,,,,,,,,,,,,,,,,piggybank,,,0,,,,,,,,"In AvroStorage, backend code should never call AvroStorageUtils.getPaths().

Currently, getPaths is called called again from the backend code if the schema parser throws an exception.",,,,,,,,,,,,,,,,,27/Sep/15 20:24;mitdesai;PIG-4686.1.patch;https://issues.apache.org/jira/secure/attachment/12762616/PIG-4686.1.patch,29/Jan/16 18:03;mitdesai;PIG-4686.2.patch;https://issues.apache.org/jira/secure/attachment/12785201/PIG-4686.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-09-29 22:34:59.74,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Sun Feb 07 23:53:57 UTC 2016,,,,,,,0|i2lkfz:,9223372036854775807,,,,,,,,,,29/Sep/15 22:34;rohini;You should not be moving that block to the beginning. The code should first check for schema from UDFContext and only if not there should it try to fetch again. Also the comment is still valid and should not be removed.,29/Jan/16 18:03;mitdesai;Update the patch,07/Feb/16 23:53;rohini;Committed to trunk. Thanks for the contribution [~mitdesai],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Please delete old releases from mirroring system,PIG-4794,12936166,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,sebb@apache.org,sebb@apache.org,02/Feb/16 22:54,03/Feb/16 19:50,13/Mar/19 23:13,03/Feb/16 19:08,0.13.0,0.14.0,,,,,,,,,,,,,,,0.13.0,0.14.0,,,,0,,,,,,,,"To reduce the load on the ASF mirrors, projects are required to delete old releases [1]

Please can you remove all non-current releases?

i.e. the ones listed as affected.

Thanks!

Also, if you have a release guide, perhaps you could add a cleanup stage to it?

[1] http://www.apache.org/dev/release.html#when-to-archive
",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-02-03 19:08:17.546,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 03 19:50:35 UTC 2016,,,,,,,0|i2sbsv:,9223372036854775807,,,,,,,,,,03/Feb/16 19:08;daijy;0.13.0 and 0.14.0 release artifacts are removed from https://dist.apache.org/repos/dist/release/pig. And changed https://cwiki.apache.org/confluence/display/Pig/HowToRelease to reflect the change.,03/Feb/16 19:50;sebb@apache.org;Thanks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pigmix should have option to delete outputs after completing the tests,PIG-4753,12917428,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,mitdesai,mitdesai,mitdesai,02/Dec/15 00:56,03/Dec/15 22:32,13/Mar/19 23:13,03/Dec/15 22:32,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Currently pigmix takes up around 10TB of disk space in the entire run. This can be brought down significantly if the outputs of the tests are deleted after the test run is completed.

By deleting the outputs, pigmix can be run with roughly 4TB of disk space.",,,,,,,,,,,,,,,,,03/Dec/15 21:30;mitdesai;PIG-4753.1.patch;https://issues.apache.org/jira/secure/attachment/12775646/PIG-4753.1.patch,03/Dec/15 21:50;mitdesai;PIG-4753.2.patch;https://issues.apache.org/jira/secure/attachment/12775657/PIG-4753.2.patch,03/Dec/15 22:21;mitdesai;PIG-4753.3.patch;https://issues.apache.org/jira/secure/attachment/12775665/PIG-4753.3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2015-12-03 21:40:49.986,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Thu Dec 03 22:31:57 UTC 2015,,,,,,,0|i2p5af:,9223372036854775807,,,,,,,,,,"03/Dec/15 21:30;mitdesai;Attached the patch that changes the behavior.

It takes in a new argument. When pigmix is run, if it is initiated with the flag set, it will delete the output dir after each each run. If nothing is specified, it runs the same way it runs currently.",03/Dec/15 21:40;rohini;Can you change the default setting to not delete the output?,03/Dec/15 21:50;mitdesai;Updated the patch.,03/Dec/15 22:31;rohini;+1. Committed to trunk. Thanks for the contribution Mit.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG HBaseStorage with HBase not working,PIG-4743,12915110,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,shashikantkulkarni,shashikantkulkarni,21/Nov/15 16:42,30/Nov/15 03:44,13/Mar/19 23:13,25/Nov/15 00:07,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"I am using Hadoop version 2.6.1, HBase version 1.1.2 and PIG version 0.15.0

I have also set the required environment variables in /home/hadoop/.bashrc file as follows
========================
# .bashrc

# Source global definitions
if [ -f /etc/bashrc ]; then
	. /etc/bashrc
fi

# User specific aliases and functions
export HADOOP_HOME=/opt/hadoop/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

### Flume Home Directory
export FLUME_HOME=/usr/lib/flume/apache-flume
export FLUME_CONF_DIR=$FLUME_HOME/conf
export FLUME_CLASSPATH=$FLUME_CONF_DIR
export PATH=$PATH:$FLUME_HOME/bin

### HBase Home Directory

export HBASE_HOME=/usr/lib/hbase
export HBASE_CONF_DIR=$HBASE_HOME/conf
export HBASE_CLASSPATH=$HBASE_CONF_DIR

export PATH=$PATH:$HBASE_HOME/bin

###Hive Home Directory
export HIVE_HOME=/usr/lib/hive/apache-hive
export HIVE_CONF_DIR=$HIVE_HOME/conf
export HIVE_CLASSPATH=$HIVE_CONF_DIR

export PATH=$PATH:$HIVE_HOME/bin

###HCatalog Home Directory
export HCAT_HOME=$HIVE_HOME/hcatalog
export HCAT_CONF_DIR=$HCAT_HOME/etc/hcatalog:$HCAT_HOME/etc/webhcat
export HCAT_CLASSPATH=$HCAT_CONF_DIR

export PATH=$PATH:$HCAT_HOME/bin:$HCAT_HOME/sbin

###PIG Home Directory
export PIG_HOME=/usr/lib/pig/pig-0.15.0
export PIG_CONF_DIR=$PIG_HOME/conf
export PIG_CLASSPATH=$PIG_CONF_DIR:$PIG_HOME/pig-0.15.0-core-h2.jar:$HBASE_HOME
export PATH=$PATH:$PIG_HOME/bin
==========================

When I try to execute the following script
raw = LOAD 'hbase://test_table' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage ('test_cf:NAME') AS (NAME:chararray);

I get following error message
Pig Stack Trace
---------------
ERROR 1200: Pig script failed to parse: 
<file sample1.pig, line 1, column 6> pig script failed to validate: java.lang.RuntimeException: could not instantiate 'org.apache.pig.backend.hadoop.hbase.HBaseStorage' with arguments '[test_cf:NAME]'

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. Pig script failed to parse: 
<file sample1.pig, line 1, column 6> pig script failed to validate: java.lang.RuntimeException: could not instantiate 'org.apache.pig.backend.hadoop.hbase.HBaseStorage' with arguments '[test_cf:NAME]'
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1748)
	at org.apache.pig.PigServer$Graph.access$000(PigServer.java:1443)
	at org.apache.pig.PigServer.parseAndBuild(PigServer.java:387)
	at org.apache.pig.PigServer.executeBatch(PigServer.java:412)
	at org.apache.pig.PigServer.executeBatch(PigServer.java:398)
	at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:171)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:234)
	at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)
	at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
	at org.apache.pig.Main.run(Main.java:631)
	at org.apache.pig.Main.main(Main.java:177)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: Failed to parse: Pig script failed to parse: 
<file sample1.pig, line 1, column 6> pig script failed to validate: java.lang.RuntimeException: could not instantiate 'org.apache.pig.backend.hadoop.hbase.HBaseStorage' with arguments '[test_cf:NAME]'
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:199)
	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1735)
	... 15 more
Caused by: 
<file sample1.pig, line 1, column 6> pig script failed to validate: java.lang.RuntimeException: could not instantiate 'org.apache.pig.backend.hadoop.hbase.HBaseStorage' with arguments '[test_cf:NAME]'
	at org.apache.pig.parser.LogicalPlanBuilder.buildLoadOp(LogicalPlanBuilder.java:897)
	at org.apache.pig.parser.LogicalPlanGenerator.load_clause(LogicalPlanGenerator.java:3568)
	at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1625)
	at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1102)
	at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:560)
	at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:421)
	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:191)
	... 16 more
Caused by: java.lang.RuntimeException: could not instantiate 'org.apache.pig.backend.hadoop.hbase.HBaseStorage' with arguments '[test_cf:NAME]'
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:772)
	at org.apache.pig.parser.LogicalPlanBuilder.buildLoadOp(LogicalPlanBuilder.java:885)
	... 22 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:740)
	... 23 more
Caused by: java.lang.NoSuchMethodError: org.apache.hadoop.hbase.client.Scan.setCacheBlocks(Z)V
	at org.apache.pig.backend.hadoop.hbase.HBaseStorage.initScan(HBaseStorage.java:427)
	at org.apache.pig.backend.hadoop.hbase.HBaseStorage.<init>(HBaseStorage.java:368)
	at org.apache.pig.backend.hadoop.hbase.HBaseStorage.<init>(HBaseStorage.java:239)
	... 28 more
================================================================================


The hbase-client-1.1.2.jar is in path of PIG which contains the org.apache.hadoop.hbase.client.Scan class.

Can someone please help if I am missing the configuration somewhere? Am I using the compatible versions of PIG and HBase.

I also tried all the suggestion found on internet to fix this problem, including compiling PIG, copying Jars, Registering Jars with PIG, etc. Nothing is working for me.

Thanks","CentOS 6.7
Hadoop 2.6.1
HBase 1.1.2
PIG 0.15.0
JDK 1.7.0_21",,,,,,,,,,,,PIG-4728,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-11-24 00:24:27.253,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 25 00:07:04 UTC 2015,,,,,,,0|i2oqzr:,9223372036854775807,"After compiling the PIG 0.15 against HBase 1.1.2, I am able to execute the PIG script successfully using HBaseStorage.
I also downgraded the Hadoop version from 2.6.1 to 2.5.1 because of version compatibility issues with HBase.",,,,,,,,,"24/Nov/15 00:24;rohini;https://github.com/apache/hbase/blob/0.98/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java

{code}
public void setCaching(int caching) {
    this.caching = caching;
  }
{code}

https://github.com/apache/hbase/blob/branch-1.2/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Scan.java

{code}
public Scan setCaching(int caching) {
    this.caching = caching;
    return this;
  }
{code}

Hbase community broke binary compatibility with https://issues.apache.org/jira/browse/HBASE-10841. Pig has to be compiled with hbase 1.x to run with hbase 1.x. Since Pig 0.15 is compiled with hbase 0.98, you are getting those errors.","24/Nov/15 06:07;shashikantkulkarni;Hi [~rohini]

Thanks for details. Could you please tell me how to compile PIG 0.15 with base 0.98 because I tried it with the steps provided by PIG document but it is still not working. Also where & how can I change the hbase version when compiling PIG?

Also tell me one more thing. After compilation, which and where to copy the Jar files? Or can I keep this source compiled folder as PIG_HOME?

Thanks
Shashikant","24/Nov/15 23:59;shashikantkulkarni;Hi [~rohini],

Thanks for your help. After compiling the PIG 0.15 against HBase 1.1.2, I am able to execute the PIG script successfully using HBaseStorage.

Regards,
Shashikant

",25/Nov/15 00:04;shashikantkulkarni;I also downgraded the Hadoop version from 2.6.1 to 2.5.1 because of version compatibility issues with HBase.,25/Nov/15 00:07;shashikantkulkarni;See the previous comments on this task for how to fix it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig 0.14 job with order by fails in mapreduce mode with Oozie,PIG-4628,12843621,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,knoguchi,knoguchi,knoguchi,08/Jul/15 19:43,12/Aug/15 18:26,13/Mar/19 23:13,12/Aug/15 17:11,0.14.0,0.15.0,,,,,,,,,,,,,,,0.15.1,,impl,,,0,,,,,,,,"A simple pig script with order-by submitted through oozie and running with mapreduce-mode 

{code}
A = LOAD '$input' AS (a1:CHARARRAY,a2:CHARARRAY, );
A_sorted = ORDER A BY url DESC PARALLEL 2;
STORE A_sorted INTO '$output';
{code}

failed on our hadoop cluster which had security turned on.  Part of the stack trace had 

{noformat}
2015-06-08 22:24:39,246 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: Exception reading file:/tmp/2/yarn-local/usercache/userA/appcache/application_1432697993142_199266/container_e06_1432697993142_199266_01_000003/container_tokens
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.init(WeightedRangePartitioner.java:155)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:75)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:58)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:712)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map.collect(PigGenericMapReduce.java:135)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:281)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:274)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)

{noformat}

This failing job was from application_1432697993142_199305 and the error path was from application_1432697993142_199266 which was a oozie pig-launcher job.

",,,,,,,,,,,,,,,,,08/Jul/15 20:21;knoguchi;pig-4628-v01.patch;https://issues.apache.org/jira/secure/attachment/12744317/pig-4628-v01.patch,10/Jul/15 21:21;knoguchi;pig-4628-v02.patch;https://issues.apache.org/jira/secure/attachment/12744814/pig-4628-v02.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-07-09 21:42:12.105,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 12 18:26:17 UTC 2015,,,,,,,0|i2h0fb:,9223372036854775807,,,,,,,,,,"08/Jul/15 19:52;knoguchi;Full stack trace
{noformat}
2015-06-08 22:24:39,246 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: Exception reading file:/tmp/2/yarn-local/usercache/userA/appcache/application_1432697993142_199266/container_e06_1432697993142_199266_01_000003/container_tokens
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.init(WeightedRangePartitioner.java:155)
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:75)
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:58)
  at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:712)
  at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
  at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map.collect(PigGenericMapReduce.java:135)
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:281)
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:274)
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
  at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
  at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
  at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:415)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)
  at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.RuntimeException: java.io.IOException: Exception reading file:/tmp/2/yarn-local/usercache/userA/appcache/application_1432697993142_199266/container_e06_1432697993142_199266_01_000003/container_tokens
  at org.apache.hadoop.mapreduce.security.TokenCache.mergeBinaryTokens(TokenCache.java:141)
  at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:119)
  at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
  at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:242)
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigFileInputFormat.listStatus(PigFileInputFormat.java:37)
  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:385)
  at org.apache.pig.impl.io.ReadToEndLoader.init(ReadToEndLoader.java:190)
  at org.apache.pig.impl.io.ReadToEndLoader.<init>(ReadToEndLoader.java:126)
  at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.init(WeightedRangePartitioner.java:127)
  ... 17 more
Caused by: java.io.IOException: Exception reading file:/tmp/2/yarn-local/usercache/userA/appcache/application_1432697993142_199266/container_e06_1432697993142_199266_01_000003/container_tokens
  at org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:174)
  at org.apache.hadoop.mapreduce.security.TokenCache.mergeBinaryTokens(TokenCache.java:136)
  ... 26 more
Caused by: java.io.FileNotFoundException: File file:/tmp/2/yarn-local/usercache/userA/appcache/application_1432697993142_199266/container_e06_1432697993142_199266_01_000003/container_tokens does not exist
  at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)
  at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
  at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
  at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:416)
  at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)
  at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)
  at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)
  at org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:169)
  ... 27 more
{noformat}


On IM, [~rohini]] taught me, 
bq. [MAPREDUCE-3727|https://issues.apache.org/jira/browse/MAPREDUCE-3727] - Hadoop ensures that for a new job submitted the mapreduce.credentials.job.binary is removed. So this is creeping in from somewhere else. If Pig 0.11 works then something in Pig 0.14 is the culprit.

Based on this feedback, ran a job with custom hadoop.jar that dumped thread when mapreduce.job.credentials.binary was set.

{code:title=Configuration.java}
 990   public void set(String name, String value) {
 991     if(name.equals(""mapreduce.job.credentials.binary"")) {
 992       System.err.println(""Koji"");
 993       Thread.dumpStack();
 994     }
 995     set(name, value, null);
 996   }
{code}

This job produced the stack trace of 
{noformat}
Log Type: stderr
Log Upload Time: 7-Jul-2015 21:28:18
Log Length: 2336
Koji
java.lang.Exception: Stack trace
	at java.lang.Thread.dumpStack(Thread.java:1342)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:993)
	at org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil.toConfiguration(ConfigurationUtil.java:48)
	at org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil.toConfiguration(ConfigurationUtil.java:38)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.init(WeightedRangePartitioner.java:113)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:75)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner.getPartition(WeightedRangePartitioner.java:58)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:712)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map.collect(PigGenericMapReduce.java:135)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:281)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:274)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
{noformat}

Looking at the stack trace, we can see that it is being set from 
{code:title=WeightedRangePartitioner.java}
112             if (!pigContext.getExecType().isLocal()) {
113                 conf = ConfigurationUtil.toConfiguration(pigContext.getProperties());
114             }
{code}

So the mapreduce.credentials.job.binary setting from the launcher was coming from the serialized pigContext inside the jobconf.
This part of the code was changed in [PIG-4257|https://issues.apache.org/jira/browse/PIG-4257] where the patch had 
{code:title=PIG-4257-3.patch}
             if (!pigContext.getExecType().isLocal()) {
-                conf = new Configuration(true);
+                conf = ConfigurationUtil.toConfiguration(pigContext.getProperties());
             } 
{code}","08/Jul/15 20:21;knoguchi;I guess the fix can be made at where it's _reading_ this invalid token path from pigcontext (WeightedRangePartitioner.java), or where it's _writing_ this invalid path to serialized pigcontext form inside jobconf.

This pig-4628-v01.patch does the latter.  Running tests.",09/Jul/15 21:42;rohini;org.apache.hadoop.mapreduce.MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY might have issues with hadoop 1.x. So better to use the actual string. Otherwise good.,"10/Jul/15 21:21;knoguchi;{quote}
org.apache.hadoop.mapreduce.MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY might have issues with hadoop 1.x. So better to use the actual string. Otherwise good.
{quote}
Thanks [~rohini]!  Uploading v02 with actual string (""mapreduce.job.credentials.binary""). ",10/Jul/15 21:31;rohini;+1,"12/Aug/15 00:49;viraj;Rohini can you please commit this to trunk and or backport to 0.14. We are running on Pig 0.14 with M/R mode and faced this problem.

Viraj",12/Aug/15 17:11;knoguchi;Patch committed to 0.15 and trunk.  Thanks for the review [~rohini]! ,"12/Aug/15 18:26;viraj;Thanks Koji for your help.
Viraj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PushUpFilter should not push before nested projection with FILTER operators,PIG-4646,12850177,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,ahoyleo,ahoyleo,30/Jul/15 06:32,04/Aug/15 00:06,13/Mar/19 23:13,04/Aug/15 00:06,0.11.1,,,,,,,,,,,,,,,,0.12.0,,,,,0,,,,,,,,"Verified the problem in 0.11.1. In short, filter should not be pushed before a nested foreach in which another filter operator is present. See the following minimum example:

{code}
cat data;

(1, {(1000, 'a'), (1001, 'b')})
(2, {(2000, 'a'), (2001, 'b'), (2002, 'c')})

A = load 'data' as (id:int, hits:{(score:int, name:chararray)});
B = foreach A {
  filtered = filter hits by score > 2000;
  generate id, filtered;
};

dump B;

(1,{})
(2,{(2001,'b'),(2002,'c')})

C = filter B by SIZE(filtered) > 0;

dump C;

(1,{})
(2,{(2001,'b'),(2002,'c')})
{code}

The desired result can be achieved with either '-optimizer_off PushUpFilter' when invoking Pig, or using the following convoluted way:
{code}
C = foreach B generate SIZE(filtered) as size, id, filtered;
D = filter C by size > 0;
E = foreach D generate id, filtered;

dump E;

(2,{(2001,'b'),(2002,'c')})
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-08-04 00:06:01.112,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 04 00:06:01 UTC 2015,,,,,,,0|i2i3zb:,9223372036854775807,,,,,,,,,,"04/Aug/15 00:06;daijy;Thanks for reporting. 

The script runs right with 0.12.0+. Not sure which patch credit to this. Marked for fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in TOP,PIG-2031,12506061,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thedatachef,thedatachef,thedatachef,03/May/11 13:29,12/Jul/15 13:57,13/Mar/19 23:13,12/Jul/15 13:57,,,,,,,,,,,,,,,,,,,,,,0,newbie,,,,,,,"If a NULL DataBag is passed to org.apache.pig.builtin.TOP then a NPE is thrown. Consider:


{code}
$: cat foo.tsv
a  {(foo,1),(bar,2)}
b
c  {(fyha,4),(asdf,9)}
{code}

then:

{code}
data  = LOAD 'foo.tsv' AS (key:chararray, a_bag:bag {t:tuple (name:chararray, value:int)});
tpd   = FOREACH data {
          top_n = TOP(1, 1, a_bag);
          GENERATE
            key   AS key,
            top_n AS top_n
          ;             
        };
DUMP tpd;
{code}

will throw an NPE when it gets to the row with no bag.",,,,,,,,,,,,,,,,,14/Nov/11 02:46;dvryaboy;PIG-2031.patch;https://issues.apache.org/jira/secure/attachment/12503559/PIG-2031.patch,04/May/11 12:42;thedatachef;toppatch.txt;https://issues.apache.org/jira/secure/attachment/12478155/toppatch.txt,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-05-04 16:24:20.822,,,no_permission,,,,,,,,,,,,,63076,,,,Sun Jul 12 13:57:44 UTC 2015,,,,,,,0|i0h0mf:,97363,,,,,,,,,,04/May/11 12:42;thedatachef;Trivial fix but the case described runs through. Are there tests for this I should modify?,"04/May/11 16:24;dvryaboy;Patch seems fine, thanks for doing that.
A new test that tries the null bag would be good.","14/Nov/11 02:46;dvryaboy;Added a test (and while I was in there, simplified out some unnecessary casts in TestBuiltin).

I think this is good to commit, I intend to commit to both 0.10 branch and trunk tomorrow .",15/Jun/15 09:36;eyal;Is this issue still relevant? I think the current code already checks for a null bag.,"12/Jul/15 13:57;eyal;The issue, as it's described, no longer occurs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The test org.apache.pig.builtin.TestOrcStoragePushdown.testPredicatePushdownTimestamp  is failing,PIG-4602,12837828,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,GauravPande,GauravPande,15/Jun/15 12:48,19/Jun/15 06:06,13/Mar/19 23:13,17/Jun/15 16:23,0.15.0,,,,,,,,,,,,,,,,0.15.0,,,,,0,,,,,,,,"The test org.apache.pig.builtin.TestOrcStoragePushdown.testPredicatePushdownTimestamp is failing. 
Assertion failure is :
Testcase: testPredicatePushdownTimestamp took 42.726 sec
	FAILED
BytesWithoutPushdown was 2481015 and bytesWithPushdown was 2488249
junit.framework.AssertionFailedError: BytesWithoutPushdown was 2481015 and bytesWithPushdown was 2488249
	at org.apache.pig.builtin.TestOrcStoragePushdown.testPredicatePushdown(TestOrcStoragePushdown.java:403)
	at org.apache.pig.builtin.TestOrcStoragePushdown.testPredicatePushdownTimestamp(TestOrcStoragePushdown.java:340)

",RHEL 7.1 X86_64,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-06-16 21:22:04.98,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 19 06:06:37 UTC 2015,,,,,,,0|i2g1on:,9223372036854775807,,,,,,,,,,16/Jun/15 21:22;daijy;It pass on Apache build (https://builds.apache.org/view/M-R/view/Pig/job/Pig-trunk-commit). Do you run with a different version of Hive? We notice there is one issue for timestamp predicate pushdown on Hive 1.2.0 (HIVE-10819).,17/Jun/15 14:27;GauravPande;I am using Hive-1.2.0,"17/Jun/15 16:23;daijy;Yes, Hive-1.2.0 has known issue. The coming 1.2.1 should be fine.","19/Jun/15 06:02;libing;Hi, Daniel
Di you mean that it's an issue in Hive 1.2, and Hive will fix it in 1.2.1?
Is there a JIRA in Hive to track this?

Thank you
- Bing","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.","19/Jun/15 06:06;libing;Sorry, just noticed your previous comment, which is HIVE-10819.",,,,,,,,,,,,,,,,,,,,,,
"Enable unit test ""TestBestFitCast"" for spark",PIG-4272,12752935,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,kellyzly,kellyzly,kellyzly,05/Nov/14 02:01,01/Dec/14 09:44,13/Mar/19 23:13,01/Dec/14 09:44,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,error log is attached,,,,,,,,,,,,,,,,,05/Nov/14 02:01;kellyzly;TEST-org.apache.pig.test.TestBestFitCast.txt;https://issues.apache.org/jira/secure/attachment/12679428/TEST-org.apache.pig.test.TestBestFitCast.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-12-01 09:38:24.472,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 01 09:38:24 UTC 2014,,,,,,,0|i21zaf:,9223372036854775807,,,,,,,,,,01/Dec/14 06:52;kellyzly;unit test passes by using PIG-4232_6.patch,01/Dec/14 09:38;praveenr019;Fixed by PIG-4232_6.patch. Thanks Liyunzhang.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig 0.14 cannot identify the uppercase of DECLARE and DEFAULT,PIG-4342,12757443,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,rohini,rohini,24/Nov/14 16:58,24/Nov/14 22:16,13/Mar/19 23:13,24/Nov/14 22:16,0.14.0,,,,,,,,,,,,,,,,0.14.1,,,,,0,,,,,,,,"One of the users encountered
{code}
Unexpected internal error. Pig Internal Error. Invalid preprocessor command
specified : %DECLARE
{code}

 Changing the %DECLARE to %declare, it can pass the preprocessor's
operations.
",,,,,,,,,,,,,,,,,24/Nov/14 18:35;daijy;PIG-4342-1.patch;https://issues.apache.org/jira/secure/attachment/12683376/PIG-4342-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-11-24 18:13:30.797,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 24 22:16:09 UTC 2014,,,,,,,0|i22pzj:,9223372036854775807,,,,,,,,,,"24/Nov/14 18:13;daijy;Here is the stack:
{code}
ERROR 2999: Unexpected internal error. Pig Internal Error. Invalid preprocessor command specified : %DEFAULT

java.lang.IllegalArgumentException: Pig Internal Error. Invalid preprocessor command specified : %DEFAULT
        at org.apache.pig.tools.parameters.PreprocessorContext.validate(PreprocessorContext.java:163)
        at org.apache.pig.tools.parameters.PigFileParser.input(PigFileParser.java:62)
        at org.apache.pig.tools.parameters.PigFileParser.Parse(PigFileParser.java:43)
        at org.apache.pig.tools.parameters.ParameterSubstitutionPreprocessor.parsePigFile(ParameterSubstitutionPreprocessor.java:95)
        at org.apache.pig.tools.parameters.ParameterSubstitutionPreprocessor.genSubstitutedFile(ParameterSubstitutionPreprocessor.java:76)
        at org.apache.pig.impl.PigContext.doParamSubstitution(PigContext.java:410)
        at org.apache.pig.Main.runParamPreprocessor(Main.java:810)
        at org.apache.pig.Main.run(Main.java:588)
        at org.apache.pig.Main.main(Main.java:170)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
{code}",24/Nov/14 18:37;rohini;+1,24/Nov/14 22:16;daijy;Patch committed to both 0.14 branch and trunk. Thanks Rohini!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing method error,PIG-4026,12722488,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,mustafa adib,mustafa adib,19/Jun/14 12:45,20/Jun/14 08:06,13/Mar/19 23:13,20/Jun/14 08:06,0.12.0,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Using PigUnit-0.12.0-cdh5.0.0 and pig-0.12.0-cdh5.0.0 to test pig scripts and getting the following error

java.lang.NoSuchMethodError: org.apache.pig.impl.PigContext.doParamSubstitution(Ljava/io/BufferedReader;Ljava/util/List;Ljava/util/List;)Ljava/lang/String;
	at org.apache.pig.pigunit.PigTest.registerScript(PigTest.java:159)
	at org.apache.pig.pigunit.PigTest.assertOutput(PigTest.java:262)
	at com.audiencescience.apollo.pig.scripts.PigBase.checkAssertUnorderedOutput(PigBase.java:50)
	at com.audiencescience.apollo.pig.scripts.PigBase.assertUnorderedOutput(PigBase.java:44)
	at com.audiencescience.apollo.pig.scripts.DataProviderAttributionReportTest.testNokiaSpecificReport(DataProviderAttributionReportTest.java:39)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

The method seems to be missing within org.apache.pig.impl.PigContext",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-06-20 01:07:22.73,,,no_permission,,,,,,,,,,,,,400679,,,,Fri Jun 20 08:06:35 UTC 2014,,,,,,,0|i1wz0v:,400773,,,,,,,,,,20/Jun/14 01:07;daijy;I don't think that's a product issue. Probably you have multiple pig.jar in your path. ,"20/Jun/14 08:06;mustafa adib;Yeah the old jar was cached. Working fine
Thank you.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After compiling trunk, I am seeing ClassLoaderObjectInputStream ClassNotFoundException.",PIG-3880,12707776,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,medined,medined,10/Apr/14 19:37,30/May/14 14:41,13/Mar/19 23:13,02/May/14 19:55,0.13.0,,,,,,,,,,,,,,,,,,grunt,,,0,,,,,,,,"I pulled trunk from subversion using the following commands:

mkdir pig
cd pig
svn co http://svn.apache.org/repos/asf/pig/trunk
cd trunk
ant
export PATH=$PATH:$HOME/pig/trunk/bin
export ACCUMULO_HOME=/opt/accumulo
export HADOOP_HOME=/opt/hadoop
export PIG_HOME=$HOME/pig/trunk
export PIG_CLASSPATH=""$HOME/pig/trunk/build/ivy/lib/Pig/*""
export PIG_CLASSPATH=""$ACCUMULO_HOME/lib/*:$PIG_CLASSPATH""
cd ~
pig

Then I ran into this error:

java.lang.NoClassDefFoundError: org/apache/commons/io/input/ClassLoaderObjectInputStream
	at org.apache.pig.Main.run(Main.java:399)

When I change PIG_JAR to use the fat jar, I was able to run the pig command without getting the exception.",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-04-15 17:48:38.136,,,no_permission,,,,,,,,,,,,,386099,,,,Fri May 30 14:41:07 UTC 2014,,,,,,,0|i1uihr:,386364,,,,,,,,,,"15/Apr/14 17:48;elserj;What version of Hadoop are you using, [~medined]? I recall situations on other projects where the dependency management expected certain artifacts to be provided by hadoop when the user's version didn't actually provide that jar. I believe commons-io was one of these artifacts that I was bit by too.

This seems to be a plausible explanation to what you're seeing. The jarwithhadoop would contain the dependencies and thus you wouldn't have the issues if your local hadoop install was missing necessary jars.","15/Apr/14 19:23;medined;Good point. Perhaps my version of hadoop is too old?

Hadoop 0.20.203.0
Subversion
http://svn.apache.org/repos/asf/hadoop/common/branches/branch-0.20-security-203-r
1099333
Compiled by oom on Wed May  4 07:57:50 PDT 2011




","22/Apr/14 21:47;medined;I tried to add the commons-io to my classpath. I got the same error. Here is the dry run showing the jar file in the path.

$ pig
dry run:
HADOOP_CLASSPATH: /home/566453/pig/conf:/usr/java/jdk1.7.0_09/lib/tools.jar:/opt/accumulo/lib/accumulo-core-1.4.2.jar:/opt/accumulo/lib/libthrift-0.6.1.jar:/opt/accumulo/lib/cloudtrace-1.4.2.jar:/opt/zookeeper/zookeeper-3.3.3.jar:/home/566453/.m2/repository/commons-io/commons-io/2.1/commons-io-2.1.jar:/home/566453/pig/build/ivy/lib/Pig/jython-standalone-2.5.3.jar:/home/566453/pig/build/ivy/lib/Pig/jruby-complete-1.6.7.jar:/home/566453/pig/pig-withouthadoop.jar:
HADOOP_OPTS: -Xmx1000m  -Dpig.log.dir=/home/566453/pig/logs -Dpig.log.file=pig.log -Dpig.home.dir=/home/566453/pig
/opt/hadoop/bin/hadoop jar /home/566453/pig/pig-withouthadoop.jar

I tried both commons-io 1.4 and 2.1. I checked that the class is in the jar:

$ jar tf .m2/repository/commons-io/commons-io/2.1/commons-io-2.1.jar | grep ClassLoaderObjectInputStream
org/apache/commons/io/input/ClassLoaderObjectInputStream.class

Anything else I can try?","23/Apr/14 04:00;elserj;I'm a bit confused as to what you're showing here. Where is this ""dry run:"" output coming from? Can you verify that the following does (not) work:

{{PIG_CLASSPATH=/home/566453/.m2/repository/commons-io/commons-io/2.1/commons-io-2.1.jar pig -x mapreduce my_script.pig}}","23/Apr/14 16:28;medined;Responding to the dry run question first. On http://pig.apache.org/docs/r0.7.0/piglatin_ref2.html, in the 'Parameter Substitution' section, it describes 'pig -dryrun' as follows: ""With this option, the script is not run and a fully substituted Pig script produced in the current working directory named original_script_name.substituted""","23/Apr/14 16:32;medined;{noformat}
$ PIG_CLASSPATH=/home/566453/.m2/repository/commons-io/commons-io/2.1/commons-io-2.1.jar pig -x mapreduce my_script.pig
14/04/23 12:30:26 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL
14/04/23 12:30:26 INFO pig.ExecTypeProvider: Trying ExecType : MAPREDUCE
14/04/23 12:30:26 INFO pig.ExecTypeProvider: Picked MAPREDUCE as the ExecType
2014-04-23 12:30:26,793 [main] INFO  org.apache.pig.Main - Apache Pig version 0.13.0-SNAPSHOT (r1587299) compiled Apr 15 2014, 09:16:52
2014-04-23 12:30:26,793 [main] INFO  org.apache.pig.Main - Logging error messages to: /home/566453/pig_1398270626790.log
2014-04-23 12:30:26,839 [main] ERROR org.apache.pig.Main - ERROR 2998: Unhandled internal error. org/apache/commons/io/input/ClassLoaderObjectInputStream
Details at logfile: /home/566453/pig_1398270626790.log
{noformat}","24/Apr/14 21:41;elserj;I'm not sure what to say at this point. PIG_CLASSPATH should be placing it on the classpath of o.a.p.Main, I don't understand why it's not working for you. Maybe someone else might have some guidance.","24/Apr/14 23:30;daijy;Not sure what happen but here is something might be related. Pig fat jar contains commons-io.jar but pig-withouthadoop.jar does not. However, when you run bin/pig, Pig will invoke ""hadoop jar pig-withouthadoop.jar"", and hadoop will provide commons-io classes in runtime.",02/May/14 19:55;medined;I feel the consensus is saying;  Don't run old versions of hadoop. And I can live with that.,30/May/14 14:41;rcompton;On Hadoop 0.20.2-cdh3u3 and have the same problem with branch-0.13. The release Pig 12.1 release works fine. I can live with this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document auto local mode for pig,PIG-3745,12693464,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,05/Feb/14 23:06,14/May/14 15:50,13/Mar/19 23:13,14/May/14 15:50,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,We need to document feature added in PIG-3463.,,,,,,,,,,,,,,,,,13/May/14 22:53;aniket486;PIG-3745-1.patch;https://issues.apache.org/jira/secure/attachment/12644723/PIG-3745-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-06 22:31:42.316,,,no_permission,,,,,,,,,,,,,372049,,,,Wed May 14 15:50:08 UTC 2014,,,,,,,0|i1s4a7:,372353,,,,,,,,,,06/Mar/14 22:31;cheolsoo;Just a reminder. Let's include local-mode specific properties (PIG-3731) in the documentation too.,13/May/14 22:59;jcoveney;+1,14/May/14 15:50;aniket486;Committed to trunk. Thanks [~jcoveney] for the review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
COUNT on null bag causes failure,PIG-1283,12458483,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,analog.sony,thejas,thejas,08/Mar/10 22:09,24/Mar/14 18:27,13/Mar/19 23:13,18/Oct/12 23:28,,,,,,,,,,,,,,,,,0.11,,impl,,,1,newbie,,,,,,,"grunt>  l = load '/tmp/e.bag' as (b : bag{t: (i : int)}, a : int);
# b is null for the only row
grunt> c = foreach l generate COUNT(b);                           
grunt> dump c       

It results in following exception-

org.apache.pig.backend.executionengine.ExecException: ERROR 2106: Error while computing count in COUNT
        at org.apache.pig.builtin.COUNT.exec(COUNT.java:59)
        at org.apache.pig.builtin.COUNT.exec(COUNT.java:39)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:212)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:293)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:358)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:288)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:232)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:227)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:52)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:583)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:176)
Caused by: java.lang.NullPointerException
        at org.apache.pig.builtin.COUNT.exec(COUNT.java:46)
        ... 12 more
",,,,,,,,,,,,,,,,,24/Aug/12 22:28;analog.sony;PIG-1283-1.patch;https://issues.apache.org/jira/secure/attachment/12542355/PIG-1283-1.patch,12/Oct/12 07:54;analog.sony;PIG-1283-2.patch;https://issues.apache.org/jira/secure/attachment/12548864/PIG-1283-2.patch,14/Oct/12 06:43;analog.sony;pig_1283-3.patch;https://issues.apache.org/jira/secure/attachment/12549061/pig_1283-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-08-24 22:28:17.987,,,no_permission,,,,,,,,,,,,,164788,,,,Thu Oct 18 23:28:52 UTC 2012,,,,,,,0|i08tqn:,49418,,,,,,,,,,"24/Aug/12 22:28;analog.sony;Hi

Please take a look at the patch and provide your feedback.",24/Aug/12 22:54;analog.sony;Please review this patch.,"27/Aug/12 20:58;daijy;Hi, Anand, can you add a test case? Usually we add a test case for bug fix if possible. Thanks.",28/Aug/12 03:03;analog.sony;Working.,12/Oct/12 07:54;analog.sony;Please find the patch for this bug,12/Oct/12 17:48;dvryaboy;marking as patch available,"12/Oct/12 19:54;jcoveney;Anand,

Thanks for working on this.

Some feedback:

1. make sure the spacing is good. No tabs, four space indents, etc. I realize a lot of existing Pig code is janky, but we try to make sure new stuff doesn't introduce that.
2. I don't know if your test case is actually testing the change? I mean, the change you made has COUNT return null in the case when you give it a null bag, right? So why does it return 0?
3. use assertEquals(expected, actual) instead of assertTrue","12/Oct/12 20:36;analog.sony;I will modify test case and make it as   
assertNull(count.exec(tup));

Also, I will clean up the code (remove tab and add space) and update the patch.
","14/Oct/12 06:45;analog.sony;Please find  pig_1283-3.patch for this bug.
I added the changes suggested by Jonathan.","18/Oct/12 23:28;jcoveney;Anand,

Thanks for the contribution! Committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FLATTEN, reorder columns, UNION causes uid conflict",PIG-2465,12537991,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,dwahler,dwahler,10/Jan/12 23:09,29/Nov/13 06:57,13/Mar/19 23:13,29/Nov/13 06:57,0.10.0,0.9.1,,,,,,,,,,,,,,,,,,,,1,,,,,,,,"This is a regression in the new logical plan that causes incorrect results in 0.8/0.9, and a fatal ""duplicate uid in schema"" error on trunk. The following script demonstrates the problem (extracted and simplified from a much larger script):

{code}A = LOAD 'bug.in' AS (x:{t:(x:int)}, y:{t:(y:int)});
B1 = FOREACH A GENERATE FLATTEN(x),FLATTEN(y);
B2 = FOREACH A GENERATE FLATTEN(y),FLATTEN(x);
C = UNION B1, B2;
D = GROUP C BY *;{code}

Input data:
{code}{(1)}	{(2)}
{(1)}	{(3)}{code}

C contains the correct data:
{code}(1,2)
(2,1)
(1,3)
(3,1){code}

D should use the entire tuple as the group key (making it essentially a DISTINCT) but instead the output is:
{code}((1,1),{(1,2),(1,3)})
((2,2),{(2,1)})
((3,3),{(3,1)}){code}

The GROUP operation is using ($0,$0) as the key instead of ($0,$1). The logical plan includes the line: {{C: (Name: LOUnion Schema: x::x#37:int,y::y#37:int)}}. Switching to the old logical plan produces the correct output in 0.8, but apparently this is no longer possible in 0.9 and later versions. ",,,,,,,,,,,,,,,,,11/Jan/12 01:28;dwahler;PIG-2465-0.8.1.out;https://issues.apache.org/jira/secure/attachment/12510141/PIG-2465-0.8.1.out,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-11 00:08:24.489,,,no_permission,,,,,,,,,,,,,223497,,,,Fri Nov 29 06:57:16 UTC 2013,,,,,,,0|i0h2sv:,97716,,,,,,,,,,"11/Jan/12 00:08;daijy;We solved similar uid conflict bug in PIG-1705. However, here we don't generate a split, we duplicate A directly. Logical plan should address this case as well. 

In the mean time, please use two load statement:
{code}
A1 = LOAD 'bug.in' AS (a:{t:(x:int)}, b:{t:(y:int)});
A2 = LOAD 'bug.in' AS (a:{t:(x:int)}, b:{t:(y:int)});
B1 = FOREACH A1 GENERATE FLATTEN(a),FLATTEN(b);
B2 = FOREACH A2 GENERATE FLATTEN(b),FLATTEN(a);
C = UNION B1, B2;
D = GROUP C BY *;
{code}

Also this bug does not show in 0.8.1.","11/Jan/12 01:28;dwahler;Thanks for taking a look at this. In the actual script that led me to discover the problem, A is actually the result of a cogroup operation, so I've worked around the problem by using JOINs instead. 

The bug is present in 0.8.1, as far as I can tell. I originally discovered it while using Cloudera's pig-0.8.1-cdh3u2, but the official release shows the same behavior. Attaching output from 0.8.1 including the output of EXPLAIN.

After poking around in the code a bit, it looks like this can be fixed by always generating new uids in LOUnion.getSchema(). I don't understand the planner deeply enough to know whether that's a good idea, but it fixes the bug without breaking the core unit tests. I'd be happy to provide a patch if this sounds reasonable.",11/Jan/12 01:34;daijy;I prefer to generate new uid when we duplicate A. But you are welcome to upload the patch for our reference. Thanks.,"21/Nov/13 01:24;dwahler;I ran into some problems with the approach I was taking, but fortunately, this seems to be fixed in 0.12.0.",29/Nov/13 06:57;daijy;Thanks for verifying. Close the ticket.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig.script's deserialized version does not maintain line numbers,PIG-3579,12679540,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jgzhang,aniket486,aniket486,16/Nov/13 01:34,18/Nov/13 20:03,13/Mar/19 23:13,18/Nov/13 20:03,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"If pig.script is decoded with base64, it loses line numbers because the buffered reader that adds the lines, removes '\n's.

{code}
ScriptState.java#setScript
protected void setScript(BufferedReader reader) {
        StringBuilder sb = new StringBuilder();
        try {
            String line = reader.readLine();
            while (line != null) {
                if (line.length() > 0) {
                    sb.append(line).append(""\n"");
                }
                line = reader.readLine();
            }
        } catch (IOException e) {
            LOG.warn(""unable to parse the script"", e);
        }
        setScript(sb.toString());
    }
{code}",,,,,,,,,,,,,,,,,18/Nov/13 19:45;jgzhang;PIG-3579.patch;https://issues.apache.org/jira/secure/attachment/12614459/PIG-3579.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-18 19:45:23.435,,,no_permission,,,,,,,,,,,,,358900,,,,Mon Nov 18 20:03:27 UTC 2013,,,,,,,0|i1pvav:,359190,,,,,,,,,,"18/Nov/13 19:45;jgzhang;Removed the if statement.
Since:
String str =""\n\na""
The if statement will evaluate to false, and we will miss a line.",18/Nov/13 19:52;aniket486;+1,18/Nov/13 20:03;aniket486;Committed to trunk and branch 0.12. Thanks [~jgzhang] for your contribution!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document how parameter values that contain spaces can be passed to Pig,PIG-2674,12553223,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,aniket486,aniket486,aniket486,28/Apr/12 00:01,05/Nov/13 00:51,13/Mar/19 23:13,05/Nov/13 00:51,,,,,,,,,,,,,,,,,,,,,,0,docs,simple,,,,,,"I tried to run this on pig-0.9 branch and pig-trunk. This isn't fixed!

Try--
{code}
java -cp pig.jar org.apache.pig.Main -x local -f temp.pig -p ""NAME=Aniket Mokashi""
{code}

temp.pig-
{code}
a = load '1.txt';
b = foreach a generate '$NAME';
dump b;
{code}",,,,,,,,,,,,,PIG-2458,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-04-28 00:37:16.406,,,no_permission,,,,,,,,,,,,,237302,,,,Tue Nov 05 00:51:08 UTC 2013,,,,,,,0|i0h45j:,97935,,,,,,,,,,"28/Apr/12 00:37;dvryaboy;try -p ""NAME='Aniket Mokashi'""","29/Apr/12 03:42;aniket486;Yes. That works. ""NAME=Aniket\ Mokashi"" works too.
Do we need this jira?",29/Apr/12 21:20;dvryaboy;Let's make this a documentation jira.,19/Nov/12 20:13;julienledem;this will be fixed in a future release,05/Nov/13 00:51;aniket486;Added to confluence at - https://cwiki.apache.org/confluence/display/PIG/FAQ#FAQ-Q%3AHowcanIpassaparameterwithspacetoapigscript%3F.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RANDOM should allow seed initialization for ease of testing,PIG-2965,12611424,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,aneeshs,aneeshs,11/Oct/12 20:20,06/Sep/13 16:12,13/Mar/19 23:13,12/Oct/12 17:33,,,,,,,,,,,,,,,,,,,,,,0,newbie,,,,,,,,,,,,,,,,,,,,,,,,12/Oct/12 02:45;jcoveney;PIG-2965-0.patch;https://issues.apache.org/jira/secure/attachment/12548849/PIG-2965-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-12 02:45:30.801,,,no_permission,,,,,,,,,,,,,247786,,,,Fri Sep 06 16:12:29 UTC 2013,,,,,,,0|i08puf:,48786,,,,,,,,,,"12/Oct/12 02:45;jcoveney;Good idea, Aneesh.",12/Oct/12 17:18;dvryaboy;+1,"05/Sep/13 20:13;sdeneefe;Which version of pig is this in?  I tried in version 11.1 (CDH4.3) and it seemed to ignore the number passed in.  I re-ran a pig script with a hardcoded number several times, and each time got different results.","05/Sep/13 23:11;dvryaboy;[~sdeneefe] are you sure you are using it right? I just tested and it works.

Here's a test script you can run a few times :
{code}
define rand RANDOM('12345');

lines = load 'random.pig';
r = foreach lines generate rand();
dump r;
{code}

run using `pig -x local random.pig 2>/dev/null`","06/Sep/13 08:47;sdeneefe;Okay, that works, thanks!

I was using it differently, and it didn't work:

{code}
lines = load 'random.pig';
r = foreach lines generate RANDOM('12345');
dump r;
{code}

I didn't understand the difference (now I think I do), but thanks for giving this example.  I am glad to see that when using define it works as expected.","06/Sep/13 16:12;dvryaboy;A UDF essentially has a constructor and an exec method. ""foreach lines udf(foo)"" calls the exec method and passes to it the foo parameter. ""define udfinstance udf(foo)"" passes foo to the constructor, and makes an instance of the foo udf initialized in that way bound to ""udfinstance"" (so you can have many differently initialized udfs in the same script).  You can read more info on all this in the docs about ""define"" keyword and the UDF author's guide.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AvroStorage failed to read paths separated by commas,PIG-3422,12663530,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,yuanlid,explosion7,explosion7,13/Aug/13 23:03,15/Aug/13 23:11,13/Mar/19 23:13,15/Aug/13 23:11,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Suppose I want to load data using this script:
a = load './newavro/data/avro/Employee3.ser,./newavro/data/avro/Employee4.ser' USING AvroStorage ();
It will fail because multiple paths separated by commas are not handled by Avrostorage",,,,,,,,,,,,,,,,,15/Aug/13 22:58;explosion7;PIG-3422_08152013.patch;https://issues.apache.org/jira/secure/attachment/12598317/PIG-3422_08152013.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-15 23:11:22.421,,,no_permission,,,,,,,,,,,,,343531,,,,Thu Aug 15 23:11:22 UTC 2013,,,Patch Available,,,,0|i1n8rz:,343835,,,,,,,,,,"15/Aug/13 00:43;explosion7;patch added, including unit test case",15/Aug/13 23:11;rohini;+1. Committed to trunk (0.12). Thanks Yuanli.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig parser  complain  same value passed to macro when expanding macro,PIG-3394,12659737,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,abacus,abacus,25/Jul/13 07:59,26/Jul/13 18:50,13/Mar/19 23:13,26/Jul/13 18:50,0.12.0,,,,,,,,,,,,,,,,,,parser,,,0,,,,,,,,"[PIG_2247|https://issues.apache.org/jira/browse/PIG-2247] add the same arguments detecting before passed to macro, but cause the following problems 
{noformat} 
define simple_macro(in_relation, min_gpa, max_gpa) returns c {
                  b = filter $in_relation by gpa >= $min_gpa;
                  $c = foreach b generate age, name;}

a = load 'sample.txt';
b = simple_macro(a, 100, 100);
{noformat} 

when launch in pig, it complain 
{noformat}
ERROR org.apache.pig.Main - ERROR 1200: <file abc.pig.substituted, line 7>
 Cannot expand macro 'simple_macro'. Reason:  Duplicated arguments names are 
 passed in macro: number of arguments: 3 number of distinct arguments: 2
{noformat}

it seems passing the same value to different paramter also not allowed",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-07-25 18:27:05.977,,,no_permission,,,,,,,,,,,,,339929,,,,Fri Jul 26 18:50:59 UTC 2013,,,,,,,0|i1mmnz:,340247,,,,,,,,,,"25/Jul/13 18:27;dreambird;[~abacus], this is the introduced by bug fix PIG-2247. So it is expected. I don't see a need to pass a same value twice as argument. In your case, is there any specific reason you want to set up duplicated argument 'min_gpa' twice in macro?","26/Jul/13 06:05;abacus;
hi, [~dreambird]
the real situation is that i want to load  a period of data in a macro like following
{noformat}
a = load_period_data('$table', '$start_day', '$end_day')
{noformat}

so it's common to load one day through passing start_day and  end_day the same value","26/Jul/13 06:14;abacus;i'm sorry  i didn't describe the problem clear, my situation is not using the same named paramters in macro, but passing same value to differnt paramters , i improved my problem description above.","26/Jul/13 06:51;dreambird;[~abacus], thanks for explanations! this is for sure a valid point. I will try to come up a patch soon, to enhance the original fix. ","26/Jul/13 17:57;cheolsoo;[~dreambird], I think we should revert PIG-2247. Your patch detects duplicate arguments, but it should detect duplicate parameters.

How about reverting it until you come up with a proper fix? Let me know what you think.","26/Jul/13 18:04;dreambird;[~cheolsoo], sure, I am fine with it. Just want to make sure how this revert works. Should we revert it in this jira, or open separate jira, or reopen original jira? Any suggestion?","26/Jul/13 18:15;cheolsoo;Since PIG-2247 is not closed (and not released), we can simply reopen it.

You can upload a new patch to PIG-2237, and we will close this jira out. Does this sound reasonable?",26/Jul/13 18:26;dreambird;sounds great! Thanks for [~abacus] report this jira!,"26/Jul/13 18:50;cheolsoo;As discussed, I reverted PIG-2247:
http://svn.apache.org/viewvc?view=revision&revision=1507395

Closing the jira now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig pom.xml does not bring in joda-time as dependency,PIG-3282,12643469,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,sushanth,sushanth,sushanth,19/Apr/13 00:44,19/Apr/13 21:16,13/Mar/19 23:13,19/Apr/13 21:16,0.11.1,,,,,,,,,,,,,,,,0.11.2,,,,,0,,,,,,,,"http://search.maven.org/remotecontent?filepath=org/apache/pig/pig/0.11.1/pig-0.11.1.pom

From the maven repo, pig does not bring in joda-time as a dependency in pom.xml. This is causing unit test failures from hcatalog.",,,,,,,,,,,,,,,,,19/Apr/13 01:57;sushanth;PIG-3282.patch;https://issues.apache.org/jira/secure/attachment/12579466/PIG-3282.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-04-19 21:14:24.18,,,no_permission,,,,,,,,,,,,,323836,Reviewed,,,Fri Apr 19 21:16:46 UTC 2013,,,,,,,0|i1jvdz:,324181,,,,,,,,,,"19/Apr/13 01:42;sushanth;Note, this is not a problem in trunk, since trunk builds directly from ivy.xml. However, with 0.11.1, it uses the provided ivy.pom file.",19/Apr/13 01:57;sushanth;Patch attached.,"19/Apr/13 21:14;daijy;We can check into branch 0.11 anyway, though we might or might not have additional 0.11 releases.",19/Apr/13 21:16;daijy;Patch committed to 0.11 branch. Thanks Sushanth!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.pig.pigunit.pig.PigServer does not initialize set default log level of pigContext,PIG-2833,12599641,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,cheolsoo,johannesch,johannesch,20/Jul/12 16:30,21/Sep/12 21:36,13/Mar/19 23:13,21/Sep/12 21:36,0.10.0,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"The class org.apache.pig.pigunit.pig.PigServer does not set the default log level of its instance of PigContext so that pigunit tests that have 

{code}
set debug off;
{code}

in them, will cause a NullPointerException at org.apache.pig.PigServer line 291 because the default log level is not set.

So I think org.apache.pig.pigunit.pig.PigServer should do something like 

{code}
pigContext.setDefaultLogLevel(Level.INFO);
{code}

in its contructors.","pig-0.10.0, Hadoop 2.0.0-cdh4.0.1 on Kubuntu 12.04 64Bit.",,,,,,,,,,,,,,,,21/Jul/12 00:58;cheolsoo;PIG-2833.patch;https://issues.apache.org/jira/secure/attachment/12537433/PIG-2833.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-21 00:58:14.629,,,no_permission,,,,,,,,,,,,,256442,,,,Fri Sep 21 21:35:53 UTC 2012,,,,,,,0|i0h5s7:,98199,,,,,,,,,,"21/Jul/12 00:58;cheolsoo;Attached is a patch that initializes the default log level of PigContext to Level.INFO.

I also added two test cases to TestGrunt to verify ""set debug on/off"" work properly.","23/Jul/12 14:09;johannesch;Thank you very much for your quick reaction, Cheolsoo! Could you also have a look at PIG-2832 which might be a similar issue?","21/Sep/12 21:35;jcoveney;Thanks for the contribution, Cheolsoo! Committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Errors and lacks in document ""Pig Latin Basics""",PIG-2901,12605852,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,miyakawataku,miyakawataku,miyakawataku,01/Sep/12 13:20,10/Sep/12 04:39,13/Mar/19 23:13,10/Sep/12 04:39,0.10.0,,,,,,,,,,,,,,,,,,documentation,,,0,documentation,,,,,,,"This is a patch to fix errors and lacks in document ""Pig Latin Basics"".

# States that COGROUP groups records with a null key _from different relations_ separately.
# ""A map key must be a -scalar- +chararray+ ""
# Removes a statement which says that a star expression is a tuple expression (it seems incorrect)
# Fixes a subject confusion of a sentence ""When two bytearrays are used in arithmetic expressions...""
# Updates a link to Java API documentation.
# Fixes a tuple example: ""LOAD 'data' as..."" -> ""A = LOAD 'data' as...""
# ""the asterisk (\*) is used to project all -tuples- +fields+ ""
# A result of COGROUP with two relations contains _three_ fields, not _two_
# Removes an example of COGROUP INNER, which is deprecated
# Removes a sentence which says ""JOIN operator always performs an inner join"". Actually, JOIN also perform an outer join.
# JOIN ""Performs an outer join of two -ore more- relations""
# Replaces an example of ""-Dpig.additional.jars"" with a jar file on HDFS. The current version incorrectly shows an example of a Pig script on HDFS.
# Fixes typos, lack of hyperlinks, inappropriate indentation, and incorrect chaptering.",,,,,,,,,,,PIG-2756,,,,,,06/Sep/12 15:10;miyakawataku;PIG-2901.patch;https://issues.apache.org/jira/secure/attachment/12544055/PIG-2901.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-05 05:31:35.583,,,no_permission,,,,,,,,,,,,,256485,,,,Mon Sep 10 04:39:12 UTC 2012,,,Patch Available,,,,0|i0h6gf:,98308,,,,,,,,,,01/Sep/12 13:23;miyakawataku;Patch which fixes the errors and lacks (for trunk),"05/Sep/12 05:31;billgraham;Thanks Miyakawa! These are great fixes.

Two comments:

Re #3 above, I don't think it's incorrect. It says ""The simplest tuple expression is the star expression, which represents all fields"" which is different than saying a star expression is a tuple expression. Any other opinions out there re this?

Looks like there's actually a second typo on one of the lines you corrected (see firend1). Would you mind updating that one too please?
{noformat}
-X: {group: chararray,A: {owner: chararray,pet: chararray},b: {firend1: chararray,friend2: chararray}}
+X: {group: chararray,A: {owner: chararray,pet: chararray},B: {firend1: chararray,friend2: chararray}}
{noformat}
  ","06/Sep/12 15:10;miyakawataku;Thank you for the comments.

I've updated the patch (firend1 -> friend1, and removes #3 tentatively) .

Re #3, I thought the sentence was wrong because a star expression does not produce a tuple, but projects all fields of the input relation. Therefore, I thought, a star expression cannot be a tuple expression.

However, I got more unsure of the whole section ""Tuple Expressions"". The section seems to talk about an expression of the form ""(x, y, z, ...)"" which produces a tuple value, but as far as I know, such a syntax is allowed only to produce a tuple constant, which is already described on the section ""Constants"".

Maybe I misunderstand or overlook something. What do you think about it?","07/Sep/12 01:20;billgraham;I think the Tuple Expressions section makes sense as written. It's pointing out how you can represent a tuple via expressions, which is different than a tuple of constants. For example these would each be tuple expressions I'd think:

{noformat}
C = FOREACH A GENERATE name, age, MyUDF(*);
C = FOREACH A GENERATE MyUDF(name), *;
{noformat}
","08/Sep/12 06:18;miyakawataku;Thank you, now I understand what the section means.

Could you apply the patch?
","10/Sep/12 04:39;billgraham;Committed, thanks Miyakawa! This will appear in the Pig 0.11 docs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobControlCompiler mis-logs after reducer estimation,PIG-2884,12604132,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,20/Aug/12 21:56,20/Aug/12 23:54,13/Mar/19 23:13,20/Aug/12 23:54,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"This section of code produces erroneous logging messages:

{noformat}
mro.estimatedParallelism = estimateNumberOfReducers(nwJob, mro);
// reducer estimation could return -1 if it couldn't estimate
log.info(""Could not estimate number of reducers and no requested or default "" +
         ""parallelism set. Defaulting to 1 reducer."");
jobParallelism = mro.estimatedParallelism > 0 ? mro.estimatedParallelism : 1;
{noformat}

Fix is to use a if/then. Patch forthcoming.",,,,,,,,,,,,,,,,,20/Aug/12 21:58;billgraham;PIG-2884.1.patch;https://issues.apache.org/jira/secure/attachment/12541660/PIG-2884.1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-20 23:45:16.556,,,no_permission,,,,,,,,,,,,,256475,,,,Mon Aug 20 23:45:16 UTC 2012,,,,,,,0|i0h6af:,98281,,,,,,,,,,20/Aug/12 23:45;jcoveney;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add flag to ant to run tests with a debugger port,PIG-2851,12600749,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,billgraham,billgraham,billgraham,30/Jul/12 23:08,09/Aug/12 22:00,13/Mar/19 23:13,09/Aug/12 22:00,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Sometimes it's helpful to start up {{ant test}} with a debug port enabled. Let's add a flag {{debugPort=[port\]}} to allow this. For example:

{noformat}
ant test -DdebugPort=5000 -Dtestcase=TestFoo
{noformat}

would run the test case and wait for a debugger client to connect to port 5000 before continuing.
",,,,,,,,,,,,,,,,,30/Jul/12 23:12;billgraham;PIG-2851.1.patch;https://issues.apache.org/jira/secure/attachment/12538450/PIG-2851.1.patch,09/Aug/12 22:00;billgraham;PIG-2851.2.patch;https://issues.apache.org/jira/secure/attachment/12540137/PIG-2851.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-08-09 16:24:56.203,,,no_permission,,,,,,,,,,,,,256455,,,,Thu Aug 09 22:00:26 UTC 2012,,,,,,,0|i0h5yn:,98228,,,,,,,,,,09/Aug/12 16:24;jcoveney;+1,09/Aug/12 22:00;billgraham;This is an updated path with merged changes from trunk. Committing now.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent URL in Docs,PIG-2841,12600364,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,eric59,eric59,eric59,26/Jul/12 19:05,27/Jul/12 20:31,13/Mar/19 23:13,27/Jul/12 20:31,0.10.0,,,,,,,,,,,,,,,,,,documentation,,,0,,,,,,,,"There are inconsistent links to ""cont.html#Parameter-Sub"" throughout the documentation. For some ""Parameter-Sub"" is all lowercase, some have it with the case shown here. 

At least for Chrome, this results in a broken link, where the browser won't scroll to the correct section in the page.

The attached patch updates all to use the ""Parameter-Sub"" casing.",,,,,,,,,,,,,,,,,26/Jul/12 19:06;eric59;PIG-2841-0.patch;https://issues.apache.org/jira/secure/attachment/12538059/PIG-2841-0.patch,27/Jul/12 20:15;eric59;PIG-2841-1.patch;https://issues.apache.org/jira/secure/attachment/12538206/PIG-2841-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-07-27 20:31:12.814,,,no_permission,,,,,,,,,,,,,256447,,,,Fri Jul 27 20:31:12 UTC 2012,,,Patch Available,,,,0|i0h5uv:,98211,,,,,,,,,,27/Jul/12 20:15;eric59;Update patch to correctly change files in the Pig rep.,"27/Jul/12 20:31;billgraham;Committed, thanks Eric!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in Documentation,PIG-2843,12600387,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,eric59,eric59,eric59,26/Jul/12 21:03,27/Jul/12 20:13,13/Mar/19 23:13,27/Jul/12 20:13,0.10.0,,,,,,,,,,,,,,,,,,documentation,,,0,,,,,,,,"There's a small typo in start.html (missing a space). The attached patch fixes the issue.

This same typo is in start.pdf as well, but I'm unsure how to update that file. If someone can point me to directions I'll gladly add that to the patch.",,,,,,,,,,,,,,,,,26/Jul/12 21:04;eric59;PIG-2843-0.patch;https://issues.apache.org/jira/secure/attachment/12538079/PIG-2843-0.patch,27/Jul/12 20:08;billgraham;PIG-2843-1.patch;https://issues.apache.org/jira/secure/attachment/12538205/PIG-2843-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-07-27 20:08:34.34,,,no_permission,,,,,,,,,,,,,256449,,,,Fri Jul 27 20:13:02 UTC 2012,,,Patch Available,,,,0|i0h5vr:,98215,,,,,,,,,,"27/Jul/12 20:08;billgraham;Thanks for the fix Eric!

This change actually needs to be made in the pig repo in {{src/docs/src/documentation/content/xdocs/start.xml}} (i.e., the user docs). These files get sourced by the site docs in {{pig/site}} at build time to generate the html file you submitted. See my revised patch.

Will commit shortly. ","27/Jul/12 20:13;billgraham;Committed, thanks Eric!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Training link on front page no longer points to Pig training,PIG-2826,12599362,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,18/Jul/12 22:02,23/Jul/12 17:47,13/Mar/19 23:13,23/Jul/12 17:47,site,,,,,,,,,,,,,,,,site,,site,,,0,,,,,,,,"The training link on Pig's website used to point to a Pig specific video on Cloudera's site.  It now points to a list of all their videos.  Also, at the time they were the only ones providing training videos for Hadoop.  Now other vendors do as well.  This link should be replaced by a link to a wiki page where vendors who wish to can list their training resources.",,,,,,,,,,,,,,,,,18/Jul/12 22:21;alangates;PIG-2826.patch;https://issues.apache.org/jira/secure/attachment/12537084/PIG-2826.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-20 21:32:38.734,,,no_permission,,,,,,,,,,,,,248752,,,,Mon Jul 23 17:47:46 UTC 2012,,,,,,,0|i09y9j:,55992,,,,,,,,,,20/Jul/12 21:32;thejas;+1,23/Jul/12 17:47;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test regressions from PIG-2632,PIG-2813,12598629,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,jcoveney,jcoveney,jcoveney,12/Jul/12 21:27,13/Jul/12 01:03,13/Mar/19 23:13,13/Jul/12 01:03,0.11,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"The following tests seemed to have been broken by PIG-2632:

TestEmptyInputDir
TestFRJoin
TestFRJoinNullValue
TestNumberOfReducers
TestPhyPatternMatch
TestScriptLanguage
TestMergeJoin

Thankfully, the fix is easy. Am making sure it works now, but it has greenlighted them (I think).",,,,,,,,,,,,,,,,,12/Jul/12 22:35;jcoveney;PIG-2813-0.patch;https://issues.apache.org/jira/secure/attachment/12536298/PIG-2813-0.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-12 22:50:38.672,,,no_permission,,,,,,,,,,,,,256429,,,,Thu Jul 12 22:50:38 UTC 2012,,,Patch Available,,,,0|i0h5l3:,98167,,,,,,,,,,12/Jul/12 22:50;julienledem;+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig cannot dereference Cassandra subcolumns in a Super Column Family,PIG-1849,12498251,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,rstrickland,rstrickland,10/Feb/11 14:45,18/May/12 14:10,13/Mar/19 23:13,18/May/12 14:10,0.8.0,,,,,,,,,,,,,,,,,,data,,,1,cassandra,,,,,,,"When using the ColumnFamilyInputFormat to load data from a Cassandra Super Column Family, the subcolumns always return in a bag where individual values cannot be dereferenced, no matter what schema is used.  Flattening does not solve the issue.","Ubuntu 10, Cassandra 0.7, Hadoop",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-11 23:21:17.34,,,no_permission,,,,,,,,,,,,,64792,,,,Wed Aug 17 16:23:45 UTC 2011,,,,,,,0|i0gzjj:,97188,,,,,,,,,,"11/Feb/11 23:21;olgan;Do you jave a loader that wraps the input format. I would suspect that the problem will be in the loader that produces the data.

Also, please, add the script and the error that you are getting","14/Feb/11 14:44;rstrickland;I am using the input format directly, with this sample data:

(6B108476-1C40-4847-A1B0-9DA4B0B0BF83,{(12345,{(TestColumn,This is a test),(TestColumn2,This is a test 2)}),(12346,{(TestColumn1,This is a test 1),(TestColumn2,This is a test 2)})})

and this load statement:

rows = LOAD 'cassandra://E3/StreamByProfile' USING CassandraStorage() AS (objectid, scolumns: bag {ST: tuple(timestamp, columns: bag {T: tuple(name:chararray, value)})});

I have tried quite a number of different schema possibilities, but all produce effectively the same result.  They don't produce an error; when you attempt to reference individual items in a bag you still get the full bag (even though it allows the syntax).  Attempts to flatten create the same issue.",11/Apr/11 17:24;jeromatron;I wonder if this is fixed by PIG-1866.,11/Apr/11 17:50;rstrickland;Looks like it probably is. Once there's a fix available I'll give it a try.,11/Apr/11 18:31;jeromatron;Robbie - look at Daniel's last comment.  Looks like a fix is in trunk for dereferencing bags inside a tuple - in case you wanted to try it.,11/Apr/11 19:40;rstrickland;PIG-1866 may be the underlying cause of this issue.,17/Aug/11 16:23;fabio.souto;Finally PIG-1866 solve this issue? ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error in EvalFunc ctor when implementing Algebraic UDF whose return type is parameterized,PIG-2685,12554115,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,hazen,hazen,07/May/12 06:57,08/May/12 23:57,13/Mar/19 23:13,08/May/12 22:17,0.10.0,,,,,,,,,,,,,,,,,,impl,,,0,,,,,,,,"EvalFunc ctor uses reference equality instead of value equality when verifying return type of ""final"" UDF and parent Algebraic UDF are equal. This is fine in the case when both values are Class instances, but breaks when return types are ParameterizedType instances. This prohibits the creation of an Algebraic UDF whose return type is {{Map<String, Long>}}, for instance. Please see the included unit test for an example along these lines.",,,,,,,,,,,,,,,,,08/May/12 22:17;jcoveney;PIG-2685_2.patch;https://issues.apache.org/jira/secure/attachment/12526057/PIG-2685_2.patch,07/May/12 14:21;hazen;fixes_eval_func_algebraic_final_return_type_check-r2.diff;https://issues.apache.org/jira/secure/attachment/12525852/fixes_eval_func_algebraic_final_return_type_check-r2.diff,07/May/12 06:57;hazen;fixes_eval_func_algebraic_final_return_type_check.diff;https://issues.apache.org/jira/secure/attachment/12525807/fixes_eval_func_algebraic_final_return_type_check.diff,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-05-07 07:34:14.534,,,no_permission,,,,,,,,,,,,,238339,,,,Tue May 08 23:57:14 UTC 2012,,,Patch Available,,,,0|i02u13:,14466,,,,,,,,,,"07/May/12 07:34;jcoveney;Thanks Andy!

Given that this is a new feature that should be supported by the language, we should add a test[1] that fails under the current build and ensures that parameterized types are picked up by Algebraics.

[1] I know, I know, nobody enjoys writing tests, but I don't think it will be a terrible one to add!","07/May/12 14:09;hazen;The patch includes a test which fails under current build and passes with the patch to EvalFunc. Please let me know what additional tests you'd like to see here. Should I have submitted the test in a separate patch file?
",07/May/12 14:14;hazen;Strange. So the patch I uploaded; Somehow it's missing the test. I'll re-up in a minute.,07/May/12 14:23;hazen;Patch updated. I'll try and avoid midnight patch submissions in the future ;),"08/May/12 22:17;jcoveney;No worries! Thanks for the submission. Committed to 0.10 and trunk. r1335797 and r1335798. One note, I moved the test to package org.apache.pig. Bill G sent out an email a bit ago about how we should add new tests to the package of the class that they test, and I agree, so starting here :) Will attach the patch I applied.","08/May/12 23:57;hazen;+1 to Bill's recommendation. I thought about that as I was adding the test!

Thanks for all the patch shepherding :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigStorageSchema initializes schema and PigContext for every tuple,PIG-2555,12544065,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rangadi,rangadi,rangadi,24/Feb/12 22:06,06/Apr/12 20:38,13/Mar/19 23:13,06/Apr/12 20:38,0.9.1,,,,,,,,,,,,,,,,0.9.3,,,,,0,,,,,,,,"
{{PigStorageSchema.getNest()}} initializes schema (and as a result PigContext) for every tuple by mistake. Initializing PigContext is extremely costly.

This does not affect 10.x or trunk since PigStorageSchema's functionality is rolled into PigStorage.

",,,,,,,,,,,,,,,,,29/Feb/12 20:18;rangadi;PIG-2555-branch-0.9.patch;https://issues.apache.org/jira/secure/attachment/12516601/PIG-2555-branch-0.9.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-25 01:54:43.646,,,no_permission,,,,,,,,,,,,,229303,,,,Fri Apr 06 20:38:26 UTC 2012,,,,,,,0|i0h37b:,97781,,,,,,,,,,"25/Feb/12 01:54;billgraham;Raghu, did you verify that this bug still exists on the trunk of piggybank? I thought this was fixed.","25/Feb/12 23:21;dvryaboy;In trunk of piggybank, it just redirects to builtins, where the problem is fixed.
We can backport to 0.9 which is what I think Raghu is suggesting in this ticket. It'd be a messy backport due to the 0.10 PigStorage rewrite that this was a part of.","29/Feb/12 20:21;rangadi;I meant to fix this only for 0.9. patch attached. in 0.10 and above this functionality is part of normal PigStorage.

Daniel, did you mean to change the Fix Version to 0.10? The bug does not exist there (didn't positively confirm, but I am pretty sure).

","02/Mar/12 08:21;daijy;Fix version was 0.9.2, since I close 0.9.2 tag, Jira move all 0.9.2 issues to 0.10. Feel free to correct it.",06/Apr/12 20:38;dvryaboy;Committed to 0.9.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig TestGrunt.testShellCommand occasionally fails,PIG-2499,12540601,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,tomwhite,tomwhite,tomwhite,31/Jan/12 19:37,01/Feb/12 06:32,13/Mar/19 23:13,01/Feb/12 06:32,,,,,,,,,,,,,,,,,,,build,,,0,,,,,,,,"The following code fails from time to time (with a NPE since {{fileReader.readLine()}} returns null) :

{noformat}
            strCmd = ""sh bash -c 'touch TouchedFileInsideGrunt_61 | ls | grep TouchedFileInsideGrunt_61 > fileContainingTouchedFileInsideGruntShell_71'"";
            cmd = new ByteArrayInputStream(strCmd.getBytes());
            reader = new InputStreamReader(cmd);
            grunt = new Grunt(new BufferedReader(reader), context);
            grunt.exec();
            fileReader = new BufferedReader(new FileReader(""fileContainingTouchedFileInsideGruntShell_71""));
            assertTrue(fileReader.readLine().equals(""TouchedFileInsideGrunt_61""));
{noformat}",,,,,,,,,,,,,,,,,31/Jan/12 23:32;tomwhite;PIG-2499.patch;https://issues.apache.org/jira/secure/attachment/12512693/PIG-2499.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-01 06:32:08.317,,,no_permission,,,,,,,,,,,,,226014,Reviewed,,,Wed Feb 01 06:32:08 UTC 2012,,,,,,,0|i0h2xz:,97739,,,,,,,,,,"31/Jan/12 19:41;tomwhite;I wonder if {{touch TouchedFileInsideGrunt_61 | ls}} is causing a race, we should probably change it to {{touch TouchedFileInsideGrunt_61 && ls}} for safety's sake anyway.

PIG-2206 was reporting this test as failing too. I don't think it's Mac specific as I have seen this failure on Linux too.",31/Jan/12 23:32;tomwhite;Trivial patch.,01/Feb/12 06:32;daijy;I do see it occasionally as well. Patch committed to trunk. Thanks Tom!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
merge join resutls in ERROR 2176,PIG-2392,12533529,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,araceli,araceli,01/Dec/11 22:55,26/Jan/12 22:44,13/Mar/19 23:13,26/Jan/12 22:44,,,,,,,,,,,,,,,,,0.9.2,,impl,,,0,,,,,,,,"This is  regression for dotNext.

a = load '/user/user1/pig/tests/data/singlefile/studenttab10k';
b = load '/user/user1/pig/tests/data/singlefile/votertab10k';
c = order a by $0;
d = order b by $0;
store c into '/user/user1/pig/out/user1.1322779146/dotNext_MergeJoin_1.out.intermediate1';
store d into '/user/user1/pig/out/user1.1322779146/dotNext_MergeJoin_1.out.intermediate2';
exec;
e = load '/user/user1/pig/out/user1.1322779146/dotNext_MergeJoin_1.out.intermediate1';
f = load '/user/user1/pig/out/user1.1322779146/dotNext_MergeJoin_1.out.intermediate2';
g = join e by $0, f by $0 using 'merge';
store g into '/user/user1/pig/out/user1.1322779146/dotNext_MergeJoin_1.out'

Backend error message
---------------------
AttemptID:attempt_1321041443489_3292_m_000000_0 Info:Error: org.apache.pig.backend.executionengine.ExecException: ERROR 2176: Error processing right input during merge join
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.throwProcessingException(POMergeJoin.java:458)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNext(POMergeJoin.java:188)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:267)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:262)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:711)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:328)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1152)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)
Caused by: java.io.IOException: Delegation Token can be issued only with kerberos or web authentication
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:4027)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:281)
        at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:365)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1490)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1486)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1152)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1484)

        at org.apache.hadoop.ipc.Client.call(Client.java:1085)
        at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:193)
        at $Proxy8.getDelegationToken(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:100)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:65)
        at $Proxy8.getDelegationToken(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getDelegationToken(DFSClient.java:429)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationToken(DistributedFileSystem.java:812)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationTokens(DistributedFileSystem.java:839)
        at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.getDelegationTokens(ChRootedFileSystem.java:311)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.getDelegationTokens(ViewFileSystem.java:490)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:134)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:90)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:83)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:205)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextInputFormat.listStatus(PigTextInputFormat.java:36)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:269)
        at org.apache.pig.impl.io.ReadToEndLoader.init(ReadToEndLoader.java:154)
        at org.apache.pig.impl.io.ReadToEndLoader.<init>(ReadToEndLoader.java:137)
        at org.apache.pig.impl.builtin.DefaultIndexableLoader.initRightLoader(DefaultIndexableLoader.java:208)
        at org.apache.pig.impl.builtin.DefaultIndexableLoader.seekNear(DefaultIndexableLoader.java:192)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.seekInRightStream(POMergeJoin.java:410)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin.getNext(POMergeJoin.java:186)
        ... 11 more

","-bash-3.1$ hadoop version
Hadoop 0.23.0.1111080202
Subversion http://svn.apache.org/repos/asf/hadoop/common/branches/branch-0.23.0/hadoop-common-project/hadoop-common -r 1196973
Compiled by hadoopqa on Tue Nov  8 02:12:04 PST 2011
From source with checksum 4e42b2d96c899a98a8ab8c7cc23f27ae
-bash-3.1$ pig -version
Apache Pig version 0.9.2.1111101150 (r1200499) 
compiled Nov 10 2011, 19:50:15

-Mountside tables are enabled, but files are being referenced as in Hadoop 20.",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-12-02 23:05:21.18,,,no_permission,,,,,,,,,,,,,219257,Reviewed,,,Thu Jan 26 22:44:59 UTC 2012,,,,,,,0|i0h2fz:,97658,,,,,,,,,,"02/Dec/11 23:05;daijy;It works without viewfs setting. But Araceli find the problem when viewfs is on, though not using any viewfs in the query.",05/Jan/12 18:34;sseth;Should be fixed by MAPREDUCE-3592. The test needs to be rerun with a new Hadoop build.,26/Jan/12 22:44;araceli;Fixed on : 0.23.1.1201120103 \ 0.9.2.1201201216,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bin/pig doesn't have any hooks for picking up ZK installation deployed from tarballs,PIG-2327,12527789,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,rvs,rvs,rvs,19/Oct/11 17:30,22/Nov/11 21:26,13/Mar/19 23:13,22/Nov/11 21:26,0.10.0,0.9.1,,,,,,,,,,,,,,,,,grunt,,,1,bigtop,,,,,,,"The following chunk of bin/pig 

{noformat}
# locate ZooKeeper
if [ -d ""${PIG_HOME}/share/zookeeper"" ]; then
    for f in ${PIG_HOME}/share/zookeeper/zookeeper-*.jar; do
        CLASSPATH=${CLASSPATH}:$f
    done
fi
{noformat}

makes it impossible for pig to pick up ZK installation area
unless the content of the binary tarballs gets massaged in a certain
way.

Would it be possible to have a more generic code in place that
would, perhaps, pay attention to things like ZOOKEEPER_HOME ?",,,,,,,,,,,,,,,,,02/Nov/11 17:42;rvs;PIG-2327.patch.txt;https://issues.apache.org/jira/secure/attachment/12501990/PIG-2327.patch.txt,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-24 06:52:38.034,,,no_permission,,,,,,,,,,,,,89287,,,,Tue Nov 22 21:26:21 UTC 2011,,,,,,,0|i0h24v:,97608,,,,,,,,,,"24/Oct/11 06:52;daijy;This code totally assumes you're using rpm to install Pig/Zookeeper. It has similar code for HBase. Yes, it is better to search for alternative Zookeeper/HBase location and put relevant jars into CLASSPATH automatically.","02/Nov/11 17:43;rvs;I'm attaching a trivial patch. If it looks good, can it be, please, included in 0.10 ?","22/Nov/11 21:26;ashutoshc;+1 Committed. Thanks, Roman!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE thrown during illustrate,PIG-2170,12514274,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,thejas,mat_kelcey,mat_kelcey,15/Jul/11 19:51,05/Aug/11 01:46,13/Mar/19 23:13,05/Aug/11 01:46,0.10.0,,,,,,,,,,,,,,,,,,impl,,,0,,,,,,,,"working with version
https://svn.apache.org/repos/asf/pig/trunk@1146777
fetched from git 
 git://git.apache.org/pig.git a7e1228a0fdfe76c3cff0e749e252dba8d387052

using file /tmp/data.tsv
id1     123
id1     234
id2     345
id2     456

this is the most cutdown/simplest script i can make that illustrates (no pun intended) the problem
grunt> data = load '/tmp/data.tsv'  as (id:chararray, value:long);
grunt> cogrouped = cogroup data by id;
grunt> exists = foreach cogrouped generate (IsEmpty(data.value) ? 0 : 1) as exists;

grunt> dump exists 
is ok

but
grunt> illustrate exists
throws
java.lang.NullPointerException
        at org.apache.pig.pen.IllustratorAttacher.visitBinCond(IllustratorAttacher.java:360)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POBinCond.visit(POBinCond.java:145)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POBinCond.visit(POBinCond.java:36)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:69)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
        at org.apache.pig.impl.plan.DepthFirstWalker.walk(DepthFirstWalker.java:52)
        at org.apache.pig.pen.IllustratorAttacher.innerPlanAttach(IllustratorAttacher.java:417)
        at org.apache.pig.pen.IllustratorAttacher.visitPOForEach(IllustratorAttacher.java:229)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.visit(POForEach.java:117)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.visit(POForEach.java:47)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:69)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
        at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
        at org.apache.pig.impl.plan.DepthFirstWalker.walk(DepthFirstWalker.java:52)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:246)
        at org.apache.pig.pen.ExampleGenerator.getData(ExampleGenerator.java:238)
        at org.apache.pig.pen.LineageTrimmingVisitor.init(LineageTrimmingVisitor.java:103)
        at org.apache.pig.pen.LineageTrimmingVisitor.<init>(LineageTrimmingVisitor.java:98)
        at org.apache.pig.pen.ExampleGenerator.getExamples(ExampleGenerator.java:166)
        at org.apache.pig.PigServer.getExamples(PigServer.java:1201)
        at org.apache.pig.tools.grunt.GruntParser.processIllustrate(GruntParser.java:698)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Illustrate(PigScriptParser.java:591)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:306)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:67)
        at org.apache.pig.Main.run(Main.java:487)
        at org.apache.pig.Main.main(Main.java:108)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

with pig.log containing
java.io.IOException: Exception : null
        at org.apache.pig.PigServer.getExamples(PigServer.java:1207)
        at org.apache.pig.tools.grunt.GruntParser.processIllustrate(GruntParser.java:698)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Illustrate(PigScriptParser.java:591)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:306)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:67)
        at org.apache.pig.Main.run(Main.java:487)
        at org.apache.pig.Main.main(Main.java:108)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

something to do with the IsEmpty since...
grunt> data = load '/tmp/data.tsv'  as (id:chararray, value:long);
grunt> cogrouped = cogroup data by id;
grunt> exists = foreach cogrouped generate data.value as exists;
illustrates ok",,,,,,,,,,,,,,,,,04/Aug/11 03:35;thejas;PIG-2170.1.patch;https://issues.apache.org/jira/secure/attachment/12489302/PIG-2170.1.patch,04/Aug/11 16:23;thejas;PIG-2170.2.patch;https://issues.apache.org/jira/secure/attachment/12489361/PIG-2170.2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-04 16:23:48.406,,,no_permission,,,,,,,,,,,,,66031,,,,Fri Aug 05 01:46:07 UTC 2011,,,,,,,0|i0h1dz:,97487,,,,,,,,,,"04/Aug/11 16:23;thejas;PIG-2170.2.patch fixes a findbugs warning in PIG-2170.1.patch. 
Changes -
- BinCond is not a boolean expression (ie it does not return boolean value), so equivalence classes are not created for it.
- Boolean expressions can appear in foreach (within bincond), and every where else after boolean type is introduced. But adding equivalence classes for each boolean expression in all these places will generate too many examples, affecting the conciseness. So I have chosen not to add equivalence classes in those cases.
- When type conversion errors happened, pig tries to increment counters for it, and was throwing NPE because the local mode simulation did not supply the object that returns counters. That has also been fixed.","04/Aug/11 17:37;thejas;Unit test and test-patch passed (except for a javac deprecation warning as I am using the old mapred Counters class to create counter. I didn't find a new replacement for it. )
In addition to new test cases TestExampleGenerator.java has changes to use local mode and also generate the input files only once. (test now takes 5 sec instead of 20).",04/Aug/11 22:00;daijy;Looks good. +1,05/Aug/11 01:46;thejas;patch committed to trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Project UDF output inside a non-foreach statement fail on 0.8,PIG-2077,12507534,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,17/May/11 21:16,18/Jul/11 23:50,13/Mar/19 23:13,18/Jul/11 23:50,0.8.1,,,,,,,,,,,,,,,,0.8.1,,impl,,,0,,,,,,,,"The following script fail on 0.8:
{code}
A = load '1.txt' as (tracking_id, day:chararray);
B = load '2.txt' as (tracking_id, timestamp:chararray);
C = JOIN A by (tracking_id, day) LEFT OUTER, B by (tracking_id,  STRSPLIT(timestamp, ' ').$0);
explain C;
{code}

Error stack:
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.get(ArrayList.java:324)
        at org.apache.pig.newplan.logical.expression.ProjectExpression.findReferent(ProjectExpression.java:207)
        at org.apache.pig.newplan.logical.expression.ProjectExpression.getFieldSchema(ProjectExpression.java:121)
        at org.apache.pig.newplan.logical.optimizer.FieldSchemaResetter.execute(SchemaResetter.java:193)
        at org.apache.pig.newplan.logical.expression.AllSameExpressionVisitor.visit(AllSameExpressionVisitor.java:53)
        at org.apache.pig.newplan.logical.expression.ProjectExpression.accept(ProjectExpression.java:75)
        at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:83)
        at org.apache.pig.newplan.logical.relational.LOJoin.accept(LOJoin.java:149)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:262)

This is not a problem on 0.9, trunk, since LogicalExpPlanMigrationVistor is dropped in 0.9.",,,,,,,,,,,,,,,,,17/May/11 21:26;daijy;PIG-2077-1.patch;https://issues.apache.org/jira/secure/attachment/12479521/PIG-2077-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-19 22:25:47.82,,,no_permission,,,,,,,,,,,,,67613,Reviewed,,,Mon Jul 18 23:50:37 UTC 2011,,,,,,,0|i0h0vz:,97406,,,,,,,,,,"17/May/11 21:26;daijy;PIG-2077-1.patch is only for Pig 0.8. However, test case should commit to 0.8, 0.9 and trunk.","19/May/11 22:25;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/767/
-----------------------------------------------------------

Review request for pig and thejas.


Summary
-------

See PIG-2077


This addresses bug PIG-2077.
    https://issues.apache.org/jira/browse/PIG-2077


Diffs
-----

  branches/branch-0.8/src/org/apache/pig/newplan/logical/LogicalExpPlanMigrationVistor.java 1104455 
  branches/branch-0.8/test/org/apache/pig/test/TestEvalPipeline2.java 1104455 

Diff: https://reviews.apache.org/r/767/diff


Testing
-------

Test patch:
     [exec] +1 overall.  
     [exec] 
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec] 
     [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
     [exec] 
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec] 
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec] 
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec] 
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

Unit test:
    all pass

End to end test:
    all pass


Thanks,

Daniel

",15/Jul/11 21:56;alangates;Marking SubmitPatch so we can get this reviewed and committed.,"18/Jul/11 23:28;jiraposter@reviews.apache.org;
-----------------------------------------------------------
This is an automatically generated e-mail. To reply, visit:
https://reviews.apache.org/r/767/#review1105
-----------------------------------------------------------

Ship it!


+1

- thejas


On 2011-05-19 22:26:01, Daniel Dai wrote:
bq.  
bq.  -----------------------------------------------------------
bq.  This is an automatically generated e-mail. To reply, visit:
bq.  https://reviews.apache.org/r/767/
bq.  -----------------------------------------------------------
bq.  
bq.  (Updated 2011-05-19 22:26:01)
bq.  
bq.  
bq.  Review request for pig and thejas.
bq.  
bq.  
bq.  Summary
bq.  -------
bq.  
bq.  See PIG-2077
bq.  
bq.  
bq.  This addresses bug PIG-2077.
bq.      https://issues.apache.org/jira/browse/PIG-2077
bq.  
bq.  
bq.  Diffs
bq.  -----
bq.  
bq.    branches/branch-0.8/src/org/apache/pig/newplan/logical/LogicalExpPlanMigrationVistor.java 1104455 
bq.    branches/branch-0.8/test/org/apache/pig/test/TestEvalPipeline2.java 1104455 
bq.  
bq.  Diff: https://reviews.apache.org/r/767/diff
bq.  
bq.  
bq.  Testing
bq.  -------
bq.  
bq.  Test patch:
bq.       [exec] +1 overall.  
bq.       [exec] 
bq.       [exec]     +1 @author.  The patch does not contain any @author tags.
bq.       [exec] 
bq.       [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.
bq.       [exec] 
bq.       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
bq.       [exec] 
bq.       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
bq.       [exec] 
bq.       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
bq.       [exec] 
bq.       [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.
bq.  
bq.  Unit test:
bq.      all pass
bq.  
bq.  End to end test:
bq.      all pass
bq.  
bq.  
bq.  Thanks,
bq.  
bq.  Daniel
bq.  
bq.

",18/Jul/11 23:50;daijy;Patch committed to 0.8 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit/Sample with variable does not work if the expression starts with an integer/double,PIG-2156,12513798,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,azaroth,azaroth,azaroth,12/Jul/11 14:52,14/Jul/11 16:13,13/Mar/19 23:13,14/Jul/11 16:13,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Pig generates an error with this script:

{code}
grunt> a = load 'a.txt';                      
grunt> b = group a all;                       
grunt> c = foreach b generate COUNT(a) as sum;
grunt> d = limit a 1 * c.sum;  
[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <line 4, column 14>  mismatched input '*' expecting SEMI_COLON
{code}

The reason is the parser chooses the wrong alternative.",,,,,,,,,,,,,,,,,13/Jul/11 10:24;azaroth;PIG-2156.2.patch;https://issues.apache.org/jira/secure/attachment/12486294/PIG-2156.2.patch,12/Jul/11 15:38;azaroth;PIG-2156.patch;https://issues.apache.org/jira/secure/attachment/12486193/PIG-2156.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-12 23:12:44.018,,,no_permission,,,,,,,,,,,,,68026,Reviewed,,,Thu Jul 14 16:13:05 UTC 2011,,,,,,,0|i0h1bb:,97475,,,,,,,,,,12/Jul/11 15:38;azaroth;Attached patch and test.,"12/Jul/11 23:12;thejas;Can we add a similar fix for scalar expressions in sample as well ? -

{code}
grunt> s = sample l 0.1 * res.$0;
2011-07-12 16:10:24,131 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1200: <line 6, column 17>  mismatched input '*' expecting SEMI_COLON
Details at logfile: /Users/thejas/test_limit_expr/trunk/pig_1310512142490.log

{code}","13/Jul/11 10:24;azaroth;Sure, here it is (PIG-2156.2.patch).
Modified a test to check for this issue as well.
","14/Jul/11 16:13;thejas;+1 . Patch committed to trunk.
Thanks Gianmarco!
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Link for old releases on site is stale,PIG-2064,12507024,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,12/May/11 18:35,12/May/11 18:59,13/Mar/19 23:13,12/May/11 18:59,site,,,,,,,,,,,,,,,,site,,site,,,0,,,,,,,,Hadoop cleaned up all of the old release artifacts for former subprojects.  Our site still points to Hadoop for 0.7 and previous releases (since we were a subproject then).  We need to update the link to point to the archives where the releases still are accessible.,,,,,,,,,,,,,,,,,12/May/11 18:39;alangates;PIG-2064.patch;https://issues.apache.org/jira/secure/attachment/12479003/PIG-2064.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,165277,,,,Thu May 12 18:59:35 UTC 2011,,,,,,,0|i0h0tb:,97394,,,,,,,,,,12/May/11 18:59;alangates;Patch checked in.  I also did 'svn up' on people.apache.org to update the website.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Class cast exception while projecting udf result,PIG-1895,12501130,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,vivekp,vivekp,11/Mar/11 11:10,03/May/11 17:16,13/Mar/19 23:13,03/May/11 17:16,0.7.0,0.8.0,0.9.0,,,,,,,,,,,,,,0.8.1,,impl,,,0,,,,,,,,"Class cast exception is thrown when I try to project the result from my udf. The udf has a defined schema DataType.BAG,DataType.LONG and DataType.INTEGER

The below is my script
{code}
Data = load 'file:/home/pvivek/Desktop/input' using PigStorage() as ( i: int );
AllData = group Data all parallel 1;
SampledData = foreach AllData generate org.vivek.TestEvalFunc(Data, 5) as rs;
SampledData1 = foreach SampledData generate rs.sampled;
{code}

Even though the output schema defines ""sampled"" as a data bag, while processing, instead of sending only the data bag generated from the UDF , the entire tuple was sent to the projection as result.
{code}
Exception recieved :
java.lang.ClassCastException: org.apache.pig.data.BinSedesTuple cannot be cast to org.apache.pig.data.DataBag
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:484)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:197)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:480)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:197)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:339)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:434)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:402)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:382)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:1)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)
{code}
This issue is happening with 0.9/0.8 and 0.7

",,,,,,,,,,,,,PIG-1866,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-15 01:30:47.013,,,no_permission,,,,,,,,,,,,,165232,,,,Tue May 03 17:16:21 UTC 2011,,,,,,,0|i0gzsn:,97229,,,,,,,,,,"11/Mar/11 11:20;vivekp;UDF Source code :
{code}
import java.io.IOException;
import java.util.ArrayList;
import java.util.Iterator;

import org.apache.pig.EvalFunc;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.DataType;
import org.apache.pig.data.DefaultBagFactory;
import org.apache.pig.data.DefaultTupleFactory;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;

public class TestEvalFunc extends EvalFunc<Tuple>{
	
	public Tuple exec(Tuple input) throws IOException {
		ArrayList<Tuple> tupleList = new ArrayList<Tuple>(2);
		DataBag values = (DataBag)(input.get(0));
		for (Iterator<Tuple> vit = values.iterator(); vit.hasNext();) {
			tupleList.add(vit.next());
		}
		DataBag sampleBag = DefaultBagFactory.getInstance().newDefaultBag(tupleList);
		Tuple output = DefaultTupleFactory.getInstance().newTuple(3);
		output.set(0, sampleBag);
		output.set(1, new Long(3));
		output.set(2, new Integer(2));
		return output;
	}
	public Schema outputSchema(Schema input) {
		Schema udfSchema = new Schema();
		udfSchema.add(new Schema.FieldSchema(""sampled"",DataType.BAG));
		udfSchema.add(new Schema.FieldSchema(""k"",DataType.LONG));
		udfSchema.add(new Schema.FieldSchema(""i"",DataType.INTEGER));
		return udfSchema;
	}
}
{code}

Test Case to Verify;
{code}
import static org.apache.pig.ExecType.LOCAL;

import java.util.ArrayList;
import java.util.Iterator;

import junit.framework.TestCase;

import org.apache.pig.PigServer;
import org.apache.pig.data.Tuple;

public class MyPigUnitTests extends TestCase {
  private static String patternString = ""(\\d+)!+(\\w+)~+(\\w+)"";
  public static ArrayList<String[]> data = new ArrayList<String[]>();
  static {
    data.add(new String[] { ""1""});
    data.add(new String[] { ""3""});
  }

  private static String [] script = new String []{
     ""Data = load 'file:/home/pvivek/Desktop/input' using PigStorage() as ( i: int );"",
     ""AllData = group Data all parallel 1;"",
     ""SampledData = foreach AllData generate org.vivek.TestEvalFunc(Data, 5) as rs;"",
     ""SampledData1 = foreach SampledData generate rs.sampled;"",
  };

  public void test () throws Exception
  {
     String filename = TestHelper.createTempFile(data, """");
     PigServer pig = new PigServer(LOCAL);
     filename = filename.replace(""\\"", ""\\\\"");
     patternString = patternString.replace(""\\"", ""\\\\"");
     
     for (String query : script) {
        pig.registerQuery(query);
    }
     Iterator<?> it = pig.openIterator(""SampledData1"");
     int tupleCount = 0;
     while (it.hasNext()) {
        Tuple tuple = (Tuple) it.next();
        if (tuple == null)
          break;
        else {
          if (tuple.size() > 0) {
              tupleCount++;
          }
        }
      }
     assertEquals(1, tupleCount);
  }
}
{code}","15/Mar/11 01:30;daijy;TestEvalFunc forget to set twolevelaccess flag for the bag. Pig 0.9 does not require twolevelaccess flag. However, we saw Pig 0.9 fail with a different stack:

ERROR 1000: Invalid field reference. Referenced field [guid] does not exist in schema: null.

org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1067: Unable to explain alias C3
        at org.apache.pig.PigServer.explain(PigServer.java:993)
        at org.apache.pig.tools.grunt.GruntParser.explainCurrentBatch(GruntParser.java:368)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:300)
        at org.apache.pig.tools.grunt.GruntParser.processExplain(GruntParser.java:263)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.Explain(PigScriptParser.java:665)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:325)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:176)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:152)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
        at org.apache.pig.Main.run(Main.java:537)
        at org.apache.pig.Main.main(Main.java:108)
Caused by: org.apache.pig.impl.plan.PlanValidationException: ERROR 1000: Invalid field reference. Referenced field [guid] does not exist in schema: null.
        at org.apache.pig.newplan.logical.visitor.ColumnAliasConversionVisitor$1.visit(ColumnAliasConversionVisitor.java:114)
        at org.apache.pig.newplan.logical.expression.DereferenceExpression.accept(DereferenceExpression.java:83)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.newplan.logical.optimizer.AllExpressionVisitor.visit(AllExpressionVisitor.java:114)
        at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:240)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.logical.optimizer.AllExpressionVisitor.visit(AllExpressionVisitor.java:104)
        at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:73)
        at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
        at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
        at org.apache.pig.PigServer$Graph.compile(PigServer.java:1538)
        at org.apache.pig.PigServer$Graph.compile(PigServer.java:1533)
        at org.apache.pig.PigServer$Graph.access$200(PigServer.java:1295)
        at org.apache.pig.PigServer.buildStorePlan(PigServer.java:1195)
        at org.apache.pig.PigServer.explain(PigServer.java:956)
","15/Mar/11 01:41;daijy;Sorry, the last comment is not mean for this Jira, ignore it.",21/Mar/11 18:06;daijy;This issue is the same nature as PIG-1866. Sounds weird but seems Pig have trouble projecting a bag out of a tuple.,03/May/11 17:16;daijy;Fixed by PIG-1866.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GROUP BY multiple column not working with new optimizer,PIG-1523,12470374,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,swati.j,swati.j,28/Jul/10 20:22,04/Mar/11 22:05,13/Mar/19 23:13,04/Mar/11 22:05,0.7.0,,,,,,,,,,,,,,,,,,impl,,,0,,,,,,,,"The following script does a GroupBy multiple columns:
{noformat}
A = load '<any file>' USING PigStorage(',') as (a1:int,a2:int,a3:int);
G1 = GROUP A by (a1,a2);
D = Filter G1 by group.$0 > 1;
explain D;
{noformat}

The above fails with the following error when the new optimizer is enabled (it fails with the old framework too but only when it gets to the execution stage):
{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.pig.experimental.logical.LogicalPlanMigrationVistor$LogicalExpPlanMigrationVistor.visit(LogicalPlanMigrationVistor.java:424)
        at org.apache.pig.impl.logicalLayer.LOProject.visit(LOProject.java:404)
        at org.apache.pig.impl.logicalLayer.LOProject.visit(LOProject.java:58)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:69)
        at org.apache.pig.experimental.logical.LogicalPlanMigrationVistor.translateExpressionPlan(LogicalPlanMigrationVistor.java:155)
        at org.apache.pig.experimental.logical.LogicalPlanMigrationVistor.visit(LogicalPlanMigrationVistor.java:295)
        at org.apache.pig.impl.logicalLayer.LOFilter.visit(LOFilter.java:116)
        at org.apache.pig.impl.logicalLayer.LOFilter.visit(LOFilter.java:41)
        at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:69)
        at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
        at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:237)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-04 22:05:31.526,,,no_permission,,,,,,,,,,,,,165006,,,,Fri Mar 04 22:05:31 UTC 2011,,,,,,,0|i0gwrz:,96740,,,,,,,,,,04/Mar/11 22:05;olgan;Works with Pig 0.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support of Map operations,PIG-233,12395475,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,amirhyoussefi,amirhyoussefi,07/May/08 03:03,03/Mar/11 19:33,13/Mar/19 23:13,03/Mar/11 19:33,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Currently we have '#'  for GET operation. We need to have more Map operations e.g. PUT. 

To address the immediate need I wrote a simple UDF for mapPut(). 

Also we need to discuss what operation/keyword to put into language.

On a side note:

Currently UDF cannot return DataMap (class is not recognized and value is not carried from UDF to caller script).

So I put it in a DataBag to make it work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-03 19:33:50.704,,,no_permission,,,,,,,,,,,,,163880,,,,Thu Mar 03 19:33:50 UTC 2011,,,,,,,0|i0ghqf:,94303,,,,,,,,,,"03/Mar/11 19:33;olgan;I believe both issues in this bugs has been addressed:

(1) UDFs can certainly return maps with all the recent releases
(2) We now have TOMAP function to construct maps.

Also we have not really seen requirements for any other operations. Lets open new ticket when we have use cases for more operations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Piggybank: ISOToDay disregards timezone (should use ISODateTimeFormat instead of DateTime to parse),PIG-1781,12494005,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,misterbeebee,misterbeebee,misterbeebee,23/Dec/10 20:14,20/Jan/11 18:59,13/Mar/19 23:13,20/Jan/11 18:59,0.8.0,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"(Apologies if this is the wrong place to file Piggybank bugs)

Bug in http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/datetime/truncate/ISOToDay.java?view=markup

and other http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/evaluation/datetime/truncate/ classes that copy-paste the same code.

These classes parse dates like so:
 	DateTimeZone.setDefault(DateTimeZone.UTC); 	
 	DateTime dt = new DateTime((String)input.get(0).toString()); 

This has two problems:
(1) It messes up JVM static state by changing the DateTimeZone default time zone.
(2) It ignore timezone information in the input string, so times like ""2009-12-09T23:59:59-0800"" get truncated to ""2009-12-10T00:00:00Z"", which is the wrong day of year. 

Instead, they should use something like this, which respects the input timezone and does not modify any global state:

  DateTime dt ISODateTimeFormat.dateTime().withOffsetParsed().parseDateTime(isoDateString);

I have not provided a patch, because I'm not really set up to hack on Piggybank locally.

As a workaround, I am copy-pasting the classes into my own packages, and making the desired change.
",,,,,,,,,,,,,,,,,25/Dec/10 04:34;misterbeebee;ASF.LICENSE.NOT.GRANTED--PIG-1781.3.patch;https://issues.apache.org/jira/secure/attachment/12466956/ASF.LICENSE.NOT.GRANTED--PIG-1781.3.patch,25/Dec/10 21:30;misterbeebee;ASF.LICENSE.NOT.GRANTED--PIG-1781.4.patch;https://issues.apache.org/jira/secure/attachment/12466963/ASF.LICENSE.NOT.GRANTED--PIG-1781.4.patch,24/Dec/10 06:20;misterbeebee;PIG-1781.2.patch;https://issues.apache.org/jira/secure/attachment/12466927/PIG-1781.2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-12-23 23:52:45.533,,,no_permission,,,,,,,,,,,,,165191,,,,Thu Jan 20 18:59:21 UTC 2011,,,,,,,0|i0gz4v:,97122,"in UDFs in org.apache.pig.piggybank.test.evaluation.datetime.truncate, add support for non-UTC timezones in ISO 8601 datetime strings,


",,,,,,,,,"23/Dec/10 21:11;misterbeebee;This patch file was generated by ""cd contrib/piggybank/java && svn diff"". I do not know if that is the desired format.

This is a patch against trunk.

Notes on the code:
* Tests included for logic change, new feature (non-UTC support), and new class's methods.
* In addition to fixing the bug, I refactored a bit of common code into a new class ISOHelper.
* Whitespace is a bit funky, because I made the changes in XCode, not Eclipse.
* It might work well against 0.8.0 as well. At a glance, I didn't see code changes in the affected classes.
* Example code (in the comments) was *not* updated to include examples of non-UTC usage.
","23/Dec/10 21:13;misterbeebee;This is the patch I intended to submit along with my previous submissionn(the ""Submit Patch"" Workflow Action).
","23/Dec/10 22:16;misterbeebee;(Cancelling patch, because I read http://wiki.apache.org/pig/HowToContribute and saw that ""Submit Patch"" is for a more formal submission. I apologize.)","23/Dec/10 22:18;misterbeebee;Re-uploading patch with "".patch"" extension.",23/Dec/10 22:20;misterbeebee;Re-Submitting Patch after re-uploading patch with proper filename.,"23/Dec/10 23:52;alangates;One quick comment:  our coding conventions are to use spaces and not tabs.  See http://wiki.apache.org/pig/HowToContribute where it talks about our coding conventions (search on Sun to find it).  

FYI most of us are off for Christmas after today, so it may be after New Years before someone gets around to reviewing this.

Russell, did you want to take a look at this since you did a lot of the original ISO work?","24/Dec/10 06:20;misterbeebee;Updated patch, created at trunk/contrib/piggybank.
'ant test' ran to BUILD SUCCESSFUL

New Changes in this patch:
* Re-indented all touched files to use 4-spaces instead of tabs.
* Removed some stray System.out.println lines in some tests
* Changed some assertTrue(foo.equals(bar)) to assertEquals(foo, bar) (for clearer output when tests fail.)

Changes between repo and this patch, that I forgot to mention earlier:
* Changed some assertTrue(foo.equals(bar)) to assertEquals(foo, bar) (for clearer output when tests fail.)

Concerns:
* piggybank does not have a 'doc' or 'test-patch' target, so I could not run those. Particularly, I couldn't run the ""hudson auto-review"" locally.
* I'm not 100% on the interaction between core pig and piggybank in the build files, so I hope I did not break or overlook anything in that interaction.

I got Eclipse set up to develop on Pig, which is much nicer than Xcode. Yay.

OK, I hope this is helpful! It would be swell if this change got merged the 0.8 branch also (if an 0.8.1 release is planned), but I'm in no personal rush, since I have a local build that works for my project.

Oh, and if this behavior change is undesired for some reason, no hard feelings. I don't know if anyone else is using Piggybank for bucketing-by-day analysis in non-UTC timezones, but I am, and so the truncate package is quite handy for me; I just need a version that works on US/Pacific-timezone dateTimes.


","24/Dec/10 22:41;dvryaboy;Michael, 
Thanks a lot for contributing, I think this is a useful feature. 

We are not planning a 0.8.1 at the moment, but it's possible we will separate piggybank from Pig releases, so this would become available before 0.9.

Overall the patch looks good, but I think that you still want to set the default time zone to be UTC, not whatever the system default is. So you will want to put the DateTimeZone.setDefault(DateTimeZone.UTC) call into the new helper class.

Also, fyi, we generally generate patches, even for piggybank, from the top-level directory, the one that has contrib/ in it.

No need to apologize for not getting the patch submission process quite right, we are not above helping with that part, and would much rather someone submit a useful patch that's not quite formatted right, than not get the contribution :).","25/Dec/10 04:34;misterbeebee;New patch PIG-1781.3.patch, supersedes PIG-1781.2.patch

Changes between PIG-1781.2.patch and  PIG-1781.3.patch:
* Changed ISOHelper's dateTime() to dateTimeParser(), which more closely matches the pre-patch behavior of accepting a date or time or dateTime.
* Renamed parseDate() to parseDateTime(), and renamed corresponding tests.
* Added a public constant ISOHelper#DEFAULT_DATE_TIME_ZONE to advertise the fact that ISOHelper has its own default time zone for parsing.
* Added code to ISOHelper#parseDateTime to use UTC as default time zone for parsing ambiguous dates.
* Added code to save/restore the System's default time zone. **This is a change in behavior from the pre-patch code.***

* Added unit tests:
** to illustrate various corner cases of date/time/dateTime and UTC/other-time-zone/no-time-zone parsing
** to illustrate how default time zones are managed.

* ran 'svn diff' at base of pig tree.

""ant clean test"" in contrib directory succeeded.


I hope this version addresses everyone's concerns, but let me know if more changes are needed.

Regarding the general issue of time zones. I see the options for behavior like so:
(A) Use the System default time zone;
(B) Use ISOHelper's preferred time zone (UTC) to parse ambiguous times, but don't touch System's default time zone;
(C) Use ISOHelper's preferred time zone (UTC) to parse ambiguous times, and mutate the System time zone to match.
(D) Let the Pig User set a time zone for parsing ambiguous dates, independently of the System default time zone, with a parser default to either UTC or System's default.

The pre-patch 0.8.0 code behaves as option (C). This patch changes the behavior to be option (B) (with a tiny, unavoidable, race condition). (I think option (D) would be best, but that is more work that no one has requested yet.) 

Let me know if you disagree with my choice.

(I think the best practice for Pig Users to assign timezones as early as possible in the data processing pipeline, and keep dates tagged with time zones throughout the pipeline.)","25/Dec/10 21:30;misterbeebee;This patch supersedes PIG-1781.3.patch

I had an unsaved change in Eclipse, so the previous patch was missing some code.

","19/Jan/11 22:00;alangates;I ran the contrib tests (including the ones added in this patch) and they all pass.

Dmitriy, Russell, any comments or concerns before I check this in?",20/Jan/11 00:17;dvryaboy;+1,20/Jan/11 18:59;alangates;Patch checked in.  Thank you Michael.,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add link to apache.org to Pig's navbar on the website,PIG-1689,12478079,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,22/Oct/10 16:04,27/Oct/10 23:36,13/Mar/19 23:13,27/Oct/10 23:36,site,,,,,,,,,,,,,,,,site,,site,,31/Dec/10 00:00,0,,,,,,,,In compliance with http://www.apache.org/foundation/marks/pmcs we need to add a link to http://apache.org to the navigation tabs on the left of our website.,,,,,,,,,,,,,,,,,26/Oct/10 21:48;alangates;PIG-1689-2.patch;https://issues.apache.org/jira/secure/attachment/12458105/PIG-1689-2.patch,25/Oct/10 20:52;alangates;PIG-1689.patch;https://issues.apache.org/jira/secure/attachment/12458001/PIG-1689.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-10-25 22:28:26.902,,,no_permission,,,,,,,,,,,,,165134,,,,Wed Oct 27 23:36:08 UTC 2010,,,,,,,0|i0gyj3:,97024,,,,,,,,,,25/Oct/10 22:28;olgan;+1,26/Oct/10 21:48;alangates;I realized that rather than adding a new link to the navigation bar I should just update the link at the top that shows Pig as part of Hadoop.  Now it shows Apache > Pig.,"26/Oct/10 22:03;olgan;+1, maybe this one will stick :)",27/Oct/10 23:36;alangates;Patch 2 checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make sure Pig is referred to as Apache Pig on the website,PIG-1688,12478078,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,alangates,alangates,22/Oct/10 16:01,25/Oct/10 20:47,13/Mar/19 23:13,25/Oct/10 20:47,site,,,,,,,,,,,,,,,,site,,site,,31/Dec/10 00:00,0,,,,,,,,"In compliance with http://www.apache.org/foundation/marks/pmcs we need to make sure that Pig is referred to as Apache Pig in ""The first and most prominent reference to a project or product on each page""",,,,,,,,,,,,,,,,,22/Oct/10 16:55;alangates;PIG-1688.patch;https://issues.apache.org/jira/secure/attachment/12457845/PIG-1688.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-22 17:24:43.07,,,no_permission,,,,,,,,,,,,,165133,,,,Mon Oct 25 20:47:40 UTC 2010,,,,,,,0|i0gyiv:,97023,,,,,,,,,,"22/Oct/10 17:24;olgan;+1 changes look good. There seems to be files in the patch no changes that I can see like deployment.xml for instance. I don't think it would have the impact, just curious why the made it into the patch","22/Oct/10 17:46;alangates;In a number of files just the title of the page changed, but not the text in the page.","22/Oct/10 17:52;olgan;yeah, I think word was just eating some of the tags. Please, commit",25/Oct/10 20:47;alangates;Patch checked in.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"pig grunt shell breaks for many commands like perl , awk , pipe , 'ls -l' etc",PIG-1650,12475255,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,nrai,nrai,27/Sep/10 23:34,30/Sep/10 16:44,13/Mar/19 23:13,30/Sep/10 16:44,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,grunt shell breaks for many unix xommands,,,,,,,,,,,,,,,,,27/Sep/10 23:38;nrai;PIG-1650_0.patch;https://issues.apache.org/jira/secure/attachment/12455776/PIG-1650_0.patch,29/Sep/10 17:26;nrai;PIG-1650_1.patch;https://issues.apache.org/jira/secure/attachment/12455930/PIG-1650_1.patch,30/Sep/10 01:07;nrai;PIG-1650_2.patch;https://issues.apache.org/jira/secure/attachment/12455960/PIG-1650_2.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-09-30 16:44:04.574,,,no_permission,,,,,,,,,,,,,165104,Reviewed,,,Thu Sep 30 16:44:04 UTC 2010,,,,,,,0|i0gy6f:,96967,,,,,,,,,,27/Sep/10 23:38;nrai;This patch will fix many broken commands inside the grunt shell.,"30/Sep/10 16:44;thejas;Niraj confirmed that unit tests and test-patch has succeded.
Patch looks good. +1 .
Committed to trunk and 0.8 branch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig gives generic message for few cases,PIG-1599,12473266,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,nrai,nrai,nrai,03/Sep/10 17:58,03/Sep/10 22:56,13/Mar/19 23:13,03/Sep/10 22:56,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"When we run the script:
register testudf.jar;
a = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age, gpa);
b = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age, gpa);
c = cogroup a by name, b by name;
d = foreach c generate flatten(org.apache.pig.test.udf.evalfunc.BadUdf(a,b));
dump d;

we get the error:
now we get ""ERROR 2088: Unable to get results for: 
hdfs://wilbur20.labs.corp.sp1.yahoo.com:9020/tmp/temp1787360727/tmp509618997:org.apache.pig.impl.io.InterStorage"".

The udf is bad udf and it should throw:
ERROR 2078: Caught error from UDF: org.apache.pig.test.udf.evalfunc.BadUdf, Out of bounds access [Index: 2, Size: 2]",,,,,,,,,,,,,,,,,03/Sep/10 18:00;nrai;pig-1599_0.patch;https://issues.apache.org/jira/secure/attachment/12453797/pig-1599_0.patch,03/Sep/10 22:21;nrai;pig-1599_1.patch;https://issues.apache.org/jira/secure/attachment/12453834/pig-1599_1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-09-03 22:44:49.06,,,no_permission,,,,,,,,,,,,,165065,Reviewed,,,Fri Sep 03 22:56:10 UTC 2010,,,Patch Available,,,,0|i0gxmv:,96879,,,,,,,,,,03/Sep/10 22:21;nrai;merged with thejas change for 1550,03/Sep/10 22:44;rding;I manually run related tests and they all passed. I'm going to check in the patch to the trunk and 0.8 branch.,03/Sep/10 22:56;rding;Patch is committed to both trunk and 0.8 branch. Thanks Niraj.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PigDump does not properly output Chinese UTF8 characters - they are displayed as question marks ??,PIG-771,12423288,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,ciemo,ciemo,20/Apr/09 16:28,27/Aug/10 23:32,13/Mar/19 23:13,27/Aug/10 23:32,,,,,,,,,,,,,,,,,0.7.0,,,,,0,,,,,,,,"PigDump does not properly output Chinese UTF8 characters.

The reason for this is that the function Tuple.toString() is called.

DefaultTuple implements Tuple.toString() and it calls Object.toString() on the opaque object d.

Instead, I think that the code should be changed instead to call the new DataType.toString() function.

{code}
    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append('(');
        for (Iterator<Object> it = mFields.iterator(); it.hasNext();) {
            Object d = it.next();
            if(d != null) {
                if(d instanceof Map) {
                    sb.append(DataType.mapToString((Map<Object, Object>)d));
                } else {
                    sb.append(DataType.toString(d));  // <<< Change this one line
                    if(d instanceof Long) {
                        sb.append(""L"");
                    } else if(d instanceof Float) {
                        sb.append(""F"");
                    }
                }
            } else {
                sb.append("""");
            }
            if (it.hasNext())
                sb.append("","");
        }
        sb.append(')');
        return sb.toString();
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-27 07:18:58.69,,,no_permission,,,,,,,,,,,,,164333,,,,Fri Aug 27 23:32:33 UTC 2010,,,,,,,0|i0go5r:,95344,,,,,,,,,,"20/Apr/09 16:46;ciemo;I was going to submit a patch for this one line change, but I discovered in compiling the code that DataType.toString(d) throws an ExecException.

Oddly, DataType.mapToString DOES NOT throw any Exceptions which is inconsistent with the other DataType.to... functions.

I am not sure how to best implement the try / catch / throw for this particular case.

Also, in doing the code review of DataType.mapToString(...) I discovered that it will also have problems with correctly dumping the data contained within it because it too uses Object.toString() on opaque data handles.

So, the code for DataType.mapToString(...) should also use DataType.toString(Object);

But now I witness a recursion problem.  DataType.toString(Object) does not work for complex types.  So maps of maps will not be recursed properly.

So DataType.toString(Object) should probably be enhanced to work on Maps as well.

But now we have another problem ... PigDump wants to append L and F for Long values and Float values.  But this won't work for nested structures.","20/Apr/09 17:00;ciemo;Actually, now that I think about it, there is a bit of inconsistency in the complex data type toString functions:

DefaultTuple.toString()
SingleTupleBag.toString()
DefaultAbstractBag.toString() // uses StringBuffer rather than StringBuilder.  Should it?

Would it not be better to be consistent and use one path for all toString() conversions?","27/Apr/09 07:18;daijy;Hi, David,
In my mind PigDump should deal with UTF8 characters correctly. Can you describe the situation in which PigDump fail? What is your OS encoding?","27/Apr/09 15:00;ciemo;Take a file of UTF-8 Chinese characters (ch.txt).  Load it and dump it.

{code}A = load 'ch.txt' using PigStorage() as (str: chararray);

dump A;

store A into 'ch.out' using PigStorage();{code}

","27/Apr/09 15:15;daijy;Seems it works fine on my computer either dump it on the screen or store it into a file. My OS encoding is UTF8. Here is a log:

grunt> daniel@ubuntu-daniel1:~/pig$ cat chinese.txt
中文测试
daniel@ubuntu-daniel1:~/pig$ java -jar pig.jar
2009-04-27 11:14:10,889 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///
2009-04-27 11:14:11,046 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Initializing JVM Metrics with processName=JobTracker, sessionId=
grunt> A = load '/home/daniel/pig/chinese.txt' using PigStorage() as (str: chararray);
grunt> dump A;
2009-04-27 11:14:18,767 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2009-04-27 11:14:18,774 [Thread-4] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
2009-04-27 11:14:19,289 [Thread-10] INFO  org.apache.hadoop.mapred.MapTask - numReduceTasks: 0
2009-04-27 11:14:19,364 [Thread-10] INFO  org.apache.hadoop.mapred.LocalJobRunner -
2009-04-27 11:14:19,368 [Thread-10] INFO  org.apache.hadoop.mapred.TaskRunner - Task 'attempt_local_0001_m_000000_0' done.
2009-04-27 11:14:19,372 [Thread-10] INFO  org.apache.hadoop.mapred.TaskRunner - Saved output of task 'attempt_local_0001_m_000000_0' to file:/tmp/temp-480716160/tmp2138119751
2009-04-27 11:14:24,220 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2009-04-27 11:14:24,222 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete
2009-04-27 11:14:24,222 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!
(中文测试)
grunt>
","27/Apr/09 16:21;ciemo;Very strange.  I can display UTF8 chinese characters in my Mac OS Terminal window.  Only dump has a problem.

Here's the transcript of what I did.  If you look, you'll see:

{code}
-bash-3.00$ cat > ch.txt
中文测试

-bash-3.00$ file ch.txt
ch.txt: UTF-8 Unicode text

-bash-3.00$ cat ch.txt
中文测试

-bash-3.00$ cat ch.pig
A = load 'ch.txt' using PigStorage() as (str: chararray);
dump A;
store A into 'ch.out' using PigStorage();

-bash-3.00$ pig -exectype local ch.pig
USING: /grid/0/gs/pig/current
2009-04-27 16:15:16,314 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - 100% complete!
2009-04-27 16:15:16,315 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Success!!
(????)
2009-04-27 16:15:16,339 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - 100% complete!
2009-04-27 16:15:16,339 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Success!!

-bash-3.00$ cat ch.out
中文测试

-bash-3.00$ pig -exectype local 
USING: /grid/0/gs/pig/current
grunt> A = load 'ch.txt' using PigStorage() as (str: chararray);
grunt> dump A;
2009-04-27 16:16:51,786 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - 100% complete!
2009-04-27 16:16:51,786 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Success!!
(????)
grunt> 
{code}","27/Apr/09 16:32;daijy;I am using Ubuntu. Maybe this problem is on Mac only. I don't know much about Mac, is there a language setting? If it is, can you change it to UTF8? ","27/Apr/09 17:06;ciemo;I'm just using Mac OS terminal to connect to a RHEL-4 gateway server to a RHEL-4 grid.

I changed the code to use PigDump() storage format for the STORE statement and reran the code, trying to eliminate the terminal aspect.  Pig itself is writing the question marks ('?', 0x3f).

{code}
-bash-3.00$ cat ch2.pig
A = load 'ch.txt' using PigStorage() as (str: chararray);
store A into 'ch.dmp' using PigDump();

-bash-3.00$ hadoop fs -cat ch.dmp/*
(????)

-bash-3.00$ hadoop fs -cat ch.dmp/* | od -xc
0000000 3f28 3f3f 293f 000a
          (   ?   ?   ?   ?   )  \n  \0
0000007
{code}","27/Apr/09 18:54;daijy;David, can you check what is the language setting on that computer? Use the command ""locale|grep LANG"". Thanks","27/Apr/09 21:17;ciemo;Daniel,

Thanks.  locale reported LANG=POSIX

I used locale -a to list the locales and then did:

export LANG=en_US.utf8

Then I got the correct PigDump output.

I found that also setting LESSCHARSET to utf-8 was valuable as well.

For bash users:

export LANG=en_US.utf8
export LESSCHARSET=utf-8


It would be useful if dump/PigDump() had a warning which indicated to the user if LANG=POSIX, then asian language characters may not display properly.  Something like:

{code}if (function_which_returns_local_setting_which_I_dont_know_name_of().equals(""POSIX""))  {
    System.out.println(""WARNING: dump will not properly display multibyte UTF-8 characters when\n"" + 
    ""environment variable LANG=\""POSIX\"".  Try setting your environment variable LANG=en_US.utf8.\n""  +
    ""See locale -a for other possible values."");
}{code}","28/Apr/09 04:37;daijy;It is good to put a warning message. Instead of checking if LANG=POSIX, we can check whether LANG=UTF8. We can see correct character only when LANG=UTF8.",27/Aug/10 23:32;olgan;PigDump is no longer supported,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pigtest ant target fails pigtrunk builds,PIG-1328,12460239,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,gkesavan,gkesavan,25/Mar/10 14:59,12/Jul/10 20:51,13/Mar/19 23:13,12/Jul/10 20:51,,,,,,,,,,,,,,,,,,,build,,,0,,,,,,,,"java.lang.NoClassDefFoundError:com_cenqua_clover/CloverVersionInfo)
    [junit] Tests run: 0, Failures: 0, Errors: 2, Time elapsed: 0.154 sec
    [junit] Test org.apache.hadoop.zebra.pig.TestTableSortStorer FAILED
    [junit] Running org.apache.hadoop.zebra.pig.TestTableSortStorerDesc
    [junit] log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration).
    [junit] log4j:WARN Please initialize the log4j system properly.
    [junit] [CLOVER] FATAL ERROR: Clover could not be initialised. Are you sure you have Clover in the runtime classpath? (class java.lang.NoClassDefFoundError:com_cenqua_clover/CloverVersionInfo)
    [junit] Tests run: 0, Failures: 0, Errors: 2, Time elapsed: 0.164 sec
    [junit] Test org.apache.hadoop.zebra.pig.TestTableSortStorerDesc FAILED",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-12 20:51:16.974,,,no_permission,,,,,,,,,,,,,164826,,,,Mon Jul 12 20:51:16 UTC 2010,,,,,,,0|i0gumv:,96393,,,,,,,,,,"12/Jul/10 20:51;olgan;I believe all tests are running now. Please, re-open and clarify if this is still an issue",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig reference manual does not mention syntax for comments,PIG-1182,12445004,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,ciemo,ciemo,08/Jan/10 08:03,12/Jul/10 19:56,13/Mar/19 23:13,12/Jul/10 19:56,0.5.0,,,,,,,,,,,,,,,,,,documentation,,,0,,,,,,,,"The Pig 0.5.0 reference manual does not mention how to write comments in your pig code using -- (two dashes).
http://hadoop.apache.org/pig/docs/r0.5.0/piglatin_reference.html

Also, does /* */ also work?",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-01-08 16:50:20.137,,,no_permission,,,,,,,,,,,,,164699,,,,Mon Jul 12 19:56:19 UTC 2010,,,,,,,0|i0gsx3:,96115,,,,,,,,,,"08/Jan/10 16:50;chandec;Actually, the Pig docs do mention how to form comments.

The section was moved to the Pig Users guide when I split up the Pig Latin Reference manual.

Both -- and /* */ are shown.

http://hadoop.apache.org/pig/docs/r0.5.0/piglatin_users.html#Using+Comments+in+Scripts

So, I'm not sure where you made the correction.

Thanks/C

","08/Jan/10 18:46;ciemo;Corinne, I made no changes.

I'm pointing out that it is an omission to not have the comment syntax documented in the reference manual.

Reference manuals for programming languages SHOULD ALWAYS have information on ALL syntax including comment syntax.

Once you are done with learning things in the User's Guide, most of the time programmer's just go back to the Reference Manual for quick look up of information and syntax.

So the documentation on comment syntax should be in BOTH the User's Guide AND the Reference Manual.

","19/Feb/10 07:15;chandec;In pig 0.6.0, Pig User and Pig Latin docs renamed Pig Latin 1 and Pig Latin 2 (the 2 docs should have been named this way when I split them up - sorry for confusion).

Pig Latin 1 and Pig Latin 2 docs have statements saying docs should be used together.

Pig Latin 1 doc includes the information about comments.",19/Feb/10 07:16;chandec;Pig Latin 1 contains information about comments.,"19/Feb/10 14:01;ciemo;Corinne, not sure what you are so resistant to following the basic principles of documenting ALL syntax, including comments, in the reference manual. If the document is open to the community to edit, I'm more than willing to do the work myself since I have contibuted as a technical writer for programming language reference manuals in my past as well as having been a developer of compilers and software development tools.

Also, I think the passage you sited could use a little work on the English: 

Using Comments in Scripts
If you place Pig Latin statements in a script, the script can include comments.

For multi-line comments use /* .... */
For single line comments use --
/* myscript.pig
My script includes three simple Pig Latin Statements.
*/

A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float); -- load statement
B = FOREACH A GENERATE name;  -- foreach statement
DUMP B;  --dump statement
Case Sensitivity
","20/Feb/10 00:00;olgan;Ciemo,

There is a reason why Corinne created to sections of the document. A single document was just too large so it was hard to manage changes and even to load it takes some time.

If I understand correctly, the real issue that you are pointing out is that it is hard to find specific information that you are looking for quickly. Traditionally indices are used for this purpose and pig documentation does not have one. 

Short term, Corinne does not have time to work on it due to other commitment. If you or other users would like to help with that, that would certainly be appreciated. ","12/Jul/10 19:56;olgan;Closing. If we do want to do an comprehansive index, please, create a separate JIRA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Semantics of * and count,PIG-813,12426055,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,breed,gmavromatis,gmavromatis,21/May/09 20:33,10/Jul/10 00:40,13/Mar/19 23:13,10/Jul/10 00:40,0.2.0,,,,,,,,,,,,,,,,,,documentation,,,0,,,,,,,,"Continuation of PIG-812. See PIG-812 for more details.


In order for this to be resolved in the right manner the following must added in the http://hadoop.apache.org/pig/docs/r0.2.0/piglatin.html

1) The semantics of * as explained by Olga.
2) An example of GROUP ALL

Otherwise people will waste their time doing the same (documentation-caused) mistakes again.",,,,,,,,,,,,PIG-812,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-05-24 04:29:53.763,,,no_permission,,,,,,,,,,,,,164370,,,,Sat Jul 10 00:40:11 UTC 2010,,,,,,,0|i0gomn:,95420,,,,,,,,,,"24/May/09 04:29;chandec;Pig Latin Manual updated (PIG-817):
1) The semantics of * as explained by Olga.
2) An example of GROUP ALL included.

George, thanks for pointing out this documentation issue.
","06/Jun/09 11:40;hudson;Integrated in Pig-trunk #465 (See [http://hudson.zones.apache.org/hudson/job/Pig-trunk/465/])
    ",10/Jul/10 00:40;olgan;Looks like this has been committed but not closed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
COUNT fails on local mode but executes correctly on grid mode for the same data,PIG-769,12422982,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,gmavromatis,gmavromatis,16/Apr/09 05:52,10/Jul/10 00:26,13/Mar/19 23:13,10/Jul/10 00:26,0.2.0,,,,,,,,,,,,,,,,0.7.0,,,,,0,,,,,,,,"The following script run on the grid executes correctly. It  prints (4L) for '/user/gmavr/k_sample_preprocessed_withj_sample'

On local mode (invoked with -x local) and the same data in the local filesystem, it failes with:
-2009-04-11 03:23:15,155 [main] ERROR org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore - Received error from storer function: org.apache.pig.backend.executionengine.ExecException: ERROR 2106: Error while computing count in COUNT


%declare k_sample_preprocessed_withj '/user/gmavr/k_sample_preprocessed_withj_sample';
-- %declare k_sample_preprocessed_withj '/homes/gmavr/mlrSite/k_sample_preprocessed_withj_sample';

webdataFiltered = LOAD '$k_sample_preprocessed_withj' USING BinStorage() AS (url:chararray, pg:bytearray);

X1 = GROUP webdataFiltered ALL;
Y1 = FOREACH X1 GENERATE COUNT(*);
DUMP Y1;


",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-10 00:26:06.203,,,no_permission,,,,,,,,,,,,,164331,,,,Sat Jul 10 00:26:06 UTC 2010,,,,,,,0|i0go4n:,95339,,,,,,,,,,"10/Jul/10 00:26;olgan;Local mode now uses the same code path as MR mode. Please, re-open if this is till a problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in PhysicalPlan clone operation,PIG-735,12419423,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,phunt,phunt,25/Mar/09 20:36,10/Jul/10 00:16,13/Mar/19 23:13,10/Jul/10 00:16,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"clone() operation in src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhysicalPlan.java can throws NPE rather than clone not supported

Note: this code fails to compile under eclipse with 1.6 target and ""null pointer access"" in java compiler
preference set to ""error""
",,,,,,,,,,,,,,,,,25/Mar/09 20:38;phunt;PIG-735.patch;https://issues.apache.org/jira/secure/attachment/12403636/PIG-735.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-10 00:16:36.067,,,no_permission,,,,,,,,,,,,,164302,,,,Sat Jul 10 00:16:36 UTC 2010,,,,,,,0|i0gnq7:,95274,,,,,,,,,,"25/Mar/09 20:38;phunt;This patch fixes the NPE, but I'm not in a position to test it so I'll leave that to someone on the pig team.
",10/Jul/10 00:16;olgan;This has been addressed as part of findbugs cleanup,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Computation of number of reducers in order by has to change for a static cluster (Hadoop 20),PIG-702,12416328,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,sms,sms,06/Mar/09 19:38,09/Jul/10 22:55,13/Mar/19 23:13,09/Jul/10 22:55,0.2.0,,,,,,,,,,,,,,,,,,impl,,,0,,,,,,,,"In Hadoop 20, a static cluster is shared amongst multiple queues. The computation of the number of reducers in order by queries when parallel keyword needs to change. Currently, the number of reducers is computed as a function of the number of reduce slots and the number of nodes in the cluster. Conservatively, this should revert to one reducer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,164276,,,,2009-03-06 19:38:15.0,,,,,,,0|i0gncn:,95213,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig help usage is missing some options,PIG-624,12412595,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,tomwhite,tomwhite,16/Jan/09 14:12,09/Jul/10 19:43,13/Mar/19 23:13,09/Jul/10 19:43,,,,,,,,,,,,,,,,,0.7.0,,,,,0,,,,,,,,"Running ""pig -help"" doesn't show -param, -param_file, or -dryrun options.",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-09 19:43:07.41,,,no_permission,,,,,,,,,,,,,164214,,,,Fri Jul 09 19:43:07 UTC 2010,,,,,,,0|i0gmfr:,95065,,,,,,,,,,09/Jul/10 19:43;olgan;The options listed on this bug are already in pig 0.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDF that requires a cast for its input arguments causes an exception,PIG-525,12408389,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,olgan,olgan,olgan,13/Nov/08 00:58,09/Jul/10 19:16,13/Mar/19 23:13,09/Jul/10 19:16,,,,,,,,,,,,,,,,,0.6.0,,,,,0,,,,,,,,"Script:

register /homes/olgan/piggybank.jar                                           
A = load 'studenttab10k' as (name, age, gpa); 
B = filter A by name is not null;                                             
C = foreach A generate org.apache.pig.piggybank.evaluation.string.UPPER(name);
D = limit C 10;                                                               
dump D; 

Stack:

 (reduce) task_200809241441_20466_r_000000java.io.IOException: Received a bytearray from the UDF. Cannot determine how to convert the bytearray to string.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:266)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:224)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:136)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:318)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)
",,,,,,,,,,,,,,,,,13/Nov/08 01:34;olgan;PIG-525.patch;https://issues.apache.org/jira/secure/attachment/12393830/PIG-525.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-11-13 01:36:24.664,,,no_permission,,,,,,,,,,,,,164128,,,,Fri Jul 09 19:16:05 UTC 2010,,,,,,,0|i0gla7:,94878,,,,,,,,,,13/Nov/08 01:36;alangates;+1,09/Jul/10 19:16;olgan;I checked that the code was committed - we just forgot to close it,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The implementation of IndexedTuple might break in cases where a custom TupleFactory is being used,PIG-406,12403359,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,shubhamc,shubhamc,29/Aug/08 10:55,09/Jul/10 18:57,13/Mar/19 23:13,09/Jul/10 18:57,0.2.0,,,,,,,,,,,,,,,,,,data,,,0,,,,,,,,"IndexedTuple extends the DefaultTuple. This implementation might break in case we are trying to use a custom TupleFactory. Any special data in the custom tuples might get lost when we convert it to an IndexedTuple. The constructor does a super(t.getAll()), other non-datum attributes will be lost after this conversion. The indexedTuple must instead have Tuple as an attribute and implement the Tuple interface and delegate all the methods. The implementation should be by composition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-09 18:57:01.388,,,no_permission,,,,,,,,,,,,,164030,,,,Fri Jul 09 18:57:01 UTC 2010,,,,,,,0|i0gjuf:,94645,,,,,,,,,,09/Jul/10 18:57;olgan;We no longer use IndexedTuple,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Difficult to debug parameter substitution problems based on the error messages when running in local mode,PIG-755,12422183,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,viraj,viraj,07/Apr/09 00:12,08/Jul/10 20:44,13/Mar/19 23:13,08/Jul/10 20:44,0.3.0,,,,,,,,,,,,,,,,,,grunt,,,2,,,,,,,,"I have a script in which I do a parameter substitution for the input file. I have a use case where I find it difficult to debug based on the error messages in local mode.

{code}
A = load '$infile' using PigStorage() as
     (
       date            : chararray,
       count           : long,
       gmean           : double
    );

dump A;
{code}

1) I run it in local mode with the input file in the current working directory
{code}
prompt  $ java -cp pig.jar:/path/to/hadoop/conf/ org.apache.pig.Main -exectype local -param infile='inputfile.txt' localparamsub.pig
{code}
2009-04-07 00:03:51,967 [main] ERROR org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore - Received error from storer function: org.apache.pig.backend.executionengine.ExecException: ERROR 2081: Unable to setup the load function.
2009-04-07 00:03:51,970 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Failed jobs!!
2009-04-07 00:03:51,971 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - 1 out of 1 failed!
2009-04-07 00:03:51,974 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1066: Unable to open iterator for alias A
====================================================================
Details at logfile: /home/viraj/pig-svn/trunk/pig_1239062631414.log
====================================================================
ERROR 1066: Unable to open iterator for alias A
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias A
        at org.apache.pig.PigServer.openIterator(PigServer.java:439)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:359)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:193)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:99)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:88)
        at org.apache.pig.Main.main(Main.java:352)
Caused by: java.io.IOException: Job terminated with anomalous status FAILED
        at org.apache.pig.PigServer.openIterator(PigServer.java:433)
        ... 5 more
====================================================================

2) I run it in map reduce mode
{code}
prompt  $ java -cp pig.jar:/path/to/hadoop/conf/ org.apache.pig.Main -param infile='inputfile.txt' localparamsub.pig
{code}

2009-04-07 00:07:31,660 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://localhost:9000
2009-04-07 00:07:32,074 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to map-reduce job tracker at: localhost:9001
2009-04-07 00:07:34,543 [Thread-7] WARN  org.apache.hadoop.mapred.JobClient - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
2009-04-07 00:07:39,540 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2009-04-07 00:07:39,540 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Map reduce job failed
2009-04-07 00:07:39,563 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2100: inputfile does not exist.
====================================================================
Details at logfile: /home/viraj/pig-svn/trunk/pig_1239062851400.log
====================================================================
ERROR 2100: inputfile does not exist.
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias A
        at org.apache.pig.PigServer.openIterator(PigServer.java:439)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:359)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:193)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:99)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:88)
        at org.apache.pig.Main.main(Main.java:352)
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable to store alias A
        at org.apache.pig.PigServer.store(PigServer.java:470)
        at org.apache.pig.PigServer.openIterator(PigServer.java:427)
        ... 5 more
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1002: Unable to store alias A
        at org.apache.pig.PigServer.store(PigServer.java:503)
        at org.apache.pig.PigServer.store(PigServer.java:466)
        ... 6 more
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2997: Unable to recreate exception from backend error: org.apache.pig.backend.executionengine.ExecException: ERROR 2100: inputfile does not exist.
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher.getStats(Launcher.java:135)
====================================================================
Here is evident that the error occurred because ""input.txt"" was truncated to ""input""
",,,,,,,,,,,,,,,,,07/Apr/09 00:14;viraj;inputfile.txt;https://issues.apache.org/jira/secure/attachment/12404783/inputfile.txt,07/Apr/09 00:14;viraj;localparamsub.pig;https://issues.apache.org/jira/secure/attachment/12404784/localparamsub.pig,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-04-08 17:15:50.227,,,no_permission,,,,,,,,,,,,,164319,,,,Thu Jul 08 20:44:54 UTC 2010,,,,,,,0|i0gnyn:,95312,,,,,,,,,,07/Apr/09 00:14;viraj;Script and testfile,"08/Apr/09 17:15;ciemo;My recommended solutions:

1)  The pig command should provide a command line option for outputting the resulting script with all parameter substitutions.

This one simple mechanism would allow developers to see what is and is not happening with parameter substitution.

I should suggest something akin to gcc --save-temps where the preprocessed output of the parameter substitutions is saved to a file such as file.pig => file.ppig.

This would be really valuable for any kind of translator to Pig such as SQL to Pig as well.

2) Any load and store statements should indicate the name of file / directory that is being processed.

The previous version of Pig indicated the name of not only the files appearing in load and store statements but also the name of any temporary files created as well.

This functionality should work in local mode as well has HDFS mode.","25/Apr/09 03:01;viraj;Ciemo presently there is an option in Pig known as dryrun or  '-r', where it produces a pigscript.substituted in ASCII text in which it replaces all the parameters with actual values..

Viraj","25/Apr/09 14:12;ciemo;Thanks.  Didn't know about the dry-run option.

Hopefully it will someday produce UTF-8 text given some of the parameters will be in Chinese or Japanese characters. :^)","14/Oct/09 06:37;daijy;Now the error message changed to:
ERROR 2999: Unexpected internal error. Can not create a Path from an empty string

java.lang.IllegalArgumentException: Can not create a Path from an empty string
        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)
        at org.apache.hadoop.fs.Path.<init>(Path.java:90)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.massageFilename(QueryParser.java:191)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.LoadClause(QueryParser.java:1440)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:1227)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:893)
        at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:682)
        at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:63)
        at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1017)
        at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:967)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:383)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:716)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:324)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:168)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:144)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:89)
        at org.apache.pig.Main.main(Main.java:397)","08/Jul/10 20:44;olgan;With Pig 0.7.0, local mode and MR code use the same code path. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent error message when the message should be about org.apache.hadoop.fs.permission.AccessControlException: Permission denied:,PIG-736,12419442,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,viraj,viraj,26/Mar/09 00:16,08/Jul/10 20:27,13/Mar/19 23:13,08/Jul/10 20:27,0.3.0,,,,,,,,,,,,,,,,0.7.0,,,,,0,,,,,,,,"Suppose I have Pig script which accesses a directory in HDFS for which I do not have permissions

shell> hadoop fs -ls /mydata/group_permissions/

drwxr-x---   - groupuser restrictedgroup          0 2009-03-24 10:58 /mydata/group_permissions/20090323

{code}

%default dates_to_process '20090323'

MYDATA = load '/mydata/group_permissions/{$dates_to_process}*' using
PigStorage() as (col1,col2,col3) ;

MYDATA_PROJECT = foreach MYDATA generate
        (chararray) col1#'acct' as acct,
        (int)col1#'country' as country,
        (int)col1#'product' as product

dump MYDATA_PROJECT;

{code}

The error message we get is:
===============================================================================================
2009-03-26 00:00:05,753 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2099: Problem in constructing slices.
Details at logfile: /home/viraj/pig_1238025596328.log
===============================================================================================
This message is definitely hard to debug

===============================================================================================
With the previous version 1.0.0 I get the following error message, which is more appropriate to this case.
===============================================================================================

2009-03-26 00:01:41,787 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - java.io.IOException: org.apache.hadoop.fs.permission.AccessControlException: Permission denied: user=viraj, access=READ_EXECUTE, inode=""20090323"":groupuser:restrictedgroup:rwxr-x--- [org.apache.hadoop.fs.permission.AccessControlException: Permission denied: user=viraj, access=READ_EXECUTE, inode=""20090323"":groupuser:restrictedgroup:rwxr-x---]
        at org.apache.pig.backend.hadoop.datastorage.HDirectory.iterator(HDirectory.java:157)
        at org.apache.pig.backend.executionengine.PigSlicer.slice(PigSlicer.java:77)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:206)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:742)
        at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:370)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
        at org.apache.hadoop.mapred.jobcontrol.JobControl.run(JobControl.java:279)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: org.apache.hadoop.fs.permission.AccessControlException: Permission denied: user=viraj, access=READ_EXECUTE, inode=""20090323"":groupuser:restrictedgroup:rwxr-x---
        ... 8 more

2009-03-26 00:01:41,798 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1066: Unable to open iterator for alias MYDATA_PROJECT
Details at logfile:  /home/viraj/pig_1238025692361.log

===============================================================================================


",,,,,,,,,,,,,,,,,26/Mar/09 23:57;viraj;pig_latestversion_errmsg.log;https://issues.apache.org/jira/secure/attachment/12403761/pig_latestversion_errmsg.log,26/Mar/09 23:57;viraj;pig_oldversion_errmsg.log;https://issues.apache.org/jira/secure/attachment/12403760/pig_oldversion_errmsg.log,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-03-26 23:30:17.292,,,no_permission,,,,,,,,,,,,,164303,,,,Thu Jul 08 20:27:04 UTC 2010,,,,,,,0|i0gnqf:,95275,,,,,,,,,,26/Mar/09 00:26;viraj;Pig error logs,26/Mar/09 00:28;viraj;re-attaching with ASF inclusion,26/Mar/09 23:30;sms;Linking this issue to PIG-728,26/Mar/09 23:57;viraj;Mixed up the files in the previous attachment,"08/Jul/10 20:27;olgan;slicer code is completely gone with Pig 0.7.0. Please, reopen if the error message is still not clear",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultTuple underestimate the memory footprint for string,PIG-1443,12466587,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,09/Jun/10 17:17,11/Jun/10 18:00,13/Mar/19 23:13,11/Jun/10 18:00,0.7.0,,,,,,,,,,,,,,,,0.7.0,,impl,,,0,,,,,,,,"Currently, in DefaultTuple, we estimate the memory footprint for string as if it is char array. The formula we use is:  length * 2 + 12. It turns out we underestimate the memory usage for string. Here is a list of real memory footprint for string we get from memory dump:

| length of string | memory in bytes |
| 7 | 56 |
| 3 | 48 |
| 1 | 40 |

I did a search and find the following formula can accurately estimate the memory footprint for string:
{code}
8 * (int) (((length * 2) + 45) / 8) 
{code}",,,,,,,,,,,,,,,,,09/Jun/10 17:32;daijy;PIG-1443-1.patch;https://issues.apache.org/jira/secure/attachment/12446712/PIG-1443-1.patch,09/Jun/10 23:58;daijy;PIG-1443-2.patch;https://issues.apache.org/jira/secure/attachment/12446732/PIG-1443-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-06-09 23:04:45.492,,,no_permission,,,,,,,,,,,,,164932,Reviewed,,,Fri Jun 11 18:00:26 UTC 2010,,,,,,,0|i0gvxr:,96604,,,,,,,,,,09/Jun/10 17:20;daijy;Reference: http://www.javamex.com/tutorials/memory/string_memory_usage.shtml,"09/Jun/10 23:04;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12446712/PIG-1443-1.patch
  against trunk revision 952098.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    -1 javac.  The applied patch generated 139 javac compiler warnings (more than the trunk's current 138 warnings).

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/321/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/321/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/321/console

This message is automatically generated.",09/Jun/10 23:58;daijy;Deal with javac warning.,10/Jun/10 19:22;rding;+1 for commit after javac warning is fixed.,11/Jun/10 18:00;daijy;Patch committed to both trunk and 0.7 branch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Zebra] Pig script with Zebra data storage brings down name node due to excessive name node call.,PIG-1421,12464697,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,xuefuz,xuefuz,xuefuz,17/May/10 17:14,17/May/10 22:19,13/Mar/19 23:13,17/May/10 22:19,0.7.0,,,,,,,,,,,,,,,,0.7.0,,,,,0,,,,,,,,"Because Pig call setLocation() on LoadFunc API on both frontent and backend, and Zebra makes name node access in its implementation, name node becomes irresponsive because of the number of name node calls.",,,,,,,,,,,,,,,,,17/May/10 19:06;xuefuz;PIG-1421.patch;https://issues.apache.org/jira/secure/attachment/12444727/PIG-1421.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-05-17 19:41:37.426,,,no_permission,,,,,,,,,,,,,164913,Reviewed,,,Mon May 17 22:19:35 UTC 2010,,,,,,,0|i0gvon:,96563,,,,,,,,,,"17/May/10 17:18;xuefuz;Fix the issue by making sure that when setLocation() is called, no name node access is conducted.","17/May/10 19:06;xuefuz;Fix includes:

1. Make setLocation() light weight and make sure no name node access. Note that setLocation() was a new API on LoadFunc introduced in 0.7. UDFContext is used for some cases.
2. Remove code for setting properties (INPUT_FE and INPUT_DELETED_CGS) in TableInputFormat because it's ineffective.
3. Move the logic in #2 to TableInputFormat.setInputPaths() and make sure that it's only done once (Because setInputPaths() are called multiple times in PIG code path).
4. Remove unnecessary list status calls in  Zebra IO layer.
5. Remove the code that makes name node calls for sorted table in Pig code path.
6. Make sure that clob check is only done on the front end.","17/May/10 19:41;yanz;Local Hudson results are as follows:

[exec] -1 overall.
     [exec]
     [exec]     +1 @author.  The patch does not contain any @author tags.
     [exec]
     [exec]     -1 tests included.  The patch doesn't appear to include any new or modified tests.
     [exec]                         Please justify why no tests are needed for this patch.
     [exec]
     [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.
     [exec]
     [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.
     [exec]
     [exec]     +1 findbugs.  The patch does not introduce any new Findbugs warnings.
     [exec]
     [exec]     +1 release audit.  The applied patch does not increase the total number of release audit warnings.

No test case is added as the problem is related to excessive name node calls on a real cluster. We manually check the fix so that name node works without any hiccups.",17/May/10 19:41;yanz;+1,"17/May/10 19:50;xuefuz;Original problem happens only in stressed scenario. It's difficult to provide a unit test case to cover this. With this, hudson result can be ignored.",17/May/10 22:19;yanz;committed to the trunk and the 0.7 branch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoadFunc signature is not correct in LoadFunc.getSchema sometimes,PIG-1415,12464470,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,13/May/10 21:38,17/May/10 17:21,13/Mar/19 23:13,17/May/10 17:21,0.7.0,,,,,,,,,,,,,,,,0.7.0,,impl,,,0,,,,,,,,"The following script does not set signature correctly when we call LoadFunc.getSchema.

a = load 'xxx' using TableLoader('xxx') as (a, b, c);

However, if we don't give schema to a, we get the right signature:

a = load 'xxx' using TableLoader('xxx);

Diagnosis:
Parser will generate LoadClause before go to the generation ""Alias = LoadClause"", which actually set signature to the LOLoad. When we give a schema, parser try to call LOLoad.setSchema(), internally it will call LoadFunc.determineSchema. And at that time, signature has not been set yet. 

This relates to the change we cache determinedSchema in LOLoad [PIG-1317|https://issues.apache.org/jira/browse/PIG-1317]. Before that change, we will later call LoadFunc.getSchema() again using the right signature. Now we cache determinedSchema, so LoadFunc don't have a chance to get the right signature inside LoadFunc.getSchema()

Solution:
We shall not call LoadFunc.determineSchema inside LOLoad.setSchema().",,,,,,,,,,,,,,,,,14/May/10 00:51;daijy;PIG-1415-1.patch;https://issues.apache.org/jira/secure/attachment/12444451/PIG-1415-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-05-14 18:52:54.658,,,no_permission,,,,,,,,,,,,,164907,Reviewed,,,Mon May 17 17:21:40 UTC 2010,,,,,,,0|i0gvmn:,96554,,,,,,,,,,14/May/10 18:52;ashutoshc;+1 Please commit if all unit tests pass.,17/May/10 17:21;daijy;Patch committed to both 0.7 branch and trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casts to complex types do not work as expected,PIG-616,12412222,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,alangates,sms,sms,12/Jan/09 19:40,10/May/10 20:24,13/Mar/19 23:13,10/May/10 20:24,0.2.0,,,,,,,,,,,,,,,,,,impl,,,0,,,,,,,,"When we specify a (complex) type as a column in Pig, the TypeCastInserter inserts the appropriate cast for the (complex) type. However, in the implementation of POCast.java, when databyte arrays are converted to the (complex) types, we invoke the bytesToXXX method. 

For complex types, especially tuples and bags, we do not enforce the typing information specified by the user in the AS clause or with the explicit cast statement. The implementation solely relies on bytesToXXX to figure out the right type.

An example of a query that fails is given below. Wrt the query, the data is a single column that is a bag of integers. The user specifies this bag to be a bag of chararray. This conversion is allowed in pig but the implementation does not perform the actual cast. Instead the bytesToBag is called on the stream. The resulting type is a bag of integers and not a bag of chararray. In the subsequent statement the user (correctly) assumes that the conversion has been performed but in reality it has not been done. At run time when a chararray based operation is performed we see a ClassCastException.

The notion of a schema has is absent in the physical operators. The schema/fieldSchema in the logical layer has to be passed on to the physical layer. The schema can be used to perform additional operations like casting, etc.

{code}

grunt> cat bag.data
{(1)}

grunt> a = load 'bag.data' as (b:{t:(c:chararray)});
grunt> b = foreach a generate flatten(b);
grunt> c = foreach b generate CONCAT('Hello ', $0);
grunt> dump c;

2009-01-12 10:44:44,417 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete
2009-01-12 10:45:09,439 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Map reduce job failed
2009-01-12 10:45:09,440 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Job failed!
2009-01-12 10:45:09,443 [main] ERROR org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.Launcher - Error message from task (map) task_200812151518_9681_m_000000java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String
        at org.apache.pig.builtin.StringConcat.exec(StringConcat.java:37)
        at org.apache.pig.builtin.StringConcat.exec(StringConcat.java:31)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:185)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:259)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:271)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:197)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:187)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:175)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.map(PigMapOnly.java:65)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)
...

2009-01-12 10:45:09,448 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1066: Unable to open iterator for alias c

2009-01-12 10:45:09,448 [main] ERROR org.apache.pig.tools.grunt.Grunt - org.apache.pig.impl.logicalLayer.FrontendException: Unable to open iterator for alias c
        at org.apache.pig.PigServer.openIterator(PigServer.java:426)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:271)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:178)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:84)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:72)
        at org.apache.pig.Main.main(Main.java:302)

Caused by: java.io.IOException: Job terminated with anomalous status FAILED
        at org.apache.pig.PigServer.openIterator(PigServer.java:420)
        ... 5 more

{code}",,,,,,,,,,,PIG-613,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-13 01:02:25.447,,,no_permission,,,,,,,,,,,,,164208,,,,Mon May 10 20:24:46 UTC 2010,,,,,,,0|i0gmc7:,95049,,,,,,,,,,"13/Jan/09 01:02;araceli;Please also verify the following test case where the source file ""MyFile.tx"" contains the following data: Fint:int, Fdouble:double,  Ftuple: ( chararray, age, avg). The load statement defines a schema in the load statement that  should result in a type conflict. ( Note that ""BADTYPE"" is being loaded as an int, but myFile contains a chararray. )

 A =LOAD 'myFile.txt' USING PigStorage () AS (Fint:int,  Fdouble:double,  Ftuple: ( BADTYPE:int, age, avg) );
STORE A INTO 'resultMyFile.out' USING PigStorage();

The expected behavior is that an error  be thrown indicating  there is a type conflict - but currently no error is thrown.",19/Jan/09 10:52;shravanmn;I don't think introducing schema in the physical side is a good way out. What we probably need is something like nested cast operators for complex types.,08/May/10 12:21;azaroth;This issue has been solved in trunk,"10/May/10 20:24;olgan;Thanks, Gianmarco for testing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problems in pushing down foreach with flatten,PIG-874,12429762,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,sms,sms,07/Jul/09 21:57,03/May/10 17:07,13/Mar/19 23:13,03/May/10 17:07,0.4.0,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"If the graph contains more than one foreach connected to an operator, pushing down foreach with flatten is not possible with the current optimizer pattern matching algorithm and current implementation of rewire. The following mechanism of pushing foreach with flatten does not work.

1. Search for foreach (with flatten) connected to an operator
2. If checks pass then unflatten the flattened column in the foreach
3. Create a new foreach that flattens the mapped column (the original column number could have changed) and insert the new foreach after the old foreach's successor.

An example to illustrate the problem:

{code}
A = load 'myfile' as (name, age, gpa:(letter_grade, point_score));
B = foreach A generate $0, $1, flatten($2);
C = load 'anotherfile' as (name, age, preference:(course_name, instructor));
D = foreach C generate $0, $1, flatten($2);
E = join B by $0, D by $0 using ""replicated"";
F = limit E 10;
{code}

In the code snipped (see above), the optimizer will find two matches, B->E and D->E. For the first pattern match (B->E), $2 will be unflattened and a new foreach will be introduced after the join.

{code}
A = load 'myfile' as (name, age, gpa:(letter_grade, point_score));
B = foreach A generate $0, $1, $2;
C = load 'anotherfile' as (name, age, preference:(course_name, instructor));
D = foreach C generate $0, $1, flatten($2);
E = join B by $0, D by $0 using ""replicated"";
E1 = foreach E generate $0, $1, flatten($2), $3, $4, $5, $6;
F = limit E1 10;
{code}

For the second match (D->E), the same transformation is applied. However, this transformation will not work for the following reason. The new foreach is now inserted between the E and E1. When E1 is rewired, rewire is unable to map $6 in E1 as it never exists in E. In order to fix such situations, the pattern matching should return a global match instead of a local match.

Reference: PIG-873",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-05-03 17:07:21.138,,,no_permission,,,,,,,,,,,,,164425,,,,Mon May 03 17:07:21 UTC 2010,,,,,,,0|i0gpcv:,95538,,,,,,,,,,03/May/10 17:07;olgan;This is getting addressed with the optimizer re-work,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
default slicer should support Hadoop's standard way to setup split size,PIG-840,12427400,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,olgan,olgan,08/Jun/09 20:47,01/May/10 00:45,13/Mar/19 23:13,01/May/10 00:45,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,It should honor mapre.min.split.size rather than using its own var of pig.overrideBlockSize,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-06-08 22:39:30.049,,,no_permission,,,,,,,,,,,,,164393,,,,Sat May 01 00:45:05 UTC 2010,,,,,,,0|i0goyf:,95473,,,,,,,,,,"08/Jun/09 22:39;milindb;Ideally, a slicer should only be a thin wrapper on top of InputSplit, so that all the different kinds of splits already part of hadoop (such as one-line input split, multi-file input split, default file-input split) etc are all supported out of the box.",01/May/10 00:45;olgan;This is no longer relevant  with Pig 0.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig patch tests are failing ,PIG-764,12422753,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,gkesavan,gkesavan,14/Apr/09 04:45,01/May/10 00:33,13/Mar/19 23:13,01/May/10 00:33,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Pig patch tests are failing... 

http://hudson.zones.apache.org/hudson/view/Pig/job/Pig-Patch-minerva.apache.org/lastBuild/testReport/",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-16 23:33:41.785,,,no_permission,,,,,,,,,,,,,164327,,,,Thu Apr 16 23:33:41 UTC 2009,,,,,,,0|i0go2n:,95330,,,,,,,,,,16/Apr/09 23:33;olgan;Looks like this is an issue with some test cases not properly using minidfs. I will be fixing them,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
splitsize is ignored in PigInputFormat,PIG-657,12414160,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,laukik,laukik,05/Feb/09 23:35,01/May/10 00:29,13/Mar/19 23:13,01/May/10 00:29,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"The way to control the number of mappers in Hadoop has been to specify a mapred.min.split.size parameter in the job conf. For eg.  mapred.min.split.size=1073741824,mapred.map.tasks=10

However, even if this parameter is specified, Pig creates the number of mappers depending only on the number of blocks in the file. This is because the parameter is not used in the PigInputFormat.

The parameter can actually be extracted from the job conf object. So, one way of doing this would be to pass an handle to the job conf object to the PigInputFormat or the custom slicer.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-05-01 00:29:24.969,,,no_permission,,,,,,,,,,,,,164241,,,,Sat May 01 00:29:24 UTC 2010,,,,,,,0|i0gmt3:,95125,,,,,,,,,,01/May/10 00:29;olgan;This is resolved with Pig 0.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"parametere substitution truncates value if it has special character like "".""",PIG-668,12414685,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,olgan,olgan,12/Feb/09 19:10,30/Apr/10 21:48,13/Mar/19 23:13,30/Apr/10 21:48,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Input script:

a = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age,gpa);
b = order a by name;
c = limit b 10;
store into '$out';

command:  pig -param out=a.txt -dryrun test_param.pig

substituted script:

a = load '/user/pig/tests/data/singlefile/studenttab10k' as (name, age,gpa);
b = order a by name;
c = limit b 10;
store into 'a';",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-04-30 21:48:03.34,,,no_permission,,,,,,,,,,,,,164248,,,,Fri Apr 30 21:48:03 UTC 2010,,,,,,,0|i0gmxr:,95146,,,,,,,,,,"30/Apr/10 21:48;thejas;Verified that this bug is not present in 0.6 and 0.7 branches, and trunk .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There should be a way for Loader to refer to the output of determineSchema() in the backend,PIG-492,12406349,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,pkamath,pkamath,14/Oct/08 01:19,15/Jan/10 06:33,13/Mar/19 23:13,15/Jan/10 06:33,0.2.0,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,Currently LoadFunc.determineSchema() is only called from LOLoad() at parse time in the front end. If the loader.getNext() needs to know what the output of determineSchema() was there is no way to get to it in the backend - there should be some way to get to it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-01-15 06:33:57.863,,,no_permission,,,,,,,,,,,,,164101,,,,Fri Jan 15 06:33:57 UTC 2010,,,,,,,0|i0gkvj:,94812,,,,,,,,,,"14/Oct/08 16:46;pkamath;To add more to the initial description:
Here is a scenario showing the need for this: Consider a loader which samples the first 100 records (say) to determine the schema and returns a schema which is {(long, chararray)}. However at runtime while constructing the tuple, the getNext() code might need to know that determineSchema() had returned {(long, chararray)} at parse time and hence construct the tuple accordingly to consists of Long and chararray (String) fields. To be able to do this, the loader should somehow have access to the schema which was returned on the initial determineSchema() call.",15/Jan/10 06:33;alangates;PIG-1085 provides this functionality.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve parsing for UDFs in QueryParser,PIG-163,12391922,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,acmurthy,acmurthy,19/Mar/08 21:39,15/Jan/10 05:58,13/Mar/19 23:13,15/Jan/10 05:58,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Parsing of UDFs in QueryParser (used in LOAD/GROUP) could be more strict, currently it just assumes it is a list of quoted-strings, so for e.g. it doesn't handle UDFs which take other UDFs as arguments.",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-01-15 05:58:09.534,,,no_permission,,,,,,,,,,,,,163817,,,,Fri Jan 15 05:58:09 UTC 2010,,,,,,,0|i0ggvr:,94165,,,,,,,,,,15/Jan/10 05:58;alangates;Fixed a long time ago.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests for Grunt,PIG-76,12387052,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,,antmagna,antmagna,24/Jan/08 15:35,15/Jan/10 05:35,13/Mar/19 23:13,15/Jan/10 05:35,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Currently there are no units tests in place for Grunt. However Grunt is extensively used as part of the end-to-end tests. If some changes break Grunt, this will become evident only later on in the development process during E2E testing.

Talked to Alan and Olga, probably the best way to address this is to put in place unit tests that integrate with the test harness used for regression.",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-01-15 05:35:58.914,,,no_permission,,,,,,,,,,,,,163742,,,,Fri Jan 15 05:35:58 UTC 2010,,,,,,,0|i0gfuv:,93999,,,,,,,,,,15/Jan/10 05:35;alangates;Tests for grunt were added some time ago.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow type merge between numerical type and non-numerical type,PIG-989,12437048,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Major,Fixed,daijy,daijy,daijy,01/Oct/09 18:27,12/Nov/09 01:32,13/Mar/19 23:13,08/Oct/09 18:20,0.5.0,,,,,,,,,,,,,,,,,,impl,,,0,,,,,,,,"Currently, we do not allow type merge between numerical type and non-numerical type. And the error message is confusing. 

Eg, if you run:

a = load '1.txt' as (a0:chararray, a1:chararray);
b = load '2.txt' as (b0:long, b1:chararray);
c = join a by a0, b by b0;
dump c;

And the error message is ""ERROR 1051: Cannot cast to Unknown""

We shall:
1. Allow the type merge between numerical type and non-numerical type
2. Or at least, provide more meaningful error message to the user",,,,,,,,,,,,,,,,,01/Oct/09 23:03;daijy;PIG-989-1.patch;https://issues.apache.org/jira/secure/attachment/12421079/PIG-989-1.patch,02/Oct/09 18:03;daijy;PIG-989-2.patch;https://issues.apache.org/jira/secure/attachment/12421132/PIG-989-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-10-01 21:17:19.502,,,no_permission,,,,,,,,,,,,,164525,Reviewed,,,Thu Oct 08 18:20:33 UTC 2009,,,,,,,0|i0gqo7:,95751,,,,,,,,,,"01/Oct/09 21:17;alangates;I agree the error message here is bad.

It is not clear to me that we should do this join though.  It's hard to see why people would want to join on two different key types when one of those is numeric and one is string.  If they really want that, I think we should require an explicit cast, as in most cases I suspect this is a user error rather than desired behavior.","01/Oct/09 21:33;daijy;I agree. Merge numerical type and non-numerical type have a semantic problem of which way to cast (whether to cast numerical to non-numerical, or vice versa). I think at this point, we can just fix the error message.",01/Oct/09 23:03;daijy;Patch to fix error message,01/Oct/09 23:04;daijy;There is no good way to add a unit test to it. I tested it manually.,"02/Oct/09 02:08;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421079/PIG-989-1.patch
  against trunk revision 820394.

    +1 @author.  The patch does not contain any @author tags.

    -1 tests included.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no tests are needed for this patch.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    +1 core tests.  The patch passed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/12/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/12/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: http://hudson.zones.apache.org/hudson/job/Pig-Patch-h8.grid.sp2.yahoo.net/12/console

This message is automatically generated.",02/Oct/09 18:03;daijy;Tiny change: error code should be in user input category. ,"08/Oct/09 17:51;olgan;+1, looks good. Please, commit",08/Oct/09 18:20;daijy;Patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"OrcStorage fails when ""bytearray"" represents unknown type",PIG-5383,13220926,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,knoguchi,knoguchi,knoguchi,11/Mar/19 20:08,12/Mar/19 04:28,13/Mar/19 23:13,12/Mar/19 04:28,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"In Pig, ""bytearray"" can be array of bytes OR unknown type.  
OrcStorage cannot handle the latter for writes and fails with 

{noformat}
2019-02-14 05:45:43,855 [PigTezLauncher-0] INFO  org.apache.pig.backend.hadoop.executionengine.tez.TezJob  - DAG Status: status=FAILED, progress=TotalTasks: 39549 Succeeded: 31451 Running: 0 Failed: 1 Killed: 8097 FailedTaskAttempts: 2865 KilledTaskAttempts: 1305, diagnostics=Vertex failed, vertexName=scope-56672, vertexId=vertex_, diagnostics=[Task failed, taskId=task_, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_:java.lang.ClassCastException: java.lang.Boolean cannot be cast to [B
	at org.apache.pig.impl.util.orc.OrcUtils$PigDataByteArrayObjectInspector.getPrimitiveWritableObject(OrcUtils.java:648)
	at org.apache.hadoop.hive.ql.io.orc.WriterImpl$BinaryTreeWriter.write(WriterImpl.java:1547)
	at org.apache.hadoop.hive.ql.io.orc.WriterImpl$MapTreeWriter.write(WriterImpl.java:1933)
	at org.apache.hadoop.hive.ql.io.orc.WriterImpl$StructTreeWriter.write(WriterImpl.java:1805)
	at org.apache.hadoop.hive.ql.io.orc.WriterImpl.addRow(WriterImpl.java:2477)
	at org.apache.hadoop.hive.ql.io.orc.OrcNewOutputFormat$OrcRecordWriter.write(OrcNewOutputFormat.java:53)
	at org.apache.hadoop.hive.ql.io.orc.OrcNewOutputFormat$OrcRecordWriter.write(OrcNewOutputFormat.java:37)
	at org.apache.pig.builtin.OrcStorage.putNext(OrcStorage.java:262)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:136)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:95)
	at org.apache.tez.mapreduce.output.MROutput$1.write(MROutput.java:557)
	at org.apache.pig.backend.hadoop.executionengine.tez.plan.operator.POStoreTez.getNextTuple(POStoreTez.java:129)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.runPipeline(POSplit.java:254)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.processPlan(POSplit.java:235)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.getNextTuple(POSplit.java:227)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.runPipeline(POSplit.java:254)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.processPlan(POSplit.java:235)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.processPlan(POSplit.java:240)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.processPlan(POSplit.java:240)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.processPlan(POSplit.java:240)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.processPlan(POSplit.java:240)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.processPlan(POSplit.java:240)
	at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.getNextTuple(POSplit.java:227)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.runPipeline(PigProcessor.java:382)
	at org.apache.pig.backend.hadoop.executionengine.tez.runtime.PigProcessor.run(PigProcessor.java:244)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{noformat}",,,,,,,,,,,,,,,,,11/Mar/19 20:12;knoguchi;pig-5383-v01.patch;https://issues.apache.org/jira/secure/attachment/12962024/pig-5383-v01.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2019-03-11 20:40:56.112,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Tue Mar 12 04:28:40 UTC 2019,,,,,,,0|z00kv4:,9223372036854775807,,,,,,,,,,11/Mar/19 20:40;rohini;+1,"12/Mar/19 04:28;knoguchi;Thanks for the review Rohini! 
Committed to trunk (0.18).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig.script is not set for scripts run via PigServer,PIG-5315,13119348,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,satishsaley,rohini,rohini,17/Nov/17 22:35,04/Dec/17 20:28,13/Mar/19 23:13,04/Dec/17 20:28,,,,,,,,,,,,,,,,,0.18.0,,,,,0,newbie,,,,,,,ScriptState.get().setScript() is only called in Main and BoundScript and not in PigServer.registerScript,,,,,,,,,,,,,,,,,27/Nov/17 22:33;satishsaley;PIG-5315-1.patch;https://issues.apache.org/jira/secure/attachment/12899502/PIG-5315-1.patch,04/Dec/17 18:17;satishsaley;PIG-5315-2.patch;https://issues.apache.org/jira/secure/attachment/12900521/PIG-5315-2.patch,04/Dec/17 19:34;satishsaley;PIG-5315-3.patch;https://issues.apache.org/jira/secure/attachment/12900536/PIG-5315-3.patch,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2017-12-04 18:18:53.116,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Mon Dec 04 20:28:46 UTC 2017,,,,,,,0|i3my5j:,9223372036854775807,,,,,,,,,,"28/Nov/17 20:08;rohini;While checking code to ensure that we have not missed any other place, found a couple of cases.
1) We should also call ScriptState.setFileName in the PigServer.registerScript(String fileName, Map<String,String> params,List<String> paramsFiles) 
2) org.apache.pig.pigunit.pig.PigServer also requires the change
3) GruntParser.loadScript is another place ScriptState.setFileName and  ScriptState.setScript is required. run/exec commands go through this code.",04/Dec/17 18:18;satishsaley;Updated patch.,"04/Dec/17 18:38;rohini;1) FileLocalizer.fetchFile will be repeated twice which will be wasteful if the file is being fetched from hdfs/s3. Can you create a overloaded loadScript method that takes FetchFile as well?
2) beginNestedScript can be changed to beginNestedScript(File) and the setters can be called in that method instead of doing it in loadScript.
3) Initializing serializedScript and truncatedScript to """" is redundant.",04/Dec/17 19:40;satishsaley;Updated patch.,04/Dec/17 20:28;rohini;+1. Committed to trunk. Thanks for fixing this Satish.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in TezOperDependencyParallelismEstimator,PIG-5307,13106559,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,rohini,rohini,rohini,02/Oct/17 21:54,02/Oct/17 22:05,13/Mar/19 23:13,02/Oct/17 22:05,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"In case of the constant being null, NPE is thrown. This was encountered by a user who was generating the field name based on a condition which expanded to NULL when condition was not met. For eg:
{code}
x = FILTER x BY (chararray) NULL == 'fieldvalue';
{code}",,,,,,,,,,,,,,,,,02/Oct/17 21:55;rohini;PIG-5307-1.patch;https://issues.apache.org/jira/secure/attachment/12890050/PIG-5307-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-10-02 21:58:20.132,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Mon Oct 02 22:05:20 UTC 2017,,,,,,,0|i3ksvj:,9223372036854775807,,,,,,,,,,02/Oct/17 21:58;olgan;+1,02/Oct/17 22:05;rohini;Committed to trunk. Thanks for the review Olga.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
REGEX_EXTRACT() logs every line that doesn't match,PIG-5306,13104456,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,satishsaley,satishsaley,satishsaley,23/Sep/17 02:55,26/Sep/17 17:18,13/Mar/19 23:13,26/Sep/17 17:18,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"
Pig logs a warning message for every call that doesn't doesn't match a capture group. The documentation only says this case returns NULL. From a developer standpoint, the messages are unlikely to be useful.

https://github.com/apache/pig/blob/trunk/src/org/apache/pig/builtin/REGEX_EXTRACT.java#L107",,,,,,,,,,,,,,,,,23/Sep/17 02:59;satishsaley;PIG-5306-1.patch;https://issues.apache.org/jira/secure/attachment/12888629/PIG-5306-1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-09-26 17:18:27.772,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Tue Sep 26 17:18:27 UTC 2017,,,,,,,0|i3kfzj:,9223372036854775807,,,,,,,,,,26/Sep/17 17:18;rohini;+1. Committed to trunk. This unnecessarily fills up logs. Thanks for fixing this Satish.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bump jython to 2.7.1,PIG-5287,13093917,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,dbist13,dbist13,dbist13,10/Aug/17 21:03,11/Aug/17 03:32,13/Mar/19 23:13,11/Aug/17 03:31,0.17.0,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"jython-standalone 2.7.0 was released in 2015, 2.7.1 was released in June of 2017, please see https://issues.apache.org/jira/browse/OOZIE-3028 for details",,,,,,,,,,,,,,,,,10/Aug/17 21:16;dbist13;PIG-5287-0.patch;https://issues.apache.org/jira/secure/attachment/12881333/PIG-5287-0.patch,10/Aug/17 22:57;dbist13;PIG-5287-1.patch;https://issues.apache.org/jira/secure/attachment/12881348/PIG-5287-1.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-08-10 21:39:05.066,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Fri Aug 11 03:31:01 UTC 2017,,,,,,,0|i3inwn:,9223372036854775807,,,,,,,,,,10/Aug/17 21:39;rohini;It should be build/ivy/lib/Pig/jython-standalone-2.7.1.jar,"10/Aug/17 21:50;dbist13;[~rohini] it is jython-standalone, eventhough ivy says jython.version, with my patch, this is what I see in the directory
{noformat}
hw12107:pig aervits$ ls build/ivy/lib/Pig/jython*
build/ivy/lib/Pig/jython-standalone-2.7.1.jar
{noformat}

based on this https://github.com/apache/pig/blob/trunk/ivy.xml#L244","10/Aug/17 22:52;rohini;That is what I meant
{code}
<classpathentry exported=""true"" kind=""lib"" path=""build/ivy/lib/Pig/jython-2.7.1.jar""/>
{code}

should be
{code}
<classpathentry exported=""true"" kind=""lib"" path=""build/ivy/lib/Pig/jython-standalone-2.7.1.jar""/>
{code}

in the patch","10/Aug/17 22:57;dbist13;[~rohini] I amended the patch, though I must say that file is out of date as it also references hadoop 0.20 etc.","11/Aug/17 03:31;rohini;+1. Committed to trunk. 

I guess the file has not been updated in a while. We have to fix it sometime to work with newer versions of hadoop and add Tez and Spark. Instead of having a hardcoded file that will go outdated often as versions are incremented would be better to have a ant target that generates the file based on dependencies in build/ivy/lib. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"building ""jar"" should not call ""clean""",PIG-5276,13090590,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,nkollar,knoguchi,knoguchi,27/Jul/17 18:38,01/Aug/17 16:38,13/Mar/19 23:13,01/Aug/17 08:59,,,,,,,,,,,,,,,,,0.18.0,,build,,,0,,,,,,,,"When adding spark 2 in PIG-5157, we started calling ""clean""  from inside ""jar"" target.  
To me, ""jar"" action should be limited to archiving classes.

For example, when I run 
% ant javadoc
% ant jar

I should not see javadoc gone after the second line.
",,,,,,,,,,,,,,,,,31/Jul/17 12:09;nkollar;PIG-5276_1.patch;https://issues.apache.org/jira/secure/attachment/12879622/PIG-5276_1.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-07-31 01:42:33.02,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 01 16:38:18 UTC 2017,,,,,,,0|i3i3p3:,9223372036854775807,,,,,,,,,,31/Jul/17 01:42;kellyzly;[~nkollar]:  can we not call clean before jar when upgrading to spark2 in PIG-5157?,"31/Jul/17 07:45;nkollar;[~kellyzly] [~knoguchi] how about having a new clean target (which we'll call from jar target) identical with the current clean, but without deleting {{$\{docs.dir\}/build}} and {{$\{jdiff.xml.dir\}\$\{name\}_$\{version\}.xml}}? I think we should delete the rest.","31/Jul/17 07:50;kellyzly;[~nkollar]:
when upgrading to spark2, we need to delete the two files you mentioned?","31/Jul/17 07:55;nkollar;I don't think we should, not deleting them should be fine, but I'm not completely sure.","31/Jul/17 12:10;nkollar;[~knoguchi], [~kellyzly] could you please review PIG-5276_1.patch? Delete only dependencies from jar target.",31/Jul/17 14:42;szita;+1 [~nkollar],01/Aug/17 05:09;knoguchi;Sorry for the late update.  I didn't have time to think of how this should be done.  But anything is better than wiping the entire build dirs. :)  I'm +1.    ,01/Aug/17 08:58;szita;Committed to trunk. Thanks for fixing this [~nkollar],01/Aug/17 16:38;kellyzly;[~nkollar]:+1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in Pig Logging,PIG-5270,13086467,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,FromAlaska49,FromAlaska49,FromAlaska49,12/Jul/17 05:12,12/Jul/17 21:44,13/Mar/19 23:13,12/Jul/17 21:44,0.13.0,0.14.0,0.15.0,0.16.0,0.17.0,,,,,,,,,,,,0.18.0,,data,,,0,easyfix,patch,,,,,,"In the log output of the internalCopyAllGeneratedToDistributedCache() method in pig/data/SchemaTupleFrontend.java the word ""cache"" is misspelled as ""cacche"". According to another issue, this was already addressed and resolved in 2013, however the issue persists in the latest releases.

Here is a link to the previous issue: https://issues.apache.org/jira/browse/PIG-3432
I also issued a pull request to the Github mirror: https://github.com/apache/pig/pull/30",All,,300,300,,0%,300,300,,,,,,,,,12/Jul/17 05:43;FromAlaska49;PIG-5270.patch;https://issues.apache.org/jira/secure/attachment/12876765/PIG-5270.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-07-12 21:44:24.06,,,no_permission,,,,https://github.com/apache/pig/pull/30.patch,Patch,,,,,,,,9223372036854775807,Reviewed,,,Wed Jul 12 21:44:24 UTC 2017,,,Patch Available,,,,0|i3hep3:,9223372036854775807,,,,,,,,,,12/Jul/17 21:44;daijy;Committed to trunk. Thank for fixing typo.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using wildcard doesn't work with OrcStorage,PIG-5263,13081860,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,satishsaley,satishsaley,satishsaley,22/Jun/17 17:47,29/Jun/17 22:36,13/Mar/19 23:13,29/Jun/17 22:36,,,,,,,,,,,,,,,,,0.18.0,,,,,0,,,,,,,,"myinput = LOAD '/user/saley/data/datestamp=20170301*' USING OrcStorage();

Its throwing an exception {{Caused by: java.io.FileNotFoundException: File hdfs://localhost:8020/user/saley/data/datestamp=20170301* does not exist.}}
Full stack trace
{code}
2017-03-03 18:50:12,651 [main] INFO org.apache.hadoop.conf.Configuration.deprecation - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
org.apache.pig.backend.executionengine.ExecException: ERROR 2118: serious problem
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:279)
at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateNewSplits(MRInputHelpers.java:411)
at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplitsToMem(MRInputHelpers.java:292) 
at org.apache.pig.backend.hadoop.executionengine.tez.plan.optimizer.LoaderProcessor.processLoads(LoaderProcessor.java:169)
at org.apache.pig.backend.hadoop.executionengine.tez.plan.optimizer.LoaderProcessor.visitTezOp(LoaderProcessor.java:182)
at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezOperator.visit(TezOperator.java:259)
at org.apache.pig.backend.hadoop.executionengine.tez.plan.TezOperator.visit(TezOperator.java:56)
at org.apache.pig.impl.plan.DependencyOrderWalker.walk(DependencyOrderWalker.java:87)
at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:46)
at org.apache.pig.backend.hadoop.executionengine.tez.TezLauncher.processLoadAndParallelism(TezLauncher.java:503)
at org.apache.pig.backend.hadoop.executionengine.tez.TezLauncher.launchPig(TezLauncher.java:187)
at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:286)
at org.apache.pig.PigServer.launchPlan(PigServer.java:1401)
at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1386)
at org.apache.pig.PigServer.storeEx(PigServer.java:1045)
at org.apache.pig.PigServer.store(PigServer.java:1008)
at org.apache.pig.PigServer.openIterator(PigServer.java:921)
at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:762)
at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:376)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)
at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
at org.apache.pig.Main.run(Main.java:630)
at org.apache.pig.Main.main(Main.java:176)
Caused by: java.lang.RuntimeException: serious problem
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1119)
at org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat.getSplits(OrcNewInputFormat.java:121)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigInputFormat.getSplits(PigInputFormat.java:265)
... 23 more
Caused by: java.util.concurrent.ExecutionException: java.io.FileNotFoundException: File hdfs://localhost:8020/user/saley/data/datestamp=20170301* does not exist. 
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:192)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1087)
... 25 more
Caused by: java.io.FileNotFoundException: File hdfs://localhost:8020/user/saley/data/datestamp=20170301* does not exist.
at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:948)
at org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator.<init>(DistributedFileSystem.java:927)
at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:872)
at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:868)
at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
at org.apache.hadoop.hdfs.DistributedFileSystem.listLocatedStatus(DistributedFileSystem.java:886)
at org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:1697)
at org.apache.hadoop.hive.shims.Hadoop23Shims.listLocatedStatus(Hadoop23Shims.java:665)
at org.apache.hadoop.hive.ql.io.AcidUtils.getAcidState(AcidUtils.java:361)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.callInternal(OrcInputFormat.java:692)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.access$600(OrcInputFormat.java:659)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator$1.run(OrcInputFormat.java:682)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator$1.run(OrcInputFormat.java:679)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1738)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:679)
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:659)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
{code}",,,,,,,,,,,,,,,,,22/Jun/17 17:51;satishsaley;PIG-5263-1.patch;https://issues.apache.org/jira/secure/attachment/12874117/PIG-5263-1.patch,24/Jun/17 23:17;satishsaley;PIG-5263-2.patch;https://issues.apache.org/jira/secure/attachment/12874393/PIG-5263-2.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2017-06-23 23:49:12.768,,,no_permission,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Thu Jun 29 22:36:52 UTC 2017,,,,,,,0|i3gmdb:,9223372036854775807,,,,,,,,,,"22/Jun/17 17:53;satishsaley;The approach to solve this issue is to glob paths using {{FileStatus[] org.apache.hadoop.fs.FileSystem.globStatus(Path pathPattern, PathFilter filter) throws IOException}} for input paths.","23/Jun/17 23:49;rohini;Few minor comments:
  - Please add javadoc for getGlobPaths method. 
  - pathString  -> commaSeparatedPaths
  - notfound -> not found",29/Jun/17 22:36;rohini;+1. Committed to trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POPartialAgg aggregates too aggressively when multiple values aggregated,PIG-4001,12720172,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,tmwoodruff,tmwoodruff,tmwoodruff,09/Jun/14 18:58,19/Jun/14 23:59,13/Mar/19 23:13,19/Jun/14 23:59,0.13.0,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"{{POPartialAgg.aggregateRawRow()}} is counting values in {{numEntriesInTarget}}, but all the values that {{numEntriesInTarget}} is compared to count tuples.

This is similar to PIG-3649 but has less of a performance impact, as it generally does not cause map-side aggregation to be disabled entirely (since {{aggregateRawRow()}} is unlikely to be called before {{checkSizeReduction()}}; however, it can cause second-level aggregation to be run too often.

",,,,,,,,,,,,,,,,,09/Jun/14 19:00;tmwoodruff;PIG-4001.patch;https://issues.apache.org/jira/secure/attachment/12649426/PIG-4001.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-18 19:40:17.851,,,no_permission,,,,,,,,,,,,,398371,,,,Thu Jun 19 23:59:17 UTC 2014,,,,,,,0|i1wl2f:,398498,,,,,,,,,,09/Jun/14 19:02;tmwoodruff;This patch simply changes the value count to a tuple count.,18/Jun/14 19:40;cheolsoo;+1. Will commit this later today.,19/Jun/14 23:59;cheolsoo;Committed to trunk. Thank you Travis!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automaton dependency missing from Pig 11.1-h2 POM.,PIG-3315,12646505,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,07/May/13 21:25,24/May/13 21:47,13/Mar/19 23:13,24/May/13 21:47,0.11.1,,,,,,,,,,,,,,,,0.11.2,,,,,0,,,,,,,,"The automaton dependency is missing from pig-11.1-h2 POM, leading to a stack trace starting with {{java.lang.NoClassDefFoundError: dk/brics/automaton/Automaton}}",Pig 11.1-h2,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-05-24 21:43:41.166,,,no_permission,,,,,,,,,,,,,326863,Reviewed,,,Fri May 24 21:47:20 UTC 2013,,,,,,,0|i1ke1j:,327208,,,,,,,,,,"07/May/13 21:25;stevel@apache.org;Full stack]{code}
java.lang.NoClassDefFoundError: dk/brics/automaton/Automaton
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	at org.apache.pig.impl.util.JarManager$DefaultPigPackages.<clinit>(JarManager.java:99)
	at org.apache.pig.impl.util.JarManager.createJar(JarManager.java:136)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:517)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:294)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:177)
	at org.apache.pig.PigServer.launchPlan(PigServer.java:1264)
	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1249)
	at org.apache.pig.PigServer.storeEx(PigServer.java:931)
	at org.apache.pig.PigServer.store(PigServer.java:898)
	at org.apache.pig.PigServer.openIterator(PigServer.java:811)
	at org.apache.pig.PigServer$openIterator.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at org.apache.hadoop.fs.swift.integration.IntegrationTestBase.runBasePigJob(IntegrationTestBase.groovy:257)
	at org.apache.hadoop.fs.swift.integration.IntegrationTestBase$runBasePigJob.callCurrent(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:46)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
	at org.apache.hadoop.fs.swift.integration.pig.TestPig.testLoadGeneratedData(TestPig.groovy:37)
{code}","07/May/13 21:27;stevel@apache.org;Workaround is for downstream projects to declare the dependency explicitly
{code}
     <dependency>
      <groupId>dk.brics.automaton</groupId>
      <artifactId>automaton</artifactId>
      <version>1.11-8</version>
    </dependency>
 {code}","24/May/13 21:43;daijy;This is not a problem in trunk, since trunk builds directly from ivy.xml. Check into branch 0.11 anyway, though we might or might not have additional 0.11 releases.",24/May/13 21:47;daijy;Committed to 0.11 branch. Thanks Steve!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig version in pig.pom is incorrect in branch-0.11,PIG-3281,12643275,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,cheolsoo,cheolsoo,cheolsoo,18/Apr/13 17:00,03/May/13 05:13,13/Mar/19 23:13,03/May/13 05:13,0.11.2,,,,,,,,,,,,,,,,0.11.2,,,,,0,,,,,,,,"The pig version in pig.pom in branch-0.11 is still ""0.11.0-SNAPSHOT"".

This makes it fail to publish the pig.pom file to Artifactory in jenkins.",,,,,,,,,,,,,,,,,18/Apr/13 17:01;cheolsoo;PIG-3281.patch;https://issues.apache.org/jira/secure/attachment/12579364/PIG-3281.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-02 20:56:56.244,,,no_permission,,,,,,,,,,,,,323685,,,,Fri May 03 05:13:58 UTC 2013,,,,,,,0|i1jugf:,324030,,,,,,,,,,02/May/13 20:56;daijy;+1,03/May/13 05:13;cheolsoo;Committed to 0.11.2. Thank you Daniel for the review!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Errors in document ""Getting Started""",PIG-2849,12600585,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,miyakawataku,miyakawataku,miyakawataku,29/Jul/12 05:53,30/Jul/12 04:23,13/Mar/19 23:13,30/Jul/12 04:23,0.10.0,,,,,,,,,,,,,,,,,,documentation,,,0,,,,,,,,"There's small errors in ""Getting Started"". The attached patches fix the issue on trunk and branch-0.10.

1.  ""Use the FILTER operator to move all records"" ->  ""Use the FILTER operator to remove all records""
2.  ""hour, count00, count12"" -> ""ngram, count00, count12""",,,,,,,,,,,,,,,,,29/Jul/12 05:56;miyakawataku;PIG-2849-branch-0.10.patch;https://issues.apache.org/jira/secure/attachment/12538268/PIG-2849-branch-0.10.patch,29/Jul/12 05:56;miyakawataku;PIG-2849-trunk.patch;https://issues.apache.org/jira/secure/attachment/12538267/PIG-2849-trunk.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-07-30 04:23:43.406,,,no_permission,,,,,,,,,,,,,256453,,,,Mon Jul 30 04:23:43 UTC 2012,,,Patch Available,,,,0|i0h5xr:,98224,,,,,,,,,,29/Jul/12 05:56;miyakawataku;Patches which fix the issue,"30/Jul/12 04:23;billgraham;Committed, thanks Miyakawa!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
top level clean target does not call clean for end-to-end tests,PIG-2226,12519189,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,alangates,alangates,alangates,17/Aug/11 21:37,20/Mar/12 05:07,13/Mar/19 23:13,20/Mar/12 05:07,0.10.0,,,,,,,,,,,,,,,,,,build,,,0,patch,,,,,,,"Doing ""ant clean"" at the top level directory does not invoke clean for the test/e2e directories.",,,,,,,,,,,,,,,,,19/Mar/12 13:01;swsachith;PIG2226.patch;https://issues.apache.org/jira/secure/attachment/12518898/PIG2226.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-19 18:41:41.078,,,no_permission,,,,,,,,,,,,,64777,,,,Tue Mar 20 05:07:32 UTC 2012,,,,,,,0|i09ylz:,56048,,,,,,,,,,"19/Mar/12 18:41;daijy;Hi, Sachith, seems it works even before your patch, the Jira may already fixed, can you double check?","20/Mar/12 02:52;swsachith;I did, if that's a mistake, sorry ! ",20/Mar/12 05:07;daijy;Already fixed on trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better error message in pig_*.log,PIG-2429,12535139,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,xutingz,anitharaju,anitharaju,14/Dec/11 11:40,20/Dec/11 20:00,13/Mar/19 23:13,20/Dec/11 20:00,0.9.0,,,,,,,,,,,,,,,,0.9.0,,,,,0,,,,,,,,"Hi,

The below script sometimes fails giving null error message and some times with correct error message in the pig_*.log

Script
{code}
register 'a.py' using jython as test;
A = load 'test.txt' as (x:chararray);
B = foreach A generate
        test.a(x) as y;
C = group B by (y);
store C into 'C';
{code}

where a.py is
{code}
@outputSchema(""n:chararray"")
def a(n):
  n = n.lower();
  return n;
{code}

input- test.txt
=======
a


=======

I have not put null check in a.py so that the script would error out.

This script when run with 0.9 version, fails printing in the pig_*.log either null error message or the correct error message

Null Error message
{code}
Pig Stack Trace
---------------
ERROR 2244: Job failed, hadoop does not return any error message

org.apache.pig.backend.executionengine.ExecException: ERROR 2244: Job failed, hadoop does not return any error message
        at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:139)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:192)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)
        at org.apache.pig.Main.run(Main.java:561)
        at org.apache.pig.Main.main(Main.java:111)
{code}

Correct Error message
{code}
Backend error message
---------------------
org.apache.pig.backend.executionengine.ExecException: ERROR 0: Error executing function
        at org.apache.pig.scripting.jython.JythonFunction.exec(JythonFunction.java:106)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:216)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:305)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:322)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:332)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:284)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:290)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNext(POLocalRearrange.java:256)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:267)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:262)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:261)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
        at org.apache.hadoop.mapred.Child.main(Child.java:255)
Caused by: Traceback (most recent call last):
  File ""a.py"", line 5, in n
AttributeError: 'NoneType' object has no attribute 'lower'

        at org.python.core.PyException.fillInStackTrace(PyException.java:70)
        at java.lang.Throwable.<init>(Throwable.java:181)
        at java.lang.Exception.<init>(Exception.java:29)
        at java.lang.RuntimeException.<init>(RuntimeException.java:32)
        at org.python.core.PyException.<init>(PyException.java:46)
        at org.python.core.PyException.<init>(PyException.java:43)
        at org.python.core.PyException.<init>(PyException.java:61)
        at org.python.core.Py.AttributeError(Py.java:145)
        at org.python.core.PyObject.noAttributeError(PyObject.java:936)
        at org.python.core.PyObject.__getattr__(PyObject.java:931)
        at org.python.pycode._pyx3.n$1(a.py:6)
        at org.python.pycode._pyx3.call_function(a.py)
        at org.python.core.PyTableCode.call(PyTableCode.java:165)
        at org.python.core.PyBaseCode.call(PyBaseCode.java:297)
        at org.python.core.PyFunction.__call__(PyFunction.java:370)
        at org.python.core.PyFunction.__call__(PyFunction.java:360)
        at org.python.core.PyFunction.__call__(PyFunction.java:355)
        at org.apache.pig.scripting.jython.JythonFunction.exec(JythonFunction.java:103)
        ... 18 more
		
{code}

Putting a couple of sysout in the code, i found it was happening when in Launcher.getStats

{code}
TaskReport[] mapRep = jobClient.getMapTaskReports(MRJobID);
getErrorMessages(mapRep, ""map"", errNotDbg, pigContext);
totalHadoopTimeSpent += computeTimeSpent(mapRep);
{code}

whenever mapRed becomes null, it gives null error message.

Can it be made better?

Regards,
Anitha
",,,,,,,,,,,,,PIG-1829,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-12-16 01:33:18.503,,,no_permission,,,,,,,,,,,,,220823,,,,Mon Dec 19 19:11:55 UTC 2011,,,,,,,0|i0h2m7:,97686,,,,,,,,,,"16/Dec/11 01:33;xutingz;Hi Anitha,
  
   I am trying to reproduce the bug and I ran that example in both branch-0.8, release-0.9 and trunk in local mode for 15 times and it gave the correct error message. May I know how often do you get the wrong message? Is there some specific condition for this bug? Thanks!

Xuting

The command line I used:
   bin/pig -x local test.pig

test.pig:
  register 'a.py' using org.apache.pig.scripting.jython.JythonScriptEngine as test;
  A = load 'input.txt' as (x:chararray);
  B = foreach A generate test.a(x) as y;
  C = group B by (y);
  store C into 'C';

a.py:
  #!/usr/bin/python
  @outputSchema(""n:chararray"")
  def a(n):
	return n.lower();

input.txt:
         
  a


","16/Dec/11 04:35;anitharaju;Hi Xuting, 

I had run in Map reduce mode. Like discussed over chat, i have sent you the code to reproduce the same.

Regards,
Anitha","19/Dec/11 19:06;xutingz;Hi Anitha,

   Thanks for the help. I have run the tests in MapReduce mode in Pig 0.9 and trunk, it all seems give the correct error message. I think this should be a Hadoop problem or communication problem between Hadoop and Pig. 

   The mapRep retrieves the running reports of the Map tasks that are running on Hadoop and then Pig will use the information stored in the reports to create different types of exception and error messages. If the mapRep is null, no exception information can be retrieved and then the wrong error is thrown.","19/Dec/11 19:11;xutingz;I think this problem is linked to PIG-1829,",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The link to the Documentation from the main page is out of date and broken,PIG-2210,12518427,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,daijy,amosshapira,amosshapira,10/Aug/11 11:11,10/Aug/11 23:14,13/Mar/19 23:13,10/Aug/11 18:49,site,,,,,,,,,,,,,,,,,,site,,,0,,,,,,,,"The link from http://pig.apache.org/ titled ""Read the documentation."" under ""Getting Started"" points to http://pig.apache.org/docs/r0.8.0/, which doesn't exist.

Following the parent path I found http://pig.apache.org/docs/r0.9.0/ and http://pig.apache.org/docs/r0.8.1/.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-08-10 17:45:24.465,,,no_permission,,,,,,,,,,,,,65518,,,,Wed Aug 10 23:14:59 UTC 2011,,,,,,,0|i0h1lb:,97520,,,,,,,,,,"10/Aug/11 17:45;daijy;Thanks for reporting. There is some issue on Apache website prevent us refreshing now, but I will track it down. It will be fixed in a few days.",10/Aug/11 18:49;daijy;Fixed.,"10/Aug/11 23:14;amosshapira;Thanks. I see the link pointing to 0.9.0 now.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix errors in pig grammars reported by ANTLRWorks,PIG-2060,12506812,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,azaroth,azaroth,azaroth,11/May/11 08:28,15/Jul/11 15:49,13/Mar/19 23:13,15/Jul/11 15:49,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"There are various errors in pig's grammar files highlighted by ANTLRWorks.
In particular, on token MATCHES, ANY and EVAL.
The first one should be removed, as there is already STR_OP_MATCHES,
the second one is an imaginary tokens that should be defined in the appropriate section.
On the third one I am not sure.
I have been told it is from the old parsers but it is not used anywhere. Is it correct?
Is it reserved for future uses? Has it anything to do with FUNC_EVAL?",,,,,,,,,,,,,,,,,15/Jul/11 15:47;thejas;PIG-2060.1.patch;https://issues.apache.org/jira/secure/attachment/12486633/PIG-2060.1.patch,23/May/11 11:52;azaroth;PIG-2060.patch;https://issues.apache.org/jira/secure/attachment/12480101/PIG-2060.patch,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-07-15 15:47:31.257,,,no_permission,,,,,,,,,,,,,67942,,,,Fri Jul 15 15:49:16 UTC 2011,,,,,,,0|i0h0sf:,97390,,,,,,,,,,"23/May/11 11:52;azaroth;Removed MATCHES and EVAL, added ANY as a token. Fixed a naming issue in AST grammar files (substituted STDERROR to ERROR).","15/Jul/11 15:47;thejas;+1 
Regenerated patch for latest svn trunk (PIG-2060.1.patch).","15/Jul/11 15:49;thejas;Patch committed to trunk.
Thanks Gianmarco!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implementation of Sample should use LessThanExpression instead of LessThanEqualExpression,PIG-2136,12511053,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,azaroth,azaroth,azaroth,21/Jun/11 08:14,23/Jun/11 18:53,13/Mar/19 23:13,23/Jun/11 18:53,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"Currently LogicalPlanBuilder uses a filter with 'Math.random() <= const' to implement Sample.
Actually it should use 'Math.random() < const' to be correct, because Math.random() generates a number 0 <= x < 1",,,,,,,,,,,,,,,,,21/Jun/11 08:20;azaroth;PIG-2136.patch;https://issues.apache.org/jira/secure/attachment/12483252/PIG-2136.patch,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-23 18:53:34.98,,,no_permission,,,,,,,,,,,,,70154,,,,Thu Jun 23 18:53:34 UTC 2011,,,,,,,0|i0h173:,97456,,,,,,,,,,"23/Jun/11 18:53;thejas;+1 . Patch committed to trunk. Thanks Gianmarco !
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestAccumulator fails on trunk,PIG-1539,12471037,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,azaroth,azaroth,07/Aug/10 13:46,08/Aug/10 13:31,13/Mar/19 23:13,08/Aug/10 13:31,0.8.0,0.9.0,,,,,,,,,,,,,,,,,impl,,,0,,,,,,,,"TestAccumulator fails systematically on trunk.
I have already tried 5 times with no success.
Once I got successful testing exit result but I still got errors in the log.

Here the error.

Testcase: testAccumWithBuildin took 17.202 sec
        Caused an ERROR
Unable to open iterator for alias D
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias D
        at org.apache.pig.PigServer.openIterator(PigServer.java:728)
        at org.apache.pig.test.TestAccumulator.testAccumWithBuildin(TestAccumulator.java:495)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2088: Unable to get results for: hdfs://localhost:37780/tmp/temp1145144453/tmp-630741957:org.apache.pig.impl.io.InterStorage
        at org.apache.pig.backend.hadoop.executionengine.HJob.getResults(HJob.java:99)
        at org.apache.pig.PigServer.openIterator(PigServer.java:716)
Caused by: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://localhost:37780/tmp/temp1145144453/tmp-630741957
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:224)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigFileInputFormat.listStatus(PigFileInputFormat.java:37)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:241)
        at org.apache.pig.impl.io.ReadToEndLoader.init(ReadToEndLoader.java:153)
        at org.apache.pig.impl.io.ReadToEndLoader.<init>(ReadToEndLoader.java:115)
        at org.apache.pig.backend.hadoop.executionengine.HJob.getResults(HJob.java:92)

Caused by this
INFO mapred.TaskInProgress: Error from attempt_20100807152543110_0002_r_000000_0: org.apache.pig.backend.executionengine.ExecException: ERROR 2078: Caught error from UDF: org.apache.pig.test.utils.AccumulatorBagCount [exec() should not be called.]
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:262)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:283)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:355)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.runPipeline(PigMapReduce.java:435)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.processOnePackageOutput(PigMapReduce.java:403)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:383)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce.reduce(PigMapReduce.java:251)
        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
        at org.apache.hadoop.mapred.Child.main(Child.java:170)
Caused by: java.io.IOException: exec() should not be called.
        at org.apache.pig.test.utils.AccumulatorBagCount.exec(AccumulatorBagCount.java:56)
        at org.apache.pig.test.utils.AccumulatorBagCount.exec(AccumulatorBagCount.java:28)
        at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:229)
        ... 11 more

",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-08-08 03:24:23.036,,,no_permission,,,,,,,,,,,,,165020,,,,Sun Aug 08 13:31:22 UTC 2010,,,,,,,0|i0gwyv:,96771,,,,,,,,,,"08/Aug/10 03:24;daijy;Hi, Gianmarco, 
I saw this error initially, but when I do an ""ant clean"" and rerun the test, the error go away. Can you try it?","08/Aug/10 13:31;azaroth;Thanks Daniel.
It worked. I assume there are temp files that need to be cleaned up when testing with MiniCluster.
It might be worth opening a separate Jira for this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TextDataParser does not handle delimiters from one complex type in another,PIG-898,12431765,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,sms,sms,30/Jul/09 05:06,12/Jul/10 18:32,13/Mar/19 23:13,12/Jul/10 18:32,0.4.0,,,,,,,,,,,,,,,,0.7.0,,impl,,,0,,,,,,,,"Currently, TextDataParser does not handle delimiters of one complex type in another. An example of such a case is key1(#value1} will not be parsed correctly. The production for strings matches any sequence of character that do not contain any delimiters for the complex types.",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-12 18:32:32.769,,,no_permission,,,,,,,,,,,,,164449,,,,Mon Jul 12 18:32:32 UTC 2010,,,,,,,0|i0gpnj:,95586,,,,,,,,,,"30/Jul/09 21:57;sms;In addition, empty bags, tuples and constants and nulls are not handled.",12/Jul/10 18:32;olgan;This has been addressed as part of 613,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig is broken loading .gz files,PIG-870,12429340,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,olgan,olgan,02/Jul/09 01:15,10/Jul/10 01:15,13/Mar/19 23:13,10/Jul/10 01:15,,,,,,,,,,,,,,,,,0.7.0,,,,,0,,,,,,,,"Looks like the code is trying to split a gz file which is not supported. In general, gz is a poor choice for compression with Pig since the parallelization is limitted to the number of files.",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-09-15 05:34:45.8,,,no_permission,,,,,,,,,,,,,164421,,,,Sat Jul 10 01:15:25 UTC 2010,,,,,,,0|i0gpb3:,95530,,,,,,,,,,"15/Sep/09 05:34;zjffdu;I load gz files, and it works fine.

I also looked into the code, (PigSlicer, line 99), it seems pig won't split a gz file

{code}
 if (name.endsWith("".gz"") || !splittable) {
                // Anything that ends with a "".gz"" we must process as a complete
                // file
                slices.add(new PigSlice(name, funcSpec, 0, size));
 }
{code}","15/Sep/09 05:36;zjffdu;BTW, if user use his own custom slicer, he has to control whether splitting files by himself.

",10/Jul/10 01:15;olgan;This is no longer relevant with Pig 0.7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding documentation for IsEmpty() in the Pig Latin Reference Manual,PIG-748,12421917,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,viraj,viraj,02/Apr/09 20:46,10/Jul/10 00:21,13/Mar/19 23:13,10/Jul/10 00:21,0.3.0,,,,,,,,,,,,,,,,0.7.0,,documentation,,,0,,,,,,,,"The built-in functions listed in:

http://wiki.apache.org/pig-data/attachments/FrontPage/attachments/plrm.htm

do not include IsEmpty. 

Kindly, add documentation for this useful built-in function.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,164314,,,,2009-04-02 20:46:50.0,,,,,,,0|i0gnvr:,95299,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unknown parameter causes pig to exit without message,PIG-680,12414787,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,neunand,neunand,14/Feb/09 00:33,09/Jul/10 20:02,13/Mar/19 23:13,09/Jul/10 20:02,,,,,,,,,,,,,,,,,0.7.0,,,,,0,,,,,,,,"This pig script: 

-- REGISTER $unknown;
x = LOAD 'nn' AS x:chararray;
DUMP x;

I run like this:

> java -jar ../../../../Pig/pig.jar -x local -file nn.pig

and pig does nothing, just exits without doiung anything.
But if I remove the $ from the first line of the script, then:

> java -jar ../../../../Pig/pig.jar -x local -file nn.pig
2009-02-13 16:30:01,062 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - 100% complete!
2009-02-13 16:30:01,063 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Success!!
(abc)
(def)

Similarly if I define the unknown parameter on the command line, it works fine:

> java -jar ../../../../Pig/pig.jar -x local -file nn.pig -param unknown=1
2009-02-13 16:32:23,652 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - 100% complete!
2009-02-13 16:32:23,653 [main] INFO  org.apache.pig.backend.local.executionengine.LocalPigLauncher - Success!!
(abc)
(def)

It seems that undefined parameters cause pig to exit without doing anything... even if they are within a comment...
-Andreas.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-09 20:02:57.191,,,no_permission,,,,,,,,,,,,,164257,,,,Fri Jul 09 20:02:57 UTC 2010,,,,,,,0|i0gn2v:,95169,,,,,,,,,,09/Jul/10 20:02;olgan;Both cases work fine in Pig 0.7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"describe does not display ""map"" types in the schema.",PIG-405,12403292,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,araceli,araceli,28/Aug/08 22:10,09/Jul/10 18:51,13/Mar/19 23:13,09/Jul/10 18:51,0.2.0,,,,,,,,,,,,,,,,0.7.0,,impl,,,0,,,,,,,,"In a load statement, if the type in the ""as clause"" is a ""map"", the describe statement does not show a type of ""map in the schema.  

A= load ':INPATH:/singlefile/studentcomplex10k' using PigStorage() as (s:map[],m,l);
describe A;
A: {s: ,m:bytearray,l:bytearray}

But it should be:
A: {s: map,m:bytearray,l:bytearray}","Linux 2.6.9-55.ELsmp #1 SMP Fri Apr 20 16:36:54 EDT 2007 x86_64 x86_64 x86_64 GNU/Linux
",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,690,,,,2008-08-28 22:10:35.0,,,,,,,0|i0gjtz:,94643,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"missing document ""review basics""",PIG-899,12431781,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,solrize,solrize,30/Jul/09 08:51,01/May/10 00:51,13/Mar/19 23:13,01/May/10 00:51,site,,,,,,,,,,,,,,,,,,documentation,,,0,,,,,,,,"http://hadoop.apache.org/pig/ ""getting started"" section has a link labelled ""review basics"" ( http://hadoop.apache.org/pig/docs/r0.3.0/quickstart.html ) which gives a 404.",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-05-01 00:51:12.258,,,no_permission,,,,,,,,,,,,,164450,,,,Sat May 01 00:51:12 UTC 2010,,,,,,,0|i0gpof:,95590,,,,,,,,,,01/May/10 00:51;olgan;The link is no longer there,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Jira Administrator: Please remove the Patch Available check box,PIG-147,12390898,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Minor,Fixed,,xuzh,xuzh,12/Mar/08 22:57,15/Jan/10 05:51,13/Mar/19 23:13,15/Jan/10 05:51,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"We now have ""Patch Available"" as a status of a JIRA Pig bug, so the ""Patch Available"" checkbox needs to be removed from the Find pane and the Edit page of the Pig project.",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,163805,,,,2008-03-12 22:57:44.0,,,,,,,0|i0ggof:,94132,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cwiki training links,PIG-5329,13132160,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,daijy,cskrabak,cskrabak,19/Jan/18 12:13,25/Jan/18 19:20,13/Mar/19 23:13,25/Jan/18 19:20,,,,,,,,,,,,,,,,,site,,documentation,,,0,,,,,,,,"Every single link on the page

[https://cwiki.apache.org/confluence/display/PIG/Pig+Training]

is broken.

Google finds better training courses, e.g.:

[https://hortonworks.com/tutorial/beginners-guide-to-apache-pig/]

[https://www.tutorialspoint.com/apache_pig/index.htm]

[https://cognitiveclass.ai/courses/introduction-to-pig/]",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-25 19:20:44.106,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 25 19:20:44 UTC 2018,,,,,,,0|i3p40v:,9223372036854775807,,,,,,,,,,"25/Jan/18 19:20;daijy;Updated, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hbase-storage-handler not found,PIG-4492,12787431,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,,JCS_123,JCS_123,01/Apr/15 18:30,03/Apr/15 13:31,13/Mar/19 23:13,03/Apr/15 13:31,0.14.0,,,,,,,,,,,,,,,,0.14.0,,,,,0,,,,,,,,"When running pig -useHCatalog I receive the following message
*ls: cannot access /usr/hdp/2.2.0.0-2041/hive-hcatalog/lib/*hbase-storage-handler-*.jar: No such file or directory*

Hostname(~)-1006> pig -useHCatalog
*ls: cannot access /usr/hdp/2.2.0.0-2041/hive-hcatalog/lib/*hbase-storage-handler-*.jar: No such file or directory*
15/04/01 14:16:02 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL
15/04/01 14:16:02 INFO pig.ExecTypeProvider: Trying ExecType : MAPREDUCE
15/04/01 14:16:02 INFO pig.ExecTypeProvider: Picked MAPREDUCE as the ExecType
2015-04-01 14:16:03,025 [main] INFO  org.apache.pig.Main - Apache Pig version 0.14.0.2.2.0.0-2041 (rexported) compiled Nov 19 2014, 15:24:46
2015-04-01 14:16:03,025 [main] INFO  org.apache.pig.Main - Logging error messages to: /home/username/pig_1427912163024.log
2015-04-01 14:16:03,055 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /home/username/.pigbootup not found
2015-04-01 14:16:04,044 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: HostName
grunt> quit//


Is there anywhere I can grab that jar file so I don't encounter this message, apparently it is not installed with Hortonworks 2.2 Ambari? also what is that jar file used for?","RHEL, Ambari 1.7, Hortonworks 2.2",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,no_permission,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 03 13:31:49 UTC 2015,,,,,,,0|i27npr:,9223372036854775807,,,,,,,,,,03/Apr/15 13:31;JCS_123;Grabbed the correct library off Maven Central Repository ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SVNException: svn: E155005: Directory './buildroot/lib/.svn' containing working copy admin area is missing,PIG-4081,12730497,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,,knoguchi,knoguchi,29/Jul/14 15:08,29/Jul/14 17:27,13/Mar/19 23:13,29/Jul/14 17:27,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,"After PIG-4047, my 'svn update' command started failing with 
{quote}
SVNException: svn: E155005: Directory './buildroot/lib/.svn' containing working copy admin area is missing
{quote}

This is probably because we started deleting ${lib.dir} in cleanup and recreating in build.
Since ./lib directory is empty at start, shall we just take it out from svn ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-07-29 16:26:11.897,,,no_permission,,,,,,,,,,,,,408570,,,,Tue Jul 29 17:25:52 UTC 2014,,,,,,,0|i1yatz:,408568,,,,,,,,,,29/Jul/14 16:26;rohini;We should. I thought Daniel already did that. ,"29/Jul/14 17:25;knoguchi;bq. We should. I thought Daniel already did that.
It's weird that I don't see the empty lib directory in git (git://git.apache.org/pig.git) but it still shows up in http://svn.apache.org/repos/asf/pig/trunk/.   

Deleted empty lib from svn trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Diff does not check if the first input is a bag or not,PIG-676,12414711,Bug,Resolved,PIG,Pig,software,olgan,Pig is a platform for analyzing large data sets.,http://pig.apache.org/,Trivial,Fixed,,araceli,araceli,13/Feb/09 00:37,09/Jul/10 22:45,13/Mar/19 23:13,09/Jul/10 22:45,,,,,,,,,,,,,,,,,0.7.0,,,,,0,,,,,,,,"I passed in a constant value ""0.0"" for the first argument, aexpecting that an error such as ""Invalid type passed to DIFF"" or some such error, but this is processed without any errors.

TEST: AggregateFunc_101

A =LOAD ':INPATH:/types/DataAll' USING PigStorage() AS ( Fint:int, Flong:long, Fdouble:double, Ffloat:float, Fchar:chararray, Fchararray:chararray, Fbytearray:bytearray, Fmap:map[], Fbag:BAG{ t:tuple( name, age, avg ) }, Ftuple:( name:chararray, age:int, avg:float) );
B =GROUP A ALL; 
X =FOREACH B GENERATE A.Fint, DIFF( 0.0, A.Fbag );
STORE X INTO ':OUTPATH:' USING PigStorage();","i686 i386 GNU/Linux

ava version ""1.6.0_02""
Java(TM) SE Runtime Environment (build 1.6.0_02-b05)
Java HotSpot(TM) Client VM (build 1.6.0_02-b05, mixed mode, sharing)
",,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-06 11:10:53.193,,,no_permission,,,,,,,,,,,,,164253,,,,Fri Jul 09 22:45:55 UTC 2010,,,,,,,0|i0gn13:,95161,,,,,,,,,,09/Jul/10 22:45;olgan;I checked the code - it is now doing the right thing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
